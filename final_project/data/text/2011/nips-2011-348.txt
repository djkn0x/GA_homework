TDγ : Re-evaluating Complex Backups in Temporal
Difference Learning

George Konidaris∗∗†
MIT CSAIL†
Cambridge MA 02139
gdk@csail.mit.edu

Scott Niekum∗‡
Philip S. Thomas∗‡
University of Massachusetts Amherst‡
Amherst MA 01003
{sniekum,pthomas}@cs.umass.edu

Abstract

We show that the λ-return target used in the TD(λ) family of algorithms is the
maximum likelihood estimator for a speciﬁc model of how the variance of an n-
step return estimate increases with n. We introduce the γ -return estimator, an
alternative target based on a more accurate model of variance, which deﬁnes the
TDγ family of complex-backup temporal difference learning algorithms. We de-
rive TDγ , the γ -return equivalent of the original TD(λ) algorithm, which elimi-
nates the λ parameter but can only perform updates at the end of an episode and
requires time and space proportional to the episode length. We then derive a sec-
ond algorithm, TDγ (C ), with a capacity parameter C . TDγ (C ) requires C times
more time and memory than TD(λ) and is incremental and online. We show that
TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains,
and that TDγ (C ) performs as well as or better than TDγ for intermediate settings
of C .

1

Introduction

Most reinforcement learning [1] algorithms are value-function based—learning is performed by es-
timating the expected return (discounted sum of rewards) obtained by following the current policy
from each state, and then updating the policy based on the resulting so-called value function. Efﬁ-
cient value function learning algorithms are crucial to this process and have been the focus of a great
deal of reinforcement learning research.
The most successful and widely-used family of value function algorithms is the TD(λ) family [2].
The TD(λ) family forms an estimate of return, called the λ-return, that blends both low variance,
bootstrapped and biased temporal-difference estimates of return with high variance, unbiased Monte
Carlo estimates of return using a parameter λ. While several different algorithms exist within the
TD(λ) family—the original incremental and online algorithm [2], replacing traces [3], least-squares
algorithms [4], algorithms for learning state-action value functions [1, 5], and algorithms for adapt-
ing λ [6], among others—the λ-return formulation has remained unchanged since its introduction in
1988 [2]. Our goal is to understand the modeling assumptions implicit in the λ-return formulation
and improve them.
We show that the λ-return estimator is only a maximum-likelihood estimator of return given a spe-
ciﬁc model of how the variance of an n-step return estimate increases with n. We propose a more
accurate model of that variance increase and use it to obtain an expression for a new return estimator,
the γ -return. This results in the TDγ family of algorithms, of which we derive TDγ , the γ -return
version of the original TD(λ) algorithm. TDγ is only suitable for the batch setting where updates
occur at the end of the episode and requires time and space proportional to the length of the episode,
∗All three authors are primary authors on this occasion.

1

but it eliminates the λ parameter. We then derive a second algorithm, TDγ (C ), which requires C
times more time and memory than TD(λ) and can be used in an incremental and online setting. We
show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains, and that
TDγ (C ) performs as well as or better than TDγ for intermediate settings of C .

2 Complex Backups

Estimates of return lie at the heart of value-function based reinforcement learning algorithms: a
value function V π (which we denote here as V , assuming a ﬁxed policy) estimates return from each
state, and the learning process aims to reduce the error between estimated and observed returns.
Temporal difference (TD) algorithms use a return estimate obtained by taking a single transition in
the MDP and then estimating the remaining return using the value function itself:
RTD
= rt + γV (st+1 ),
st
where RTD
is the return estimate from state st and rt is the reward for going from st to st+1 via
st
action at . Monte Carlo algorithms (for episodic tasks) do not use intermediate estimates but instead
L−1(cid:88)
use the full return sample directly:
i=0
for an episode L transitions in length after time t. These two types of return estimates can be
considered instances of the more general notion of an n-step return sample, for n ≥ 1:
= rt + γ rt+1 + γ 2 rt+2 + . . . + γ n−1 rt+n−1 + γ nV (st+n ).
R(n)
st

γ i rt+i ,

RMC
st

(1)

(2)

=

(3)

Here, n transitions are observed from the MDP and the remaining portion of return is estimated using
the value function. The important observation here is that all n-step return estimates can be used
simultaneously for learning. The TD(λ) family of algorithms accomplishes this using a parameter
λ ∈ [0, 1] to average n-step return estimates, according to the following equation:
∞(cid:88)
n=0

= (1 − λ)

λnR(n+1)
st

Rλ
st

(4)

.

Note that for any episodic MDP we always obtain a ﬁnite episode length. The usual formulation of
an episodic MDP uses absorbing terminal states—states where only zero-reward self-transitions are
available. In such cases the n-step returns past the end of the episode are all equal, and therefore
TD(λ) allocates the weights corresponding to all of those return estimates to the ﬁnal transition.
st , known as the λ-return, is an estimator that blends one-step temporal difference estimates (which
Rλ
are biased, but low variance) at λ = 0 and Monte Carlo estimates (which are unbiased, but high
variance) at λ = 1. This forms the target for the entire family of TD(λ) algorithms, whose members
differ largely in their use of the resulting estimates to update the value function.
What makes this a good way to average the n-step returns? Why choose this method over any
other? Viewing this as a statistical estimation problem where each n-step return is a sample of the
true return, under what conditions and for what model is equation (4) a good estimator for return?
The most salient feature of the n-step returns is that their variances increase with n. Therefore, con-
sider the following model: each n-step return estimate R(n)
is sampled from a Gaussian distribution
st
centered on the true return, Rst ,1 with variance k(n) that is some increasing function of n. If we
assume the n-step returns are independent,2 then the likelihood function for return estimate ˆRst is:
L(cid:89)
n=1

L( ˆRst |R(1)
st

| ˆRst , k(n)).

N (R(n)
st

, ..., R(n)
st

; k) =

(5)

1We should note that this assumption is not quite true: only the Monte Carlo return is unbiased.
2Again, this assumption is not true. However, it allows us to obtain a simple, closed-form estimator.

2

ˆRst =

(cid:80)L
Maximizing the log of this equation obtains the maximum likelihood estimator for ˆRst :
(cid:80)L
n=1 k(n)−1R(n)
st
n=1 k(n)−1
Thus, we obtain a weighted sum: each n-step return is weighted by the inverse of its variance and
since (cid:80)∞
the entire sum is normalized so that the weights sum to 1. From here we can see that if we let L go
to inﬁnity and set k(n) = λ−n , 0 ≤ λ ≤ 1, then we obtain the λ-return estimator in equation (4),
n=0 λn = 1/(1 − λ).
Thus, λ-return (as used in the TD(λ) family of algorithms) is the maximum-likelihood estimator of
return under the following assumptions:

(6)

.

1. The n-step returns from a given state are independent.
2. The n-step returns from a given state are normally distributed with a mean of the true return.
3. The variances of the n-step returns from each state increase according to a geometric pro-
gression in n, with common ratio λ−1 .

All of these assumptions could be improved, but the third is the most interesting. In this view, the
variance of an n-step sample return increases geometrically with n and λ parametrizes the shape of
this geometric increase.

V ar

V ar

C ov

=C ov

=C ov

=C ov

=V ar

=V ar

(7)

(8)

, R(n)
st

3 γ -Return and the TDγ Family of Algorithms
(cid:104)
(cid:105)
(cid:104)
(cid:105)
Consider the variance of an n-step sample return, n > 1:
(cid:105)
(cid:105)
(cid:104)
(cid:104)
− γ n−1V (st+n−1 ) + γ n−1 rt+n−1 + γ nV (st+n )
R(n−1)
R(n)
st
st
(cid:105)
(cid:104)
V (st+n−1 ) − (rt+n−1 + γV (st+n ))
R(n−1)
+ γ 2(n−1)V ar
st
, −γ n−1V (st+n−1 ) + γ n−1 rt+n−1 + γ nV (st+n )
R(n−1)
+ 2C ov
st
(cid:104)
(cid:105)
Examining the last term, we see that:
(cid:104)
(cid:105)
, −γ n−1V (st+n−1 ) + γ n−1 rt+n−1 + γ nV (st+n )
R(n−1)
st
(cid:104)
(cid:105) − C ov
(cid:104)
(cid:105)
− R(n−1)
R(n−1)
st
st
(cid:104)
(cid:105) − V ar
(cid:104)
R(n−1)
R(n−1)
st
st
R(n−1)
R(n−1)
st
st
Since R(n−1)
and R(n)
st are highly correlated—being two successive return samples—we assume that
st
C ov [R(n−1)
st and R(n−1)
st ] ≈ V ar [R(n−1)
, R(n)
] (equality holds when R(n)
are perfectly correlated).
(cid:104)
(cid:105) ≈ V ar
(cid:104)
(cid:105)
(cid:104)
(cid:105)
st
st
st
Thus, equation (12) is approximately zero. Hence, equation (8) becomes:
V (st+n−1 ) − (rt+n−1 + γV (st+n ))
R(n−1)
+ γ 2(n−1)V ar
R(n)
st
st
Notice that the ﬁnal term on the right hand side of equation (13) is the discounted variance of the
temporal difference error n-steps after the current state. We assume that this variance is roughly the
same for all states; let that value be κ. Since κ also approximates the variance of the 1-step return
n(cid:88)
(i.e., k(1) = κ), we obtain the following simple model of the variance of an n-step sample of return:
i=1

(cid:105)
, R(n−1)
st

γ 2(i−1)κ.

k(n) =

, R(n)
st

, R(n)
st

(9)

(10)

(11)

(12)

.

(13)

.

3

.

(14)

Rγ
st

=

,

(15)

where

w(n, L) =

w(n, L)R(n)
st

n=1 ((cid:80)n
= κ−1 (cid:80)L
Substituting equation (14) into the general return estimator in equation (6), we obtain:
L(cid:88)
κ−1 (cid:80)L
n=1 ((cid:80)n
i=1 γ 2(i−1) )−1R(n)
st
i=1 γ 2(i−1) )−1
n=1
((cid:80)n
n=1 ((cid:80)n
(cid:80)L
i=1 γ 2(i−1) )−1
i=1 γ 2(i−1) )−1
is the weight associated with the nth-step return in a trajectory of length L after time t. This estimator
has the virtue of being parameter-free since the κ values cancel. Therefore, we need not estimate κ—
under the assumption of independent, Gaussian n-step returns with variances increasing according
to equation (13), equation (15) is the maximum likelihood estimator for any value of κ. We call this
estimator the γ -return since it weights the n-step returns according to the discount factor.
Figure 1 compares the weightings obtained using λ-return and γ -return for a few example trajectory
lengths. There are two important qualitative differences. First, the λ-return weightings spike at the
end of the trajectory, whereas the γ -return weightings do not. This occurs because even though any
sample trajectory has ﬁnite length, the λ-return as deﬁned in equation (4) is actually an inﬁnite sum;
the remainder of the weight mass is allocated to the Monte Carlo return. This allows the normalizing
factor in equation (4) to be a constant, rather than having it depend on the length of the trajectory, as
it does in equation (15) for the γ -return. This signiﬁcantly complicates the problem of obtaining an
incremental algorithm using γ -return, as we shall see in later sections.

(16)

Figure 1: Example weights for trajectories of various lengths for λ-return (left) and γ -return (right).

Second, while the λ-return weightings tend to zero over time, the γ -return weightings tend toward
a constant. This means that very long trajectories will be effectively “cut-off ” after some point and
have effectively no contribution to the λ-return, whereas after a certain length in the γ -return all
n-step returns have roughly equal weighting. This also complicates the problem of obtaining an
incremental algorithm using γ -return: even if we were to assume inﬁnitely many n-step returns past
the end of the episode, the normalizing constant would not become ﬁnite.
Nevertheless, we can use the γ -return estimator to obtain an entire family of TDγ learning algo-
rithms; for any TD(λ) algorithm we can derive an equivalent TDγ algorithm.
In the following
section, we derive TDγ , the γ -return equivalent of the original TD(λ) algorithm.

4 TDγ
Given a set of m trajectories T = {τ1 , τ2 , . . . , τm }, where lτ = |τ | denotes the number of
t+1 ) tuples in the trajectory τ . Using approximator ˆVθ with parameters θ to approximate
(sτ
t , rτ
t , sτ

4

05101520253000.050.10.150.20.25Return Estimate LengthWeight  Lambda=0.75, L=10Lambda=0.85, L=20Lambda=0.8, L=3005101520253000.050.10.150.20.250.30.35Return Estimate LengthWeight  L=10, Gamma=0.95L=20, Gamma=0.95L=30, Gamma=0.95L=30, Gamma=0.8E (θ) =

(cid:33)2

.

R(n)
sτ
t

.

(17)

(18)

(19)

(20)

1
2

1
2

=

1
2

1
2

=

−w(n, lτ − t)

∆θ = −α∇θE (θ) = −α

w(n, lτ − t)R(n)
sτ
t

− ˆVθ (sτ
t )

(cid:17)2
− ˆVθ (sτ
t )

w(n, lτ − t)R(n)
(cid:104)
sτ
t

w(n, lτ − t)

(cid:33)2
− lτ −t(cid:88)
w(n, lτ − t) ˆVθ (sτ
(cid:105)(cid:33)2
t )
n=1
− ˆVθ (sτ
t )

(cid:16)
(cid:88)
lτ −1(cid:88)
V , the objective function for regression is:
Rγ
(cid:32)lτ −t(cid:88)
sτ
lτ −1(cid:88)
(cid:88)
t
τ ∈T
t=0
Because (cid:80)lτ −t
τ ∈T
n=1
t=0
(cid:32)lτ −t(cid:88)
n=1 w(n, lτ − t) = 1, we can write
(cid:88)
lτ −1(cid:88)
(cid:32)lτ −t(cid:88)
E (θ) =
lτ −1(cid:88)
(cid:88)
τ ∈T
t=0
n=1
τ ∈T
t=0
n=1
Our goal is to minimize E (θ). One approach is to descend the gradient ∇θE (θ), assuming that the
(cid:105) ∇θ ˆVθ (sτ
(cid:104)
R(n)
are noisy samples of V (sτ
t ) and not a function of θ , as in the derivation of TD(λ) [7]:
(cid:88)
lτ −1(cid:88)
lτ −t(cid:88)
sτ
t
− ˆVθ (sτ
R(n)
t )
t ),
sτ
t
τ ∈T
n=1
t=0
where α is a learning rate. We can substitute in n = u − t (where u is the current time step, st is the
state we want to update the value estimate of, and n is the length of the n-step return that ends at the
(cid:105) ∇θ ˆVθ (sτ
(cid:104)
lτ(cid:88)
lτ −1(cid:88)
(cid:88)
current time step) to get:
− ˆVθ (sτ
−w(u − t, lτ − t)
t )
t ).
τ ∈T
u=t+1
t=0
(cid:105) ∇θ ˆVθ (sτ
(cid:104)
lτ(cid:88)
u−1(cid:88)
(cid:88)
Swapping the sums allows us to derive the backward version of TDγ :
− ˆVθ (sτ
t )
t ).
τ ∈T
t=0
u=1
Expanding and rearranging the terms gives us an algorithm for TDγ when using linear function
(cid:32)u−1(cid:88)
(cid:34)
(cid:33)
(cid:35)
u−1(cid:88)
lτ(cid:88)
(cid:88)
approximation with weights θ :
−
)
(cid:34)
τ ∈T
t=0
u=1
i=t
(cid:88)
u−1(cid:88)
lτ(cid:88)
θ · (φsτ
(cid:88)
lτ(cid:88)
u−1(cid:88)
t
τ ∈T
t=0
u=1
τ ∈T
t=0
u=1

u−1(cid:80)
− γ u−tφsτ
γ i−t rsτ
t , a = φsτ
u , and b =
. This leads to
t is the feature vector at state sτ
where φsτ
t
i
i=t
TDγ for episodic tasks (given in Algorithm 1), which eliminates the eligibility trace parameter λ. For
episode length lτ and feature vector size F , the algorithm can be implemented with time complexity
of O(lτ F ) per step and space complexity O(lτ F ). Unfortunately, implementing this backward TDγ
incrementally is problematic: lτ is not known until the end of the trajectory is reached, and without

− γ u−t (θ · φsτ
u
(cid:32)u−1(cid:88)
i=t

) −

w(u − t, lτ − t) [θ · a − b] φsτ
t

,

) + (θ · φsτ
t
(cid:33)(cid:35)

w(u − t, lτ − t)

−w(u − t, lτ − t)

R(u−t)
sτ
t

(24)

(25)

(26)

w(u − t, lτ − t)

γ i−t rsτ
i

φsτ
t

= −α

= −α

(22)

(23)

γ i−t rsτ
i

− γ u−tφsτ
u

∆θ = −α

∆θ = −α

(21)

φsτ
t

∆θ = −α

R(u−t)
sτ
t

5

Algorithm 1 TDγ
Given: A discount factor, γ
1: θ ← →
0
2: for each trajectory τ ∈ T do
Store φ0 in memory
3:
for u = 1 to lτ do
4:
Store φu and ru−1 in memory
5:
δ ← →
0
6:
for t = 0 to u − 1 do
b ← u−1(cid:80)
7:
a ← φt − γ u−tφu
8:
γ i−t ri
δ ← δ + w(u − t, lτ − t)[θ · a − b]φt
i=t
10:
end for
11:
θ ← θ − αδ
12:
end for
13:
Discard all φ and r from memory
14:
15: end for

9:

it, the normalizing constant of the weights used in the updates cannot be computed. Thus, Algorithm
1 can only be used for batch updates where each episode’s trajectory data is stored until an update
is performed at the end of an episode; this is often undesirable, and in continuing tasks, impossible.
TD(λ) is able to achieve O(F ) time and space for two reasons. First, the weight normalization is
a constant and does not depend on the length of the episode. Second, the operation that must be
performed on each trace is the same—a multiplication by λ. Thus, TD(λ) need only store the sum
of the return estimates from each state, rather than having to store each individually.
One approach to deriving an incremental algorithm is to use only the ﬁrst C n-step returns from
each state, similar to truncated temporal differences [8]. This eliminates the ﬁrst barrier: weight
normalization no longer relies on the episode length, except for the ﬁnal C − 1 states, which can
be corrected for after the episode ends. This approach has time complexity O(C F ) and space
complexity O(C F )—and is therefore C times more expensive than TD(λ)—and replaces λ with
the more intuitive parameter C rather than eliminating it, but it affords the incremental TDγ (C )
algorithm given in Algorithm 2. Note that setting C = 1 obtains TD(0), and C ≥ lτ obtains TDγ .

5 Empirical Comparisons

Figure 2 shows empirical comparisons of TD(λ) (for various values of λ), TDγ and TDγ (C ) for 5
common benchmarks. The ﬁrst is a 10 × 10 discrete gridworld with the goal in the bottom right,
deterministic movement in the four cardinal directions, a terminal reward of +10 and −0.5 for all
other transitions, and γ = 1.0. For the gridworld, the agent selected one of the optimal actions with
probability 0.4, and each of the other actions with probability 0.2. The second domain is the 5-state
discrete chain domain [1] with random transitions to the left and right, and γ = 0.95. The third
domain is the pendulum swing-up task [7] with a reward of 1.0 for entering a terminal state (the
pendulum is nearly vertical) and zero elsewhere, and γ = 0.95. The optimal action was selected
with probability 0.5, with a random action selected otherwise. The fourth domain is mountain car
[1] with γ = 0.99, and using actions from a hand-coded policy with probability 0.75, and random
actions otherwise. The ﬁfth and ﬁnal domain is the acrobot [1] with a terminal reward of 10 and
−0.1 elsewhere. A random policy was used with γ = 0.95. In all cases the start state was selected
uniformly from the set of nonterminal states. A 5th order Fourier basis [9] was used as the function
approximator for the 3 continuous domains. We used 10, 5, 10, 3, and 10 trajectories, respectively.
TDγ outperforms TD(λ) for all settings of λ in 4 out of the 5 domains. In the chain domain TDγ
performs better than most settings of λ but slightly worse than the optimal setting. An interesting and
somewhat unexpected result is that TDγ (C ) with a relatively low setting of C does at least as well
as—or in some cases better than—TDγ . This could occur because the n-step returns become very

6

Algorithm 2 TDγ (C )
Given: A discount factor, γ
A cut-off length, C
1: θ ← →
0
2: for each trajectory τ ∈ T do
Store φ0 in memory
3:
for u = 1 to lτ do
4:
If u > C , discard φu−C−1 , θu−C−1 , and ru−C−1 from memory
5:
θu−1 ← θ
6:
Store φu , θu−1 , and ru−1 in memory
7:
δ ← →
0
8:
m = max(0, u − C )
9:
for t = m to u − 1 do
b ← u−1(cid:80)
10:
a ← φt − γ u−tφu
11:
γ i−t ri
δ ← δ + w(u − t, C )[θ · a − b]φt
i=t
end for
θ ← θ − αδ
end for

13:
14:
15:
16:

12:

17: m = min(lτ , C )
θ ← θ lτ −m
18:
for ˆu = lτ − m to lτ do
19:
δ ← →
0
20:
for t = m to ˆu − 1 do
b ← ˆu−1(cid:80)
21:
a ← φt − γ ˆu−tφ ˆu
22:
γ i−t ri
δ ← δ + w( ˆu − t, m − t)[θ · a − b]φt
i=t
24:
end for
25:
θ ← θ − αδ
26:
Discard φ ˆu , θ ˆu−1 , and r ˆu−1 from memory
27:
end for
28:
29: end for

23:

similar for large n due to either γ discounting diminishing the difference, or to the additional one-
step rewards accounting for a very small fraction of the total return. These near-identical estimates
will accumulate a large fraction of the weighting (see Figure 1) and come to dominate the γ -return
estimate. This suggests that once the returns start to become almost identical they should not be
considered independent samples and should instead be discarded.

6 Discussion and Future Work

An immediate goal of future work is ﬁnding an automatic way to set C . We may be able to calculate
bounds on the diminishing differences between n-step returns due to γ , or empirically track the
point at which those differences begin to diminish. Another avenue for future research is deriving a
version of TDγ or TDγ (C ) that provably converges for off-policy data with function approximation,
most likely using recent insights on gradient-descent based TD algorithms [10]. Thereafter, we aim
to develop an algorithm based on γ -return for control rather than just prediction, for example Sarsaγ .
We have shown that the widely used λ-return formulation is the maximum-likelihood estimator of
return given three assumptions (see section 2). The results presented here have shown that reevaluat-
ing just one of these assumptions results in more accurate value function approximation algorithms.
We expect that re-evaluating all three will prove a fruitful avenue for future research.

7

Gridworld

Chain

Pendulum

Mountain Car

Acrobot

Figure 2: Mean squared error (MSE) over sample trajectories from ﬁve benchmark domains for
TD(λ) with various settings of λ, TDγ , and TDγ (C ), for various settings of C . Error bars are
standard error over 100 samples. Each result is the minimum MSE (weighted by state visitation
frequency) between each algorithm’s approximation and the correct value function (obtained using
a very large number of Monte Carlo samples), found by searching over α at increments of 0.0001.

Acknowledgments

We would like to thank David Silver, Hamid Maei, Gustavo Goretkin, Sridhar Mahadevan and Andy
Barto for useful discussions. George Konidaris was supported in part by the AFOSR under grant
AOARD-104135 and the Singapore Ministry of Education under a grant to the Singapore-MIT In-
ternational Design Center. Scott Niekum was supported by the AFOSR under grant FA9550-08-1-
0418.

8

300340E220260300=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10000080.090.1E0.050.060.070.08=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10000.60.8E0.20.40.6=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10009001000E600700800=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(100056905710E563056505670=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(1000References
[1] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,
MA, 1998.
[2] R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
3(1):9–44, 1988.
[3] S. Singh and R.S. Sutton. Reinforcement learning with replacing eligibility traces. Machine
Learning, 22:123–158, 1996.
[4] J.A. Boyan. Least squares temporal difference learning. In Proceedings of the 16th Interna-
tional Conference on Machine Learning, pages 49–56, 1999.
[5] H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artiﬁcial
General Intelligence, 2010.
[6] C. Downey and S. Sanner. Temporal difference Bayesian model averaging: A Bayesian per-
spective on adapting lambda. In Proceedings of the 27th International Conference on Machine
Learning, pages 311–318, 2010.
[7] K. Doya. Reinforcement learning in continuous time and space. Neural Computation,
12(1):219–245, 2000.
[8] P. Cichosz. Truncating temporal differences: On the efﬁcient implementation of TD(λ) for
reinforcement learning. Journal of Artiﬁcial Intelligence Research, 2:287–318, 1995.
[9] G.D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforcement
learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial
Intelligence, pages 380–385, 2011.
[10] R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvari, and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approxi-
mation. In Proceedings of the 26th International Conference on Machine Learning, 2009.

9


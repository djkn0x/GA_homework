Multilinear Subspace Regression: An Orthogonal
Tensor Decomposition Approach

Qibin Zhao 1 , Cesar F. Caiafa 2 , Danilo P. Mandic 3 , Liqing Zhang4 , Tonio Ball 5 , Andreas
Schulze-Bonhage5 , and Andrzej Cichocki1

1Brain Science Institute, RIKEN, Japan
2 Instituto Argentino de Radioastronom´ıa (IAR), CONICET, Argentina
3Dept. of Electrical & Electronic Engineering, Imperial College, UK
4Dept. of Computer Science & Engineering, Shanghai Jiao Tong University, China
5BCCN, Albert-Ludwigs-University, Germany
qbzhao@brain.riken.jp

Abstract

A multilinear subspace regression model based on so called latent variable de-
composition is introduced. Unlike standard regression methods which typically
employ matrix (2D) data representations followed by vector subspace transfor-
mations, the proposed approach uses tensor subspace transformations to model
common latent variables across both the independent and dependent data. The
proposed approach aims to maximize the correlation between the so derived la-
tent variables and is shown to be suitable for the prediction of multidimensional
dependent data from multidimensional independent data, where for the estimation
of the latent variables we introduce an algorithm based on Multilinear Singular
Value Decomposition (MSVD) on a specially deﬁned cross-covariance tensor. It
is next shown that in this way we are also able to unify the existing Partial Least
Squares (PLS) and N-way PLS regression algorithms within the same framework.
Simulations on benchmark synthetic data conﬁrm the advantages of the proposed
approach, in terms of its predictive ability and robustness, especially for small
sample sizes. The potential of the proposed technique is further illustrated on a
real world task of the decoding of human intracranial electrocorticogram (ECoG)
from a simultaneously recorded scalp electroencephalograph (EEG).

1

Introduction

The recent progress in sensor technology has made possible a plethora of novel applications, which
typically require increasingly large amount of multidimensional data, such as large-scale images,
3D video sequences, and neuroimaging data. To match the data dimensionality, tensors (also called
multiway arrays) have been proven to be a natural and efﬁcient representation for such massive data.
In particular, tensor subspace learning methods have been shown to outperform their corresponding
vector subspace methods, especially for small sample size problems [1, 2]; these methods include
multilinear PCA [3], multilinear LDA [4, 5], multiway covariates regression [6] and tensor subspace
analysis [7]. These desirable properties have made tensor decomposition becoming a promising tool
in exploratory data analysis [8, 9, 10, 11].
The Partial Least Squares (PLS) is a well-established estimation, regression and classiﬁcation frame-
work that aims to predict a set of dependent variables (responses) Y from a large set of independent
variables (predictors) X, and has been proven to be particularly useful for highly collinear data [12].
Its optimization objective is to maximize pairwise covariance of a set of latent variables (also called
latent vectors, score vectors) by projecting both X and Y onto a new subspace. A popular way

1

to estimate the model parameters is the Non-linear Iterative Partial Least Squares (NIPALS) [13],
an iterative procedure similar to the power method; for an overview of PLS and its applications in
multivariate regression analysis see [14, 15, 16]. As an extension of PLS to multiway data, the N -
way PLS (NPLS) decomposes the independent and dependent data into rank-one tensors, subject to
maximum pairwise covariance of the latent vectors [17]. The widely reported sensitivity to noise of
PLS is attributed to redundant (irrelevant) latent variables, whose selection remains an open prob-
lem. The number of latent variables also dependents on the rank of independent data, resulting in
overﬁtting when the number of observations is smaller than the number of latent variables. Although
the standard PLS can also handle an N -way tensor dataset differently, e.g. applied on a mode-1 ma-
tricization of X and Y , this would make it difﬁcult to interpret the loadings as the physical meaning
would be lost due to the unfolding.
To alleviate these issues, in this study, a new tensor subspace regression model, called the Higer-
Order Partial Least Squares (HOPLS), is proposed to predict an M th-order tensor Y from an N th-
order tensor X. It considers each data sample as a higher order tensor represented as a linear com-
bination of tensor subspace bases. This way, the dimensionality of parameters estimated by HOPLS
is much smaller than the dimensionality of parameters estimated by PLS, thus making HOPLS par-
ticularly suited for small sample sizes. In addition, the latent variables and tensor subspace can be
optimized to ensure a maximum correlation between the latent variables of X and Y with a con-
straint imposed to ensure a special structure of the core tensor. This is achieved by a simultaneous
stepwise rank-(1, L2 , . . . , LN ) decompositions of X and rank-(1, K2 , . . . , KM ) decomposition of
Y [18], using multiway singular value decomposition (MSVD) [19].

2 Preliminaries

2.1 Notation and deﬁnitions

We denote N th-order tensors (multi-way arrays) by underlined boldface capital letters, matrices
(two-way arrays) by boldface capital letters, and vectors by boldface lower-case letters, e.g., X, P
and t are examples of a tensor, a matrix and a vector, respectively.
The ith entry of a vector x is denoted by xi , element (i, j ) of a matrix X by xij , and element
(i1 , i2 , . . . , iN ) of an N th-order tensor X ∈ RI1×I2×···×IN by xi1 i2 ...iN or (X)i1 i2 ...iN . Indices
typically range from 1 to their capital version, e.g., iN = 1, . . . , IN . The nth matrix in a sequence
is denoted by a superscript in parentheses, e.g., X(n) . The nth-mode matricization of a tensor X is
denoted by X(n) .
The n-mode product of a tensor X ∈ RI1×···×In×···×IN and matrix A ∈ RJn×In is denoted by
Y = X ×n A ∈ RI1×···×In−1×Jn×In+1×···×IN and is deﬁned as:
(cid:88)
in

yi1 i2 ...in−1 jn in+1 ...iN =

xi1 i2 ...in ...iN ajn in .

(1)

The n-mode cross-covariance between an N th-order tensor X ∈ RI1×···×In×···×IN and an M th-
order tensor Y ∈ RJ1×···×Jn×···×JM with the same size In = Jn on the nth-mode, denoted by
COV{n;n} (X, Y) ∈ RI1×···×In−1×In+1×···×IN ×J1×···×Jn−1×Jn+1×···×JM , is deﬁned as

C = COV{n;n} (X, Y) =< X, Y >{n;n} ,
where the symbol < •, • >{n;n} represents a multiplication between two tensors, and is deﬁned as
In(cid:88)
in=1

ci1 ,...,in−1 ,in+1 ...iN ,j1 ,...,jn−1 jn+1 ...jM =

xi1 ,...,in ,...,iN yj1 ,...,in ,...,jM .

(3)

(2)

2

2.2 Partial Least Squares

X = TPT + E =

tr pT
r + E,

(4)

The objective of the PLS method is to ﬁnd a set of latent vectors that explains as much as possible the
R(cid:88)
covariance between X and Y , which can be achieved by performing the following decomposition
R(cid:88)
r=1
r=1
where T = [t1 , t2 , . . . , tR ] ∈ RI×R is a matrix of R extracted orthogonal latent variables from X,
that is, TT T = I, and U = [u1 , u2 , . . . , uR ] ∈ RI×R are latent variables from Y that have max-
imum covariance with T column-wise. The matrices P and C represent loadings (vector subspace
bases) and E and F are residuals. A useful property is that the relation between T and U can be
approximated linearly by
U ≈ TD,
(5)
where D is an (R × R) diagonal matrix, and scalars drr = uT
r tr play the role of regression
r tr /tT
coefﬁcients.

Y = UCT + F =

ur cT
r + F,

3 Higher-order PLS (HOPLS)

Figure 1: Schematic diagram of the HOPLS model: decomposing X as a sum of rank-(1, L2 , L3 )
tensors. Decomposition for Y follows a similar principle.
For an N th-order independent tensor X ∈ RI1×···×IN and an M th-order dependent tensor Y ∈
RJ1×···×JM , having the same size on the ﬁrst mode1 , i.e., I1 = J1 , similar to PLS, our objective is
to ﬁnd the optimal subspace approximation of X and Y , in which the latent vectors of independent
and dependent variables have maximum pairwise covariance.

3.1 Proposed model

The new tensor subspace represented by the Tucker model can be obtained by approximating X
with a sum of rank-(1, L2 , . . . , LN ) decompositions (see Fig.1), while dependent data Y are ap-
proximated by a sum of rank-(1, K2 , . . . , KM ) decompositions. From the relation between the

1The ﬁrst mode is usually associated with the sample mode or time mode, and for each sample, we have an
independent data represented by an (N − 1)th-order tensor and a dependent data represented by an (M − 1)th-
order tensor.

3

+...+=+=+Raw DataLatent variablesLoadingsResiduals(6)

,

.

+E,

+F,

X =

Y =

(7)

(8)

r ×3 · · ·×N P(N −1)
Gr ×1 tr ×2 P(1)
r

latent vectors in (5), upon replacing U by TD and integrating D into the core tensor, the operation
R(cid:88)
of HOPLS can be expressed as
R(cid:88)
r=1
r ×3 · · ·×M Q(M −1)
Dr ×1 tr ×2 Q(1)
(cid:110)
(cid:111)N −1
r
r=1
(cid:110)
(cid:111)M −1
where R is the number of latent vectors, tr ∈ RI1 is the r-th latent vector,
∈
P(n)
r
n=1
∈ RJn+1×Kn+1 (Jn+1 (cid:62) Kn+1 ) are loading
RIn+1×Ln+1 (In+1 (cid:62) Ln+1 ) and
Q(m)
r
matrices corresponding to the latent vector tr on mode-n and mode-m respectively, and Gr ∈
m=1
R1×L2×···×LN and Dr ∈ R1×K2×···×KM are core tensors. Note that the new tensor subspace for X
{ (cid:101)Pr }R
is spanned by R tensor bases represented by Tucker model
r ×3 · · ·×N P(N −1)
r=1 = Gr ×2 P(1)
r
{ (cid:101)Q
while the new subspace for Y is represented by Tucker model
}R
r ×3 · · ·×N Q(M −1)
r=1 = Dr ×2 Q(1)
r
r
The rank-(1, L2 , . . . , LN ) decomposition in (6) is not unique, however, since MSVD generates both
an all-orthogonal core [19] and column-wise orthogonal factors, these can be applied to obtain the
unique components of the Tucker decomposition. This way, we ensure that Gr and Dr are all-
r = I ∈ RLn+1×Ln+1 and
r P(n)
are column-wise orthogonal, i.e. P(n)T
, Q(m)
orthogonal and P(n)
r
r
r = I ∈ RKm+1×Km+1 .
r Q(m)
Q(m)T
By deﬁning a latent matrix T = [t1 , . . . , tR ] ∈ RI1×R , mode-n loading matrix P
(n)
=
R ] ∈ RIn+1×RLn+1 , mode-m loading matrix Q
R ] ∈
(m)
= [Q(m)
[P(n)
1 , . . . , P(n)
, . . . , Q(m)
1
RJn+1×RKm+1 and core tensor G = blockdiag(G1 , . . . , GR ) ∈ RR×RL2×···×RLN , D =
blockdiag(D1 , . . . , DR ) ∈ RR×RK2×···×RKM , the HOPLS model in (6) can be rewritten as
(N −1)
X = G ×1 T ×2 P
(1) × · · · ×N P
+ E,
(M −1)
Y = D ×1 T ×2 Q
(1) ×3 · · · ×M Q
where E and F are residuals. The core tensors G and D have a special block-diagonal structure (see
Fig. 1) whose elements indicate the level of interactions between the corresponding latent vectors
and loading matrices.
Note that HOPLS simpliﬁes into NPLS if we deﬁne ∀n : {Ln} = 1 and ∀m : {Km} = 1. On the
other hand, for ∀n : {Ln} = rankn (X) and ∀m : {Km } = rankm (Y)2 , HOPLS obtains the same
solution as the standard PLS performed on a mode-1 matricization of X and Y . This is obvious
(cid:17)T
(cid:16)
from a matricized form of (6), given by
X(1) ≈ trGr(1)
(cid:16)
(cid:17)T
⊗ · · · ⊗ P(1)
r

can approximate arbitrarily well the pT
r in (4) computed

P(N −1)
r

⊗ · · · ⊗ P(1)
r

P(N −1)
r

+ F,

(9)

,

(10)

where Gr(1)
from X(1) .

3.2 Objective function and algorithm

The optimization of subspace transformation yielding the common latent variables will be formu-
lated as a problem of determining a set of loading matrices P(n)
, Q(m)
, r = 1, 2, . . . , R that max-
2 rankn (X) = rank (cid:0)X(n)
(cid:1) .
r
r
imize an objective function. Since the latent vectors can be optimized sequentially with the same

4

Algorithm 1 The Higher-order Partial Least Squares (HOPLS) Algorithm
Input: X ∈ RI1×···×IN , Y ∈ RJ1×···×JM with I1 = J1
n=2 and {Km }M
Number of latent vectors is R and number of loading vectors are {Ln}N
m=2 .
}; {Gr }; {Dr }; Tr
Output: {P(n)
r }; {Q(m)
r
r = 1, . . . , R; n = 1, . . . , N − 1; m = 1, . . . , M − 1.
Initialization: E1 = X, F1 = Y .
for r = 1 to R do
if (cid:107)Er (cid:107) >  and (cid:107)Fr (cid:107) >  then
Cr ←< Er , Fr >{1,1} ;
(cid:20)(cid:16)
Rank-(L2 , . . . , LN , K2 , . . . , KM ) decomposition of Cr by HOOI [8] as
r , . . . , P(N −1)
r , . . . , Q(M −1)
Cr ≈ [[Hr ; P(1)
, Q(1)
]];
r
r
r ×3 · · · ×N P(N −1)T
Er ×2 P(1)T
tr ← the ﬁrst leading left singular vector by SVD
r
, . . . , P(N −1)T
Gr ← [[Er ; tT
r , P(1)T
]];
r
r
, . . . , Q(M −1)T
Dr ← [[Fr ; tT
r , Q(1)T
]];
r
r
Deﬂation:
r , . . . , P(N −1)
Er+1 ← Er − [[Gr ; tr , P(1)
]];
r
r , . . . , Q(M −1)
Fr+1 ← Fr − [[Dr ; tr , Q(1)
]];
r
else
Break;
end if
end for
r }; {Q(m)
Return all {P(n)
r

}; {Gr }; {Dr }; Tr .

(cid:21)
;

(cid:17)

(1)

s. t.

criteria based on deﬂation3 , we shall simplify the problem to that of the ﬁrst latent vector t1 and two
groups of loading matrices P(n)
and Q(m)
. To simplify the notation, r is removed in the following
1
1
equations. An objective function employed to determine the tensor bases, represented by P(n) and
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Y − [[D; t, Q(1) , . . . , Q(M −1) ]]
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)X − [[G; t, P(1) , . . . , P(N −1) ]]
Q(m) , can be deﬁned as
min
+
{P(n) , Q(m) }
{P(n)T P(n)} = ILn+1 ,
{Q(m)T Q(m) } = IKm+1 ,
(11)
and yields the common latent vector t that best approximates X and Y . The solution can be obtained
(cid:13)(cid:13)(cid:13)[[< X, Y >{1;1} ; P(1) , . . . , P(N −1) , Q(1) , . . . Q(M −1) ]]
(cid:13)(cid:13)(cid:13)2
by maximizing the norm of the core tensors G and D simultaneously. Since tT t = 1, we have
(cid:107)G ×1 D(cid:107)2 =
(12)
.
We now deﬁne a mode-1 cross-covariance tensor C = COV{1;1} (X, Y) ∈ RI2×···×IN ×J2×···×JM .
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)[[C; P(1) , . . . , P(N −1) , Q(1) , . . . Q(M −1) ]]
Using the property of (cid:107)G ×1 D(cid:107)2 ≤ (cid:107)G(cid:107)2 (cid:107)D(cid:107)2 and based on (11), (12), we have
max
{P(n) ,Q(m) }
P(n)T P(n) = ILn+1 and Q(n)T Q(n) = IKm+1 ,
indicating that
instead of decomposing X directly, we may opt
to ﬁnd a
rank-
(L2 , . . . , LN , K2 , . . . , KM ) tensor decomposition of C. According to (11), for a given set of
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)X − [[G; t, P(1) , . . . , P(N −1) ]]
loading matrices {P(n)}, the latent vector t must explain variance of X as much as possible, that is
t = arg min
t
The HOPLS algorithm is outlined in Algorithm 1.

(13)

s. t.

.

(14)

3As in the NPLS case, this deﬂation does not reduce the rank of the residuals.

5

3.3 Prediction

Predictions of the new observations are performed using the matricization form of data tensors X
(1)(cid:17) (cid:16)
(cid:17)+
(cid:16)
and Y . More speciﬁcally, for any new observation Xnew , we can predict the Ynew as
(cid:16)
(1)(cid:17)T
(N −1) ⊗ · · · ⊗ P
T
G
P
(1)
(M −1) ⊗ · · · ⊗ Q
(1) = ˆTnewD(1)
ˆYnew
Q
where (·)+ denotes the Moore-Penrose pseudoinverse operation.

ˆTnew = Xnew
(1)

,

(15)

Figure 2: Performance comparison between HOPLS, NPLS and PLS, for a varying number of latent
vectors under the conditions of noise free (A) and SNR=10dB (B).

4 Experimental results
as Q2 = 1 − (cid:80)I
i=1 (yi − ˆyi )2/ (cid:80)I
We performs two case studies, one on synthetic data which illustrates the beneﬁts of HOPLS, and the
other on real-life electrophysiological data. To quantify the predictability the index Q2 was deﬁned
i=1 (yi − ¯y)2 , where ˆyi denotes the prediction of yi using a model
created with the ith sample omitted.
4.1 Simulations on synthetic datasets

A simulation study on synthetic datasets was undertaken to evaluate the HOPLS regression method
in terms of its predictive ability and effectiveness under different conditions related to small number
of samples and noise levels. The HOPLS and NPLS were performed on tensor datasets whereas

Figure 3: The optimal performance after choosing an appropriate number of latent vectors. (A)
Noise free case. (B) For case with SNR=10dB.

6

PLS was performed on a mode-1 matricization of the corresponding datasets (i.e. X(1) and Y(1) ).
The tensor X was generated from a full-rank standard normal distribution and the tensor Y as a
linear combination of X. Noise was added to both independent and dependent datasets to evaluate
performance at different noise levels. To reduce random ﬂuctuations, the results were averaged over
50 simulation trials with datasets generated repeatedly according to the same criteria.
We considered a 3th-order tensor X and a 3th-order tensor Y , for the case where the sample size
was much smaller than the number of predictors, i.e., I1 << I2 × I3 . Fig. 2 illustrates the pre-
dictive performances on the validation datasets for a varying number of latent vectors. Observe that
when the number of latent vectors was equal to the number of samples, both PLS and NPLS had
the tendency to be unstable, while HOPLS had no such problems. With an increasing number of
latent vectors, HOPLS exhibited enhanced performance while the performance of NPLS and PLS
deteriorated due to the noise introduced by excess latent vectors (see Fig. 2B). Fig. 3 illustrates the
optimal prediction performances obtained by selecting an appropriate number of latent vectors. The
HOPLS outperformed the NPLS and PLS at different noise levels and the superiority of HOPLS was
more pronounced in the presence of noise, indicating its enhanced robustness to noise.

Figure 4: Stability of the performance of HOPLS, NPLS and PLS for a varying number of latent
vectors, under the conditions of (A) SNR=5dB and (B) SNR=0dB.

Observe that PLS was sensitive to the number of latent vectors, indicating that the selection of latent
vectors is a crucial issue for obtaining an optimal model. Finding the optimal number of latent
vectors for unseen test data remains a challenging problem, implying that the stability of prediction
performance for a varying number of latent vectors is essential for alleviating the sensitivity of the
model. Fig. 4 illustrates the stable predictive performance of HOPLS for a varying number of latent
vectors, this behavior was more pronounced for higher noise levels.

4.2 Decoding ECoG from EEG

In the last decade, considerable progress has been made in decoding the movement kinematics (e.g.
trajectories or velocity) from neuronal signals recorded both invasively, such as spiking activity
[20] and electrocorticogram (ECoG) [21, 22], and noninvasively- from scalp electroencephalogra-
phy (EEG) [23]. To extract more information from brain activities, neuroimaging data fusion has
also been investigated, whereby mutimodal brain activities were recorded continuously and syn-
chronously. In contrast to the task of decoding the behavioral data from brain activity, in this study,
our aim was to decode intracranial ECoG from scalp EEG. Assuming that both ECoG and EEG are
related to the same brain sources, we set out to extract the common latent components between EEG
and ECoG and examined whether ECoG can be decoded from the corresponding EEG by employing
our proposed HOPLS method.
ECoG (8×8 grid) and EEG (21 electrodes) were recorded simultaneously at a sample rate of 1024Hz
from a human subject during relaxed state. After the preprocessing by spatial ﬁlter of common aver-

7

age reference (CAR), ECoG and EEG signals were transformed into a time-frequency representation
and downsampled to 8 Hz by the continuous complex Morlet wavelet transformation with frequency
range of 2-150Hz and 2-40Hz, respectively. To ease the computation burden, we employed a 4 sec-
ond time window of EEG to predict the corresponding ECoG with the same window length. Thus,
our objective was to decode the ECoG dataset comprised in a 4th-order tensor Y (trial × channel
× frequency × time) from an EEG dataset contained in a 4th-order tensor X (trial × channel ×
frequency × time).
components that establish a bridge between EEG and ECoG, while the loading tensors (cid:101)Pr and
According to the HOPLS model, the common latent vectors in T can be regarded as brain source
(cid:101)Q
, r = 1, . . . , R can be regarded as a set of tensor bases, as shown in Fig. 5(A). These bases
r
are computed from the training dataset and explain the relationship of spatio-temporal frequency
patterns between EEG and ECoG. The decoding model was calibrated from 30 second datasets and
was applied to predict the subsequent 30 second datasets. The quality of prediction was evaluated
by the values of total correlation coefﬁcients between the predicted and actual time-frequency rep-
resentation of ECoG, denoted by rvec( ˆY),vec(Y) .
Fig. 5(B) illustrates the prediction performance by using a different number of latent vectors, ranging
from 1 to 8 and compared with the standard PLS performed on a mode-1 matricization of tensors
X and Y . The optimal number of latent vectors for HOPLS and PLS were 4 and 1, respectively.
Conforming with analysis, HOPLS was more stable for a varying number of latent vectors and
outperformed the standard PLS in terms of its predictive ability.

Figure 5: (A) The basis of the tensor subspace computed from the spatial, temporal, and spectral
representation of EEG and ECoG. (B) The correlation coefﬁcient r between predicted and actual
spatio-temporal-frequency representation of ECoG signals for a varying number of latent vectors.

5 Conclusion

We have introduced the Higher-order Partial Least Squares (HOPLS) framework for tensor subspace
regression, whereby data samples are represented in a tensor form, thus providing an natural general-
ization of the existing Partial Least Squares (PLS) and N -way PLS (NPLS) approaches. Compared
to the standard PLS, our proposed method has been shown to be more ﬂexible and robust, especially
for small sample size cases. Simulation results have demonstrated the superiority and effective-
ness of HOPLS over the existing algorithms for different noise levels. A challenging application of
decoding intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroen-
cephalography (EEG) (both from human brain) has been studied and the results have demonstrated
the large potential of HOPLS for multi-way correlated datasets.

Acknowledgments

The work was supported in part by the national natural science foundation of China under grant
number 90920014 and NSFC international cooperation program under grant number 61111140019.

8

In IEEE

SIAM Review,

References
[1] L. Wolf, H. Jhuang, and T. Hazan. Modeling appearances with low-rank SVM.
Conference on Computer Vision and Pattern Recognition, pages 1–6. IEEE, 2007.
[2] Hamed Pirsiavash, Deva Ramanan, and Charless Fowlkes. Bilinear classiﬁers for visual recog-
nition. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors,
Advances in Neural Information Processing Systems 22, pages 1482–1490. 2009.
[3] H. Lu, K.N. Plataniotis, and A.N. Venetsanopoulos. MPCA: Multilinear principal component
analysis of tensor objects. IEEE Transactions on Neural Networks, 19(1):18–39, 2008.
[4] S. Yan, D. Xu, Q. Yang, L. Zhang, X. Tang, and H.J. Zhang. Multilinear discriminant analysis
for face recognition. IEEE Transactions on Image Processing, 16(1):212–220, 2007.
[5] D. Tao, X. Li, X. Wu, and S.J. Maybank. General tensor discriminant analysis and Gabor
features for gait recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,
29(10):1700–1715, 2007.
[6] A.K. Smilde and H.A.L. Kiers. Multiway covariates regression models. Journal of Chemo-
metrics, 13(1):31–48, 1999.
[7] X. He, D. Cai, and P. Niyogi. Tensor subspace analysis. Advances in Neural Information
Processing Systems, 18:499, 2006.
[8] T.G. Kolda and B.W. Bader. Tensor decompositions and applications.
51(3):455–500, 2009.
[9] A. Cichocki, R. Zdunek, A. H. Phan, and S. I. Amari. Nonnegative Matrix and Tensor Factor-
izations. John Wiley & Sons, 2009.
[10] E. Acar, D.M. Dunlavy, T.G. Kolda, and M. Mørup. Scalable tensor factorizations for incom-
plete data. Chemometrics and Intelligent Laboratory Systems, 2010.
[11] R. Bro, R.A. Harshman, N.D. Sidiropoulos, and M.E. Lundy. Modeling multi-way data with
linearly dependent loadings. Journal of Chemometrics, 23(7-8):324–340, 2009.
[12] S. Wold, M. Sjostroma, and L. Erikssonb. PLS-regression: A basic tool of chemometrics.
Chemometrics and Intelligent Laboratory Systems, 58:109–130, 2001.
[13] H. Wold. Soft modeling by latent variables: The nonlinear iterative partial least squares ap-
proach. Perspectives in probability and statistics, papers in honour of MS Bartlett, pages
520–540, 1975.
[14] A. Krishnan, L.J. Williams, A.R. McIntosh, and H. Abdi. Partial least squares (PLS) methods
for neuroimaging: A tutorial and review. NeuroImage, 56(2):455 – 475, 2011.
[15] H. Abdi. Partial least squares regression and projection on latent structure regression (PLS
Regression). Wiley Interdisciplinary Reviews: Computational Statistics, 2(1):97–106, 2010.
[16] R. Rosipal and N. Kr ¨amer. Overview and recent advances in partial least squares. In Subspace,
Latent Structure and Feature Selection, volume 3940 of Lecture Notes in Computer Science,
pages 34–51. Springer, 2006.
[17] R. Bro. Multiway calibration. Multilinear PLS. Journal of Chemometrics, 10(1):47–61, 1996.
[18] L. De Lathauwer. Decompositions of a higher-order tensor in block terms - Part II: Deﬁnitions
and uniqueness. SIAM J. Matrix Anal. Appl, 30(3):1033–1066, 2008.
[19] L. De Lathauwer, B. De Moor, and J. Vandewalle. A multilinear singular value decomposition.
SIAM Journal on Matrix Analysis and Applications, 21(4):1253–1278, 2000.
[20] M. Velliste, S. Perel, M.C. Spalding, A.S. Whitford, and A.B. Schwartz. Cortical control of a
prosthetic arm for self-feeding. Nature, 453(7198):1098–1101, 2008.
[21] Z.C. Chao, Y. Nagasaka, and N. Fujii. Long-term asynchronous decoding of arm motion using
electrocorticographic signals in monkeys. Frontiers in Neuroengineering, 3(3), 2010.
[22] T. Pistohl, T. Ball, A. Schulze-Bonhage, A. Aertsen, and C. Mehring. Prediction of arm
movement trajectories from ECoG-recordings in humans. Journal of Neuroscience Methods,
167(1):105–114, 2008.
[23] T.J. Bradberry, R.J. Gentili, and J.L. Contreras-Vidal. Reconstructing three-dimensional hand
movements from noninvasive electroencephalographic signals. The Journal of Neuroscience,
30(9):3432, 2010.

9


Approximating Semideﬁnite Programs in Sublinear
Time

Dan Garber
Technion - Israel Institute of Technology
Haifa 32000 Israel
dangar@cs.technion.ac.il

Elad Hazan
Technion - Israel Institute of Technology
Haifa 32000 Israel
ehazan@ie.technion.ac.il

Abstract

In recent years semideﬁnite optimization has become a tool of major importance
in various optimization and machine learning problems. In many of these prob-
lems the amount of data in practice is so large that there is a constant need for
faster algorithms. In this work we present the ﬁrst sublinear time approximation
algorithm for semideﬁnite programs which we believe may be useful for such
problems in which the size of data may cause even linear time algorithms to have
prohibitive running times in practice. We present the algorithm and its analysis
alongside with some theoretical lower bounds and an improved algorithm for the
special problem of supervised learning of a distance metric.

1

Introduction

Semideﬁnite programming (SDP) has become a tool of great importance in optimization in the past
years. In the ﬁeld of combinatorial optimization for example, numerous approximation algorithms
have been discovered starting with Goemans and Williamson [1] and [2, 3, 4]. In the ﬁeld of machine
learning solving semideﬁnite programs is at the heart of many learning tasks such as learning a
distance metric [5], sparse PCA [6], multiple kernel learning [7], matrix completion [8], and more. It
is often the case in machine learning that the data is assumed no be noisy and thus when considering
the underlying optimization problem, one can settle for an approximated solution rather then an
exact one. Moreover it is also common in such problems that the amounts of data are so large that
fast approximation algorithms are preferable to exact generic solvers, such as interior-point methods,
which have impractical running times and memory demands and are not scalable.
In the problem of learning a distance metric [5] one is given a set of points in Rn and similarity
information in the form of pairs of points and a label indicating weather the two points are in the
same class or not. The goal is to learn a distance metric over Rn which respects this similarity
information. That is it assigns small distances to points in the same class and bigger distances to
points in different classes. Learning such a metric is important for other learning tasks which rely on
having a good metric over the input space, such as K-means, nearest-neighbours and kernel-based
algorithms.
In this work we present the ﬁrst approximation algorithm for general semideﬁnite programming
which runs in time that is sublinear in the size of the input. For the special case of learning a
pseudo-distance metric, we present an even faster sublinear time algorithm. Our algorithms are the
fastest possible in terms of the number of constraints and the dimensionality, although slower than
other methods in terms of the approximation guarantee.

1.1 Related Work

Semideﬁnite programming is a notoriously difﬁcult optimization formulation, and has attracted a
host of attempts at fast approximation methods. Klein and Lu [9] gave a fast approximate solver for

1

the MAX-CUT semideﬁnite relaxation of [1]. Various faster and more sophisticated approximate
solvers followed [10, 11, 12], which feature near-linear running time albeit polynomial dependence
on the approximation accuracy. For the special case of covering an packing SDP problems, [13]
and [14] respectively give approximation algorithms with a smaller dependency on the approxima-
tion parameter . Our algorithms are based on the recent work of [15] which described sublinear
algorithms for various machine learning optimization problems such has linear classiﬁcation and
minimum enclosing ball. We describe here how such methods, coupled with techniques, may be
used for semideﬁnite optimization.

2 Preliminaries
(cid:113)(cid:80)
In this paper we denote vectors in Rn by a lower case letter (e.g. v ) and matrices in Rn×n by
upper case letters (e.g. A). We denote by (cid:107)v(cid:107) the standard euclidean norm of the vector v and by
(cid:107)A(cid:107) the frobenius norm norm of the matrix A, that is (cid:107)A(cid:107) =
i,j A(i, j )2 . We denote by (cid:107)v(cid:107)1
The notation C ◦ X is just the dot product between matrices, that is C ◦ X = (cid:80)
the l1 -norm of v . The notation X (cid:23) 0 states that the matrix X is positive semi deﬁnite, i.e. it is
symmetric and all of its eigenvalues are non negative. The notation X (cid:23) B states that X − B (cid:23) 0.
We denote by ∆m the m-dimensional simplex, that is ∆m = {p| (cid:80)m
i,j C (i, j )X (i, j ).
i=1 pi = 1, ∀i : pi ≥ 0}.
We denote by 1n the all ones n-dimensional vector and by 0n×n the all zeros n × n matrix. We
denote by I the identity matrix when its size is obvious from context. Throughout the paper we will
use the complexity notation ˜O(·) which is the same as the notation O(·) with the difference that it
suppresses poly-logarithmic factors that depend on n, m, −1 .
We consider the following general SDP problem
Maximise C ◦ X
subject to Ai ◦ X ≥ 0
X (cid:23) 0

i = 1, ..., m

(1)

Where C, A1 , ..., Am ∈ Rn×n . For reasons that will be made clearer in the analysis, we will assume
that for all i ∈ [m], (cid:107)Ai (cid:107) ≤ 1
The optimization problem (1) can be reduced to a feasibility problem by a standard reduction of
performing a binary search over the value of the objective C ◦X and adding an appropriate constraint.
Thus we will only consider the feasibility problem of ﬁnding a solution that satisﬁes all constraints.
The feasibility problem can be rewritten using the following min-max formulation
Ai ◦ X

(2)

max
X(cid:23)0

min
i∈[m]

Clearly if the optimum value of (2) is non-negative, then a feasible solution exists and vice versa.
Denoting the optimum of (2) by σ , an  additive approximation algorithm to (2) is an algorithm that
produces a solution X such that X (cid:23) 0 and for all i ∈ [m], Ai ◦ X ≥ σ − .
For the simplicity of the presentation we will only consider constraints of the form A ◦ X ≥ 0 but
we mention in passing that SDPs with other linear constraints can be easily rewritten in the form of
(1).
We will be interested in a solution to (2) which lies in the bounded semideﬁnite cone K =
{X |X (cid:23) 0, Tr(X ) ≤ 1}. The demand on a solution to (2) to have bounded trace is due to the
observation that in case σ > 0, any solution needs to be bounded or else the products Ai ◦ X could
be made to be arbitrarily large.

Learning distance pseudo metrics
In the problem of learning a distance metric from examples,
we are given a set triplets S = {{xi , x(cid:48)
i , yi }}m
i ∈ Rn and yi ∈ {−1, 1}. A value
i=1 such that xi , x(cid:48)
i are in the same class and a value yi = −1 indicates that they
yi = 1 indicates that the vectors xi , x(cid:48)
are from different classes. Our goal is to learn a pseudo-metric over Rn which respects the example
of the form dA (x, x(cid:48) ) ≡ (cid:112)(x − x(cid:48) )(cid:62)A(x − x(cid:48) ). Its easily veriﬁed that if A (cid:23) 0 then dA is indeed a
set. A pseudo-metric is a function d : R × R → R, which satisﬁes three conditions: (i) d(x, x(cid:48) ) ≥ 0,
(ii) d(x, x(cid:48) ) = d(x(cid:48) , x) , and (iii) d(x1 , x2 ) + d(x2 , x3 ) ≥ d(x1 , x3 ). We consider pseudo-metrics
pseudo-metric. A reasonable demand from a ”good” pseudo metric is that it separates the examples

2

(assuming such a separation exists). That is we would like to have a matrix A (cid:23) 0 and a threshold
i , yi } ∈ S it will hold that,
value b ∈ R such that for all {xi , x(cid:48)
(dA (xi − x(cid:48)
i ))2 = (xi − x(cid:48)
i )(cid:62)A(xi − x(cid:48)
i ) ≤ b − σ/2
yi = 1
(dA (xi − x(cid:48)
i ))2 = (xi − x(cid:48)
i )(cid:62)A(xi − x(cid:48)
i ) ≥ b + σ/2
yi = −1
where σ is the margin of separation which we would like to maximize. Denoting by vi = (xi − x(cid:48)
(cid:0)b − v(cid:62)
(cid:1) ≥ σ
i )
for all i ∈ [m], (3) can be summarized into the following formalism:
yi
i Avi
(cid:0)1 − v(cid:62)
(cid:1)
Without loss of generality we can assume that b = 1 and derive the following optimization problem
i Avi

(3)

(4)

yi

max
A(cid:23)0

min
i∈[m]

(5)

3 Algorithm for General SDP
(cid:80)
Our algorithm for general SDPs is based on the generic framework for constrained optimization
problems that ﬁt a max-min formulation, such as (2), presented in [15]. Noticing that mini∈[m] Ai ◦
i∈[m] p(i)Ai ◦ X , we can rewrite (2) in the following way
X = minp∈∆m
p(i)A(cid:62)
x∈K min
max
i x
two online algorithms: one that wishes to maximize (cid:80)
p∈∆m
other that wishes to minimize (cid:80)
Building on [15], we use an iterative primal-dual algorithm that simulates a repeated game between
i∈[m] p(i)Ai ◦ X as a function of X and the
i∈[m] p(i)Ai ◦ X as a function of p. If both algorithms achieve
sublinear regret, then this framework is known to approximate max-min problems such as (5), in
solution a vector in the direction of the gradient (cid:80)
case a feasible solution exists [16].
The primal algorithm which controls X is a gradient ascent algorithm that given p adds to the current
i∈[m] p(i)Ai . Instead of adding the exact gradient
we actually only sample from it by adding Ai with probability p(i) (lines 5-6). The dual algorithm
which controls p is a variant of the well known multiplicative (or exponential) update rule for online
optimization over the simplex which updates the weight p(i) according to the product Ai ◦ X (line
11). Here we replace the exact computation of Ai ◦ X by employing the l2 -sampling technique
used in [15] in order to estimate this quantity by viewing only a single entry of the matrix Ai (line
9). An important property of this sampling procedure is that if (cid:107)Ai (cid:107) ≤ 1, then E[˜vt (i)2 ] ≤ 1.
Thus, we can estimate the product Ai ◦ X with constant variance, which is important for our anal-
ysis. A problem that arises with this estimation procedure is that it might yield unbounded values
which do not ﬁt well with the multiplicative weights analysis. Thus we use a clipping procedure
clip(z , V ) ≡ min{V , max{−V , Z }} to bound these estimations in a certain range (line 10). Clip-
ping the samples yields unbiased estimators of the products Ai ◦ X but the analysis shows that this
bias is not harmful.
The algorithm is required to generate a solution X ∈ K. This constraint is enforced by performing
a projection step onto the convex set K after each gradient improvement step of the primal online
algorithm. A projection of a matrix Y ∈ Rn×n onto K is given by Yp = arg minX ∈K (cid:107)Y − X (cid:107).
Unlike the algorithms in [15] that perform optimization over simple sets such as the euclidean unit
ball which is trivial to project onto, projecting onto the bounded semideﬁnite cone is more compli-
cated and usually requires to diagonalize the projected matrix (assuming it is symmetric). Instead,
we show that one can settle for an approximated projection which is faster to compute (line 4). Such
approximated projections could be computed by Hazan’s algorithm for ofﬂine optimization over the
bounded semideﬁnite cone, presented in [12]. Hazan’s algorithm gives the following guarantee
Lemma 3.1. Given a matrix Y ∈ Rn×n ,  > 0, let f (X ) = −(cid:107)Y − X (cid:107)2 and denote X ∗ =
(cid:17)
(cid:16) n2
arg maxX ∈K f (X ). Then Hazan’s algorithm produces a solution ˜X ∈ K of rank at most −1 such
that (cid:107)Y − ˜X (cid:107)2 − (cid:107)Y − X ∗ (cid:107)2 ≤  in O
time.
1.5
(cid:17)
(cid:16) m
We can now state the running time of our algorithm.
2 + n2
Lemma 3.2. Algorithm SublinearSDP has running time ˜O
5

.

3

2: Let T ← 602 −2 log m, Y1 ← 0n×n , w1 ← 1m , η ← (cid:113) log m
Algorithm 1 SublinearSDP
1: Input:  > 0, Ai ∈ Rn×n for i ∈ [m].
T , P ← /2.
3: for t = 1 to T do
pt ← wt(cid:107)wt (cid:107)1
, Xt ←ApproxProject(Yt , 2
P ).
4:
Choose it ∈ [m] by it ← i w.p. pt (i).
5:
Yt+1 ← Yt + 1√
6:
Ait
2T
Choose (jt , lt ) ∈ [n] × [n] by (jt , lt ) ← (j, l) w.p. Xt (j, l)2/(cid:107)Xt(cid:107)2 .
7:
for i ∈ [m] do
8:
˜vt ← Ai (jt , lt )(cid:107)Xt(cid:107)2/Xt (jt , lt )
9:
vt (i) ←clip(˜vt (i), 1/η)
10:
wt+1 (i) ← wt (i)(1 − ηvt (i) + η2 vt (i)2 )
11:
(cid:80)
end for
12:
13: end for
14: return ¯X = 1
T

t Xt

(cid:17)
(cid:16) m
We also have the following lower bound.
Theorem 3.3. Any algorithm which computes an -approximation with probability at least 2
3 to (2)
2 + n2
.
has running time Ω
2
We note that while the dependency of our algorithm on the number of constraints m is close to
optimal (up to poly-logarithmic factors), there is a gap of ˜O(−3 ) between the dependency of our
algorithm on the size of the constraint matrices n2 and the above lower bound. Here it is important to
note that our lower bound does not reﬂect the computational effort in computing a general solution
that is positive semideﬁnite which is in fact the computational bottleneck of our algorithm (due to
the use of the projection procedure).

4 Analysis

We begin with the presentation of the Multiplicative Weights algorithm used in our algorithm.
Deﬁnition 4.1. Consider a sequence of vectors q1 , ..., qT ∈ Rm . The Multiplicative Weights (MW)
algorithm is as follows. Let 0 < η ∈ R, w1 ← 1m , and for t ≥ 1,
pt ← wt/(cid:107)wt(cid:107)1 , wt+1 ← wt (i)(1 − ηqt (i) + η2 qt (i)2 )

} +

log m
η

+ η

The following lemma gives a bound on the regret of the MW algorithm, suitable for the case in
which the losses are random variables with bounded variance.
(cid:88)
(cid:88)
(cid:88)
Lemma 4.2. The MW algorithm satisﬁes
max{qt (i), − 1
t qt ≤ min
p(cid:62)
i∈[m]
η
t∈[t]
t∈[T ]
t∈[T ]
Lemma 4.3. For 1/4 ≥ η ≥ (cid:113) log m
The following lemma gives concentration bounds on our random variables from their expectations.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 8ηT
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
T , with probability at least 1 − O(1/m), it holds that
(cid:88)
Ait ◦ Xt − (cid:88)
(cid:80)
t∈[T ] [vt (i) − Ai ◦ Xt ] ≤ 4ηT (ii)
t∈[T ]
t∈[T ]
The following Lemma gives a regret bound on the lazy gradient ascent algorithm used in our algo-
rithm (line 6). For a proof see Lemma A.2 in [17].

(i) maxi∈[m]

p(cid:62)
t q2
t

p(cid:62)
t vt

4

(cid:13)(cid:13)(cid:13) 1√
(cid:13)(cid:13)(cid:13) Then
(cid:80)t
Lemma 4.4. Consider matrices A1 , ..., AT ∈ Rn×n such that for all i ∈ [m] (cid:107)Ai (cid:107) ≤ 1. Let
At ◦ X − (cid:88)
(cid:88)
X0 = 0n×n and for all t ≥ 1 let Xt+1 = arg minX ∈K
τ =1 Aτ − X
2T
√
At ◦ Xt ≤ 2
t∈[T ]
t∈[T ]

max
X ∈K

2T

2T

p(cid:62)
t v2
t

2T

√

(6)

(7)

2T + T P

√
Ait ◦ ( ˜Xt − Xt ) ≤ 2

We are now ready to state the main theorem and prove it.
Theorem 4.5 (Main Theorem). With probability 1/2, the SublinearSDP algorithm returns an -
additive approximation to (5).
Proof. At ﬁrst assume that the projection onto the set K in line 4 is an exact projection and not an
Ait ◦ X − (cid:88)
(cid:88)
approximation and denote by ˜Xt the exact projection of Yt . In this case, by lemma 4.4 we have
√
Ait ◦ ˜Xt ≤ 2
max
x∈K
t∈[T ]
t∈[T ]
By the law of cosines and lemma 3.1 we have for every t ∈ [T ]
(cid:107)Xt − ˜Xt(cid:107)2 ≤ (cid:107)Yt − Xt(cid:107)2 − (cid:107)Yt − ˜Xt(cid:107)2 ≤ 2
(cid:88)
Ait ◦ X − (cid:88)
Ait ◦ Xt − (cid:88)
P
Rewriting (6) we have
max
x∈K
t∈[T ]
t∈[T ]
t∈[T ]
Ait ◦ X − (cid:88)
(cid:88)
(cid:88)
Using the Cauchy-Schwarz inequality, (cid:107)Ait (cid:107) ≤ 1 and (7) we get
√
Ait ◦ Xt ≤ 2
(cid:107)Ait (cid:107)(cid:107) ˜Xt − Xt(cid:107) ≤ 2
2T +
max
x∈K
t∈[T ]
t∈[T ]
t∈[T ]
(cid:88)
Rearranging and plugging maxx∈K mini∈[m] Ai ◦ X = σ we get
√
Ait ◦ Xt ≥ T σ − 2
2T − T P
t∈[T ]
vt (i) we have (cid:88)
(cid:88)
(cid:88)
Turning to the MW part of the algorithm, by the MW Regret Lemma 4.2, and using the clipping of
t vt ≤ min
p(cid:62)
vt (i) + (log m)/η + η
i∈[i]
t∈[T ]
t∈[t]
t∈[T ]
(cid:88)
vt (i) ≤ (cid:88)
By Lemma 4.3, with high probability and for any i ∈ [n],
Ai ◦ Xt + 4ηT
t∈[T ]
t∈[T ]
(cid:88)
(cid:88)
(cid:88)
Thus with high probability it holds that
t vt ≤ min
Ai ◦ Xt + (log m)/η + η
p(cid:62)
p(cid:62)
t v2
t + 4ηT
i∈[i]
t∈[T ]
t∈[t]
t∈[T ]
(cid:88)
(cid:88)
Combining (8) and (9) we get
Ai ◦ Xt ≥ − (log m)/η − η
t − 4ηT + T σ
p(cid:62)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − T P
t v2
t vt − (cid:88)
(cid:88)
t∈[t]
t∈[T ]
√
p(cid:62)
t∈[T ]
t∈[T ]
(cid:88)
By a simple Markov inequality argument it holds that w.p. at least 3/4,
t ≤ 8T
p(cid:62)
t v2
t∈[T ]

Ait ◦ Xt

− 2

2T −

(8)

(9)

min
i∈[i]

5

(cid:88)
n ) ≥ 1
4 − O( 1
Combined with lemma 4.3, we have w.p. at least 3
2
√
Ai ◦ Xt ≥ −(log m)/η − 8ηT − 4ηT + T σ − 2
t∈[t]
√

min
i∈[i]

2T − 8ηT − T P

2T − T P
≥ T σ − log m
− 20ηT − 2
η
Dividing through by T and plugging in our choice for η and P , we have mini∈[m] Ai ◦ ¯X ≥ σ − 
w.p. at least 1/2.

5 Application to Learning Pseudo-Metrics
As in the problem of general SDP, we can also rewrite (4) by replacing the mini∈[m] objective with
(cid:0)1 − v(cid:62)
(cid:1)
minp∈∆m and arrive at the following formalism,
(10)
min
max
yi
i Avi
p∈∆m
A(cid:23)0
(cid:18) vi
(cid:26)(cid:18) A 0
(cid:19)
(cid:19)
(cid:27)
As we demanded a solution to general SDP to have bounded trace, here we demand that (cid:107)A(cid:107) ≤ 1.
and deﬁning the set of matrices P =
|A (cid:23) 0, (cid:107)A(cid:107) ≤ 1
Letting v (cid:48)
0 −1
, we
i =
1
can rewrite (10) in the following form.

−yi v (cid:48)
i ◦ A
i v (cid:48)(cid:62)
A∈P min
max
p∈∆m
In what comes next, we use the notation Ai = −yi v (cid:48)
i v (cid:48)
i .
Since projecting a matrix onto the set P is as easy as projecting a matrix onto the set
{A (cid:23) 0, (cid:107)A(cid:107) ≤ 1}, we assume for the simplicity of the presentation that the set on which we opti-
mize is indeed P = {A (cid:23) 0, (cid:107)A(cid:107) ≤ 1}.
We proceed with presenting a simpler algorithm for this problem than the one given for general SDP.
i ◦ A with respect to A is a symmetric rank one matrix and here we have the
The gradient of yi v (cid:48)
i v (cid:48)(cid:62)
following useful fact that was previously stated in [18].
Theorem 5.1. If A ∈ Rn×n is positive semi deﬁnite, v ∈ Rn and α ∈ R then the matrix B =
A + αvv(cid:62) has at most one negative eigenvalue.

(11)

The proof is due to the eigenvalue Interlacing Theorem (see [19] pp. 94-97 and [20] page 412).
Thus after performing a gradient step improvement of the form Yt+1 = Xt + ηyi vi v(cid:62)
i , projecting
Yt+1 onto to the feasible set P comes down to the removal of at most one eigenvalue in case we
subtracted a rank one matrix (yit = −1) or normalizing the l2 norm in case we added a rank
one matrix (yit = 1). Since in practice computing eigenvalues fast, using the Power or Lanczos
methods, can be done only up to a desired approximation, in fact the resulting projection Xt+1
might not be positive semideﬁnite. Nevertheless, we show by care-full analysis that we can still
settle for a single eigenvector computation in order to compute an approximated projection with the
price that Xt+1 (cid:23) −3 I . That is Xt+1 might be slightly outside of the positive semideﬁnite cone.
The beneﬁt is an algorithm with improved performance over the general SDP algorithm since far
less eigenvalue computations are required than in Hazan’s algorithm.
The projection to the set P is carried out in lines 7-11. In line 7 we check if Yt+1 has a negative
eigenvalue and if so, we compute the corresponding eigenvector in line 8 and remove it in line 9. In
line 11 we normalize the l2 norm of the solution. The procedure Sample(Ai , Xt ) will be detailed
later on when we discuss the running time.

The following Lemma is a variant of Zinkevich’s Online Gradient Ascent algorithm [21] suitable
for the use of approximated projections when Xt is not necessarily inside the set P .
Lemma 5.2. Consider a set of matrices A1 , ..., AT ∈ Rn×n such that (cid:107)Ai (cid:107) ≤ 1. Let X0 = 0n×n
and for all t ≥ 0 let

Yt+1 = Xt + ηAt ,

X ∈P (cid:107)Yt+1 − X (cid:107)
˜Xt+1 = arg min

6

2: Let T ← 602 −2 log m, X1 =← 0n×n , w1 ← 1m , η ← (cid:113) log m
Algorithm 2 SublinearPseudoMetric
i ∈ Rn×n for i ∈ [m].
1: Input:  > 0, Ai = yi vi v(cid:62)
T .
3: for t = 1 to T do
pt ← wt(cid:107)wt (cid:107)1
(cid:113) 2
.
4:
Choose it ∈ [m] by it ← i w.p. pt (i).
5:
Yt+1 ← Xt +
T yit vit v(cid:62)
6:
it
if yi < 0 and λmin (Yt+1 ) < 0 then
7:
u ← arg minz :(cid:107)z(cid:107)=1 z(cid:62)Yt+1 z
8:
Yt+1 = Yt+1 − λuu(cid:62)
9:
end if
10:
11: Xt+1 ←
Yt+1
max {1,(cid:107)Yt+1 (cid:107)}
for i ∈ [m] do
12:
vt (i) ← clip(Sample(Ai , Xt ), 1/η)
13:
wt+1 (i) ← wt (i)(1 − ηvt (i) + η2 vt (i)2 )
14:
(cid:80)
end for
15:
16: end for
17: return ¯X = 1
T

and let Xt+1 be such that

t Xt
(cid:13)(cid:13)(cid:13) ≤ d . Then, for a proper choice of η it holds that,
(cid:13)(cid:13)(cid:13) ˜Xt+1 − Xt+1
At ◦ X − (cid:88)
(cid:88)
√
At ◦ Xt ≤
3
dT 3/2
2T +
2
t∈[T ]
t∈[T ]

max
X ∈P

The following lemma states the connection between the precision used in eigenvalues approximation
in lines 7-8, and the quality of the approximated projection.
Lemma 5.3. Assume that on each iteration t of the algorithm, the eigenvalue computation in
4T 1.5 additive approximation of the smallest eigenvalue of Yt+1 and let ˜Xt =
line 7 is a δ = d
arg minX ∈P (cid:107)Yt − X (cid:107). It holds that

√

2T +

dT 3/2

3
2

max
X ∈P

√
Setting d = 2P
3
T

(cid:107) ˜Xt − Xt(cid:107) ≤ d
Theorem 5.4. Algorithm SublinearPseudoMetric computes an  additive approximation to (11) w.p.
1/2.
At ◦ X − (cid:88)
(cid:88)
Proof. Combining lemmas 5.2, 5.3 we have,
At ◦ Xt ≤
t∈[T ]
t∈[T ]
At ◦ X − (cid:88)
(cid:88)
where P is the same as in theorem 4.5 yields,
√
At ◦ Xt ≤
t∈[T ]
t∈[T ]
The rest of the proof follows the same lines as theorem 4.5.
matrices. That is Xt is of the form Xt = (cid:80)
We move on to discus the time complexity of the algorithm. It is easily observed from the algorithm
that for all t ∈ [T ], the matrix Xt can be represented as the sum of kt ≤ 2T symmetric rank-one
i , (cid:107)zi (cid:107) = 1 for all i. Thus instead of
i∈[kt ] αi zi z(cid:62)
computing Xt explicitly, we may represent it by the vectors zi and scalars αi . Denote by α the
vector of length kt in which the ith entry is just αi , for some iteration t ∈ [T ]. Since (cid:107)Xt(cid:107) ≤
1 it holds that (cid:107)α(cid:107) ≤ 1. The sampling procedure Sample(Ai , Xt ) in line 13, returns the value
Ai (j,l)(cid:107)α(cid:107)2
k(cid:107)α(cid:107)2 · (zk (j )zk (l))2 . That is we ﬁrst sample a vector zi according to
with probability α2
zk (j )zk (l)αk

arg max
X ∈P

2T + P T

7

E[˜vt (i)2 ] =

α and then we sample an entry (j, l) according to the chosen vector zi . It is easily observed that
(cid:18) α2
(cid:19)
˜vt (i) = Sample(Ai , Xt ) is an unbiased estimator of Ai ◦ Xt . It also holds that:
(cid:88)
k(cid:107)α(cid:107)2 (zk (j )zk (l))2 · Ai (j, l)2 (cid:107)α(cid:107)4
(zk (j )zk (l))2α2
j∈[n],l∈[n],k∈[kt ]
k
= kt(cid:107)α(cid:107)2(cid:107)Ai (cid:107)2 = ˜O(−2 )
Thus taking ˜vt (i) to be the average of ˜O(−2 ) i.i.d samples as described above yields an unbiased
estimator of Ai · Xt with variance at most 1 as required for the analysis of our algorithm.
Lemma 5.5. Algorithm SublinearPseudoMetric can be implemented to run in time ˜O (cid:0) m
(cid:1).
We can now state the running time of the algorithm.
4 + n
6.5
Proof. According the lemmas 5.3, 5.4, the required precision in eigenvalue approximation is
O(1)T 2 .

Using the Lanczos method for eigenvalue approximation and the sparse representation of Xt de-
scribed above, a single eigenvalue computation takes ˜O(n−4.5 ) time per iteration. Estimating the
products Ai ◦ Xt on each iteration takes by the discussion above ˜O(m−2 ). Overall the running
time on all iteration is as stated in the lemma.

6 Conclusions

We have presented the ﬁrst sublinear time algorithm for approximate semi-deﬁnite programming, a
widely used optimization framework in machine learning. The algorithm’s running time is optimal
up to poly-logarithmic factors and its dependence on ε - the approximation guarantee. The algorithm
is based on the primal-dual approach of [15], and incorporates methods from previous SDP solvers
[12].
For the problem of learning peudo-metrics, we have presented further improvements to the basic
method which entail an algorithm that performs O( log n
ε2 ) iterations, each encompassing at most one
approximate eigenvector computation.

Acknowledgements

This work was supported in part by the IST Programme of the European Community, under the
PASCAL2 Network of Excellence, IST-2007-216886. This publication only reﬂects the authors’
views.

References

[1] Michel. X. Goemans and David P. Williamson. Improved approximation algorithms for maxi-
mum cut and satisﬁability problems using semideﬁnite programming. In Journal of the ACM,
volume 42, pages 1115–1145, 1995.
[2] Sanjeev Arora, Satish Rao, and Umesh Vazirani. Expander ﬂows, geometric embeddings and
graph partitioning. In Proceedings of the thirty-sixth annual ACM symposium on Theory of
computing, STOC ’04, pages 222–231, 2004.
[3] Amit Agarwal, Moses Charikar, Konstantin Makarychev, and Yury Makarychev. O(sqrt(log
n)) approximation algorithms for min uncut, min 2cnf deletion, and directed cut problems. In
Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, STOC ’05,
pages 573–581, 2005.
[4] Sanjeev Arora, James R. Lee, and Assaf Naor. Euclidean distortion and the sparsest cut. In
Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, STOC ’05,
pages 553–562, 2005.
[5] Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. Distance metric learn-
ing, with application to clustering with side-information. In Advances in Neural Information
Processing Systems 15, pages 505–512, 2002.

8

[6] Alexandre d’Aspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A di-
rect formulation of sparse PCA using semideﬁnite programming. In SIAM Review, volume 49,
pages 41–48, 2004.
[7] Gert R. G. Lanckriet, Nello Cristianini, Laurent El Ghaoui, Peter Bartlett, and Michael I.
Jordan. Learning the kernel matrix with semi-deﬁnite programming. In Journal of Machine
Learning Research, pages 27–72, 2004.
[8] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
2004.
[9] Philip Klein and Hsueh-I Lu. Efﬁcient approximation algorithms for semideﬁnite programs
arising from max cut and coloring. In Proceedings of the twenty-eighth annual ACM sympo-
sium on Theory of computing, STOC ’96, pages 338–347, 1996.
[10] Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semide.nite
In Proceedings of the 46th
programming using the multiplicative weights update method.
Annual IEEE Symposium on Foundations of Computer Science, FOCS ’05, pages 339–348,
2005.
[11] Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semideﬁnite pro-
grams. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,
STOC ’07, pages 227–236, 2007.
[12] Elad Hazan. Sparse approximate solutions to semideﬁnite programs. In Proceedings of the 8th
Latin American conference on Theoretical informatics, LATIN’08, pages 306–316, 2008.
[13] Garud Iyengar, David J. Phillips, and Clifford Stein. Feasible and accurate algorithms for
covering semideﬁnite programs. In SWAT, pages 150–162, 2010.
[14] Garud Iyengar, David J. Phillips, and Clifford Stein. Approximating semideﬁnite packing
programs. In SIAM Journal on Optimization, volume 21, pages 231–268, 2011.
[15] Kenneth L. Clarkson, Elad Hazan, and David P. Woodruff. Sublinear optimization for ma-
chine learning. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of
Computer Science, FOCS ’10, pages 449–457, 2010.
[16] Elad Hazan.
Approximate convex optimization by online game playing.
abs/cs/0610119, 2006.
[17] Kenneth L. Clarkson, Elad Hazan, and David P. Woodruff. Sublinear optimization for machine
learning. CoRR, abs/1010.4408, 2010.
[18] Shai Shalev-shwartz, Yoram Singer, and Andrew Y. Ng. Online and batch learning of pseudo-
metrics. In ICML, pages 743–750, 2004.
[19] James Hardy Wilkinson. The algebric eigenvalue problem. Claderon Press, Oxford, 1965.
[20] Gene H. Golub and Charles F. Van Loan. Matrix computations. John Hopkins University
Press, 1989.
[21] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.
In ICML, pages 928–936, 2003.

CoRR,

9


CS 229 Project : Improving on Yelp Reviews Using NLP
and Bayesian Scoring

Patrick Bechon
pbechon@stanford.edu

Léo Grimaldi
leo.grimaldi@stanford.edu

Yacine Merouchi
merouchi@stanford.edu

1.
INTRODUCTION
Yelp allows its users to share reviews of local businesses.
For each business, the reviews and star ratings are used to
display some key quotes from reviews, and an average star
rating. Our goal in this pro ject was to improve Yelp’s user
experience, by ﬁnding a new way of summarizing the rat-
ings and reviews of each business. For our experiments, we
used the Yelp Academic Dataset 1 , which contains the data
of the 250 closest businesses for 30 universities in the US.
This dataset includes user proﬁles, business proﬁles, reviews,
and the votes that users have given to other users’ reviews.
We used Bayesian scoring to improve the global star rat-
ing of every business, and then two methods, TF-IDF and
ExpandRank (an algorithm derived from PageRank), to ex-
tract the keyphrases that best describe each business.

2. DATA PREPROCESSING
For Bayesian scoring, we only need the star ranking of every
review, so, from the whole dataset, we extract a sparse 2D
matrix whose element i, j is the ranking of user i for busi-
ness j . One of the main characteristics of the Yelp dataset
is that this matrix is very sparse, because many users only
submit a low number of reviews.
For key-phrase extraction, we need to process the text of
each review. To do so, we split the text into sentences and
tokens. We remove all non-alphabetic words, we set all the
characters to lowercase, we remove stopwords and we stem
each word to regroup words with the same root. For all
these operations, we use the nltk library in Python2 .
Among the diﬀerent choices we had to make, the choice of
the stemmer is probably the most important. We consid-
ered two algorithms, the Porter stemmer and the Lancaster
stemmer. To choose between those two, we decided to test
them on a supervised learning problem. The spam classiﬁer
of Problem Set 2 was our source of inspiration. By consid-
ering each review as a text and trying to predict whether

1http://www.yelp.com/academic dataset
2http://www.nltk.org

it is a positive review (with a star ranking between 3 and
5) or a negative review (with a star ranking between 1 and
2), we were able to compare the performances of supervised
learning algorithms using the two diﬀerent stemming meth-
ods. Figure 1 shows the evolution of the training error and
the testing error as the training set size increases. We chose
to use the Porter stemmer as it outperforms the Lancaster
stemmer on every test.

Figure 1: Comparaison of training error and testing
error on a 1000-reviews test set, for two stemmers,
using SVM and Naive Bayes

3. BAYESIAN RATING
Introduction
On Yelp, reviewers can give a business an integer score be-
tween 1 and 5 (”star rating”). Then businesses can be ranked
according to their average star rating among reviewers. Yet
the variance within these average ratings is not very high
and it is sometimes hard for users to pick their destination.
Thus we want to come up with a better way of assessing the
intrinsic value of a business b, denoted µb , that would have
a higher variance among businesses. While reviewers try
to estimate this ”true” value with star ratings, they can be
inconsistent: while some tend to give all businesses a good
score, others may be very harsh with their rating. Thus we
denote νr the bias of reviewer r and use a model similar to
the one studied in Problem Set 4. The star rating given by
reviewer r to business b is denoted x(br) and assumed to be

1000200030004000500060007000800090001000002468101214161820Error comparisonTraining set sizeError (%)  NB train error with porterNB test error with porterSVM train error with porterSVM test error with porterNB train error with lancasterNB test error with lancasterSVM train error with lancasterSVM test error with lancasterand pre-determine our model. In practice, we experimented
on α and β , looking at the shape of the likelihood curve to
determine small values that would make sure the algorithm
converges over a ﬁnite number of iterations.

EM Algorithm
E-step: For each business-reviewer pair (b, r), we used the
observed value x(br) and the current set of parameters
{µb , νr , σ2
r , σ} to compute
b , τ 2
(cid:20)µb (τ 2
(cid:21)
b (x(br) − νr )
(cid:20)σ2
(cid:21)
r + σ2 ) + σ2
r (x(br) − µb )
νr (σ2
b + σ2 ) + τ 2
−σ2
b (τ 2
r + σ2 )
b τ 2
−1
r
−σ2
r (σ2
τ 2
b τ 2
b + σ2 )
r

Σ(br)
b + τ 2
y,z |x = (σ2
r + σ2 )

µ(br)
b + τ 2
y,z |x = (σ2
r + σ2 )

−1

.

,

Thus,

generated by a random process as follows.
y (br) ∼ N (µb , σ2
b )
z (br) ∼ N (νr , τ 2
r )
x(br) | y (br) , z (br) ∼ N (y (br) + z (br) , σ2 )

The variables x(br) (related to the business’ true value and
service consistency) and z (br) (related to the reviewer’s bias
and rating consistency) are assumed to be independent while
the variables x(br) , y (br) , z (br) ) for diﬀerent business-reviewer
pairs are also jointly independent. We are to train this model
on our dataset using an Expectation - Maximization (EM)
algorithm.

Regularization
Now the key diﬀerence with Problem Set 4 is that not all
reviewers reviewed every business. We are even far from this
ideal situation since each reviewer wrote less than 2 reviews
on average. Thus for many reviewers, we don’t have enough
data to estimate the many parameters properly. While over-
ﬁtting is a concern, the lack of data leads to a more crucial
issue as the EM algorithm may fail to converge.
Indeed,
because of the very few data points available for some re-
viewers, some of the variance parameters σb , τ 2
r , σ tend to
converge towards 0. These lead to degenerate Gaussian dis-
tributions (singular covariance matrix) hence singularities
in the likelihood function. As the likelihood is no longer
”smooth”, the EM algorithm is not guaranteed to converge.
In order to regularize the likelihood, we use MAP estima-
tion with Inverse Gamma distributions as conjugate priors
on the variances of our model’s Gaussian distributions.
b ∼ I nvGamma(α, β )
σ2
r ∼ I nvGamma(α, β )
ν 2
σ2 ∼ I nvGamma(α, β )

∗
r =

∗
b =

µ

Qbr (y (br) , z (br) ) = p(y (br) , z (br) | x(br) ) ∼ N (µ(br)
y,z |x , Σ(br)
y,z |x ).
viewer r and N = (cid:80)
b R(b) = (cid:80)
M-step: Denoting R(b) the total number of reviews of busi-
ness b, B (r) the total number of businesses reviewed by re-
r B (r) the total number
of reviews, we used the quantities derived in the E-step to
compute the updated values of the parameters:
(cid:88)
(cid:88)
1
1
(µ(br)
(µ(br)
y,z |x )1 , ν
y,z |x )2
(cid:16)
(cid:16)
(cid:17)
(cid:17)2
B (r)
R(b)
2β + (cid:80)
r
b
µb − (µ(br)
Σ(br)
y,z |x )1
y,z |x
r
(cid:16)
(cid:17)2
(cid:16)
(cid:17)
2β + (cid:80)
R(b) + 2(α + 1)
νr − (µ(br)
Σ(br)
y,z |x )2
y,z |x
b
B (r) + 2(α + 1)
2β +
(cid:32)
(cid:20)1
(cid:88)
x(br) −
 (cid:20)1
(cid:21) .
1
b,r
Σ(br)
y,z |x
1

1
(cid:88)
(cid:20)1
(cid:21)T
N + 2(α + 1)
1
b,r

∗
b )2 =
(σ

∗
r )2 =
(τ

(cid:33)2

(cid:21)T

µ(br)
y,z |x

∗

(σ

)2 =

+

+

11

22

+

Figure 2: CDF of Inverse Gamma Distribution for
diﬀerent values of (α, β )

As we can observe in the following expressions, the param-
eters α and β can be simply interpreted as controlling the
number of ”virtual” prior samples that we add to the data in
order to regularize the learning process. Thus α and β must
be high enough to prevent the variance parameters from con-
verging towards 0, but low enough not to take over real data

Computation of the Likelihood
Using the distributions Qbr ∼ N (µ(br)
y,z |x , Σ(br)
y,z |x ) derived in
the E-step, the lower bound
(cid:123)(cid:122)
(cid:125)
(cid:124)
φ(µ1 . . . µB , ν1 . . . νR , σ1 . . . σB , τ1 . . . τR , σ
ξ

)

x(br) −

+

(cid:19)
Σ(br)
y,z |x
(cid:19)
11

on the log-likelihood of the parameters, abbreviated as ξ , is
(cid:32)(cid:32)
(cid:33)2
(cid:20)1
(cid:20)1
(cid:20)1
(cid:21)T
(cid:21)T
(cid:88)
given (up to an additive constant) by
φ(ξ ) = − 1
(cid:18)(cid:16)
µ(br)
(cid:16)
(cid:17)2
(cid:17)
− (cid:88)
y,z |x
1
1
1
2σ2
b,r
µb − (µ(br)
(cid:18)(cid:16)
Σ(br)
(cid:17)
(cid:16)
(cid:17)2
y,z |x )1
− (cid:88)
y,z |x
b,r
νr − (µ(br)
1
Σ(br)
(cid:88)
(cid:88)
y,z |x )2
y,z |x
2τ 2
r
b,r
N log σ2 − 1
b − 1
− 1
(cid:16)|Σ(br)
y,z |x |(cid:17)
(cid:88)
R(b) log σ2
2
2
2
r
b
1
(cid:88)
b − (cid:88)
log
+
2
b,r
− (α + 1)
β
log σ2
− (cid:88)
σ2
b
b
b
− (α + 1) log σ2 − β
σ2
b

(cid:88)
r

B (r) log τ 2
r

− (α + 1)

log ν 2
r

1
2σ2
b

β
ν 2
r

+

+

22

This quantity was maximized in the M-step to update the
parameters to ξ ∗ . But we can also use this expression right
after the E-step to compute the likelihood of the current set
of parameters. Indeed, we know that the distributions Qbr
computed in the E-step ensure that this lower-bound is tight
for the current set of parameters ξ .

Results
Since our dataset is organized around ma jor US universi-
ties, we ﬁrst trained our model on the businesses around
Stanford. After determining the regularization parameters
(α, β ) = (5, 1), we experimented with three diﬀerent random
initializations for the EM algorithm and picked the model
that gave the highest likelihood. First of all, people are

(cid:21)(cid:33)

Figure 4: Reviewer’s Bias (Stanford)

Figure 5: Bayesian vs Average Star Ratings (Stan-
ford)

Figure 6: Top 10 Comparison (Stanford)

looks like people are very positive here in California!

Figure 3: Likelihood Convergence (Stanford)

overly positive! Indeed the distribution of reviewers’ biases
range from -5 to + 5 and is packed around +1. Then we
can observe that among Stanford businesses, the inferred
”intrinsic” mean values µb have higher variance (2.21) than
average star ratings (0.95). To the reader’s appreciation, we
also extracted the top 10 businesses according to each score
(Bayesian Rating vs Average Star Ranking). Finally, we
learnt a similar model for each of the other 29 universities
from our dataset and compared people’s mood (reviewer’s
average bias) and the quality of surrounding businesses (av-
erage business intrinsic value) around each university.
It

Figure 7: Ranking of US Universities

4. KEY-PHRASE EXTRACTION
4.1 Models
We consider the problem of extracting sentences that de-
scribe businesses. For this task, we use two popular meth-
ods available for extracting keyphrases, TF-IDF and Ex-
pandRank. Both these methods involve assigning scores to
words based on their importance, and then selecting sen-
tences with the largest cumulative word scores. This task
can be formalized as a unsupervised learning problem. We
discuss these two algorithms and describe our evaluation of
the results.

The idea of TF-IDF is to measure the importance of words
relative to the documents in a corpus. We concatenate all
the reviews from a given business and consider this as a
document, and take only businesses from the same category
for the corpus. We then calculate the TF-IDF score of each
token relative to every business, and, for each business, we
select the sentence with the highest cumulative word score.
We eliminate sentences that are longer than seven words,
to avoid a bias towards longer sentences when comparing
cumulative scores and because we only want to display short
sentences for the user.

The second approach that we consider is the ExpandRank
algorithm, which was introduced in [1] using the example of
extracting keyphrases from news articles. The ExpandRank
algorithm works by selecting a small number of neighbor-
ing documents, assigning aﬃnities to words by counting the
number of times they occur within a ﬁxed distance. This
is similar to the PageRank algorithm, if we interpret every
word as a web page and co-occurrence as the presence of a
hyperlink. Instead of using a similarity distance as in [1], we
use the business metadata to select neighboring documents,
by restricting the businesses to a given category.

4.2 Evaluation of results

Figure 8: Evaluation of key-phrase extraction algo-
rithms using correlation with review votes

Figure 9: Evaluation of key-phrase extraction algo-
rithms using cumulative scores of reviews

Our ﬁrst idea is that the ExpandRank or TF-IDF score of a
sentence might be correlated with the ’funny’, ’useful’ and
’cool’ votes associated with the corresponding review. We
compare the user’s votes on reviews that we selected as con-
taining the best keyphrase with the top review from the
relevant business, and notice that there is no pattern. We
then calculate the total cumulative scores of reviews using
ExpandRank and TF-IDF, and ﬁnd that these scores have
a weak positive correlation with the user-submitted votes.

We also compare the results given by both algorithms on the
same businesses. Given the same data, the two algorithms
usually yield diﬀerent results. We present an example of the
results for ﬁve Mexican restaurants.

TF-IDF
1- We were at Lustre Pearl last night, ready for some tacos.
2- or me, the burrito ultimo is baja fresh’s only saving grace.
3- Chef Jose Garces doesn’t disappoint with Distrito.
4- took forever for our drinks to arrive.
5- Casa Moreno has the best house Margarita ever!
ExpandRank
1- might go back later at night for the food
2- Good fake mexican food.
3- Overall, good food, good drinks, and great ambiance/crowd.
4- Do not expect a good Mexican food experience if you go.
5- Do not order this dish unless you like spicy food.

In this example, as in other examples we have considered,
ExpandRank seems to yield more relevant keyphrases. The
word ’food’ also appears in every sentence with ExpandRank,
and we can verify that this word received a high individual
score. This is due to the fact that this word is one of the
most commonly used words in reviews of restaurants. The
last example also shows a limitation of this algorithm: even
the sentence is relevant, one needs more context to under-
stand it (here the reviewer was talking about the chili verde).

Having no gold standard keyphrases at our disposal, we con-
sider two methods to try to evaluate the results.

5. CONCLUSION

1234500.10.20.30.40.50.60.70.80.91BusinessCorrelationCorrelation with Useful / Funny / Cool scores  PageRankTF−IDF123450255075100BusinessRelative errorDifference between predicted review’s U/F/C score and best review’s U/F/C score  PageRankTF−IDFThis pro ject proposes two ways of improving the user expe-
rience on Yelp: using Bayesian rating to adjust star ratings
for the eﬀects of user bias, and using keyphrase extraction
techniques to describe businesses concisely. We found that
the inferred intrinsic mean values can achieve a variance
among businesses more than twice the variance of the aver-
age star ratings, and that we could deal with data sparsity
providing a clever choice of prior on the model parameters.
We compared TF-IDF and ExpandRank in the task of se-
lecting keyphrases from reviews and found that they tend
to provide diﬀerent results, and only found a weak positive
correlation with users’ votes on relevant reviews.

Bayesian rating proved to be an interesting approach and fu-
ture work could look at live implementation: how to adapt
the algorithm for online update of the inferred grades at
minimal cost, that is every time a person writes a new re-
view. The results of ExpandRank are encouraging and this
method should be tested on a large number of businesses
with hand-selected keyphrases.

6. REFERENCES
[1] X. Wan and J. Xiao. Single document keyphrase
extraction using neighborhood knowledge. In
Proceedings of the 23rd national conference on Artiﬁcial
intel ligence - Volume 2, AAAI’08, pages 855–860.
AAAI Press, 2008.


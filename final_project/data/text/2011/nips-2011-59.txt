Efﬁcient Learning of Generalized Linear and Single
Index Models with Isotonic Regression

Sham M. Kakade
Microsoft Research and Wharton, U Penn
skakade@microsoft.com

Adam Tauman Kalai
Microsoft Research
adum@microsoft.com

Varun Kanade
SEAS, Harvard University
vkanade@fas.harvard.edu

Ohad Shamir
Microsoft Research
ohadsh@microsoft.com

Abstract

Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide
powerful generalizations of linear regression, where the target variable is assumed
to be a (possibly unknown) 1-dimensional function of a linear predictor. In gen-
eral, these problems entail non-convex estimation procedures, and, in practice,
iterative local search heuristics are often used. Kalai and Sastry (2009) provided
the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and
GLMs, under the assumption that the data is in fact generated under a GLM and
under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron
algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁt-
ting a one-dimensional non-decreasing function). However, to obtain provable
performance, the method requires a fresh sample every iteration. In this paper, we
provide algorithms for learning GLMs and SIMs, which are both computationally
and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt
a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algo-
rithm for this step, improving upon the previous O(n2 ) algorithm. We provide a
brief empirical study, demonstrating the feasibility of our algorithms in practice.

1

Introduction

The oft used linear regression paradigm models a dependent variable Y as a linear function of a
vector-valued independent variable X . Namely, for some vector w , we assume that E[Y |X ] = w ·X .
Generalized linear models (GLMs) provide a ﬂexible extension of linear regression, by assuming
that the dependent variable Y is of the form, E[Y |X ] = u(w · X ); u is referred to as the inverse link
function or transfer function (see [1] for a review). Generalized linear models include commonly
used regression techniques such as logistic regression, where u(z ) = 1/(1 + e−z ) is the logistic
function. The class of perceptrons also falls in this category, where u is a simple piecewise linear
function of the form /¯, with the slope of the middle piece being the inverse of the margin.
In the case of linear regression, the least-squares method is an highly efﬁcient procedure for pa-
rameter estimation. Unfortunately, in the case of GLMs, even in the setting when u is known, the
problem of ﬁtting a model that minimizes squared error is typically not convex. We are not aware
of any classical estimation procedure for GLMs which is both computationally and statistically efﬁ-
cient, and with provable guarantees. The standard procedure is iteratively reweighted least squares,
based on Newton-Raphson (see [1]).
The case when both u and w are unknown (sometimes referred to as Single Index Models (SIMs)),
involves the more challenging (and practically relevant) question of jointly estimating u and w ,

1

where u may come from a large non-parametric family such as all monotonic functions. There are
two questions here: 1) What statistical rate is achievable for simultaneous estimation of u and w? 2)
Is there a computationally efﬁcient algorithm for this joint estimation? With regards to the former,
under mild Lipschitz-continuity restrictions on u, it is possible to characterize the effectiveness of an
(appropriately constrained) joint empirical risk minimization procedure. This suggests that, from a
purely statistical viewpoint, it may be worthwhile to attempt jointly optimizing u and w on empirical
data.
However, the issue of computationally efﬁciently estimating both u and w (and still achieving a
good statistical rate) is more delicate, and is the focus of this work. We note that this is not a trivial
problem: in general, the joint estimation problem is highly non-convex, and despite a signiﬁcant
body of literature on the problem, existing methods are usually based on heuristics, which are not
guaranteed to converge to a global optimum (see for instance [2, 3, 4, 5, 6]).
The Isotron algorithm of Kalai and Sastry [7] provides the ﬁrst provably efﬁcient method for learning
GLMs and SIMs, under the common assumption that u is monotonic and Lipschitz, and assuming
that the data corresponds to the model.1 The sample and computational complexity of this algo-
rithm is polynomial, and the sample complexity does not explicitly depend on the dimension. The
algorithm is a variant of the “gradient-like” perceptron algorithm, where apart from the perceptron-
like updates, an isotonic regression procedure is performed on the linear predictions using the Pool
Adjacent Violators (PAV) algorithm, on every iteration.
While the Isotron algorithm is appealing due to its ease of implementation (it has no parameters
other than the number of iterations to run) and theoretical guarantees (it works for any u, w), there
is one principal drawback. It is a batch algorithm, but the analysis given requires the algorithm to
be run on fresh samples each batch. In fact, as we show in experiments, this is not just an artifact of
the analysis – if the algorithm loops over the same data in each update step, it really does overﬁt in
very high dimensions (such as when the number of dimensions exceeds the number of examples).
Our Contributions: We show that the overﬁtting problem in Isotron stems from the fact that al-
though it uses a slope (Lipschitz) condition as an assumption in the analysis, it does not constrain
the output hypothesis to be of this form. To address this issue, we introduce the S L I SOTRON algo-
rithm (pronounced slice-o-tron, combining slope and Isotron). The algorithm replaces the isotonic
regression step of the Isotron by ﬁnding the best non-decreasing function with a bounded Lipschitz
parameter - this constraint plays here a similar role as the margin in classiﬁcation algorithms. We
also note S L I SOTRON (like Isotron) has a signiﬁcant advantage over standard regression techniques,
since it does not require knowing the transfer function. Our two main contributions are:
1. We show that the new algorithm, like Isotron, has theoretical guarantees, and signiﬁcant new
analysis is required for this step.
2. We provide an efﬁcient O(n log(n)) time algorithm for ﬁnding the best non-decreasing function
with a bounded Lipschitz parameter, improving on the previous O(n2 ) algorithm [10]. This makes
S L I SOTRON practical even on large datasets.
We begin with a simple perceptron-like algorithm for ﬁtting GLMs, with a known transfer function
u which is monotone and Lipschitz. Somewhat surprisingly, prior to this work (and Isotron [7])
a computationally efﬁcient procedure that guarantees to learn GLMs was not known. Section 4
contains the more challenging S L I SOTRON algorithm and also the efﬁcient O(n log(n)) algorithm
for Lipschitz isotonic regression. We conclude with a brief empirical analysis.

2 Setting
We assume the data (x, y) are sampled i.i.d. from a distribution supported on Bd × [0, 1], where
Bd = {x ∈ Rd : (cid:107)x(cid:107) ≤ 1} is the unit ball in d-dimensional Euclidean space. Our algorithms and
1 In the more challenging agnostic setting, the data is not required to be distributed according to a true u and
w , but it is required to ﬁnd the best u, w which minimize the empirical squared error. Similar to observations
of Kalai et al. [8], it is straightforward to show that this problem is likely to be computationally intractable in
the agnostic setting. In particular, it is at least as hard as the problem of “learning parity with noise,” whose
hardness has been used as the basis for designing multiple cryptographic systems. Shalev-Shwartz et al. [9]
present a kernel-based algorithm for learning certain types of GLMs and SIMs in the agnostic setting. However,
their worst-case guarantees are exponential in the norm of w (or equivalently the Lipschitz parameter).

2

Algorithm 1 GLM - TRON
Input: data (cid:104)(xi , yi )(cid:105)m
i=1 ∈ Rd × [0, 1], u : R → [0, 1], held-out data (cid:104)(xm+j , ym+j )(cid:105)s
j=1
w1 := 0;
for t = 1, 2, . . . do
m(cid:88)
ht (x) := u(wt · x);
(yi − u(wt · xi ))xi ;
1
wt+1 := wt +
Output: arg minht (cid:80)s
m
i=1
end for
j=1 (ht (xm+j ) − ym+j )2
analysis also apply to the case where Bd is the unit ball in some high (or inﬁnite)-dimensional kernel
feature space. We assume there is a ﬁxed vector w , such that (cid:107)w(cid:107) ≤ W , and a non-decreasing
1-Lipschitz function u : R → [0, 1], such that E[y |x] = u(w · x) for all x. The restriction that u is
1-Lipschitz is without loss of generality, since the norm of w is arbitrary (an equivalent restriction
is that (cid:107)w(cid:107) = 1 and that u is W -Lipschitz for an arbitrary W ).
(cid:2)(h(x) − y)2 (cid:3)
Our focus is on approximating the regression function well, as measured by the squared loss. For a
real valued function h : Bd → [0, 1], deﬁne
(cid:2)(h(x) − u(w · x))2 (cid:3)
err(h) = E(x,y)
ε(h) = err(h) − err(E [y |x]) = E(x,y)
err(h) measures the error of h, and ε(h) measures the excess error of h compared to the Bayes-
optimal predictor x (cid:55)→ u(w · x). Our goal is to ﬁnd h such that ε(h) (equivalently, err(h)) is as
In addition, we deﬁne the empirical counterparts (cid:99)err(h),
small as possible.
m(cid:88)
m(cid:88)
(x1 , y1 ), . . . , (xm , ym ), to be
(cid:99)err(h) =
(h(xi ) − yi )2 ;
(h(xi ) − u(w · xi ))2 .
1
1
m
m
i=1
i=1
Note that ˆε is the standard ﬁxed design error (as this error conditions on the observed x’s).
Our algorithms work by iteratively constructing hypotheses ht of the form ht (x) = ut (wt · x), where
ut is a non-decreasing, 1-Lipschitz function, and wt is a linear predictor. The algorithmic analysis
provides conditions under which ˆε(ht ) is small, and using statistical arguments, one can guarantee
that ε(ht ) would be small as well.

ˆε(h), based on a sample

ˆε(h) =

3 The GLM -TRON algorithm

We begin with the simpler case, where the transfer function u is assumed to be known (e.g. a sig-
moid), and the problem is estimating w properly. We present a simple, parameter-free, perceptron-
like algorithm, GLM - TRON (Alg. 1), which efﬁciently ﬁnds a close-to-optimal predictor. We note
that the algorithm works for arbitrary non-decreasing, Lipschitz functions u, and thus covers most
generalized linear models. We refer the reader to the pseudo-code in Algorithm 1 for some of the
notation used in this section.
To analyze the performance of the algorithm, we show that if we run the algorithm for sufﬁciently
many iterations, one of the predictors ht obtained must be nearly-optimal, compared to the Bayes-
optimal predictor.
Theorem 1. Suppose (x1 , y1 ), . . . , (xm , ym ) are drawn independently from a distribution supported
least 1 − δ : there exists some iteration t < O(W (cid:112)m/ log(1/δ)) of GLM - TRON such that the
on Bd × [0, 1], such that E[y |x] = u(w · x), where (cid:107)w(cid:107) ≤ W , and u : R → [0, 1] is a known non-
decreasing 1-Lipschitz function. Then for any δ ∈ (0, 1), the following holds with probability at
(cid:33)
(cid:32)(cid:114)
hypothesis ht (x) = u(wt · x) satisﬁes
max{ ˆε(ht ), ε(ht )} ≤ O
W 2 log(m/δ)
m

.

3

Algorithm 2 S L I SOTRON
Input: data (cid:104)(xi , yi )(cid:105)m
i=1 ∈ Rd × [0, 1], held-out data (cid:104)(xm+j , ym+j )(cid:105)s
j=1
w1 := 0;
for t = 1, 2, . . . do
m(cid:88)
ut := LIR ((wt · x1 , y1 ), . . . , (wt · xm , ym )) // Fit 1-d function along wt
(yi − ut (wt · xi ))xi
1
wt+1 := wt +
Output: arg minht (cid:80)s
m
i=1
end for
j=1 (ht (xm+j ) − ym+j )2
up to a constant, we can easily ﬁnd an appropriate ht by picking the one that has least (cid:99)err(ht ) on a
In particular, the theorem implies that some ht has small enough ε(ht ). Since ε(ht ) equals err(ht )
held-out set.
tance (cid:13)(cid:13)wt+1 − w(cid:13)(cid:13)2 is substantially smaller than (cid:107)wt − w(cid:107)2 . Since the squared distance is bounded
below by 0, and (cid:13)(cid:13)w0 − w(cid:13)(cid:13)2 ≤ W 2 , there is an iteration (arrived at within reasonable time) such that
The main idea of the proof is showing that at each iteration, if ˆε(ht ) is not small, then the squared dis-
the hypothesis ht at that iteration is highly accurate. Although the algorithm minimizes empirical
squared error, we can bound the true error using a uniform convergence argument. The complete
proofs are provided in the full version of the paper ([11] Appendix A).

4 The SL I SOTRON algorithm

In this section, we present S L I SOTRON (Alg. 2), which is applicable to the harder setting where
the transfer function u is unknown, except for it being non-decreasing and 1-Lipschitz. S L I SOTRON
does have one parameter, the Lipschitz constant; however, in theory we show that this can simply be
set to 1. The main difference between S L I SOTRON and GLM - TRON is that now the transfer function
must also be learned, and the algorithm keeps track of a transfer function ut which changes from
iteration to iteration. The algorithm is inspired by the Isotron algorithm [7], with the main difference
being that at each iteration, instead of applying the PAV procedure to ﬁt an arbitrary monotonic
function along the direction wt , we use a different procedure, (Lipschitz Isotonic Regression) LIR,
to ﬁt a Lipschitz monotonic function, ut , along wt . This key difference allows for an analysis that
does not require a fresh sample each iteration. We also provide an efﬁcient O(m log(m)) time
algorithm for LIR (see Section 4.1), making S L I SOTRON an extremely efﬁcient algorithm.
We now turn to the formal theorem about our algorithm. The formal guarantees parallel those of
the GLM - TRON algorithm. However, the rates achieved are somewhat worse, due to the additional
difﬁculty of simultaneously estimating both u and w .
Theorem 2. Suppose (x1 , y1 ), . . . , (xm , ym ) are drawn independently from a distribution supported
on Bd × [0, 1], such that E[y |x] = u(w · x), where (cid:107)w(cid:107) ≤ W , and u : R → [0, 1] is an unknown
non-decreasing 1-Lipschitz function. Then the following two bounds hold:
(cid:17)1/3(cid:19)
(cid:18)(cid:16)
1. (Dimension-dependent) With probability at least 1 − δ , there exists some iteration t <
(cid:19)1/3(cid:33)
(cid:32)(cid:18) dW 2 log(W m/δ)
of S L I SOTRON such that
W m
O
d log(W m/δ)
max{ ˆε(ht ), ε(ht )} ≤ O
m
(cid:18)(cid:16) W m
(cid:17)1/4(cid:19)
2. (Dimension-independent) With probability at least 1 − δ , there exists some iteration t <
(cid:32)(cid:18) W 2 log(m/δ)
(cid:19)1/4(cid:33)
of S L I SOTRON such that
O
log(m/δ)
m

max{ ˆε(ht ), ε(ht )} ≤ O

.

4

As in the case of Thm. 1, one can easily ﬁnd ht which satisﬁes the theorem’s conditions, by running
minimizes (cid:99)err(ht ) on a held-out set. The algorithm minimizes empirical error and generalization
the S L I SOTRON algorithm for sufﬁciently many iterations, and choosing the hypothesis ht which
bounds are obtained using a uniform convergence argument. The proofs are somewhat involved and
appear in the full paper ([11] Appendix B).

4.1 Lipschitz isotonic regression

The S L I SOTRON algorithm (Alg. 2) performs Lipschitz Isotonic Regression (LIR) at each iteration.
The goal is to ﬁnd the best ﬁt (least squared error) non-decreasing 1-Lipschitz function that ﬁts the
data in one dimension. Let (z1 , y1 ), . . . (zm , ym ) be such that zi ∈ R, yi ∈ [0, 1] and z1 ≤ z2 ≤
· · · ≤ zm . The Lipschitz Isotonic Regression (LIR) problem is deﬁned as the following quadratic
m(cid:88)
program:
i=1

Minimize w.r.t ˆyi : 1
2

(yi − ˆyi )2

(1)

(2)
(3)

subject to:
1 ≤ i ≤ m − 1 (Monotonicity)
ˆyi ≤ ˆyi+1
1 ≤ i ≤ m − 1 (Lipschitz)
ˆyi+1 − ˆyi ≤ (zi+1 − zi )
Once the values ˆyi are obtained at the data points, the actual function can be constructed by interpo-
lating linearly between the data points. Prior to this work, the best known algorithm for this problem
wass due to Yeganova and Wilbur [10] and required O(m2 ) time for m points. In this work, we
present an algorithm that performs the task in O(m log(m)) time. The actual algorithm is fairly
complex and relies on designing a clever data structure. We provide a high-level view here; the
details are provided in the full version ([11] Appendix D).
Algorithm Sketch: We deﬁne functions Gi (·), where Gi (s) is the minimum squared loss that can
be attained if ˆyi is ﬁxed to be s, and ˆyi+1 , . . . ˆym are then chosen to be the best ﬁt 1-Lipschitz
non-decreasing function to the points (zi , yi ), . . . , (zm , ym ). Formally, for i = 1, . . . , m, deﬁne the
m(cid:88)
functions,
( ˆyj − yj )2
j=i+1

Gi (s) = min
ˆyi+1 ,..., ˆym

(s − yi )2 +

(4)

1
2

1
2

subject to the constraints (where s = ˆyi ),
i ≤ j ≤ m − 1 (Monotonic)
ˆyj ≤ ˆyj+1
i ≤ j ≤ m − 1 (Lipschitz)
ˆyj+1 − ˆyj ≤ zj+1 − zj
Furthermore, deﬁne: s∗
i = mins Gi (s). The functions Gi are piecewise quadratic, differentiable
everywhere and strictly convex, a fact we prove in full paper [11]. Thus, Gi is minimized at s∗
i . Note that Gm (s) = (1/2)(s − ym )2 and hence
and it is strictly increasing on both sides of s∗
i
is piecewise quadratic, differentiable everywhere and strictly convex. Let δi = zi+1 − zi . The
(cid:40) Gi (s + δi−1 )
remaining Gi obey the following recursive relation.
Gi (s∗
i )
Gi (s)

If s ≤ s∗
i − δi−1
i − δi−1 < s ≤ s∗
If s∗
If s∗
i
i < s
As intuition for the above relation, note that Gi−1 (s) is obtained ﬁxing ˆyi−1 = s and then by
choosing ˆyi as close to s∗
i (since Gi is strictly increasing on both sides of s∗
i ) as possible without
violating either the monotonicity or Lipschitz constraints.
The above argument can be immediately translated into an algorithm, if the values s∗
i are known.
1 minimizes G1 (s), which is the same as the objective of (1), start with ˆy1 = s∗
Since s∗
1 , and then
successively chose values for ˆyi to be as close to s∗
i as possible without violating the Lipschitz
or monotonicity constraints. This will produce an assignment for ˆyi which achieves loss equal to
G1 (s∗
1 ) and hence is optimal.

(s − yi−1 )2 +

Gi−1 (s) =

(5)

1
2

5

(a)
(b)
i to G(cid:48)
i . (b) Update step to transform representation of G(cid:48)
Figure 1: (a) Finding the zero of G(cid:48)
i−1

(6)

i−1 (s) = (s − yi−1 ) +
G(cid:48)

The harder part of the algorithm is ﬁnding the values s∗
i . Notice that G(cid:48)
i are all piecewise linear,
(cid:40) G(cid:48)
m (s) = s − ym ):
continuous and strictly increasing, and obey a similar recursive relation (G(cid:48)
If s ≤ s∗
i − δi−1
i (s + δi−1 )
i − δi−1 < s ≤ s∗
If s∗
0
G(cid:48)
If s∗
i
i (s)
i < s
m = s − ym , and s∗
The algorithm then ﬁnds s∗
i . Starting from m, G(cid:48)
i by ﬁnding zeros of G(cid:48)
m = ym .
We design a special data structure, called notable red-black trees, for representing piecewise linear,
m (s) = s − ym .
continuous, strictly increasing functions. We initialize such a tree T to represent G(cid:48)
Assuming that at some time it represents G(cid:48)
i , we need to support two operations:
1. Find the zero of G(cid:48)
i to get s∗
i . Such an operation can be done efﬁciently O(log(m)) time using a
tree-like structure (Fig. 1 (a)).
2. Update T to represent G(cid:48)
i−1 . This operation is more complicated, but using the relation (6), we
do the following: Split the interval containing s(cid:48)
i . Move the left half of the piecewise linear function
G(cid:48)
i by δi−1 (Fig. 1(b)), adding the constant zero function in between. Finally, we add the linear
function s − yi−1 to every interval, to get G(cid:48)
i−1 , which is again piecewise linear, continuous and
strictly increasing.
To perform the operations in step (2) above, we cannot na¨ıvely apply the transformations,
shift-by(δi−1 ) and add(s − yi−1 ) to every node in the tree, as it may take O(m) operations.
Instead, we simply leave a note (hence the name notable red-black trees) that such a transformation
should be applied before the function is evaluated at that node or at any of its descendants. To pre-
vent a large number of such notes accumulating at any given node we show that these notes satisfy
certain commutative and additive relations, thus requiring us to keep track of no more than 2 notes
at any given node. This lazy evaluation of notes allows us to perform all of the above operations in
O(log(m)) time. The details of the construction are provided in the full paper ([11] Appendix D).

5 Experiments

In this section, we present an empirical study of the S L I SOTRON and GLM - TRON algorithms. We
perform two evaluations using synthetic data. The ﬁrst one compares S L I SOTRON and Isotron [7]
and illustrates the importance of imposing a Lipschitz constraint. The second one demonstrates the
advantage of using S L I SOTRON over standard regression techniques, in the sense that S L I SOTRON
can learn any monotonic Lipschitz function.
We also report results of an evaluation of S L I SOTRON, GLM - TRON and several competing ap-
proaches on 5 UCI[12] datasets.
All errors are reported in terms of average root mean squared error (RMSE) using 10 fold cross
validation along with the standard deviation.

5.1 Synthetic Experiments

Although, the theoretical guarantees for Isotron are under the assumption that we get a fresh sample
each round, one may still attempt to run Isotron on the same sample each iteration and evaluate the

6

bS L I SOTRON
0.289 ± 0.014

Isotron
0.334 ± 0.026

∆
0.045 ± 0.018

S L I SOTRON
0.058 ± 0.003

Logistic
0.073 ± 0.006

∆
0.015 ± 0.004

(a) Synthetic Experiment 1

(b) Synthetic Experiment 2

Figure 2: (a) The ﬁgure shows the transfer functions as predicted by S L I SOTRON and Isotron. The
table shows the average RMSE using 10 fold cross validation. The ∆ column shows the average
difference between the RMSE values of the two algorithms across the folds. (b) The ﬁgure shows the
transfer function as predicted by S L I SOTRON. Table shows the average RMSE using 10 fold cross
validation for S L I SOTRON and Logistic Regression. The ∆ column shows the average difference
between the RMSE values of the two algorithms across folds.

empirical performance. Then, the main difference between S L I SOTRON and Isotron is that while
S L I SOTRON ﬁts the best Lipschitz monotonic function using LIR each iteration, Isotron merely ﬁnds
the best monotonic ﬁt using PAV. This difference is analogous to ﬁnding a large margin classiﬁer
vs. just a consistent one. We believe this difference will be particularly relevant when the data is
sparse and lies in a high dimensional space.
Our ﬁrst synthetic dataset is the following: The dataset is of size m = 1500 in d = 500 dimensions.
The ﬁrst co-ordinate of each point is chosen uniformly at random from {−1, 0, 1}. The remaining
co-ordinates are all 0, except that for each data point one of the remaining co-ordinates is randomly
set to 1. The true direction is w = (1, 0, . . . , 0) and the transfer function is u(z ) = (1 + z )/2. Both
S L I SOTRON and Isotron put weight on the ﬁrst co-ordinate (the true direction). However, Isotron
overﬁts the data using the remaining (irrelevant) co-ordinates, which S L I SOTRON is prevented from
doing because of the Lipschitz constraint. Figure 2(a) shows the transfer functions as predicted by
the two algorithms, and the table below the plot shows the average RMSE using 10 fold cross valida-
tion. The ∆ column shows the average difference between the RMSE values of the two algorithms
across the folds.
A principle advantage of S L I SOTRON over standard regression techniques is that it is not necessary
to know the transfer function in advance. The second synthetic experiment is designed as a sanity
check to verify this claim. The dataset is of size m = 1000 in d = 4 dimensions. We chose a random
direction as the “true” w and used a piecewise linear function as the “true” u. We then added random
noise (σ = 0.1) to the y values. We compared S L I SOTRON to Logistic Regression on this dataset.
S L I SOTRON correctly recovers the true function (up to some scaling). Fig. 2(b) shows the actual
transfer function as predicted by S L I SOTRON, which is essentially the function we used. The table
below the ﬁgure shows the performance comparison between S L I SOTRON and logistic regression.

5.2 Real World Datasets

We now turn to describe the results of experiments performed on the following 5 UCI datasets:
communities, concrete, housing, parkinsons, and wine-quality. We compared
the performance of S L I SOTRON (Sl-Iso) and GLM - TRON with logistic transfer function (GLM-t)
against Isotron (Iso), as well as standard logistic regression (Log-R), linear regression (Lin-R) and
a simple heuristic algorithm (SIM) for single index models, along the lines of standard iterative
maximum-likelihood procedures for these types of problems (e.g., [13]). The SIM algorithm works
by iteratively ﬁxing the direction w and ﬁnding the best transfer function u, and then ﬁxing u and

7

−1−0.8−0.6−0.4−0.200.20.40.60.8100.10.20.30.40.50.60.70.80.91  SlisotronIsotron−0.6−0.4−0.200.20.40.60.80.20.30.40.50.60.70.80.91  Slisotronoptimizing w via gradient descent. For each of the algorithms we performed 10-fold cross validation,
using 1 fold each time as the test set, and we report averaged results across the folds.
Table 1 shows average RMSE values of all the algorithms across 10 folds. The ﬁrst column shows
the mean Y value (with standard deviation) of the dataset for comparison. Table 2 shows the average
difference between RMSE values of S L I SOTRON and the other algorithms across the folds. Negative
values indicate that the algorithm performed better than S L I SOTRON. The results suggest that the
performance of S L I SOTRON (and even Isotron) is comparable to other regression techniques and in
many cases also slightly better. The performance of GLM - TRON is similar to standard implementa-
tions of logistic regression on these datasets. This suggests that these algorithms should work well
in practice, while providing non-trivial theoretical guarantees.
It is also illustrative to see how the transfer functions found by S L I SOTRON and Isotron compare.
In Figure 3, we plot the transfer functions for concrete and communities. We see that the ﬁts
found by S L I SOTRON tend to be smoother because of the Lipschitz constraint. We also observe that
concrete is the only dataset where S L I SOTRON performs noticeably better than logistic regres-
sion, and the transfer function is indeed somewhat far from the logistic function.

Table 1: Average RMSE values using 10 fold cross validation. The ¯Y column shows the mean Y
value and standard deviation.

dataset
communities
concrete
housing
parkinsons
winequality

¯Y
0.24 ± 0.23
35.8 ± 16.7
22.5 ± 9.2
29 ± 10.7
5.9 ± 0.9

Sl-Iso
0.13 ± 0.01
9.9 ± 0.9
4.65 ± 1.00
10.1 ± 0.2
0.78 ± 0.04

GLM-t
0.14 ± 0.01
10.5 ± 1.0
4.85 ± 0.95
10.3 ± 0.2
0.79 ± 0.04

Iso
0.14 ± 0.01
9.9 ± 0.8
4.68 ± 0.98
10.1 ± 0.2
0.78 ± 0.04

Lin-R
0.14 ± 0.01
10.4 ± 1.1
4.81 ± 0.99
10.2 ± 0.2
0.75 ± 0.04

Log-R
0.14 ± 0.01
10.4 ± 1.0
4.70 ± 0.98
10.2 ± 0.2
0.75 ± 0.04

SIM
0.14 ± 0.01
9.9 ± 0.9
4.63 ± 0.78
10.3 ± 0.2
0.78 ± 0.03

Table 2: Performance comparison of S L I SOTRON with the other algorithms. The values reported
are the average difference between RMSE values of the algorithm and S L I SOTRON across the folds.
Negative values indicate better performance than S L I SOTRON.
Lin-R
Iso
GLM-t
dataset
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
communities
0.56 ± 0.35
0.04 ± 0.17
0.52 ± 0.35
concrete
0.20 ± 0.48
0.03 ± 0.55
0.16 ± 0.49
housing
0.11 ± 0.07
0.01 ± 0.03
0.19 ± 0.09
parkinsons
0.01 ± 0.01
0.00 ± 0.00
-0.03 ± 0.02
winequality

Log-R
0.00 ± 0.00
0.55 ± 0.32
0.05 ± 0.43
0.09 ± 0.07
-0.03 ± 0.02

SIM
0.00 ± 0.00
-0.03 ± 0.26
-0.02 ± 0.53
0.21 ± 0.20
0.01 ± 0.01

(a) concrete

(b) communities

Figure 3: The transfer function u as predicted by S L I SOTRON (blue) and Isotron (red) for the
concrete and communities datasets. The domain of both functions was normalized to [−1, 1].

8

−1−0.8−0.6−0.4−0.200.20.40.60.8100.10.20.30.40.50.60.70.80.91  SlisotronIsotron−1−0.8−0.6−0.4−0.200.20.40.60.8100.10.20.30.40.50.60.70.80.91  SlisotronIsotronReferences
[1] P. McCullagh and J. A. Nelder. Generalized Linear Models (2nd ed.). Chapman and Hall, 1989.
[2] P. Hall W. H ¨ardle and H. Ichimura. Optimal smoothing in single-index models. Annals of Statistics,
21(1):157–178, 1993.
[3] J. Horowitz and W. H ¨ardle. Direct semiparametric estimation of single-index models with discrete co-
variates, 1994.
[4] A. Juditsky M. Hristache and V. Spokoiny. Direct estimation of the index coefﬁcients in a single-index
model. Technical Report 3433, INRIA, May 1998.
[5] P. Naik and C. Tsai. Isotonic single-index model for high-dimensional database marketing. Computational
Statistics and Data Analysis, 47:775–790, 2004.
[6] P. Ravikumar, M. Wainwright, and B. Yu. Single index convex experts: Efﬁcient estimation via adapted
bregman losses. Snowbird Workshop, 2008.
[7] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT ’09,
2009.
[8] A. T. Kalai, A. R. Klivans, Y. Mansour, and R. A. Servedio. Agnostically learning halfspaces. In Pro-
ceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science, FOCS ’05, pages
11–20, Washington, DC, USA, 2005. IEEE Computer Society.
[9] S. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the zero-one
loss. In COLT, 2010.
[10] L. Yeganova and W. J. Wilbur. Isotonic regression under lipschitz constraint. Journal of Optimization
Theory and Applications, 141(2):429–443, 2009.
[11] S. M. Kakade, A. T. Kalai, V. Kanade, and O. Shamir. Efﬁcient learning of generalized linear and single
index models with isotonic regression. arxiv.org/abs/1104.2018.
[12] UCI. University of california, irvine: http://archive.ics.uci.edu/ml/.
[13] S. Cosslett. Distribution-free maximum-likelihood estimator of the binary choice model. Econometrica,
51(3), May 1983.

9


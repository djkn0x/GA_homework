A Brain-Machine Interface Operating with a
Real-Time Spiking Neural Network Control
Algorithm

Julie Dethier∗
Department of Bioengineering
Stanford University, CA 94305
jdethier@stanford.edu

Paul Nuyujukian
Department of Bioengineering
School of Medicine
Stanford University, CA 94305
paul@npl.stanford.edu

Chris Eliasmith
Centre for Theoretical Neuroscience
University of Waterloo, Canada
celiasmith@uwaterloo.ca

Terry Stewart
Centre for Theoretical Neuroscience
University of Waterloo, Canada
tcstewar@uwaterloo.ca

Shauki A. Elassaad
Department of Bioengineering
Stanford University, CA 94305
shauki@stanford.edu

Krishna V. Shenoy
Department of Electrical Engineering
Department of Bioengineering
Department of Neurobiology
Stanford University, CA 94305
shenoy@stanford.edu

Kwabena Boahen
Department of Bioengineering
Stanford University, CA 94305
boahen@stanford.edu

Abstract

Motor prostheses aim to restore function to disabled patients. Despite compelling
proof of concept systems, barriers to clinical translation remain. One challenge
is to develop a low-power, fully-implantable system that dissipates only minimal
power so as not to damage tissue. To this end, we implemented a Kalman-ﬁlter
based decoder via a spiking neural network (SNN) and tested it in brain-machine
interface (BMI) experiments with a rhesus monkey. The Kalman ﬁlter was trained
to predict the arm’s velocity and mapped on to the SNN using the Neural Engineer-
ing Framework (NEF). A 2,000-neuron embedded Matlab SNN implementation
runs in real-time and its closed-loop performance is quite comparable to that of the
standard Kalman ﬁlter. The success of this closed-loop decoder holds promise for
hardware SNN implementations of statistical signal processing algorithms on neu-
romorphic chips, which may offer power savings necessary to overcome a major
obstacle to the successful clinical translation of neural motor prostheses.

∗Present: Research Fellow F.R.S.-FNRS, Systmod Unit, University of Liege, Belgium.

1

1 Cortically-controlled motor prostheses: the challenge

Motor prostheses aim to restore function for severely disabled patients by translating neural signals
from the brain into useful control signals for prosthetic limbs or computer cursors. Several proof
of concept demonstrations have shown encouraging results, but barriers to clinical translation still
remain. One example is the development of a fully-implantable system that meets power dissipation
constraints, but is still powerful enough to perform complex operations. A recently reported closed-
loop cortically-controlled motor prosthesis is capable of producing quick, accurate, and robust com-
puter cursor movements by decoding neural signals (threshold-crossings) from a 96-electrode array
in rhesus macaque premotor/motor cortex [1]-[4]. This, and previous designs (e.g., [5]), employ
versions of the Kalman ﬁlter, ubiquitous in statistical signal processing. Such a ﬁlter and its variants
are the state-of-the-art decoder for brain-machine interfaces (BMIs) in humans [5] and monkeys [2].
While these recent advances are encouraging, clinical translation of such BMIs requires fully-
implanted systems, which in turn impose severe power dissipation constraints. Even though it is an
open, actively-debated question as to how much of the neural prosthetic system must be implanted,
we note that there are no reports to date demonstrating a fully implantable 100-channel wireless
transmission system, motivating performing decoding within the implanted chip. This computation
is constrained by a stringent power budget: A 6 × 6mm2 implant must dissipate less than 10mW to
avoid heating the brain by more than 1◦C [6], which is believed to be important for long term cell
health. With this power budget, current approaches can not scale to higher electrode densities or to
substantially more computer-intensive decode/control algorithms.
The feasibility of mapping a Kalman-ﬁlter based decoder algorithm [1]-[4] on to a spiking neural
network (SNN) has been explored off-line (open-loop). In these off-line tests, the SNN’s perfor-
mance virtually matched that of the standard implementation [7]. These simulations provide conﬁ-
dence that this algorithm—and others similar to it—could be implemented using an ultra-low-power
approach potentially capable of meeting the severe power constraints set by clinical translation. This
neuromorphic approach uses very-large-scale integrated systems containing microelectronic analog
circuits to morph neural systems into silicon chips [8, 9]. These neuromorphic circuits may yield
tremendous power savings—50nW per silicon neuron [10]—over digital circuits because they use
physical operations to perform mathematical computations (analog approach). When implemented
on a chip designed using the neuromorphic approach, a 2,000-neuron SNN network can consume as
little as 100µW.
Demonstrating this approach’s feasibility in a closed-loop system running in real-time is a key,
non-incremental step in the development of a fully implantable decoding chip, and is necessary
before proceeding with fabricating and implanting the chip. As noise, delay, and over-ﬁtting play
a more important role in the closed-loop setting, it is not obvious that the SNN’s stellar open-loop
performance will hold up. In addition, performance criteria are different in the closed-loop and open-
loop settings (e.g., time per target vs. root mean squared error). Therefore, a SNN of a different
size may be required to meet the desired speciﬁcations. Here we present results and assess the
performance and viability of the SNN Kalman-ﬁlter based decoder in real-time, closed-loop tests,
with the monkey performing a center-out-and-back target acquisition task. To achieve closed-loop
operation, we developed an embedded Matlab implementation that ran a 2,000-neuron version of
the SNN in real-time on a PC. We achieved almost a 50-fold speed-up by performing part of the
computation in a lower-dimensional space deﬁned by the formal method we used to map the Kalman
ﬁlter on to the SNN. This shortcut allowed us to run a larger SNN in real-time than would otherwise
be possible.

2 Spiking neural network mapping of control theory algorithms

As reported in [11], a formal methodology, called the Neural Engineering Framework (NEF), has
been developed to map control-theory algorithms onto a computational fabric consisting of a highly
heterogeneous population of spiking neurons simply by programming the strengths of their connec-
tions. These artiﬁcial neurons are characterized by a nonlinear multi-dimensional-vector-to-spike-
rate function—ai (x(t )) for the ith neuron—with parameters (preferred direction, maximum ﬁring
rate, and spiking-threshold) drawn randomly from a wide distribution (standard deviation ≈ mean).

2

(cid:10) ˜φ x
i · x(cid:11) + J bias
Representation
x → ai (x) → ˆx = ∑i ai (x)φ x
i
ai (x) = G(αi
)
i

Transformation
y = Ax → b j (A ˆx)
A ˆx = ∑i ai (x)Aφ x
i

Dynamics
˙x = Ax → x = h ∗ A(cid:48) x
A(cid:48) = τ A + I

Figure 1: NEF’s three principles. Representation. 1D tuning curves of a population of 50 leaky
integrate-and-ﬁre neurons. The neurons’ tuning curves map control variables (x) to spike rates
(ai (x)); this nonlinear transformation is inverted by linear weighted decoding. G() is the neurons’
nonlinear current-to-spike-rate function. Transformation. SNN with populations bk (t ) and a j (t )
representing y(t ) and x(t ). Feedforward and recurrent weights are determined by B(cid:48) and A(cid:48) , as
described next. Dynamics. The system’s dynamics is captured in a neurally plausible fashion by
replacing integration with the synapses’ spike response, h(t ), and replacing the matrices with A(cid:48) =
τ A + I and B(cid:48) = τ B to compensate.

The neural engineering approach to conﬁguring SNNs to perform arbitrary computations is under-
lined by three principles (Figure 1) [11]-[14]:
Representation is deﬁned by nonlinear encoding of x(t ) as a spike rate, ai (x(t ))—represented by
the neuron tuning curve—combined with optimal weighted linear decoding of ai (x(t )) to recover
an estimate of x(t ), ˆx(t ) = ∑i ai (x(t ))φ x
i , where φ x
i are the decoding weights.
Transformation is performed by using alternate decoding weights in the decoding operation to
map transformations of x(t ) directly into transformations of ai (x(t )). For example, y(t ) = Ax(t )
is represented by the spike rates b j (A ˆx(t )), where unit j’s input is computed directly from unit i’s
output using A ˆx(t ) = ∑i ai (x(t ))Aφ x
i , an alternative linear weighting.
Dynamics brings the ﬁrst two principles together and adds the time dimension to the circuit. This
principle aims at reuniting the control-theory and neural levels by modifying the matrices to render
the system neurally plausible, thereby permitting the synapses’ spike response, h(t ), (i.e., impulse
response) to capture the system’s dynamics. For example, for h(t ) = τ −1 e−t /τ , ˙x = Ax(t ) is realized
by replacing A with A(cid:48) = τ A + I. This so-called neurally plausible matrix yields an equivalent
dynamical system: x(t ) = h(t ) ∗ A(cid:48) x(t ), where convolution replaces integration.
The nonlinear encoding process—from a multi-dimensional stimulus, x(t ), to a one-dimensional
soma current, Ji (x(t )), to a ﬁring rate, ai (x(t ))—is speciﬁed as:
ai (x(t )) = G(Ji (x(t ))).
(cid:110)
(cid:111)−1
Here G is the neurons’ nonlinear current-to-spike-rate function, which is given by
τ ref − τ RC ln (1 − Jth/Ji (x))
G(Ji (x)) =
(2)
for the leaky integrate-and-ﬁre model (LIF). The LIF neuron has two behavioral regimes: sub-
threshold and super-threshold. The sub-threshold regime is described by an RC circuit with time
constant τ RC . When the sub-threshold soma voltage reaches the threshold, Vth , the neuron emits a
spike δ (t − tn ). After this spike, the neuron is reset and rests for τ ref seconds (absolute refractory pe-
riod) before it resumes integrating. Jth = Vth /R is the minimum input current that produces spiking.
Ignoring the soma’s RC time-constant when specifying the SNN’s dynamics are reasonable because
the neurons cross threshold at a rate that is proportional to their input current, which thus sets the
spike rate instantaneously, without any ﬁltering [11].
The conversion from a multi-dimensional stimulus, x(t ), to a one-dimensional soma current, Ji , is
(cid:10) ˜φ x
i · x(t )(cid:11) + J bias
performed by assigning to the neuron a preferred direction, ˜φ x
i , in the stimulus space and taking the
dot-product:
i
3

Ji (x(t )) = αi

(1)

,

,

(3)

−1010200400Stimulus xSpike rate (spikes/s)y(t)B'x(t)A'bk(t)aj(t)y(t)A'x(t)B'h(t)E =

where αi is a gain or conversion factor, and J bias
is a bias current that accounts for background
i
is either +1 or −1 (drawn randomly), for ON and OFF neurons,
activity. For a 1D space, ˜φ x
i
respectively. The resulting tuning curves are illustrated in Figure 1, left.
The linear decoding process is characterized by the synapses’ spike response, h(t ) (i.e., post-synaptic
currents), and the decoding weights, φ x
i , which are obtained by minimizing the mean square error.
A single noise term, η , takes into account all sources of noise, which have the effect of introducing
uncertainty into the decoding process. Hence, the transmitted ﬁring rate can be written as ai (x(t )) +
ηi , where ai (x(t )) represents the noiseless set of tuning curves and ηi is a random variable picked
from a zero-mean Gaussian distribution with variance σ 2 . Consequently, the mean square error can
(cid:35)2(cid:43)
(cid:42)(cid:34)
[x(t ) − ˆx(t )]2(cid:69)
(cid:68)
be written as [11]:
1
1
x(t ) − ∑
(ai (x(t )) + ηi ) φ x
i
2
2
x,η ,t
i
x,η ,t
where (cid:104)·(cid:105)x,η denotes integration over the range of x and η , the expected noise. We assume that the
(cid:42)(cid:34)
(cid:35)2(cid:43)
noise is independent and has the same variance for each neuron [11], which yields:
x(t ) − ∑
σ 2 ∑
(φ x
i )2 ,
i
i
x,t
where σ 2 is the noise variance ηiη j . This expression is minimized by:
N
Γ−1
∑
with Γi j = (cid:10)ai (x)a j (x)(cid:11)
i j ϒ j ,
(cid:10)xa j (x)(cid:11)
j
x + σ 2 δi j , where δ is the Kronecker delta function matrix, and ϒ j =
x [11]. One consequence of modeling noise in the neural representation is that the matrix
Γ is invertible despite the use of a highly overcomplete representation. In a noiseless representation,
Γ is generally singular because, due to the large number of neurons, there is a high probability of
having two neurons with similar tuning curves leading to two similar rows in Γ.

ai (x(t ))φ x
i

φ x
i =

+

1
2

E =

1
2

=

(4)

(5)

(6)

3 Kalman-ﬁlter based cortical decoder

In the 1960’s, Kalman described a method that uses linear ﬁltering to track the state of a dynamical
system throughout time using a model of the dynamics of the system as well as noisy measure-
ments [15]. The model dynamics gives an estimate of the state of the system at the next time step.
This estimate is then corrected using the observations (i.e., measurements) at this time step. The
relative weights for these two pieces of information are given by the Kalman gain, K [15, 16].
Whereas the Kalman gain is updated at each iteration, the state and observation matrices (deﬁned
below)—and corresponding noise matrices—are supposed constant.
In the case of prosthetic applications, the system’s state vector is the cursor’s kinematics, xt =
t , vel y
[vel x
t , 1], where the constant 1 allows for a ﬁxed offset compensation. The measurement vector,
yt , is the neural spike rate (spike counts in each time step) of 192 channels of neural threshold
crossings. The system’s dynamics is modeled by:
xt = Axt−1 + wt ,
(7)
yt = Cxt + qt ,
(8)
where A is the state matrix, C is the observation matrix, and wt and qt are additive, Gaussian noise
sources with wt ∼ N (0, W) and qt ∼ N (0, Q). The model parameters (A, C, W and Q) are ﬁt with
training data by correlating the observed hand kinematics with the simultaneously measured neural
signals (Figure 2).
For an efﬁcient decoding, we derived the steady-state update equation by replacing the adaptive
Kalman gain by its steady-state formulation: K = (I + WCQ−1C)−1 W CTQ−1 . This yields the
following estimate of the system’s state:
xt = (I − KC)Axt−1 + Kyt = MDT
x xt−1 + MDT
y yt ,

(9)

4

Figure 2: Neural and kinematic measurements (monkey J, 2011-04-16, 16 continuous trials) used to
ﬁt the standard Kalman ﬁlter model. a. The 192 cortical recordings fed as input to ﬁt the Kalman
ﬁlter’s matrices (color code refers to the number of threshold crossings observed in each 50ms bin).
b. Hand x- and y-velocity measurements correlated with the neural data to obtain the Kalman ﬁlter’s
matrices. c. Cursor kinematics of 16 continuous trials under direct hand control.

x = (I − KC)A and MDT
where MDT
y = K are the discrete time (DT) Kalman matrices. The steady-
state formulation improves efﬁciency with little loss in accuracy because the optimal Kalman gain
rapidly converges (typically less than 100 iterations).
Indeed, in neural applications under both
open-loop and closed-loop conditions, the difference between the full Kalman ﬁlter and its steady-
state implementation falls to within 1% in a few seconds [17]. This simplifying assumption reduces
the execution time for decoding a typical neuronal ﬁring rate signal approximately seven-fold [17],
a critical speed-up for real-time applications.

4 Kalman ﬁlter with a spiking neural network
To implement the Kalman ﬁlter with a SNN by applying the NEF, we ﬁrst convert Equation 9 from
x(t ) = h(t ) ∗ (cid:0)A(cid:48) x(t ) + B(cid:48)y(t )(cid:1) ,
DT to continuous time (CT), and then replace the CT matrices with neurally plausible ones, which
yields:
(cid:1) /∆t , the CT
y = (cid:0)MDT
x − I(cid:1) /∆t and MCT
x = (cid:0)MDT
(10)
where A(cid:48) = τMCT
x + I, B(cid:48) = τMCT
, with MCT
y
y
Kalman matrices, and ∆t = 50ms, the discrete time step; τ is the synaptic time-constant.
The j th neuron’s input current (see Equation 3) is computed from the system’s current state, x(t ),
which is computed from estimates of the system’s previous state ( ˆx(t ) = ∑i ai (t )φ x
i ) and current
j · x(t )(cid:11) + J bias
(cid:10) ˜φ x
input ( ˆy(t ) = ∑k bk (t )φ y
k ) using Equation 10. This yields:
j · h(t ) ∗ (cid:0)A(cid:48) ˆx(t ) + B(cid:48) ˆy(t )(cid:1)(cid:11) + J bias
(cid:10) ˜φ x
J j (x(t )) = α j
(cid:42)
(cid:32)
j
= α j
j
i + B(cid:48) ∑
A(cid:48) ∑
j · h(t ) ∗
bk (t )φ y
ai (t )φ x
˜φ x
k
(cid:33)
(cid:32)
i
k
This last equation can be written in a neural network form:
J j (x(t )) = h(t ) ∗
(cid:69)
(cid:68) ˜φ x
(cid:68) ˜φ x
(cid:69)
ω jiai (t ) + ∑
∑
ω jk bk (t )
i
k
j A(cid:48)φ x
j B(cid:48)φ y
i
k

are the recurrent and feedforward weights, re-

and ω jk = α j

(11)

(12)

= α j

(cid:33)(cid:43)

+ J bias
j

+ J bias
j

where ω ji = α j
spectively.

5 Efﬁcient implementation of the SNN

In this section, we describe the two distinct steps carried out when implementing the SNN: creating
and running the network. The ﬁrst step has no computational constraints whereas the second must
be very efﬁcient in order to be successfully deployed in the closed-loop experimental setting.

5

Neuron  1501005002000400060008000100001200014000−20020Time (ms)Velocity (cm/s)  0510x−velocityy−velocityabTrials: 0034-0049c1cmFigure 3: Computing a 1000-neuron pool’s recurrent connections. a. Using connection weights re-
quires multiplying a 1000 × 1000 matrix by a 1000 × 1 vector. b. Operating in the lower-dimensional
state space requires multiplying a 1 × 1000 vector by a 1000 × 1 vector to get the decoded state, mul-
tiplying this state by a component of the A(cid:48) matrix to update it, and multiplying the updated state by
a 1000 × 1 vector to re-encode it as ﬁring rates, which are then used to update the soma current for
every neuron.

Network creation: This step generates, for a speciﬁed number of neurons composing the network,
, preferred direction ˜φ x
j , and decoding weight φ x
the gain α j , bias current J bias
j for each neuron. The
j
preferred directions ˜φ x
j are drawn randomly from a uniform distribution over the unit sphere. The
maximum ﬁring rate, max G(J j (x)), and the normalized x-axis intercept, G(J j (x)) = 0, are drawn
randomly from a uniform distribution on [200, 400] Hz and [-1, 1], respectively. From these two
speciﬁcations, α j and J bias
are computed using Equation 2 and Equation 3. The decoding weights
j
φ x
j are computed by minimizing the mean square error (Equation 6).
For efﬁcient implementation, we used two 1D integrators (i.e., two recurrent neuron pools, with
each pool representing a scalar) rather than a single 3D integrator (i.e., one recurrent neuron pool,
with the pool representing a 3D vector by itself) [13]. The constant 1 is fed to the 1D integrators as
an input, rather than continuously integrated as part of the state vector. We also replaced the bk (t )
units’ spike rates (Figure 1, middle ) with the 192 neural measurements (spike counts in 50ms bins),
which is equivalent to choosing φ y
k from a standard basis (i.e., a unit vector with 1 at the kth position
and 0 everywhere else) [7].
Network simulation: This step runs the simulation to update the soma current for every neuron,
based on input spikes. The soma voltage is then updated following RC circuit dynamics. Gaussian
noise is normally added at this step, the rest of the simulation being noiseless. Neurons with soma
voltage above threshold generate a spike and enter their refractory period. The neuron ﬁring rates
are decoded using the linear decoding weights to get the updated states values, x and y-velocity.
These values are smoothed with a ﬁlter identical to h(t ), but with τ set to 5ms instead of 20ms to
avoid introducing signiﬁcant delay. Then the simulation step starts over again.
In order to ensure rapid execution of the simulation step, neuron interactions are not updated di-
rectly using the connection matrix (Equation 12), but rather indirectly with the decoding matrix φ x
j ,
dynamics matrix A(cid:48) , and preferred direction matrix ˜φ x
j (Equation 11). To see why this is more efﬁ-
cient, suppose we have 1000 neurons in the a population for each of the state vector’s two scalars.
Computing the recurrent connections using connection weights requires multiplying a 1000 × 1000
matrix by a 1000-dimensional vector (Figure 3a). This requires 106 multiplications and about 106
sums. Decoding each scalar (i.e., ∑i ai (t )φ x
i ), however, requires only 1000 multiplications and 1000
sums. The decoded state vector is then updated by multiplying it by the (diagonal) A(cid:48) matrix, another
2 products and 1 sum. The updated state vector is then encoded by multiplying it with the neurons’
preferred direction vectors, another 1000 multiplications per scalar (Figure 3b). The resulting to-
tal of about 3000 operations is nearly three orders of magnitude fewer than using the connection
weights to compute the identical transformation.
To measure the speedup, we simulated a 2,000-neuron network on a computer running Matlab 2011a
(Intel Core i7, 2.7-GHz, Mac OS X Lion). Although the exact run-times depend on the computing
hardware and software, the run-time reduction factor should remain approximately constant across
platforms. For each reported result, we ran the simulation 10 times to obtain a reliable estimate of
the execution time. The run-time for neuron interactions using the recurrent connection weights was
9.9ms and dropped to 2.7µ s in the lower-dimensional space, approximately a 3,500-fold speedup.
Only the recurrent interactions beneﬁt from the speedup, the execution time for the rest of the oper-
ations remaining constant. The run-time for a 50ms network simulation using the recurrent connec-

6

x=1000x100010001000a10001000x=1000(1000x(bTable 1: Model parameters
Description
Range
Symbol
max G(J j (x))
200-400 Hz
Maximum ﬁring rate
G(J j (x)) = 0 −1 to 1
Normalized x-axis intercept
(cid:13)(cid:13)(cid:13) = 1
(cid:13)(cid:13)(cid:13) ˜φ x
J bias
Satisﬁes ﬁrst two Bias current
j
α j
Satisﬁes ﬁrst two Gain factor
˜φ x
Preferred-direction vector
j
j
σ 2
Gaussian noise variance
0.1
τ RC
RC time constant
20 ms
j
τ ref
Refractory period
1 ms
j
τ PSC
20 ms
PSC time constant
j

tion weights was 0.94s and dropped to 0.0198s in the lower-dimensional space, a 47-fold speedup.
These results demonstrate the efﬁciency the lower-dimensional space offers, which made the closed-
loop application of SNNs possible.

6 Closed-loop implementation

An adult male rhesus macaque (monkey J) was trained to perform a center-out-and-back reaching
task for juice rewards to one of eight targets, with a 500ms hold time (Figure 4a) [1]. All animal
protocols and procedures were approved by the Stanford Institutional Animal Care and Use Com-
mittee. Hand position was measured using a Polaris optical tracking system at 60Hz (Northern
Digital Inc.). Neural data were recorded from two 96-electrode silicon arrays (Blackrock Microsys-
tems) implanted in the dorsal pre-motor and motor cortex. These recordings (-4.5 RMS threshold
crossing applied to each electrode’s signal) yielded tuned activity for the direction and speed of arm
movements. As detailed in [1], a standard Kalman ﬁlter model was ﬁt by correlating the observed
hand kinematics with the simultaneously measured neural signals, while the monkey moved his arm
to acquire virtual targets (Figure 2). The resulting model was used in a closed-loop system to control
an on-screen cursor in real-time (Figure 4a, Decoder block). A steady-state version of this model
serves as the standard against which the SNN implementation’s performance is compared.
We built a SNN using the NEF methodology based on derived Kalman ﬁlter parameters mentioned
above. This SNN was then simulated on an xPC Target (Mathworks) x86 system (Dell T3400, In-
tel Core 2 Duo E8600, 3.33GHz). It ran in closed-loop, replacing the standard Kalman ﬁlter as the
decoder block in Figure 4a. The parameter values listed in Table 1 were used for the SNN implemen-
tation. We ensured that the time constants τ RC
,τ ref
, and τ PSC
were smaller than the implementation’s
i
i
i
time step (50ms). Noise was not explicitly added. It arose naturally from the ﬂuctuations produced
by representing a scalar with ﬁltered spike trains, which has been shown to have effects similar to
Gaussian noise [11]. For the purpose of computing the linear decoding weights (i.e., Γ), we modeled
the resulting noise as Gaussian with a variance of 0.1.
A 2,000-neuron version of the SNN-based decoder was tested in a closed-loop system, the largest
network our embedded MatLab implementation could run in real-time. There were 1206 trials
total among which 301 (center-outs only) were performed with the SNN and 302 with the standard
(steady-state) Kalman ﬁlter. The block structure was randomized and interleaved, so that there is
no behavioral bias present in the ﬁndings. 100 trials under hand control are used as a baseline
comparison. Success corresponds to a target acquisition under 1500ms, with 500ms hold time.
Success rates were higher than 99% on all blocks for the SNN implementation and 100% for the
standard Kalman ﬁlter. The average time to acquire the target was slightly slower for the SNN
(Figure 5b)—711ms vs. 661ms, respectively—we believe this could be improved by using more
neurons in the SNN.1 The average distance to target (Figure 5a) and the average velocity of the
cursor (Figure 5c) are very similar.

1Off-line, the SNN performed better as we increased the number of neurons [7].

7

Figure 4: Experimental setup and results. a. Data are recorded from two 96-channel silicon electrode
arrays implanted in dorsal pre-motor and motor cortex of an adult male monkey performing a center-
out-and-back reach task for juice rewards to one of eight targets with a 500ms hold time. b. BMI
position kinematics of 16 continuous trials for the standard Kalman ﬁlter implementation. c. BMI
position kinematics of 16 continuous trials for the SNN implementation.

Figure 5: SNN (red) performance compared to standard Kalman ﬁlter (blue) (hand trials are shown
for reference (yellow)). The SNN achieves similar results—success rates are higher than 99% on
all blocks—as the standard Kalman ﬁlter implementation. a. Plot of distance to target vs. time both
after target onset for different control modalities. The thicker traces represent the average time when
the cursor ﬁrst enters the acceptance window until successfully entering for the 500ms hold time.
b. Histogram of target acquisition time. c. Plot of mean cursor velocity vs. time.

7 Conclusions and future work

The SNN’s performance was quite comparable to that produced by a standard Kalman ﬁlter im-
plementation. The 2,000-neuron network had success rates higher than 99% on all blocks, with
mean distance to target, target acquisition time, and mean cursor velocity curves very similar to
the ones obtained with the standard implementation. Future work will explore whether these re-
sults extend to additional animals. As the Kalman ﬁlter and its variants are the state-of-the-art in
cortically-controlled motor prostheses [1]-[5], these simulations provide conﬁdence that similar lev-
els of performance can be attained with a neuromorphic system, which can potentially overcome the
power constraints set by clinical applications.
Our ultimate goal is to develop an ultra-low-power neuromorphic chip for prosthetic applications
on to which control theory algorithms can be mapped using the NEF. As our next step in this direc-
tion, we will begin exploring this mapping with Neurogrid, a hardware platform with sixteen pro-
grammable neuromorphic chips that can simulate up to a million spiking neurons in real-time [9].
However, bandwidth limitations prevent Neurogrid from realizing random connectivity patterns. It
can only connect each neuron to thousands of others if neighboring neurons share common inputs
— just as they do in the cortex. Such columnar organization may be possible with NEF-generated
networks if preferred directions vectors are assigned topographically rather than randomly. Imple-
menting this constraint effectively is a subject of ongoing research.

Acknowledgment

This work was supported in part by the Belgian American Education Foundation(J. Dethier), Stan-
ford NIH Medical Scientist Training Program (MSTP) and Soros Fellowship (P. Nuyujukian),
DARPA Revolutionizing Prosthetics program (N66001-06-C-8005, K. V. Shenoy), and two NIH
Director’s Pioneer Awards (DP1-OD006409, K. V. Shenoy; DPI-OD000965, K. Boahen).

8

DecoderNeuralSpikesCursorVelocityab1cmTrials: 2056-2071BMI: Kalman decoderc1cmTrials: 1748-1763BMI: SNN decoder04008000510Time after Target Onset (ms)Distance to Target (cm) Mean distance to target  a050010001500010203040Target Acquire Time (ms)Percent of Trials (%)Target acquisition time histogram  b0200400600800100001020304050Time after Target Onset (ms)Cursor Velocity (cm/s)Mean cursor velocity  Standard Kalman filterHandSpiking Neural NetworkcReferences
[1] V. Gilja, Towards clinically viable neural prosthetic systems, Ph.D. Thesis, Department of Computer
Science, Stanford University, 2010, pp 19–22 and pp 57–73.
[2] V. Gilja, P. Nuyujukian, C.A. Chestek, J.P. Cunningham, J.M. Fan, B.M. Yu, S.I. Ryu, and K.V. Shenoy, A
high-performance continuous cortically-controlled prosthesis enabled by feedback control design, 2010
Neuroscience Meeting Planner, San Diego, CA: Society for Neuroscience, 2010.
[3] P. Nuyujukian, V. Gilja, C.A. Chestek, J.P. Cunningham, J.M. Fan, B.M. Yu, S.I. Ryu, and K.V. Shenoy,
Generalization and robustness of a continuous cortically-controlled prosthesis enabled by feedback con-
trol design, 2010 Neuroscience Meeting Planner, San Diego, CA: Society for Neuroscience, 2010.
[4] V. Gilja, C.A. Chestek, I. Diester, J.M. Henderson, K. Deisseroth, and K.V. Shenoy, Challenges and op-
portunities for next-generation intra-cortically based neural prostheses, IEEE Transactions on Biomedical
Engineering, 2011, in press.
[5] S.P. Kim, J.D. Simeral, L.R. Hochberg, J.P. Donoghue, and M.J. Black, Neural control of computer
cursor velocity by decoding motor cortical spiking activity in humans with tetraplegia, Journal of Neural
Engineering, vol. 5, 2008, pp 455–476.
[6] S. Kim, P. Tathireddy, R.A. Normann, and F. Solzbacher, Thermal impact of an active 3-D microelectrode
array implanted in the brain, IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol.
15, 2007, pp 493–501.
[7] J. Dethier, V. Gilja, P. Nuyujukian, S.A. Elassaad, K.V. Shenoy, and K. Boahen, Spiking neural network
decoder for brain-machine interfaces, IEEE Engineering in Medicine & Biology Society Conference on
Neural Engineering, Cancun, Mexico, 2011, pp 396–399.
[8] K. Boahen, Neuromorphic microchips, Scientiﬁc American, vol. 292(5), 2005, pp 56–63.
[9] R. Silver, K. Boahen, S. Grillner, N. Kopell, and K.L. Olsen, Neurotech for neuroscience: unifying con-
cepts, organizing principles, and emerging tools, Journal of Neuroscience, vol. 27(44), 2007, pp 11807–
11819.
[10] J.V. Arthur and K. Boahen, Silicon neuron design: the dynamical systems approach, IEEE Transactions
on Circuits and Systems, vol. 58(5), 2011, pp 1034-1043.
[11] C. Eliasmith and C.H. Anderson, Neural engineering: computation, representation, and dynamics in
neurobiological systems, MIT Press, Cambridge, MA; 2003.
[12] C. Eliasmith, A uniﬁed approach to building and controlling spiking attractor networks, Neural Compu-
tation, vol. 17, 2005, pp 1276–1314.
[13] R. Singh and C. Eliasmith, Higher-dimensional neurons explain the tuning and dynamics of working
memory cells, The Journal of Neuroscience, vol. 26(14), 2006, pp 3667–3678.
[14] C. Eliasmith, How to build a brain: from function to implementation, Synthese, vol. 159(3), 2007, pp
373–388.
[15] R.E. Kalman, A new approach to linear ﬁltering and prediction problems, Transactions of the ASME–
Journal of Basic Engineering, vol. 82(Series D), 1960, pp 35–45.
[16] G. Welsh and G. Bishop, An introduction to the Kalman Filter, University of North Carolina at Chapel
Hill Chapel Hill NC, vol. 95(TR 95-041), 1995, pp 1–16.
[17] W.Q. Malik, W. Truccolo, E.N. Brown, and L.R. Hochberg, Efﬁcient decoding with steady-state Kalman
ﬁlter in neural interface systems, IEEE Transactions on Neural Systems and Rehabilitation Engineering,
vol. 19(1), 2011, pp 25–34.

9


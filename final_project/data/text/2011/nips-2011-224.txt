Expressive Power and Approximation Errors of
Restricted Boltzmann Machines

Guido F. Mont ´ufar1 , Johannes Rauh1 , and Nihat Ay1,2
1Max Planck Institute for Mathematics in the Sciences, Inselstraße 22 04103 Leipzig, Germany
2Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, New Mexico 87501, USA
{montufar,jrauh,nay}@mis.mpg.de

Abstract

We present explicit classes of probability distributions that can be learned by Re-
stricted Boltzmann Machines (RBMs) depending on the number of units that they
contain, and which are representative for the expressive power of the model. We
use this to show that the maximal Kullback-Leibler divergence to the RBM model
with n visible and m hidden units is bounded from above by (n − 1) − log(m + 1).
In this way we can specify the number of hidden units that guarantees a sufﬁciently
rich model containing different classes of distributions and respecting a given er-
ror tolerance.

1

Introduction

A Restricted Boltzmann Machine (RBM) [24, 10] is a learning system consisting of two layers
of binary stochastic units, a hidden layer and a visible layer, with a complete bipartite interaction
graph. RBMs are used as generative models to simulate input distributions of binary data. They can
be trained in an unsupervised way and more efﬁciently than ge neral Boltzmann Machines, which are
not restricted to have a bipartite interaction graph [11, 6]. Furthermore, they can be used as building
blocks to progressively train and study deep learning systems [13, 4, 16, 21]. Hence, RBMs have
received increasing attention in the past years.

pW,C,B (v) =

∀v ∈ {0, 1}n ,

An RBM with n visible and m hidden units generates a stationary distribution on the states of the
visible units which has the following form:
1
ZW,C,B Xh∈{0,1}m
exp (cid:0)h⊤W v + C ⊤h + B⊤ v(cid:1)
where h ∈ {0, 1}m denotes a state vector of the hidden units, W ∈ Rm×n , C ∈ Rm and B ∈
Rn constitute the model parameters, and ZW,C,B is a corresponding normalization constant.
In
the sequel we denote by RBMn,m the set of all probability distributions on {0, 1}n which can be
approximated arbitrarily well by a visible distribution generated by the RBM with m hidden and n
visible units for an appropriate choice of the parameter values.
As shown in [21] (generalizing results from [15]) RBMn,m contains any probability distribution if
m ≥ 2n−1 − 1. On the other hand, if RBMn,m equals the set P of all probability distributions on
{0, 1}n , then it must have at least dim(P ) = 2n − 1 parameters, and thus at least ⌈2n /(n + 1)⌉ − 1
hidden units [21]. In fact, in [8] it was shown that for most combinations of m and n the dimension
of RBMn,m (as a manifold, possibly with singularities) equals either the number of parameters or
2n − 1, whatever is smaller. However, the geometry of RBMn,m is intricate, and even an RBM of
dimension 2n − 1 is not guaranteed to contain all visible distributions, see [20] for counterexamples.
In summary, an RBM that can approximate any distribution arbitrarily well must have a very large
number of parameters and hidden units. In practice, training such a large system is not desirable or
even possible. However, there are at least two reasons why in many cases this is not necessary:

1

• An appropriate approximation of distributions is sufﬁcien t for most purposes.
• The interesting distributions the system shall simulate belong to a small class of distribu-
tions. Therefore, the model does not need to approximate all distributions.

For example, the set of optimal policies in reinforcement learning [25], the set of dynamics kernels
that maximize predictive information in robotics [26] or the information ﬂow in neural networks [3]
are contained in very low dimensional manifolds; see [2]. On the other hand, usually it is very
hard to mathematically describe a set containing the optimal solutions to general problems, or a
set of interesting probability distributions (for example the class of distributions generating natural
images). Furthermore, although RBMs are parametric models and for any choice of the parameters
we have a resulting probability distribution, in general it is difﬁcult to explicitly specify this resulting
probability distribution (or even to estimate it [18]). Due to these difﬁculties the number of hidden
units m is often chosen on the basis of experience [12], or m is considered as a hyperparameter
which is optimized by extensive search, depending on the distributions to be simulated by the RBM.
In this paper we give an explicit description of classes of distributions that are contained in RBMn,m ,
and which are representative for the expressive power of this model. Using this description, we
estimate the maximal Kullback-Leibler divergence between an arbitrary probability distribution and
the best approximation within RBMn,m .
This paper is organized as follows: Section 2 discusses the different kinds of errors that appear when
an RBM learns. Section 3 introduces the statistical models studied in this paper. Section 4 studies
submodels of RBMn,m . An upper bound of the approximation error for RBMs is found in Section 5.

2 Approximation Error

When training an RBM to represent a distribution p, there are mainly three contributions to the
discrepancy between p and the state of the RBM after training:

1. Usually the underlying distribution p is unknown and only a set of samples generated by
p is observed. These samples can be represented as an empirical distribution pData , which
usually is not identical with p.
2. The set RBMn,m does not contain every probability distribution, unless the number of
hidden units is very large, as we outlined in the introduction. Therefore, we have an ap-
proximation error given by the distance of pData to the best approximation pData
RBM contained
in the RBM model.
RBM in RBM which is not the optimum pData
3. The learning process may yield a solution ˜pData
RBM .
This occurs, for example, if the learning algorithm gets trapped in a local optimum, or if
it optimizes an objective different from Maximum Likelihood, e.g. contrastive divergence
(CD), see [6].

In this paper we study the expressive power of the RBM model and the Kullback-Leibler diver-
gence from an arbitrary distribution to its best representation within the RBM model. Estimating the
approximation error is difﬁcult, because the geometry of th e RBM model is not sufﬁciently under-
stood. Our strategy is to ﬁnd subsets M ⊆ RBMn,m that are easy to describe. Then the maximal
error when approximating probability distributions with an RBM is upper bounded by the maximal
error when approximating with M.
Consider a ﬁnite set X . A real valued function on X can be seen as a real vector with |X | entries.
The set P = P (X ) of all probability distributions on X is a (|X | − 1)-dimensional simplex in
R|X | . There are several notions of distance between probability distributions, and in turn for the
error in the representation (approximation) of a probability distribution. One possibility is to use the
induced distance of the Euclidian space R|X | . From the point of view of information theory, a more
meaningful distance notion for probability distributions is the Kullback-Leibler divergence:
p(x)
D(pkq) := Xx
q(x)
In this paper we use the basis 2 logarithm. The Kullback-Leibler (KL) divergence is non-negative
and vanishes if and only if p = q . If the support of q does not contain the support of p it is deﬁned

p(x) log

.

2

q = p

q = 1
|X |

relative error

0

128
255

1

Figure 1: This ﬁgure gives an intuition on what the size of an e rror means for probability distri-
butions on images with 16 × 16 pixels. Every column shows four samples drawn from the best
approximation q of the distribution p = 1
2 (δ(1...1) + δ(0...0) ) within a partition model with 2 ran-
domly chosen cubical blocks, containing (0 . . . 0) and (1 . . . 1), of cardinality from 1 ( ﬁrst column)
to |X |
|X | (cid:1). The
2 (last column). As a measure of error ranging from 0 to 1 we take D(pkq)/D(cid:0)pk 1
last column shows samples from the uniform distribution, which is, in particular, the best approxi-
mation of p within RBMn,0 . Note that an RBM with 1 hidden unit can approximate p with arbitrary
accuracy, see Theorem 4.1.

as ∞. The summands with p(x) = 0 are set to 0. The KL-divergence is not symmetric, but it has
nice information theoretic properties [14, 7].
If E ⊆ P is a statistical model and if p ∈ P , then any probability distribution pE ∈ E satisfying

D(pkpE ) = D(pkE ) := min{D(pkq) : q ∈ E }

is called a (generalized) reversed information projection, or rI -projection. Here, E denotes the
closure of E . If p is an empirical distribution, then one can show that any rI -projection is a maximum
likelihood estimate.

In order to assess an RBM or some other model M we use the maximal approximation error with
respect to the KL-divergence when approximating arbitrary probability distributions using M:

DM := max {D(pkM) : p ∈ P } .

For example, the maximal KL-divergence to the uniform distribution 1
|X | is attained by any Dirac
delta distributions δx , x ∈ X , and amounts to:
|X | } = D(δx k 1
|X | ) = log |X | .
D{ 1

(1)

3 Model Classes

3.1 Exponential families and product measures

In this work we only need a restricted class of exponential families, namely exponential families on
a ﬁnite set with uniform reference measure. See [5] for more o n exponential families. The boundary
of discrete exponential families is discussed in [23], which uses a similar notation.

Let A ∈ Rd×|X | be a matrix. The columns Ax of A will be indexed by x ∈ X . The rows of A can
be interpreted as functions on R. The exponential family EA with sufﬁcient statistics A consists of
all probability distributions of the form pλ , λ ∈ Rd , where

pλ (x) =

,

exp(λ⊤Ax )
Px exp(λ⊤Ax )
Note that any probability distribution in EA has full support. Furthermore, EA is in general not a
closed set. The closure EA (with respect to the usual topology on RX ) will be important in the
following. Exponential families behave nicely with respect to rI -projection: Any p ∈ P has a
unique rI -projection pE to EA .

for all x ∈ X .

3

The most important exponential families in this work are the independence models. The indepen-
dence model of n binary random variables consists of all probability distributions on {0, 1}n that
factorize:

n
pi (xi ) for some pi ∈ P ({0, 1})o .
En = np ∈ P (X ) : p(x1 , . . . , xn ) =
Yi=1
It is the closure of an n-dimensional exponential family En . This model corresponds to the RBM
model with no hidden units. An element of the independence model is called a product distribution.

Lemma 3.1 (Corollary 4.1 of [1]) Let En be the independence model on {0, 1}n . If n > 0, then
DEn = (n − 1). The global maximizers are the distributions of the form 1
2 (δx + δy ), where x, y ∈
{0, 1}n satisfy xi + yi = 1 for all i.

This result should be compared with (1). Although the independence model is much larger than the
set { 1
|X | }, the maximal divergence decreases only by 1. As shown in [22], if E is any exponential
family of dimension k , then DE ≥ log(|X |/(k + 1)). Thus, this notion of distance is rather strong.
The exponential families satisfying DE = log(|X |/(k+1)) are partition models; they will be deﬁned
in the following section.

3.2 Partition models and mixtures of products with disjoint supports

The mixture of m models M1 , . . . , Mm ⊆ P is the set of all convex combinations
p = Xi
αi pi , where pi ∈ Mi , αi ≥ 0, Xi
In general, mixture models are complicated objects. Even if all models M1 = · · · = Mm are equal,
it is difﬁcult to describe the mixture [17, 19]. The situatio n simpli ﬁes considerably if the models
have disjoint supports. Note that given any partition ξ = {X1 , . . . , Xm } of X , any p ∈ P can be
written as p(x) = pXi (x)p(Xi ) for all x ∈ Xi and i ∈ {1, . . . , m}, where pXi is a probability
measure in P (Xi ) for all i.

αi = 1 .

(2)

Lemma 3.2 Let ξ = {X1 , . . . , Xm} be a partition of X and let M1 , . . . , Mm be statistical models
such that Mi ⊆ P (Xi ). Consider any p ∈ P and corresponding pXi such that p(x) = pXi (x)p(Xi )
for x ∈ Xi . Let pi be an rI -projection of pXi to Mi . Then the rI -projection pM of P to the mixture
M of M1 , . . . , Mm satis ﬁes

whenever x ∈ Xi .
pM (x) = p(Xi )pi (x),
Therefore, D(pkM) = Pi p(Xi )D(pXi kMi ), and so DM = maxi=1,...,m DMi .
Proof Let p ∈ M be as in (2). Then D(qkp) = Pm
i=1 q(Xi )D(qXi kpi ) for all q ∈ P . For ﬁxed q
this sum is minimal if and only if each term is minimal.
(cid:3)
If each Mi is an exponential family, then the mixture is also an exponential family (this is not true if
the supports of the models Mi are not disjoint). In the rest of this section we discuss two examples.
If each Mi equals the set containing just the uniform distribution on Xi , then M is called the
partition model of ξ , denoted with Pξ . The partition model Pξ is given by all distributions with
constant value on each block Xi , i.e. those that satisfy p(x) = p(y) for all x, y ∈ Xi . This is the
closure of the exponential family with sufﬁcient statistic s
Ax = (χ1 (x), χ2 (x), . . . , χd (x))⊤ ,
is 1 on x ∈ Xi , and 0 everywhere else. See [22] for interesting properties of

where χi := χXi
partition models.

The partition models include the set of ﬁnite exchangeable d istributions (see e.g. [9]), where the
blocks of the partition are the sets of binary vectors which have the same number of entries equal to
one. The probability of a vector v depends only on the number of ones, but not on their position.

Corollary 3.3 Let ξ = {X1 , . . . , Xm} be a partition of X . Then DPξ = maxi=1,...,m log |Xi |.

4


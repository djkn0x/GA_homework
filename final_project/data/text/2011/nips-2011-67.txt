Statistical Performance of Convex Tensor
Decomposition

Ryota Tomioka†
Taiji Suzuki†
†Department of Mathematical Informatics,
The University of Tokyo
Tokyo 113-8656, Japan
tomioka@mist.i.u-tokyo.ac.jp
s-taiji@stat.t.u-tokyo.ac.jp

Kohei Hayashi‡
‡Graduate School of Information Science,
Nara Institute of Science and Technology
Nara 630-0192, Japan
kohei-h@is.naist.jp

Hisashi Kashima†,∗
∗Basic Research Programs PRESTO,
Synthesis of Knowledge for Information Oriented Society, JST
Tokyo 102-8666, Japan
kashima@mist.i.u-tokyo.ac.jp

Abstract

We analyze the statistical performance of a recently proposed convex tensor de-
composition algorithm. Conventionally tensor decomposition has been formu-
lated as non-convex optimization problems, which hindered the analysis of their
performance. We show under some conditions that the mean squared error of
the convex method scales linearly with the quantity we call the normalized rank
of the true tensor. The current analysis naturally extends the analysis of convex
low-rank matrix estimation to tensors. Furthermore, we show through numerical
experiments that our theory can precisely predict the scaling behaviour in practice.

1 Introduction

Tensors (multi-way arrays) generalize matrices and naturally represent data having more than two
modalities. For example, multi-variate time-series, for instance, electroencephalography (EEG),
recorded from multiple subjects under various conditions naturally form a tensor. Moreover, in
collaborative ﬁltering, users’ preferences on products, conventionally represented as a matrix, can
be represented as a tensor when the preferences change over time or context.
For the analysis of tensor data, various models and methods for the low-rank decomposition of
tensors have been proposed (see Kolda & Bader [12] for a recent survey). These techniques have
recently become increasingly popular in data-mining [1, 14] and computer vision [25, 26]. Besides
they have proven useful in chemometrics [4], psychometrics [24], and signal processing [20, 7, 8].
Despite empirical success, the statistical performance of tensor decomposition algorithms has not
been fully elucidated. The difﬁculty lies in the non-convexity of the conventional tensor decom-
position algorithms (e.g., alternating least squares [6]).
In addition, studies have revealed many
discrepancies (see [12]) between matrix rank and tensor rank, which make extension of studies on
the performance of low-rank matrix models (e.g., [9]) challenging.
Recently, several authors [21, 10, 13, 23] have focused on the notion of tensor mode-k rank (instead
of tensor rank), which is related to the Tucker decomposition [24]. They discovered that regularized
estimation based on the Schatten 1-norm, which is a popular technique for recovering low-rank
matrices via convex optimization, can also be applied to tensor decomposition. In particular, the

1

ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ
Figure 1: Result of estimation of rank-(7, 8, 9) tensor of dimensions 50 × 50 × 20 from partial
measurements; see [23] for the details. The estimation error
F is plotted against the
fraction of observed elements m = M /N . Error bars over 10 repetitions are also shown. Convex
refers to the convex tensor decomposition based on the minimization problem (7). Tucker (exact)
refers to the conventional (non-convex) Tucker decomposition [24] at the correct rank. Gray dashed
line shows the optimization tolerance 10−3 . The question is how we can predict the point where the
generalization begins (roughly m = 0.35 in this plot).

study in [23] showed that there is a clear transition at certain number of samples where the error
drops dramatically from no generalization to perfect generalization (see Figure 1).
In this paper, motivated by the above recent work, we mathematically analyze the performance of
convex tensor decomposition. The new convex formulation for tensor decomposition allows us to
generalize recent results on Schatten 1-norm-regularized estimation of matrices (see [17, 18, 5, 19]).
Under a general setting we show how the estimation error scales with the mode-k ranks of the true
tensor. Furthermore, we analyze the speciﬁc settings of (i) noisy tensor decomposition and (ii)
random Gaussian design. In the ﬁrst setting, we assume that all the elements of a low-rank tensor
is observed with noise and the goal is to recover the underlying low-rank structure. This is the most
common setting a tensor decomposition algorithm is used. In the second setting, we assume that
the unknown tensor is a coefﬁcient of a tensor-input scalar-output regression problem and the input
tensors (design) are randomly given from independent Gaussian distributions. Surprisingly, it turns
out that the random Gaussian setting can precisely predict the phase-transition-like behaviour in
Figure 1. To the best of our knowledge, this is the ﬁrst paper that rigorously studies the performance
of a tensor decomposition algorithm.

2 Notation

In this section, we introduce the notations we use in this paper. Moreover, we introduce a H ¨older-
Q
like inequality (3) and the notion of mode-k decomposability (5), which play central roles in our
analysis.
p〈X , X 〉.
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
Let X ∈ Rn1×···nK be a K -way tensor. We denote the number of elements in X by N =
K
k=1 nk .
The inner product between two tensors 〈W , X 〉 is deﬁned as 〈W , X 〉 = vec(W )⊤vec(X ), where
Q
vec is a vectorization. In addition, we deﬁne the Frobenius norm of a tensor
F =
The mode-k unfolding X (k) is the nk × ¯n\k ( ¯n\k :=
k′ ̸=k nk′ ) matrix obtained by concatenating
the mode-k ﬁbers (the vectors obtained by ﬁxing every index of X but the k th index) of X as column
vectors. The mode-k rank of a tensor X , denoted by rankk (X ), is the rank of the mode-k unfolding
X (k) (as a matrix). Note that when K = 2 and X is actually a matrix, and X (2) = X (1)
⊤ . We say
a tensor X is rank (r1 , . . . , rK ) when rk = rankk (X ) for k = 1, . . . , K . Note that the mode-k rank
can be computed in a polynomial time, because it boils down to computing a matrix rank, whereas
computing tensor rank is NP complete [11]. See [12] for more details.
Since for each k , the convex envelope of the mode-k rank is given as the Schatten 1-norm [18]
(known as the trace norm [22] or the nuclear norm [3]), it is natural to consider the following

2

00.20.40.60.8110−3100Fraction of observed elements  ConvexTucker (exact)Optimization tolerance(1)

overlapped Schatten 1-norm

ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
KX
ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
(cid:176)(cid:176)W (k)
(cid:176)(cid:176)
of a tensor W ∈ Rn1×···×nK (see also [21]):
S1
1
=
,
Xr
S1
S1
K
where W (k) is the mode-k unfolding of W . Here ∥ · ∥S1 is the Schatten 1-norm for a matrix
k=1
∥W ∥S1 =
σj (W ),
j=1
where σj (W ) is the j th largest singular-value of W . The dual norm of the Schatten 1-norm is the
Schatten ∞-norm (known as the spectral norm) as follows:
∥X ∥S∞ = max
σj (X ).
j=1,...,r
Since the two norms ∥ · ∥S1 and ∥ · ∥S∞ are dual to each other, we have the following inequality:
∥X ∥S∞ ,
|〈W , X 〉| ≤ ∥W ∥S1
(2)
where 〈W , X 〉 is the inner product of W and X .
ﬂﬂﬂﬂﬂﬂ·ﬂﬂﬂﬂﬂﬂ
The same inequality holds for the overlapped Schatten 1-norm (1) and its dual norm. The dual norm
of the overlapped Schatten 1-norm can be characterized by the following lemma.
Lemma 1. The dual norm of the overlapped Schatten 1-norm denoted as
is deﬁned as the
∗
tensor X as follows:ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
S
1
inﬁmum of the maximum mode-k spectral norm over the tensors whose average equals the given
∥Y (k)
∥S∞ ,
max
inf
=
∗
ﬂﬂﬂﬂﬂﬂ·ﬂﬂﬂﬂﬂﬂ
K (Y (1)+Y (2)+···+Y (K ) )=X
(k)
S
1
k=1,...,K
1
(k) is the mode-k unfolding of Y (k) . Moreover, the following upper bound on the dual norm
where Y (k)
XK
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
≤ ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
is valid:
∗
S
1
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
k=1
:=
Proof. The ﬁrst part can be shown by solving the dual of the maximization problem
∗
KPK
S
X /ck ,
sup 〈W , X 〉 s.t.
≤ 1. The second part is obtained by setting Y (k) =
1
k′=1 1/ck′
where ck = ∥X (k) ∥S∞ , and using Jensen’s inequality.
S1
|〈W , X 〉| ≤ ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
≤ ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
Note that the above bound is tighter than the more intuitive relation | 〈W , X 〉 | ≤ ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
According to Lemma 1, we have the following H ¨older-like inequality
ﬂﬂﬂﬂﬂﬂX ﬂﬂﬂﬂﬂﬂ
mean .
∗
S1
S1
S
1
S∞
S∞ := max1,...,K ∥X (k) ∥S∞ ), which one might come up as an analogy to the matrix case (2).
S1
(
Finally, let W ∗ ∈ Rn1×···×nK be the low-rank tensor that we wish to recover. We assume that W ∗
is rank (r1 , . . . , rK ). Thus, for each k we have
∗
(k = 1, . . . , K ),
(k) = U kS kV k
W
where U k ∈ Rnk×rk and V k ∈ R ¯n\k×rk are orthogonal, and S k ∈ Rrk×rk is diagonal. Let
∆ ∈ Rn1×···×nK be an arbitrary tensor. We deﬁne the mode-k orthogonal complement ∆
′′
k of an
unfolding ∆(k) ∈ Rnk× ¯n\k of ∆ with respect to the true low-rank tensor W ∗ as follows:
− U kU k
− V kV k
′′
⊤ )∆(k) (I ¯n\k
⊤ ).
k = (I nk
∆
(4)
k := ∆(k) − ∆
′′
′
In addition ∆
k is the component having overlapped row/column space with the
∗
′
′′
(k) . Note that the decomposition ∆(k) = ∆
k + ∆
unfolding of the true tensor W
k is deﬁned for
each mode; thus we use subscript k instead of (k).
Using the decomposition deﬁned above we have the following equality, which we call mode-k de-
composability of the Schatten 1-norm:
∥S1
∥S1 + ∥∆
∥W
∥S1 = ∥W
′′
∗
′′
∗
(k = 1, . . . , K ).
(k) + ∆
(5)
k
(k)
k
The above decomposition is deﬁned for each mode and thus it is weaker than the notion of decom-
posability discussed by Negahban et al. [15].

∥X (k) ∥S∞ .

(3)

∗
1

S

mean :=

1
K

3

3 Theory

In this section, we ﬁrst present a deterministic result that holds under a certain choice of regular-
ization constant λM and an assumption called the restricted strong convexity. Then, we focus on
special cases to justify the choice of regularization constant and the restricted strong convexity as-
sumption. We analyze the setting of (i) noisy tensor decomposition and (ii) random Gaussian design
in Section 3.2 and Section 3.3, respectively.

3.1 Main result
Our goal is to estimate an unknown rank (r1 , . . . , rK ) tensor W ∗ ∈ Rn1×···nK from observations
yi = 〈Xi , W ∗ 〉 + ϵi
(i = 1, . . . , M ).
(6)
Here the noise ϵi follows the independent zero-mean Gaussian distribution with variance σ2 .
ﬂﬂﬂﬂﬂﬂW ﬂﬂﬂﬂﬂﬂ
We employ the regularized empirical risk minimization problem proposed in [21, 10, 13, 23] for the
estimation of W as follows:
∥y − X(W )∥2
1
minimize
2 + λM
(7)
,
W ∈Rn1×···×nK
2M
S1
where y = (y1 , . . . , yM )⊤ is the collection of observations; X : Rn1×···×nK → RM is a linear
operator that maps W to the M dimensional output vector X(W ) = (〈X1 , W 〉 , . . . , 〈XM , W 〉) ⊤ ∈
RM . The Schatten 1-norm term penalizes every mode of W to be jointly low-rank (see Equation (1));
P
λM > 0 is the regularization constant. Accordingly, the solution of the minimization problem (7) is
typically a low-rank tensor when λM is sufﬁciently large. In addition, we denote the adjoint operator
∗ : RM → Rn1×···×nK ; that is X
i=1 ϵiXi ∈ Rn1×···×nK .
∗ (ϵ) =
M
of X as X
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
The ﬁrst step in our analysis is to characterize the particularity of the residual tensor ∆ := ˆW − W ∗
as in the following lemma.
Lemma 2. Let ˆW be the solution of the minimization problem (7) with λM ≥ 2
∗ (ϵ)
mean /M ,
and let ∆ := ˆW − W ∗ , where W ∗ is the true low-rank tensor. Let ∆(k) = ∆
′′
′
k + ∆
k be the
decomposition deﬁned in Equation (4). Then we have the following inequalities:
P
P
k ) ≤ 2rk for each k = 1, . . . , K .
′
1. rank(∆
∥∆
∥S1 .
∥∆
∥S1
≤ 3
′
′′
K
K
k
k
k=1
k=1
Proof. The proof uses the mode-k decomposability (5) and is analogous to that of Lemma 1 in
[17].

2.

The second ingredient of our analysis is the restricted strong convexity. Although, “strong” may
sound like a strong assumption, the point is that we require this assumption to hold only for the
particular residual tensor we characterized in Lemma 2. The assumption can be stated as follows.
ﬂﬂﬂﬂﬂﬂ2
ﬂﬂﬂﬂﬂﬂ∆
Assumption 1 (Restricted strong convexity). We suppose that there is a positive constant κ(X) such
that the operator X satisﬁes the inequality
P
≥κ(X)
∥X(∆)∥2
1
P
F ,
2
M
for all ∆ ∈ Rn1×···×nK such that for each k = 1, . . . , K , rank(∆
k ) ≤ 2rk and
∥∆
′
K
∥∆
∥S1 , where ∆
′
′
′′
k=1
K
3
k and ∆
k are deﬁned through the decomposition (4).
k
k=1
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
Now using the above two ingredients, we are ready to prove the following deterministic guarantee
on the performance of the estimation procedure (7).
Theorem 1. Let ˆW be the solution of the minimization problem (7) with λM ≥ 2
∗ (ϵ)
mean /M .
P
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ
Suppose that the operator X satisﬁes the restricted strong convexity condition. Then the following
bound is true:
√
K
≤ 32λM
k=1
κ(X)K

(8)
≤

(9)

′′
k

∥S1

rk

.

F

4

ﬂﬂﬂﬂﬂﬂW ∗ ﬂﬂﬂﬂﬂﬂ
− ﬂﬂﬂﬂﬂﬂ ˆW ﬂﬂﬂﬂﬂﬂ
≤ ﬂﬂﬂﬂﬂﬂ∆
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
Proof. Let ∆ = ˆW − W ∗ . Combining the fact that the objective value for ˆW is smaller than that for
W ∗ , the H ¨older-like inequality (3), the triangular inequality
, and
ﬂﬂﬂﬂﬂﬂ∆
ﬂﬂﬂﬂﬂﬂ∆
ﬂﬂﬂﬂﬂﬂ∆
≤ ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ
≤ λM /2, we obtain
S1
S1
S1
∗ (ϵ)/M
the assumption
mean
≤ 2λM
∥X(∆)∥2
1
∗ (ϵ)/M
2M
2
mean
S1
S1
S1
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ
Now the left-hand side can be lower-bounded using the restricted strong convexity (8). On the other
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ∆
P
P
P
hand, using Lemma 2, the right-hand side can be upper-bounded as follows:
ﬂﬂﬂﬂﬂﬂ∆
ﬂﬂﬂﬂﬂﬂ
√
∥S1 ) ≤ 4
∥S1
∥S1 + ∥∆
≤ 1
≤ 4
∥∆
k=1 (∥∆
′′
′
∆
K
K
K
2rk ,
(11)
F
k
k
k=1
k=1
K
K
K
S1
F = ∥∆(k) ∥F for k = 1, . . . , K . Combining in-
where the last inequality follows because
equalities (8), (10), and (11), we obtain our claim (9).
Negahban et al. [15] (see also [17]) pointed out that the key properties for establishing a sharp con-
vergence result for a regularized M -estimator is the decomposability of the regularizer and the re-
stricted strong convexity. What we have shown suggests that the weaker mode-k decomposability (5)
sufﬁce to obtain the above convergence result for the overlapped Schatten 1-norm (1) regularization.

+ λM

(10)

′
k

.

3.2 Noisy Tensor Decomposition

mean

.

(12)

nk +

In this subsection, we consider the setting where all the elements are observed (with noise) and the
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ
goal is to recover the underlying low-rank tensor without noise.
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
Since all the elements are observed only once, X is simply a vectorization (M = N ), and the left-
hand side of inequality (10) gives the quantity of interest ∥X(∆)∥2
2 =
F . Therefore, the
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
∗ (ϵ)
mean as in the following lemma.
remaining task is to bound
Lemma 3. Suppose that X : n1 × · · ·× nK → N is a vectorization of a tensor. With high probability
KX
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
¡√
¢
∗ (ϵ)
p
mean is concentrated around its mean, which can be bounded as follows:
the quantity
≤ σ
∗ (ϵ)
E
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂX
¯n\k
K
k=1
∗ (ϵ)
Setting the regularization constant as λM = c0E
mean /N , we obtain the following theorem.
P
p
Theorem 2. Suppose that X : n1 × · · · × nK → N is a vectorization of a tensor. There are universal
√
constants c0 and c1 , such that, with high probability, any solution of the minimization problem (7)
¢!2 ˆ
ˆ
!2
KX
KX
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
¡√
p
K
¯n\k )/(KN ) satisﬁes the following bound:
k=1 (
with regularization constant λM = c0σ
nk +
√
¯n\k
F
k=1
k=1
P
Proof. Combining Equations (10)–(11) with the fact that X is simply a vectorization and M = N ,
we have
√
√
∥ ˆW − W ∗ ∥F ≤ 16
K
2λM
1
rk .
k=1
K
N
Substituting the choice of regularization constant λM and squaring both sides, we obtain our claim.⁄
P
We can simplify the result of Theorem 2 by noting that ¯n\k = N/nk ≫ nk , when the dimen-
√
Introducing the notation ∥r∥1/2 = ( 1
−1 :=
(1/n1 , . . . , 1/nK ), we haveﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
K
rk )2 and n
sions are of the same order.
¡
¢
k=1
K
≤ Op
σ2 ∥n
−1 ∥1/2 ∥r∥1/2
(13)
F
.
N
We call the quantity ¯r = ∥n
−1 ∥1/2 ∥r∥1/2 the normalized rank, because ¯r = r/n when the dimen-
sions are balanced (nk = n and rk = r for all k = 1, . . . , K ).

≤ c1σ2

nk +

1
K

1
K

rk

.

5

3.3 Random Gaussian Design
In this subsection, we consider the case the elements of the input tensors Xi (i = 1, . . . , M ) in the
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
observation model (6) are distributed according to independent identical standard Gaussian distribu-
tions. We call this setting random Gaussian design.
∗ (ϵ)
First we show an upper bound on the norm
mean , which we use to specify the scaling of
the regularization constant λM in Theorem 1.
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
Lemma 4. Let X : Rn1×···×nK → RM be a random Gaussian design. In addition, we assume
that the noise ϵi is sampled independently from N (0, σ2 ). Then with high probability the quantity
KX
∗ (ϵ)
ﬂﬂﬂﬂﬂﬂX
ﬂﬂﬂﬂﬂﬂ
¡√
¢
p
mean is concentrated around its mean, which can be bounded as follows:
√
≤ σ
∗ (ϵ)
E
k=1
Next the following lemma, which is a generalization of a result presented in Negahban and Wain-
ˆr
! ﬂﬂﬂﬂﬂﬂ∆
wright [17, Proposition 1], provides a ground for the restricted strong convexity assumption (8).
r
KX
Lemma 5. Let X : Rn1×···×nK → RM be a random Gaussian design. Then it satisﬁes
ﬂﬂﬂﬂﬂﬂ∆
ﬂﬂﬂﬂﬂﬂ
ﬂﬂﬂﬂﬂﬂ
∥X(∆)∥2√
¯n\k
− 1
≥ 1
4
F
K
M
M
k=1
with probability at least 1 − 2 exp(−N/32).

M
K

nk +

nk
M

¯n\k

mean

.

+

,

S1

nk +

σ2 ( 1
K

Proof. The proof is analogous to that of Proposition 1 in [17] except that we use H ¨older-like in-
equality (3) for tensors instead of inequality (2) for matrices.
P
P
p
Finally, we obtain the following convergence bound.
P
p
Theorem 3. Under the random Gaussian design setup, there are universal constants c0 , c1 , and c2
√
√
such that for a sample size M ≥ c1 ( 1
K
K
√
¯n\k ))2 ( 1
k=1 (
nk +
rk )2 , any solution of the
√
k=1
K
K
P
P
p
K
¯n\k )/(K
k=1 (
nk +
minimization problem (7) with regularization constant λM = c0σ
M )
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
satisﬁes the following bound:
√
√
K
K
¯n\k ))2 ( 1
k=1 (
≤ c2
k=1
K
F
M
with high probability.
¶
(cid:181)
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
Again we can simplify the result of Theorem 3 as follows: for sample size M ≥ c1N ¯r we have
−1 ∥1/2 ∥r∥1/2
σ2 N ∥n
≤ Op
(14)
,
F
M
−1 ∥1/2∥r∥1/2 is the normalized rank. Note that the condition on the number of
where ¯r = ∥n
samples M does not depend on the noise variance σ2 . Therefore in the limit σ2 → 0, the bound (14)
is sufﬁciently small but only valid for sample size M that exceeds c1N ¯r , which implies a threshold
behavior as in Figure 1.
−1 ∥1/2 = O(n1 + n2 ). Therefore
Note also that in the matrix case (K = 2), r1 = r2 = r and N ∥n
we can restate the above result as for sample size M ≥ c1 r(n1 + n2 ), we have ∥ ˆW − W
∗ ∥2
≤
F
Op (r(n1 + n2 )/M ), which is compatible with the result in [17, 18].

rk )2

,

4 Experiments

In this section, we conduct two numerical experiments to conﬁrm our analysis in Section 3.2 and
Section 3.3.

6

(a) Small noise (σ = 0.01).
(b) Large noise (σ = 0.1).
Figure 2: Result of noisy tensor decomposition for tensors of size 50 × 50 × 20 and 100 × 100 × 50.

4.1 Noisy Tensor Decomposition

We randomly generated low-rank tensors of dimensions n(1) = (50, 50, 20) and n(2) =
(100, 100, 50) for various ranks (r1 , . . . , rK ). For a speciﬁc rank, we generated the true tensor
by drawing elements of the r1 × · · · × rK “core tensor” from the standard normal distribution and
multiplying its each mode by an orthonormal factor randomly drawn from the Haar measure. As
described in Section 3.2, the observation y consists of all the elements of the original tensor once
(M = N ) with additive independent Gaussian noise with variance σ2 . We used the alternating
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
direction method of multipliers (ADMM) for “constraint” approaches described in [23, 10] to solve
the minimization problem (7). The whole experiment was repeated 10 times and averaged.
The results are shown in Figure 2. The mean squared error
F /N is plotted against
the normalized rank ¯r = ∥n
−1 ∥1/2 ∥r∥1/2 (of the true tensor) deﬁned in Equation (13). Since the
choice of the regularization constant λM only depends on the size of the tensor and not on the ranks
of the underlying tensor in Theorem 2, we ﬁx the regularization constant to some different values
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ2
and report the dependency of the estimation error on the normalized rank ¯r of the true tensor.
Figure 2(a) shows the result for small noise (σ = 0.01) and Figure 2(b) shows the result for large
noise (σ = 0.1). As predicted by Theorem 2, the squared error
F grows linearly
against the normalized rank ¯r . This behaviour is consistently observed not only around the preferred
regularization constant value (triangles) but also in the over-ﬁtting case (circles) and the under-
ﬁtting case (crosses). Moreover, as predicted by Theorem 2, the preferred regularization constant
value scales linearly and the squared error scales quadratically to the noise standard deviation σ .
p
As predicted by Lemma 3, the curves for the smaller 50 × 50 × 20 tensor and those for the larger
100 × 100 × 50 tensor seem to agree when the regularization constant is scaled by the factor two.
¯n\k , which is roughly scaled by
Note that the dominant term in inequality (12) is the second term
the factor two from 50 × 50 × 20 to 100 × 100 × 50.

4.2 Tensor completion from partial observations

In this subsection, we repeat the simulation originally done by Tomioka et al. [23] and demonstrate
that our results in Section 3.3 can precisely predict the empirical scaling behaviour with respect to
both the size and rank of a tensor.
We present results for both matrix completion (K = 2) and tensor completion (K = 3). For
the matrix case, we randomly generated low-rank matrices of dimensions 50 × 20, 100 × 40, and
250 × 200. For the tensor case, we randomly generated low-rank tensors of dimensions 50 × 50 × 20
and 100 × 100 × 50. We generated the matrices or tensors as in the previous subsection for various
ranks. We randomly selected some elements of the true matrix/tensor for training and kept the

7

00.20.40.60.810123x 10−4Normalized rankMean squared error  size=[50 50 20] λM=0.03/Nsize=[50 50 20] λM=0.33/Nsize=[50 50 20] λM=0.54/Nsize=[100 100 50] λM=0.06/Nsize=[100 100 50] λM=0.69/Nsize=[100 100 50] λM=1.11/N00.20.40.60.8100.0050.010.0150.020.0250.03Normalized rankMean squared error  size=[50 50 20] λM=0.33/Nsize=[50 50 20] λM=2.34/Nsize=[50 50 20] λM=6/Nsize=[100 100 50] λM=0.66/Nsize=[100 100 50] λM=4.5/Nsize=[100 100 50] λM=12/N(a) Matrix completion (K = 2).

(b) Tensor completion (K = 3).

Figure 3: Scaling behaviour of matrix/tensor completion with respect to the size n and the rank r .

remaining elements for testing. No observation noise is added. We used the ADMM for “as a
matrix” and “constraint” approaches described in [23] to solve the minimization problem (7) for
matrix completion and tensor completion, respectively. Since there is no observation noise, we
chose the regularization constant λ → 0. A single experiment for a speciﬁc size and rank can be
ﬂﬂﬂﬂﬂﬂ ˆW − W ∗ ﬂﬂﬂﬂﬂﬂ
visualized as in Figure 1.
In Figure 3, we plot the minimum fraction of observations m = M /N that achieved error
F smaller than 0.01 against the normalized rank ¯r = ∥n
−1∥1/2 ∥r∥1/2 (of the true ten-
sor) deﬁned in Equation (13). The matrix case is plotted in Figure 3(a) and the tensor case is plotted
in Figure 3(b). Each series (blue crosses or red circles) corresponds to different matrix/tensor size
and each data-point corresponds to a different core size (rank). We can see that the fraction of obser-
vations m = M /N scales linearly against the normalized rank ¯r , which agrees with the condition
M /N ≥ c1 ∥n
−1 ∥1/2 ∥r∥1/2 = c1 ¯r in Theorem 3 (see Equation (14)). The agreement is especially
good for tensor completion (Figure 3(b)), where the two series almost overlap. Interestingly, we
can see that when compared at the same normalized rank, tensor completion is easier than matrix
completion. For example, when nk = 50 and rk = 10 for each k = 1, . . . , K , the normalized rank
is 0.2. From Figure 3, we can see that we only need to see 30% of the entries in the tensor case to
achieve error smaller than 0.01, whereas we need about 60% of the entries in the matrix case.

5 Conclusion

We have analyzed the statistical performance of a tensor decomposition algorithm based on the
overlapped Schatten 1-norm regularization (7). Numerical experiments show that our theory can
predict the empirical scaling behaviour well. The fraction of observation m = M /N at the threshold
predicted by our theory is proportional to the quantity we call the normalized rank, which reﬁnes
conjecture (sum of the mode-k ranks) in [23].
There are numerous directions that the current study can be extended. In this paper, we have focused
on the convergence of the estimation error; it would be meaningful to also analyze the condition for
the consistency of the estimated rank as in [2]. Second, although we have succeeded in predicting
the empirical scaling behaviour, the setting of random Gaussian design does not match the tensor
completion setting in Section 4.2. In order to analyze the latter setting, the notion of incoherence in
[5] or spikiness in [16] might be useful. This might also explain why tensor completion is easier than
matrix completion at the same normalized rank. Moreover, when the target tensor is only low-rank
in a certain mode, Schatten 1-norm regularization fails badly (as predicted by the high normalized
rank).
It would be desirable to analyze the “Mixture” approach that aims at this case [23].
In
a broader context, we believe that the current paper could serve as a basis for re-examining the
concept of tensor rank and low-rank approximation of tensors based on convex optimization.
Acknowledgments. We would like to thank Franz Kir ´aly and Hiroshi Kajino for their valuable
comments and discussions. This work was supported in part by MEXT KAKENHI 22700138,
23240019, 23120004, 22700289, and NTT Communication Science Laboratories.

8

00.10.20.30.40.50.600.20.40.60.81Normalized rank ||n−1||1/2||r||1/2Fraction at err<=0.01  size=[50 20]size=[100 40]size=[250 200]00.20.40.60.800.20.40.60.81Normalized rank ||n−1||1/2||r||1/2Fraction at Error<=0.01  size=[50 50 20]size=[100 100 50]References
[1] E. Acar and B. Yener. Unsupervised multiway data analysis: A literature survey. IEEE T. Knowl. Data.
En., 21(1):6–20, 2009.
[2] F.R. Bach. Consistency of trace norm minimization. J. Mach. Learn. Res., 9:1019–1048, 2008.
[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[4] R. Bro. PARAFAC. Tutorial and applications. Chemometr. Intell. Lab., 38(2):149–171, 1997.
[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,
9(6):717–772, 2009.
[6] J.D. Carroll and J.J. Chang. Analysis of individual differences in multidimensional scaling via an n-way
generalization of “Eckart-Young” decomposition. Psychometrika, 35(3):283–319, 1970.
[7] P. Comon. Tensor decompositions. In J. G. McWhirter and I. K. Proudler, editors, Mathematics in signal
processing V. Oxford University Press, 2002.
[8] L. De Lathauwer and J. Vandewalle. Dimensionality reduction in higher-order signal processing and
rank-(r1 , r2 , . . . , rn ) reduction in multilinear algebra. Linear Algebra Appl., 391:31–55, 2004.
In Algorithmic
[9] K. Fukumizu. Generalization error of linear neural networks in unidentiﬁable cases.
Learning Theory, pages 51–62. Springer, 1999.
[10] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex opti-
mization. Inverse Problems, 27:025010, 2011.
[11] J. H ˚astad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644–654, 1990.
[12] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455–500,
2009.
[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data.
In Prof. ICCV, 2009.
[14] M. Mørup. Applications of tensor (multiway array) factorizations and decompositions in data mining.
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24–40, 2011.
[15] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional
analysis of m-estimators with decomposable regularizers.
In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in NIPS 22, pages 1348–1356. 2009.
[16] S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal
bounds with noise. Technical report, arXiv:1009.2118, 2010.
[17] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and high-
dimensional scaling. Ann. Statist., 39(2), 2011.
[18] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via
nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.
[19] A. Rohde and A.B. Tsybakov. Estimation of high-dimensional low-rank matrices. Ann. Statist.,
39(2):887–930, 2011.
[20] N.D. Sidiropoulos, R. Bro, and G.B. Giannakis. Parallel factor analysis in sensor array processing. IEEE
T. Signal Proces., 48(8):2377–2388, 2000.
[21] M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex
multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.
[22] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Lawrence K.
Saul, Yair Weiss, and L ´eon Bottou, editors, Advances in NIPS 17, pages 1329–1336. MIT Press, Cam-
bridge, MA, 2005.
[23] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization.
Technical report, arXiv:1010.0789, 2011.
[24] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279–311,
1966.
[25] M. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. Computer
Vision—ECCV 2002, pages 447–460, 2002.
[26] H. Wang and N. Ahuja. Facial expression decomposition. In Proc. 9th ICCV, pages 958 – 965, 2003.

9


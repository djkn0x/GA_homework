Variance Penalizing AdaBoost

Pannagadatta K. Shivaswamy
Department of Computer Science
Cornell University, Ithaca NY
pannaga@cs.cornell.edu

Tony Jebara
Department of Compter Science
Columbia University, New York NY
jebara@cs.columbia.edu

Abstract

This paper proposes a novel boosting algorithm called VadaBoost which is mo-
tivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a
cost function that balances the sample mean and the sample variance of the expo-
nential loss. Each step of the proposed algorithm minimizes the cost efﬁciently
by providing weighted data to a weak learner rather than requiring a brute force
evaluation of all possible weak learners. Thus, the proposed algorithm solves a
key limitation of previous empirical Bernstein boosting methods which required
brute force enumeration of all possible weak learners. Experimental results con-
ﬁrm that the new algorithm achieves the performance improvements of EBBoost
yet goes beyond decision stumps to handle any weak learner. Signiﬁcant perfor-
mance gains are obtained over AdaBoost for arbitrary weak learners including
decision trees (CART).

1

Introduction

Many machine learning algorithms implement empirical risk minimization or a regularized variant
of it. For example, the popular AdaBoost [4] algorithm minimizes exponential loss on the training
examples. Similarly, the support vector machine [11] minimizes hinge loss on the training examples.
The convexity of these losses is helpful for computational as well as generalization reasons [2].
The goal of most learning problems, however, is not to obtain a function that performs well on
training data, but rather to estimate a function (using training data) that performs well on future
unseen test data. Therefore, empirical risk minimization on the training set is often performed
while regularizing the complexity of the function classes being explored. The rationale behind this
regularization approach is that it ensures that the empirical risk converges (uniformly) to the true
unknown risk. Various concentration inequalities formalize the rate of convergence in terms of the
function class complexity and the number of samples.
A key tool in obtaining such concentration inequalities is Hoeffding’s inequality which relates the
empirical mean of a bounded random variable to its true mean. Bernstein’s and Bennett’s inequalities
relate the true mean of a random variable to the empirical mean but also incorporate the true variance
of the random variable. If the true variance of a random variable is small, these bounds can be
signiﬁcantly tighter than Hoeffding’s bound. Recently, there have been empirical counterparts of
Bernstein’s inequality [1, 5]; these bounds incorporate the empirical variance of a random variable
rather than its true variance. The advantage of these bounds is that the quantities they involve are
empirical. Previously, these bounds have been applied in sampling procedures [6] and in multi-
armed bandit problems [1]. An alternative to empirical risk minimization, called sample variance
penalization [5], has been proposed and is motivated by empirical Bernstein bounds.
A new boosting algorithm is proposed in this paper which implements sample variance penalization.
The algorithm minimizes the empirical risk on the training set as well as the empirical variance. The
two quantities (the risk and the variance) are traded-off through a scalar parameter. Moreover, the

1

algorithm proposed in this article does not require exhaustive enumeration of the weak learners
(unlike an earlier algorithm by [10]).
i=1 is provided where Xi ∈ X and yi ∈ {±1} are drawn inde-
Assume that a training set (Xi , yi )n
pendently and identically distributed (iid) from a ﬁxed but unknown distribution D . The goal is to
learn a classiﬁer or a function f : X → {±1} that performs well on test examples drawn from the
same distribution D . In the rest of this article, G : X → {±1} denotes the so-called weak learner.
The notation Gs denotes the weak learner in a particular iteration s. Further, the two indices sets Is
and Js , respectively, denote examples that the weak learner Gs correctly classiﬁed and misclassiﬁed,
i.e., Is := {i|Gs (Xi ) = yi } and Js := {j |Gs (Xj ) (cid:54)= yj }.

i=1 , and weak learners H
Algorithm 1 AdaBoost Require: (Xi , yi )n
Initialize the weights: wi ← 1/n for i = 1, . . . , n; Initialize f to predict zero on all inputs.
(cid:16)(cid:80)
(cid:17)
for s ← 1 to S do
wi / (cid:80)
Estimate a weak learner Gs (·) from training examples weighted by (wi )n
i=1 .
αs = 1
2 log
wj
j :Gs (Xj )(cid:54)=yj
i:Gs (Xi )=yi
wi ← wi exp(−yiGs (Xi )αs )/Zs where Zs is such that (cid:80)n
if αs ≤ 0 then break end if
f (·) ← f (·) + αsGs (·)
i=1 wi = 1.
end for

i=1 , scalar parameter 1 ≥ λ ≥ 0, and weak learners H
Algorithm 2 VadaBoost Require: (Xi , yi )n
Initialize the weights: wi ← 1/n for i = 1, . . . , n; Initialize f to predict zero on all inputs.
for s ← 1 to S do
(cid:16)(cid:80)
(cid:17)
ui ← λnw2
i + (1 − λ)wi
ui / (cid:80)
Estimate a weak learner Gs (·) from training examples weighted by (ui )n
i=1 .
αs = 1
4 log
uj
j :Gs (Xj )(cid:54)=yj
i:Gs (Xi )=yi
wi ← wi exp(−yiGs (Xi )αs )/Zs where Zs is such that (cid:80)n
if αs ≤ 0 then break end if
f (·) ← f (·) + αsGs (·)
i=1 wi = 1.
end for

2 Algorithms

In this section, we brieﬂy discuss AdaBoost [4] and then propose a new algorithm called the Vad-
aBoost. The derivation of VadaBoost will be provided in detail in the next section.
AdaBoost iteratively builds (cid:80)S
AdaBoost (Algorithm 1) assigns a weight wi to each training example. In each step of the AdaBoost,
a weak learner Gs (·) is obtained on the weighted examples and a weight αs is assigned to it. Thus,
s=1 αsGs (·). If a training example is correctly classiﬁed, its weight
i=1 e−yi f (Xi ) (cid:1) by greedily constructing the function f (·) via (cid:80)S
tion: minf ∈F (cid:0) 1
(cid:80)n
is exponentially decreased; if it is misclassiﬁed, its weight is exponentially increased. The process
is repeated until a stopping criterion is met. AdaBoost essentially performs empirical risk minimiza-
s=1 αsGs (·).
n
Recently an alternative to empirical risk minimization has been proposed. This new criterion, known
(cid:115)
as the sample variance penalization [5] trades-off the empirical risk with the empirical variance:
n(cid:88)
ˆV[l(f (X ), y)]
1
arg min
f ∈F
n
n
i=1
where τ ≥ 0 explores the trade-off between the two quantities. The motivation for sample variance
penalization comes from the following theorem [5]:

l(f (Xi ), yi ) + τ

,

(1)

2

i=1 be drawn iid from a distribution D . Let F be a class of functions
Theorem 1 Let (Xi , yi )n
(cid:115)
f : X → R. Then, for a loss l : R × Y → [0, 1], for any δ > 0, w.p. at least 1 − δ, ∀f ∈ F
n(cid:88)
18 ˆV[l(f (X ), y)] ln(M(n)/δ)
15 ln(M(n)/δ)
E[l(f (X ), y)] ≤ 1
(n − 1)
n
n
i=1
where M(n) is a complexity measure.

l(f (Xi ), yi ) +

(2)

+

,

From the above uniform convergence result, it can be argued that future loss can be minimized by
minimizing the right hand side of the bound on training examples. Since the variance ˆV[l(f (X ), y)]
has a multiplicative factor involving M(n), δ and n, for a given problem, it is difﬁcult to specify the
relative importance between empirical risk and empirical variance a priori. Hence, sample variance
penalization (1) necessarily involves a trade-off parameter τ .
Empirical risk minimization or sample variance penalization on the 0 − 1 loss is a hard problem;
this problem is often circumvented by minimizing a convex upper bound on the 0 − 1 loss. In this
paper, we consider the exponential loss l(f (X ), y) := e−yf (X ) . With the above loss, it was shown
n
(cid:33)2 .
(cid:32) n(cid:88)
(cid:33)2
(cid:32) n(cid:88)
by [10] that sample variance penalization is equivalent to minimizing the following cost,
n(cid:88)
i=1
i=1
i=1
Theorem 1 requires that the loss function be bounded. Even though the exponential loss is un-
bounded, boosting is typically performed only for a ﬁnite number of iterations in most practical
applications. Moreover, since weak learners typically perform only slightly better than random
guessing, each αs in AdaBoost (or in VadaBoost) is typically small thus limiting the range of the
function learned. Furthermore, experiments will conﬁrm that sample variance penalization results
in a signiﬁcant empirical performance improvement over empirical risk minimization.
Our proposed algorithm is called VadaBoost1 and is described in Algorithm 2. VadaBoost iteratively
performs sample variance penalization (i.e., it minimizes the cost (3) iteratively). Clearly, VadaBoost
shares the simplicity and ease of implementation found in AdaBoost.

e−2yi f (Xi ) −

e−yi f (Xi )

e−yi f (Xi )

+ λ

(3)

wi e−α +

3 Derivation of VadaBoost
(cid:80)s
(cid:80)s−1
didate weak learner Gs (·), the cost (3) for the function (cid:80)s−1
In the sth iteration, our objective is to choose a weak learner Gs and a weight αs such that
t=1 αtGt (·) reduces the cost (3). Denote by wi the quantity e−yi
t=1 αtGt (xi ) /Zs . Given a can-
t=1 αtGt (·) + αGs (·) can be expressed as
a function of α:
n
2 .
(cid:88)
2
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
V (α; w, λ, I , J ) :=
wj eα
+λ
j∈J
i∈I
j∈J
i∈I
j∈J
i∈I
up to a multiplicative factor. In the quantity above, I and J are the two index sets (of correctly
classiﬁed and incorrectly classiﬁed examples) over Gs . Let the vector w whose ith component is
wi denote the current set of weights on the training examples. Here, we have dropped the sub-
scripts/superscripts s for brevity.
(cid:1) e2α .
(cid:88)
(cid:32)(cid:88)
(cid:1)(cid:33)
Lemma 2 The update of αs in Algorithm 2 minimizes the cost
(cid:0)λnw2
i + (1 − λ)wi
i∈I
j∈J
1The V in VadaBoost emphasizes the fact that Algorithm 2 penalizes the empirical variance.

(cid:0)λnw2
j + (1 − λ)wj

i e−2α + n
w2

U (α; w, λ, I , J ) :=

j e2α −
w2

wi e−α +

e−2α +

wj eα

(4)

(5)

3

2
wj eα




wi e−α +

(cid:88)
j∈J
(cid:33)(cid:88)
j∈J

Proof By obtaining the second derivative of the above expression (with respect to α), it is easy to
see that it is convex in α. Thus, setting the derivative with respect to α to zero gives the optimal
choice of α as shown in Algorithm 2.
Theorem 3 Assume that 0 ≤ λ ≤ 1 and (cid:80)n
i=1 wi = 1 (i.e. normalized weights). Then,
V (α; w, λ, I , J ) ≤ U (α; w, λ, I , J ) and V (0; w, λ, I , J ) = U (0; w, λ, I , J ). That is, U is an upper
bound on V and the bound is exact at α = 0.
n
(cid:88)
2
(cid:88)
Proof Denoting 1 − λ by ¯λ, we have:
(cid:88)
(cid:88)
(cid:88)
j e2α −
i e−2α + n
wi e−α +
w2
w2
wj eα
V (α; w, λ, I , J ) =
+ λ

(cid:88)
n
2
j∈J
i∈I
i∈I
j∈J
i∈I
(cid:88)
(cid:88)
(cid:88)
i e−2α + n
wi e−α +
(cid:32)(cid:88)
w2
j e2α
w2
wj eα
n
+ ¯λ
2
(cid:88)
+ λ
(cid:33)2
(cid:32)(cid:88)
j∈J
i∈I
i∈I
j∈J
(cid:88)
(cid:88)
i e−2α + n
e−2α +
w2
j e2α
w2
e2α + 2
n
(cid:32)(cid:88)
 + ¯λ
(cid:33) 1 − (cid:88)
 e−2α
wi
wj
i∈I
j∈J
i∈I
j∈J
i∈I
(cid:88)
(cid:88)
i e−2α + n
(cid:88)
 (cid:32)
 + 2¯λ
(cid:33) (cid:88)

w2
w2
j e2α
(cid:33)
(cid:32)(cid:88)
wi
wj
1 − (cid:88)
i∈I
j∈J
i∈I
j∈J
(cid:1) e2α
(cid:88)
e2α
(cid:32)(cid:88)
(cid:1)(cid:33)
+
wi
wi
(cid:0)λnw2
(cid:0)λnw2
j∈J
i∈I
i∈I
j∈J
e−2α +
(cid:33) (cid:88)
 (cid:0)−e2α − e−2α + 2(cid:1)
i + ¯λwi
j + ¯λwj
(cid:32)(cid:88)
i∈I
j∈J
(cid:88)
(cid:1) e2α = U (α; w, λ, I , J ).
+ ¯λ
(cid:1)(cid:33)
(cid:32)(cid:88)
(cid:0)λnw2
(cid:0)λnw2
j∈J
i∈I
≤
e−2α +
j + ¯λwj
i + ¯λwi
i∈I
j∈J
expanded. On the next line, we used the fact that (cid:80)
i∈I wi + (cid:80)
j∈J = (cid:80)n
On line two, terms were simply regrouped. On line three, the square term from line two was
i=1 wi = 1. On the ﬁfth
line, we once again regrouped terms; the last term in this expression (which is e2α + e−2α − 2)
can be written as (eα − e−α )2 . When α = 0 this term vanishes. Hence the bound is exact at α = 0.

wi

wj

wi

wj

wj

wj

= ¯λ

= λ

= λ

=

Corollary 4 VadaBoost monotonically decreases the cost (3).

The above corollary follows from:
V (αs ; w, λ, I , J ) ≤ U (αs ; w, λ, I , J ) < U (0; w, λ, I , J ) = V (0; w, λ, I , J ).
In the above, the ﬁrst inequality follows from Theorem (3). The second strict inequality holds
because αs is a minimizer of U from Lemma (2); it is not hard to show that U (αs ; w, λ, I , J ) is
cost (3) at (cid:80)s−1
strictly less than U (0; w, λ, I , J ) from the termination criterion of VadaBoost. The third equality
again follows from Theorem (3). Finally, we notice that V (0; w, λ, I , J ) merely corresponds to the
t=1 αtGt (·). Thus, we have shown that taking a step αs decreases the cost (3).

4

Figure 1: Typical Upper bound U (α; w, λ, I , J ) and the actual cost function V (α; w, λ, I , J ) values
under varying α. The bound is exact at α = 0. The bound gets closer to the actual function value as
λ grows. The left plot shows the bound for λ = 0 and the right plot shows it for λ = 0.9

We point out that we use a different upper bound in each iteration since V and U are parameterized
by the current weights in the VadaBoost algorithm. Also note that our upper bound holds only for
0 ≤ λ ≤ 1. Although the choice 0 ≤ λ ≤ 1 seems restrictive, intuitively, it is natural to have a
term is multiplied by (cid:112)1/n while the empirical mean is multiplied by one. Thus, for large values of
higher penalization on the empirical mean rather than the empirical variance during minimization.
Also, a closer look at the empirical Bernstein inequality in [5] shows that the empirical variance
n, the weight on the sample variance is small. Furthermore, our experiments suggest that restricting
λ to this range does not signiﬁcantly change the results.

4 How good is the upper bound?

First, we observe that our upper bound is exact when λ = 1. Also, our upper bound is loosest for
the case λ = 0. We visualize the upper bound and the true cost for two settings of λ in Figure 1.
Since the cost (4) is minimized via an upper bound (5), a natural question is: how good is this ap-
proximation? We evaluate the tightness of this upper bound by considering its impact on learning
efﬁciency. As is clear from ﬁgure (1), when λ = 1, the upper bound is exact and incurs no inefﬁ-
ciency. In the other extreme when λ = 0, the cost of VadaBoost coincides with AdaBoost and the
bound is effectively at its loosest. Even in this extreme case, VadaBoost derived through an upper
bound only requires at most twice the number of iterations as AdaBoost to achieve a particular cost.
The following theorem shows that our algorithm remains efﬁcient even in this worst-case scenario.

Theorem 5 Let OA denote the squared cost obtained by AdaBoost after S iterations. For weak
learners in any iteration achieving a ﬁxed error rate  < 0.5, VadaBoost with the setting λ = 0
attains a cost at least as low as OA in no more than 2S iterations.
classiﬁer is s = (cid:80)
Proof Denote the weight on the example i in sth iteration by ws
i . The weighted error rate of the sth
(cid:80)S
j . We have, for both algorithms,
ws
j∈Js
n (cid:81)S
exp(−yi
i exp(−yiαS GS (Xi ))
s=1 αsGs (Xi ))
wS
wS+1
=
i
Zs
s=1 Zs
(cid:88)
(cid:88)
i e−αs = 2(cid:112)s (1 − s ).
The value of the normalization factor in the case of AdaBoost is
ws
Z a
ws
j eαs +
s =
j∈js
i∈Is
(cid:88)
(cid:88)
Similarly, the value of the normalization factor for VadaBoost is given by
√
i e−αs = ((s )(1 − s ))
ws
j∈Js
i∈Is

1 − s ).

ws
j eαs +

1
4 (

s +

Z v
s =

(6)

(7)

(8)

=

.

√

5

00.30.60.912αCost  Actual Cost:VUpper Bound:U00.30.60.9123αCost  Actual Cost:VUpper Bound:U(cid:33)2
(cid:32) S(cid:89)
(cid:32)
(cid:33)2
(cid:32) n(cid:88)
(cid:33)2
S(cid:89)
n(cid:88)
S(cid:88)
S(cid:89)
The squared cost function of AdaBoost after S steps is given by
4s (1 − s ).
exp(−yi
We used (6), (7) and the fact that (cid:80)n
ws+1
= n2
Z a
Z a
αs yiGs (X ))
= n2
OA =
=
n
s
s
i
s=1
s=1
s=1
s=1
i=1
i=1
i=1 wS+1
= 1 to derive the above expression. Similarly, for
(cid:32) n(cid:88)
(cid:32)
(cid:33)2
(cid:33)2
(cid:32) S(cid:89)
(cid:33)2
i
S(cid:88)
S(cid:89)
n(cid:88)
λ = 0 the cost of VadaBoost satisﬁes2
exp(−yi
αs yiGs (X ))
=
S(cid:89)
(2s (1 − s ) + (cid:112)s (1 − s )).
s=1
s=1
s=1
i=1
i=1
s=1
Now, suppose that s =  for all s. Then, the squared cost achieved by AdaBoost is given by
n2 (4(1 − ))S . To achieve the same cost value, VadaBoost, with weak learners with the same
√
log(4(1−))
error rate needs at most S
times. Within the range of interest for , the term
log(2(1−)+
(1−))
multiplying S above is at most 2.

ws+1
i

OV =

n

Z a
s

= n2

= n2

Z v
s

Although the above worse-case bound achieves a factor of two, for  > 0.4, VadaBoost requires
only about 33% more iterations than AdaBoost. To summarize, even in the worst possible scenario
where λ = 0 (when the variational bound is at its loosest), the VadaBoost algorithm takes no more
than double (a small constant factor) the number of iterations of AdaBoost to achieve the same cost.
i=1 , scalar parameter λ ≥ 0, and weak learners H
Algorithm 3 EBBoost:
Require: (Xi , yi )n
Initialize the weights: wi ← 1/n for i = 1, . . . , n; Initialize f to predict zero on all inputs.
(cid:19)
(cid:18) (1−λ)((cid:80)
for s ← 1 to S do
wi )2+λn (cid:80)
Get a weak learner Gs (·) that minimizes (3) with the following choice of αs :
wi )2+λn (cid:80)
(1−λ)((cid:80)
w2
i∈Is
i∈Is
αs = 1
i
4 log
w2
i∈Js
i∈Js
wi ← wi exp(−yiGs (Xi )αs )/Zs where Zs is such that (cid:80)n
i
if αs < 0 then break end if
f (·) ← f (·) + αsGs (·)
i=1 wi = 1.
end for

5 A limitation of the EBBoost algorithm

A sample variance penalization algorithm known as EBBoost was previously explored [10]. While
this algorithm was simple to implement and showed signiﬁcant improvements over AdaBoost, it
suffers from a severe limitation:
it requires enumeration and evaluation of every possible weak
learner per iteration. Recall the steps implementing EBBoost in Algorithm 3. An implementation
of EBBoost requires exhaustive enumeration of weak learners in search of the one that minimizes
cost (3). It is preferable, instead, to ﬁnd the best weak learner by providing weights on the training
i + (cid:0)(cid:80)
(cid:1)2 and the weight on correctly classi-
on all the misclassiﬁed examples is (cid:80)
examples and efﬁciently computing the rule whose performance on that weighted set of examples
i + (cid:0)(cid:80)
(cid:1)2 ; these aggregate weights on misclassiﬁed examples and
is guaranteed to be better than random guessing. However, with the EBBoost algorithm, the weight
ﬁed examples is (cid:80)
w2
wi
i∈Js
i∈Js
w2
wi
i∈Is
i∈Is
correctly classiﬁed examples do not translate into weights on the individual examples. Thus, it be-
comes necessary to exhaustively enumerate weak learners in Algorithm 3. While enumeration of
weak learners is possible in the case of decision stumps, it poses serious difﬁculties in the case of
weak learners such as decision trees, ridge regression, etc. Thus, VadaBoost is the more versatile
boosting algorithm for sample variance penalization.

2The cost which VadaBoost minimizes at λ = 0 is the squared cost of AdaBoost, we do not square it again.

6

Table 1: Mean and standard errors with decision stump as the weak learner.
RQP-Boost
RLP-Boost
VadaBoost
AdaBoost
Dataset
EBBoost
16.04 ± 0.1
16.21 ± 0.1
16.22 ± 0.1
16.05 ± 0.1
16.15 ± 0.1
a5a
21.64 ± 0.2
21.52 ± 0.2
21.63 ± 0.2
22.29 ± 0.2
21.79 ± 0.2
abalone
3.37 ± 0.1
3.14 ± 0.1
3.14 ± 0.1
3.18 ± 0.1
3.09 ± 0.1
image
0.02 ± 0.0
0.02 ± 0.0
0.01 ± 0.0
0.01 ± 0.0
0.00 ± 0.0
mushrooms
3.41 ± 0.1
3.60 ± 0.1
3.59 ± 0.1
3.51 ± 0.1
3.84 ± 0.1
musk
0.89 ± 0.0
0.85 ± 0.0
0.84 ± 0.0
0.98 ± 0.0
0.88 ± 0.0
mnist09
0.64 ± 0.0
0.58 ± 0.0
0.60 ± 0.0
0.68 ± 0.0
0.63 ± 0.0
mnist14
2.11 ± 0.1
1.86 ± 0.1
2.01 ± 0.1
2.06 ± 0.1
1.95 ± 0.1
mnist27
4.25 ± 0.1
4.51 ± 0.1
4.32 ± 0.1
4.45 ± 0.1
4.12 ± 0.1
mnist38
2.72 ± 0.1
2.77 ± 0.1
2.62 ± 0.1
2.56 ± 0.1
2.79 ± 0.1
mnist56
11.74 ± 0.6
13.16 ± 0.6
12.46 ± 0.6
13.02 ± 0.6
12.86 ± 0.6
ringnorm
5.90 ± 0.1
5.64 ± 0.1
5.78 ± 0.1
5.81 ± 0.1
5.75 ± 0.1
spambase
8.83 ± 0.2
8.33 ± 0.1
8.48 ± 0.1
8.55 ± 0.2
8.47 ± 0.1
splice
3.07 ± 0.1
3.29 ± 0.1
3.09 ± 0.1
3.16 ± 0.1
2.98 ± 0.1
twonorm
2.60 ± 0.1
2.38 ± 0.1
2.50 ± 0.1
2.44 ± 0.1
2.36 ± 0.1
w4a
10.96 ± 0.1
10.99 ± 0.1
10.75 ± 0.1
10.95 ± 0.1
10.60 ± 0.1
waveform
23.62 ± 0.2
23.52 ± 0.2
23.41 ± 0.1
24.16 ± 0.1
23.61 ± 0.1
wine
4.72 ± 0.3
4.96 ± 0.3
5.00 ± 0.2
4.38 ± 0.2
5.32 ± 0.3
wisc

Table 2: Mean and standard errors with CART as the weak learner.
Dataset
AdaBoost
VadaBoost
RLP-Boost
RQP-Boost
17.59 ± 0.2
17.16 ± 0.1
18.24 ± 0.1
17.99 ± 0.1
a5a
21.84 ± 0.2
22.16 ± 0.2
21.30 ± 0.2
21.87 ± 0.2
abalone
1.93 ± 0.1
1.98 ± 0.1
1.99 ± 0.1
1.95 ± 0.1
image
0.01 ± 0.0
0.01 ± 0.0
0.02 ± 0.0
0.01 ± 0.0
mushrooms
2.36 ± 0.1
2.07 ± 0.1
2.40 ± 0.1
2.29 ± 0.1
musk
0.71 ± 0.0
0.76 ± 0.0
0.72 ± 0.0
0.73 ± 0.0
mnist09
0.52 ± 0.0
0.55 ± 0.0
0.50 ± 0.0
0.52 ± 0.0
mnist14
1.31 ± 0.0
1.24 ± 0.0
1.32 ± 0.0
1.29 ± 0.0
mnist27
1.89 ± 0.1
1.72 ± 0.1
1.88 ± 0.1
1.87 ± 0.1
mnist38
1.23 ± 0.1
1.17 ± 0.0
1.20 ± 0.0
1.19 ± 0.1
mnist56
7.84 ± 0.4
8.60 ± 0.5
7.78 ± 0.4
7.94 ± 0.4
ringnorm
6.03 ± 0.1
6.25 ± 0.1
5.76 ± 0.1
6.14 ± 0.1
spambase
4.02 ± 0.1
3.67 ± 0.1
4.03 ± 0.1
3.97 ± 0.1
splice
3.40 ± 0.1
3.27 ± 0.1
3.50 ± 0.1
3.38 ± 0.1
twonorm
2.90 ± 0.1
2.90 ± 0.1
2.90 ± 0.1
2.90 ± 0.1
w4a
10.82 ± 0.1
11.11 ± 0.1
10.59 ± 0.1
11.09 ± 0.1
waveform
21.94 ± 0.2
21.18 ± 0.2
22.44 ± 0.2
22.18 ± 0.2
wine
4.61 ± 0.2
4.18 ± 0.2
4.63 ± 0.2
4.37 ± 0.2
wisc

6 Experiments

In this section, we evaluate the empirical performance of the VadaBoost algorithm with respect to
several other algorithms. The primary purpose of our experiments is to compare sample variance
penalization versus empirical risk minimization and to show that we can efﬁciently perform sample
variance penalization for weak learners beyond decision stumps. We compared VadaBoost against
EBBoost, AdaBoost, regularized LP and QP boost algorithms [7]. All the algorithms except Ad-
aBoost have one extra parameter to tune.
Experiments were performed on benchmark datasets that have been previously used in [10]. These
datasets include a variety of tasks including all digits from the MNIST dataset. Each dataset was
divided into three parts: 50% for training, 25% for validation and 25% for test. The total number
of examples was restricted to 5000 in the case of MNIST and musk datasets due to computational
restrictions of solving LP/QP.
The ﬁrst set of experiments use decision stumps as the weak learners. The second set of experiments
used Classiﬁcation and Regression Trees or CART [3] as weak learners. A standard MATLAB
implementation of CART was used without modiﬁcation. For all the datasets, in both experiments,

7

AdaBoost, VadaBoost and EBBoost (in the case of stumps) were run until there was no drop in the
error rate on the validation for the next 100 consecutive iterations. The values of the parameters for
VadaBoost and EBBoost were chosen to minimize the validation error upon termination. RLP-Boost
and RQP-Boost were given the predictions obtained by AdaBoost. Their regularization parameter
was also chosen to minimize the error rate on the validation set. Once the parameter values were
ﬁxed via the validation set, we noted the test set error corresponding to that parameter value. The
entire experiment was repeated 50 times by randomly selecting train, test and validation sets. The
numbers reported here are average from these runs.
The results for the decision stump and CART experiments are reported in Tables 1 and 2. For each
dataset, the algorithm with the best percentage test error is represented by a dark shaded cell. All
lightly shaded entries in a row denote results that are not signiﬁcantly different from the minimum
error (according to a paired t-test at a 1% signiﬁcance level). With decision stumps, both EBBoost
and VadaBoost have comparable performance and signiﬁcantly outperform AdaBoost. With CART
as the weak learner, VadaBoost is once again signiﬁcantly better than AdaBoost.
We gave a guarantee on the number of itera-
tions required in the worst case for Vadaboost
(which approximately matches the AdaBoost
cost (squared) in Theorem 5). An assumption
in that theorem was that the error rate of each
weak learner was ﬁxed. However, in practice,
the error rates of the weak learners are not con-
stant over the iterations. To see this behavior
in practice, we have shown the results with the
MNIST 3 versus 8 classiﬁcation experiment. In
ﬁgure 2 we show the cost (plus 1) for each al-
gorithm (the AdaBoost cost has been squared)
versus the number of iterations using a loga-
rithmic scale on the Y-axis. Since at λ = 0,
EBBoost reduces to AdaBoost, we omit its plot
at that setting. From the ﬁgure, it can be seen
that the number of iterations required by VadaBoost is roughly twice the number of iterations re-
quired by AdaBoost. At λ = 0.5, there is only a minor difference in the number of iterations
required by EBBoost and VadaBoost.

Figure 2: 1+ cost vs the number of iterations.

7 Conclusions

This paper identiﬁed a key weakness in the EBBoost algorithm and proposed a novel algorithm
that efﬁciently overcomes the limitation to enumerable weak learners. VadaBoost reduces a well
motivated cost by iteratively minimizing an upper bound which, unlike EBBoost, allows the boosting
method to handle any weak learner by estimating weights on the data. The update rule of VadaBoost
has a simplicity that is reminiscent of AdaBoost. Furthermore, despite the use of an upper bound,
the novel boosting method remains efﬁcient. Even when the bound is at its loosest, the number
of iterations required by VadaBoost is a small constant factor more than the number of iterations
required by AdaBoost. Experimental results showed that VadaBoost outperforms AdaBoost in terms
of classiﬁcation accuracy and efﬁciently applying to any family of weak learners. The effectiveness
of boosting has been explained via margin theory [9] though it has taken a number of years to settle
certain open questions [8]. Considering the simplicity and effectiveness of VadaBoost, one natural
future research direction is to study the margin distributions it obtains. Another future research
direction is to design efﬁcient sample variance penalization algorithms for other problems such as
multi-class classiﬁcation, ranking, and so on.

Acknowledgements This material is based upon work supported by the National Science Founda-
tion under Grant No. 1117631, by a Google Research Award, and by the Department of Homeland
Security under Grant No. N66001-09-C-0080.

8

100200300400500600700100101102103104105Iterationcost + 1  AdaBoostEBBoost λ=0.5VadaBoost λ=0VadaBoost λ=0.5References
[1] J-Y. Audibert, R. Munos, and C. Szepesv ´ari. Tuning bandit algorithms in stochastic environ-
ments. In ALT, 2007.
[2] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, 2006.
[3] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classiﬁcation and Regression Trees.
Chapman and Hall, New York, 1984.
[4] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.
[5] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In
COLT, 2009.
[6] V. Mnih, C. Szepesv ´ari, and J-Y. Audibert. Empirical Bernstein stopping. In COLT, 2008.
[7] G. Raetsch, T. Onoda, and K.-R. Muller. Soft margins for AdaBoost. Machine Learning,
43:287–320, 2001.
[8] L. Reyzin and R. Schapire. How boosting the margin can also boost classiﬁer complexity. In
ICML, 2006.
[9] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee. Boosting the margin: a new explanation
for the effectiveness of voting methods. Annals of Statistics, 26(5):1651–1686, 1998.
[10] P. K. Shivaswamy and T. Jebara. Empirical Bernstein boosting. In AISTATS, 2010.
[11] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, NY, 1995.

9


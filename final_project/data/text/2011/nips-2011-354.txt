Speedy Q-Learning

Mohammad Gheshlaghi Azar
Radboud University Nijmegen
Geert Grooteplein 21N, 6525 EZ
Nijmegen, Netherlands
m.azar@science.ru.nl

Mohammad Ghavamzadeh
INRIA Lille, SequeL Project
40 avenue Halley
59650 Villeneuve d’Ascq, France
m.ghavamzadeh@inria.fr

Remi Munos
INRIA Lille, SequeL Project
40 avenue Halley
59650 Villeneuve d’Ascq, France
r.munos@inria.fr

Hilbert J. Kappen
Radboud University Nijmegen
Geert Grooteplein 21N, 6525 EZ
Nijmegen, Netherlands
b.kappen@science.ru.nl

Abstract

We introduce a new convergent variant of Q-learning, called speedy Q-learning
(SQL), to address the problem of slow convergence in the standard form of the
Q-learning algorithm. We prove a PAC bound on the performance of SQL, which
shows that for an MDP with n state-action pairs and the discount factor γ only T =
O(cid:0) log(n)/(ǫ2 (1 − γ )4 )(cid:1) steps are required for the SQL algorithm to converge to
an ǫ-optimal action-value function with high probability. This bound has a better
dependency on 1/ǫ and 1/(1 − γ ), and thus, is tighter than the best available result
for Q-learning. Our bound is also superior to the existing results for both model-
free and model-based instances of batch Q-value iteration that are considered to
be more efﬁcient than the incremental methods like Q-learni ng.

1

Introduction

Q-learning [20] is a well-known model-free reinforcement learning (RL) algorithm that ﬁnds an
estimate of the optimal action-value function. Q-learning is a combination of dynamic programming,
more speci ﬁcally the value iteration algorithm, and stocha stic approximation. In ﬁnite state-action
problems, it has been shown that Q-learning converges to the optimal action-value function [5, 10].
However, it suffers from slow convergence, especially when the discount factor γ is close to one [8,
17]. The main reason for the slow convergence of Q-learning is the combination of the sample-based
stochastic approximation (that makes use of a decaying learning rate) and the fact that the Bellman
operator propagates information throughout the whole space (specially when γ is close to 1).
In this paper, we focus on RL problems that are formulated as ﬁ nite state-action discounted inﬁnite
horizon Markov decision processes (MDPs), and propose an algorithm, called speedy Q-learning
(SQL), that addresses the problem of slow convergence of Q-learning. At each time step, SQL uses
two successive estimates of the action-value function that makes its space complexity twice as the
standard Q-learning. However, this allows SQL to use a more aggressive learning rate for one of
the terms in its update rule and eventually achieves a faster convergence rate than the standard Q-
learning (see Section 3.1 for a more detailed discussion). We prove a PAC bound on the performance
of SQL, which shows that only T = O(cid:0) log(n)/((1 − γ )4 ǫ2 )(cid:1) number of samples are required for
SQL in order to guarantee an ǫ-optimal action-value function with high probability. This is superior
to the best result for the standard Q-learning by [8], both in terms of 1/ǫ and 1/(1 − γ ). The rate
for SQL is even better than that for the Phased Q-learning algorithm, a model-free batch Q-value

1

iteration algorithm proposed and analyzed by [12]. In addition, SQL’s rate is slightly better than
the rate of the model-based batch Q-value iteration algorithm in [12] and has a better computational
and memory requirement (computational and space complexity), see Section 3.3.2 for more detailed
comparisons. Similar to Q-learning, SQL may be implemented in synchronous and asynchronous
fashions. For the sake of simplicity in the analysis, we only report and analyze its synchronous
version in this paper. However, it can easily be implemented in an asynchronous fashion and our
theoretical results can also be extended to this setting by following the same path as [8].

The idea of using previous estimates of the action-values has already been used to improve the per-
formance of Q-learning. A popular algorithm of this kind is Q(λ) [14, 20], which incorporates the
concept of eligibility traces in Q-learning, and has been empirically shown to have a better perfor-
mance than Q-learning, i.e., Q(0), for suitable values of λ. Another recent work in this direction
is Double Q-learning [19], which uses two estimators for the action-value function to alleviate the
over-estimation of action-values in Q-learning. This over-estimation is caused by a positive bias in-
troduced by using the maximum action value as an approximation for the expected action value [19].

The rest of the paper is organized as follows. After introducing the notations used in the paper
in Section 2, we present our Speedy Q-learning algorithm in Section 3. We ﬁrst describe the al-
gorithm in Section 3.1, then state our main theoretical result, i.e., a high-probability bound on the
performance of SQL, in Section 3.2, and ﬁnally compare our bo und with the previous results on
Q-learning in Section 3.3. Section 4 contains the detailed proof of the performance bound of the
SQL algorithm. Finally, we conclude the paper and discuss some future directions in Section 5.

2 Preliminaries

In this section, we introduce some concepts and deﬁnitions f rom the theory of Markov decision
processes (MDPs) that are used throughout the paper. We start by the deﬁnition of supremum norm.
For a real-valued function g : Y 7→ R, where Y is a ﬁnite set, the supremum norm of g is deﬁned as
kgk , maxy∈Y |g(y)|.
We consider the standard reinforcement learning (RL) framework [5, 16] in which a learning agent
interacts with a stochastic environment and this interaction is modeled as a discrete-time discounted
MDP. A discounted MDP is a quintuple (X, A, P , R, γ ), where X and A are the set of states and
actions, P is the state transition distribution, R is the reward function, and γ ∈ (0, 1) is a discount
factor. We denote by P (·|x, a) and r(x, a) the probability distribution over the next state and the
immediate reward of taking action a at state x, respectively. To keep the representation succinct, we
use Z for the joint state-action space X × A.
Assumption 1 (MDP Regularity). We assume Z and, subsequently, X and A are ﬁnite sets with
cardinalities n, |X| and |A|, respectively. We also assume that the immediate rewards r(x, a) are
uniformly bounded by Rmax and deﬁne the horizon of the MDP β , 1/(1 − γ ) and Vmax , βRmax .

A stationary Markov policy π(·|x) is the distribution over the control actions given the current
state x. It is deterministic if this distribution concentrates over a single action. The value and the
action-value functions of a policy π , denoted respectively by V π : X 7→ R and Qπ : Z 7→ R,
are deﬁned as the expected sum of discounted rewards that are
encountered when the policy π
is executed. Given a MDP, the goal is to ﬁnd a policy that attai ns the best possible values,
V ∗ (x) , supπ V π (x), ∀x ∈ X. Function V ∗ is called the optimal value function. Similarly
the optimal action-value function is deﬁned as Q∗ (x, a) = supπ Qπ (x, a), ∀(x, a) ∈ Z. The opti-
mal action-value function Q∗ is the unique ﬁxed-point of the Bellman optimality operator T deﬁned
as (TQ)(x, a) , r(x, a) + γ Py∈X P (y |x, a) maxb∈A Q(y , b), ∀(x, a) ∈ Z. It is important to note
that T is a contraction with factor γ , i.e., for any pair of action-value functions Q and Q′ , we have
kTQ − TQ′k ≤ γ kQ − Q′ k [4, Chap. 1]. Finally for the sake of readability, we deﬁne th e max
operator M over action-value functions as (MQ)(x) = maxa∈A Q(x, a), ∀x ∈ X.

3 Speedy Q-Learning

In this section, we introduce our RL algorithm, called speedy Q-Learning (SQL), derive a perfor-
mance bound for this algorithm, and compare this bound with similar results on standard Q-learning.

2

The derived performance bound shows that SQL has a rate of convergence of order O(p1/T ),
which is better than all the existing results for Q-learning.
3.1 Speedy Q-Learning Algorithm

The pseudo-code of the SQL algorithm is shown in Algorithm 1. As it can be seen, this is the
synchronous version of the algorithm, which will be analyzed in the paper. Similar to the standard
Q-learning, SQL may be implemented either synchronously or asynchronously. In the asynchronous
version, at each time step, the action-value of the observed state-action pair is updated, while the
rest of the state-action pairs remain unchanged. For the convergence of this instance of the algo-
rithm, it is required that all the states and actions are visited inﬁnitely many times, which makes
the analysis slightly more complicated. On the other hand, given a generative model, the algo-
rithm may be also formulated in a synchronous fashion, in which we ﬁrst generate a next state
y ∼ P (·|x, a) for each state-action pair (x, a), and then update the action-values of all the state-
action pairs using these samples. We chose to include only the synchronous version of SQL in
the paper just for the sake of simplicity in the analysis. However, the algorithm can be imple-
mented in an asynchronous fashion (similar to the more familiar instance of Q-learning) and our
theoretical results can also be extended to the asynchronous case under some mild assumptions.1
Algorithm 1: Synchronous Speedy Q-Learning (SQL)
Input: Initial action-value function Q0 , discount factor γ , and number of iteration T
Q−1 := Q0 ;
// Initialization
for k := 0, 1, 2, 3, . . . , T − 1 do
// Main loop
αk := 1
k+1 ;
for each (x, a) ∈ Z do
Generate the next state sample yk ∼ P (·|x, a);
TkQk−1 (x, a) := r(x, a) + γMQk−1 (yk );
TkQk (x, a) := r(x, a) + γMQk (yk );
// Empirical Bellman operator
Qk+1 (x, a) := Qk (x, a)+αk `TkQk−1 (x, a)−Qk (x, a)´+ (1−αk )`TkQk (x, a)−TkQk−1 (x, a)´;
// SQL update rule

end
end
return QT
As it can be seen from Algorithm 1, at each time step k , SQL keeps track of the action-value func-
tions of the two time-steps k and k − 1, and its main update rule is of the following form:
Qk+1 (x, a) = Qk (x, a) + αk (cid:0)TkQk−1 (x, a) − Qk (x, a)(cid:1) + (1 − αk )(cid:0)TkQk (x, a) − TkQk−1 (x, a)(cid:1),
(1)
where TkQ(x, a) = r(x, a) + γMQ(yk ) is the empirical Bellman optimality operator for the sam-
pled next state yk ∼ P (·|x, a). At each time step k and for state-action pair (x, a), SQL works as
follows: (i) it generates a next state yk by drawing a sample from P (·|x, a), (ii) it calculates two
sample estimates TkQk−1 (x, a) and TkQk (x, a) of the Bellman optimality operator (for state-action
pair (x, a) using the next state yk ) applied to the estimates Qk−1 and Qk of the action-value function
at the previous and current time steps, and ﬁnally (iii) it updates the action-value function of (x, a),
generates Qk+1 (x, a), using the update rule of Eq. 1. Moreover, we let αk decays linearly with
time, i.e., αk = 1/(k + 1), in the SQL algorithm. 2The update rule of Eq. 1 may be rewritten in the
following more compact form:

Qk+1 (x, a) = (1 − αk )Qk (x, a) + αkDk [Qk , Qk−1 ](x, a),

(2)

where Dk [Qk , Qk−1 ](x, a) , kTkQk (x, a) − (k − 1)TkQk−1 (x, a). This compact form will come
speci ﬁcally handy in the analysis of the algorithm in Sectio n 4.

Let us consider the update rule of Q-learning
Qk+1 (x, a) = Qk (x, a) + αk (cid:0)TkQk (x, a) − Qk (x, a)(cid:1),
1See [2] for the convergence analysis of the asynchronous variant of SQL.
2Note that other (polynomial) learning steps can also be used with speedy Q-learning. However one can
show that the rate of convergence of SQL is optimized for αk = 1‹(k + 1). This is in contrast to the standard
Q-learning algorithm for which the rate of convergence is optimized for a polynomial learning step [8].

3

which may be rewritten as
Qk+1 (x, a) = Qk (x, a) + αk (cid:0)TkQk−1 (x, a) − Qk (x, a)(cid:1) + αk (cid:0)TkQk (x, a) − TkQk−1 (x, a)(cid:1). (3)
Comparing the Q-learning update rule of Eq. 3 with the one for SQL in Eq. 1, we ﬁrst notice that
the same terms: TkQk−1 − Qk and TkQk − TkQk−1 appear on the RHS of the update rules of both
algorithms. However, while Q-learning uses the same conservative learning rate αk for both these
terms, SQL uses αk for the ﬁrst term and a bigger learning step 1 − αk = k/(k + 1) for the second
one. Since the term TkQk − TkQk−1 goes to zero as Qk approaches its optimal value Q∗ , it is not
necessary that its learning rate approaches zero. As a result, using the learning rate αk , which goes
to zero with k , is too conservative for this term. This might be a reason why SQL that uses a more
aggressive learning rate 1 − αk for this term has a faster convergence rate than Q-learning.

3.2 Main Theoretical Result

The main theoretical result of the paper is expressed as a high-probability bound over the perfor-
mance of the SQL algorithm.
Theorem 1. Let Assumption 1 holds and T be a positive integer. Then, at iteration T of SQL with
probability at least 1 − δ , we have
kQ∗ − QT k ≤ 2β 2Rmax 

 .

We report the proof of Theorem 1 in Section 4. This result, combined with Borel-Cantelli lemma [9],
guarantees that QT converges almost surely to Q∗ with the rate p1/T . Further, the following result
which quanti ﬁes the number of steps T required to reach the error ǫ > 0 in estimating the optimal
action-value function, w.p. 1 − δ , is an immediate consequence of Theorem 1.
Corollary 1 (Finite-time PAC ( “probably approximately correct ”) perf ormance bound for SQL).
Under Assumption 1, for any ǫ > 0, after

+ s 2 log 2n
δ
T

γ
T

max log 2n
11.66β 4R2
δ
ǫ2
steps of SQL, the uniform approximation error kQ∗ − QT k ≤ ǫ, with probability at least 1 − δ .

T =

3.3 Relation to Existing Results

In this section, we ﬁrst compare our results for SQL with the e xisting results on the convergence of
standard Q-learning. This comparison indicates that SQL accelerates the convergence of Q-learning,
especially for γ close to 1 and small ǫ. We then compare SQL with batch Q-value iteration (QI) in
terms of sample and computational complexities, i.e., the number of samples and the computational
cost required to achieve an ǫ-optimal solution w.p. 1 − δ , as well as space complexity, i.e., the
memory required at each step of the algorithm.

3.3.1 A Comparison with the Convergence Rate of Standard Q-Learning

There are not many studies in the literature concerning the convergence rate of incremental model-
free RL algorithms such as Q-learning. [17] has provided the asymptotic convergence rate for Q-
learning under the assumption that all the states have the same next state distribution. This result
shows that the asymptotic convergence rate of Q-learning has exponential dependency on 1 − γ , i.e.,
the rate of convergence is of ˜O(1/t1−γ ) for γ ≥ 1/2.
The ﬁnite time behavior of Q-learning have been throughly in vestigated in [8] for different
time scales. Their main result indicates that by using the polynomial learning step αk =
1(cid:14) (k + 1)ω , 0.5 < ω < 1, Q-learning achieves ǫ-optimal performance w.p. at least 1 − δ after
1
T = O 
1−ω 
1
" β 4R2
#
max log nβRmax
w
(cid:21)
+ (cid:20)β log
βRmax
δǫ

ǫ2
ǫ

(4)

4

steps. When γ ≈ 1, one can argue that β = 1/(1 − γ ) becomes the dominant term in the bound of
Eq. 4, and thus, the optimized bound w.r.t. ω is obtained for ω = 4/5 and is of ˜O(cid:0)β 5 /ǫ2.5 (cid:1). On the
other hand, SQL is guaranteed to achieve the same precision in only O(cid:0)β 4 /ǫ2 (cid:1) steps. The difference
between these two bounds is signi ﬁcant for large values of β , i.e., γ ’s close to 1.
3.3.2 SQL vs. Q-Value Iteration

Finite sample bounds for both model-based and model-free (Phased Q-learning) QI have been de-
rived in [12] and [7]. These algorithms can be considered as the batch version of Q-learning.
They show that to quantify ǫ-optimal action-value functions with high probability, we need
O(cid:0)nβ 5 /ǫ2 log(1/ǫ)(cid:0) log(nβ ) + log log 1(cid:14)ǫ(cid:1)(cid:1) and O(cid:0)nβ 4 /ǫ2 (log(nβ ) + log log 1(cid:14)ǫ)(cid:1) samples in
model-free and model-based QI, respectively. A comparison between their results and the main re-
sult of this paper suggests that the sample complexity of SQL, which is of order O(cid:0)nβ 4 /ǫ2 log n(cid:1),3
is better than model-free QI in terms of β and log(1/ǫ). Although the sample complexities of SQL
is only slightly tighter than the model-based QI, SQL has a signi ﬁcantly better computational and
space complexity than model-based QI: SQL needs only 2n memory space, while the space com-
plexity of model-based QI is given by either ˜O(nβ 4 /ǫ2 ) or n(|X| + 1), depending on whether the
learned state transition matrix is sparse or not [12]. Also, SQL improves the computational com-
plexity by a factor of ˜O(β ) compared to both model-free and model-based QI.4 Table 1 summarizes
the comparisons between SQL and the other RL methods discussed in this section.

Table 1: Comparison between SQL, Q-learning, model-based and model-free Q-value iteration in
terms of sample complexity (SC), computational complexity (CC), and space complexity (SPC).
Method
SQL
Q-learning (optimized) Model-based QI Model-free QI
˜O (cid:18) nβ 4
˜O (cid:18) nβ 5
˜O (cid:18) nβ 4
˜O (cid:18) nβ 5
ǫ2 (cid:19)
ǫ2.5 (cid:19)
ǫ2 (cid:19)
ǫ2 (cid:19)
˜O (cid:18) nβ 4
˜O (cid:18) nβ 5
˜O (cid:18) nβ 5
˜O (cid:18) nβ 5
ǫ2 (cid:19)
ǫ2.5 (cid:19)
ǫ2 (cid:19)
ǫ2 (cid:19)
˜O (cid:18) nβ 4
ǫ2 (cid:19)
Θ(n)
Θ(n)
Θ(n)

SPC

SC

CC

4 Analysis

In this section, we give some intuition about the convergence of SQL and provide the full proof of
the ﬁnite-time analysis reported in Theorem 1. We start by in troducing some notations.
Let Fk be the ﬁltration generated by the sequence of all random samp les {y1 , y2 , . . . , yk } drawn
from the distribution P (·|x, a), for all state action (x, a) up to round k . We deﬁne the operator
D[Qk , Qk−1 ] as the expected value of the empirical operator Dk conditioned on Fk−1 :

D[Qk , Qk−1 ](x, a) , E(Dk [Qk , Qk−1 ](x, a) |Fk−1 )
= kTQk (x, a) − (k − 1)TQk−1 (x, a).

Thus the update rule of SQL writes

Qk+1 (x, a) = (1 − αk )Qk (x, a) + αk (D[Qk , Qk−1 ](x, a) − ǫk (x, a)) ,

(5)

3Note that at each round of SQL n new samples are generated. This combined with the result of Corollary 1
deduces the sample complexity of order O(nβ 4 /ǫ2 log(n/δ)).
4SQL has the sample and computational complexity of a same order since it performs only one Q-value
update per sample, whereas, in the case of model-based QI, the algorithm needs to iterate the action-value
function of all state-action pairs at least ˜O(β ) times using Bellman operator, which leads to a computational
complexity bound of order ˜O(nβ 5 /ǫ2 ) given that only ˜O(nβ 4 /ǫ2 ) entries of the estimated transition matrix
are non-zero [12].

5

where the estimation error ǫk is deﬁned as the difference between the operator D[Qk , Qk−1 ] and its
sample estimate Dk [Qk , Qk−1 ] for all (x, a) ∈ Z:
ǫk (x, a) , D[Qk , Qk−1 ](x, a) − Dk [Qk , Qk−1 ](x, a).
We have the property that E[ǫk (x, a)|Fk−1 ] = 0 which means that for all (x, a) ∈ Z the sequence
of estimation error {ǫ1 (x, a), ǫ2 (x, a), . . . , ǫk (x, a)} is a martingale difference sequence w.r.t. the
ﬁltration Fk . Let us deﬁne the martingale Ek (x, a) to be the sum of the estimation errors:
k
Xj=0
The proof of Theorem 1 follows the following steps: (i) Lemma 1 shows the stability of the algorithm
(i.e., the sequence of Qk stays bounded). (ii) Lemma 2 states the key property that the SQL iterate
Qk+1 is very close to the Bellman operator T applied to the previous iterate Qk plus an estimation
error term of order Ek /k . (iii) By induction, Lemma 3 provides a performance bound kQ∗ − Qk k
in terms of a discounted sum of the cumulative estimation errors {Ej }j=0:k−1 . Finally (iv) we use
a maximal Azuma’s inequality (see Lemma 4) to bound Ek and deduce the ﬁnite time performance
for SQL.

Ek (x, a) ,

∀(x, a) ∈ Z.

ǫj (x, a),

(6)

For simplicity of the notations, we remove the dependence on (x, a) (e.g., writing Q for Q(x, a),
Ek for Ek (x, a)) when there is no possible confusion.
Lemma 1 (Stability of SQL). Let Assumption 1 hold and assume that the initial action-value func-
tion Q0 = Q−1 is uniformly bounded by Vmax , then we have, for all k ≥ 0,
and
kDk [Qk , Qk−1 ]k ≤ Vmax .
kǫk k ≤ 2Vmax ,
kQk k ≤ Vmax ,

Qk +

Proof. We ﬁrst prove that kDk [Qk , Qk−1 ]k ≤ Vmax by induction. For k = 0 we have:
kD0 [Q0 , Q−1 ]k ≤ krk + γ kMQ−1 k ≤ Rmax + γVmax = Vmax .
Now for any k ≥ 0, let us assume that the bound kDk [Qk , Qk−1 ]k ≤ Vmax holds. Thus
kDk+1 [Qk+1 , Qk ]k ≤ krk + γ k(k + 1)MQk+1 − kMQk k
Dk [Qk , Qk−1 ](cid:19) − kMQk (cid:13)(cid:13)(cid:13)(cid:13)
= krk + γ (cid:13)(cid:13)(cid:13)(cid:13)
(k + 1)M(cid:18) k
1
k + 1
k + 1
≤ krk + γ kM (kQk + Dk [Qk , Qk−1 ] − kQk )k
≤ krk + γ kDk [Qk , Qk−1 ]k ≤ Rmax + γVmax = Vmax ,
and by induction, we deduce that for all k ≥ 0, kDk [Qk , Qk−1 ]k ≤ Vmax .
Now the bound on ǫk follows from kǫk k = kE(Dk [Qk , Qk−1 ]|Fk−1 ) − Dk [Qk , Qk−1 ]k ≤ 2Vmax ,
and the bound kQk k ≤ Vmax is deduced by noticing that Qk = 1/k Pk−1
j=0 Dj [Qj , Qj−1 ].
The next lemma shows that Qk is close to TQk−1 , up to a O(1/k) term plus the average cumulative
estimation error 1
k Ek−1 .
Lemma 2. Under Assumption 1, for any k ≥ 1:
1
k

(TQ0 + (k − 1)TQk−1 − Ek−1 ) .

Qk =

(7)

Qk+1 =

Proof. We prove this result by induction. The result holds for k = 1, where (7) reduces to (5). We
now show that if the property (7) holds for k then it also holds for k + 1. Assume that (7) holds for
k . Then, from (5) we have:
k
1
Qk +
(kTQk − (k − 1)TQk−1 − ǫk )
k + 1
k + 1
k + 1 (cid:18) 1
(TQ0 + (k − 1)TQk−1 − Ek−1 )(cid:19) +
k
k
1
1
(TQ0 + kTQk − Ek−1 − ǫk ) =
(TQ0 + kTQk − Ek ).
k + 1
k + 1
Thus (7) holds for k + 1, and is thus true for all k ≥ 1.

(kTQk − (k − 1)TQk−1 − ǫk )

1
k + 1

=

=

6

+

1
k

(8)

γ k−j kEj−1 k .

kQ∗ − Qk k ≤

2γβVmax
k

Now we bound the difference between Q∗ and Qk in terms of the discounted sum of cumulative
estimation errors {E0 , E1 , . . . , Ek−1 }.
Lemma 3 (Error Propagation of SQL). Let Assumption 1 hold and assume that the initial action-
value function Q0 = Q−1 is uniformly bounded by Vmax , then for all k ≥ 1, we have
k
Xj=1
Proof. Again we prove this lemma by induction. The result holds for k = 1 as:
kQ∗ − Q1 k = kTQ∗ − T0Q0 k = ||TQ∗ − TQ0 + ǫ0 ||
≤ ||TQ∗ − TQ0 || + ||ǫ0 || ≤ 2γVmax + ||ǫ0 || ≤ 2γβVmax + kE0 k
We now show that if the bound holds for k , then it also holds for k + 1. Thus, assume that (8) holds
for k . By using Lemma 2:
(cid:13)(cid:13)Q∗ − Qk+1(cid:13)(cid:13) = (cid:13)(cid:13)(cid:13)(cid:13)
(TQ0 + kTQk − Ek )(cid:13)(cid:13)(cid:13)(cid:13)
Q∗ −
= (cid:13)(cid:13)(cid:13)(cid:13)
Ek (cid:13)(cid:13)(cid:13)(cid:13)
1
k
(TQ∗ − TQ0 ) +
(TQ∗ − TQk ) +
k + 1
k + 1
γ
1
kγ
kQ∗ − Qk k +
kQ∗ − Q0k +
≤
kEk k
k + 1
k + 1
k + 1
γ k−j kEj−1 k
k + 1 
kγ
 +
≤

k+1
1
Xj=1
k + 1
Thus (8) holds for k + 1 thus for all k ≥ 1 by induction.

k
Xj=1
γ k+1−j kEj−1k .

2γβVmax
k

2γβVmax
k + 1

2γ
k + 1

1
k + 1

1
k + 1

1
k + 1

Vmax +

kEk k

1
k

+

=

+

Now, based on Lemmas 3 and 1, we prove the main theorem of this paper.

.

+

1
T

γ T −k kEk−1k .

kQ∗ − QT k ≤

Proof of Theorem 1. We begin our analysis by recalling the result of Lemma 3 at round T :
T
2γβVmax
1
Xk=1
T
T
Note that the difference between this bound and the result of Theorem 1 is just in the second term.
So, we only need to show that the following inequality holds, with probability at least 1 − δ :
γ T −k kEk−1k ≤ 2βVmaxs 2 log 2n
T
Xk=1
δ
T

We ﬁrst notice that:
T
T
1
β max1≤k≤T kEk−1k
Xk=1
Xk=1
T
T
Therefore,
to bound max1≤k≤T kEk−1 k =
sufﬁcient
is
it
to prove
(9)
in order
max(x,a)∈Z max1≤k≤T |Ek−1 (x, a)| in high probability. We start by providing a high probability
bound for max1≤k≤T |Ek−1 (x, a)| for a given (x, a). First notice that
(−Ek−1 (x, a))(cid:21) > ǫ(cid:19)
|Ek−1 (x, a)| > ǫ(cid:19) = P (cid:18)max (cid:20) max
P (cid:18) max
(Ek−1 (x, a)), max
1≤k≤T
1≤k≤T
1≤k≤T
(Ek−1 (x, a)) > ǫ(cid:27) [ (cid:26) max
= P (cid:18)(cid:26) max
(−Ek−1 (x, a)) > ǫ(cid:27)(cid:19)
1≤k≤T
1≤k≤T
(−Ek−1 (x, a)) > ǫ(cid:19) ,
(Ek−1 (x, a)) > ǫ(cid:19) + P (cid:18) max
≤ P (cid:18) max
1≤k≤T
1≤k≤T
(11)
and each term is now bounded by using a maximal Azuma inequality, reminded now (see e.g., [6]).

γ T −k max
1≤k≤T

γ T −k kEk−1k ≤

kEk−1k ≤

(10)

1
T

(9)

.

7

Lemma 4 (Maximal Hoeffding-Azuma Inequality). Let V = {V1 , V2 , . . . , VT } be a mar-
tingale difference sequence w.r.t. a sequence of random variables {X1 , X2 , . . . , XT } (i.e.,
E(Vk+1 |X1 , . . . Xk ) = 0 for all 0 < k ≤ T ) such that V is uniformly bounded by L > 0. If
we deﬁne Sk = Pk
i=1 Vi , then for any ǫ > 0, we have
Sk > ǫ(cid:19) ≤ exp (cid:18) −ǫ2
P (cid:18) max
2T L2 (cid:19) .
1≤k≤T
As mentioned earlier,
the sequence of random variables {ǫ0 (x, a), ǫ1 (x, a), · · · , ǫk (x, a)} is
a martingale difference sequence w.r.t.
the ﬁltration Fk (generated by the random samples
{y0 , y1 , . . . , yk }(x, a) for all (x, a)), i.e., E[ǫk (x, a)|Fk−1 ] = 0. It follows from Lemma 4 that
for any ǫ > 0 we have:
(Ek−1 (x, a)) > ǫ(cid:19) ≤ exp (cid:18) −ǫ2
P (cid:18) max
max (cid:19)
8T V 2
1≤k≤T
(−Ek−1 (x, a)) > ǫ(cid:19) ≤ exp (cid:18) −ǫ2
max (cid:19) .
P (cid:18) max
8T V 2
1≤k≤T
max (cid:17) ,
By combining (12) with (11) we deduce that P (max1≤k≤T |Ek−1 (x, a)| > ǫ) ≤ 2 exp (cid:16) −ǫ2
8T V 2
and by a union bound over the state-action space, we deduce that
kEk−1k > ǫ(cid:19) ≤ 2n exp (cid:18) −ǫ2
P (cid:18) max
max (cid:19) .
(13)
8T V 2
1≤k≤T
This bound can be rewritten as: for any δ > 0,
P   max
δ ! ≥ 1 − δ ,
kEk−1k ≤ Vmaxr8T log
2n
1≤k≤T
which by using (10) proves (9) and Theorem 1.

(12)

(14)

5 Conclusions and Future Work

In this paper, we introduced a new Q-learning algorithm, called speedy Q-learning (SQL). We ana-
lyzed the ﬁnite time behavior of this algorithm as well as its asymptotic convergence to the optimal
action-value function. Our result is in the form of high probability bound on the performance loss
of SQL, which suggests that the algorithm converges to the optimal action-value function in a faster
rate than the standard Q-learning. Overall, SQL is a simple, efﬁcient and theoretically well-founded
reinforcement learning algorithm, which improves on existing RL algorithms such as Q-learning
and model-based value iteration.

In this work, we are only interested in the estimation of the optimal action-value function and not the
problem of exploration. Therefore, we did not compare our result to the PAC-MDP methods [15, 18]
and the upper-conﬁdence bound based algorithms [3, 11], in w hich the choice of the exploration
policy impacts the behavior of the learning algorithms. However, we believe that it would be possible
to gain w.r.t. the state of the art in PAC-MDPs, by combining the asynchronous version of SQL with
a smart exploration strategy. This is mainly due to the fact that the bound for SQL has been proved to
be tighter than the RL algorithms that have been used for estimating the value function in PAC-MDP
methods, especially in the model-free case. We consider this as a subject for future research.

Another possible direction for future work is to scale up SQL to large (possibly continuous) state
and action spaces where function approximation is needed. We believe that it would be possible to
extend our current SQL analysis to the continuous case along the same path as in the ﬁtted value
iteration analysis by [13] and [1]. This would require extending the error propagation result of
Lemma 3 to a ℓ2 -norm analysis and combining it with the standard regression bounds.

Acknowledgments

The authors appreciate supports from the PASCAL2 Network of Excellence Internal-Visit Pro-
gramme and the European Community’s Seventh Framework Programme (FP7/2007-2013) under
grant agreement no 231495. We also thank Peter Auer for helpful discussion and the anonymous
reviewers for their valuable comments.

8

References

[1] A. Antos, R. Munos, and Cs. Szepesv ´ari. Fitted Q-iteration in continuous action-space MDPs.
In Proceedings of the 21st Annual Conference on Neural Information Processing Systems,
2007.
[2] M. Gheshlaghi Azar, R. Munos, M. Ghavamzadeh, and H.J. Kappen. Reinforcement learning
with a near optimal rate of convergence. Technical Report inria-00636615, INRIA, 2011.
[3] P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the 25th Conference on Uncer-
tainty in Arti ﬁcial Intelligence , 2009.
[4] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume II. Athena Scienti ﬁc,
Belmount, Massachusetts, third edition, 2007.
[5] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scienti ﬁc, Belmont,
Massachusetts, 1996.
[6] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University
Press, New York, NY, USA, 2006.
[7] E. Even-Dar, S. Mannor, and Y. Mansour. PAC bounds for multi-armed bandit and Markov
decision processes.
In 15th Annual Conference on Computational Learning Theory, pages
255–270, 2002.
[8] E. Even-Dar and Y. Mansour. Learning rates for Q-learning. Journal of Machine Learning
Research, 5:1–25, 2003.
[9] W. Feller. An Introduction to Probability Theory and Its Applications, volume 1. Wiley, 1968.
[10] T. Jaakkola, M. I. Jordan, and S. Singh. On the convergence of stochastic iterative dynamic
programming. Neural Computation, 6(6):1185–1201, 1994.
[11] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research, 11:1563–1600, 2010.
[12] M. Kearns and S. Singh. Finite-sample convergence rates for Q-learning and indirect algo-
rithms.
In Advances in Neural Information Processing Systems 12, pages 996–1002. MIT
Press, 1999.
[13] R. Munos and Cs. Szepesv ´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9:815–857, 2008.
[14] J. Peng and R. J. Williams.
Incremental multi-step Q-learning. Machine Learning, 22(1-
3):283–290, 1996.
[15] A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning in ﬁnite MDPs: PAC analysis.
Journal of Machine Learning Research, 10:2413–2444, 2009.
[16] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cam-
bridge, Massachusetts, 1998.
[17] Cs. Szepesv ´ari. The asymptotic convergence-rate of Q-learning. In Advances in Neural Infor-
mation Processing Systems 10, Denver, Colorado, USA, 1997, 1997.
[18] I. Szita and Cs. Szepesv ´ari. Model-based reinforcement learning with nearly tight exploration
complexity bounds. In Proceedings of the 27th International Conference on Machine Learning,
pages 1031–1038. Omnipress, 2010.
[19] H. van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems
23, pages 2613–2621, 2010.
[20] C. Watkins. Learning from Delayed Rewards. PhD thesis, Kings College, Cambridge, England,
1989.

9


Co-Training for Domain Adaptation

Minmin Chen, Kilian Q. Weinberger
Department of Computer Science and Engineering
Washington University in St. Louis
St. Louis, MO 63130
mc15,kilian@wustl.edu

John C. Blitzer
Google Research
1600 Amphitheatre Parkway
Mountain View, CA 94043
blitzer@google.com

Abstract

Domain adaptation algorithms seek to generalize a model trained in a source do-
main to a new target domain. In many practical cases, the source and target dis-
tributions can differ substantially, and in some cases crucial target features may
not have support in the source domain. In this paper we introduce an algorithm
that bridges the gap between source and target domains by slowly adding to the
training set both the target features and instances in which the current algorithm
is the most conﬁdent. Our algorithm is a variant of co-training [7], and we name
it CODA (Co-training for domain adaptation). Unlike the original co-training
work, we do not assume a particular feature split. Instead, for each iteration of co-
training, we formulate a single optimization problem which simultaneously learns
a target predictor, a split of the feature space into views, and a subset of source and
target features to include in the predictor. CODA signiﬁcantly out-performs the
state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed,
over a wide range (65 of 84 comparisons) of target supervision CODA achieves
the best performance.

1

Introduction

Domain adaptation addresses the problem of generalizing from a source distribution for which
we have ample labeled training data to a target distribution for which we have little or no la-
bels [3, 14, 28]. Domain adaptation is of practical importance in many areas of applied machine
learning, ranging from computational biology [17] to natural language processing [11, 19] to com-
puter vision [23].
In this work, we focus primarily on domain adaptation problems that are characterized by missing
features. This is often the case in natural language processing, where different genres often use
very different vocabulary to describe similar concepts. For example, in our experiments we use the
sentiment data of Blitzer et al. [4], where a breeze to use is a way to express positive sentiment about
kitchen appliances, but not about books. In this situation, most domain adaptation algorithms seek
to eliminate the difference between source and target distributions, either by re-weighting source
instances [14, 18] or learning a new feature representation [6, 28].
We present an algorithm which differs from both of these approaches. Our method seeks to slowly
adapt its training set from the source to the target domain, using ideas from co-training. We accom-
plish this in two ways: First, we train on our own output in rounds, where at each round, we include
in our training data the target instances we are most conﬁdent of. Second, we select a subset of
shared source and target features based on their compatibility. Different from most previous work
on selecting features for domain adaptation, the compatibility is measured across the training set
and the unlabeled set, instead of across the two domains. As more target instances are added to the
training set, target speciﬁc features become compatible across the two sets, therefore are included
in the predictor. Finally, we exploit the pseudo multiview co-training algorithm of Chen et al. [10]

1

to exploit the unlabeled data efﬁciently. These three intuitive ideas can be combined in a single
optimization problem. We name our algorithm CODA (Co-Training for Domain Adaptation).
By allowing us to slowly change our training data from source to target, CODA has an advantage
over representation-learning algorithms [6, 28], since they must decide a priori what the best repre-
sentation is. In contrast, each iteration of CODA can choose exactly those few target features which
can be related to the current (source and pseudo-labeled target) training set. We ﬁnd that in the
sentiment prediction data set of Blitzer et al. [4] CODA improves the state-of-the-art cross widely
varying amounts of target labeled data in 65 out of 84 settings.

2 Notation and Setting

We assume our data originates from two domains, Source (S) and Target (T). The source data
is fully labeled DS = {(x1 , y1 ), . . . , (xns , yns )} ⊂ Rd × Y and sampled from some distribu-
tion PS (X, Y ). The target data is sampled from PT (X, Y ) and is divided into labeled D l
T =
{(x1 , y1 ), . . . , (xnt , ynt )} ⊂ Rd × Y and unlabeled Du
T = {(x1 , ?), . . . (xmt , ?)} ⊂ Rd × Y parts,
where in the latter the labels are unknown during training time. Both domains are of equal dimen-
sionality d. Our goal is to learn a classiﬁer h ∈ H to accurately predict the labels on the unlabeled
portion of DT , but also to extend to out-of-sample test points, such that for any (x, y) sampled
from PT , we have h(x) = y with high probability. For simplicity we assume that Y = {+1, −1},
although our method can easily be adapted to multi-class or regression settings.
We assume the existence of a base classiﬁer, which determines the set H. Throughout this paper we
simply use logistic regression, i.e. our classiﬁer is parameterized by a weight-vector w ∈ Rd and
(cid:88)
deﬁned as hw (x) = (1 + e−w(cid:62) x )−1 . The weights w are set to minimize the loss function
(cid:96)(w; D) = − 1
log(1 + exp(−yw
(cid:62)
|D |
(x,y)∈D
If trained on data sampled from PS (X, Y ), logistic regression models the distribution PS (Y |X ) [13]
through Ph (Y = y |X = x; w) = (1 + e−w(cid:62) xy )−1 . In this paper, our goal is to adapt this classiﬁer
to the target distribution PT (Y |X ).

x)).

(1)

3 Method

In this section, we begin with a semi-supervised approach and describe the rote-learning procedure to
automatically annotate target domain inputs. The algorithm maintains and grows a training set that is
iteratively adapted to the target domain. We then incorporate feature selection into the optimization,
a crucial element of our domain-adaptation algorithm. The feature selection addresses the change in
distribution and support from PS to PT . Further, we introduce pseudo multi-view co-training [7, 10],
which improves the rote-learning procedure by adding inputs with features that are still not used
effectively by the current classiﬁer. We use automated feature decomposition to artiﬁcially split our
data into multiple views, explicitly to enable successful co-training.

3.1 Self-training for Domain Adaptation

First, we assume we are given a loss function (cid:96) – in our case the log-loss from eq. (1) – which
provides some estimate of conﬁdence in its predictions. In logistic regression, if ˆy = sign(h(x)) is
the prediction for an input x, the probability Ph (Y = ˆy |X = x; w) is a natural metric of certainty
(as h(x) can be interpreted as a probability for x to be of label +1), but other methods [22] can
be used. Self-training [19] is a simple and intuitive iterative algorithm to leverage unlabeled data.
During training one maintains a labeled training set L and an unlabeled test set U , initialized as
L = DS ∪ D l
T and U = Du
T . Each iteration, a classiﬁer hw is trained to minimize the loss function
(cid:96) over L and is evaluated on all elements of U . The c most conﬁdent predictions on U are moved to
L for the next iteration, labeled by the prediction of sign(hw ). The algorithm terminates when U is
empty or all predictions are below a pre-deﬁned conﬁdence threshold (and considered unreliable).
Algorithm 1 summarizes self-training in pseudo-code with the use of feature selection, described in
the following section.

2

Algorithm 1 SEDA pseudo-code.
1: Inputs: L and U .
2: repeat
3: w∗ = argminw (cid:96)(w; L) + γ s(L, U, w)
4:
Apply hw∗ on all elements of U .
5: Move up-to c conﬁdent inputs xi from U to L, labeled as sign(h(xi )).
6: until No more predictions are conﬁdent
7: Return hw∗

3.2 Feature Selection

So far, we have not addressed that the two data sets U and L are not sampled from the same dis-
tribution. In domain adaptation, the training data is no longer representative of the test data. More
explicitly, PS (Y |X = x) is different from PT (Y |X = x). For illustration, consider the sentiment
analysis problem in section 4, where data consists of unigram and bigram bag-of-words features
and the task is to classify if a book-review (source domain) or dvd-review (target domain) is pos-
itive or negative. Here, the bigram feature “must read” is indicative of a positive opinion within
the source (“books”) domain, but rarely appears in the target (“dvd”) domain. A classiﬁer, trained
on the source-dominated set L, that relies too heavily on such features will not make enough high-
T .
conﬁdence predictions on the set U = Du
To address this issue, we extend the classiﬁer with a weighted (cid:96)1 regularization for feature selection.
The weights are assigned to encourage the classiﬁer to only use features that behave similarly in both
L and U . Different from previous work on feature selection for domain adaptation [25], where the
goal is to ﬁnd a new representation to minimize the difference between the distributions of the source
and target domain, what we are proposing is to minimize the difference between the distributions of
the labeled training set L and the unlabeled set U (which coincides with the testing set in our setting).
This difference is crucial, as it makes the empirical distributions of L and U align gradually. For
example, after some iterations, the classiﬁer can pick features that are never present in the source
domain, but which have entered L through the rote-learning procedure.
We perform the feature selection implicitly through w. For a feature α, let us denote the Pearson
correlation coefﬁcient (PCC)1 between feature value xα and the label y for all pairs (x, y) ∈ L as
ρL (xα , y). It can be shown that ρL (xα , y) ∈ [−1, 1] with a value of +1 if a feature is perfectly
the feature is the label), 0 if it has no correlation, and −1 if it is of
aligned with the label (i.e.
opposite polarity (i.e.
the inverted label). Similarly, let us deﬁne the PCC for all pairs in U as
ρU ;w (xα , Y ), where the unknown label Y is a random variable drawn from the conditional proba-
bility Ph (Y |X ; w). The two PCC values indicate how predictive a feature is of the (estimated) class
label in the two respective data sets. Ideally, we would like to choose features that are similarly
predictive across the two sets. We measure how similarly a feature behaves across L and U with
the product ρL (xα , y)ρU ;w (xα , Y ). With this notation, we deﬁne the feature weight that reﬂects the
cross-domain incompatibility of a feature as
∆L,U,w (α) = (1 − ρL (xα , y)ρU ;w (xα , Y )).
(2)
It is straight-forward to show that ∆L,U,w ∈ [0, 2]. Intuitively, ∆L,U,w expresses to what degree
we would like to remove a feature. A perfect feature, that is the label itself (and the prediction in
U ), results in a score of 0. A feature that is not correlated with the class label in at least one of
the two domains (and therefore is too domain-speciﬁc) obtains a score of 1. A feature that switches
polarization across domains (and therefore is “malicious”) has a score ∆L,U,w (α) > 1 (in the
extreme case if it is the label in L and the inverted label in U , its score would be 2).
d(cid:88)
We incorporate (2) into a weighted (cid:96)1 regularization
∆L,U,w (α)|wα |.
α=1
Intuitively (3) encourages feature sparsity with a strong emphasis on features with little or oppo-
site correlation across the domains, whereas good features that are consistently predictive in both
1The PCC for two random variables X, Y is deﬁned as ρ = E [(X−µX )(Y −µY )]
σX σY
mean and σX the standard deviation of X .

, where µX denotes the

s(L, U, w) =

(3)

3

domains become cheap. We refer to this version of the algorithm as Self-training for Domain Adap-
tation (SEDA). The optimization with feature selection, used in Algorithm 1, becomes
(4)
w = argminw (cid:96)(L) + γ s(L, U, w).
Here, γ ≥ 0 denotes the loss-regularization trade-off parameter. As we have very few labeled inputs
from the target domain in the early iterations, stronger regularization is imposed so that only features
shared across the two domains are used. When more and more inputs from the target domain are
included in the training set, we gradually decrease the regularization to accommodate target speciﬁc
features. The algorithm is very insensitive to the exact initial choice of γ . The guideline is to start
with a relatively large number, and decrease it until the selected feature set is not empty. In our
implementation, we set it to γ0 = 0.1, and we divide it by a factor of 1.1 during each iteration.

3.3 Co-training for Domain Adaptation

For rote-learning to be effective, we need to move test inputs from U to L that 1) are correctly
classiﬁed (with high probability) and 2) have potential to improve the classiﬁer in future iterations.
The former is addressed by the feature selecting regularization from the previous section – restricting
the classiﬁer to a sub-set of features that are known to be cross-data set compatible reduces the
generalization error on U . In this section we address the second requirement. We want to add inputs
xi that contain additional features, which were not used to obtain the prediction hw (xi ) and would
enrich the training set L.
If the exact labels of the inputs in U were known, a good active learning [26] strategy would be to
move inputs to L on which the current classiﬁer hw is most uncertain. In our setting, this would
be clearly ill advised as the uncertain prediction is also used as the label. A natural solution to this
dilemma is co-training [7]. Co-training assumes the data set is presented in two separate views and
two classiﬁers are trained, one in each view. Each iteration, only inputs that are conﬁdent according
to exactly one of the two classiﬁers are moved to the training set. This way, one classiﬁer provides
the (estimated) labels to the inputs on which the other classiﬁer is uncertain.
In our setting we do not have multiple views and which features are selected varies in each iteration.
Hence, co-training does not apply out-of-the-box. We can, however, split our features into two mu-
tually exclusive views such that co-training is effective. To this end we follow the pseudo-multiview
regularization introduced by Chen et al. [10]. The main intuition is to train two classiﬁers on a single
view X such that: (1) both perform well on the labeled data; (2) both are trained on strictly different
features; (3) together they are likely to satisfy Balcan’s condition of -expandability [2], a necessary
and sufﬁcient pre-condition for co-training to work2 . These three aspects can be formulated explic-
itly as three modiﬁcations of our optimization problem (4). We discuss each of them in detail in the
following.
Loss. Two classiﬁers are required for co-training, whose weight vectors we denote by u and v. The
performance of each classiﬁer is measured by the log-loss (cid:96)(·; L) in eq. (1). To ensure that both
(cid:16)
e(cid:96)(u;L) + e(cid:96)(v;L)(cid:17)
classiﬁers perform well on the training set L, i.e. both have a small training loss, we train them
jointly while minimizing the soft-maximum3 of the two losses,
Feature Decomposition. Co-training requires the two classiﬁers to be trained on different feature
spaces. We create those by splitting the feature-space into two mutually exclusive sub-sets. More
precisely, for each feature α, at least one of the two classiﬁers must have a zero weight in the αth
d(cid:88)
dimension. We can enforce this across all features with the equality constraint
α=1

αv2
u2
α = 0.

(5)

log

.

(6)

-Expandability. In the original co-training formulation [7], it is assumed that the two views of
the data are class conditionally independent. This assumption is very strong and can easily be
3The soft-max of a set of elements S is a differentiable approximation of max(S ) ≈ log((cid:80)
2Provided that the classiﬁers are never conﬁdent and wrong — which can be violated in practice.
s∈S es ).
4

(cid:35)

(7)

(8)

cu (x) =

cu (x)cv (x),

[cu (x)¯cv (x) + ¯cu (x)cv (x)] ≥  min

violated in practice [20]. Recent work [2] weakens this requirement signiﬁcantly to a condition of
-expandability. Loosely phrased, for the two classiﬁers to be able to teach each other, they must
make conﬁdent predictions on different subsets of the unlabeled set U .
For the classiﬁer hu , let ˆy = sign(u(cid:62)x) ∈ {±1} denote the class prediction and Ph ( ˆy |x; u) its
(cid:26) 1
conﬁdence. Deﬁne cu (x) as a conﬁdence indicator function (for some conﬁdence threshold τ > 0)4
if p( ˆy |x; u) > τ
otherwise,
0
(cid:34)(cid:88)
(cid:88)
(cid:88)
and cv respectively. Then the -expanding condition translates to
x∈U
x∈U
x∈U
for some  > 0. Here, cu (x) = 1 − cu (x) indicates that classiﬁer hu is not conﬁdent about input x.
Intuitively, the constraint in eq. (8) ensures that the total number of inputs in U that can be used for
rote-learning because exactly one classiﬁer is conﬁdent (LHS), is larger than the set of inputs which
cannot be used because both classiﬁers are already conﬁdent or both are not conﬁdent (RHS).
In summary, the framework splits the feature space into two mutually exclusive sub-sets. This rep-
resentation enables us to train two logistic regression classiﬁers, both with small loss on the labeled
data set, while satisfying two constraints to ensure feature decomposition and -expandability. Our
ﬁnal classiﬁer has the weight vector w = u + v. We refer to the resulting algorithm as CODA (Co-
training for Domain Adaptation), which can be stated concisely with the following optimization
log (cid:0)e(cid:96)(u;L) + e(cid:96)(v;L) (cid:1) + γ s(L, U, w)
problem:
min
(1) (cid:80)d
w,u,v
subject to:
x∈U [cu (x)¯cv (x) + ¯cu (x)cv (x)] ≥  min (cid:2)(cid:80)
x∈U ¯cu (x)¯cv (x)(cid:3)
(2) (cid:80)
x∈U cu (x)cv (x), (cid:80)
i v2
i=1 u2
i = 0
(3) w = u + v

¯cu (x)¯cv (x)

,

The optimization is non-convex. However, as it is not particularly sensitive to initialization, we set
u, v randomly and optimize with standard conjugate gradient descent5 . Due to space constraints we
do not include a pseudo-code implementation of CODA. The implementation is essentially identical
to that of SEDA (Algorithm 1) where the above optimization problem is solved instead of eq. (4) in
line 3. In line 5, we move inputs that one classiﬁer is conﬁdent about while the other one is uncertain
to the training set L to improve the classiﬁer in future iterations.

4 Results

We evaluate our algorithm together with several other domain adaptation algorithms on the “Amazon
reviews” benchmark data sets [6]. The data set contains reviews of four different types of products:
books, DVDs, electronics, and kitchen appliances from Amazon.com. In the original dataset, each
review is associated with a rating of 1-5 stars. For simplicity, we are only concerned about whether
or not a review is positive (higher than 3 stars) or negative (3 stars or lower). That is, yi = {+1, −1},
where yi = 1 indicates that it is a positive review, and −1 otherwise. The data from four domains
results in 12 directed adaptation tasks (e.g. books → dvds). Each domain adaptation task consists
of 2, 000 labeled source inputs and around 4, 000 unlabeled target test inputs (varying slightly be-
tween tasks). We let the amount of labeled target data vary from 0 to 1600. For each setting with
target labels we ran 10 experiments with different, randomly chosen, labeled instances. The origi-
nal feature space of unigrams and bigrams is on average approximately 100, 000 dimensions across

4 In our implementation, the 0-1 indicator was replaced by a very steep differentiable sigmoid function, and
τ was set to 0.8 across different experiments.
5We use minimize.m (http://tinyurl.com/minimize-m).

5

different domains. To reduce the dimensionality, we only use features that appear at least 10 times
in a particular domain adaptation task (with approximately 40, 000 features remaining). Further, we
pre-process the data set with standard tf-idf [24] feature re-weighting.

Figure 1: Relative test-error reduction over logistic regression, averaged across all 12 domain adap-
tation tasks, as a function of the target training set size. Left: A comparison of the three algorithms
from section 3. The graph shows clearly that self-training (Self-training vs. Logistic Regression),
feature-selection (SEDA vs. Self-training) and co-training (CODA vs. SEDA), each improve the
accuracy substantially. Right: A comparison of CODA with four state-of-the-art domain adaptation
algorithms. CODA leads to particularly strong improvements under little target supervision.

As a ﬁrst experiment, we compare the three algorithms from Section 3 and logistic regression as a
baseline. The results are in the left plot of ﬁgure 1. For logistic regression, we ignore the difference
between source and target distribution, and train a classiﬁer on the union of both labeled data sets.
We use (cid:96)2 regularization, and set the regularization constant with 5-fold cross-validation. In ﬁgure 1,
all classiﬁcation errors are shown relative to this baseline. Our second baseline is self-training,
which adds self-training to logistic regression – as described in section 3.1. We start with the set
of labeled instances from source and target domain, and gradually add conﬁdent predictions to the
training set from the unlabeled target domain (without regularization). SEDA adds feature selection
to the self-training procedure, as described in section 3.2. We optimize over 100 iterations of self-
training, at which stage the regularization was effectively zero and the classiﬁer converged. For
CODA we replace self-training with pseudo-multi-view co-training, as described in section 3.3.
The left plot in ﬁgure 1 shows the relative classiﬁcation errors of these four algorithms averaged over
all 12 domain adaptation tasks, under varying amounts of target labels. We observe two trends: First,
there are clear gaps between logistic regression, self-training, SEDA, and CODA. From these three
gaps one can conclude that self-training, feature-selection and co-training each lead to substantial
improvements in classiﬁcation error. A second trend is that the relative improvement over logistic
regression reduces as more labeled target data becomes available. This is not surprising, as with
sufﬁcient target labels the task turns into a classical supervised learning problem and the source data
becomes irrelevant.
As a second experiment, we compare CODA against three state-of-the-art domain adaptation algo-
rithms. We refer to these as Coupled, the coupled-subspaces approach [6], EasyAdapt [11], and
EasyAdapt++. [15]. Details about the respective algorithms are provided in section 5. Coupled
subspaces, as described in [6], does not utilize labeled target data and its result is depicted as a
single point. The right plot in ﬁgure 1 compares these algorithms, relative to logistic regression.
Figure 3 shows the individual results on all the 12 adaptation tasks with absolute classiﬁcation error
rates. The error bars show the standard deviation across the 10 runs with different labeled instances.
EasyAdapt and EasyAdapt++, both consistently improve over logistic regression once sufﬁcient tar-
get data is available. It is noteworthy that, on average, CODA outperforms the other algorithms
in almost all settings when 800 labeled target points or less are present. With 1600 labeled target
points all algorithms perform similar to the baseline and additional source data is irrelevant. All
hyper-parameters of competing algorithms were carefully set by 5-fold cross validation.
Concerning computational requirements, it is fair to say that CODA is signiﬁcantly slower than the
other algorithms, as each iteration is of comparable complexity as logistic regression or EasyAdapt.

6

05010020040080016000.70.750.80.850.90.9511.05Relative Test ErrorNumber of target labeled data  Logistic RegressionSelf−trainingSEDACODA05010020040080016000.750.80.850.90.9511.051.11.15Relative Test ErrorNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODAFigure 2: The ratio of the average number of used features between source and target inputs (9),
tracked throughout the CODA optimization. The three plots show the same statistic at different
amounts of target labels. Initially, an input from the source domain has on average 10-35% more
features that are used by the classiﬁer than a target input. At around iteration 40, this relation changes
and the classiﬁer uses more target-typical features. The graph shows the geometric mean across all
adaptation tasks. With no target data available (left plot), the early spike in source dominance is
more pronounced and decreases when more target labels are available (middle and right plot).

In typical domain adaptation settings this is generally not a problem, as training sets tend to be small.
In our experiments, the average training time for CODA6 was about 20 minutes.
Finally, we investigate the feature-selection process during CODA training. Let us deﬁne the indi-
cator function δ(a) ∈ {0, 1} to be δ(a) = 0 if and only if a = 0, which operates element-wise on
vectors. The vector δ(w) ∈ {0, 1}d indicates which features are used in the classiﬁer and δ(xi ) in-
dicates which features are present in input xi . We can denote the ratio between the average number
(cid:80)
of used features in labeled training inputs over those in unlabeled target inputs as
(cid:80)
δ(w)(cid:62) δ(xs )
1|Dl
xs∈Dl
S |
S
δ(w)(cid:62) δ(xt )
1|Dl
xt∈Dl
T |
T
Figure 2 shows the plot of r(w) for all weight vectors during the 100 iterations of CODA, averaged
across all 12 data sets. The three plots show the same statistic under varying amounts of target
labels. Two trends can be observed: First, during CODA training, the classiﬁer initially selects
more source-speciﬁc features. For example in the case with zero labeled target data, during early
iterations the average source input contains 20 − 35% more used features relative to target inputs.
This source-heavy feature distribution changes and eventually turns into target-heavy distribution as
the classiﬁer adapts to the target domain. As a second trend, we observe that with more target labels
(right plot), this spike in source features is much less pronounced whereas the ﬁnal target-heavy
ratio is unchanged but starts earlier. This indicates that as the target labels increase, the classiﬁer
makes less use of the source data and relies sooner and more directly on the target signal.

r(w) =

.

(9)

5 Related Work and Discussion

Domain adaptation algorithms that do not use labeled target domain data are sometimes called un-
supervised adaptation algorithms. There are roughly three types of algorithms in this group. The
ﬁrst type, which includes the coupled subspaces algorithm of Blitzer et al. [5], learns a shared rep-
resentation under which the source and target distributions are closer than under the ambient feature
space [28]. The largest disadvantage of these algorithms is that they do not jointly optimize the
predictor and the representation, which prevents them from focusing on those features which are
both different and predictive. By jointly optimizing the feature selection, the multi-view split and
the prediction, CODA allows us to do both.
The second type of algorithm attempts to directly minimize the divergence between domains, typ-
ically by weighting individual instances [14, 16, 18]. These algorithms do not assume highly di-
vergent domains (e.g.
those with unique target features), but they have the advantage over both
CODA and representation-learning of learning asymptotically optimal target predictors from only

6We used a straight-forward MatlabT M implementation.

7

204060801000.911.11.21.31.4IterationsRatio of used featuresSource heavyTarget heavy204060801000.911.11.21.31.4IterationsRatio of used features204060801000.911.11.21.31.4IterationsRatio of used featuresSource heavyTarget heavySource heavyTarget heavy0 target labels400 target labels1600 target labelsRatio of used features (source/target)r(w)Ratio of used features (source/target)Figure 3: The individual results on all domain adaptation tasks under varying amounts of labeled
target data. The graphs show the absolute classiﬁcation error rates. All settings with existing labeled
target data were averaged over 10 runs (with randomly selected labeled instances). The vertical bars
indicate the standard deviation in these cases.

source training data (when their assumptions hold). We did not explore them here because their
assumptions are clearly violated for this data set.
In natural language processing, a ﬁnal type of very successful algorithm self-trains on its own target
predictions to automatically annotate new target domain features [19]. These methods are most
closely related, in spirit, to our own CODA algorithm. Indeed, our self-training baseline is intended
to mimic this style of algorithm.
The ﬁnal set of domain adaptation algorithms, which we compared against but did not describe, are
those which actively seek to minimize the labeling divergence between domains using multi-task
techniques [1, 8, 9, 12, 21, 27]. Most prominently, Daum ´e [11] trains separate source and target
models, but regularizes these models to be close to one another. The EasyAdapt++ variant of this
algorithm, which we compared against, generalizes this to the semi-supervised setting by making the
assumption that for unlabeled target instances, the tasks should be similar. Although these methods
did not signiﬁcantly out-perform our baselines in the sentiment data set, we note that there do exist
data sets on which such multi-task techniques are especially important [11], and we hope soon to
explore combinations of CODA with multi-task learning on those data sets.

8

05010020040080016000.10.150.20.250.30.35Test ErrorDvd −> BooksNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorElectronics −> BooksNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorKitchen −> BooksNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorBooks −> DvdNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.20.250.30.35Test ErrorElectronics −> DvdNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorKitchen −> DvdNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorBooks −> ElectronicsNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorDvd −> ElectronicsNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.050.10.150.20.250.3Test ErrorKitchen −> ElectronicsNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.250.30.35Test ErrorBooks −> KitchenNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.10.150.20.25Test ErrorDvd −> KitchenNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODA05010020040080016000.080.10.120.140.160.18Test ErrorElectronics −> KitchenNumber of target labeled data  Logistic RegressionCoupledEasyAdaptEasyAdapt++CODAIn Conference on

References
[1] R.K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-
beled data. The Journal of Machine Learning Research, 6:1817–1853, 2005.
[2] M.F. Balcan, A. Blum, and K. Yang. Co-training and expansion: Towards bridging theory and practice.
NIPS, 17:89–96, 2004.
[3] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and Jenn Wortman. A theory of learning
from different domains. Machine Learning, 2009.
[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adapta-
tion for sentiment classiﬁcation. In Association for Computational Linguistics, Prague, Czech Republic,
2007.
[5] J. Blitzer, D. Foster, and S. Kakade. Domain adaptation with coupled subspaces.
Artiﬁcial Intelligence and Statistics, Fort Lauterdale, 2011.
[6] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages
120–128. Association for Computational Linguistics, 2006.
[7] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning theory, page 100. ACM, 1998.
[8] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.
[9] O. Chapelle, P. Shivaswamy, S. Vadrevu, K.Q. Weinberger, Y. Zhang, and B. Tseng. Multi-task learning
for boosting with application to web search ranking. In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining, KDD ’10, pages 1189–1198, New York, NY,
USA, 2010. ACM.
[10] M. Chen, K.Q. Weinberger, and Y. Chen. Automatic Feature Decomposition for Single View Co-training.
In International Conference on Machine Learning, 2011.
[11] H. Daume III. Frustratingly easy domain adaptation. In Association for Computational Linguistics, 2007.
[12] T. Evgeniou, C.A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of
Machine Learning Research, 6(1):615, 2006.
[13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Verlag, New
York, 2009.
[14] J. Huang, A.J. Smola, A. Gretton, K. M. Borgwardt, and B. Scholkopf. Correcting sample selection bias
by unlabeled data. In NIPS 19, pages 601–608. MIT Press, Cambridge, MA, 2007.
[15] H. Daume III, A. Kumar, and A. Saha. Co-regularization based semi-supervised domain adaptation. In
NIPS 23, pages 478–486. MIT Press, 2010.
[16] J. Jiang and C.X. Zhai. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June
2007. Association for Computational Linguistics.
[17] Qian Liu, Aaron Mackey, David Roos, and Fernando Pereira. Evigan: a hidden variable model for
integrating gene evidence for eukaryotic gene prediction. Bioinformatics, 2008.
[18] T. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In NIPS 21,
pages 1041–1048. MIT Press, 2009.
[19] D. McClosky, E. Charniak, and M. Johnson. Reranking and self-training for parser adaptation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 337–344. Association for Computational Lin-
guistics, 2006.
[20] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In Proceedings
of the ninth international conference on Information and knowledge management, pages 86–93. ACM,
2000.
[21] S. Parameswaran and K.Q. Weinberger. Large margin multi-task metric learning.
1867–1875. 2010.
[22] J.C. Platt et al. Probabilities for sv machines. NIPS, pages 61–74, 1999.
[23] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. Computer
Vision–ECCV 2010, pages 213–226, 2010.
[24] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information processing
& management, 24(5):513–523, 1988.
[25] S. Satpal and S. Sarawagi. Domain adaptation of conditional probability models via feature subsetting.
Knowledge Discovery in Databases: PKDD 2007, pages 224–235, 2007.
[26] B. Settles. Active learning literature survey. Machine Learning, 15(2):201–221, 1994.
[27] K.Q. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg. Feature hashing for large scale
multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 1113–1120. ACM, 2009.
[28] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged plsa for cross-domain text classication. In SIGIR,
2008.

In NIPS 23, pages

9


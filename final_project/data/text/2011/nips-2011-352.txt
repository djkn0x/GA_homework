k -NN Regression Adapts to Local Intrinsic Dimension

Samory Kpotufe
Max Planck Institute for Intelligent Systems
samory@tuebingen.mpg.de

Abstract

Many nonparametric regressors were recently shown to converge at rates that de-
pend only on the intrinsic dimension of data. These regressors thus escape the
curse of dimension when high-dimensional data has low intrinsic dimension (e.g.
a manifold). We show that k-NN regression is also adaptive to intrinsic dimen-
sion. In particular our rates are local to a query x and depend only on the way
masses of balls centered at x vary with radius.
Furthermore, we show a simple way to choose k = k(x) locally at any x so as to
nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension
in the vicinity of x. We also establish that the minimax rate does not depend on a
particular choice of metric space or distribution, but rather that this minimax rate
holds for any metric space and doubling measure.

1 Introduction

We derive new rates of convergence in terms of dimension for the popular approach of Nearest
Neighbor (k-NN) regression. Our motivation is that, for good performance, k-NN regression can
require a number of samples exponential in the dimension of the input space X . This is the so-called
“curse of dimension”. Formally stated, the curse of dimension is the fact that, for any nonparametric
regressor there exists a distribution in RD such that, given a training size n, the regressor converges
(cid:0)1=O(D) (see e.g. [1, 2]).
at a rate no better than n
Fortunately it often occurs that high-dimensional data has low intrinsic dimension: typical examples
are data lying near low-dimensional manifolds [3, 4, 5]. We would hope that in these cases non-
parametric regressors can escape the curse of dimension, i.e. their performance should only depend
on the intrinsic dimension of the data, appropriately formalized. In other words, if the data in RD
(cid:0)1=O(d)
has intrinsic dimension d << D , we would hope for a better convergence rate of the form n
(cid:0)1=O(D) . This has recently been shown to indeed be the case for methods such as kernel
instead of n
regression [6], tree-based regression [7] and variants of these methods [8]. In the case of k-NN
regression however, it is only known that 1-NN regression (where k = 1) converges at a rate that de-
pends on intrinsic dimension [9]. Unfortunately 1-NN regression is not consistent. For consistency,
it is well known that we need k to grow as a function of the sample size n [10] .
Our contributions are the following. We assume throughout that the target function f is Lipschitz.
First we show that, for a wide range of values of k ensuring consistency, k-NN regression converges
at a rate that only depends on the intrinsic dimension in a neighborhood of a query x. Our local
notion of dimension in a neighborhood of a point x relies on the well-studied notion of doubling
(cid:0)
(cid:1)
measure (see Section 2.3). In particular our dimension quantiﬁes how the mass of balls vary with
radius, and this captures standard examples of data with low intrinsic dimension. Our second, and
perhaps most important contribution, is a simple procedure for choosing k = k(x) so as to nearly
(cid:0)2=(2+d)
achieve the minimax rate of O
in terms of the unknown dimension d in a neighborhood
n
of x. Our ﬁnal contribution is in showing that this minimax rate holds for any metric space and
doubling measure. In other words the hardness of the regression problem is not tied to a particular

1

(cid:0)
(cid:1)
(cid:0)
(cid:1)
choice of metric space X or doubling measure (cid:22), but depends only on how the doubling measure (cid:22)
expands on a metric space X . Thus, for any marginal (cid:22) on X with expansion constant (cid:2)
, the
2d
minimax rate for the measure space (X ; (cid:22)) is (cid:10)
(cid:0)2=(2+d)
.
n

1.1 Discussion

It is desirable to express regression rates in terms of a local notion of dimension rather than a global
one because the complexity of data can vary considerably over regions of space. Consider for ex-
ample a dataset made up of a collection of manifolds of various dimensions. The global complexity
is necessarily of a worst case nature, i.e. is affected by the most complex regions of the space while
we might happen to query x from a less complex region. Worse, it can be the case that the data
is not complex locally anywhere, but globally the data is more complex. A simple example of this
is a so-called space ﬁlling curve where a low-dimensional manifold curves enough that globally it
seems to ﬁll up space. We will see that the global complexity does not affect the behavior of k-NN
regression, provided k=n is sufﬁciently small. The behavior of k-NN regression is rather controlled
by the often smaller local dimension in a neighborhood B (x; r) of x, where the neighborhood size
r shrinks with k=n.
Given such a neighborhood B (x; r) of x, how does one choose k = k(x) optimally relative to the
unknown local dimension in B (x; r)? This is nontrivial as standard methods of (global) parameter
selection do not easily apply. For instance, it is unclear how to choose k by cross-validation over
possible settings: we do not know reliable surrogates for the true errors at x of the various estimators
ffn;k (x)g
k2[n] . Another possibility is to estimate the dimension of the data in the vicinity of x, and
use this estimate to set k . However, for optimal rates, we have to estimate the dimension exactly and
(cid:0)
(cid:1)
we know of no ﬁnite sample result that guarantees the exact estimate of intrinsic dimension. Our
over all x 2 X , a near optimal rate of eO
method consists of ﬁnding a value of k that balances quantities which control estimator variance and
bias at x, namely 1=k and distances to x’s k nearest neighbors. The method guarantees, uniformly
(cid:0)2=(2+d)
where d = d(x) is exactly the unknown local
n
dimension on a neighborhood B (x; r) of x, where r ! 0 as n ! 1.

2 Setup
We are given n i.i.d samples (X; Y) = f(Xi ; Yi )gn
i=1 from some unknown distribution where the
input variable X belongs to a metric space (X ; (cid:26)), and the output Y is a real number. We assume
that the class B of balls on (X ; (cid:26)) has ﬁnite VC dimension VB . This is true for instance for any
subset X of a Euclidean space, e.g. the low-dimensional spaces discussed in Section 2.3. The VC
assumption is however irrelevant to the minimax result of Theorem 3.
We denote the marginal distribution on X by (cid:22) and the empirical distribution on X by (cid:22)n .

2.1 Regression function and noise
The regression function f (x) = E [Y jX = x] is assumed to be (cid:21)-Lipschitz, i.e. there exists (cid:21) > 0
such that 8x; x
0 2 X , jf (x) (cid:0) f (x
)j (cid:20) (cid:21)(cid:26) (x; x
0
0
).
We assume a simple but general noise model: the distributions of the noise at points x 2 X have
uniformly bounded tails and variance. In particular, Y is allowed to be unbounded. Formally:
8(cid:14) > 0 there exists t > 0 such that sup
PY jX=x (jY (cid:0) f (x)j > t) (cid:20) (cid:14):
x2X
We denote by tY ((cid:14)) the inﬁmum over all such t. Also, we assume that the variance of (Y jX = x)
Y uniformly over all x 2 X .
is upper-bounded by a constant (cid:27)2
To illustrate our noise assumptions, consider for instance the standard assumption of bounded noise,
i.e. jY (cid:0) f (x)j is uniformly bounded by some M > 0; then 8(cid:14) > 0, tY ((cid:14)) (cid:20) M , and can thus be
p
replaced by M in all our results. Another standard assumption is that where the noise distribution
has exponentially decreasing tail; in this case 8(cid:14) > 0, tY ((cid:14)) (cid:20) O(ln 1=(cid:14)). As a last example, in the
case of Gaussian (or sub-Gaussian) noise, it’s not hard to see that 8(cid:14) > 0, tY ((cid:14)) (cid:20) O(
ln 1=(cid:14)).

2

2.2 Weighted k-NN regression estimate
We assume a kernel function K : R+ 7! R+ , non-increasing, such that K (1) > 0, and K ((cid:26)) = 0
for (cid:26) > 1. For x 2 X , let rk;n (x) denote the distance to its k’th nearest neighbor in the sample X.
X
X
P
The regression estimate at x given the n-sample (X; Y) is then deﬁned as
K ((cid:26)(x; xi )=rk;n (x))
j K ((cid:26)(x; xj )=rk;n (x))
i
i

wi;k (x)Yi :

fn;k (x) =

Yi =

2.3 Notion of dimension

We start with the following deﬁnition of doubling measure which will lead to the notion of local
dimension used in this work. We stay informal in developing the motivation and refer the reader to
[?, 11, 12] for thorough overviews of the topic of metric space dimension and doubling measures.
Deﬁnition 1. The marginal (cid:22) is a doubling measure if there exist Cdb > 0 such that for any x 2 X
and r (cid:21) 0, we have (cid:22)(B (x; r)) (cid:20) Cdb(cid:22)(B (x; r=2)). The quantity Cdb is called an expansion
constant of (cid:22).
An equivalent deﬁnition is that, (cid:22) is doubling if there exist C and d such that for any x 2 X , for any
r (cid:21) 0 and any 0 < (cid:15) < 1, we have (cid:22)(B (x; r)) (cid:20) C (cid:15)
(cid:0)d(cid:22)(B (x; (cid:15)r)). Here d acts as a dimension. It
is not hard to show that d can be chosen as log2 Cdb and C as Cdb (see e.g. [?]).
A simple example of a doubling measure is the Lebesgue volume in the Euclidean space Rd . For any
x 2 Rd and r > 0, vol (B (x; r)) = vol (B (x; 1)) rd . Thus vol (B (x; r)) = vol (B (x; (cid:15)r)) = (cid:15)
(cid:0)d
for any x 2 Rd , r > 0 and 0 < (cid:15) < 1. Building upon the doubling behavior of volumes in Rd ,
we can construct various examples of doubling probability measures. The following ingredients are
sufﬁcient. Let X (cid:26) RD be a subset of a d-dimensional hyperplane, and let X satisfy for all balls
B (x; r) with x 2 X , vol (B (x; r) \ X ) = (cid:2)(rd ), the volume being with respect to the containing
hyperplane. Now let (cid:22) be approximately uniform, that is (cid:22) satisﬁes for all such balls B (x; r),
(cid:22)(B (x; r) \ X ) = (cid:2)(vol (B (x; r) \ X )). We then have (cid:22)(B (x; r))=(cid:22)(B (x; (cid:15)r)) = (cid:2)((cid:15)
(cid:0)d ).
Unfortunately a global notion of dimension such as the above deﬁnition of d is rather restrictive as
it requires the same complexity globally and locally. However a data space can be complex globally
and have small complexity locally. Consider for instance a d-dimensional submanifold X of RD ,
and let (cid:22) have an upper and lower bounded density on X . The manifold might be globally complex
but the restriction of (cid:22) to a ball B (x; r); x 2 X , is doubling with local dimension d, provided r is
sufﬁciently small and certain conditions on curvature hold. This is because, under such conditions
(see e.g. the Bishop-Gromov theorem [13]), the volume (in X ) of B (x; r) \ X is (cid:2)(rd ).
The above example motivates the following deﬁnition of local dimension d.
Deﬁnition 2. Fix x 2 X , and r > 0. Let C (cid:21) 1 and d (cid:21) 1. The marginal (cid:22) is (C; d)-homogeneous
)) (cid:20) C (cid:15)
0 (cid:20) r and 0 < (cid:15) < 1.
0
(cid:0)d(cid:22)(B (x; (cid:15)r
0
on B (x; r) if we have (cid:22)(B (x; r
)) for all r
P
The above deﬁnition covers cases other than manifolds. In particular, another space with small local
dimension is a sparse data space X (cid:26) RD where each vector x has at most d non-zero coordinates,
i.e. X is a collection of ﬁnitely many hyperplanes of dimension at most d. More generally suppose
i (cid:25)i(cid:22)i of ﬁnitely many distributions (cid:22)i with potentially differ-
the data distribution (cid:22) is a mixture
ent low-dimensional supports. Then if all (cid:22)i supported on a ball B are (Ci ; d)-homogeneous on B ,
i.e. all have local dimension d on B , then (cid:22) is also (C; d)-homogeneous on B for some C .
We want rates of convergence which hold uniformly over all regions where (cid:22) is doubling. We
therefore also require (Deﬁnition 3) that C and d from Deﬁnition 2 are uniformly upper bounded.
This will be the case in many situations including the above examples.
Deﬁnition 3. The marginal (cid:22) is (C0 ; d0 )-maximally-homogeneous for some C0 (cid:21) 1 and d0 (cid:21) 1,
if the following holds for all x 2 X and r > 0: suppose there exists C (cid:21) 1 and d (cid:21) 1 such that (cid:22) is
(C; d)-homogeneous on B (x; r), then (cid:22) is (C0 ; d0 )-homogeneous on B (x; r).

We note that, rather than assuming as in Deﬁnition 3 that all local dimensions are at most d0 , we
can express our results in terms of the subset of X where local dimensions are at most d0 . In this
case d0 would be allowed to grow with n. The less general assumption of Deﬁnition 3 allows for a
clearer presentation which still captures the local behavior of k-NN regression.

3

3 Overview of results

3.1 Local rates for ﬁxed k
The ﬁrst result below establishes the rates of convergence for any k & ln n in terms of the (unknown)
complexity on B (x; r) where r is any r satisfying (cid:22)(B (x; r)) > (cid:10)(k=n) (we need at least (cid:10)(k)
samples in the relevant neighborhoods of x).
Theorem 1. Suppose (cid:22) is (C0 ; d0 )-maximally-homogeneous, and B has ﬁnite VC dimension VB .
Let 0 < (cid:14) < 1. With probability at least 1 (cid:0) 2(cid:14) over the choice of (X; Y), the following holds
simultaneously for all x 2 X and k satisfying n > k (cid:21) VB ln 2n + ln(8=(cid:14)).
Pick any x 2 X . Let r > 0 satisfy (cid:22)(B (x; r)) > 3C0k=n. Suppose (cid:22) is (C; d)-homogeneous on
B (x; r), with 1 (cid:20) C (cid:20) C0 and 1 (cid:20) d (cid:20) d0 . We have
(cid:19)2=d
(cid:18)

Y ((cid:14)=2n) (cid:1) ln(2n=(cid:14)) + (cid:27)2
(cid:1) VB (cid:1) t2
jfn;k (x) (cid:0) f (x)j2 (cid:20) 2K (0)
3C k
Y
n(cid:22)(B (x; r))
k
K (1)
Note that the above rates hold uniformly over x, k & ln n, and any r where (cid:22)(B (x; r)) (cid:21) (cid:10)(k=n).
The rate also depends on (cid:22)(B (x; r)) and suggests that the best scenario is that where x has a small
neighborhood of large mass and small dimension d.

+ 2(cid:21)2 r2

:

3.2 Minimax rates for a doubling measure

Our next result shows that the hardness of the regression problem is not tied to a particular choice
of the metric X or the doubling measure (cid:22). The result relies mainly on the fact that (cid:22) is doubling on
X . We however assume that (cid:22) has the same expansion constant everywhere and that this constant
(cid:0)
(cid:1)
is tight. This does not however make the lower-bound less expressive, as it still tells us which rates
to expect locally. Thus if (cid:22) is (C; d)-homogeneous near x, we cannot expect a better rate than
(cid:0)2=(2+d)
(assuming a Lipschitz regression function f ).
O
n
Theorem 2. Let (cid:22) be a doubling measure on a metric space (X ; (cid:26)) of diameter 1, and suppose (cid:22)
satisﬁes, for all x 2 X , for all r > 0 and 0 < (cid:15) < 1,
(cid:0)d(cid:22)(B (x; (cid:15)r)) (cid:20) (cid:22)(B (x; r)) (cid:20) C2 (cid:15)
(cid:0)d(cid:22)(B (x; (cid:15)r));
C1 (cid:15)
where C1 , C2 and d are positive constants independent of x, r , and (cid:15). Let Y be a subset of R and
let (cid:21) > 0. Deﬁne D(cid:22);(cid:21) as the class of distributions on X (cid:2) Y such that X (cid:24) (cid:22) and the output
Y = f (X ) + N (0; 1) where f is any (cid:21)-Lipschitz function from X to Y . Fix a sample size n > 0 and
(cid:12)(cid:12)fnj(X;Y) (x) (cid:0) f (x)
(cid:12)(cid:12)2
let fn denote any regressor on samples (X; Y) of size n, i.e. fn maps any such sample to a function
fnj(X;Y) ((cid:1)) : X 7! Y in L2 ((cid:22)). There exists a constant C independent of n and (cid:21) such that
E X;Y;x
(cid:21) C:
(cid:21)2d=(2+d)n(cid:0)2=(2+d)

infffn g supD(cid:22);(cid:21)

3.3 Choosing k for near-optimal rates at x

Our last result shows a practical and simple way to choose k locally so as to nearly achieve the
minimax rate at x, i.e. a rate that depends on the unknown local dimension in a neighborhood
B (x; r) of x, where again, r satisﬁes (cid:22)(B (x; r)) > (cid:10)(k=n) for good choices of k . It turns out that
(cid:0)1=3 ).
we just need (cid:22)(B (x; r)) > (cid:10)(n
As we will see, the choice of k simply consists of monitoring the distances from x to its nearest
neighbors. The intuition, similar to that of a method for tree-pruning in [7], is to look for a k that
k;n (x)) of the estimate. The
balances the variance (roughly 1=k) and the square bias (roughly r2
procedure is as follows:
(cid:17)
(cid:16)
Choosing k at x: Pick (cid:1) (cid:21) maxi (cid:26) (x; Xi ), and pick (cid:18)n;(cid:14) (cid:21) ln n=(cid:14) .
Let k1 be the highest integer in [n] such that (cid:1)2 (cid:1) (cid:18)n;(cid:14) =k1 (cid:21) r2
k1 ;n (x).
Deﬁne k2 = k1 + 1 and choose k as arg minki ;i2[2]
(cid:18)n;(cid:14) =ki + r2
ki ;n (x)

.

4

;

1 + 4(cid:1)2

where Cn;(cid:14) =

3C (cid:18)n;(cid:14)
n(cid:22)(B (x; r))

The parameter (cid:18)n;(cid:14) guesses how the noise in Y affects the risk. This will soon be clearer. Perfor-
mance guarantees for the above procedure are given in the following theorem.
Theorem 3. Suppose (cid:22) is (C0 ; d0 )-maximally-homogeneous, and B has ﬁnite VC dimension VB .
Assume k is chosen for each x 2 X using the above procedure, and let fn;k (x) be the corresponding
estimate. Let 0 < (cid:14) < 1 and suppose n4=(6+3d0 ) > (VB ln 2n + ln(8=(cid:14))) =(cid:18)n;(cid:14) . With probability at
least 1 (cid:0) 2(cid:14) over the choice of (X; Y), the following holds simultaneously for all x 2 X .
Pick any x 2 X . Let 0 < r < (cid:1) satisfy (cid:22)(B (x; r)) > 6C0n
(cid:19)2=(2+d)
(cid:1) (cid:18)
(cid:19) (cid:0)
(cid:18)
(cid:0)1=3 . Suppose (cid:22) is (C; d)-homogeneous
on B (x; r), with 1 (cid:20) C (cid:20) C0 and 1 (cid:20) d (cid:20) d0 . We have
(cid:0)
(cid:1)
jfn;k (x) (cid:0) f (x)j2 (cid:20)
2Cn;(cid:14)
+ 2(cid:21)2
(cid:18)n;(cid:14)
Y ((cid:14)=2n) (cid:1) ln(2n=(cid:14)) + (cid:27)2
VB (cid:1) t2
K (0)=K (1).
Y
Suppose we set (cid:18)n;(cid:14) = ln2 n=(cid:14) . Then, as per the discussion in Section 2.1, if the noise in Y is
Gaussian, we have t2
Y ((cid:14)=2n) = O(ln n=(cid:14)), and therefore the factor Cn;(cid:14) =(cid:18)n;(cid:14) = O(1). Thus
Y ((cid:14)=2n) (cid:1) ln n=(cid:14)).
ideally we want to set (cid:18)n;(cid:14) to the order of (t2
Just as in Theorem 1, the rates of Theorem 3 hold uniformly for all x 2 X , and all 0 < r < (cid:1)
(cid:0)1=3 ). For any such r , let us call B (x; r) an admissible neighborhood. It is
where (cid:22)(B (x; r)) > (cid:10)(n
0
clear that, as n grows to inﬁnity, w.h.p. any neighborhood B (x; r) of x, 0 < r < supx02X (cid:26) (x; x
),
becomes admissible. Once a neighborhood B (x; r) is admissible for some n, our procedure nearly
attains the minimax rates in terms of the local dimension on B (x; r), provided (cid:22) is doubling on
B (x; r). Again, the mass of an admissible neighborhood affects the rate, and the bound in Theorem
3 is best for an admissible neighborhood with large mass (cid:22)(B (x; r)) and small dimension d.
P
Deﬁne efn;k (x) = EYjX fn;k (x) =
4 Analysis
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12) efn;k (x) (cid:0) f (x)
(cid:12)(cid:12)(cid:12)2
i wi;k (x)f (Xi ). We will bound the error of the estimate at a
point x in a standard way as
jfn;k (x) (cid:0) f (x)j2 (cid:20) 2
Theorem 1 is therefore obtained by combining bounds on the above two r.h.s terms (variance and
bias). These terms are bounded separately in Lemma 2 and Lemma 3 below.

+ 2

:

(1)

4.1 Local rates for ﬁxed k: bias and variance at x

In this section we bound the bias and variance terms of equation (1) with high probability, uniformly
over x 2 X . We will need the following lemma which follows easily from standard VC theory [14]
results. The proof is given in the long version [15].
Lemma 1. Let B denote the class of balls on X , with VC-dimension VB . Let 0 < (cid:14) < 1, and deﬁne
(cid:11)n = (VB ln 2n + ln(8=(cid:14))) =n. The following holds with probability at least 1 (cid:0) (cid:14) for all balls in
B . Pick any a (cid:21) (cid:11)n . Then (cid:22)(B ) (cid:21) 3a =) (cid:22)n (B ) (cid:21) a and (cid:22)n (B ) (cid:21) 3a =) (cid:22)(B ) (cid:21) a.
We start with the bias which is simpler to handle: it is easy to show that the bias of the estimate
at x depends on the radius rk;n (x). This radius can then be bounded, ﬁrst in expectation using the
doubling assumption on (cid:22), then by calling on the above lemma to relate this expected bound to
rk;n (x) with high probability.
Lemma 2 (Bias). Suppose (cid:22) is (C0 ; d0 )-maximally-homogeneous. Let 0 < (cid:14) < 1. With probability
at least 1 (cid:0) (cid:14) over the choice of X, the following holds simultaneously for all x 2 X and k satisfying
n > k (cid:21) VB ln 2n + ln(8=(cid:14)).
Pick any x 2 X . Let r > 0 satisfy (cid:22)(B (x; r)) > 3C0k=n. Suppose (cid:22) is (C; d)-homogeneous on
(cid:19)2=d
(cid:18)
(cid:12)(cid:12)(cid:12) efn;k (x) (cid:0) f (x)
(cid:12)(cid:12)(cid:12)2 (cid:20) (cid:21)2 r2
B (x; r), with 1 (cid:20) C (cid:20) C0 and 1 (cid:20) d (cid:20) d0 . We have:

:

3C k
n(cid:22)(B (x; r))

5

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)X
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:20)
(cid:12)(cid:12)(cid:12) efn;k (x) (cid:0) f (x)
(cid:12)(cid:12)(cid:12) =
X
Proof. First ﬁx X, x 2 X and k 2 [n]. We have:
X
wi;k (x) (f (Xi ) (cid:0) f (x))
i
i
wi;k (x)(cid:21)(cid:26) (Xi ; x) (cid:20) (cid:21)rk;n (x):
(cid:20)
i
We therefore just need to bound rk;n (x). We proceed as follows.
Fix x 2 X and k and pick any r > 0 such that (cid:22)(B (x; r)) > 3C0k=n. Suppose (cid:22) is (C; d)-
homogeneous on B (x; r), with 1 (cid:20) C (cid:20) C0 and 1 (cid:20) d (cid:20) d0 . Deﬁne
(cid:19)1=d
(cid:18)

wi;k (x) jf (Xi ) (cid:0) f (x)j

(2)

;

:
=

(cid:15)

3C k
n(cid:22)(B (x; r))

:

Y ((cid:14)=2n) (cid:1) ln(2n=(cid:14)) + (cid:27)2
(cid:1) VB (cid:1) t2
Y
k

so that (cid:15) < 1 by the bound on (cid:22)(B (x; r)); then by the local doubling assumption on B (x; r),
we have (cid:22)(B (x; (cid:15)r)) (cid:21) C
(cid:0)1 (cid:15)d(cid:22)(B (x; r)) (cid:21) 3k=n. Let (cid:11)n as deﬁned in Lemma 1, and assume
k=n (cid:21) (cid:11)n (this is exactly the assumption on k in the lemma statement). By Lemma 1, it follows that
with probability at least 1 (cid:0) (cid:14) uniform over x, r and k thus chosen, we have (cid:22)n ((B (x; (cid:15)r)) (cid:21) k=n
implying that rk;n (x) (cid:20) (cid:15)r . We then conclude with the lemma statement by using equation (2).
Lemma 3 (Variance). Let 0 < (cid:14) < 1. With probability at least 1 (cid:0) 2(cid:14) over the choice of (X; Y),
the following then holds simultaneously for all x 2 X and k 2 [n]:
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
(cid:12)(cid:12)(cid:12)2 (cid:20) K (0)
K (1)
Proof. First, condition on X ﬁxed. For any x 2 X , k 2 [k ], let Yx;k denote the subset of Y
corresponding to points from X falling in B (x; rk;n (x)). For X ﬁxed, the number of such subsets
Yx;k is therefore at most the number of ways we can intersect balls in B with the sample X; this is
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
(cid:12)(cid:12)(cid:12). We’ll proceed by showing that with high probability, for all
VB as is well-known in VC theory.
in turn upper-bounded by n
:
Let  (Yx;k )
=
x 2 X ,  (Yx;k ) is close to its expectation, then we bound this expectation.
Let (cid:14)0 (cid:20) 1=2n. We further condition on the event Y(cid:14)0 that for all n samples Yi , jYi (cid:0) f (Xi )j (cid:20)
tY ((cid:14)0 ). By deﬁnition of tY ((cid:14)0 ), the event Y(cid:14)0 happens with probability at least 1 (cid:0) n(cid:14)0 (cid:21) 1=2 . It
follows that for any x 2 X
 (Yx;k ) (cid:21) 1
E  (Yx;k ) (cid:21) P (Y(cid:14)0 ) (cid:1) E
E
 (Yx;k );
Y(cid:14)0
Y(cid:14)0
2
(cid:19)
(cid:18)
[(cid:1)] denote conditional expectation under the event. Let (cid:15) > 0, we in turn have
where EY(cid:14)0
(cid:19)
(cid:18)
9x; k ;  (Yx;k ) > E
P (9x; k ;  (Yx;k ) > 2E  (Yx;k ) + (cid:15)) (cid:20) P
 (Yx;k ) + (cid:15)
Y(cid:14)0
9x; k ;  (Yx;k ) > E
(cid:20) PY(cid:14)0
+ n(cid:14)0 :
Y(cid:14)0
This last probability can be bounded by applying McDiarmid’s inequality: changing any Yi value
changes  (Yx;k ) by at most wi;k (cid:1) tY ((cid:14)0 ) when we condition on the event Y(cid:14)0 . This, followed by a
)
(
(cid:19)
(cid:18)
X
union-bound yields
(cid:0)2(cid:15)2 =t2
9x; k ;  (Yx;k ) > E
Y ((cid:14)0 )
Y(cid:14)0
i

 (Yx;k ) + (cid:15)

 (Yx;k ) + (cid:15)

PY(cid:14)0

(cid:20) n

VB exp

w2
i;k

:

6

)

!

w2
i;k

;

!
w2
i;k

(
X
Combining with the above we get
(cid:0)2(cid:15)2 =t2
P (9x 2 X ;  (Yx;k ) > 2E  (Yx;k ) + (cid:15)) (cid:20) n
VB exp
w2
Y ((cid:14)0 )
+ n(cid:14)0 :
i;k
 
(cid:18)
(cid:12)(cid:12)(cid:12)(cid:19)2
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
(cid:12)(cid:12)(cid:12)2 (cid:20) 8
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
i
X
In other words, let (cid:14)0 = (cid:14)=2n, with probability at least 1 (cid:0) (cid:14) , for all x 2 X and k 2 [n]
 
VB ln(2n=(cid:14))
(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
E
X
+ t2
Y ((cid:14)=2n)
YjX
i
VB ln(2n=(cid:14))
(cid:20) 8 E
+ t2
Y ((cid:14)=2n)
YjX
i
the fact that for i.i.d random variables zi with zero mean, E jP
P
where the second inequality is an application of Jensen’s.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)X
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
We bound the above expectation on the r.h.s. next. In what follows (second equality below) we use
(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
i zi j2 =
E jzi j2 . We have
i
X
X
wi;k (x) (Yi (cid:0) f (Xi ))
= E
E
YjX
YjX
i
jYi (cid:0) f (Xi )j2 (cid:20)
i;k (x) E
w2
=
YjX
i
i
(cid:12)(cid:12)(cid:12)fn;k (x) (cid:0) efn;k (x)
(cid:12)(cid:12)(cid:12)2 (cid:20) (cid:0)VB (cid:1) t2
X
(cid:1) (cid:1)
Combining with the previous bound we get that, with probability at least 1 (cid:0) (cid:14) , for all x and k ,
Y ((cid:14)=2n) (cid:1) ln(2n=(cid:14)) + (cid:27)2
P
Y
i
X
P
P
i;k (x) as follows:
We can now bound
i w2
i;k (x) (cid:20) max
K ((cid:26)(x; xi )=rk;n (x))
K (0)
w2
P
i2[n]
j K ((cid:26)(x; xj )=rk;n (x))
j K ((cid:26)(x; xj )=rk;n (x))
i
(cid:20) K (0)
(cid:20)
K (0)
xj 2B (x;rk;n (x)) K ((cid:26)(x; xj )=rk;n (x))
K (1)k
Plug this back into equation 3 and conclude.

wi;k (x) = max
i2[n]

i;k (x)(cid:27)2
w2
Y :

(cid:20)

:

w2
i;k (x):

(3)

4.2 Minimax rates for a doubling measure

The minimax rates of theorem 2 (proved in the long version [15]) are obtained as is commonly
done by constructing a regression problem that reduces to the problem of binary classiﬁcation (see
e.g. [1, 2, 10]). Intuitively the problem of classiﬁcation is hard in those instances where labels (say
(cid:0)1; +1) vary wildly over the space X , i.e. close points can have different labels. We make the
regression problem similarly hard. We will consider a class of candidate regression functions such
that each function f alternates between positive and negative in neighboring regions (f is depicted
as the dashed line below).

The reduction relies on the simple observation that for a regressor fn to approximate the right f
from data it needs to at least identify the sign of f in the various regions of space. The more we can
make each such f change between positive and negative, the harder the problem. We are however
constrained in how much f changes since we also have to ensure that each f is Lipchitz continuous.

7

+−4.3 Choosing k for near-optimal rates at x
(cid:18)
(cid:19)2=(2+d)
(cid:18)
(cid:19)1=d
Proof of Theorem 3. Fix x and let r; d; C as deﬁned in the theorem statement. Deﬁne
(cid:1)

and (cid:15)

:
=

:
= (cid:18)d=(2+d)
n;(cid:14)

(cid:20)

:

n(cid:22)(B (x; r))
3C

3C (cid:20)
n(cid:22)(B (x; r))

Note that, by our assumptions,

:

(4)

= (cid:1)2

(cid:22)(B (x; r)) > 6C (cid:18)n;(cid:14) n

(cid:0)1=3 (cid:21) 6C (cid:18)n;(cid:14) n
(cid:21) 6C
(cid:0)d=(2+d) = 6C (cid:18)n;(cid:14)
n2=(2+d)
(cid:20)
n
n
The above equation (4) implies (cid:15) < 1. Thus, by the homogeneity assumption on B (x; r),
(cid:22)(B (x; (cid:15)r)) (cid:21) C
(cid:0)1 (cid:15)d(cid:22)(B (x; r)) (cid:21) 3(cid:20)=n. Now by the ﬁrst inequality of (4) we also have
n4=(6+3d0 ) (cid:21) (cid:11)n ;
n4=(6+3d) (cid:21) (cid:18)n;(cid:14)
(cid:21) (cid:18)n;(cid:14)
(cid:20)
n
n
n
where (cid:11)n = (VB ln 2n + ln(8=(cid:14))) =n is as deﬁned in Lemma 1. We can thus apply Lemma 1 to
have that, with probability at least 1 (cid:0) (cid:14) , (cid:22)n (B (x; (cid:15)r)) (cid:21) (cid:20)=n. In other words, for any k (cid:20) (cid:20),
(cid:19)2=d (cid:21) ((cid:15)r)2 (cid:21) r2
(cid:18)
rk;n (x) (cid:20) (cid:15)r . It follows that if k (cid:20) (cid:20),
(cid:21) (cid:1)2 (cid:1) (cid:18)n;(cid:14)
(cid:1)2 (cid:1) (cid:18)n;(cid:14)
3C (cid:20)
k;n (x):
n(cid:22)(B (x; r))
k
(cid:20)
Remember that the above inequality is exactly the condition on the choice of k1 in the theorem
statement. Therefore, suppose k1 (cid:20) (cid:20), it must be that k2 > (cid:20) otherwise k2 is the highest integer
satisfying the condition, contradicting our choice of k1 . Thus we have (i) (cid:18)n;(cid:14) =k2 < (cid:18)n;(cid:14) =(cid:20) = (cid:15)2 :
We also have (ii) rk2 ;n (x) (cid:20) 21=d (cid:15)r . To see this, notice that since k1 (cid:20) (cid:20) < k2 = k1 + 1 we have
k2 (cid:20) 2(cid:20); now by repeating the sort of argument above, we have (cid:22)(B (x; 21=d (cid:15)r)) (cid:21) 6(cid:20)=n which by
Lemma 1 implies that (cid:22)n (B (x; 21=d (cid:15)r)) (cid:21) 2(cid:20)=n (cid:21) k2 =n.
Now suppose instead that k1 > (cid:20), then by deﬁnition of k1 , we have (iii)
rk1 ;n (x)2 (cid:20) (cid:1)2 (cid:1) (cid:18)n;(cid:14)
(cid:20) (cid:1)2 (cid:1) (cid:18)n;(cid:14)
= ((cid:1)(cid:15))2 :
k1
(cid:20)
(cid:1) (cid:18)
(cid:19)
(cid:18)
(cid:19)2=(2+d)
The following holds by (i), (ii), and (iii). Let k be chosen as in the theorem statement. Then, whether
(cid:20) (cid:0)
(cid:1)
(cid:0)
k1 > (cid:20) or not, it is true that
3C (cid:18)n;(cid:14)
(cid:18)n;(cid:14)
1 + 4(cid:1)2
(cid:15)2 =
1 + 4(cid:1)2
+ r2
k;n (x)
:
n(cid:22)(B (x; r))
k
Now combine Lemma 3 with equation (2) and we have that with probability at least 1 (cid:0) 2(cid:14) (account-
(cid:18)
(cid:19) (cid:18)
(cid:19)
ing for all events discussed)
(cid:19)2=(2+d)
(cid:1) (cid:18)
(cid:19) (cid:0)
(cid:18)
jfn;k (x) (cid:0) f (x)j2 (cid:20) 2Cn;(cid:14)
k;n (x) (cid:20)
(cid:18)n;(cid:14)
+ 2(cid:21)2
+ 2(cid:21)2 r2
+ r2
k;n (x)
k
(cid:18)n;(cid:14)

2Cn;(cid:14)
(cid:18)n;(cid:14)

(cid:18)n;(cid:14)
k

3C (cid:18)n;(cid:14)
n(cid:22)(B (x; r))

:

(cid:20)

2Cn;(cid:14)
(cid:18)n;(cid:14)

+ 2(cid:21)2

1 + 4(cid:1)2

5 Final remark

The problem of choosing k = k(x) optimally at x is similar to the problem of local bandwidth
selection for kernel-based methods (see e.g. [16, 17]), and our method for choosing k might yield
insights into bandwidth selection, since k-NN and kernel regression methods only differ in their
notion of neighborhood of a query x.

Acknowledgments

I am grateful to David Balduzzi for many useful discussions.

8

References
[1] C. J. Stone. Optimal rates of convergence for non-parametric estimators. Ann. Statist., 8:1348–
1360, 1980.
[2] C. J. Stone. Optimal global rates of convergence for non-parametric estimators. Ann. Statist.,
10:1340–1353, 1982.
[3] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-
ence, 290, 2000.
[4] J. Tenebaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear di-
mensionality reduction. Science, 290, 2000.
[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-
sentation. Neural Computation, 15(6):1373–1396, 2003.
[6] P. Bickel and B. Li. Local polynomial regression on unknown manifolds. Tech. Re. Dep. of
Stats. UC Berkley, 2006.
[7] S. Kpotufe. Escaping the curse of dimensionality with a tree-based regressor. Conference On
Learning Theory, 2009.
[8] S. Kpotufe. Fast, smooth, and adaptive regression in metric spaces. Neural Information Pro-
cessing Systems, 2009.
[9] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under arbitrary
sampling. IEEE Transactions on Information Theory, 41, 1995.
[10] L. Gyorﬁ, M. Kohler, A. Krzyzak, and H. Walk. A Distribution Free Theory of Nonparametric
Regression. Springer, New York, NY, 2002.
[11] C. Cutler. A review of the theory and estimation of fractal dimension. Nonlinear Time Series
and Chaos, Vol. I: Dimension Estimation and Models, 1993.
[12] K. Clarkson. Nearest-neighbor searching and metric space dimensions. Nearest-Neighbor
Methods for Learning and Vision: Theory and Practice, 2005.
[13] M. do Carmo. Riemannian Geometry. Birkhauser, 1992.
[14] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events
to their expectation. Theory of probability and its applications, 16:264–280, 1971.
[15] S. Kpotufe. k-NN regression adapts to local intrinsic dimension. arXiv:1110.4300, 2011.
[16] J. G. Staniswalis. Local bandwidth selection for kernel estimates. Journal of the American
Statistical Association, 84:284–288, 1989.
[17] R. Cao-Abad. Rate of convergence for the wild bootstrap in nonpara- metric regression. Annals
of Statistics, 19:2226–2231, 1991.

9


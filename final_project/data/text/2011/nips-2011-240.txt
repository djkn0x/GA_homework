Object Detection with Grammar Models

Ross B. Girshick
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
rbg@cs.uchicago.edu

Pedro F. Felzenszwalb
School of Engineering and
Dept. of Computer Science
Brown University
Providence, RI 02912
pff@brown.edu

David McAllester
TTI-Chicago
Chicago, IL 60637
mcallester@ttic.edu

Abstract

Compositional models provide an elegant formalism for representing the visual
appearance of highly variable objects. While such models are appealing from a
theoretical point of view, it has been difﬁcult to demonstrate that they lead to per-
formance advantages on challenging datasets. Here we develop a grammar model
for person detection and show that it outperforms previous high-performance sys-
tems on the PASCAL benchmark. Our model represents people using a hierar-
chy of deformable parts, variable structure and an explicit model of occlusion for
partially visible objects. To train the model, we introduce a new discriminative
framework for learning structured prediction models from weakly-labeled data.

1

Introduction

The idea that images can be hierarchically parsed into objects and their parts has a long history in
computer vision, see for example [15]. Image parsing has also been of considerable recent interest
[11, 13, 21, 22, 24]. However, it has been difﬁcult to demonstrate that sophisticated compositional
models lead to performance advantages on challenging metrics such as the PASCAL object detection
benchmark [9]. In this paper we achieve new levels of performance for person detection using a
grammar model that is richer than previous models used in high-performance systems. We also
introduce a general framework for learning discriminative models from weakly-labeled data.
Our models are based on the object detection grammar formalism in [11]. Objects are represented
in terms of other objects through compositional rules. Deformation rules allow for the parts of an
object to move relative to each other, leading to hierarchical deformable part models. Structural
variability provides choice between multiple part subtypes — effectively creating mixture models
throughout the compositional hierarchy — and also enables optional parts. In this formalism parts
may be reused both within an object category and across object categories.
Our baseline and departure point is the UoC-TTI object detector [10, 12]. This system represents a
class of objects with three different pictorial structure models. Although these models are learned
automatically, making semantic interpretation unclear, it seems that the three components for the
person class differ in how much of the person is taken to be visible — just the head and shoulders,
the head and shoulders together with the upper body, or the whole standing person. Each of the three
components has independently trained parts. For example, each component has a head part trained
independently from the head part of the other components.
Here we construct a single grammar model that allows more ﬂexibility in describing the amount of
the person that is visible. The grammar model avoids dividing the training data between different
components and thus uses the training data more efﬁciently. The parts in the model, such as the
head part, are shared across different interpretations of the degree of visibility of the person. The
grammar model also includes subtype choice at the part level to accommodate greater appearance

1

variability across object instances. We use parts with subparts to beneﬁt from high-resolution image
data, while also allowing for deformations. Unlike previous approaches, we explicitly model the
source of occlusion for partially visible objects.
Our approach differs from that of Jin and Geman [13] in that theirs focuses on whole scene inter-
pretation with generative models, while we focus on discriminatively trained models of individual
objects. We also make Markovian restrictions not made in [13]. Our work is more similar to that of
Zhu et al. [21] who impose similar Markovian restrictions. However, our training method, image
features, and grammar design are substantially different.
The model presented here is designed to accurately capture the visible portion of a person. There
has been recent related work on occlusion modeling in pedestrian and person images [7, 18]. In
[7], Enzweiler et al. assume access to depth and motion information in order to estimate occlusion
boundaries. In [18], Wang et al. rely on the observation that the scores of individual ﬁlter cells (using
the Dalal and Triggs detector [5]) can reliably predict occlusion in the INRIA pedestrian data. This
does not hold for the harder PASCAL person data.
In addition to developing a grammar model for detecting people, we develop new training methods
which contribute to our boost in performance. Training data for vision is often assigned weak labels
such as bounding boxes or just the names of objects occurring in the image. In contrast, an image
analysis system will often produce strong predictions such as a segmentation or a pose. Existing
structured prediction methods, such as structural SVM [16, 17] and latent structural SVM [19], do
not directly support weak labels together with strong predictions. We develop the notion of a weak-
label structural SVM which generalizes structural SVMs and latent-structural SVMs. The key idea
is to introduce a loss L(y , s) for making a strong prediction s when the weak training label is y .
A formalism for learning from weak labels was also developed in [2]. One important difference is
that [2] generalizes ranking SVMs.1 Our framework also allows for softer relations between weak
labels and strong predictions.

2 Grammar models
Object detection grammars [11] represent objects recursively in terms of other objects. Let N be a
set of nonterminal symbols and T be a set of terminal symbols. We can think of the terminals as the
basic building blocks that can be found in an image. The nonterminals deﬁne abstract objects whose
appearance are deﬁned in terms of expansions into terminals.
Let Ω be a set of possible locations for a symbol within an image. A placed symbol, Y (ω), speciﬁes
a placement of Y ∈ N ∪ T at a location ω ∈ Ω. The structure of a grammar model is deﬁned by a
set, R, of weighted productions of the form
s−→ { Y1 (ω1 ), . . . , Yn (ωn ) },
(1)
X (ω0 )
where X ∈ N , Yi ∈ N ∪ T , ωi ∈ Ω and s ∈ R is a score. We denote the score of r ∈ R by s(r).
We can expand a placed nonterminal to a bag of placed terminals by repeatedly applying produc-
tions. An expansion of X (ω) leads to a derivation tree T rooted at X (ω). The leaves of T are
labeled with placed terminals, and the internal nodes of T are labeled with placed nonterminals and
with the productions used to replace those symbols.
We deﬁne appearance models for the terminals using a function score(A, ω) that computes a score
for placing the terminal A at location ω . This score depends implicitly on the image data. We deﬁne
(cid:88)
(cid:88)
the score of a derivation tree T to be the sum of the scores of the productions used to generate T ,
plus the score of placing the terminals associated with T ’s leaves in their respective locations.
r∈internal(T )
A(w)∈leaves(T )

score(A, ω)

(2)

score(T ) =

s(r) +

To generalize the models from [10] we let Ω be positions and scales within a feature map pyramid
H . We deﬁne the appearance models for terminals by associating a ﬁlter FA with each terminal A.
1 [2] claims the ranking framework overcomes a loss in performance when the number of background exam-
ples is increased. In contrast, we don’t use a ranking framework but always observed a performance improve-
ment when increasing the number of background examples.

2

Figure 1: Shallow grammar model. This ﬁgure illustrates a shallow version of our grammar model
(Section 2.1). This model has six person parts and an occlusion model (“occluder”), each of which
comes in one of two subtypes. A detection places one subtype of each visible part at a location
and scale in the image. If the derivation does not place all parts it must place the occluder. Parts
are allowed to move relative to each other, but their displacements are constrained by deformation
penalties.
Then score(A, ω) = FA · φ(H, ω) is the dot product between the ﬁlter coefﬁcients and the features
in a subwindow of the feature map pyramid, φ(H, ω). We use the variant of histogram of oriented
gradient (HOG [5]) features described in [10].
We consider models with productions speciﬁed by two kinds of schemas (a schema is a template for
generating productions). A structure schema speciﬁes one production for each placement ω ∈ Ω,
s−→ { Y1 (ω ⊕ δ1 ), . . . , Yn (ω ⊕ δn ) }.
(3)
X (ω)
Here the δi specify constant displacements within the feature map pyramid. Structure schemas can
be used to deﬁne decompositions of objects into other objects.
Let ∆ be the set of possible displacements within a single scale of a feature map pyramid. A
deformation schema speciﬁes one production for each placement ω ∈ Ω and displacement δ ∈ ∆,
α·φ(δ)−→ { Y (ω ⊕ δ) }.
(4)
X (ω)
Here φ(δ) is a feature vector and α is a vector of deformation parameters. Deformation schemas
can be used to deﬁne deformable models. We deﬁne φ(δ) = (dx, dy , dx2 , dy2 ) so that deformation
scores are quadratic functions of the displacements.
The parameters of our models are deﬁned by a weight vector w with entries for the score of each
structure schema, the deformation parameters of each deformation schema and the ﬁlter coefﬁcients
associated with each terminal. Then score(T ) = w · Φ(T ), where Φ(T ) is the sum of (sparse)
feature vectors associated with each placed terminal and production in T .

2.1 A grammar model for detecting people

Each component in the person model learned by the voc-release4 system [12] is tuned to detect
people under a prototypical visibility pattern. Based on this observation we designed, by hand, the
structure of a grammar that models visibility by using structural variability and optional parts. For
clarity, we begin by describing a shallow model (Figure 1) that places all ﬁlters at the same resolution
in the feature map pyramid. After explaining this model, we describe a deeper model that includes
deformable subparts at higher resolutions.
Fine-grained occlusion Our grammar model has a start symbol Q that can be expanded using one
of six possible structure schemas. These choices model different degrees of visibility ranging from
heavy occlusion (only the head and shoulders are visible) to no occlusion at all.
Beyond modeling ﬁne-grained occlusion patterns when compared to the mixture models from [7]
and [12], our grammar model is also richer in a number of ways. In Section 5 we show that each of
the following modeling choices improves detection performance.

3

Parts 1-6 (no occlusion)Parts 1-4 & occluderParts 1-2 & occluderExample detections and derived ﬁltersSubtype 1Subtype 2Part 1Part 2Part 3Part 4Part 5Part 6OccluderOcclusion model If a person is occluded, then there must be some cause of the occlusion — either
the edge of the image or an occluding object, such as a desk or dinner table. We use a nontrivial
model to capture the appearance of the stuff that occludes people.
Part subtypes The mixture model from [12] has two subtypes for each mixture component. The
subtypes are forced to be mirror images of each other and correspond roughly to left-facing versus
right-facing people. Our grammar model has two subtypes for each part, which are also forced to
be mirror images of each other. But in the case of our grammar model, the decision of which part
subtype to instantiate at detection time is independent for each part.
The shallow person grammar model is deﬁned by the following grammar. The indices p (for part), t
(for subtype), and k have the following ranges: p ∈ {1, . . . , 6}, t ∈ {L, R} and k ∈ {1, . . . , 5}.
sk−→ { Y1 (ω ⊕ δ1 ), . . . , Yk (ω ⊕ δk ), O(ω ⊕ δk+1 ) }
Q(ω)
s6−→ { Y1 (ω ⊕ δ1 ), . . . , Y6 (ω ⊕ δ6 ) }
Q(ω)
αp,t ·φ(δ)−→ { Ap,t (ω ⊕ δ) }
0−→ { Yp,t (ω) }
Yp,t (ω)
Yp (ω)
αt ·φ(δ)−→ { At (ω ⊕ δ) }
0−→ { Ot (ω) }
Ot (ω)
O(ω)
The grammar has a start symbol Q with six alternate choices that derive people under varying de-
grees of visibility (occlusion). Each part has a corresponding nonterminal Yp that is placed at some
ideal position relative to Q. Derivations with occlusion include the occlusion symbol O . A derivation
selects a subtype and displacement for each visible part. The parameters of the grammar (production
scores, deformation parameters and ﬁlters) are learned with the discriminative procedure described
in Section 4. Figure 1 illustrates the ﬁlters in the resulting model and some example detections.
Deeper model We extend the shallow model by adding deformable subparts at two scales: (1)
the same as, and (2) twice the resolution of the start symbol Q. When detecting large objects,
high-resolution subparts capture ﬁne image details. However, when detecting small objects, high-
resolution subparts cannot be used because they “fall off the bottom” of the feature map pyramid.
The model uses derivations with low-resolution subparts when detecting small objects.
We begin by replacing the productions from Yp,t in the grammar above, and then adding new pro-
ductions. Recall that p indexes the top-level parts and t indexes subtypes. In the following schemas,
the indices r (for resolution) and u (for subpart) have the ranges: r ∈ {H, L}, u ∈ {1, . . . , Np},
where Np is the number of subparts in a top-level part Yp .
αp,t ·φ(δ)−→
{ Zp,t (ω ⊕ δ) }
0−→
{Ap,t (ω), Wp,t,r,1 (ω ⊕ δp,t,r,1 ), . . . , Wp,t,r,Np (ω ⊕ δp,t,r,Np )}
αp,t,r,u ·φ(δ)
{Ap,t,r,u (ω ⊕ δ)}
−→
Wp,t,r,u (ω)
We note that as in [23] our model has hierarchical deformations. The part terminal Ap,t can move
relative to Q and the subpart terminal Ap,t,r,u can move relative to Ap,t .
The displacements δp,t,H,u place the symbols Wp,t,H,u one octave below Zp,t in the feature map
pyramid. The displacements δp,t,L,u place the symbols Wp,t,L,u at the same scale as Zp,t . We add
subparts to the ﬁrst two top-level parts (p = 1 and 2), with the number of subparts set to N1 = 3
and N2 = 2. We ﬁnd that adding additional subparts does not improve detection performance.

Yp,t (ω)
Zp,t (ω)

2.2 Inference and test time detection

Inference involves ﬁnding high scoring derivations. At test time, because images may contain mul-
tiple instances of an object class, we compute the maximum scoring derivation rooted at Q(ω), for
each ω ∈ Ω. This can be done efﬁciently using a standard dynamic programming algorithm [11].
We retain only those derivations that score above a threshold, which we set low enough to ensure
high recall. We use box(T ) to denote a detection window associated with a derivation T . Given a
set of candidate detections, we apply nonmaximal suppression to produce a ﬁnal set of detections.
We deﬁne box(T ) by assigning a detection window size, in feature map coordinates, to each struc-
ture schema that can be applied to Q. This leads to detections with one of six possible aspect ratios,
depending on which production was used in the ﬁrst step of the derivation. The absolute location
and size of a detection depends on the placement of Q. For the ﬁrst ﬁve production schemas, the
ideal location of the occlusion part, O , is outside of box(T ).

4

E (w) =

3 Learning from weakly-labeled data
Here we deﬁne a general formalism for learning functions from weakly-labeled data. Let X be an
input space, Y be a label space, and S be an output space. We are interested in learning functions
f : X → S based on a set of training examples {(x1 , y1 ), . . . , (xn , yn )} where (xi , yi ) ∈ X × Y .
In contrast to the usual supervised learning setting, we do not assume that the label space and the
output space are the same. In particular there may be many output values that are compatible with
a label, and we can think of each example as being only weakly labeled. It will also be useful to
associate a subset of possible outputs, S (x) ⊆ S , with an example x. In this case f (x) ∈ S (x).
A connection between labels and outputs can be made using a loss function L : Y × S → R. L(y , s)
associates a cost with the prediction s ∈ S on an example labeled y ∈ Y . Let D be a distribution
over X × Y . Then a natural goal is to ﬁnd a function f with low expected loss ED [L(y , f (x))].
A simple example of a weakly-labeled training problem comes from learning sliding window clas-
siﬁers in the PASCAL object detection dataset. The training data speciﬁes pixel-accurate bounding
boxes for the target objects while a sliding window classiﬁer reports boxes with a ﬁxed aspect ratio
and at a ﬁnite number of scales. The output space is, therefore, a subset of the label space.
As usual, we assume f is parameterized by a vector of model parameters w and generates predictions
by maximizing a linear function of a joint feature map Φ(x, s), f (x) = argmaxs∈S (x) w · Φ(x, s).
We can train w by minimizing a regularized risk on the training set. We deﬁne a weak-label struc-
n(cid:88)
tural SVM (WL-SSVM) by the following training equation,
||w||2 + C
L(cid:48) (w, xi , yi ).
1
2
i=1
The surrogate training loss L(cid:48) is deﬁned in terms of two different loss augmented predictions.
(cid:124)
(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:124)
[w · Φ(x, s) − Loutput (y , s)]
− max
[w · Φ(x, s) + Lmargin (y , s)]
L(cid:48) (w, x, y) = max
s∈S (x)
s∈S (x)
(6a)
(6b)
Lmargin encourages high-loss outputs to “pop out” of (6a), so that their scores get pushed down.
Loutput suppresses high-loss outputs in (6b), so the score of a low-loss prediction gets pulled up.
It is natural to take Lmargin = Loutput = L. In this case L(cid:48) becomes a type of ramp loss [4, 6, 14].
Alternatively, taking Lmargin = L and Loutput = 0 gives the ramp loss that has been shown to
be consistent in [14]. As we discuss below, the choice of Loutput can have a signiﬁcant effect on
the computational difﬁculty of the training problem. Several popular learning frameworks can be
derived as special cases of WL-SSVM. For the examples below, let I(a, b) = 0 when a = b, and
I(a, b) = ∞ when a (cid:54)= b.
Structural SVM Let S = Y , Lmargin = L and Loutput (y , ˆy) = I(y , ˆy). Then L(cid:48) (w, x, y) is the
hinge loss used in a structural SVM [17]. In this case L(cid:48) is convex in w because the maximization
in (6b) disappears. We note, however, that this choice of Loutput may be problematic and lead to
inconsistent training problems. Consider the following situation. A training example (x, y) may
be compatible with a different label ˆy (cid:54)= y , in the sense that L(y , ˆy) = 0. But even in this case a
structural SVM pushes the score w · Φ(x, y) to be above w · Φ(x, ˆy). This issue can be addressed
by relaxing Loutput to include a maximization over labels in (6b).
Latent structural SVM Now let Z be a space of latent values, S = Y × Z , Lmargin = L and
Loutput (y , ( ˆy , ˆz )) = I(y , ˆy). Then L(cid:48) (w, x, y) is the hinge loss used in a latent structural SVM [19].
In this case L(cid:48) is not convex in w due to the maximization over latent values in (6b). As in the
previous example, this choice of Loutput can be problematic because it “requires” that the training
labels be predicted exactly. This can be addressed by relaxing Loutput , as in the previous example.

(5)

(6)

4 Training grammar models

Now we consider learning the parameters of an object detection grammar using the training data
in the PASCAL VOC datasets with the WL-SSVM framework. For two rectangles a and b let
overlap(a, b) = area(a ∩ b)/ area(a ∪ b). We will use this measure of overlap in our loss functions.

5

For training, we augment our model’s output space (the set of all derivation trees), with the back-
ground output ⊥. We deﬁne Φ(x, ⊥) to be the zero vector, as was done in [1]. Thus the score of a
background hypothesis is zero independent of the model parameters w.
The training data speciﬁes a bounding box for each instance of an object in a set of training im-
ages. We construct a set of weakly-labeled examples {(x1 , y1 ), . . . , (xn , yn )} as follows. For each
training image I , and for each bounding box B in I , we deﬁne a foreground example (x, y), where
y = B , x speciﬁes the image I , and the set of valid predictions S (x) includes:
1. Derivations T with overlap(box(T ), B ) ≥ 0.1 and overlap(box(T ), B (cid:48) ) < 0.5 for all B (cid:48)
in I such that B (cid:48) (cid:54)= B .
2. The background output ⊥.
The overlap requirements in (1) ensure that we consider only predictions that are relevant for a
particular object instance, while avoiding interactions with other objects in the image.
We also deﬁne a very large set of background examples. For simplicity, we use images that do not
contain any bounding boxes. For each background image I , we deﬁne a different example (x, y) for
each position and scale ω within I . In this case y = ⊥, x speciﬁes the image I , and S (x) includes
derivations T rooted at Q(ω) and the background output ⊥. The set of background examples is very
large because the number of positions and scales within each image is typically around 250K.
 l
The PASCAL benchmark requires a correct detection to have at least 50% overlap with a ground-
truth bounding box. We use this rule to deﬁne our loss functions. First, deﬁne Ll,τ (y , s) as follows
if y = ⊥ and s (cid:54)= ⊥
if y = ⊥ and s = ⊥
0
if y (cid:54)= ⊥ and overlap(y , s) < τ
l
if y (cid:54)= ⊥ and overlap(y , s) ≥ τ .
0
Following the PASCAL VOC protocol we use Lmargin = L1,0.5 . For a foreground example this
pushes down the score of detections that don’t overlap with the bounding box label by at least 50%.
Instead of using Loutput = Lmargin , we let Loutput = L∞,0.7 . For a foreground example this
ensures that the maximizer of (6b) is a detection with high overlap with the bounding box label. For
a background example, the maximizer of (6b) is always ⊥. Later we discuss how this simpliﬁes our
optimization algorithm. While our choice of Loutput does not produce a convex objective, it does
tightly limit the range of outputs, making our optimization less prone to reaching bad local optima.

4.1 Loss functions

Ll,τ (y , s) =

(7)

Econvex (w) =

1
2
Econcave (w) = −C

4.2 Optimization
Since L(cid:48) is not convex, the WL-SSVM objective (5) leads to a nonconvex optimization problem. We
follow [19] in which the CCCP procedure [20] was used to ﬁnd a local optima of a similar objective.
CCCP is an iterative algorithm that uses a decomposition of the objective into a sum of convex and
n(cid:88)
concave parts E (w) = Econvex (w) + Econcave (w).
||w||2 + C
max
n(cid:88)
s∈S (xi )
i=1
[w · Φ(xi , s) − Loutput (yi , s)]
i=1
In each iteration, CCCP computes a linear upper bound to Econcave based on a current weight vector
wt . The bound depends on subgradients of the summands in (9). For each summand, we take the
subgradient Φ(xi , si (wt )), where si (w) = argmaxs∈S (xi ) [w · Φ(xi , s) − Loutput (yi , s)] is a loss
augmented prediction.
We note that computing si (wt ) for each training example can be costly. But from our deﬁnition of
Loutput , we have that si (w) = ⊥ for a background example independent of w. Therefore, for a
background example Φ(xi , si (wt )) = 0.

[w · Φ(xi , s) + Lmargin (yi , s)]

max
s∈S (xi )

(8)

(9)

6

Table 1: PASCAL 2010 results. UoC-TTI and our method compete in comp3. Poselets competes
comp4 due to its use of detailed pose and visibility annotations and non-PASCAL images.
+context Poselets [9]
+bbox
+context UoC-TTI [9]
+bbox
Grammar
49.5
47.5
47.6
44.4
45.2
47.5
48.5

AP

Table 2: Training objective and model structure evaluation on PASCAL 2007.
Grammar LSVM Grammar WL-SSVM Mixture LSVM Mixture WL-SSVM
46.7
45.3
42.6
43.2

AP

After computing si (wt ) and Φ(xi , si (wt )) for all examples (implicitly for background examples),
the weight vector is updated by minimizing a convex upper bound on the objective E (w):
(cid:21)
(cid:20)
n(cid:88)
max
s∈S (xi )
i=1

[w · Φ(xi , s) + Lmargin (yi , s)] − w · Φ(xi , si (wt ))

||w||2 + C

wt+1 =

argmin
w

1
2

.

(10)

The optimization subproblem deﬁned by equation (10) is similar in form to a structural SVM opti-
mization. Given the size and nature of our training dataset we opt to solve this subproblem using
stochastic subgradient descent and a modiﬁed form of the data mining procedure from [10]. As
in [10], we data mine over background images to collect support vectors for background examples.
However, unlike in the binary LSVM setting considered in [10], we also need to apply data mining
to foreground examples. This would be slow because it requires performing relatively expensive in-
ference (more than 1 second per image) on thousands of images. Instead of applying data mining to
the foreground examples, each time we compute si (wt ) for a foreground example, we also compute
the top M scoring outputs s ∈ S (xi ) of wt · Φ(xi , s) + Lmargin (yi , s), and place the corresponding
feature vectors in the data mining cache. This is efﬁcient since much of the required computation
is shared with computation already necessary for computing si (wt ). While this is only a heuris-
tic approximation to true data mining, it leads to an improvement over training with binary LSVM
(see Section 5). In practice, we ﬁnd that M = 1 is sufﬁcient for improved performance and that
increasing M beyond 1 does not improve our results.

4.3

Initialization

Using CCCP requires an initial model or heuristic for selecting the initial outputs si (w0 ). Inspired
by the methods in [10, 12], we train a single ﬁlter for fully visible people using a standard binary
SVM. To deﬁne the SVM’s training data, we select vertically elongated examples. We apply the ori-
entation clustering method in [12] to further divide these examples into two sets that approximately
correspond to left-facing versus right-facing orientations. Examples from one of these two sets are
then anisotropically rescaled so their HOG feature maps match the dimensions of the ﬁlter. These
form the positive examples. For negative examples, random patches are extracted from background
images. After training the initial ﬁlter, we slice it into subﬁlters (one 8 × 8 and ﬁve 3 × 8) that form
the building blocks of the grammar model. We mirror these six ﬁlters to get subtypes, and then add
subparts using the energy covering heuristic in [10, 12].

5 Experimental results

We evaluated the performance of our person grammar and training framework on the PASCAL VOC
2007 and 2010 datasets [8, 9]. We used the standard PASCAL VOC comp3 test protocol, which
measures detection performance by average precision (AP) over different recall levels. Figure 2
shows some qualitative results, including failure cases.
PASCAL VOC 2010 Our results on the 2010 dataset are presented in Table 1 in the context of
two strong baselines. The ﬁrst, UoC-TTI, won the person category in the comp3 track of the 2010
competition [9]. The 2010 entry of the UoC-TTI method extended [12] by adding an extra octave
to the HOG feature map pyramid, which allows the detector to ﬁnd smaller objects. We report the
AP score of the UoC-TTI “raw” person detector, as well as the scores after applying the bounding

7

(a) Full visibility
(b) Occlusion boundaries
(c) Early termination
(d) Mistakes
Figure 2: Example detections. Parts are blue. The occlusion part, if used, is dashed cyan. (a) Detec-
tions of fully visible people. (b) Examples where the occlusion part detects an occlusion boundary.
(c) Detections where there is no occlusion, but a partial person is appropriate. (d) Mistakes where
the model did not detect occlusion properly.

box prediction and context rescoring methods described in [10]. Comparing raw detector outputs
our grammar model signiﬁcantly outperforms the mixture model: 47.5 vs. 44.4.
We also applied the two post-processing steps to the grammar model, and found that unlike with
the mixture model, the grammar model does not beneﬁt from bounding box prediction. This is
likely because our ﬁne-grained occlusion model reduces the number of near misses that are ﬁxed
by bounding box prediction. To test context rescoring, we used the UoC-TTI detection data for the
other 19 object classes. Context rescoring boosts our ﬁnal score to 49.5.
The second baseline is the poselets system described in [3]. Their system requires detailed pose and
visibility annotations, in contrast to our grammar model which was trained only with bounding box
labels. Prior to context rescoring, our model scores one point lower than the poselets model, and
after rescoring it scores one point higher.
Structure and training We evaluated several aspects of our model structure and training objective
on the PASCAL VOC 2007 dataset. In all of our experiments we set the regularization constant
to C = 0.006. In Table 2 we compare the WL-SSVM framework developed here with the binary
LSVM framework from [10]. WL-SSVM improves performance of the grammar model by 1.4 AP
points over binary LSVM training. WL-SSVM also improves results obtained using a mixture of
part-based models by 0.6 points. To investigate model structure, we evaluated the effect of part
subtypes and occlusion modeling. Removing subtypes reduces the score of the grammar model
from 46.7 to 45.5. Removing the occlusion part also decreases the score from 46.7 to 45.5. The
shallow model (no subparts) achieves a score of 40.6.

6 Discussion
Our results establish grammar-based methods as a high-performance approach to object detection
by demonstrating their effectiveness on the challenging task of detecting people in the PASCAL
VOC datasets. To do this, we carefully designed a ﬂexible grammar model that can detect people
under a wide range of partial occlusion, pose, and appearance variability. Automatically learning
the structure of grammar models remains a signiﬁcant challenge for future work. We hope that
our empirical success will provide motivation for pursing this goal, and that the structure of our
handcrafted grammar will yield insights into the properties that an automatically learned grammar
might require. We also develop a structured training framework, weak-label structural SVM, that
naturally handles learning a model with strong outputs, such as derivation trees, from data with
weak labels, such as bounding boxes. Our training objective is nonconvex and we use a strong loss
function to avoid bad local optima. We plan to explore making this loss softer, in an effort to make
learning more robust to outliers.
Acknowledgments This research has been supported by NSF grant IIS-0746569.

8

References
[1] M. Blaschko and C. Lampert. Learning to localize objects with structured output regression. In ECCV,
2008.
[2] M. Blaschko, A. Vedaldi, and A. Zisserman. Simultaneous object detection and ranking with weak super-
vision. In NIPS, 2010.
[3] L. Bourdev, S. Maji, T. Brox, and J. Malik. Detecting people using mutually consistent poselet activations.
In ECCV, 2010.
[4] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability. In ICML, 2006.
[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
[6] C. Do, Q. Le, C. Teo, O. Chapelle, and A. Smola. Tighter bounds for structured estimation. In NIPS,
2008.
[7] M. Enzweiler, A. Eigenstetter, B. Schiele, and D. M. Gavrila. Multi-cue pedestrian classiﬁcation with
partial occlusion handling. In CVPR, 2010.
[8] M. Everingham, L. Van Gool, C. K.
I. Williams,
J. Winn,
and A. Zisserman.
The
PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.
http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
and A. Zisserman.
[9] M. Everingham, L. Van Gool, C. K.
The
J. Winn,
I. Williams,
PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results.
http://www.pascal-
network.org/challenges/VOC/voc2010/workshop/index.html.
[10] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively
trained part based models. PAMI, 2009.
[11] P. Felzenszwalb and D. McAllester. Object detection grammars. Univerity of Chicago, CS Dept., Tech.
Rep. 2010-02.
[12] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Discriminatively trained deformable part models,
release 4. http://people.cs.uchicago.edu/˜pff/latent-release4/.
[13] Y. Jin and S. Geman. Context and hierarchy in a probabilistic image model. In CVPR, 2006.
[14] D. McAllester and J. Keshet. Generalization bounds and consistency for latent structural probit and ramp
loss. In NIPS, 2011.
[15] Y. Ohta, T. Kanade, and T. Sakai. An analysis system for scenes containing objects with substructures. In
ICPR, 1978.
[16] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.
[17] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and
interdependent output variables. JMLR, 2006.
[18] X. Wang, T. Han, and S. Yan. An hog-lbp human detector with partial occlusion handling. In ICCV, 2009.
[19] C.-N. J. Yu and T. Joachims. Learning structural svms with latent variables. In ICML, 2009.
[20] A. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 2003.
[21] L. Zhu, Y. Chen, A. Torralba, W. Freeman, and A. Yuille. Part and appearance sharing: Recursive
compositional models for multi-view multi-object detection. In CVPR, 2010.
[22] L. Zhu, Y. Chen, and A. Yuille. Unsupervised learning of probabilistic grammar-markov models for object
categories. PAMI, 2009.
[23] L. Zhu, Y. Chen, A. Yuille, and W. Freeman. Latent hierarchical structural learning for object detection.
In CVPR, 2010.
[24] S. Zhu and D. Mumford. A stochastic grammar of images. Foundations and Trends in Computer Graphics
and Vision, 2006.

9


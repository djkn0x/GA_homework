Periodic Finite State Controllers for Efﬁcient
POMDP and DEC-POMDP Planning

Joni Pajarinen
Aalto University, Department of
Information and Computer Science,
P.O. Box 15400, FI-00076 Aalto, Finland
Joni.Pajarinen@aalto.fi

Jaakko Peltonen
Aalto University, Department of Information
and Computer Science, Helsinki Institute for
Information Technology HIIT,
P.O. Box 15400, FI-00076 Aalto, Finland
Jaakko.Peltonen@aalto.fi

Abstract

Applications such as robot control and wireless communication require planning
under uncertainty. Partially observable Markov decision processes (POMDPs)
plan policies for single agents under uncertainty and their decentralized versions
(DEC-POMDPs) ﬁnd a policy for multiple agents. The policy in inﬁnite-horizon
POMDP and DEC-POMDP problems has been represented as ﬁnite state con-
trollers (FSCs). We introduce a novel class of periodic FSCs, composed of layers
connected only to the previous and next layer. Our periodic FSC method ﬁnds
a deterministic ﬁnite-horizon policy and converts it to an initial periodic inﬁnite-
horizon policy. This policy is optimized by a new inﬁnite-horizon algorithm to
yield deterministic periodic policies, and by a new expectation maximization al-
gorithm to yield stochastic periodic policies. Our method yields better results than
earlier planning methods and can compute larger solutions than with regular FSCs.

1 Introduction

Many machine learning applications involve planning under uncertainty. Such planning is neces-
sary in medical diagnosis, control of robots and other agents, and in dynamic spectrum access for
wireless communication systems. The planning task can often be represented as a reinforcement
learning problem, where an action policy controls the behavior of an agent, and the quality of the
policy is optimized to maximize a reward function. Single agent policies can be optimized with
partially observable Markov decision processes (POMDPs) [1], when the world state is uncertain.
Decentralized POMDPs (DEC-POMDPs) [2] optimize policies for multiple agents that act without
direct communication, with separate observations and beliefs of the world state, to maximize a joint
reward function. POMDP and DEC-POMDP methods use various representations for the policies,
such as value functions [3], graphs [4, 5], or ﬁnite state controllers (FSCs) [6, 7, 8, 9, 10].

We present a novel efﬁcient method for POMDP and DEC-POMDP planning. We focus on inﬁnite-
horizon problems, where policies must operate forever. We introduce a new policy representation:
periodic ﬁnite state controllers, which can be seen as an intelligent restriction which speeds up
optimization and can yield better solutions. A periodic FSC is composed of several layers (subsets
of states), and transitions are only allowed to states in the next layer, and from the ﬁnal layer to the
ﬁrst. Policies proceed through layers in a periodic fashion, and policy optimization determines the
probabilities of state transitions and action choices to maximize reward. Our work has four main
contributions. Firstly, we introduce an improved optimization method for standard ﬁnite-horizon
problems with FSC policies by compression. Secondly, we give a method to transform the ﬁnite-
horizon FSC into an initial inﬁnite-horizon periodic FSC. Thirdly, we introduce compression to
the periodic FSC. Fourthly, we introduce an expectation-maximization (EM) training algorithm for
planning with periodic FSCs. We show that the resulting method performs better than earlier DEC-

1

POMDP methods and POMDP methods with a restricted-size policy and that use of the periodic
FSCs enables computing larger solutions than with regular FSCs. Online execution has complexity
O(const) for deterministic FSCs and O(log(F SC layer width)) for stochastic FSCs.

We discuss existing POMDP and DEC-POMDP solution methods in Section 2 and formally deﬁne
the inﬁnite-horizon (DEC-)POMDP. In Section 3 we introduce the novel concept of periodic FSCs.
We then describe the stages of our method: improving ﬁnite-horizon solutions, transforming them
to periodic inﬁnite-horizon solutions, and improving the periodic solutions by a novel EM algorithm
for (DEC-)POMDPs (Section 3.2). In Section 4 we show the improved performance of the new
method on several planning problems, and we conclude in Section 5.

2 Background

Partially observable Markov decision processes (POMDPs) and decentralized POMDPs (DEC-
POMDPs) are model families for decision making under uncertainty. POMDPs optimize policies for
a single agent with uncertainty of the environment state while DEC-POMDPs optimize policies for
several agents with uncertainty of the environment state and each other’s states. Given the actions of
the agents the environment evolves according to a Markov model. The agents’ policies are optimized
to maximize the expected reward earned for actions into the future. In inﬁnite-horizon planning the
expected reward is typically discounted to emphasize current and near-future actions. Computation-
ally POMDPs and DEC-POMDPs are complex: even for ﬁnite-horizon problems, ﬁnding solutions
is in the worst case PSPACE-complete for POMDPs and NEXP-complete for DEC-POMDPs [11].

For inﬁnite-horizon DEC-POMDP problems, state of the art methods [8, 12] store the policy as a
stochastic ﬁnite state controller (FSC) for each agent which keeps the policy size bounded. The
FSC parameters can be optimized by expectation maximization (EM) [12]. An advantage of EM is
that it can be adapted to for example continuous probability distributions [7] or to take advantage of
factored problems [10]. Alternatives to EM include formulating FSC optimization as a non-linear
constraint satisfaction (NLP) problem solvable by an NLP solver [8], or iteratively improving each
FSC by linear programming with other FSCs ﬁxed [13]. Deterministic FSCs with a ﬁxed size could
also be found by a best-ﬁrst search [14]. If a DEC-POMDP problem has a speciﬁc goal state, then
a goal-directed [15] approach can achieve good results. The NLP and EM methods have yielded
the best results for the inﬁnite-horizon DEC-POMDP problems. In a recent variant called mealy
NLP [16], the NLP based approach to DEC-POMDPs is adapted to FSC policies represented by
Mealy machines instead of traditional Moore machine representations. In POMDPs, Mealy machine
based controllers can achieve equal or better solutions than Moore controllers of the same size.

This paper recognizes the need to improve general POMDP and DEC-POMDP solutions. We intro-
duce an approach where FSCs have a periodic layer structure, which turns out to yield good results.

2.1 Inﬁnite-horizon DEC-POMDP: deﬁnition

The tuple h{αi}, S, {Ai}, P, {Ωi}, O, R, b0 , γ i deﬁnes an inﬁnite-horizon DEC-POMDP problem
for N agents αi , where S is the set of environment states and Ai and Ωi are the sets of possible
actions and observations for agent αi . A POMDP is the special case when there is only one agent.
P (s′ |s, ~a) is the probability to move from state s to s′ , given the actions of all agents (jointly denoted
~a = ha1 , . . . , aN i). The observation function O(~o|s′ , ~a) is the probability that the agents observe
~o = ho1 , . . . , oN i, where oi is the observation of agent i, when actions ~a were taken and the environ-
ment transitioned to state s′ . The initial state distribution is b0 (s). R(s, ~a) is the real-valued reward
for executing actions ~a in state s. For brevity, we denote transition probabilities given the actions
by Ps′ s~a , observation probabilities by P~os′~a , reward functions by Rs~a , and the set of all agents other
than i by ¯i. At each time step, agents perform actions, the environment state changes, and agents
receive observations. The goal is to ﬁnd a joint policy π for the agents that maximizes expected
discounted inﬁnite-horizon reward E (cid:2)P∞
t=0 γ tRs(t)~a(t) |π(cid:3), where γ is the discount factor, and s(t)
and ~a(t) are the state and action at time t, and E [·|π ] denotes expected value under policy π . Here,
the policy is stored as a set of stochastic ﬁnite state controllers (FSCs), one for each agent. The
FSC of agent i is deﬁned by the tuple hQi , νqi , πai qi , λq′
i qi oi i, where Qi is the set of FSC nodes qi ,
νqi is the initial distribution P (qi ) over nodes, πai qi is the probability P (ai |qi ) to perform action
i qi oi is the probability P (q ′
i |qi , oi ) to transition from node qi to node q ′
ai in node qi , and λq′
i when

2

time t

time t +1

r

s

o

a

q

r

s

o

a

q

t=0,3,6,...

t=1,4,7,...

t=2,5,8,...

Figure 1: Left: inﬂuence diagram for a DEC-POMDP with ﬁnite state controllers ~q , states s, joint
observations ~o, joint actions ~a and reward r (given by a reward function R(s, ~a)). A dotted line sep-
arates two time steps. Right: an example of the new periodic ﬁnite state controller, with three layers
and three nodes in each layer, and possible transitions shown as arrows. The controller controls one
of the agents. Which layer is active depends only on the current time; which node is active, and
which action is chosen, depend on transition probabilities and action probabilities of the controller.

observing oi . The current FSC nodes of all agents are denoted ~q = hq1 , . . . , qN i. The policies are
optimized by optimizing the parameters νqi , πai qi , and λq′
i qi oi . Figure 1 (left) illustrates the setup.

3 Periodic ﬁnite state controllers

State-of-the-art algorithms [6, 13, 8, 12, 16] for optimizing POMDP/DEC-POMDP policies with
restricted-size FSCs ﬁnd a local optimum. A well-chosen FSC initialization could yield better solu-
tions, but initializing (compact) FSCs is not straightforward: one reason is that dynamic program-
ming is difﬁcult to apply on generic FSCs.
In [17] FSCs for POMDPs are built using dynamic
programming to add new nodes, but this yields large FSCs and cannot be applied on DEC-POMDPs
as it needs a piecewise linear convex value function. Also, general FSCs are irreducible, so a prob-
ability distribution over FSC nodes is not sparse over time even if a FSC starts from a single node.
This makes computations with large FSCs difﬁcult and FSC based methods are limited by FSC size.
We introduce periodic FSCs, which allow the use of much larger controllers with a small complexity
increase, efﬁcient FSC initialization, and new dynamic programming algorithms for FSCs.

A periodic FSC is composed of M layers of controller nodes. Nodes in each layer are connected
only to nodes in the next layer: the ﬁrst layer is connected to the second, the second layer to the third
and so on, and the last layer is connected to the ﬁrst. The width of a periodic FSC is the number of
controller nodes in a layer. Without loss of generality we assume all layers have the same number of
nodes. A single-layer periodic FSC equals an ordinary FSC. A periodic FSC has different action and
transition probabilities for each layer. π (m)
ai qi is the layer m probability to take action ai when in node
qi , and λ(m)
is the layer m probability to move from node qi to q ′
i when observing oi . Each layer
q′
i qi oi
connects only to the next one, so the policy cycles periodically through each layer: for t ≥ M we
= λ(t mod M )
and λ(t)
ai qi = π (t mod M )
have π (t)
where ‘mod’ denotes remainder. Figure 1 (right)
ai qi
q′
q′
i qi oi
i qi oi
shows an example periodic FSC.

We now introduce our method for solving (DEC-)POMDPs with periodic FSC policies. We show
that the periodic FSC structure allows efﬁcient computation of deterministic controllers, show how
to optimize periodic stochastic FSCs, and show how a periodic deterministic controller can be used
as initialization to a stochastic controller. The algorithms are discussed in the context of DEC-
POMDPs, but can be directly applied to POMDPs.

3.1 Deterministic periodic ﬁnite state controllers

In a deterministic FSC, actions and node transitions are deterministic functions of the current node
and observation. To optimize deterministic periodic FSCs we ﬁrst compute a non-periodic ﬁnite-
horizon policy. The ﬁnite-horizon policy is transformed into a periodic inﬁnite-horizon policy by
connecting the last layer to the ﬁrst layer and the resulting deterministic policy can then be im-

3

proved with a new algorithm (see Section 3.1.2). A periodic deterministic policy can also be used as
initialization for a stochastic FSC optimizer based on expectation maximization (see Section 3.2).

3.1.1 Deterministic ﬁnite-horizon controllers

We brieﬂy discuss existing methods for deterministic ﬁnite-horizon controllers and introduce an
improved ﬁnite-horizon method, which we use as the initial solution for inﬁnite-horizon controllers.

State-of-the-art point based ﬁnite-horizon DEC-POMDP methods [4, 5] optimize a policy graph,
with restricted width, for each agent. They compute a policy for a single belief, instead of all possible
beliefs. Beliefs over world states are sampled centrally using various action heuristics. Policy graphs
are built by dynamic programming from horizon T to the ﬁrst time step. At each time step a policy
is computed for each policy graph node, by assuming that the nodes all agents are in are associated
with the same belief. In a POMDP, computing the deterministic policy for a policy graph node means
ﬁnding the best action, and the best connection (best next node) for each observation; this can be
done with a direct search. In a DEC-POMDP this approach would go through all combinations of
actions, observations and next nodes of all agents: the number of combinations grows exponentially
with the number of agents, so direct search works only for simple problems. A more efﬁcient way
is to go through all action combinations, for each action combination sample random policies for all
agents, and then improve the policy of each agent in turn while holding the other agents’ policies
ﬁxed. This is not guaranteed to ﬁnd the best policy for a belief, but has yielded good results in the
Point-Based Policy Generation (PBPG) algorithm [5].

We introduce a new algorithm which improves on [5]. PBPG used linear programming to ﬁnd
policies for each agent and action-combination, but with a ﬁxed joint action and ﬁxed policies of
other agents we can use fast and simple direct search as follows. Initialize the value function V (s, ~q)
to zero. Construct an initial policy graph for each agent, starting from horizon t = T : (1) Project
the initial belief along a random trajectory to horizon t to yield a sampled belief b(s) over world
states. (2) Add, to the graph of each agent, a node to layer t. Find the best connections to the next
layer as follows. Sample random connections for each agent, then for each agent in turn optimize its
connection with connections of other agents ﬁxed: for each action-combination ~a and observation
connect to the next-layer node that maximizes value, computed using b(s) and the next layer value
function; repeat this until convergence, using random restarts to escape local minima. The best
connections and action combination ~a become the policy for the current policy graph node. (3) Run
(1)-(2) until the graph layer has enough nodes. (4) Decrease t and run (1)-(3), until t = 0.

We use the above-described algorithm for initialization, and then use a new policy improvement
approach shown in Algorithm 1 that improves the policy value monotonically: (1) Here we do not
use a random trajectory for belief projection, instead we project the belief bt (s, ~q) over world states
s and controller nodes ~q (agents are initially assumed to start from the ﬁrst controller node) from
time step t = 0 to horizon T , through the current policy graph; this yields distributions for the FSC
nodes that match the current policy. (2) We start from the last layer and proceed towards the ﬁrst.
At each layer, we optimize each agent separately: for each graph node qi of agent i, for each action
ai of the agent, and for each observation oi we optimize the (deterministic) connection to the next
layer. (3) If the optimized policy at the node (action and connections) is identical to policy π of
another node in the layer, we sample a new belief over world states, and re-optimize the node for
the new belief; if no new policy is found even after trying several sampled beliefs, we try several
uniformly random beliefs for ﬁnding policies. We also redirect any connections from the previous
policy graph layer to the current node to go instead to the node having policy π ; this “compresses”
the policy graph without changing its value (in POMDPs the redirection step is not necessary, it
will happen naturally when the previous layer is reoptimized). The computational complexity of
Algorithm 1 is O(2M |Q|2N |A|N |O|N |S |2 + M N |Q|N |O|N |A|N |S |2 + CN |Q|2 |A|N |O||S |).

Our ﬁnite-horizon method gets rid of the simplifying assumption that all FSCs are in the same node,
for a certain belief, made in [4, 5]. We only assume that for initialization steps, but not in actual
optimization. Our optimization monotonically improves the value of a ﬁxed size policy graph and
converges to a local optimum. Here we applied the procedure to ﬁnite-horizon DEC-POMDPs; it
is adapted for improving deterministic inﬁnite-horizon FSCs in Section 3.1.2. We also have two
simple improvements: (1) a speedup: [5] used linear programming to ﬁnd policies for each agent
and action-combination in turn, but simple direct search is faster, and we use that; (2) improved
duplicate handling: [5] tried sampled beliefs to avoid duplicate nodes, we also try uniformly random

4

1

2

3

4

5

6

7

8

9

10

11

12

13

14

Initialize VT +1 (s, ~q) = 0
Using current policy project bt (s, ~q) for 1 ≤ t ≤ T
for Time step t = T to 0 do
foreach Agent i do
foreach Node q of agent i do
foreach ai do
hai
j |qj , oj )Vt+1 (s′ , ~q ′ )
~o,~q′ = Ps,s′ ,~q,~a P (~o, s′ |s, ~a)bt (s, ~q ) Qj 6=i Pt (aj |qj )Pt (q ′
P ai
i |qi = q , oi )hai
∀oi P ai
t (q ′
t (q ′
i |qi=q,oi ) P~q′ ,{oj }j 6=i
i |qi = q , oi ) = argmaxP
ai
t (q′
~o,~q′
[bt (s, ~q)R(s, ~a) Yj 6=i
ai Xs,s′ ,~q,~a,~o,~q′
Pt (aj |qj ) + γP ai
i |qi = q , oi )hai
a∗
t (q ′
i = argmax
~o,~q′ ]
i |qi , oi ) = P a∗
t (q ′
i |qi ) = 1, Pt (q ′
i |qi ) = 0, Pt (ai = a∗
Pt (ai 6= a∗
i |qi , oi )
i
if Any node p already has same policy as q then
For each qi for which Pt−1 (q ′
i = q |qi , oj ) = 1 redirect link to q ′
i = p
Sample belief b(s, qj = q∀j ) and use it to compute new policy by steps 7-13
i |qi , oi )P (s′ , ~o|s, ~a)Vt+1 (s′ , ~q ′ )
Vt (s, ~q) = R(s, ~a) Qi Pt (ai |qi ) + γ Qi Pt (q ′
Algorithm 1: Monotonic policy graph improvement algorithm

beliefs, and for DEC-POMDPs we redirect previous-layer connections to duplicate nodes. Unlike
the recursion idea in [4] our projection approach is guaranteed to improve value at each graph node
and ﬁnd a local optimum.

3.1.2 Deterministic inﬁnite-horizon controllers

To initialize an inﬁnite-horizon problem, we transform a deterministic ﬁnite-horizon policy graph
(computed as in Section 3.1.1) into an inﬁnite-horizon periodic controller by connecting the last
layer to the ﬁrst. Assuming controllers start from policy graph node 1, we compute policies for the
other nodes in the ﬁrst layer with beliefs sampled for time step M + 1, where M is the length of the
controller period. It remains to compute the (deterministic) connections from the last layer to the
ﬁrst: approximately optimal connections are found using the beliefs at the last layer and the value
function projected from the last layer through the graph to the ﬁrst layer. This approach can yield
efﬁcient controllers on its own, but may not be suitable for problems with a long effective horizon.

To optimize controllers further, we give two changes to Algorithm 1 that enable optimization of
inﬁnite-horizon policies: (1) To compute beliefs ˆbu (s, ~q) over time steps u by projecting the initial
belief, ﬁrst determine an effective projection horizon Tproj . Compute a QM DP policy [18] (an upper
bound to the optimal DEC-POMDP policy) by dynamic programming. As the projection horizon,
use the number of dynamic programming steps needed to gather enough value in the corresponding
MDP. Compute the belief bt (s, ~q) for each FSC layer t (needed on line 2 of Algorithm 1) as a
C Pu∈{t,t+M ,t+2M ,...;u≤Tproj } γ uˆbu (s, ~q).
discounted sum of projected beliefs: bt (s, ~q) = 1
(2)
Compute value function Vt (s, ~q) for a policy graph layer by backing up (using line 14 of Algorithm
1) M − 1 steps from the previous periodic FSC layer to current FSC layer, one layer at a time.
The complexity of one iteration of the inﬁnite-horizon approach is O(2M |Q|2N |A|N |O|N |S |2 +
M (M − 1)N |Q|N |O|N |A|N |S |2 + M CN |Q|2 |A|N |O||S |). There is no convergence guarantee
due to the approximations, but approximation error decreases exponentially with the period M .

3.2 Expectation maximization for stochastic inﬁnite-horizon controllers

A stochastic FSC provides a solution of equal or larger value [6] compared to a deterministic FSC
with the same number of controller nodes. Many algorithms that optimize stochastic FSCs could be
adapted to use periodic FSCs; in this paper we adapt the expectation-maximization (EM) approach
[7, 12] to periodic FSCs. The adapted version retains the theoretical properties of regular EM, such
as monotonic convergence to a local optimum.

In the EM approach [7, 12] the optimization of policies is written as an inference problem: rewards
are scaled into probabilities and the policy, represented as a stochastic FSC, is optimized by EM

5

iteration to maximize the probability of getting rewards. We now introduce an EM algorithm for
(DEC-)POMDPs with periodic stochastic FSCs. We build on the EM method for DEC-POMDPs
with standard (non-periodic) FSCs by Kumar and Zilberstein [12]; see [7, 12] for more details of
non-periodic EM. First, the reward function is scaled into a probability ˆR(r = 1|s, ~a) = (R(s, ~a) −
Rmin )/(Rmax − Rmin ), where Rmin and Rmax are the minimum and maximum rewards possible
and ˆR(r = 1|s, ~a) is the conditional probability for the binary reward r to be 1. The FSC parameters
θ are optimized by maximizing the reward likelihood P∞
T =0 P (T )P (r = 1|T , θ) with respect to θ,
where the horizon is inﬁnite and P (T ) = (1 − γ )γ T . This is equivalent to maximizing expected
discounted reward in the DEC-POMDP. The EM approach improves the policy, i.e. the stochastic
periodic ﬁnite state controllers, in each iteration. We next describe the E-step and M-step formulas.

In the E-step, alpha messages ˆα(m) (~q , s) and beta messages ˆβ (m) (~q , s) are computed for each layer
of the periodic FSC. Intuitively, ˆα(~q , s) corresponds to the discount weighted average probability
that the world is in state s and FSCs are in nodes ~q , when following the policy deﬁned by the
current FSCs, and ˆβ (~q , s) is intuitively the expected discounted total scaled reward, when starting
from state s and FSC nodes ~q . The alpha messages are computed by projecting an initial nodes-
and-state distribution forward, while beta messages are computed by projecting reward probabilities
backward. We compute separate ˆα(m) (~q , s) and ˆβ (m) (~q , s) for each layer m. We use a projection
horizon T = M TM − 1, where M TM is divisible by the number of layers M . This means that when
we have accumulated enough probability mass in the E-step we still project a few steps in order to
reach a valid T . For a periodic FSC the forward projection of the joint distribution over world and
ai qi λ(t)
FSC states from time step t to time step t + 1 is Pt (~q ′ , s′ |~q , s) = P~o,~a Ps′ s~aP~os′~a Qi [π (t)
].
q′
q˜i o˜i
˜i
Each ˆα(m) (~q , s) can be computed by projecting a single trajectory forward starting from the ini-
tial belief and then adding only messages belonging to layer m to each ˆα(m) (~q , s). In contrast,
each ˆβ (m) (~q , s) has to be projected separately backward, because we don’t have a “starting point”
similar to the alpha messages. Denoting such projections by β (m)
ˆRs~a Qi π (m)
(~q , s) = P~a
ai qi and
0
(~q , s) = Ps′ ,~q′ β (m)
β (m)
t−1 (~q ′ , s′ )Pt (~q ′ , s′ |~q , s) the equations for the messages become
t
TM −1
T
γ t (1− γ )β (m)
γ (m+tmM ) (1− γ )α(m+tmM ) (~q , s) and ˆβ (m) (~q , s) =
Xt=0
Xtm=0
ˆα(m) (~q , s) =
t
(1)
This means that the complexity of the E-step for periodic FSCs is M times the complexity of the
E-step for usual FSCs with a total number of nodes equal to the width of the periodic FSC. The
complexity increases linearly with the number of layers.

(~q , s) .

In the M-step we can update the parameters of each layer separately using the alpha and beta mes-
sages for that layer, as follows. EM maximizes the expected complete log-likelihood Q(θ, θ∗ ) =
PT PL P (r = 1, L, T |θ) log P (r = 1, L, T |θ∗) , where L denotes all latent variables: actions,
observations, world states, and FSC states, θ denotes previous parameters, and θ∗ denotes new pa-
rameters. For periodic FSCs P (r = 1, L, T |θ) is
P (r = 1, L, T |θ) = P (T )[ ˆRs~a ]t=T " T
~a~q Ps′ s~aP~os′~aΛ~q′ ~q~ot# hτ (0)
~a~q b0 (s)it=0
τ (t)
Yt=1
(2)
where we denoted τ (t)
~a~q = Qi π (t)
ai qi for t = 1, . . . , T , τ (0)
~a~q = Qi π (0)
ai qi νqi , and Λ~q′ ~q~ot = Qi λ(t−1)
q′
i qi oi
The log in the expected complete log-likelihood Q(θ, θ∗ ) transforms the product of probabilities into
a sum; we can divide the sums into smaller sums, where each sum contains only parameters from
the same periodic FSC layer. Denoting fs′ s ~q′ ~o~am = Ps′ s~aP~os′~a ˆβ (m+1) ( ~q ′ , s′ ), the M-step periodic
FSC parameter update rules can then be written as:
νqi
Ci Xs,q˜i

ˆβ (0) (~q , s)νq˜i b0 (s)

ν ∗
qi =

.

(3)

π∗ (m)
ai qi =

π (m)
ai qi
Cqi Xs,s′ ,q˜i , ~q′ ,~o,a˜i

{ ˆα(m) (~q , s)π (m)
a˜i q˜i

· (cid:2) ˆRs~a +

γ
1 − γ

λ(m)
q′
q˜i o˜i
˜i

λ(m)
′ qi oi
qi

fs′ s ~q′ ~o~am (cid:3)}

(4)

6

,o˜i ,~a

fs′ s ~q′ ~o~am .

(5)

λ∗ (m)
q′
i qi oi

=

ˆα(m) (~q , s)π (m)
a˜i q˜i

λ(m)
q′
i qi oi
Cqi oi Xs,s′ ,q˜i ,q′
˜i
Note about initialization. Our initialization procedure (Sections 3.1.1 and 3.1.2) yields determin-
istic periodic controllers as initializations; a deterministic ﬁnite state controller is a stable point of
the EM algorithm, since for such a controller the M-step of the EM approach does not change the
probabilities. To allow EM to escape the stable point and ﬁnd even better optima, we add noise to
the controllers in order to produce stochastic controllers that can be improved by EM.

ai qi λ(m)
π (m)
q′
q˜i o˜i
˜i

4 Experiments

Experiments were run for standard POMDP and DEC-POMDP benchmark problems [8, 15, 16, 10]
with a time limit of two hours. For both types of benchmarks we ran the proposed inﬁnite-horizon
method for deterministic controllers (denoted “Peri”) with nine improvement rounds as described
in Section 3.1.2. For DEC-POMDP benchmarks we also ran the proposed periodic expectation
maximization approach in Section 3.2 (denoted “PeriEM”), initialized by the ﬁnite-horizon approach
in Section 3.1.1 with nine improvement rounds and the inﬁnite-horizon transformation in Section
3.1.2, paragraph 1. For “PeriEM” a period of 10 was used. For “Peri” a period of 30 was used for
problems with discount factor 0.9, 60 for discount factor 0.95, and 100 for larger discount factors.
The main comparison methods EM [12] and Mealy NLP [16] (with removal of dominated actions
and unreachable state-observation pairs) were implemented using Matlab and the NEOS server was
utilized for solving the Mealy NLP non-linear programs. We used the best of parallel experiment
runs to choose the number of FSC nodes. EM was run for all problems and Mealy NLP for the
Hallway2, decentralized tiger, recycling robots, and wireless network problems. SARSOP [3] was
run for all POMDP problems and we also report results from literature [8, 15, 16].

Table 1 shows DEC-POMDP results for the decentralized tiger, recycling robots, meeting in a grid,
wireless network [10], co-operative box pushing, and stochastic mars rover problems. A discount
factor of 0.99 was used in the wireless network problem and 0.9 in the other DEC-POMDP bench-
marks. Table 2 shows POMDP results for the benchmark problems Hallway2, Tag-avoid, Tag-avoid
repeat, and Aloha. A discount factor of 0.999 was used in the Aloha problem and 0.95 in the other
POMDP benchmarks. Methods whose 95% conﬁdence intervals overlap with that of the best method
are shown in bold. The proposed method “Peri” performed best in the DEC-POMDP problems and
better than other restricted policy size methods in the POMDP problems. “PeriEM” also performed
well, outperforming EM.

5 Conclusions and discussion

We introduced a new class of ﬁnite state controllers, periodic ﬁnite state controllers (periodic FSCs),
and presented methods for initialization and policy improvement. In comparisons the resulting meth-
ods outperformed state-of-the-art DEC-POMDP and state-of-the-art restricted size POMDP meth-
ods and worked very well on POMDPs in general.

In our method the period length was based simply on the discount factor, which already performed
very well; even better results could be achieved, for example, by running solutions of different
periods in parallel. In addition to the expectation-maximization presented here, other optimization
algorithms for inﬁnite-horizon problems could also be adapted to periodic FSCs: for example, the
non-linear programming approach [8] could be adapted to periodic FSCs. In brief, a separate value
function and separate FSC parameters would be used for each time slice in the periodic FSCs, and
the number of constraints would grow linearly with the number of time slices.

Acknowledgments

We thank Ari Hottinen for discussions on decision making in wireless networks. The authors belong
to the Adaptive Informatics Research Centre (CoE of the Academy of Finland). The work was
supported by Nokia, TEKES, Academy of Finland decision number 252845, and in part by the
PASCAL2 EU NoE, ICT 216886. This publication reﬂects the authors’ views only.

7

Table 1: DEC-POMDP benchmarks. Most
comparison results are from [8, 15, 16]; we ran
EM and Mealy NLP on many of the tests (see
Section 4). Note that “Goal-directed” is a spe-
cial method that can only be applied to prob-
lems with goals.

Table 2: POMDP benchmarks. Most compari-
son method results are from [16]; we ran EM,
SARSOP, and Mealy NLP on one test (see Sec-
tion 4).

Algorithm (Size,Time): Value

Algorithm (Size, Time): Value
DecTiger
(|S | = 2, |Ai | = 3, |Oi | = 2)
13.45
Peri (10 × 30, 202s):
PeriEM (7 × 10, 6540s):
9.42
5.041
Goal-directed (11, 75s):
−1.088
NLP (19, 6173s):
−1.49
Mealy NLP (4, 29s):
−16.30
EM (6, 142s):
Recycling robots
(|S | = 4, |Ai | = 3, |Oi | = 2)
31.93
Mealy NLP (1, 0s):
31.84
Peri (6 × 30, 77s):
PeriEM (6 × 10, 272s):
31.80
31.50
EM (2, 13s):
Meeting in a 2x2 grid
(|S | = 16, |Ai | = 5, |Oi | = 2)
6.89
Peri (5 × 30, 58s):
PeriEM (5 × 10, 6019s):
6.82
6.80
EM (8, 5086s):
6.13
Mealy NLP (5, 116s):
6.04
HPI+NLP (7, 16763s):
5.66
NLP (5, 117s):
5.64
Goal-directed (4, 4s):
Wireless network
(|S | = 64, |Ai | = 2, |Oi | = 6)
−175.40
EM (3, 6886s):
Peri (15 × 100, 6492s):−181.24
PeriEM (2 × 10, 3557s):−218.90
−296.50
Mealy NLP (1, 9s):
Box pushing
(|S | = 100, |Ai | = 4, |Oi | = 5)
Goal-directed (5, 199s): 149.85
148.65
Peri (15 × 30, 5675s):
143.14
Mealy NLP (4, 774s):
PeriEM (4 × 10, 7164s): 106.68
95.63
HPI+NLP (10, 6545s):
43.33
EM (6, 7201s):
Mars rovers
(|S | = 256, |Ai | = 6, |Oi | = 8)
Peri (10 × 30, 6088s):
24.13
21.48
Goal-directed (6, 956s):
19.67
Mealy NLP (3, 396s):
18.13
PeriEM (3 × 10, 7132s):
17.75
EM (3, 5096s):
9.29
HPI+NLP (4, 111s):

Hallway2
(|S | = 93, |A| = 5, |O| = 17)
0.35
Perseus (56, 10s):
0.35
HSVI2 (114, 1.5s):
0.35
PBPI (320, 3.1s):
0.35
SARSOP (776, 7211s):
0.35
HSVI (1571, 10010s):
0.34
PBVI (95, 360s):
0.34
Peri (160 × 60, 5252s):
0.32
biased BPI (60, 790s):
0.29
NLP ﬁxed (18, 240s):
0.28
NLP (13, 420s):
0.28
EM (30, 7129s):
0.028
Mealy NLP (1, 2s):

Tag-avoid
(|S | = 870, |A| = 5, |O| = 30)
−5.87
PBPI (818, 1133s):
SARSOP (13588, 7394s): −6.04
Peri (160 × 60, 6394s):
−6.15
RTDP-BEL (2.5m, 493s): −6.16
−6.17
Perseus (280, 1670s):
−6.36
HSVI2 (415, 24s):
−6.65
Mealy NLP (2, 323s):
−6.65
biased BPI (17, 250s):
−9.18
BPI (940, 59772s):
−13.94
NLP (2, 5596s):
−20.00
EM (2, 30s):

Tag-avoid repeat
(|S | = 870, |A| = 5, |O| = 30)
SARSOP (15202, 7203s):−10.71
Peri (160 × 60, 6316s): −11.02
−11.44
Mealy NLP (2, 319s):
−12.35
Perseus (163, 5656s):
−14.33
HSVI2 (8433, 5413s):
−20.00
NLP (1, 37s):
−20.00
EM (2, 72s):

Aloha
(|S | = 90, |A| = 29, |O| = 3)
SARSOP (82, 7201s): 1237.01
Peri (160 × 100, 6793s): 1236.70
1221.72
Mealy NLP (7, 312s):
1217.95
HSVI2 (5434, 5430s):
1211.67
NLP (6, 1134s):
1120.05
EM (3, 7200s):
853.42
Perseus (68, 5401s):

8

References

[1] R. D. Smallwood and E. J. Sondik. The optimal control of partially observable Markov pro-
cesses over a ﬁnite horizon. Operations Research, pages 1071–1088, 1973.

[2] S. Seuken and S. Zilberstein. Formal models and algorithms for decentralized decision making
under uncertainty. Autonomous Agents and Multi-Agent Systems, 17(2):190–250, 2008.

[3] H. Kurniawati, D. Hsu, and W.S. Lee. Sarsop: Efﬁcient point-based pomdp planning by ap-
proximating optimally reachable belief spaces. In Proc. Robotics: Science and Systems, 2008.

[4] S. Seuken and S. Zilberstein. Memory-bounded dynamic programming for DEC-POMDPs. In
Proc. of 20th IJCAI, pages 2009–2016. Morgan Kaufmann, 2007.

[5] F. Wu, S. Zilberstein, and X. Chen. Point-based policy generation for decentralized POMDPs.
In Proc. of 9th AAMAS, pages 1307–1314. IFAAMAS, 2010.

[6] P. Poupart and C. Boutilier. Bounded ﬁnite state controllers. Advances in neural information
processing systems, 16:823–830, 2003.

[7] M. Toussaint, S. Harmeling, and A. Storkey. Probabilistic inference for solving (PO)MDPs.
Technical report, University of Edinburgh, 2006.

[8] C. Amato, D. Bernstein, and S. Zilberstein. Optimizing Memory-Bounded Controllers for
Decentralized POMDPs. In Proc. of 23rd UAI, pages 1–8. AUAI Press, 2007.

[9] A. Kumar and S. Zilberstein. Point-Based Backup for Decentralized POMDPs: Complexity
and New Algorithms. In Proc. of 9th AAMAS, pages 1315–1322. IFAAMAS, 2010.

[10] Joni Pajarinen and Jaakko Peltonen. Efﬁcient Planning for Factored Inﬁnite-Horizon DEC-
POMDPs. In Proc. of 22nd IJCAI, pages 325–331. AAAI Press, July 2011.

[11] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The Complexity of Decentralized
Control of Markov Decision Processes. Mathematics of Operations Research, 27(4):819–840,
2002.

[12] A. Kumar and S. Zilberstein. Anytime Planning for Decentralized POMDPs using Expectation
Maximization. In Proc. of 26th UAI, 2010.

[13] D.S. Bernstein, E.A. Hansen, and S. Zilberstein. Bounded policy iteration for decentralized
POMDPs. In Proc. of 19th IJCAI, pages 1287–1292. Morgan Kaufmann, 2005.

[14] D. Szer and F. Charpillet. An optimal best-ﬁrst search algorithm for solving inﬁnite horizon
DEC-POMDPs. Proc. of 16th ECML, pages 389–399, 2005.

[15] C. Amato and S. Zilberstein. Achieving goals in decentralized POMDPs.
AAMAS, volume 1, pages 593–600. IFAAMAS, 2009.

In Proc. of 8th

[16] C. Amato, B. Bonet, and S. Zilberstein. Finite-State Controllers Based on Mealy Machines for
Centralized and Decentralized POMDPs. In Proc. of 24th AAAI, 2010.

[17] S. Ji, R. Parr, H. Li, X. Liao, and L. Carin. Point-based policy iteration. In Proc. of 22nd AAAI,
volume 22, page 1243, 2007.

[18] F.A. Oliehoek, M.T.J. Spaan, and N. Vlassis. Optimal and approximate q-value functions for
decentralized pomdps. Journal of Artiﬁcial Intelligence Research, 32(1):289–353, 2008.

9


Action-Gap Phenomenon in Reinforcement Learning

Amir-massoud Farahmand∗
School of Computer Science, McGill University
Montreal, Quebec, Canada

Abstract

Many practitioners of reinforcement learning problems have observed that often-
times the performance of the agent reaches very close to the optimal performance
even though the estimated (action-)value function is still far from the optimal one.
The goal of this paper is to explain and formalize this phenomenon by introducing
performance loss E (cid:2)V ∗ (X ) − V ˆπ (X )(cid:3) is upper bounded by O((cid:107) ˆQ − Q∗ (cid:107)1+ζ∞ ),
the concept of the action-gap regularity. As a typical result, we prove that for an
agent following the greedy policy ˆπ with respect to an action-value function ˆQ, the
in which ζ ≥ 0 is the parameter quantifying the action-gap regularity. For ζ > 0,
our results indicate smaller performance loss compared to what previous analyses
had suggested. Finally, we show how this regularity affects the performance of
the family of approximate value iteration algorithms.

1

Introduction

This paper introduces a new type of regularity in the reinforcement learning (RL) and planning
problems with ﬁnite-action spaces that suggests that the convergence rate of the performance loss to
zero is faster than what previous analyses had indicated. The effect of this regularity, which we call
the action-gap regularity, is that oftentimes the performance of the RL agent reaches very close to
the optimal performance (e.g., it always solves the mountain-car problem with the optimal number
of steps) even though the estimated action-value function is still far from the optimal one.
Figure 1 illustrates the effect of this regularity in a simple problem. We use value iteration to
solve a stochastic 1D chain walk problem (slight modiﬁcation of the example in Section 9.1 of [1]).
The behavior of the supremum of the difference between the estimate after k iterations and the
optimal action-value function is O(γ k ), in which 0 ≤ γ < 1 is the discount factor (notations shall
be introduced in Section 2). The current theoretical results suggest that the convergence of the
performance loss, which is deﬁned as the average difference between the value of the optimal policy
and the value of the greedy policy w.r.t. (with respect to) the estimated action-value function, should
have the same O(γ k ) behavior (cf. Proposition 6.1 of Bertsekas and Tsitsiklis [2]). However, the
behavior of the performance loss is often considerably faster, e.g., it is approximately O(γ 1.85k ) in
this example.
To gain a better understanding of the action-gap regularity, focus on a single state and suppose that
there are only two actions available. When the estimated action-value function has a large error, the
greedy policy w.r.t. it can possibly choose the suboptimal action. However, when the error becomes
smaller than the (half of the) gap between the value of the optimal action and the other one, the
selected greedy action is the optimal action. After passing this threshold, the size of the error in
the estimate of the action-value function in that state does not have any effect on the quality of the
selected action. The larger the gap is, the more inaccurate the estimate can be while the selected
greedy action is the optimal one. On the other hand, if the estimated action-value function does not
suggest a correct ordering of actions but the gap is negligibly small, the performance loss of not
∗www.SoloGen.net

1

Figure 1: Comparison of the action-value estimation error (cid:107) ˆQ − Q∗ (cid:107)∞ and the performance loss
(cid:107)V ∗ − V ˆπ (cid:107)1 ( ˆπ is the greedy policy with respect to ˆQ) at different iterations of the value iteration
algorithm. The rate of decrease for the performance loss is considerably faster than that of the
estimation error. The problem is a 1D stochastic chain walk with 500 states and γ = 0.95.

choosing the optimal action is small as well. The presence of this gap in the optimal action-value
function is what we call the action-gap regularity of the problem and the described behavior is called
the action-gap phenomenon.
Action-gap regularity is similar to the low-noise (or margin) condition in the classiﬁcation literature.
The low-noise condition is the assumption that the conditional probability of the class label given
input is “far” from the critical decision point. If this condition holds, “fast” convergence rate is
obtainable as was shown by Mammen and Tsybakov [3], Tsybakov [4], Audibert and Tsybakov
[5]. The low-noise condition is believed to be one reason that many high-dimensional classiﬁcation
problems can be solved with efﬁcient sample complexity (cf. Rinaldo and Wasserman [6]). We
borrow techniques developed in the classiﬁcation literature, in particular by Audibert and Tsybakov
[5], in our analysis.
It is notable that there have been some works that used classiﬁcation algorithms to solve reinforce-
ment learning (e.g., Lagoudakis and Parr [7], Lazaric et al. [8]) or the related problem of appren-
ticeship learning (e.g., Syed and Schapire [9]). Nevertheless, the connection of this work to the
classiﬁcation literature is only by borrowing theoretical ideas from that literature and not in using
any particular algorithm. The focus of this work is indeed on the value-based approaches, though
one might expect that similar behavior can be observed in classiﬁcation-based approaches as well.
In the rest of this paper, we formalize the action-gap phenomenon and prove that whenever the MDP
has a favorable action-gap regularity, fast convergence rate is achievable. Theorem 1 upper bounds
the performance loss of the greedy policy w.r.t. the estimated action-value function by a function of
the Lp -norm of the difference between the estimated action-value function and the optimal one. Our
result complements previous theoretical analyses of RL/Planning problems such as those by Antos
et al. [10], Munos and Szepesv ´ari [11], Farahmand et al. [12, 13], Maillard et al. [14], who mainly
focused on the quality of the (action-)value function estimate and ignored the action-gap regularity.
This synergy provides a clearer picture of what makes an RL/Planning problem easy or difﬁcult.
Finally as an example of Theorem 1, we address the question of how the errors caused at each
iteration of the Approximate Value Iteration (AVI) algorithm affect the quality of the outcome policy
and show that the AVI procedure beneﬁts from the action-gap regularity of the problem (Theorem 2).

2 Notations

In this section, we provide a brief summary of some of the concepts and deﬁnitions from the theory
of MDPs and RL. For more information, the reader is referred to Bertsekas and Tsitsiklis [2], Sutton
and Barto [15], Szepesv ´ari [16].

2

102030405060708090100110−410−310−210−1100101k (iteration number)Error/Loss  L!!error of the estimated action!value functionPerformance lossO("1.85k)O("k)For a space Ω, with σ -algebra σΩ , we deﬁne M(Ω) as the set of all probability measures over σΩ .
B (Ω) denotes the space of bounded measurable functions w.r.t. (with respect to) σΩ and B (Ω, L)
denotes the subset of B (Ω) with bound 0 < L < ∞.
A ﬁnite-action discounted MDP is a 5-tuple (X , A, P, R, γ ), where X is a measurable state space,
A is a ﬁnite set of actions, P : X × A → M(X ) is the transition probability kernel, R : X × A → R
is the reward distribution, and 0 ≤ γ < 1 is a discount factor. We denote r(x, a) = E [R(·|x, a)].
A measurable mapping π : X → A is called a deterministic Markov stationary policy, or just a policy
in short. An agent’s following a policy π in an MDP means that at each time step At = π(Xt ).
(P π )(A|x) (cid:44) (cid:82) P (dy |x, π(x))I{y∈A} and (P π )(B |x, a) (cid:44) (cid:82) P (dy |x, a)I{(y ,π(y))∈B} . The m-step
A policy π induces two transition probability kernels P π : X → M(X ) and P π : X × A →
M(X × A). For a measurable subset A of X and a measurable subset B of X × A, we deﬁne
as (P π )m (B |x, a) (cid:44) (cid:82)
transition probability kernel (P π )m : X ×A → M(X ×A) for m = 2, 3, · · · are inductively deﬁned
X P (dy |x, a)(P π )m−1 (B |y , π(y)) (similarly for (P π )m : X → M(X )).
B (X ) → B (X ) by (P V )(x) (cid:44) (cid:82)
Given a transition probability kernel P : X → M(X ), deﬁne the right-linear operator P ·
:
(ρP )(A) = (cid:82) ρ(dx)P (dy |x)I{y∈A} . A typical choice of P is (P π )m : M(X ) → M(X ). These
X P (dy |x)V (y). For a probability measure ρ ∈ M(X )
and a measurable subset A of X , deﬁne the left-linear operators ·P : M(X ) → M(X ) by
operators for P : X × A → M(X × A) are deﬁned similarly.
The value function V π and and the action-value function Qπ of a policy π are deﬁned as follows:
(cid:12)(cid:12)(cid:12) X1 = x
X (X × A) and the agent follows the policy π . Then V π (x) (cid:44) E (cid:104)(cid:80)∞
(cid:105)
Let (Rt ; t ≥ 1) be the sequence of rewards when the Markov chain is started from state X1 (state-
action (X1 , A1 ) for the action-value function) drawn from a positive probability distribution over
(cid:12)(cid:12)(cid:12) X1 = x, A1 = a
Qπ (x, a) (cid:44) E (cid:104)(cid:80)∞
(cid:105)
t=1 γ t−1Rt
and
t=1 γ t−1Rt
.
For a discounted MDP, we deﬁne the optimal value and optimal action-value functions by V ∗ (x) =
supπ V π (x) for all states x ∈ X and Q∗ (x, a) = supπ Qπ (x, a) for all state-actions (x, a) ∈ X ×A.
We say that a policy π∗ is optimal if it achieves the best values in every state, i.e., if V π∗
= V ∗ .
We say that a policy π is greedy w.r.t. an action-value function Q and write π = ˆπ(·; Q), if π(x) =
argmaxa∈A Q(x, a) holds for all x ∈ X (if there exist multiple maximizers, a maximizer is chosen
in an arbitrary deterministic manner). Greedy policies are important because a greedy policy w.r.t.
the optimal action-value function Q∗ is an optimal policy.
(for the action-value functions) are deﬁned as (T π V )(x) (cid:44) r(x, π(x)) + γ (cid:82)
For a ﬁxed policy π , the Bellman operators T π : B (X ) → B (X ) and T π : B (X × A) → B (X × A)
and (T πQ)(x, a) (cid:44) r(x, a) + γ (cid:82)
X V (y)P (dy |x, π(x))
X Q(y , π(y))P (dy |x, a). The ﬁxed point of the Bellman operator
(cid:110)
(cid:111)
r(x, a) + γ (cid:82)
is the (action-)value function of the policy π , i.e., T πQπ = Qπ and T π V π = V π . Similarly, the
Bellman optimality operators T ∗ : B (X ) → B (X ) and T ∗ : B (X × A) → B (X × A) (for the
and (T ∗Q)(x, a) (cid:44) r(x, a) + γ (cid:82)
R×X V (y)P (dr, dy |x, a)
action-value functions) are deﬁned as (T ∗V )(x) (cid:44) maxa
R×X maxa(cid:48) Q(y , a(cid:48) )P (dr, dy |x, a). Again, these operators enjoy
a ﬁxed-point property similar to that of the Bellman operators: T ∗Q∗ = Q∗ and T ∗V ∗ = V ∗ .
(cid:44) (cid:2)(cid:82)
X |V (x)|p dρ(x)(cid:3)1/p . The L∞ (X )-norm is deﬁned as
For a probability measure ρ ∈ M(X ), and a measurable function V ∈ B (X ), we deﬁne the Lp (ρ)-
norm (1 ≤ p < ∞) of V as (cid:107)V (cid:107)p,ρ
(cid:44) (cid:104) 1|A|
(cid:105)1/p
(cid:80)|A|
(cid:107)V (cid:107)∞ (cid:44) supx∈X |V (x)|. For ρ ∈ M(X × A) and Q ∈ B (X × A), we deﬁne (cid:107)Q(cid:107)p,ρ (1 ≤ p < ∞)
and (cid:107)Q(cid:107)∞ (cid:44) sup(x,a)∈X ×A |Q(x, a)|.
a=1 (cid:107)Q(·, a)(cid:107)p
by (cid:107)Q(cid:107)p,ρ
p,ρ

3 Action-Gap Theorem

In this section, we present the action-gap theorem for an MDP (X , A, P, R, γ ). To simplify the
analysis, we assume that the number of actions |A| is only 2. We denote ρ∗ ∈ M(X ) as the station-

3

Figure 2: The action-gap function gQ∗ (x) and the relative ordering of the optimal and the estimated
action-value functions for a single state x. Depending on the ordering of the estimates, the greedy
action is the same as ((cid:88)) or different from (X) the optimal action. This ﬁgure does not show all
possible conﬁgurations.

ary distribution induced by π∗ , and we let ρ ∈ M(X ) be a user-speciﬁed evaluation distribution.
This distribution indicates the relative importance of regions of the state space to the user.
Suppose that algorithm A receives a dataset Dn = {(X1 , A1 , R1 , X (cid:48)
n )}
1 ), . . . , (Xn , An , Rn , X (cid:48)
(with Ri is being drawn from R(·|Xt , At ) and X (cid:48)
t is being drawn from P (·|Xt , At )) and outputs ˆQ
as an estimate of the optimal action-value function, i.e., ˆQ ← A(Dn ). The exact nature of this algo-
rithm is not important and it can be any online or ofﬂine, batch or incremental algorithms of choice
such as Q-learning, SARSA [15], and their variants [17], LSPI [1], LARS-TD [18] in a policy it-
eration procedure, REG-LSPI [13], various Fitted Q-Iterations algorithms [19, 20, 12], or Linear
Programming-based approaches [21, 22]. The only relevant aspect of ˆQ is how well it approximates
Q∗ . We quantify the quality of the approximation by the Lp -norm (cid:107) ˆQ − Q∗ (cid:107)p,ρ∗ (p ∈ [1, ∞]).
The performance loss (or regret) of a policy π is the expected difference between the value of the
(cid:90)
optimal policy π∗ to the value of π when the initial state distribution is selected according to ρ, i.e.,
(V ∗ (x) − V π (x)) dρ(x).
Loss(π ; ρ) (cid:44)
X
The value of Loss( ˆπ ; ρ), in which ˆπ is the greedy policy w.r.t. ˆQ, is the main quantity of interest
and indicates how much worse the agent following policy ˆπ would perform, in average, compared
to the optimal one. The choice of ρ enables the user to specify the relative importance of regions in
the state space.
We deﬁne the action(-value)-gap function gQ∗ : X → R as
gQ∗ (x) (cid:44) |Q∗ (x, 1) − Q∗ (x, 2)| .
This gap is shown in Figure 2. The following assumption quantiﬁes the action-gap regularity.
Assumption A1 (Action-Gap). For a ﬁxed MDP (X , A, P, R, γ ) with |A| = 2, there exist con-
(cid:90)
stants cg > 0 and ζ ≥ 0 such that for all t > 0, we have
Pρ∗ (0 < gQ∗ (X ) ≤ t) (cid:44)
I{0 < gQ∗ (x) ≤ t} dρ∗ (x) ≤ cg tζ .
X

(1)

The value of ζ controls the distribution of the action-gap gQ∗ (X ). A large value of ζ indicates that
the probability that Q(X, 1) being very close to Q(X, 2) is small and vice versa. The smallness of
this probability implies that the estimated action-value function ˆQ might be rather inaccurate in a

4

Figure 3: The probability distribution Pρ∗ (0 < gQ∗ (X ) ≤ t) for a 1D stochastic chain walk with
500 states and γ = 0.95. Here the probability of the action-gap being close to zero is small.

large subset of the state space (measured according to ρ∗ ) but its corresponding greedy policy would
still be the same as the optimal one. The case of ζ = 0 and cg = 1 is equivalent to not having
any assumption on the action-gap. This assumption is inspired by the low-noise condition in the
classiﬁcation literature [5]. As an example of a typical behavior of an action-gap function, Figure 3
depicts Pρ∗ (0 < gQ∗ (X ) ≤ t) for the same 1D stochastic chain walk problem as mentioned in the
Introduction. It is seen that the probability that the action-gap function gQ∗ being close to zero is
very small. Note that the speciﬁc polynomial form of the upper bound in Assumption A1 is only a
modeling assumption that captures the essence of the action-gap regularity without trying to be too
general to lead to unnecessarily complicated analyses.
As a result of the dynamical nature of MDP, the performance loss depends not only on the choice
of ρ and ρ∗ , but also on the transition probability kernel P . To analyze this dependence, we deﬁne
a concentrability coefﬁcient and use a change of measure argument similar to the work of Munos
[23, 24], Antos et al. [10].
Deﬁnition 1 (Concentrability of the Future-State Distribution). Given ρ, ρ∗ ∈ M(X ), a policy π ,
and an integer m ≥ 0, let ρ(P π )m ∈ M(X ) denote the future-state distribution obtained when
the ﬁrst state is distributed according to ρ and we then follow the policy π for m steps. Denote the
(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)(cid:13) d(ρ(P π )m )
supremum of the Radon-Nikodym derivative of ρ(P π )m w.r.t. ρ∗ by c(m; π), i.e.,
c(m; π) (cid:44)
.
dρ∗
If ρ(P π )m is not absolutely continuous w.r.t. ρ∗ , we set c(m; π) = ∞. The concentrability of the
(cid:88)
future-state distribution coefﬁcient is deﬁned as
C (ρ, ρ∗ ) (cid:44) sup
m≥0
π

γm c(m; π).

The following theorem upper bounds the performance loss Loss( ˆπ ; ρ) as a function of (cid:107)Q∗ − ˆQ(cid:107)p,ρ∗ ,
the action-gap distribution, and the concentrability coefﬁcient.
Theorem 1. Consider an MDP (X , A, P, R, γ ) with |A| = 2 and an estimate ˆQ of the optimal
action-value function. Let Assumption A1 hold and C (ρ, ρ∗ ) < ∞. Denote ˆπ as the greedy policy
21+ζ cg C (ρ, ρ∗ )
(cid:13)(cid:13)(cid:13) ˆQ − Q∗(cid:13)(cid:13)(cid:13)1+ζ
w.r.t. ˆQ. We then have
(cid:13)(cid:13)(cid:13) ˆQ − Q∗(cid:13)(cid:13)(cid:13) p(1+ζ)
∞ ,
p−1
g C (ρ, ρ∗ )
21+ p(1+ζ)
p+ζ
p+ζ
p+ζ c
p,ρ∗
5

(1 ≤ p < ∞)

Loss( ˆπ ; ρ) ≤

.

00.250.50.7511.251.51.75200.10.20.30.40.50.60.70.80.91tP (0 < gQ(X) ! t)γm

γm

(2)

We have

F2 (x) =

P (dy |x, ˆπ(x))Qπ∗

−

= F1 (x) + F2 (x).

(x, π∗ (x)) −

(cid:21)
(y , π∗ (y))

P (dy |x, ˆπ(x))Q ˆπ (y , ˆπ(y))

Proof. Let function F : X → R be deﬁned as F (x) = V ∗ (x) − V ˆπ (x) = Qπ∗
(cid:16)
(cid:17)
(cid:16)
(cid:17)
Q ˆπ (x, ˆπ(x)) for any x ∈ X . Note that Loss( ˆπ ; ρ) = ρF . Decompose F (x) as
(x, π∗ (x)) − Qπ∗
(x, ˆπ(x)) − Q ˆπ (x, ˆπ(x))
Qπ∗
Qπ∗
+
(x, ˆπ(x))
F (x) =
(cid:20)
(cid:21)
(cid:90)
(cid:20)
(cid:90)
r(x, ˆπ(x)) + γ
X
r(x, ˆπ(x)) + γ
X
Therefore, F = (I − γP ˆπ )−1F1 = (cid:80)
= γP ˆπ (·|x)F (·).
(cid:90)
(cid:88)
(cid:88)
(cid:0)ρ(P ˆπ )m (cid:1) (dy)F1 (y)
m≥0 (γP ˆπ )mF1 . Thus,
d (cid:0)ρ(P ˆπ )m (cid:1)
(cid:90)
ρ(γP ˆπ )mF1 =
ρF =
(cid:88)
X
m≥0
m≥0
(y)dρ∗ (y)F1 (y)
≤ (cid:88)
=
dρ∗
X
m≥0
γm c(m; ˆπ)ρ∗F1 ≤ C (ρ, ρ∗ ) ρ∗F1 .
m≥0
in which we used the Radon-Nikodym theorem and the deﬁnition of concentrability coefﬁcient. Let
us turn to F1 and provide an upper bound for it. We use techniques similar to [5].
L∞ result: Note that for any given x ∈ X , if for some value of ε > 0 we have ˆπ(x) (cid:54)= π∗ (x)
and |Qπ∗
(x, a) − ˆQ(x, a)| ≤ ε (for both a = 1, 2), then it holds that gQ∗ (x) = |Qπ∗
(x, 1) −
(x, 2)| ≤ 2ε. To show it, suppose that instead gQ∗ (x) = |Qπ∗
(x, 2)| > 2ε.
(x, 1) − Qπ∗
Qπ∗
Then because of the assumption |Qπ∗
(x, a) − ˆQ(x, a)| ≤ ε (a = 1, 2), the ordering of ˆQ(x, 1) and
ˆQ(x, 2) is the same as the ordering of Q∗ (x, 1) and Q∗ (x, 2), which contradicts the assumption that
ˆπ(x) (cid:54)= π∗ (x) (see Figure 2).
(cid:104)
(cid:105)
Denote ε0 = (cid:107)Qπ∗ − ˆQ(cid:107)∞ . Whenever ˆπ(x) = π∗ (x), the value of F1 (x) is zero, so we get
(cid:105) I{ ˆπ(x) (cid:54)= π∗ (x)}
(cid:104)
(x, π∗ (x)) − Qπ∗
[I{ ˆπ(x) = π∗ (x)} + I{ ˆπ(x) (cid:54)= π∗ (x)}]
Qπ∗
(x, ˆπ(x))
F1 (x) =
(x, 1 − π∗ (x))
(x, π∗ (x)) − Qπ∗
Qπ∗
× [I{gQ∗ (x) = 0} + I{0 < gQ∗ (x) ≤ 2ε0} + I{gQ∗ (x) > 2ε0}]
≤ 0 + 2ε0 I{0 < gQ∗ (x) ≤ 2ε0} + 0.
Here we used the deﬁnition of gQ∗ (x) and the fact that gQ∗ (x) is no larger than 2ε0 . This result
together with Assumption A1 show that ρ∗F1 ≤ 2ε0 Pρ∗ (0 < gQ∗ (X ) ≤ 2ε0 ) ≤ 2ε0 cg (2ε0 )ζ .
Plugging this result in (2) ﬁnishes the proof of the ﬁrst part.
(x, 2) − ˆQ(x, 2)|.
(x, 1) − ˆQ(x, 1)| + |Qπ∗
Lp result: For any given x ∈ X , let D(x) = |Qπ∗
(cid:104)
(cid:105) I{ ˆπ(x) (cid:54)= π∗ (x)}
Whenever ˆπ(x) (cid:54)= π∗ (x), we have gQ∗ (x) ≤ D(x). Similar to the previous case, we have
(x, π∗ (x)) − Qπ∗
(x, 1 − π∗ (x))
Qπ∗
F1 (x) =
× [I{gQ∗ (x) = 0} + I{0 < gQ∗ (x) ≤ t} + I{gQ∗ (x) > t}]
≤ D(x) [I{0 < gQ∗ (x) ≤ t} + I{gQ∗ (x) > t}]
Take expectation w.r.t. ρ∗ and use H ¨older’s inequality to get
≤ (cid:107)D(cid:107)p,ρ∗ (cid:0)cg tζ (cid:1) p−1
p−1
p + (cid:107)D(cid:107)p,ρ∗ [Pρ∗ (gQ∗ (X ) > t)]
ρ∗F1 ≤ (cid:107)D(cid:107)p,ρ∗ [Pρ∗ (0 < gQ∗ (X ) ≤ t)]
p−1
p + (cid:107)D(cid:107)p,ρ∗ [Pρ∗ (D(X ) > t)]
≤ (cid:107)D(cid:107)p,ρ∗ (cid:0)cg tζ (cid:1) p−1
p
(cid:107)D(cid:107)p
p,ρ∗
p +
tp−1

=

p−1
p

.

6

where we used Assumption A1 and the deﬁnition of D(·) in the second inequality, and Markov’s
−1
(cid:107)D(cid:107) p
inequality in the last one. Minimize the upper bound in t to get t = c
p,ρ∗ . This leads to
p+ζ
p+ζ
g
p−1
p,ρ∗ ≤ 2p(cid:107)Qπ∗ − ˆQ(cid:107)p
, which in turn alongside inequality (2) and (cid:107)D(cid:107)p
ρ∗F1 ≤ 2c
(cid:107)D(cid:107) p(1+ζ)
p+ζ
p+ζ
p,ρ∗
p,ρ∗
g
proves the second part of this result.
This theorem indicates that if (cid:107) ˆQ − Q∗ (cid:107)p (1 < p ≤ ∞) has an error upper bound of O(n−β ) (with
β typically in the range of (0, 1/2] depending on the properties of the MDP and the estimator), we
obtain faster convergence upper bounds on the performance loss Loss( ˆπ ; ρ) whenever the problem
has an action-gap regularity (ζ > 0).
One might compare Theorem 1 with classical upper bounds such as (cid:107)V ˆπ − V π∗ (cid:107)∞ ≤ 2γ
1−γ (cid:107) ˆV −
V ∗ (cid:107)∞ (Proposition 6.1 of Bertsekas and Tsitsiklis [2]). In order to make these two bounds compa-
rable, we slightly modify the proof of our theorem to get the L∞ -norm in the left hand side. The
result would be (cid:107)V ∗ − V ˆπ (cid:107)∞ ≤ 21+ζ cg
1−γ (cid:107) ˆQ − Q∗ (cid:107)1+ζ∞ . If there is no action-gap assumption (ζ = 0
and cg = 1), the results are similar (except for a factor of γ and that we measure the error by the
difference in the action-value function as opposed to the value function), but when ζ > 0 the error
bound signiﬁcantly improves.

4 Application of the Action-Gap Theorem in Approximate Value Iteration

The goal of this section is to show how the analysis based on the action-gap phenomenon might lead
to a tighter upper bound on the performance loss for the family of the AVI algorithms. There are
various AVI algorithms (Riedmiller [19], Ernst et al. [20], Munos and Szepesv ´ari [11], Farahmand
et al. [12]), that work by generating a sequence of action-value function estimates ( ˆQk )K
k=0 , in
which ˆQk+1 is the result of approximately applying the Bellman optimality operator to the previous
estimate ˆQk , i.e., ˆQk+1 ≈ T ∗ ˆQk . Let us denote the error caused at each iteration by
εk (cid:44) T ∗ ˆQk − ˆQk+1 .
(3)
The following theorem, which is based on Theorem 3 of Farahmand et al. [25], relates the per-
formance loss (cid:107)Q ˆπ(·; ˆQK ) − Q∗ (cid:107)1,ρ of the obtained greedy policy ˆπ(·; ˆQK ) to the error sequence
(εk )K−1
(cid:40) (1−γ )
k=0 and the action-gap assumption on the MDP. Before stating the theorem, we deﬁne the
following sequence:
0 ≤ k < K,
1−γK+1 γK−k−1
This sequence has αk ∝ γK−k behavior and satisﬁes (cid:80)K
αk =
(1−γ )
1−γK+1 γK
k = K.
k=0 αk = 1.
Theorem 2 (Error Propagation for AVI). Consider an MDP (X , A, P, R, γ ) with |A| = 2 that
satisﬁes Assumption A1 and has C (ρ, ρ∗ ) < ∞. Let p ≥ 1 be a real number and K be a positive
k=0 ⊂ B (X × A, Qmax ) and the corresponding sequence
integer. Then for any sequence ( ˆQk )K
(cid:35) 1+ζ
(cid:34)K−1(cid:88)
(εk )K−1
(cid:19) p(1+ζ)
(cid:18) 2
k=0 deﬁned in (3), we have
p+ζ
p+ζ
1 − γ
k=0

αk (cid:107)εk (cid:107)p
p,ρ∗ + αK (2Qmax )p

Loss( ˆπ(·, QK ); ρ) ≤ 2

p−1
g C (ρ, ρ∗ )
p+ζ
c

.

Proof. Similar to Lemma 4.1 of Munos [24], one may derive
Q∗ − ˆQk+1 = T π∗
(Q∗ − ˆQk ) + εk
Q∗ − T π∗ ˆQk + T π∗ ˆQk − T ∗ ˆQk + εk ≤ γP π∗
where we used the property of the Bellman optimality operator T ∗ ˆQk ≥ T π∗ ˆQk and the deﬁnition
Q∗ − ˆQK ≤ K−1(cid:88)
of εk (3). By induction, we get
k=0

)K−k−1 εk + γK (P π∗

)K (Q∗ − ˆQ0 ).

γK−k−1 (P π∗

7

ρ∗ |Q∗ − ˆQK |p ≤

αk ρ∗ (P π∗

,

≤

(cid:35)p
(cid:19)p (cid:34)K−1(cid:88)
(cid:18) 1 − γK+1
Therefore, for any p ≥ 1, the value of (cid:107)Q∗ − ˆQK (cid:107)p,ρ∗ = ρ∗ |Q∗ − ˆQK |p is upper bounded by
)K−k−1 |εk | + αK ρ∗ (P π∗
)K |Q∗ − ˆQ0 |
(cid:35)
(cid:19)p (cid:34)K−1(cid:88)
(cid:18) 1 − γK+1
1 − γ
k=0
1 − γ
k=0
)m = ρ∗ (for any m ≥ 0) and Jensen’s inequality. The application of
where we used ρ∗ (P π∗
Theorem 1 and noting that (1 − γK+1 )/(1 − γ ) ≤ 1/(1 − γ ) lead to the desired result.
(cid:80)K−1
Comparing this theorem with Theorem 3 of Farahmand et al. [25] is instructive. Denoting E =
k=0 αk (cid:107)εk (cid:107)2
2,ρ∗ , this paper’s result indicates that the effect of the size of εk on Loss( ˆπ(·, ˆQK ); ρ)
depends on E 1+ζ
2+ζ , while [25], which does not consider the action-gap regularity, suggests that the
effect depends on E 1/2 . For ζ > 0, this indicates a faster convergence rate for the performance loss
while for ζ = 0, they are the same.

αk (cid:107)εk (cid:107)p
p,ρ∗ + αK (2Qmax )p

5 Conclusion

This work introduced the action-gap regularity in reinforcement learning and planning problems
and analyzed the action-gap phenomenon for two-action discounted MDPs. We showed that when
the problem has a favorable action-gap regularity, quantiﬁed by the parameter ζ , the performance
loss is much smaller than the error of the estimated optimal action-value function. The action-gap
regularity, among other regularities such as the smoothness of the action-value function [13], is a
step forward to better understanding of what properties of a sequential decision-making problem
makes learning and planning easy or difﬁcult.
There are several issues that deserve to be studied in the future. Among them is the extension of
the current framework to multi-action discounted MDPs. Also it is important to study the relation
between the parameter ζ of the action-gap regularity assumption to the properties of the MDP such
as the transition probability kernel and the reward distribution.

Acknowledgments

I thank the anonymous reviewers for their useful comments. This work was partly supported by
AICML and NSERC.

References
[1] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine
Learning Research, 4:1107–1149, 2003.
[2] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and
Neural Computation Series, 3). Athena Scientiﬁc, 1996.
[3] Enno Mammen and Alexander B. Tsybakov. Smooth discrimination analysis. The Annals of
Statistics, 27(6):1808–1829, 1999.
[4] Alexander B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals
of Statistics, 32 (1):135–166, 2004.
[5] Jean-Yves Audibert and Alexander B. Tsybakov. Fast learning rates for plug-in classiﬁers. The
Annals of Statistics, 35(2):608–633, 2007.
[6] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. The Annals of
Statistics, 38(5):2678–2722, 2010.
[7] Michail G. Lagoudakis and Ronald Parr. Reinforcement learning as classiﬁcation: Leveraging
modern classiﬁers. In ICML ’03: Proceedings of the 20th international conference on Machine
learning, pages 424–431, 2003.

8

[8] Alessandro Lazaric, Mohammad Ghavamzadeh, and R ´emi Munos. Analysis of a classiﬁcation-
based policy iteration algorithm. In Proceedings of the 27th International Conference on Ma-
chine Learning (ICML-10), pages 607–614. Omnipress, 2010.
[9] Omar Syed and Robert E. Schapire. A reduction from apprenticeship learning to classiﬁcation.
In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems (NIPS - 23), pages 2253–2261, 2010.
[10] Andr ´as Antos, Csaba Szepesv ´ari, and R ´emi Munos. Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine
Learning, 71:89–129, 2008.
[11] R ´emi Munos and Csaba Szepesv ´ari. Finite-time bounds for ﬁtted value iteration. Journal of
Machine Learning Research, 9:815–857, 2008.
[12] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv ´ari, and Shie Mannor.
Regularized ﬁtted Q-iteration for planning in continuous-space Markovian Decision Problems.
In Proceedings of American Control Conference (ACC), pages 725–730, June 2009.
[13] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv ´ari, and Shie Mannor.
Regularized policy iteration. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,
Advances in Neural Information Processing Systems (NIPS - 21), pages 441–448. MIT Press,
2009.
[14] Odalric Maillard, R ´emi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh. Finite-
sample analysis of Bellman residual minimization. In Proceedings of the Second Asian Con-
ference on Machine Learning (ACML), 2010.
[15] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT Press, 1998.
[16] Csaba Szepesv ´ari. Algorithms for Reinforcement Learning. Morgan Claypool Publishers,
2010.
[17] Hamid Reza Maei, Csaba Szepesv ´ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward
off-policy learning control with function approximation. In Johannes F ¨urnkranz and Thorsten
Joachims, editors, Proceedings of the 27th International Conference on Machine Learning
(ICML-10), pages 719–726, Haifa, Israel, June 2010. Omnipress.
[18] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares tempo-
ral difference learning. In ICML ’09: Proceedings of the 26th Annual International Conference
on Machine Learning, pages 521–528. ACM, 2009.
[19] Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural
reinforcement learning method. In 16th European Conference on Machine Learning, pages
317–328, 2005.
[20] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement
learning. Journal of Machine Learning Research, 6:503–556, 2005.
[21] Daniela Pucci de Farias and Benjamin Van Roy. The linear programming approach to approx-
imate dynamic programming. Operations Research, 51(6):850–865, 2003.
[22] Marek Petrik and Shlomo Zilberstein. Constraint relaxation in approximate linear programs.
In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,
pages 809–816, New York, NY, USA, 2009. ACM.
[23] R ´emi Munos. Error bounds for approximate policy iteration. In ICML 2003: Proceedings of
the 20th Annual International Conference on Machine Learning, pages 560–567, 2003.
[24] R ´emi Munos. Performance bounds in Lp norm for approximate value iteration. SIAM Journal
on Control and Optimization, pages 541–561, 2007.
[25] Amir-massoud Farahmand, R ´emi Munos, and Csaba Szepesv ´ari. Error propagation for ap-
proximate policy and value iteration. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems (NIPS -
23), pages 568–576. 2010.

9


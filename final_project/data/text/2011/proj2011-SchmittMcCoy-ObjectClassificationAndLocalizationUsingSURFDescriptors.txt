Ob ject Classiﬁcation and Localization Using SURF Descriptors

Drew Schmitt, Nicholas McCoy

December 13, 2011

This paper presents a method for identifying and match-
ing objects within an image scene. Recognition of this
type is becoming a promising ﬁeld within computer vision
with applications in robotics, photography, and security.
This technique works by extracting salient features, and
matching these to a database of pre-extracted features to
perform a classiﬁcation. Localization of the classiﬁed ob-
ject is performed using a hierarchical pyramid structure.
The proposed method performs with high accuracy on the
Caltech-101 image database, and shows potential to per-
form as wel l as other leading methods.

1

Introduction

There are numerous applications for ob ject recognition
and classiﬁcation in images. The leading uses of ob ject
classiﬁcation are in the ﬁelds of robotics, photography,
and security. Robots commonly take advantage of ob ject
classiﬁcation and localization in order to recognize certain
ob jects within a scene. Photography and security both
stand to beneﬁt from advancements in facial recognition
techniques, a subset of ob ject recognition.
Our method ﬁrst obtains salient features from an input
image using a robust local feature extractor. The leading
techniques for such a purpose include the Scale Invari-
ant Feature Transform (SIFT) and Speeded Up Robust
Features (SURF).
After extracting all keypoints and descriptors from the
set of training images, our method clusters these descrip-
tors into N centroids. This operation is performed using
the standard K-means unsupervised learning algorithm.
The key assumption in this paper is that the extracted
descriptors are independent and hence can be treated as a
“bag of words” (BoW) in the image. This BoW nomen-
clature is derived from text classiﬁcation algorithms in
classical machine learning.
For a query image, descriptors are extracted using the
same robust local feature extractor. Each descriptor is

mapped to its visual word equivalent by ﬁnding the near-
est cluster centroid in the dictionary. An ensuing count of
words for each image is passed into a learning algorithm
to classify the image.
A hierarchical pyramid scheme is incorporated into this
structure to allow for localization of classiﬁcations within
the image.
In Section 2, the local robust feature extractor used
in this paper is further discussed. Section 3 elaborates
on the K-means clustering technique. The learning algo-
rithm framework is detailed in Section 4. A hierarchical
pyramid scheme is presented in Section 5. Experimental
results and closing remarks are provided in Section 6.

2 SURF

Our method extracts salient features and descriptors from
images using SURF. This extractor is preferred over SIFT
due to its concise descriptor length. Whereas the stan-
dard SIFT implementation uses a descriptor consisting of
128 ﬂoating point values, SURF condenses this descriptor
length to 64 ﬂoating point values.
Modern feature extractors select prominent features by
ﬁrst searching for pixels that demonstrate rapid changes
in intensity values in both the horizontal and vertical di-
rections. Such pixels yield high Harris corner detection
scores and are referred to as keypoints. Keypoints are
searched for over a subspace of {x, y , σ} ∈ R3 . The vari-
able σ represents the Gaussian scale space at which the
keypoint exists. In SURF, a descriptor vector of length
64 is constructed using a histogram of gradient orienta-
tions in the local neighborhood around each keypoint.
Figure 1 shows the manner in which a SURF descriptor
vector is constructed. David Lowe provides the inclined
reader with further information on local robust feature
extractors [1].
The implementation of SURF used in this paper is pro-
vided by the library OpenSURF [2]. OpenSURF is an

1

words. Experimental methods verify the computational
eﬃciency of K-means as opposed to EM. Our speciﬁc ap-
plication necessitates rapid training and image classiﬁca-
tion, which precludes the use of the slower EM algorithm.

4 Learning Algorithms

Naive Bayes and Support Vector Machine (SVM) super-
vised learning algorithms are investigated in this paper.
The learning algorithms are used to classify an image
using the histogram vector constructed in the K-means
step.

4.1 Naive Bayes

φy=c =

1
m

1{y (i) = c}.

A Naive Bayes classiﬁer is applied to this BoW approach
to obtain a baseline classiﬁcation system. The probability
m(cid:88)
φy=c that an image ﬁts into a classiﬁcation c is given by
i=1
Additionally, the probability φk|y=c , that a certain cluster
centroid, k , will contain a word count, xk , given that it
(cid:18) m(cid:80)
(cid:19)
is in classiﬁcation c, is deﬁned to be
(cid:18) m(cid:80)
(cid:19)
1{y (i) = c}x(i)
k
i=1
1{y (i) = c}ni
i=1
Laplacian smoothing accounts for the null probabilities
of “words” not yet encountered. Using Equation 1 with
Equation 2, the classiﬁcation of a query image is given
(cid:32)
(cid:33)
n(cid:89)
by
φy=c
i=1

arg max
c

φi|y=c

.

φk|y=c =

+ 1

.

+ N

(1)

(2)

(3)

4.2 SVM

A natural extension to this baseline framework is to in-
troduce an SVM to classify the image based on its BoW.
Our investigation starts by considering an SVM with a
linear kernel

K (x, y) = xT y + c,

(4)

due to its simplicity and computational eﬃciency in train-
ing and classiﬁcation. An intrinsic ﬂaw of linear kernels

2

Figure 1: Demonstration of how SURF feature vector is
built from image gradients.

open-source, MATLAB-optimized keypoint and descrip-
tor extractor.

3 K-means

A key development in image classiﬁcation using keypoints
and descriptors is to represent these descriptors using a
BoW model. Although spatial and geometric relation-
ship information between descriptors is lost using this as-
sumption, the inherent simpliﬁcation gains make it highly
advantageous.
The descriptors extracted from the training images are
grouped into N clusters of visual words using K-means.
A descriptor is categorized into its cluster centroid using
a Euclidean distance metric. For our purposes, we choose
a value of N = 500. This parameter provides our model
with a balance between high bias (underﬁtting) and high
variance (overﬁtting).
For a query image, each extracted descriptor is mapped
into its nearest cluster centroid. A histogram of counts is
constructed by incrementing a cluster centroid’s number
of occupants each time a descriptor is placed into it. The
result is that each image is represented by a histrogram
vector of length N . It is necessary to normalize each his-
togram by its L2-norm to make this procedure invariant
to the number of descriptors used. Applying Laplacian
smoothing to the histogram appears to improve classiﬁ-
cation results.
K-means clustering is selected over Expectation Max-
imization (EM) to group the descriptors into N visual

is that they are unable to capture subtle correlations be-
tween separate words in the visual dictionary of length
N .
To improve on the linear kernel’s performance, non-
linear kernels are considered in spite of their increased
complexity and computation time. More speciﬁcally the
n(cid:88)
χ2 kernel given by
i=1

(xi − yi )2
xi + yi

K (x, y) = 1 − 2

(5)

,

is implemented.
Given that an SVM is a binary classiﬁer, and it is often
desirable to classify an image into more than two distinct
groups, multiple SVM’s must be used in conjunction to
produce a multiclass classiﬁcation.
A one-vs-one scheme can be used in which a diﬀerent
SVM is trained for each combination of individual classes.
An incoming image must be classiﬁed using each of these
scheme involves making (cid:0)N
(cid:1) diﬀerent classiﬁcations for
diﬀerent SVM’s. The resulting classiﬁcation of the image
is the class that tallies the most “wins”. The one-vs-one
2
N classes, which grows factorially with the number of
classes. This scheme also suﬀers from false positives if
an image is queried that does not belong to any of the
classes.
A more robust scheme is the one-vs-all classiﬁcation
system in which an SVM is trained to classify an image
as either belonging to class c, or belonging to class ¬c. For
the training data {(xi , yi )}m
i=1 , yi ∈ 1, ..., N , a multiclass
SVM aims to train N separate SVM’s that optimize the
dual optimization problem
m(cid:88)
m(cid:88)
i=1
i,j=1

y (i) y (j )αiαj K (x(i) , x(j ) ),

αi − 1
2

W (α) =

max
a

(6)
using John Platt’s SMO algorithm [3].
In Equation 6,
K (x, z ) corresponds to one of the Kernel functions dis-
cussed above.
(cid:41)
(cid:40) m(cid:88)
A query image is then classiﬁed using
αi y (i)K (x(i) , z )
i=1

sgn

(7)

,

where sgn(x) is an operator that returns the sign of its
argument and z is the query vector of BoW counts.
Figure 2 represents this concept visually. When the
query image is of class A, the A-vs-all SVM will classify

3

Figure 2: Portrayal of one-vs-all SVM. When query image
is of type A, the A-vs-all SVM will correctly classify it.
When the query image is not of class A, B , or C , it will
likely not be classiﬁed into any.

the image correctly, and thus the overall output will place
the image into class A. When the query image is of a
diﬀerent class, D , which is not already existent in the
class structure, the query will always fall into the “all”
class on the individual SVM’s. Hence, the query will not
be falsely categorized into any class.
It is important to reiterate that each multiclass SVM
only distinguishes between classes c and ¬c. A diﬀer-
ent SVM is trained in this manner for each class. Thus,
the number of SVM’s needed in a one-vs-all scheme only
grows linearly with the number of classes, N . This system
also does not suﬀer from as many false positives because
images that do not belong to any of the classes are usually
classiﬁed as such in each individual SVM.
The speciﬁc multiclass SVM implementation used in
this paper was MATLAB’s built-in version as described
by Kecman [4].

5 Ob ject Localization

The methods described thus far are suﬃcient for the role
of classifying an image into a class when an ob ject is
prominently displayed in the forefront of the image. How-
ever, in the case when the desired ob ject is a small subset
of the overall image, this ob ject classiﬁcation algorithm
will fail to classify it correctly. Additionally, there is mo-

Figure 3: Visual representation of partitioning an image
into sub-images and constructing the histograms.

Figure 4: Results showing both image classiﬁcation and
localization.

tivation to localize an ob ject in a scene using classiﬁcation
techniques. The solution to these shortcomings is ob ject
localization using a hierarchical pyramid scheme. Figure
3 illustrates the general idea behind extracting descrip-
tors using a pyramid scheme.
First, the set of image descriptors, D , are extracted
from the image using SURF. Next, the image is seg-
mented into L pyramid levels, where L is a user-selected
parameter that controls the granularity of the localiza-
tion search. Each level l, has subsections 0 ≤ i ≤ 4(l−1) ,
where 0 ≤ l ≤ (L − 1). At each level l, the entire set
of image descriptors, D , are segmented into a subgroup
d ∈ D for section i which can be found as
(cid:34)
(cid:32)
(cid:33)
(cid:32)
(cid:33)(cid:35)

i =

idiv

+ idiv

2l + 1, (8)

p.col − 1
C
2l

p.row − 1
R
2l

for a given pixel p. The notation idiv(x) represents an
integer division operator. The symbols R and C are the
maximum number of rows and columns, respectively, in
the original image. Then, for pixel p the votes at each
level of the pyramid can be tallied into an N x1 map com-
puted using
L−1(cid:88)
l=0

2l−11{labelpyr (l, i) = c}.

voteMap(c) =

(9)

The resulting eﬀect is that pixel p is most highly inﬂu-
enced by the label of its lowest-level containing subsec-
tion, in l = (L − 1), and less inﬂuenced by the label of its
highest-level containing subsection, in l = 0. The result-
ing label given to pixel p can then be calculated as

labelpix (p) = arg max
c

voteMap(c).

(10)

6 Results and Future Work

Figure 4 shows the classiﬁcation and localization results
of our proposed algorithm on a generic image consisting
of multiple classes of ob jects.
A more rigorous test of our method was done using
a subset of the CalTech-101 database [5]. Images falling
into the four categories of airplanes, cars, motorbikes, and
faces were trained and tested using our method. Figure 5
shows the improvement in percent correct classiﬁcations
in classiﬁcation of Naive Bayes, linear SVM, and nonlin-
ear SVM as the training set size increases.
The f -score, computed using the precision, P , and re-
call, R, of the algorithm by

f =

2P R
P + R

,

(11)

is perhaps a better indicator of performance because it
is a statistical measure of a test’s accuracy. Figure 6

4

Figure 5: Percent correct classiﬁcations of supervised
learning classiﬁers.

Figure 6: f -score of classiﬁcations of supervised learning
classiﬁers.

shows a visible improvement in the f -score for all three
classiﬁcation algorithms as the training set size increases.
The nonlinear SVM maintains the largest f -score over
all training set sizes, which aligns with our hypothesized
result.
Future work for this research should focus on replacing
K-means with a more robust clustering algorithm. One
option is Linearly Localized Codes (LLC) [6]. The LLC
method performs sparse coding on extracted descriptors
to make soft assignments that are more robust to local
spatial translations [7]. Furthermore, there is still open-
ended work to be done on the reconstruction of ob jects
using the individually labeled pixels from the pyramid lo-
calization scheme. Hrytsyk and Vlakh present a method
of conglomerating pixels into their neighboring groups in
an optimal fashion [8].

References

[1] D. Lowe. Towards a computational model for ob ject
recognition in IT cortex. Proc. Biological ly Motivated
Computer Vision, pages 2031, 2000.

[2] C. Evans. Opensurf.
http://www.chrisevansdev.com/computer-vision-

5

opensurf.html, retrieved 11/04/2011.

[3] J. Platt. Sequential Minimal Optimization: A Fast
Algorithm for Training Support Vector Machines,
1998.

[4] V. Kecman. Learning and Soft Computing, MIT
Press, Cambridge, MA. 2001.

[5] L. Fei-Fei, R. Fergus and P. Perona. Learning genera-
tive visual models from few training examples. CVPR,
2004.

[6] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spa-
tial pyramid matching using sparse coding for image
classiﬁcation. CVPR, 2009.

[7] T. Serre, L. Wolf, and T.Poggio. Ob ject recognition
with features inspired by visual cortex. CVPR, 2005.

[8] N. Hrytsyk, V. Vlakh. Method of conglomerates
recognition and their separation into parts. Methods
and Instruments of AI, 2009.

1002003004007075808590Percent Correct vs. Training Set SizePercent CorrectTraining Set Size  Naive BayesLinear SVMNon Linear SVM1002003004000.80.850.90.951F Score vs. Training Set SizeF ScoreTraining Set Size  Naive BayesLinear SVMNon Linear SVM
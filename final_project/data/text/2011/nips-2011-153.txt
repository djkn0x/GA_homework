Dimensionality Reduction
Using the Sparse Linear Model

Ioannis Gkioulekas
Harvard SEAS
Cambridge, MA 02138
igkiou@seas.harvard.edu

Todd Zickler
Harvard SEAS
Cambridge, MA 02138
zickler@seas.harvard.edu

Abstract

We propose an approach for linear unsupervised dimensionality reduction, based
on the sparse linear model that has been used to probabilistically interpret sparse
coding. We formulate an optimization problem for learning a linear projection
from the original signal domain to a lower-dimensional one in a way that approxi-
mately preserves, in expectation, pairwise inner products in the sparse domain. We
derive solutions to the problem, present nonlinear extensions, and discuss relations
to compressed sensing. Our experiments using facial images, texture patches, and
images of object categories suggest that the approach can improve our ability to
recover meaningful structure in many classes of signals.

1
Introduction
Dimensionality reduction methods are important for data analysis and processing, with their use
motivated mainly from two considerations: (1) the impracticality of working with high-dimensional
spaces along with the deterioration of performance due to the curse of dimensionality; and (2) the
realization that many classes of signals reside on manifolds of much lower dimension than that
of their ambient space. Linear methods in particular are a useful sub-class, for both the reasons
mentioned above, and their potential utility in resource-constrained applications like low-power
sensing [1, 2]. Principal component analysis (PCA) [3], locality preserving projections (LPP) [4],
and neighborhood preserving embedding (NPE) [5] are some common approaches. They seek to
reveal underlying structure using the global geometry, local distances, and local linear structure,
respectively, of the signals in their original domain; and have been extended in many ways [6–8]. 1
On the other hand, it is commonly observed that geometric relations between signals in their origi-
nal domain are only weakly linked to useful underlying structure. To deal with this, various feature
transforms have been proposed to map signals to different (typically higher-dimensional) domains,
with the hope that geometric relations in these alternative domains will reveal additional structure,
for example by distinguishing image variations due to changes in pose, illumination, object class,
and so on. These ideas have been incorporated into methods for dimensionality reduction by ﬁrst
mapping the input signals to an alternative (higher-dimensional) domain and then performing di-
mensionality reduction there, for example by treating signals as tensors instead of vectors [9, 10] or
using kernels [11]. In the latter case, however, it can be difﬁcult to design a kernel that is beneﬁcial
for a particular signal class, and ad hoc selections are not always appropriate.

In this paper, we also address dimensionality reduction through an intermediate higher-dimensional
space: we consider the case in which input signals are samples from an underlying dictionary model.
This generative model naturally suggests using the hidden covariate vectors as intermediate features,
and learning a linear projection (of the original domain) to approximately preserve the Euclidean ge-
ometry of these vectors. Throughout the paper, we emphasize a particular instance of this model that
is related to sparse coding, motivated by studies suggesting that data-adaptive sparse representations

1Other linear methods, most notably linear discriminant analysis (LDA), exploit class labels to learn pro-
jections. In this paper, we focus on the unsupervised setting.

1

are appropriate for signals such as natural images and facial images [12, 13], and enable state-of-
the-art performance for denoising, deblurring, and classi ﬁcation tasks [14–19].

Formally, we assume our input signal to be well-represented by a sparse linear model [20], previ-
ously used for probabilistic sparse coding. Based on this generative model, we formulate learning
a linear projection as an optimization problem with the objective of preservation, in expectation, of
pairwise inner products between sparse codes, without having to explicitly obtain the sparse repre-
sentation for each new sample. We study the solutions of this optimization problem, and we discuss
how they are related to techniques proposed for compressed sensing. We discuss applicability of our
results to general dictionary models, and nonlinear extensions. Finally, by applying our method to
the visualization, clustering, and classi ﬁcation of facia l images, texture patches, and general images,
we show experimentally that it improves our ability to uncover useful structure. Omitted proofs and
additional results can be found in the accompanying supplementary material.

2 The sparse linear model
We use RN to denote the ambient space of the input signals, and assume that each signal x ∈ RN is
generated as the sum of a noise term ε ∈ RN and a linear combination of the columns, or atoms, of
a N × K dictionary matrix D = [d1 , . . . , dK ], with the coefﬁcients arranged as a vector a ∈ RK ,
(1)
x = Da + ε.
We assume the noise to be white Gaussian, ε ∼ N (0N ×1 , σ2I N ×N ). We are interested in the
sparse linear model [20], according to which the elements of a are a-priori independent from ε and
are identically and independently drawn from a Laplace distribution,
K
exp (cid:26)− |ai |
τ (cid:27) .
Yi=1
In the context of this model, D is usually overcomplete (K > N ), and in practice often learned in
an unsupervised manner from training data. Several efﬁcien t algorithms exist for dictionary learn-
ing [21–23], and we assume in our analysis that a dictionary D adapted to the signals of interest is
given.

p (ai ) , p (ai ) =

p (a) =

1
2τ

(2)

Our adoption of the sparse linear model is motivated by signi ﬁcant empirical evidence that it is
accurate for certain signals of interest, such as natural and facial images [12, 13], as well as the
fact that it enables high performance for such diverse tasks as denoising and inpainting [14, 24],
deblurring [15], and classi ﬁcation and clustering [13, 16 –
19]. Typically, the model (1) with an
appropriate dictionary D is employed as a means for feature extraction, in which input signals x in
RN are mapped to higher-dimensional feature vectors a ∈ RK . When inferring features a (termed
sparse codes) through maximum-a-posteriori (MAP) estimation, they are solutions to

1
1
σ2 kx − Dak2
τ kak1 .
2 +
This problem, known as the lasso [25], is a convex relaxation of the more general problem of sparse
coding [26] (in the rest of the paper we use both terms interchangeably). A number of efﬁcient
algorithms for computing a exist, with both MAP [21, 27] and fully Bayesian [20] procedures.

min
a

(3)

3 Preserving inner products
Linear dimensionality reduction from RN to RM , M < N , is completely speci ﬁed by a projection
matrix L that maps each x ∈ RN to y = Lx, y ∈ RM , and different algorithms for linear di-
mensionality reduction correspond to different methods for ﬁnding this matrix. Typically, we are
interested in projections that reveal useful structure in a given set of input signals.

As mentioned in the introduction, structure is often better revealed in a higher-dimensional space
of features, say a ∈ RK . When a suitable feature transform can be found, this structure may exist
as simple Euclidean geometry and be encoded in pairwise Euclidean distances or inner products
between feature vectors. This is used, for example, in support vector machines and nearest-neighbor
classi ﬁers based on Euclidean distance, as well as k-means and spectral clustering based on pair-
wise inner products. For the problem of dimensionality reduction, this motivates learning a pro-
jection matrix L such that, for any two input samples, the inner product between their resulting
low-dimensional representations is close to that of their corresponding high-dimensional features.

2

(4)

min
LM ×N

min
LM ×N

More formally, for two samples xk , k = 1, 2 with corresponding low-dimensional representations
yk = Lxk and feature vectors ak , we deﬁne δp = yT
1 a2 as a quantity whose magnitude
1 y2 − aT
we want on average to be small. Assuming that an accurate probabilistic generative model for the
samples x and features a is available, we propose learning L by solving the optimization problem
(E denoting expectation with respect to subscripted variables)
E x1 ,x2 ,a1 ,a2 (cid:2)δp2 (cid:3) .
Solving (4) may in general be a hard optimization problem, depending on the model used for ak and
xk . Here we solve it for the case of the sparse linear model of Section 2, under which the feature
vectors are the sparse codes. Using (1) and denoting S = LT L, (4) becomes
1 S ε2(cid:17)2 i.
1 (cid:16)DT SD − I (cid:17) a2 + εT
E a1 ,a2 ,ε1 ,ε2 h(cid:16)aT
1 SDa2 + εT
2 SDa1 + εT
Assuming that x1 and x2 are drawn independently, we prove that (5) is equivalent to problem
2
4τ 4 (cid:13)(cid:13)(cid:13)
DT SD − I (cid:13)(cid:13)(cid:13)
F + σ4 kS k2
+ 4τ 2σ2 kSDk2
min
F ,
LM ×N
F
where k·kF is the Frobenius norm, which has the closed-form solution (up to an arbitrary rotation):
L = diag (f (λM )) V T
(7)
M .
Here, λM = (λ1 , . . . , λM ) is a M × 1 vector composed of the M largest eigenvalues of the N × N
matrix DDT , and V M is the N × M matrix with the corresponding eigenvectors as columns. The
function f (·) is applied element-wise to the vector λM such that
f (λi ) = s
4τ 4λi
σ4 + 4τ 2σ2λi + 4τ 4λ2
i
and diag (f (λM )) is the M × M diagonal matrix formed from f (λM ). This solution assumes that
DDT has full rank N , which in practice is almost always true as D is overcomplete.

(5)

(6)

(8)

,

2

F

, and L = diag (λM )− 1
2 V T
M .

Through comparison with (5), we observe that (6) is a trade-off between bringing inner products
of sparse codes and their projections close ( ﬁrst term), and suppressing noise (second and third
terms). Their relative inﬂuence is controlled by the varian ce of ε and a, through the constants σ
and τ respectively. It is interesting to compare their roles in (3) and (6): as σ increases relative to
τ , data ﬁtting in (3) becomes less important, and (7) emphasiz es noise suppression. As τ increases,
l1 -regularization in (3) is weighted less, and the ﬁrst term in (6) more. In the extreme case of σ = 0,
the data term in (3) becomes a hard constraint, whereas (6) and (7) simplify, respectively, to
LM ×N (cid:13)(cid:13)(cid:13)
DT SD − I (cid:13)(cid:13)(cid:13)
min
Interestingly, in this noiseless case, an ambiguity arises in the solution of (9), as a minimizer is
obtained for any subset of M eigenpairs and not necessarily the M largest ones.
ical —to the whitening transform of
The solution to (7) is similar—and in the noiseless case ident
the atoms of D . When the atoms are centered at the origin, this essentially means that solving (4)
for the sparse linear model amounts to performing PCA on dictionary atoms learned from training
samples instead of the training samples themselves. The above result can also be interpreted in the
setting of [28]: dimensionality reduction in the case of the sparse linear model with the objective
of (4) corresponds to kernel PCA using the kernel DDT , modulo centering and the normalization.
3.1 Other dictionary models
Even though we have presented our results using the sparse linear model described in Section 2,
it is important to realize that our analysis is not limited to this model. The assumptions required
for deriving (5) are that signals are generated by a linear dictionary model such as (1), where the
coefﬁcients of each of the noise and code vectors are indepen dent and identically distributed ac-
cording to some zero-mean distribution, with the two vectors also independent from each other.
The above assumptions apply for several other popular dictionary models. Examples include the
models used implicitly by ridge and bridge regression [29] and elastic-net [30], where the Laplace

(9)

3

prior on the code coefﬁcients is replaced by a Gaussian, and p riors of the form exp(−λ kakq
q ) and
exp(−λ kak1 − γ kak2
2 ), respectively. In the context of sparse coding, other sparsity-inducing priors
that have been proposed in the literature, such as Student’s t-distribution [31], also fall into the same
framework. We choose to emphasize the sparse linear model, however, due to the apparent structure
present in dictionaries learned using this model, and its empirical success in diverse applications.

2

It is possible to derive similar results for a more general model. Speci ﬁcally, we make the same as-
sumptions as above, except that we only require that elements of a be zero-mean and not necessarily
identically distributed, and similarly for ε. Then, we prove that (4) becomes
2
2
LM ×N (cid:13)(cid:13)(cid:13)(cid:16)DT SD − I (cid:17) ⊙ pW 1(cid:13)(cid:13)(cid:13)
(SD) ⊙ pW 2(cid:13)(cid:13)(cid:13)
S ⊙ pW 3(cid:13)(cid:13)(cid:13)
+ (cid:13)(cid:13)(cid:13)
+ (cid:13)(cid:13)(cid:13)
(10)
min
,
F
F
F
where ⊙ denotes the Hadamard product and (cid:16)√W (cid:17)ij
= q(W )ij . The elements of the weight
matrices W 1 , W 2 and W 3 in (10), of sizes K × K , N × K , and N × N respectively, are
(11)
1j (cid:3) , (W 3 )ij = E (cid:2)ε2
1i ε2
2j (cid:3) + E (cid:2)ε2
2ia2
2j (cid:3) , (W 2 )ij = E (cid:2)ε2
1ia2
(W 1 )ij = E (cid:2)a2
1ia2
2j (cid:3) .
Problem (10) can still be solved efﬁciently, see for example [32].
3.2 Extension to the nonlinear case
We consider a nonlinear extension of the above analysis through the use of kernels. We denote by
Φ : RN → H a mapping from the signal domain to a reproducing kernel Hilbert space H associated
with a kernel function k : RN × RN → R [33]. Using a set D = { ˜di ∈ H, i = 1, . . . , K } as
dictionary, we extend the sparse linear model of Section 2 by replacing (1) for each x ∈ RN with
(12)
Φ (x) = Da + ˜ε,
i=1 ai ˜di . For a ∈ RK we make the same assumptions as in the sparse linear model.
where Da ≡ PK
The term ˜ε denotes a Gaussian process over the domain RN whose sample paths are functions in H
and with covariance operator C ˜ε = σ2I , where I is the identity operator on H [33, 34].
This nonlinear extension of the sparse linear model is valid only in ﬁnite dimensional spaces H.
In the inﬁnite dimensional case, constructing a Gaussian pr ocess with both sample paths in H and
identity covariance operator is not possible, as that would imply that the identity operator in H
has ﬁnite Hilbert-Schmidt norm [33, 34]. Related problems a rise in the construction of cylindrical
Gaussian measures on inﬁnite dimensional spaces [35]. We de ﬁne ˜ε this way to obtain a probabilistic
model for which MAP inference of a corresponds to the kernel extension of the lasso (3) [36],
1
1
2σ2 kΦ (x) − Dak2
τ kak1 ,
min
H +
a∈RK
where k·kH is the norm H deﬁned through k . In the supplementary material, we discuss an alter-
native to (12) that resolves these problems by requiring that all Φ (x) be in the subspace spanned
by the atoms of D . Our results can be extended to this alternative, however in the following we
adopt (12) and limit ourselves to ﬁnite dimensional spaces H, unless mentioned otherwise.
In the kernel case, the equivalent of the projection matrix L (transposed) is a compact, linear operator
V : H → RM , that maps an element x ∈ RN to y = V Φ (x) ∈ RM . We denote by V ∗ : RM → H
the adjoint of V , and by S : H → H the self-adjoint positive semi-deﬁnite linear operator of
rank
M from their synthesis, S = V ∗V . If we consider optimizing over S , we prove that (4) reduces to
K
K
K
Xi=1 (cid:16)D ˜di , S ˜dj EH − δij (cid:17)2
Xi=1 DS ˜di , S ˜diEH
Xi=1
+ kS k2
(14)
+ 4τ 2σ2
4τ 4
min
H S ,
S
where k·kH S is the Hilbert-Schmidt norm. Assuming that K DD has full rank (which is almost
always true in practice due to the very large dimension of the Hilbert spaces used) we extend the
representer theorem of [37] to prove that all solutions of (14) can be written in the form
(15)
S = (DB ) ⊗ (DB ) ,
where ⊗ denotes the tensor product between all pairs of elements of its operands, and B is a K × M
matrix. Then, denoting Q = BB T , problem (14) becomes
+ σ4 (cid:13)(cid:13)(cid:13)
F + 4τ 2σ2 (cid:13)(cid:13)(cid:13)
DD (cid:13)(cid:13)(cid:13)
DD (cid:13)(cid:13)(cid:13)
1
1
4τ 4 kK DDQK DD − I k2
2
2

1
DDQK
2

K DDQK

min
BK×M

(16)

2

F

,

(13)

2

F

K

4

Figure 1: Two-dimensional projection of CMU PIE dataset, colored by identity. Shown at high resolution and
at their respective projections are identity-averaged faces across the dataset for various illuminations, poses,
and expressions. Insets show projections of samples from only two distinct identities. (Best viewed in color.)

1
where K DD (i, j ) = h ˜di , ˜dj iH , i, j = 1, . . . , K . We can replace ˜L = B T K
DD to turn (16) into an
2
1
equivalent problem over ˜L of the form (6), with K
DD instead of D , and thus use (8) to obtain
2

B = V M diag (g (λM ))

(17)

g (λi ) =

1
√λi

.

where, similar to the linear case, λM and V M are the M largest eigenpairs of the matrix K DD , and
f (λi ) = s
Using the derived solution, a vector x ∈ RN is mapped to y = B T K D (x), where K D (x) =
[h ˜d1 , Φ (x)iH , . . . , h ˜dM , Φ (x)iH ]T . As in the linear case, this is similar to the result of applying
kernel PCA on the dictionary D instead of the training samples. Note that, in the noiseless case,
σ = 0, the above analysis is also valid for inﬁnite dimensional sp aces H. Expression (17) simpli ﬁes
to B = V M diag (λM )−1 where, as in the linear case, any subset of M eigenvalues may be selected.
Even though in the inﬁnite dimensional case selecting the M largest eigenvalues cannot be justi ﬁed
probabilistically, it is a reasonable heuristic given the analysis in the ﬁnite dimensional case.

4τ 4
σ4 + 4τ 2σ2λi + 4τ 4λ2
i

(18)

3.3 Computational considerations
It is interesting to compare the proposed method in the nonlinear case with kernel PCA, in terms of
computational and memory requirements. If we require dictionary atoms to have pre-images in RN ,
that is D = (cid:8)Φ (di ) , di ∈ RN , i = 1, . . . , K (cid:9) [36], then the proposed algorithm requires calculating
and decomposing the K × K kernel matrix K DD when learning V , and performing K kernel
evaluations for projecting a new sample x. For kernel PCA on the other hand, the S ×S matrix K X X
and S kernel evaluations are needed respectively, where X = (cid:8)Φ (xi ) , xi ∈ RN , i = 1, . . . , S(cid:9) and
xi are the representations of the training samples in H, with S ≫ K . If the pre-image constraint is
dropped and the usual alternating procedure [21] is used for learning D , then the representer theorem
of [38] implies that D = X F , where F is an S × K matrix. In this case, the proposed method also
requires calculating K X X during learning and S kernel evaluations for out-of-sample projections,
but only the eigendecomposition of the K × K matrix F T K 2
X X F is required.
On the other hand, we have assumed so far, in both the linear and nonlinear cases, that a dictionary
is given. When this is not true, we need to take into account the cost of learning a dictionary,
which greatly outweights the computational savings described above, despite advances in dictionary
learning algorithms [21, 22].
In the kernel case, whereas imposing the pre-image constraint has
the advantages we mentioned, it also makes dictionary learning a harder nonlinear optimization
problem, due to the need for evaluation of kernel derivatives. In the linear case, the computational
savings from applying (linear) PCA to the dictionary instead of the training samples are usually
negligible, and therefore the difference in required computation becomes even more severe.

5

Figure 2: Classi ﬁcation accuracy results. From left to right: CMU PIE (varying valu e of M ); CMU PIE
(varying number of training samples); brodatz texture patches; Caltech-101. (Best viewed in color.)

4 Experimental validation
In order to evaluate our proposed method, we compare it with other unsupervised dimensionality
reduction methods on visualization, clustering, and classi ﬁcation tasks. We use facial images in the
linear case, and texture patches and images of object categories in the kernel case.
Facial images: We use the CMU PIE [39] benchmark dataset of faces under pose, illumination and
expression changes, and speci ﬁcally the subset used in [8]. 2 We visualize the dataset by projecting
all face samples to M = 2 dimensions using LPP and the proposed method, as shown in Figure 1.
Also shown are identity-averaged faces over the dataset, for various illumination, pose, and expres-
sion combinations, at the location of their projection. We observe that our method recovers a very
clear geometric structure, with changes in illumination corresponding to an ellipsoid, changes in
pose to moving towards its interior, and changes in expression accounting for the density on the
horizontal axis. We separately show the projections of samples from two distinct indviduals, and
see that different identities are mapped to parallely shifted ellipsoids, easily separated by a nearest-
neighbor classi ﬁer. On the other hand, such structure is not
apparent when using LPP. A larger
version of Figure 1 and the corresponding for PCA are provided in the supplementary material.

To assess how well identity structure is recovered for increasing values of the target dimension
M , we also perform face recognition experiments. We compare against three baseline methods,
PCA, NPE, and LPP, linear extensions (spectral regression “ SRLPP” [7], spatially smooth LPP
“SmoothLPP” [8]), and random projections (see Section 5). W e produce 20 random splits into
training and testing sets, learn a dictionary and projection matrices from the training set, and use the
obtained low-dimensional representations with a k-nearest neighbor classi ﬁer ( k = 4) to classify the
test samples, as is common in the literature. In Figure 2, we show the average recognition accuracy
for the various methods as the number of projections is varied, when using 100 training samples
for each of the 68 individuals in the dataset. Also, we compare the proposed method with the best
performing alternative, when the number of training samples per individual is varied from 40 to 120.
We observe that the proposed method outperforms all other by a wide margin, in many cases even
when trained with fewer samples. However, it can only be used when there are enough training
samples to learn a dictionary, a limitation that does not apply to the other methods. For this reason,
we do not experiment with cases of 5-20 samples per individual, as commonly done in the literature.
Texture patches: We perform classi ﬁcation experiments on texture patches, u sing the Brodatz
dataset [40], and speci ﬁcally classes 4, 5, 8, 12, 17, 84, and 92 from the 2-texture images. We
extract 12 × 12 patches and use those from the training images to learn dictionaries and projections
for the Gaussian kernel.3 We classify the low-dimensional representations using an one-versus-all
linear SVM. In Figure 2, we compare the classi ﬁcation accura cy of the proposed method ( “ker.dict ”)
with the kernel variants of PCA and LPP ( “KPCA” and “KLPP” res
pectively), for varying M . KLPP
and the proposed method both outperform KPCA. Our method achieves much higher accuracy at
small values of M , and KLPP is better for large values; otherwise they perform similarly.

This dataset provides an illustrative example for the discussion in Section 3.3. For 20000 training
samples, KPCA and KLPP require storing and processing a 20000×20000 kernel matrix, as opposed
to 512 × 512 for our method. On the other hand, training a dictionary with K = 512 for this dataset
takes approximately 2 hours, on an 8 core machine and using a C++ implementation of the learning
algorithm, as opposed to the few minutes required for the eigendecompositions in KPCA and KLPP.

2 Images are pre-normalized to unit length. We use the algorithm of [21] to learn dictionaries, with K equal
2
= 0.05 as in [19].
to the number of pixels N = 1024, due to the limited amount of training data, and λ = σ
τ
3Following [36], we set the kernel parameter γ = 8, and use their method for dictionary learning with
K = 512 and λ = 0.30, but with a conjugate gradient optimizer for the dictionary update step.

6

NMI
Accuracy
Method
0.6380
0.6217
KPCA (k-means)
0.6788
0.6900
KLPP (spectral clustering)
0.7188
0.7233
ker.dict (k-means)
Table 1: Clustering results on Caltech-101.

Rand Index
0.4279
0.5143
0.5275

Images of object categories: We use the Caltech-101 [41] object recognition dataset, with the
average of the 39 kernels used in [42]. Firstly, we use 30 training samples from each class to learn a
dictionary4 and projections using KPCA, KLPP, and the proposed method. In Figure 2, we plot the
classi ﬁcation accuracy achieved using a linear SVM for each method and varying M . We see that
the proposed method and KPCA perform similarly and outperform KLPP. Our algorithm performs
consistently well in both the datasets we experiment with in the kernel case.

We also perform unsupervised clustering experiments, where we randomly select 30 samples from
each of the 20 classes used in [43] to learn projections with the three methods, over a range of
values for M between 10 and 150. We combine each with three clustering algorithms, k-means,
spectral clustering [44], and afﬁnity propagation [43] (us ing negative Euclidean distances of the
low-dimensional representations as similarities). In Table 1, we report for each method the best
overall result in terms of accuracy, normalized mutual information, and rand index [45], along with
the clustering algorithm for which these are achieved. We observe that the low-dimensional repre-
sentations from the proposed method produce the best quality clusterings, for all three measures.

5 Discussion and future directions
As we remarked in Section 3, the proposed method uses available training samples to learn D and
ignores them afterwards, relying exclusively on the assumed generative model and the correlation
information in D . To see how this approach could fail, consider the degenerate case when D is the
identity matrix, that is the signal and sparse domains coincide. Then, to discover structure we need
to directly examine the training samples. Better use of the training samples within our framework
can be made by adopting a richer probabilistic model, using available data to train it, naturally
with appropriate regularization to avoid over ﬁtting, and t hen minimizing (4) for the learned model.
For example, we can use the more general model of Section 3.1, and assume that each ai follows a
Laplace distribution with a different τi . Doing so agrees with empirical observations that, when D is
learned, the average magnitude of coefﬁcients ai varies signi ﬁcantly with i. An orthogonal approach
is to forgo adopting a generative model, and learn a projection matrix directly from training samples
using an appropriate empirical loss function. One possibility is minimizing kAT A−X T LT LX k2
F ,
where the columns of X and A are the training samples and corresponding sparse code estimates,
which is an instance of multidimensional scaling [46] (as modi ﬁed to achieve linear induction).

For the sparse linear model case, objective function (4) is related to the Restricted Isometry Property
(RIP) [47], used in the compressed sensing literature as a condition enabling reconstruction of a
sparse vector a ∈ RK from linear measurements y ∈ RM when M ≪ K . The RIP is a worst-
case condition, requiring approximate preservation, in the low-dimensional domain, of pairwise
Euclidean distances of all a, and therefore stronger than the expectation condition (4). Verifying
the RIP for an arbitrary matrix is a hard problem, but it is known to hold for the equivalent dictio-
nary ˜D = LD with high probability, if L is drawn from certain random distributions, and M is
of the order of only O (cid:0)k log K
k (cid:1) [48]. Despite this property, our experiments demonstrate that a
learned matrix L is in practice more useful than random projections (see left of Figure 2). The for-
mal guarantees that preservation of Euclidean geometry of sparse codes is possible with few linear
projections are unique for the sparse linear model, thus further justifying our choice to emphasize
this model throughout the paper.

Another quantity used in compressed sensing is the mutual coherence of ˜D [49], and its approximate
minimization has been proposed as a way for learning L for signal reconstruction [50, 51]. One of
the optimization problems arrived at in this context [51] is the same as problem (9) we derived in
the noiseless case, the solution of which as we mentioned in Section 3 is not unique. This ambiguity
has been addressed heuristically by weighting the objective function with appropriate multiplicative
terms, so that it becomes kΛ−ΛV T LT LV Λk2
F , where Λ and V are eigenpairs of DDT [51]. This
4We use a kernel extension of the algorithm of [21] without pre-image constraints. We select K = 300
and λ = 0.1 from a range of values, to achieve about 10% non-zero coefﬁcients in the sparse codes and small
reconstruction error for the training samples. Using K = 150 or 600 affected accuracy by less than 1.5%.

7

problem admits as only minimizer the one corresponding to the M largest eigenvalues. Our analysis
addresses the above issue naturally by incorporating noise, thus providing formal justi ﬁcation for
the heuristic. Also, the closed-form solution of (9) is not shown in [51], though its existence is
mentioned, and the (weighted) problem is instead solved through an iterative procedure.

In Section 3, we motivated preserving inner products in the sparse domain by considering exist-
ing algorithms that employ sparse codes. As our understanding of sparse coding continues to im-
prove [52], there is motivation for considering other structure in RK . Possibilities include preserva-
tion of linear subspace (as determined by the support of the sparse codes) or local group relations
in the sparse domain. Extending our analysis to also incorporate supervision is another important
future direction.

Linear dimensionality reduction has traditionally been used for data preprocessing and visualiza-
tion, but we are also beginning to see its utility for low-power sensors. A sensor can be designed to
record linear projections of an input signal, instead of the signal itself, with projections implemented
through a low-power physical process like optical ﬁltering . In these cases, methods like the ones
proposed in this paper can be used to obtain a small number of informative projections, thereby
reducing the power and size of the sensor while maintaining its effectiveness for tasks like recog-
nition. An example for visual sensing is described in [2], where a heuristically-modi ﬁed version
of our linear approach is employed to select projections for face detection. Rigorously extending
our analysis to this domain will require accounting for noise and constraints on the projections (for
example non-negativity, limited resolution) induced by fabrication processes. We view this as a
research direction worth pursuing.
Acknowledgments
This research was supported by NSF award IIS-0926148, ONR award N000140911022, and the US
Army Research Laboratory and the US Army Research Ofﬁce unde r contract/grant number 54262-
CI.
References

[1] M.A. Davenport, P.T. Boufounos, M.B. Wakin, and R.G. Baraniuk. Signal processing with compressive
measurements. IEEE JSTSP, 2010.
[2] S.J. Koppal, I. Gkioulekas, T. Zickler, and G.L. Barrows. Wide-angle micro sensors for vision on a tight
budget. CVPR, 2011.
[3] I. Jolliffe. Principal component analysis. Wiley, 1986.
[4] X. He and P. Niyogi. Locality Preserving Projections. NIPS, 2003.
[5] X. He, D. Cai, S. Yan, and H.J. Zhang. Neighborhood preserving embedding. ICCV, 2005.
[6] D. Cai, X. He, J. Han, and H.J. Zhang. Orthogonal laplacianfaces for face recognition. IEEE IP, 2006.
[7] D. Cai, X. He, and J. Han. Spectral regression for efﬁcient reg ularized subspace learning. ICCV, 2007.
[8] D. Cai, X. He, Y. Hu, J. Han, and T. Huang. Learning a spatially smooth subspace for face recognition.
CVPR, 2007.
[9] X. He, D. Cai, and P. Niyogi. Tensor subspace analysis. NIPS, 2006.
[10] J. Ye, R. Janardan, and Q. Li. Two-dimensional linear discriminant analysis. NIPS, 2004.
[11] B. Scholkopf, A. Smola, and K.R. Muller. Nonlinear component analysis as a kernel eigenvalue problem.
Neural computation, 1998.
[12] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by
V1? Vision Research, 1997.
[13] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse representa-
tion. PAMI, 2008.
[14] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictio-
naries. IEEE IP, 2006.
[15] J.F. Cai, H. Ji, C. Liu, and Z. Shen. Blind motion deblurring from a single image using sparse approxi-
mation. CVPR, 2009.
[16] R. Raina, A. Battle, H. Lee, B. Packer, and A.Y. Ng. Self-taught learning: Transfer learning from unla-
beled data. ICML, 2007.
[17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. NIPS, 2008.

8

[18] I. Ramirez, P. Sprechmann, and G. Sapiro. Classiﬁcation and clu stering via dictionary learning with
structured incoherence and shared features. CVPR, 2010.
[19] J. Yang, K. Yu, and T. Huang. Supervised translation-invariant sparse coding. CVPR, 2010.
[20] M.W. Seeger. Bayesian inference and optimal design for the sparse linear model. JMLR, 2008.
[21] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. NIPS, 2007.
[22] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding.
JMLR, 2010.
[23] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Non-Parametric Bayesian Dictionary
Learning for Sparse Image Representations. NIPS, 2009.
[24] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration.
ICCV, 2009.
[25] R. Tibshirani. Regression shrinkage and selection via the lasso. JRSS-B, 1996.
[26] A.M. Bruckstein, D.L. Donoho, and M. Elad. From sparse solutions of systems of equations to sparse
modeling of signals and images. SIAM review, 2009.
[27] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of statistics, 2004.
[28] J. Ham, D.D. Lee, S. Mika, and B. Sch ¨olkopf. A kernel view of the dimensionality reduction of manifolds.
ICML, 2004.
[29] W.J. Fu. Penalized regressions: the bridge versus the lasso. JCGS, 1998.
[30] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. JRSS-B, 2005.
[31] S. Ji, Y. Xue, and L. Carin. Bayesian compressive sensing. IEEE SP, 2008.
[32] N. Srebro and T. Jaakkola. Weighted low-rank approximations. ICML, 2003.
[33] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Kluwer, 2004.
[34] V.I. Bogachev. Gaussian measures. AMS, 1998.
[35] J. Kuelbs, FM Larkin, and J.A. Williamson. Weak probability distributions on reproducing kernel hilbert
spaces. Rocky Mountain J. Math, 1972.
[36] S. Gao, I. Tsang, and L.T. Chia. Kernel Sparse Representation for Image Classiﬁcation and Face Recog-
nition. ECCV, 2010.
[37] J. Abernethy, F. Bach, T. Evgeniou, and J.P. Vert. A new approach to collaborative ﬁltering: Operator
estimation with spectral regularization. JMLR, 2009.
[38] B. Scholkopf, R. Herbrich, and A. Smola. A generalized representer theorem. COLT, 2001.
[39] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination, and expression (PIE) database.
ICAFGR, 2002.
[40] T. Randen and J.H. Husoy. Filtering for texture classiﬁcation: A co mparative study. PAMI, 2002.
[41] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: an
incremental bayesian approach tested on 101 object categories. CVPR Workshops, 2004.
[42] P. Gehler and S. Nowozin. On feature combination for multiclass object classiﬁcation.
ICCV, 2009.
[43] D. Dueck and B.J. Frey. Non-metric afﬁnity propagation for uns upervised image categorization. ICCV,
2007.
[44] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 2000.
[45] N.X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: Variants,
properties, normalization and correction for chance. JMLR, 2010.
[46] T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman & Hall, 2000.
[47] E.J. Cand `es and T. Tao. Decoding by linear programming. IEEE IT, 2005.
[48] H. Rauhut, K. Schnass, and P. Vandergheynst. Compressed sensing and redundant dictionaries. IEEE IT,
2008.
[49] D.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE IT, 2001.
[50] M. Elad. Optimized projections for compressed sensing. IEEE SP, 2007.
[51] J.M. Duarte-Carvajalino and G. Sapiro. Learning to sense sparse signals: Simultaneous sensing matrix
and sparsifying dictionary optimization. IEEE IP, 2009.
[52] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. NIPS, 2009.

IEEE

9


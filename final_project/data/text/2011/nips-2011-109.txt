Generalized Lasso based Approximation of Sparse
Coding for Visual Recognition

Nobuyuki Morioka
The University of New South Wales & NICTA
Sydney, Australia
nmorioka@cse.unsw.edu.au

Shin’ichi Satoh
National Institute of Informatics
Tokyo, Japan
satoh@nii.ac.jp

Abstract

Sparse coding, a method of explaining sensory data with as few dictionary bases
as possible, has attracted much attention in computer vision. For visual object cat-
egory recognition, (cid:96)1 regularized sparse coding is combined with the spatial pyra-
mid representation to obtain state-of-the-art performance. However, because of its
iterative optimization, applying sparse coding onto every local feature descriptor
extracted from an image database can become a major bottleneck. To overcome
this computational challenge, this paper presents “Generalized Lasso based Ap-
proximation of Sparse coding” (GLAS). By representing the distribution of sparse
coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with
the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to
perform mutual inhibition between bases which is essential for an overcomplete
setting. The experiments show that GLAS obtains a comparable performance to
(cid:96)1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its
effectiveness for large-scale visual recognition problems.

1

Introduction

Recently, sparse coding [3, 18] has attracted much attention in computer vision research. Its ap-
plications range from image denoising [23] to image segmentation [17] and image classiﬁcation
[10, 24], achieving state-of-the-art results. Sparse coding interprets an input signal x ∈ RD×1 with
a sparse vector u ∈ RK×1 whose linear combination with an overcomplete set of K bases (i.e.,
D (cid:28) K ), also known as dictionary B ∈ RD×K , reconstructs the input as precisely as possible. To
enforce sparseness on u, the (cid:96)1 norm is a popular choice due to its computational convenience and its
interesting connection with the NP-hard (cid:96)0 norm in compressed sensing [2]. Several efﬁcient (cid:96)1 reg-
ularized sparse coding algorithms have been proposed [4, 14] and are adopted in visual recognition
[10, 24]. In particular, Yang et al. [24] compute the spare codes of many local feature descriptors
with sparse coding. However, due to the (cid:96)1 norm being non-smooth convex, the sparse coding algo-
rithm needs to optimize iteratively until convergence. Therefore, the local feature descriptor coding
step becomes a major bottleneck for large-scale problems like visual recognition.
The goal of this paper is to achieve state-of-the-art performance on large-scale visual recognition
that is comparable to the work of Yang et al. [24], but with a signiﬁcant improvement in efﬁciency.
To this end, we propose “Generalized Lasso based Approximation of Sparse coding”, GLAS for
short. Speciﬁcally, we encode the distribution of each dimension in sparse codes with the slice
transform representation [9] and learn a piece-wise linear mapping function with the generalized
lasso to obtain the best ﬁt [21] to approximate (cid:96)1 regularized sparse coding. We further propose
an efﬁcient post-reﬁnement procedure to capture the dependency between overcomplete bases. The
effectiveness of our approach is demonstrated with several challenging object and scene category
datasets, showing a comparable performance to Yang et al. [24] and performing better than other
fast algorithms that obtain sparse codes [22]. While there have been several supervised dictionary

1

learning methods for sparse coding to obtain more discriminative sparse representations [16, 25],
they have not been evaluated on visual recognition with many object categories due to its com-
putational challenges. Furthermore, Ranzato et al. [19] have empirically shown that unsupervised
learning of visual features can obtain a more general and effective representation. Therefore, in this
paper, we focus on learning a fast approximation of sparse coding in an unsupervised manner.
The paper is organized as follows: Section 2 reviews some related work including the linear spatial
pyramid combined with sparse coding and other fast algorithms to obtain sparse codes. Section 3
presents GLAS. This is followed by the experimental results on several challenging categorization
datasets in Section 4. Section 5 concludes the paper with discussion and future work.

2 Related Work

2.1 Linear Spatial Pyramid Matching Using Sparse Coding

min
B,U

(cid:107)xi − Bui (cid:107)2
2 + λ(cid:107)ui (cid:107)1

s.t. (cid:107)bk (cid:107)2
2 ≤ 1, k = 1, 2, . . . , K.

This section reviews the linear spatial pyramid matching based on sparse coding by Yang et al. [24].
Given a collection of N local feature descriptors randomly sampled from training images X =
[x1 , x2 , . . . , xN ] ∈ RD×N , an over-complete dictionary B = [b1 , b2 , . . . , bK ] ∈ RD×K is learned
N(cid:88)
by
i=1
The cost function above is a combination of the reconstruction error and the (cid:96)1 sparsity penalty
which is controlled by λ. The (cid:96)2 norm on each bk is constrained to be less than or equal to 1
to avoid a trival solution. Since both B and [u1 , u2 , . . . , uN ] are unknown a priori, an alternating
optimization technique is often used [14] to optimize over the two parameter sets.
Under the spatial pyramid matching framework, each image is divided into a set of sub-regions
r = [r1 , r2 , . . . , rR ]. For example, if 1×1, 2×2 and 4×4 partitions are used on an image, we have
21 sub-regions. Then, we compute the sparse solutions of all local feature descriptors, denoted as
Urj , appearing in each sub-region rj by
(cid:107)Xrj − BUrj (cid:107)2
2 + λ(cid:107)Urj (cid:107)1 .
min
Urj

(1)

(2)

The sparse solutions are max pooled for each sub-region and concatenated with other sub-regions to
build a statistic of the image by
h = [max(|Ur1 |)(cid:62) , max(|Ur2 |)(cid:62) , . . . , max(|UrR |)(cid:62) ](cid:62) ,
where max(.) is a function that ﬁnds the maximum value at each row of a matrix and returns a
column vector. Finally, a linear SVM is trained on a set of image statistics for classiﬁcation.
The main advantage of using sparse coding is that state-of-the-art results can be achieved with a
simple linear classiﬁer as reported in [24]. Compared to kernel-based methods, this dramatically
speeds up training and testing time of the classiﬁer. However, the step of ﬁnding a sparse code for
each local descriptor with sparse coding now becomes a major bottleneck. Using the efﬁcient sparse
coding algorithm based on feature-sign search [14], the time to compute the solution for one local
descriptor u is O(K Z ) where Z is the number of non-zeros in u. This paper proposes an approx-
imation method whose time complexity reduces to O(K ). With the post-reﬁnement procedure, its
time complexity is O(K + Z 2 ) which is still much lower than O(K Z ).

(3)

2.2 Predictive Sparse Decomposition

Predictive sparse decomposition (PSD) described in [10, 11] is a feedforward network that applies a
non-linear mapping function on linearly transformed input data to match the optimal sparse coding
solution as accurate as possible. Such feedfoward network is deﬁned as: ˆui = Gg(Wxi , θ), where
g(z , θ) denotes a non-linear parametric mapping function which can be of any form, but to name
a few there are hyperbolic tangent, tanh(z + θ) and soft shrinkage, sign(z ) max(|z | − θ , 0). The
function is applied to linearly transformed data Wxi and subsequently scaled by a diagonal matrix

2

min
B,G,W,θ,U

(4)

2 + λ(cid:107)ui (cid:107)1 + γ (cid:107)ui − Gg(Wxi , θ)(cid:107)2
(cid:107)xi − Bui(cid:107)2
2 .

G. Given training samples {xi }N
i=1 , the parameters can be estimated either jointly or separately from
N(cid:88)
the dictionary B. When learning jointly, we minimize the cost function given below:
i=1
When learning separately, B and U are obtained with Eqn. (1) ﬁrst. Then, other remaining parame-
ters G, W and θ are estimated by solving the last term of Eqn. (4) only. Gregor and LeCun [7] have
later proposed a better, but iterative approximation scheme for (cid:96)1 regularized sparse coding.
One downside of the parametric approach is its accuracy is largely dependent on how well its para-
metric function ﬁts the target statistical distribution, as argued by Hel-Or and Shaked [9]. This paper
explores a non-parametric approach which can ﬁt any distribution as long as data samples available
are representative. The advantage of our approach over the parametric approach is that we do not
need to seek an appropriate parametric function for each distribution. This is particularly useful in
visual recognition that uses multiple feature types, as it automatically estimates the function form
for each feature type from data. We demonstrate this with two different local descriptor types in our
experiments.

2.3 Locality-constrained Linear Coding

Another notable work that overcomes the bottleneck of the local descriptor coding step is locality-
constrained linear coding (LLC) proposed by Wang et al. [22], a fast version of local coordinate
coding [26]. Given a local feature descriptor xi , LLC searches for M nearest dictionary bases of
each local descriptor xi and these nearest bases stacked in columns are denoted as Bφi ∈ RD×M
where φi indicates the index list of the bases. Then, the coefﬁcients uφi ∈ RM ×1 whose linear
combination with Bφi reconstructs xi is solved by:
(cid:107)xi − Bφi uφi (cid:107)2
s.t. 1(cid:62)uφi = 1.
min
2
uφi
This is the least squares problem which can be solved quite efﬁciently. The ﬁnal sparse code ui is
obtained by setting its elements indexed at φi to uφi . The time complexity of LLC is O(K + M 2 ).
This excludes the time required to ﬁnd M nearest neighbours. While it is fast, the resulting sparse
solutions obtained are not as discriminative as the ones obtained by sparse coding. This may be due
to the fact that M is ﬁxed across all local feature descriptors. Some descriptors may need more bases
for accurate representation and others may need less bases for more distinctiveness. In contrast, the
number of bases selected with our post-reﬁnement procedure to handle the mutual inhibition is
different for each local descriptor.

(5)

3 Generalized Lasso based Approximation of Sparse Coding

This section describes GLAS. We ﬁrst learn a dictionary from a collection of local feature descriptors
as given Eqn. (1). Then, based on slice transform representation, we ﬁt a piece-wise linear mapping
function with the generalized lasso to approximate the optimal sparse solutions of the local feature
descriptors under (cid:96)1 regularized sparse coding. Finally, we propose an efﬁcient post-reﬁnement
procedure to perform the mutual inhibition.

3.1 Slice Transform Representation

Slice transform representation is introduced as a way to discretize a function space so to ﬁt a piece-
wise linear function for the purpose of image denoising by Hel-Or and Shaked [9]. This is later
adopted by Adler et al. [1] for single image super resolution. In this paper, we utilise the repre-
sentation to approximate sparse coding to obtain sparse codes for local feature descriptor as fast as
possible.
Given a local descriptor x, we can linearly combine with B(cid:62) to obtain z. For the moment, we just
consider one dimension of z denoted as z which is a real value and lies in a half open interval of
[a, b). The interval is divided into Q − 1 equal-sized bins whose boundaries form a vector q =
[q1 , q2 , . . . , qQ ](cid:62) such that a = q1 < q2 · · · < qQ = b.

3

(a)

(b)

(c)

Figure 1: Different approaches to ﬁt a piece-wise linear mapping function. Regularized least squares
(RLS) in red (see Eqn. (8)). (cid:96)1 -regularized sparse coding (L1-SC) in magenta (see Eqn. (9)). GLAS
in green (see Eqn. (10)). (a) All three methods achieving a good ﬁt. (b) A case when L1-SC fails to
extrapolate well at the end and RLS tends to align itself to q in black. (c) A case when data samples
at around 0.25 are removed artiﬁcially to illustrate that RLS fails to interpolate as no neighoring
prior is used. In contrast, GLAS can both interpolate and extrapolate well in the case of missing or
noisy data.

(6)

The interval into which the value of z falls is expressed as: π(z ) = j if z ∈ [qj−1 , qj ), and its
corresponding residue is calculated by: r(z ) = z−qπ(z)−1
.
qπ(z)−qπ(z)−1
Based on the above, we can re-express z as
z = (1 − r(z ))qπ(z)−1 + r(z )qπ(z) = Sq (z )q,
where Sq (z ) = [0, . . . , 0, 1 − r(z ), r(z ), 0, . . . , 0].
If we now come back to the multivariate case of z = B(cid:62)x, then we have the following: z =
[Sq (z1 )q, Sq (z2 )q, . . . , Sq (zK )q](cid:62) , where zk implies the k th dimension of z. Then, we replace the
boundary vector q with p = {p1 , p2 , . . . , pK } such that resulting vector approximates the optimal
sparse solution of x obtained by (cid:96)1 regularized sparse coding as much as possible. This is written as
ˆu = [Sq (z1 )p1 , Sq (z2 )p2 , . . . , Sq (zK )pK ](cid:62) .
(7)
Hel-Or and Shaked [9] have formulated the problem of learning each pk as regularized least squares
either independently in a transform domain or jointly in a spatial domain. Unlike their setting,
we have signiﬁcantly large number of bases which makes joint optimization of all pk difﬁcult.
Moreover, since we are interested in approximating the sparse solutions which are in the transform
domain, we learn each pk independently. Given N local descriptors X = [x1 , x2 , . . . , xN ] ∈ RD×N
and their corresponding sparse solutions U = [u1 , u2 , . . . , uN ] = [y1 , y2 , . . . , yK ](cid:62) ∈ RK×N
obtained with (cid:96)1 regularized sparse coding, we have an optimization problem given as
2 + α(cid:107)q − pk (cid:107)2
(cid:107)yk − Sk pk (cid:107)2
min
2 ,
pk
where Sk = Sq (zk ). The regularization of the second term is essential to avoid singularity when
computing the inverse and its consequence is that pk is encouraged to align itself to q when not many
data samples are available. This might have been a reasonable prior for image denoising [9], but not
desirable for the purpose of approximating sparse coing, as we would like to suppress most of the
coefﬁcients in u to zero. Figure 1 shows the distribution of one dimension of sparse coefﬁcients z
obtained from a collection of SIFT descriptors and q does not look similar to the distribution. This
motivates us to look at the generalized lasso [21] as an alternative for obtaining a better ﬁt of the
distribution of the coefﬁcients.

(8)

3.2 Generalized Lasso

In the previous section, we have argued that regularized least squares stated in Eqn. (8) does not give
the desired result. Instead most intervals need to be set to zero. This naturally leads us to consider
(cid:96)1 regularized sparse coding also known as the lasso which is formulated as:
(cid:107)yk − Sk pk (cid:107)2
2 + α(cid:107)pk (cid:107)1 .
min
pk

(9)

4

−0.500.51−0.200.20.40.60.811.2zu  DataRLSL1−SCGLAS−0.500.51−0.200.20.40.60.811.2zu  DataRLSL1−SCGLAS−0.500.51−0.200.20.40.60.811.2zu  DataRLSL1−SCGLAS(10)

(11)

D =

However, the drawback of this is that the learnt piece-wise linear function may become unstable in
cases when training data is noisy or missing as illustrated in Figure 1 (b) and (c). It turns out (cid:96)1
trend ﬁltering [12], generally known as the generalized lasso [21], can overcome this problem. This
is expressed as
(cid:107)yk − Sk pk (cid:107)2
2 + α(cid:107)Dpk (cid:107)1 ,
min
pk
 −1
 .
where D ∈ R(Q−2)×Q is referred to as a penalty matrix and deﬁned as
2 −1
−1
2 −1
. . .
. . .
. . .
2 −1
−1
To solve the above optimization problem, we can turn it into the sparse coding problem [21]. Since
D is not invertible, the key is to augment D with A ∈ R2×Q to build a square matrix ˜D = [D; A] ∈
RQ×Q such that rank( ˜D) = Q and the rows of A are orthogonal to the rows of D. To satisfy such
constraints, A can for example be set to [1, 2, . . . , Q; 2, 3, . . . , Q + 1]. If we let θ = [θ1 ; θ2 ] = ˜Dpk
−1
where θ1 = Dpk and θ2 = Apk , then Sk pk = Sk ˜D
θ = Sk1 θ1 + Sk2 θ2 . After some substitutions,
k2 (yk − Sk1 θ1 ), given θ1 is solved already. Now,
k2Sk2 )−1S(cid:62)
we see that θ2 can be solved by: θ2 = (S(cid:62)
to solve θ1 , we have the following sparse coding problem:
(cid:107)(I − P)yk − (I − P)Sk1 θ1(cid:107)2
2 + α(cid:107)θ1(cid:107)1 ,
min
θ1
k2Sk2 )−1S(cid:62)
where P = Sk2 (S(cid:62)
k2 . Having computed both θ1 and θ2 , we can recover the solution of pk
−1
by ˜D
θ . Further details can be found in [21].
Given the learnt p, we can approximate sparse solution of x by Eqn. (7). However, explicitly com-
puting Sq (z) and multiplying it by p is somewhat redundant. Thus, we can alternatively compute
each component of ˆu as follows:
ˆuk = (1 − r(zk )) × pk (π(zk ) − 1) + r(zk ) × pk (π(zk )),
(13)
whose time complexity becomes O(K ). In Eqn. (13), since we are essentially using pk as a lookup
table, the complexity is independent from Q. This is followed by (cid:96)1 normalization on ˆu.
While ˆu can readily be used for the spatial max pooling as stated in Eqn. (3), it does not yet capture
any “explaining away” effect where the corresponding coefﬁcients of correlated bases are mutually
inhibited to remove redundancy. This is because each pk is estimated independently in the transform
domain [9]. In the next section, we propose an efﬁcient post-reﬁnement technique to mutually inhibit
between the bases.

(12)

3.3 Capturing Dependency Between Bases

To handle the mutual inhibition between overcomplete bases, this section explains how to reﬁne the
sparse codes by solving regularized least squares on a signiﬁcantly small active basis set. Given a
local descriptor x and its initial sparse code ˆu estimated with above method, we set the non-zero
components of the code to be active. By denoting a set of these active components as φ, we have
ˆuφ and Bφ which are the subsets of the sparse code and dictionary bases respectively. The goal is
to compute the reﬁned code of ˆuφ denoted as ˆvφ such that Bφvφ reconstructs xi as accurately as
possible. We formulate this as regularised least squares given below:
2 + β (cid:107)ˆvφ − ˆuφ (cid:107)2
(cid:107)x − Bφˆvφ(cid:107)2
min
2 ,
ˆvφ
where β is the weight parameter of the regularisation. This is convex and has the following analytical
solution: ˆvφ = (B(cid:62)
φ Bφ + β I )−1 (B(cid:62)
φ x + β ˆuφ ).
The intuition behind the above formulation is that the initial sparse code ˆu is considered as a good
starting point for reﬁnement to further reduce the reconstruction error by allowing redundant bases to
compete against each other. Empirically, the number of active components for each ˆu is substantially
small compared to the whole basis set. Hence, a linear system to be solved becomes much smaller

(14)

5

Methods
15 Train
30 Train
Time (sec)

KM
55.5±1.2
63.0±1.2
0.06

SIFT (128 Dim.) [15]
SC [24]
PSD [11]
LLC [22]
62.7±1.0
64.0±1.2
65.2±1.2
71.6±0.7
70.6±0.9
69.6±0.8
0.25
0.06
3.53

GLAS
64.4±1.2
71.6±1.0
0.15

GLAS+
65.1±1.1
72.3±0.7
0.23

Methods
15 Train
30 Train
Time (sec)

KM
60.1±1.3
63.0±1.2
0.05

Local Self-Similarity (30 Dim.) [20]
LLC [22]
PSD [11]
SC [24]
62.4±0.8
59.7±0.8
64.8±0.9
72.5±1.6
67.2±0.9
69.7±1.3
0.24
0.05
1.97

GLAS
62.3±1.2
69.8±1.4
0.13

GLAS+
63.8±0.9
71.0±1.1
0.18

Table 1: Recognition accuracy on Caltech-101. The dictionary sizes for all methods are set to 1024.
We also report the time taken to process 1000 local descriptors for each method.

which is computationally cheap. We also make sure that we do not deviate too much from the initial
solution by introducing the regularization on ˆvφ . This reﬁnement procedure may be similar to LLC
[22]. However, in our case, we do not preset the number of active bases and determine by non-zero
components of ˆu. More importantly, we base our ﬁnal solution on ˆu and do not perform nearest
neighbor search. With this reﬁnement procedure, the total time complexity becomes O(K + Z 2 ).
We refer GLAS with this post-reﬁnement procedure as GLAS+.

4 Experimental Results

This section evaluates GLAS and GLAS+ on several challenging categorization datasets. To learn
the mapping function, we have used 50,000 local descriptors as data samples. The parameters Q,
α and β are ﬁxed to 10, 0.1 and 0.25 respectively for all experiments, unless otherwise stated. For
comparison, we have implemented methods discussed in Section 2. SC is our re-implementation
of Yang et al. [24]. LLC is locality-constrained linear coding proposed by Wang et al. [22]. The
number of nearest neighors to consider is set to 5. PSD is predictive sparse decomposition [11].
Shrinkage function is used as its parametric mapping function. We also include KM which builds
its codebook with k-means clustering and adopts hard-assignment as its local descriptor coding.
For all methods, exactly the same local feature descriptors, spatial max pooling technique and linear
SVM are used to only compare the difference between the local feature descriptor coding techniques.
As for the descriptors, SIFT [15] and Local Self-Similarity [20] are used. SIFT is a histogram of
gradient directions computed over an image patch - capturing appearance information. We have
sampled a 16×16 patch at every 8 pixel step. In contrast, Local Self-Similarity computes correlation
between a small image patch of interest and its surrounding region which captures the geometric
layout of a local region. Spatial max pooling with 1 × 1, 2 × 2 and 4 × 4 image partitions is used.
The implementation is all done in MATLAB for fair comparison.

4.1 Caltech-101

The Caltech-101 dataset [5] consists of 9144 images which are divided into 101 object categories.
The images are scaled down to 300 × 300 preserving their aspect ratios. We train with 15/30 images
per class and test with 15 images per class. The dictionary size of each method is set to 1024 for
both SIFT and Local Self-Similarity.
The results are averaged over eight random training and testing splits and are reported in Table
1. For SIFT, GLAS+ is consistently better than GLAS demonstrating the effectiveness of mutual
inhibition by the post-reﬁnement procedure. Both GLAS and GLAS+ performs better than other
fast algorithms that produces sparse codes. In addition GLAS and GLAS+ performs competitively
against SC. In fact, GLAS+ is slightly better when 30 training images per class are used. While
sparse codes for both GLAS and GLAS+ are learned from the solutions of SC, the approximated
codes are not exactly the same as the ones of SC. Moreover, SC sometimes produces unstable codes
due to the non-smooth convex property of (cid:96)1 norm as previously observed in [6]. In contrast, GLAS+

6

(a)

(b)

(c)

Figure 2: (a) Q, the number of bins to quantize the interval of each sparse code component. (b) α,
the parameter that controls the weight of the norm used for the generalized lasso. (c) When some
data samples are missing GLAS is more robust than regularized least squares given in Eqn. (8).

approximates its sparse codes with a relatively smooth piece-wise linear mapping function learned
with the generalized lasso (note that the (cid:96)1 norm penalizes on changes in the shape of the function)
and performs smooth post-reﬁnement. We suspect these differences may be contributing to the
slightly better results of GLAS+ on this dataset.
Although PSD performs quite close to GLAS for SIFT, this is not the case for Local Self-Similarity.
GLAS outperforms PSD probably due to the distribution of sparse codes is not captured well by a
simple shrinkage function. Therefore, GLAS might be more effective for a wide range of distri-
butions. This is useful for recognition using multiple feature types where speed is critical. GLAS
performs worse than SC, but GLAS+ closes the gap between GLAS and SC. We suspect that due to
Local Self-Similarity (30 dim.) being relatively low-dimensional than SIFT (128 dim.), the mutual
inhibition becomes more important. This might also explain why LLC has performed reasonably
well for this descriptor.
Table 1 also reports computational time taken to process 1000 local descriptors for each method.
GLAS and GLAS+ are slower than KM and PSD, but are slightly faster than LLC and signiﬁcantly
faster than SC. This demonstrates the practical importance of our approach where competitive recog-
nition results are achieved with fast computation.
Different values for Q, α and β are evaluated one parameter at a time. Figure 2 (a) shows the results
of different Q. The results are very stable after 10 bins. As sparse codes are computed by Eqn. (13),
the time complexity is not affected by what Q is chosen. Figure 2 (b) shows the results for different
α which look very stable. We also observe similar stability for β .
We also validate if the generalized lasso given in Eqn. (10) is more robust than the regularized least
squares solution given in Eqn. (8) when some data samples are missing. When learning each qk ,
we artiﬁcially remove data samples from an interval centered around a randomly sampled point,
as also illustrated in Figure 1 (c). We evaluate with different numbers of data samples removed in
terms of percentages of the whole data sample set. The results are shown in Figure 2 (c) where the
performance of RLS signiﬁcantly drops as the number of missing data is increased. However, both
GLAS and GLAS+ are not affected that much.

4.2 Caltech-256

Caltech-256 [8] contains 30,607 images and 256 object categories in total. Like Caltech-101, we
scale the images down to 300×300 preserving their aspect ratios. The results are averaged over
eight random training and testing splits and are reported in Table 2. We use 25 testing images per
class. This time, for SIFT, GLAS performs slightly worse than SC, but GLAS+ outperforms SC
probably due to the same argument given in the previous experiments on Caltech-101. For Local
Self-Similarity, results similar to Caltech-101 are obtained. The performance of PSD is close to KM
and is outperformed by GLAS, suggesting the inadequate ﬁtting of sparse codes. LLC performs
slightly better than GLAS, but could not perform better than GLAS+. While SC performed the best,
the performance of GLAS+ is quite close to SC. We also plot a graph of the computational time
taken for each method with its achieved accuracy on SIFT and Local Self-Similarity in Figure 3 (a)
and (b) respectively.

7

05101520256668707274QAverage Recognition  SCGLASGLAS+00.516668707274AlphaAverage Recognition  SCGLASGLAS+0%10%20%30%40%62646668707274% of Missing DataAverage Recognition  SCRLSGLASGLAS+Methods
15 Train
30 Train

KM
22.7±0.4
27.4±0.5

SIFT (128 Dim.) [15]
SC [24]
PSD [11]
LLC [22]
28.1±0.5
30.4±0.6
30.7±0.4
34.0±0.6
36.3±0.5
36.8±0.4

GLAS
30.4±0.4
36.1±0.4

GLAS+
32.1±0.4
38.2±0.4

Methods
15 Train
30 Train

KM
23.7±0.4
28.5±0.4

Local Self-Similarity (30 Dim.) [20]
SC [24]
PSD [11]
LLC [22]
26.3±0.5
24.3±0.6
28.7±0.5
31.9±0.5
29.3±0.5
34.7±0.4

GLAS
26.0±0.5
31.2±0.5

GLAS+
27.6±0.6
33.3±0.5

Table 2: Recognition accuracy on Caltech-256. The dictionary sizes are all set to 2048 for SIFT and
1024 for Local Self-Similarity.

(a)

(b)

(c)

Figure 3: Plotting computational time vs. average recognition. (a) and (b) are SIFT and Local-Self
Similarity respectively evaluated on Caltech-256 with 30 training images. The dictionary size is set
to 2048. (c) is SIFT evaluated on 15 Scenes. The dictionary size is set to 1024.

4.3

15 Scenes

The 15 Scenes [13] dataset contains 4485 images divided into 15 scene classes ranging from indoor
scenes to outdoor scenes. 100 training images per class are used for training and the rest for testing.
We used SIFT to learn 1024 dictionary bases for each method. The results are plotted with com-
putational time taken in Figure 3 (c). The result of GLAS+ (80.6%) are very similar to that of SC
(80.7%), yet the former is signiﬁcantly faster. In summary, we show that our approach works well
on three different challenging datasets.

5 Conclusion

This paper has presented an approximation of (cid:96)1 sparse coding based on the generalized lasso called
GLAS. This is further extended with the post-reﬁnement procedure to handle mutual inhibition
between bases which are essential in an overcomplete setting. The experiments have shown compet-
itive performance of GLAS against SC and achieved signiﬁcant computational speed up. We have
also demonstrated that the effectiveness of GLAS on two local descriptor types, namely SIFT and
Local Self-Similarity where LLC and PSD only perform well on one type. GLAS is not restricted
to only approximate (cid:96)1 sparse coding, but should be applicable to other variations of sparse coding
in general. For example, it may be interesting to try GLAS on Laplacian sparse coding [6] that
achieves smoother sparse codes than (cid:96)1 sparse coding.

Acknowledgment

NICTA is funded by the Australian Government as represented by the Department of Broadband, Communi-
cations and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence
program.

8

024625303540Computational TimeAverage Recognition  KMLLCPSDSCGLASGLAS+00.511.522830323436Computational TimeAverage Recognition  KMLLCPSDSCGLASGLAS+01234767778798081Computational TimeAverage Recognition  KMLLCPSDSCGLASGLAS+References
[1] A. Adler, Y. Hel-Or, and M. Elad. A Shrinkage Learning Approach for Single Image Super-
Resolution with Overcomplete Representations. In ECCV, 2010.
[2] D.L. Donoho. For Most Large Underdetermined Systems of Linear Equations the Minimal L1-
norm Solution is also the Sparse Solution. Communications on Pure and Applied Mathematics,
2006.
[3] D.L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio-
naries via L1 minimization. PNAS, 100(5):2197–2202, 2003.
[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least Angle Regression. Annals of
Statistics, 2004.
[5] L. Fei-Fei, R. Fergus, and P. Perona. Learning Generative Visual Models from Few Training
Examples: An Incremental Bayesian Approach Tested on 101 Object Categories. In CVPR
Workshop, 2004.
[6] S. Gao, W. Tsang, L. Chia, and P. Zhao. Local Features Are Not Lonely - Laplacian Sparse
Coding for Image Classiﬁcation. In CVPR, 2010.
[7] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In ICML, 2010.
[8] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 Object Category Dataset. Technical Report,
California Institute of Technology, 2007.
[9] Y. Hel-Or and D. Shaked. A Discriminative Approach for Wavelet Denoising. TIP, 2008.
[10] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the Best Multi-Stage Archi-
tecture for Object Recognition. In ICCV, 2009.
[11] K Kavukcuoglu, M Ranzato, and Y Lecun. Fast inference in sparse coding algorithms with
applications to object recognition. Technical rRport CBLL-TR-2008-12-01, Computational
and Biological Learning Lab, Courant Institute, NYU, 2008.
[12] S.-J. Kim, K. Koh, S. Boyd, and D. Gorinevsky. L1 trend ﬁltering. SIAM Review, 2009.
[13] S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching
for Recognizing Natural Scene Categories. In CVPR, 2006.
[14] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2006.
[15] D.G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. IJCV, 2004.
[16] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised Dictionary Learning. In
NIPS, 2008.
[17] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. Discriminative Sparse Image
Models for Class-Speciﬁc Edge Detection and Image Interpretation. In ECCV, 2008.
[18] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy
employed by V1? Vision Research, 37, 1997.
[19] M. Ranzato, F.J. Huang, Y. Boureau, and Y. LeCun. Unsupervised Learning of Invariant Fea-
ture Hierarchies with Applications to Object Recognition. In CVPR, 2007.
[20] E. Shechtman and M. Irani. Matching Local Self-Similarities across Image and Videos. In
CVPR, 2007.
[21] R. Tibshirani and J. Taylor. The Solution Path of the Generalized Lasso. The Annals of Statis-
tics, 2010.
[22] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding
for Image Classiﬁcation. In CVPR, 2010.
[23] J. Yang, J. Wright, T. Huang, and Y. Ma. Image Super-Resolution via Sparse Representation.
TIP, 2010.
[24] J. Yang, K. Yu, Y. Gong, and T.S. Huang. Linear spatial pyramid matching using sparse coding
for image classiﬁcation. In CVPR, 2009.
[25] J. Yang, K. Yu, and T. Huang. Supervised Translation-Invariant Sparse Coding.
2010.
[26] K. Yu, T. Zhang, and Y. Gong. Nonlinear Learning using Local Coordinate Coding. In NIPS,
2009.

In CVPR,

9


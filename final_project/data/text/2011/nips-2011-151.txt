Analysis and Improvement of
Policy Gradient Estimation

Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama
Tokyo Institute of Technology
{tingting@sg., hachiya@sg., gang@sg., sugiyama@}cs.titech.ac.jp

Abstract

Policy gradient is a useful model-free reinforcement learning approach, but it
tends to suffer from instability of gradient estimates. In this paper, we analyze
and improve the stability of policy gradient methods. We ﬁrst prove that the vari-
ance of gradient estimates in the PGPE (policy gradients with parameter-based
exploration) method is smaller than that of the classical REINFORCE method
under a mild assumption. We then derive the optimal baseline for PGPE, which
contributes to further reducing the variance. We also theoretically show that PGPE
with the optimal baseline is more preferable than REINFORCE with the optimal
baseline in terms of the variance of gradient estimates. Finally, we demonstrate
the usefulness of the improved PGPE method through experiments.

1

Introduction

The goal of reinforcement learning (RL) is to ﬁnd an optimal decision-making policy that maximizes
the return (i.e., the sum of discounted rewards) through interaction with an unknown environment
[13]. Model-free RL is a ﬂexible framework in which decision-making policies are directly learned
without going through explicit modeling of the environment. Policy iteration and policy search are
two popular formulations of model-free RL.
In the policy iteration approach [6], the value function is ﬁrst estimated and then policies are deter-
mined based on the learned value function. Policy iteration was demonstrated to work well in many
real-world applications, especially in problems with discrete states and actions [14, 17, 1]. Although
policy iteration can naturally deal with continuous states by function approximation [8], continuous
actions are hard to handle due to the difﬁculty of ﬁnding maximizers of value functions with respect
to actions. Moreover, since policies are indirectly determined via value function approximation,
misspeciﬁcation of value function models can lead to inappropriate policies even in very simple
problems [15, 2]. Another limitation of policy iteration especially in physical control tasks is that
control policies can vary drastically in each iteration. This causes severe instability in the physical
system and thus is not favorable in practice.
Policy search is another approach to model-free RL that can overcome the limitations of policy
iteration [18, 4, 7]. In the policy search approach, control policies are directly learned so that the
return is maximized, for example, via a gradient method (called the REINFORCE method) [18], an
EM algorithm [4], and a natural gradient method [7]. Among them, the gradient-based method is
particularly useful in physical control tasks since policies are changed gradually. This ensures the
stability of the physical system.
However, since the REINFORCE method tends to have a large variance in the estimation of the
gradient directions, its naive implementation converges slowly [9, 10, 12]. Subtraction of the optimal
baseline [16, 5] can ease this problem to some extent, but the variance of gradient estimates is
still large. Furthermore, the performance heavily depends on the choice of an initial policy, and
appropriate initialization is not straightforward in practice.

1

To cope with this problem, a novel policy gradient method called policy gradients with parameter-
based exploration (PGPE) was proposed recently [12]. In PGPE, an initial policy is drawn from
a prior probability distribution, and then actions are chosen deterministically. This construction
contributes to mitigating the problem of initial policy choice and stabilizing gradient estimates.
Moreover, by subtracting a moving-average baseline, the variance of gradient estimates can be fur-
ther reduced. Through robot-control experiments, PGPE was demonstrated to achieve more stable
performance than existing policy-gradient methods.
The goal of this paper is to theoretically support the usefulness of PGPE, and to further improve its
performance. More speciﬁcally, we ﬁrst give bounds of the gradient estimates of the REINFORCE
and PGPE methods. Our theoretical analysis shows that gradient estimates for PGPE have smaller
variance than those for REINFORCE under a mild condition. We then show that the moving-average
baseline for PGPE adopted in the original paper [12] has excess variance; we give the optimal
baseline for PGPE that minimizes the variance, following the line of [16, 5]. We further theoretically
show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal
baseline in terms of the variance of gradient estimates. Finally, the usefulness of the improved PGPE
method is demonstrated through experiments.

2 Policy Gradients for Reinforcement Learning

In this section, we review policy gradient methods.

2.1 Problem Formulation
Let us consider a Markov decision problem speciﬁed by (S , A, PT , PI , r, γ ), where S is a set of
(cid:96)-dimensional continuous states, A is a set of continuous actions, PT (s(cid:48) |s, a) is the transition prob-
ability density from current state s to next state s(cid:48) when action a is taken, PI (s) is the probability
of initial states, r(s, a, s(cid:48) ) is an immediate reward for transition from s to s(cid:48) by taking action a,
and 0 < γ < 1 is the discount factor for future rewards. Let p(a|s, θ) be a stochastic policy with
parameter θ , which represents the conditional probability density of taking action a in state s.
Let h = [s1 , a1 , . . . , sT , aT ] be a trajectory of length T. Then the return (i.e., the discounted sum
R(h) := (cid:80)T
of future rewards) along h is given by
t=1 γ t−1 r(st , at , st+1 ).
J (θ) := (cid:82) p(h|θ)R(h)dh, where p(h|θ) = p(s1 ) (cid:81)T
The expected return for parameter θ is deﬁned by
t=1 p(st+1 |st , at )p(at |st , θ).
The goal of reinforcement learning is to ﬁnd the optimal policy parameter θ∗ that maximizes the
expected return J (θ):
θ∗ := arg max J (θ).

2.2 Review of the REINFORCE Algorithm

In the REINFORCE algorithm [18], the policy parameter θ is updated via gradient ascent:
θ ←− θ + ε∇θ J (θ),
∇θ J (θ) = (cid:82) ∇θ p(h|θ)R(h)dh = (cid:82) p(h|θ)∇θ log p(h|θ)R(h)dh
where ε is a small positive constant. The gradient ∇θ J (θ) is given by
= (cid:82) p(h|θ) (cid:80)T
t=1 ∇θ log p(at |st , θ)R(h)dh,
where we used the so-called ‘log trick’: ∇θ p(h|θ) = p(h|θ)∇θ log p(h|θ). Since p(h|θ) is un-
(cid:80)N
(cid:80)T
∇θ (cid:98)J (θ) = 1
known, the expectation is approximated by the empirical average:
t=1 ∇θ log p(an
t |sn
t , θ)R(hn ),
n=1
N
where hn := [sn
T ] is a roll-out sample.
1 , an
1 , . . . , sn
T , an

2

(cid:16)− (a−µ(cid:62)s)2
(cid:17)
Let us employ the Gaussian policy model with parameter θ = (µ, σ), where µ is the mean vector
and σ is the standard deviation:
p(a|s, θ) = 1
√
2σ2
σ
2π
Then the policy gradients are explicitly given as
σ2 s and ∇σ log p(a|s, θ) = (a−µ(cid:62)s)2−σ2
∇µ log p(a|s, θ) = a−µ(cid:62)s
σ3
A drawback of REINFORCE is that the variance of the above policy gradients is large [10, 11],
which leads to slow convergence.

exp

.

.

2.3 Review of the PGPE Algorithm

One of the reasons for large variance of policy gradients in the REINFORCE algorithm is that the
empirical average is taken at each time step, which is caused by stochasticity of policies.
In order to mitigate this problem, another method called policy gradients with parameter-based
exploration (PGPE) was proposed recently [11]. In PGPE, a linear deterministic policy,
π(a|s, θ) = θ(cid:62)s,
is adopted, and stochasticity is introduced by considering a prior distribution over policy parameter
θ with hyper-parameter ρ: p(θ |ρ). Since entire history h is solely determined by a single sample of
parameter θ in this formulation, it is expected that the variance of gradient estimates can be reduced.
J (ρ) = (cid:82)(cid:82) p(h|θ)p(θ |ρ)R(h)dhdθ .
The expected return for hyper-parameter ρ is expressed as
∇ρJ (ρ) = (cid:82)(cid:82) p(h|θ)∇ρp(θ |ρ)R(h)dhdθ = (cid:82)(cid:82) p(h|θ)p(θ |ρ)∇ρ log p(θ |ρ)R(h)dhdθ ,
Differentiating this with respect to ρ, we have
where the log trick for ∇ρp(θ |ρ) is used. We then approximate the expectation over h and θ by the
(cid:80)N
∇ρ (cid:98)J (ρ) = 1
empirical average:
n=1 ∇ρ log p(θn |ρ)R(hn ),
N
where each trajectory sample hn is drawn from p(h|θn ) and the parameter θn is drawn from
p(θn |ρ).
Let us employ the Gaussian prior distribution with hyper-parameter ρ = (η , τ ) to draw parameter
(cid:16)− (θi−ηi )2
(cid:17)
vector θ , where η is the mean vector and τ is the vector consisting of the standard deviation in each
element:
p(θi |ρi ) = 1
√
exp
.
2τ 2
2π
τi
i
Then the derivative of log p(θ |ρ) with respect to ηi and τi are given as follows:
(θi − ηi )2 − τ 2
and ∇τi log p(θ |ρ) =
∇ηi log p(θ |ρ) = θi−ηi
i
τ 2
τ 3
i
i

.

3 Variance of Gradient Estimates

In this section, we theoretically investigate the variance of gradient estimates in REINFORCE and
PGPE.
For multi-dimensional state space, we consider the trace of the covariance matrix of gradient vectors.
(cid:16)E(cid:2)(A − E[A])(A − E[A])(cid:62)(cid:3)(cid:17)
E(cid:104)
(Am − E[Am ])2 (cid:105)
= (cid:80)(cid:96)
That is, for a random vector A = (A1 , . . . , A(cid:96) )(cid:62), we deﬁne
where E denotes the expectation. Let B = (cid:80)(cid:96)
Var(A) = tr
m=1
i=1 τ −2
, where (cid:96) is the dimensionality of state s.
i
Below, we consider a subset of the following assumptions:

(1)

,

3

Assumption (A): r(s, a, s(cid:48) ) ∈ [−β , β ] for β > 0.
Assumption (B): r(s, a, s(cid:48) ) ∈ [α, β ] for 0 < α < β .
t=1 and {dt}T
Assumption (C): For δ > 0, there exist two series {ct}T
t=1 such that (cid:107)st(cid:107)2 ≥ ct and
(cid:107)st(cid:107)2 ≤ dt hold with probability at least (1− δ)1/2N respectively over the choice of sample
paths, where (cid:107) · (cid:107)2 denotes the (cid:96)2 -norm.
L(T ) = CT α2 − DT β 2 /(2π), CT = (cid:80)T
and DT = (cid:80)T
Note that Assumption (B) is stronger than Assumption (A). Let
t=1 c2
t=1 d2
t ,
t .
First, we analyze the variance of gradient estimates in PGPE (the proofs of all the theorems are
provided in the supplementary material):
(cid:105) ≤ β 2 (1−γ T )2B
(cid:104)∇η (cid:98)J (ρ)
(cid:105) ≤ 2β 2 (1−γ T )2B
(cid:104)∇τ (cid:98)J (ρ)
Theorem 1. Under Assumption (A), we have the following upper bounds:
and Var
,
Var
N (1−γ )2
N (1−γ )2
This theorem means that the upper bound of the variance of ∇η (cid:98)J (ρ) is proportional to β 2 (the upper
and is inverse-proportional to sample size N . The upper bound of the variance of ∇τ (cid:98)J (ρ) is twice
bound of squared rewards), B (the trace of the inverse Gaussian covariance), and (1− γ T )2 /(1− γ )2 ,
larger than that of ∇η (cid:98)J (ρ). When T goes to inﬁnity, (1 − γ T )2 will converge to 1.
Next, we analyze the variance of gradient estimates in REINFORCE:
Theorem 2. Under Assumptions (B) and (C), we have the following lower bound with probability
(cid:104)∇µ (cid:98)J (θ)
(cid:105) ≥ (1−γ T )2
at least 1 − δ :
N σ2 (1−γ )2 L(T ).
Var
Under Assumptions (A) and (C), we have the following upper bound with probability at least (1 −
(cid:104)∇µ (cid:98)J (θ)
(cid:105) ≤ DT β 2 (1−γ T )2
δ)1/2 :
N σ2 (1−γ )2
(cid:105) ≤ 2T β 2 (1−γ T )2
(cid:104)∇σ (cid:98)J (θ)
Var
N σ2 (1−γ )2 .
with respect to trajectory length T . The lower bound for the variance of ∇µ (cid:98)J (θ) will be non-trivial
The upper bounds for REINFORCE are similar to those for PGPE, but they are monotone increasing
Deriving a lower bound of the variance of ∇σ (cid:98)J (θ) is left open as future work.
if it is positive, i.e., L(T ) > 0. This can be fulﬁlled, e.g., if α and β satisfy 2πCT α2 > DT β 2 .
Finally, we compare the variance of gradient estimates in REINFORCE and PGPE:
Theorem 3. In addition to Assumptions (B) and (C), we assume L(T ) is positive and mono-
Var[∇µ (cid:98)J (θ)] > Var[∇η (cid:98)J (ρ)] for all T > T0 , with probability at least 1 − δ .
If there exists T0 such that L(T0 ) ≥ β 2Bσ2 , then we have
tone increasing with respect to T .
The above theorem means that PGPE is more favorable than REINFORCE in terms of the variance
of gradient estimates of the mean, if trajectory length T is large. This theoretical result would
partially support the experimental success of the PGPE method [12].

Under Assumption (A), we have

Var

.

4 Variance Reduction by Subtracting Baseline

In this section, we give a method to reduce the variance of gradient estimates in PGPE and analyze
its theoretical properties.

4

4.1 Basic Idea of Introducing Baseline
It is known that the variance of gradient estimates can be reduced by subtracting a baseline b: for
n=1 (R(hn ) − b) (cid:80)T
(cid:80)N
∇θ (cid:98)J b (θ) = 1
REINFORCE and PGPE, modiﬁed gradient estimates are given by
(cid:80)N
∇ρ (cid:98)J b (ρ) = 1
t |sn
t=1 ∇θ log p(an
t , θ),
N
n=1 (R(hn ) − b)∇ρ log p(θn |ρ).
N
The adaptive reinforcement baseline [18] was derived as the exponential moving average of the past
experience:
b(n) = γR(hn−1 ) + (1 − γ )b(n − 1),
where 0 < γ ≤ 1. Based on this, an empirical gradient estimate with the moving-average baseline
was proposed for REINFORCE [18] and PGPE [12].
The above moving-average baseline contributes to reducing the variance of gradient estimates. How-
ever, it was shown [5, 16] that the moving-average baseline is not optimal; the optimal baseline is,
by deﬁnition, given as the minimizer of the variance of gradient estimates with respect to a baseline.
REINFORCE := arg minb Var[∇θ (cid:98)J b (θ)] =
E[R(h)(cid:107) (cid:80)T
Following this formulation, the optimal baseline for REINFORCE is given as follows [10]:
E[(cid:107) (cid:80)T
t=1 ∇θ log p(at |st ,θ)(cid:107)2 ]
b∗
t=1 ∇θ log p(at |st ,θ)(cid:107)2 ]
However, only the moving-average baseline was introduced to PGPE so far [12], which is subopti-
mal. Below, we derive the optimal baseline for PGPE, and study its theoretical properties.

.

,

E[(cid:107)∇ρ log p(θ |ρ)(cid:107)2 ].

4.2 Optimal Baseline for PGPE
PGPE := arg minb Var[∇ρ (cid:98)J b (ρ)].
Let b∗
PGPE be the optimal baseline for PGPE that minimizes the variance:
b∗
Then the following theorem gives the optimal baseline for PGPE:
Theorem 4. The optimal baseline for PGPE is given by
E[R(h)(cid:107)∇ρ log p(θ |ρ)(cid:107)2 ]
b∗
PGPE =
E[(cid:107)∇ρ log p(θ |ρ)(cid:107)2 ]
Var[∇ρ (cid:98)J b (ρ)] − Var[∇ρ (cid:98)J b∗
and the excess variance for a baseline b is given by
PGPE (ρ)] = (b−b∗
PGPE )2
N
The above theorem gives an analytic-form expression of the optimal baseline for PGPE. When ex-
pected return R(h) and the squared norm of characteristic eligibility (cid:107)∇ρ log p(θ |ρ)(cid:107)2 are indepen-
dent of each other, the optimal baseline is reduced to average expected return E[R(h)]. However,
the optimal baseline is generally different from the average expected return. The above theorem also
shows that the excess variance is proportional to the squared difference of baselines (b − b∗
PGPE )2
and the expected squared norm of characteristic eligibility E[(cid:107)∇ρ log p(θ |ρ)(cid:107)2 ], and is inverse-
proportional to sample size N .
Next, we analyze the contribution of the optimal baseline to the variance with respect to mean
parameter η in PGPE:
Theorem 5. If r(s, a, s(cid:48) ) ≥ α > 0, we have the following lower bound:
Var[∇η (cid:98)J (ρ)] − Var[∇η (cid:98)J b∗
PGPE (ρ)] ≥ α2 (1−γ T )2B
N (1−γ )2
Var[∇η (cid:98)J (ρ)] − Var[∇η (cid:98)J b∗
Under Assumption (A), we have the following upper bound:
PGPE (ρ)] ≤ β 2 (1−γ T )2B
N (1−γ )2
This theorem shows that the lower and upper bounds of the excess variance are proportional to α2
and β 2 (the bounds of squared immediate rewards), B (the trace of the inverse Gaussian covariance),
and (1 − γ T )2/(1 − γ )2 , and are inverse-proportional to sample size N . When T goes to inﬁnity,
(1 − γ T )2 will converge to 1.

.

.

5

4.3 Comparison with REINFORCE

.

.

E

(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13)(cid:80)T
Next, we analyze the contribution of the optimal baseline for REINFORCE, and compare it with that
for PGPE. It was shown [5, 16] that the excess variance for a baseline b in REINFORCE is given by
Var[∇θ (cid:98)J b (θ)] − Var[∇θ (cid:98)J b∗
REINFORCE (θ)] = (b−b∗
t=1 ∇θ log p(at |st , θ)
REINFORCE )2
N
Based on this, we have the following theorem:
Theorem 6. Under Assumptions (B) and (C), we have the following bounds with probability at least
1 − δ :
N σ2 (1−γ )2 ≤ Var[∇µ (cid:98)J (θ)] − Var[∇µ (cid:98)J b∗
CT α2 (1−γ T )2
REINFORCE (θ)] ≤ β 2 (1−γ T )2DT
N σ2 (1−γ )2
The above theorem shows that the lower and upper bounds of the excess variance are monotone
increasing with respect to trajectory length T .
In the aspect of the amount of reduction in the variance of gradient estimates, Theorem 5 and Theo-
rem 6 show that the optimal baseline for REINFORCE contributes more than that for PGPE.
Finally, based on Theorem 1 and Theorem 5 and based on Theorem 2 and Theorem 6, we have the
following theorem:
Var[∇η (cid:98)J b∗
Theorem 7. Under Assumptions (B) and (C), we have
PGPE (ρ)] ≤ (1−γ T )2
N (1−γ )2 (β 2 − α2 )B ,
Var[∇µ (cid:98)J b∗
REINFORCE (θ)] ≤ (1−γ T )2
N σ2 (1−γ )2 (β 2DT − α2CT ),
where the latter inequality holds with probability at least 1 − δ .

This theorem shows that the upper bound of the variance of gradient estimates for REINFORCE with
the optimal baseline is still monotone increasing with respect to trajectory length T . On the other
with the optimal baseline can be further upper-bounded as Var[∇η (cid:98)J b∗
hand, since (1 − γ T )2 ≤ 1, the above upper bound of the variance of gradient estimates in PGPE
PGPE (ρ)] ≤ (β 2−α2 )B
N (1−γ )2 , which
is independent of T . Thus, when trajectory length T is large, the variance of gradient estimates in
REINFORCE with the optimal baseline may be signiﬁcantly larger than the variance of gradient
estimates in PGPE with the optimal baseline.

5 Experiments

In this section, we experimentally investigate the usefulness of the proposed method, PGPE with the
optimal baseline.

5.1
Illustrative Data
Let the state space S be one-dimensional and continuous, and the initial state is randomly chosen
N (0, 0.52 ) is stochastic noise. The immediate reward is deﬁned as r = exp (cid:0)−s2/2 − a2/2(cid:1) + 1,
from the standard normal distribution. The action space A is also set to be one-dimensional and
continuous. The transition dynamics of the environment is set at st+1 = st + at + ε, where ε ∼
which is bounded as 1 < r ≤ 2. The discount factor is set at γ = 0.9.
Here, we compare the following ﬁve methods: REINFORCE without any baselines, REINFORCE
with the optimal baseline (OB), PGPE without any baselines, PGPE with the moving-average base-
line (MB), and PGPE with the optimal baseline (OB). For fair comparison, all of these methods
use the same parameter setup: the mean and standard deviation of the Gaussian distribution is set
at µ = −1.5 and σ = 1, the number of episodic samples is set at N = 100, and the length of the
trajectory is set at T = 10 or 50. We then calculate the variance of gradient estimates over 100 runs.
Table 1 summarizes the results, showing that the variance of REINFORCE is overall larger than
PGPE. A notable difference between REINFORCE and PGPE is that the variance of REINFORCE

6

Table 1: Variance and bias of estimated gradients for the illustrative data.
T = 50
T = 10

Method

Variance
σ, τ
µ, η
26.9173
REINFORCE
13.2570
REINFORCE-OB 0.0914
0.1203
PGPE
1.6855
0.9707
0.3238
0.2127
PGPE-MB
PGPE-OB
0.0372
0.0685

Bias

µ, η
-0.3102
0.0672
-0.0691
0.0828
-0.0164

σ, τ
-1.5098
0.1286
0.1319
-0.1295
0.0512

Variance
σ, τ
278.3095
0.8996
3.3720
0.8332
0.1815

µ, η
188.3860
0.5454
1.6572
0.4123
0.0850

Bias

µ, η
-1.8126
-0.2988
-0.1048
0.0925
0.0480

σ, τ
-5.1747
-0.2008
-0.3293
-0.2556
-0.0779

(a) REINFORCE and REINFORCE-OB

(b) PGPE, PGPE-MB and PGPE-OB

(a) Good initial policy

(c) REINFORCE and PGPE

(d) REINFORCE-OB and PGPE-OB

(b) Poor initial policy

Figure 1: Variance of gradient estimates with respect to the
mean parameter through policy-update iterations for the illus-
trative data.

Figure 2: Return as functions of
the number of episodic samples
N for the illustrative data.

signiﬁcantly grows as T increases, whereas that of PGPE is not inﬂuenced that much by T . This well
agrees with our theoretical analysis in Section 3. The results also show that the variance of PGPE-
OB (the proposed method) is much smaller than that of PGPE-MB. REINFORCE-OB contributes
highly to reducing the variance especially when T is large, which also well agrees with our theory.
However, PGPE-OB still provides much smaller variance than REINFORCE-OB.
We also investigate the bias of gradient estimates of each method. We regard gradients estimated
with N = 1000 as true gradients, and compute the bias of gradient estimates when N = 100. The
results are also included in Table 1, showing that introduction of baselines does not increase the bias;
rather, it tends to reduce the bias.
Next, we investigate the variance of gradient estimates when policy parameters are updated over it-
erations. In this experiment, we set N = 10 and T = 20, and the variance is computed from 50 runs.
Policies are updated over 50 iterations. In order to evaluate the variance in a stable manner, we repeat
the above experiments 20 times with random choice of initial mean parameter µ from [−3.0, −0.1],
and investigate the average variance of gradient estimates with respect to mean parameter µ over 20
trials, in log10 -scale.
The results are summarized in Figure 1. Figure 1(a) compares the variance of REINFORCE
with/without baselines, whereas Figure 1(b) compares the variance of PGPE with/without baselines.
These plots show that introduction of baselines contributes highly to the reduction of the variance
over iterations. Figure 1(c) compares the variance of REINFORCE and PGPE without baselines,
showing that PGPE provides much more stable gradient estimates than REINFORCE. Figure 1(d)
compares the variance of REINFORCE and PGPE with the optimal baselines, showing that gradi-
ent estimates obtained by PGPE-OB are much smaller than those by REINFORCE-OB. Overall, in
terms of the variance of gradient estimates, the proposed PGPE-OB compares favorably with other
methods.
Next, we evaluate returns obtained by each method. The trajectory length is ﬁxed at T = 20, and
the maximum number of policy-update iterations is set at 50. We investigate average returns over 20
runs as functions of the number of episodic samples N . We have two experimental results for differ-
ent initial policies. Figure 2(a) shows the results when initial mean parameter µ is chosen randomly

7

010203040500123456IterationVariance in log10 scale  REINFORCEREINFORCE−OB01020304050−101234IterationVariance in log10 scale  PGPEPGPE−MBPGPE−OB01020304050123456IterationVariance in log10−scale  REINFORCEPGPE01020304050−1−0.500.511.522.53IterationVariance in log10−scale  REINFORCE−OBPGPE−OB024681012141618201414.51515.51616.517Number of episodes NReturn R  REINFORCEREINFORCE−OBPGPEPGPE−MBPGPE−OB0246810121416182012.51313.51414.51515.51616.517Number of episodes NReturn R  REINFORCEREINFORCE−OBPGPEPGPE−MBPGPE−OBfrom [−1.6, −0.1], which tends to perform well. The graph shows that PGPE-OB performs the best,
especially when N < 5; then REINFORCE-OB follows with a small margin. PGPE-MB and plain
PGPE also work reasonably well, although they are slightly unstable due to larger variance. Plain
REINFORCE is highly unstable, which is caused by the huge variance of gradient estimates (see
Figure 1 again).
Figure 2(b) describes the results when initial mean parameter µ is chosen randomly from
[−3.0, −0.1], which tends to result in poorer performance.
In this setup, difference among the
compared methods is more signiﬁcant than the case with good initial policies. Overall, plain REIN-
FORCE performs very poorly, and even REINFORCE-OB tends to be outperformed by the PGPE
methods. This means that REINFORCE is very sensitive to the choice of initial policies. Among
the PGPE methods, the proposed PGPE-OB works very well and converges quickly.

5.2 Cart-Pole Balancing

ϕt+1 = ϕt + ˙ϕt+1∆t

and

∆t,

Finally, we evaluate the performance of our proposed method in a more complex task of cart-pole
balancing [3]. A pole is hanged to the roof of a cart, and the goal is to swing up the pole by moving
the cart properly and try to keep the pole at the top.
The state space S is two-dimensional and continuous, which consists of the angle ϕ ∈ [0, 2π ] and
angular velocity ˙ϕ ∈ [−3π , 3π ] of the pole. The action space A is one-dimensional and continu-
ous, which corresponds to the force applied to the cart (note that we can not directly control the
pole, but only indirectly through moving the cart). We use the Gaussian policy model for REIN-
FORCE and linear policy model for PGPE, where state s is non-linearly transformed to a feature
space via a basis function vector. We use 20 Gaussian kernels with standard deviation σ = 0.5
as the basis functions, where the kernel centers are distributed over the following grid points:
{0, π/2, π , 3π/2} × {−3π , −3π/2, 0, 3π/2, 3π}. The dynamics of the pole (i.e., the update rule
of the angle and the angular velocity) is given by
˙ϕt+1 = ˙ϕt + 9.8 sin(ϕt )−αwl ˙ϕ2
t sin(2ϕt )/2+α cos(ϕt )at
4l/3−αwl cos2 (ϕt )
where α = 1/W + w and at is the action taken at time t. We set the problem parameters as: the
mass of the cart W = 8[kg], the mass of the pole w = 2[kg], and the length of the pole l = 0.5[m].
We set the time step ∆t for the position and velocity updates at 0.01[s] and action selection at 0.1[s].
The reward function is deﬁned as r(st , at , st+1 ) = cos(ϕt+1 ). That is, the higher the pole is, the
more rewards we can obtain. The initial policy is chosen randomly, and the initial-state probability
density is set to be uniform. The agent collects N = 100 episodic samples with trajectory length
T = 40. The discount factor is set at γ = 0.9.
We investigate average returns over 10 trials as the functions of policy-update iterations. The return
at each trial is computed over 100 test episodic samples (which are not used for policy learning).
The experimental results are plotted in Figure 3, showing that the improvement of both plain REIN-
FORCE and REINFORCE-OB tend to be slow, and all PGPE methods outperformed REINFORCE
methods overall. Among the PGPE methods, the proposed PGPE-OB converges faster than others.
6 Conclusion
In this paper, we analyzed and improved the stability of the policy
gradient method called PGPE (policy gradients with parameter-
based exploration). We theoretically showed that, under a mild
condition, PGPE provides more stable gradient estimates than
the classical REINFORCE method. We also derived the optimal
baseline for PGPE, and theoretically showed that PGPE with the
optimal baseline is more preferable than REINFORCE with the
optimal baseline in terms of the variance of gradient estimates.
Finally, we demonstrated the usefulness of PGPE with optimal
baseline through experiments.

Figure 3: Performance of policy

Acknowledgments: TZ and GN were supported by the MEXT scholarship and the GCOE program,
HH was supported by the FIRST program, and MS was supported by MEXT KAKENHI 23120004.

8

050100150200250300−2−1012345IterationReturn R  REINFORCEREINFORCE−OBPGPEPGPE−MBPGPE−OBReferences
[1] N. Abe, P. Melville, C. Pendus, C. K. Reddy, D. L. Jensen, V. P. Thomas, J. J. Bennett, G. F.
Anderson, B. R. Cooley, M. Kowalczyk, M. Domick, and T. Gardinier. Optimizing debt col-
lections using constrained reinforcement learning. In Proceedings of The 16th ACM SGKDD
Conference on Knowledge Discovery and Data Mining, pages 75–84, 2010.
[2] J. Baxter, P. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon, policy-gradient esti-
mation. Journal of Artiﬁcial Intelligence Research, 15:351–381, 2001.
[3] M. Bugeja. Non-linear swing-up and stabilizing control of an inverted pendulum system. In
Proceedings of IEEE Region 8 EUROCON, volume 2, pages 437–441, 2003.
[4] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neu-
ral Computation, 9(2):271–278, 1997.
[5] E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient esti-
mates in reinforcement learning. Journal of Machine Learning Research, 5:1471–1530, 2004.
[6] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal
of Artiﬁcial Intelligence Research, 4:237–285, 1996.
[7] S. Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Processing Systems 14, pages 1531–1538, Cambridge,
MA, 2002. MIT Press.
[8] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning
Research, 4:1107–1149, 2003.
[9] P. Marbach and J. N. Tsitsiklis. Approximate gradient methods in policy-space optimization
of Markov reward processes. Discrete Event Dynamic Systems, 13(1-2):111–148, 2004.
[10] J. Peters and S. Schaal. Policy gradient methods for robotics. In Processing of the IEEE/RSJ
International Conferece on Inatelligent Robots and Systems(IROS), 2006.
[11] F. Sehnke, C. Osendorfer, T. R ¨uckstiess, A. Graves, J. Peters, and J. Schmidhuber. Policy gra-
dients with parameter-based exploration for control. In Proceedings of The 18th International
Conference on Artiﬁcial Neural Networks, pages 387–396, 2008.
[12] F. Sehnke, C. Osendorfer, T. R ¨uckstiess, A. Graves, J. Peters, and J. Schmidhuber. Parameter-
exploring policy gradients. Neural Networks, 23(4):551–559, 2010.
[13] R. S. Sutton and G. A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA, 1998.
[14] G. Tesauro. TD-gammon, a self-teaching backgammon program, achieves master-level play.
Neural Computation, 6(2):215–219, 1994.
[15] L. Weaver and J. Baxter. Reinforcement learning from state and temporal differences. Techni-
cal report, Department of Computer Science, Australian National University, 1999.
[16] L. Weaver and N. Tao. The optimal reward baseline for gradient-based reinforcement learning.
In Processings of The Seventeeth Conference on Uncertainty in Artiﬁcial Intelligence, pages
538–545, 2001.
[17] J. D. Williams and S. Young. Partially observable Markov decision processes for spoken dialog
systems. Computer Speech and Language, 21(2):231–422, 2007.
[18] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8:229, 1992.

9


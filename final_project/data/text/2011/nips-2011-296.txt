Solving Decision Problems with Limited Information

Denis D. Mau ´a
IDSIA
Manno, CH 6928
denis@idsia.ch

Cassio P. de Campos
IDSIA
Manno, CH 6928
cassio@idsia.ch

Abstract

We present a new algorithm for exactly solving decision-making problems rep-
resented as an inﬂuence diagram. We do not require the usual assumptions of
no forgetting and regularity, which allows us to solve problems with limited in-
formation. The algorithm, which implements a sophisticated variable elimination
procedure, is empirically shown to outperform a state-of-the-art algorithm in ran-
domly generated problems of up to 150 variables and 1064 strategies.

1

Introduction

In many tasks, bounded resources and physical constraints force decisions to be made based on lim-
ited information [1, 2]. For instance, a policy for a partially observable Markov decision process
(POMDP) might be forced to disregard part of the available information in order to meet computa-
tional demands [3]. Cooperative multi-agent settings offer another such example: each agent might
perceive only its surroundings and be unable to communicate with all other agents; hence, a policy
specifying an agent’s behavior must rely exclusively on local information [4]; it might be further
constrained to a maximum size to be computationally tractable [5].
Inﬂuence diagrams [6] are representational devices for utility-based decision making under uncer-
tainty. Many popular decision-making frameworks such as ﬁnite-horizon POMDPs can be casted
as inﬂuence diagrams [7]. Traditionally, inﬂuence diagrams target problems involving a single,
non-forgetful decision maker; this makes them unﬁtted to represent decision-making with limited
information. Limited memory inﬂuence diagrams (LIMIDs) generalize inﬂuence diagrams to allow
for (explicit representation of) bounded memory policies and simultaneous decisions [1, 2]. More
precisely, LIMIDs relax the regularity and no forgetting assumptions of inﬂuence diagrams, namely,
that there is a complete temporal ordering over the decisions, and that observations and decisions
are permanently remembered.
Solving a LIMID refers to ﬁnding a combination of policies that maximizes expected utility. This
task has been empirically and theoretically shown to be a very hard problem [8]. Under certain
graph-structural conditions (which no forgetting and regularity imply), Lauritzen and Nilsson [2]
show that LIMIDs can be solved by dynamic programming with complexity exponential in the
treewidth of the graph. However, when these conditions are not met, their iterative algorithm might
converge to a local optimum that is far from the optimum. Recently, de Campos and Ji [8] formulated
the CR (Credal Reformulation) algorithm that solves a LIMID by mapping it into a mixed integer
programming problem; they show that CR is able to solve small problems exactly and obtain good
approximations for medium-sized problems.
In this paper, we formally describe LIMIDs (Section 2) and show that policies can be partially
ordered, and that the ordering can be extended monotonically, allowing for the generalized variable
elimination procedure in Section 3. We show experimentally in Section 4 that the algorithm built
on these ideas can enormously save computational resources, allowing many problems to be solved
exactly. In fact, our algorithm is orders of magnitude faster than the CR algorithm on randomly
generated diagrams containing up to 150 variables. Finally, we write our conclusions in Section 5.

1

2 Limited memory inﬂuence diagrams

In the LIMID formalism, the quantities and events of interest are represented by three distinct types
of variables or nodes: chance variables (oval nodes) represent events on which the decision maker
has no control, such as outcomes of tests or consequences of actions; decision variables (square
nodes) represent the alternatives a decision maker might have; value variables (diamond-shaped
nodes) represent additive parcels of the overall utility. Let U be the set of all variables relevant to
a problem. Each variable X in U has associated a domain ΩX , which is the ﬁnite non-empty set
of values or states X can assume. The empty domain Ω∅ (cid:44) {λ} contains a single element λ that
is not in any other domain. Decision and chance variables have domains different from the empty
domain, whereas value variables are always associated to the empty domain. The domain Ωx of a
set of variables x = {X1 , . . . , Xn} ⊆ U is the Cartesian product ΩX1 × · · · × ΩXn of the variable
domains. If x and y are sets of variables such that y ⊆ x ⊆ U , and x is an element of the domain Ωx ,
we write x↓y to denote the projection of x onto the smaller domain Ωy , that is, x↓y ∈ Ωy contains
only the components of x that are compatible with the variables in y . By convention, x↓∅ (cid:44) λ. The
cylindrical extension of y ∈ Ωy to Ωx is the set y↑x (cid:44) {x ∈ Ωx : x↓y = y}. Oftentimes, if clear
from the context, we write X1 · · · Xn to denote the set {X1 , . . . , Xn}, and X to denote {X }.
We notate point-wise comparison of functions implicitly. For example, if f and g are real-valued
functions over a domain Ωx and k is a real number, we write f ≥ g and f = k meaning f (x) ≥ g(x)
and f (x) = k , respectively, for all x ∈ Ωx . Any function over a domain containing a single element
is identiﬁed by the real number it returns.
If f and g are functions over domains Ωx and Ωy ,
over Ωx , and y ⊆ U , the sum-marginal (cid:80)
respectively, their product f g is the function over Ωx∪y such that (f g)(w) = f (w↓x )g(w↓y ) for all
w of its domain we have ((cid:80)
y f )(w) = (cid:80)
x∈w↑x f (x). Notice that if y ∩ x = ∅, then (cid:80)
w . Sum of functions is deﬁned analogously: (f + g)(w) = f (w↓x ) + g(w↓y ). If f is a function
y f returns a function over Ωx\y such that for any element
y f = f .
Let C , D and V denote the sets of chance, decision and value variables, respectively, in U . A LIMID
L is an annotated direct acyclic graph (DAG) over the set of variables U , where the nodes in V have
no children. The precise meanings of the arcs in L vary according to the type of node to which
they point. Arcs entering chance and value nodes denote stochastic and functional dependency,
respectively; arcs entering decision nodes describe information awareness or relevance at the time
the decision is made. If X is a node in L, we denote by paX the set of parents of X , that is, the
set of nodes of L from which there is an arc pointing to X . Similarly, we let chX denote the set
of children of X (i.e., nodes to which there is an arc from X ), and faX (cid:44) paX ∪ {X } denote
its family. Each chance variable C in C has an associated function ppaC
C specifying the probability
Pr(C = x↓C |paC = x↓paC ) of C assuming value x↓C ∈ ΩC given that the parents take on values
x↓paC ∈ ΩpaC for all x ∈ ΩfaC . We assume that the probabilities associated to any chance node
respect the Markov condition, that is, that any variable X ∈ C is stochastically independent from its
non-descendant non-parents given its parents. Each value variable V ∈ V is associated to a bounded
sum of utility functions (cid:80)
real-valued utility function uV over ΩpaV , which quantiﬁes the (additive) contribution of the states
of its parents to the overall utility. Thus, the overall utility of a joint state x ∈ ΩC∪D is given by the
V ∈V uV (x↓paV ). For any decision variable D ∈ D , a policy δD speciﬁes
→ ΩD . If D has no
an action for each possible state conﬁguration of its parents, that is, δD : ΩpaD
parents, then δD is a function from the empty domain to ΩD , and therefore constitutes a choice of
x ∈ ΩD . The set of all policies δD for a variable D is denoted by ∆D .
To illustrate the use of LIMIDs, consider the following example involving a memoryless robot in
a 5-by-5 gridworld (Figure 1a). The robot has 9 time steps to ﬁrst reach a position sA of the grid,
for which it receives 10 points, and then a position sB , for which it is rewarded with 20 points. If
the positions are visited in the wrong order, or if a point is re-visited, no reward is given. At each
step, the robot can perform actions move north, south, east or west, which cost 1 point and succeed
with 0.9 probability, or do nothing, which incurs no cost and always succeeds. Finally, the robot can
estimate its position in the grid by measuring the distance to each of the four walls. The estimated
position is correct 70% of the time, wrong by one square 20% of the time, and by two squares 10%
of the time. The LIMID in Figure 1b formally represents the environment and the robot behavior.
The action taken by the robot at time step t is represented by variable Dt (t = 1, . . . , 8). The costs
associated to decisions are represented by variables Ct , which have associated functions uCt that

2

sA

sB

R

(a)

C1

C2

C8

O1

D1

O2

D2

O8

D8

S1

A1

B1

S2

A2

B2

· · ·

S8

A8

B8

S9

A9

B9

R1

R2

R8

R9

(b)

Figure 1: (a) A robot R in a 5-by-5 gridworld with two goal-states. (b) The corresponding LIMID.

return zero if Dt = nothing, and otherwise return -1. The variables St (t = 1, . . . , 9) represent
the robot’s actual position at time step t, while variables Ot denote its estimated position. The
associated to St speciﬁes the probabilities Pr(St = st |St−1 = st−1 , Dt = dt ) of
function pSt−1Dt
St
transitioning to state St = st from a state St−1 = st−1 when the robot executes action Dt = dt .
The function pSt
is associated to Ot and quantiﬁes the likelihood of estimating position Ot = ot
Ot
when in position St = st . We use binary variables At and Bt to denote whether positions sA and
sB , respectively, have been visited by the robot before time step t. Hence, the function pAt−1 St−1
At
associated to At equals one for At = y if St−1 = sa or At−1 = y , and zero otherwise. Likewise, the
function pBt−1 St−1
equals one for Bt = y only if either St−1 = sB or Bt−1 = y . The reward received
Bt
by the robot in step t is represented by variable Rt . The utility function uRt associated to Rt equals
10 if st = sA and At = n and Bt = n, 20 if st = sB and At = y and Bt = n, and zero otherwise.
Let ∆ (cid:44) ×D∈D ∆D denote the space of possible combinations of policies. An element s =
(δD )D∈D ∈ ∆ is said to be a strategy for L. Given a policy δD , let ppaD
D denote a function such that
for each x ∈ ΩfaD it equals one if x↓D = δD (x↓paD ) and zero otherwise. In other words, ppaD
D is a
conditional probability table representing policy δD . There is a one-to-one correspondence between
D and policies δD ∈ ∆D , and specifying a policy δD is equivalent to specifying ppaD
functions ppaD
D .
(cid:89)
ps (cid:44) (cid:89)
D by PD . A strategy s induces a joint probability mass function
We denote the set of all functions ppaD
over the variables in C ∪ D by
ppaC
C
C∈C
D∈D
(cid:88)
Es [L] (cid:44) (cid:88)
and has an associated expected utility given by
x∈ΩC∪D
V ∈V
The treewidth of a graph measures its resemblance to a tree and is given by the number of vertices
in the largest clique of the corresponding triangulated moral graph minus one. Given a LIMID L
of treewidth ω , we can evaluate the expected utility of any strategy s in time and space at most
exponential in ω . Hence, if ω is bounded by a constant, computing Es [L] takes polynomial time [9].
The primary task of a LIMID is to ﬁnd an optimal strategy s∗ with maximal expected utility, that is,
to ﬁnd s∗ such that Es [L] ≤ Es∗ [L] for all s ∈ ∆. The value Es∗ [L] is called the maximum expected
utility of L and it is denoted by MEU[L]. In the LIMID of Figure 1, the goal is to ﬁnd an optimal
strategy s = (δD1 , . . . , δD8 ), where the optimal policies δDt for t = 1, . . . , 8 prescribe an action in
ΩDt = {north, south, west, east, nothing} for each possible estimated position in ΩOt .
For most real problems, enumerating all the strategies is prohibitively costly. In fact, computing the
MEU is NP-hard even in bounded treewidth diagrams [8]. It is well-known that any LIMID L can
be mapped into an equivalent LIMID L(cid:48) where all utilities take values on the real interval [0, 1] [10].
The mapping preserves optimality of strategies, that is, any optimal strategy for L(cid:48) is also an optimal

uV (x↓paV ) =

(cid:88)
V ∈V

(cid:88)
C∪D

(1)

(2)

ppaD
D ,

ps (x)

ps

uV .

3

strategy for L (and vice-versa). This allows us, in the rest of the paper, to focus on LIMIDs whose
utilities are deﬁned in [0, 1] with no loss of generality for the algorithm we devise.

3 A fast algorithm for solving LIMIDs exactly

The basic ingredients of our algorithmic framework for representing and handling information in
LIMIDs are the so-called valuations, which encode information (probabilities, utilities and policies)
about the elements of a domain. Each valuation is associated to a subset of the variables in U ,
called its scope. More concretely, we deﬁne a valuation φ with scope x as a pair (p, u) of bounded
nonnegative real-valued functions p and u over the domain Ωx ; we refer to p and u as the probability
all possible valuations is given by Φ (cid:44) (cid:83)
and utility part, respectively, of φ. Often, we write φx to make explicit the scope x of a valuation
φ. For any x ⊆ U , we denote the set of all possible valuations with scope x by Φx . The set of
x⊆U Φx . The set Φ is closed under the operations of
combination and marginalization. Combination represents the aggregation of information and is
deﬁned as follows. If φ = (p, u) and ψ = (q , v) are valuations with scopes x and y , respectively, its
x\y p, (cid:80)
variables such that y ⊆ x, the marginal φ↓y is the valuation ((cid:80)
combination φ ⊗ ψ is the valuation (pq , pv + qu) with scope x ∪ y . Marginalization, on the other
hand, acts by coarsening information. If φ = (p, u) is a valuation with scope x, and y is a set of
x\y u) with scope y . In this
case, we say that z (cid:44) x \ y has been eliminated from φ, which we denote by φ−z . The following
result shows that our framework respects the necessary conditions for computing efﬁciently with
valuations (in the sense of keeping the scope of valuations minimal during the variable elimination
procedure).
Proposition 1. The system (Φ, U , ⊗, ↓) satisﬁes the following three axioms of a (weak) labeled
valuation algebra [11, 12].
(A1) For any φ1 , φ2 , φ3 ∈ Φ we have that φ1 ⊗ φ2 = φ2 ⊗ φ1 and φ1 ⊗ (φ2 ⊗ φ3 ) = (φ1 ⊗ φ2 ) ⊗ φ3 .
(A2) For any φz ∈ Φz and y ⊆ x ⊆ z we have that (φ↓x
z )↓y = φ↓y
z .
(A3) For any φx ∈ Φx , φy ∈ Φy and x ⊆ z ⊆ x ∪ y we have that (φx ⊗ φy )↓z = φx ⊗ φ↓y∩z
y

.

Proof. (A1) follows directly from commutativity, associativity and distributivity of product and sum
of real-valued functions, and (A2) follows directly from commutativity of the sum-marginal opera-
x∪y\z pq , (cid:80)
[(p, u) ⊗ (q , v)]↓z = ((cid:80)
tion. To show (A3), consider any two valuations (p, u) and (q , v) with scopes x and y , respectively,
and a set z such that x ⊆ z ⊆ x ∪ y . By deﬁnition of combination and marginalization, we have that
tions over Ωx , it follows that ((cid:80)
x∪y\z pq , (cid:80)
x∪y\z (pv + qu)) = (p (cid:80)
y\z q , p (cid:80)
y\z v + u (cid:80)
x∪y\z (pv + qu)). Since x ∪ y \ z = y \ z , and p and u are func-
which equals (p, u) ⊗ ((cid:80)
y\z q , (cid:80)
y\z q),
y\z v) = (p, y) ⊗ (q , v)↓y∩z . Hence, [(p, u) ⊗ (q , v)]↓z =
(p, y) ⊗ (q , v)↓y∩z .

The following lemma is a direct consequence of (A3) shown by [12], required to prove the correct-
ness of our algorithm later on.
Lemma 2. If z ⊆ y and z ∩ x = ∅ then (φx ⊗ φy )−z = φx ⊗ φ−z
y .
The framework of valuations allows us to compute the expected utility of a given strategy efﬁciently:
(cid:105)
φs (cid:44) (cid:104)(cid:79)
(cid:105) ⊗ (cid:104) (cid:79)
(cid:105) ⊗ (cid:104)(cid:79)
Proposition 3. Given a LIMID L and a strategy s = (δD )D∈D , let
(ppaD
(ppaC
(1, uV )
D , 0)
C , 0)
V ∈V
D∈D
C∈C
D is the function in PD associated with policy δD . Then φ↓∅
s = (1, Es [L]).
where, for each D , ppaD
V ∈V uV ), where ps = (cid:81)
(cid:80)
ps is a probability distribution over C ∪ D , it follows that p = (cid:80)
Proof. Let p and u denote the probability and utility part, respectively, of φ↓∅
s . By deﬁnition of
u = (cid:80)C∪D ps
(cid:80)
X ∈C∪D ppaX
combination, we have that φs = (ps , ps
X as in (1). Since
x∈ΩC∪D ps (x) = 1. Finally,
V ∈V uV , which equals Es [L] by (2).

(3)

,

4

A

B

D

C

E

Input: elimination ordering B < C < A and strategy s = (δB , δC )
Initialization:

φA = (pA , 0) φB = (pA
B , 0) φC = (pA
C , 0) φD = (1, uD ) φE = (1, uE )
Propagation:
ψ1 = (φB ⊗ φD )
ψ2 = (φC ⊗ φE )
Termination: return the utility part of φ↓∅
s = ψ3

ψ3 = (ψ1 ⊗ ψ2 ⊗ φA )

−A

−B

−C

Figure 2: Computing the expected utility of a strategy by variable elimination.

Given any strategy s, we can use a variable elimination procedure to efﬁciently compute φ↓∅
s and
hence its expected utility in time polynomial in the largest domain of a variable but exponential in
the width of the elimination ordering.1 Figure 2 shows a variable elimination procedure used to
compute the expected utility of a strategy of the simple LIMID on the left-hand side. However,
computing the MEU in this way is unfeasible for any reasonable diagram due to the large number of
strategies that would need to be enumerated. For example, if the variables A, B and C in the LIMID
in Figure 2 have each ten states, there are 10101010 = 1020 possible strategies.
In order to avoid considering all possible strategies, we deﬁne a partial order (i.e., a reﬂexive, an-
tisymmetric and transitive relation) over Φ as follows. For any two valuations φ = (p, u) and
ψ = (q , v) in Φ, if φ and ψ have equal scope, p ≤ q and u ≤ v , then φ ≤ ψ holds. The following
result shows that ≤ is monotonic with respect to combination and marginalization.
Proposition 4. The system (Φ, U , ⊗, ↓, ≤) satisﬁes the following two additional axioms of an or-
dered valuation algebra [13].
(A4) If φx ≤ ψx and φy ≤ ψy , then (φx ⊗ φy ) ≤ (ψx ⊗ ψy ).
x ≤ ψ↓y
(A5) If φx ≤ ψx then φ↓y
x .
Proof. (A4). Consider two valuations (px , ux ) and (qx , vx ) with scope x such that (px , ux ) ≤
(qx , vx ), and two valuations (py , uy ) and (qy , vy ) with scope y satisfying (py , uy ) ≤ (qy , vy ). By
deﬁnition of ≤, we have that px ≤ qx , ux ≤ vx , py ≤ qy and uy ≤ vy . Since all functions are
nonnegative, it follows that pxpy ≤ qx qy , pxuy ≤ qx vy and py ux ≤ qy vx . Hence, (px , ux ) ⊗
(px , ux )↓y = ((cid:80)
x\y px , (cid:80)
x\y ux ) ≤ ((cid:80)
x\y qx , (cid:80)
(py , uy ) = (pxpy , pxuy + py ux ) ≤ (qx qy , qx vy + qy vx ) = (qx , vx ) ⊗ (qy , vy ). (A5). Let y be
It follows from monotonicity of ≤ with respect to addition of real numbers that
a subset of x.
x\y vx ) = (qx , vx )↓y .
The monotonicity of ≤ allows us to detect suboptimal strategies during variable elimination. To
illustrate this, consider the variable elimination scheme in Figure 2 for two different strategies s
and s(cid:48) , and let ψs
3 be the valuations produced in the propagation step for strategy s and
2 , ψs
1 , ψs
2 ≤ ψs(cid:48)
1 ≤ ψs(cid:48)
3 the valuations for s(cid:48) .
2 , ψs(cid:48)
1 , ψs(cid:48)
ψs(cid:48)
2 then Proposition 4 tells us that
1 and ψs
If ψs
3 , which implies Es [L] ≤ Es(cid:48) [L]. As a consequence, we can abort variable elimination for
3 ≤ ψs(cid:48)
ψs
s after the second iteration. We can also exploit the redundancy between valuations produced during
variable elimination for neighbor strategies. For example, if s and s(cid:48) specify the same policy for B ,
1 = ψs(cid:48)
2 , so that only one of them needs to be computed.
then we know in advance that ψs
In order to facilitate the description of our algorithm, we deﬁne operations over sets of valuations.
If Ψx is a set of valuations with scope x and Ψy is a set of valuations with scope y the operation
Ψx ⊗ Ψy (cid:44) {φx ⊗ φy : φx ∈ Ψx , φy ∈ Ψy } returns the set of combinations of a valuation in Ψx
(cid:44) {φ−X
and a valuation in Ψy . For X ∈ x, the operation Ψ−X
: φx ∈ Ψx } eliminates variable X
from all valuations in Ψx . Given a ﬁnite set of valuations Ψ ⊆ Φ, we say that a valuation φ ∈ Ψ is
x
x
maximal if for all ψ ∈ Ψ such that φ ≤ ψ it holds that ψ ≤ φ. The operator prune returns the set
prune(Ψ) of maximal valuations of Ψ (by pruning non-maximal valuations).

1The width of an elimination ordering is the maximum cardinality of the scope of a valuation produced
during variable elimination minus one.

5

We are now ready to describe the Multiple Policy Updating (MPU) algorithm, which solves ar-
bitrary LIMIDs exactly. Consider a LIMID L and an elimination ordering X1 < · · · < Xn over
the variables in C ∪ D . The elimination ordering can be selected using the standard methods for
Bayesian networks [9]. Note that unlike standard algorithms for variable elimination in inﬂuence
diagrams we allow any elimination ordering. The algorithm is initialized by generating one set of
valuations for each variable X in U as follows.
Initialization: Let V0 be initially the empty set.
1. For each chance variable X ∈ C , add a singleton ΨX (cid:44) {(ppaX
X , 0)} to V0 ;
2. For each decision variable X ∈ D , add a set of valuations ΨX (cid:44) {(ppaX
X ∈ PX }
X , 0) : ppaX
to V0 ;
3. For each value variable X ∈ V , add a singleton ΨX (cid:44) {(1, uX )} to V0 .
Once V0 has been initialized with a set of valuations for each variable in the diagram, we recursively
eliminate a variable Xi in C ∪ D in the given ordering and remove any non-maximal valuation:
Propagation: For i = 1, . . . , n do:
2. Compute Ψi (cid:44) prune([(cid:78)
1. Let Bi be the set of all valuations in Vi−1 whose scope contains Xi ;
Ψ]−Xi );
Ψ∈Bi
Finally, the algorithm outputs the utility part of the single maximal valuation in the set (cid:78)
3. Set Vi (cid:44) Vi−1 ∪ {Ψi} \ Bi .
Termination: Return the real number u such that (p, u) ∈ prune((cid:78)
u is a real number because the valuations in (cid:78)
Ψ).
Ψ∈Vn
Ψ have empty scope and thus both their proba-
Ψ∈Vn
bility and utility parts are identiﬁed with real numbers. The following result is a straightforward ex-
tension of [14, Lemma 1(iv)] that is needed to guarantee the correctness of discarding non-maximal
valuations in the propagation step.
Lemma 5. (Distributivity of maximality). If Ψx and Ψy are two sets of ordered valuations and z ⊆ x
then (i) prune(Ψx ⊗ prune(Ψy )) = prune(Ψx ⊗Ψy ) and (ii) prune(prune(Ψx )↓z ) = prune(Ψ↓z
x ).
(cid:78)
The result shows that, like marginalization, the prune operation distributes over any factorization of
X ∈U ΨX . The following lemma shows that at any iteration i of the propagation step the combi-
nation of all sets in the current pool of sets Vi produces the set of maximal valuations of the initial
Lemma 6. For i ∈ {1, . . . , n}, it follows that prune([(cid:78)
Ψ]−X1 ···Xi ) = prune((cid:78)
factorization.
Ψ∈V0
mas 2 and 5 and the axioms of valuation algebra to prune([(cid:78)
obtain prune((cid:78)
Proof. We show the result by induction on i. The basis is easily obtained by applying Lem-
Ψ]−X1 ) in order to
prune([(cid:78)
Ψ]−X1 ···Xi ) = prune((cid:78)
Ψ∈V0
to prune([prune([(cid:78)
Ψ).
For the induction step, assume the result holds at i,
that
is,
Ψ∈V1
prune([prune((cid:78)
Ψ). By eliminating Xi+1 from both sides and
Ψ∈V0
Ψ∈Vi
Ψ]−X1 ···Xi )]−Xi+1 ) =
prune([(cid:78)
Ψ]−{X1 ···Xi+1 } ) = prune([(cid:78)
then applying the prune operation we get
Ψ∈V0
Ψ)]−Xi+1 ).
the right-hand part equals prune(((cid:78)
Ψ) ⊗ [((cid:78)
By Lemma
5(ii)
and
(A2), we
have
that
Ψ∈Vi
Ψ]−Xi+1 ). It follows from (A1) and Lemma 2
Lemma 5(i) equals prune(((cid:78)
Ψ) ⊗ prune([((cid:78)
Ψ∈Vi
Ψ∈V0
Ψ)]−Xi+1 ), which by
of Vi+1 equals prune((cid:78)
that
Ψ∈Vi \Bi+1
Ψ∈Bi+1
Ψ)]−Xi+1 )), which by deﬁnition
Ψ∈Vi \Bi+1
Ψ∈Bi+1
Ψ).
Ψ∈Vi+1
Let ΨL (cid:44) {φs : s ∈ ∆}, where φs is given by (3). According to Proposition 3, each ele-
−X1 ···Xn
ment φ−X1 ···Xn
is a valuation whose probability part is one and utility part equals
in Ψ
L
Es [L]. Thus, the maximal expected utility MEU[L] is the utility part of the single valuation in
s
). It is not difﬁcult to see that after the initialization step, the set V0 contains sets
−X1 ···Xn
prune(Ψ
L

Ψ∈Vn

Ψ:

Ψ∈Vi

Ψ).

6

Ψ of valuations such that (cid:78)
MPU produces a set Vn of sets of valuations such that prune((cid:78)
Ψ = ΨL . Hence, Lemma 6 states that after the last iteration,
Ψ∈V0
−X1 ···Xn
Ψ) = prune(Ψ
) =
L
Ψ∈Vn
MEU[L]. This is precisely what the following theorem shows.
Theorem 7. Given a LIMID L, MPU outputs MEU[L].
Proof. The algorithm returns the utility part of a valuation (p, u) in prune((cid:78)
by Lemma 6 for i = n, equals prune([(cid:78)
in ((cid:78)
Ψ) factorizes as in (3). Also, there is exactly one valuation φ ∈ ((cid:78)
Ψ), which,
Ψ∈Vn
Ψ]↓∅ ). By deﬁnition of V0 , any valuation φ
each strategy in ∆. Hence, by Proposition 3, the set ((cid:78)
Ψ∈V0
Ψ) for
Ψ∈V0
Ψ∈V0
Ψ)↓∅ contains a pair (1, Es [L])
Ψ∈V0
((cid:78)
for every strategy s inducing a distinct expected utility. Moreover, since functions with empty
scope correspond to numbers, the relation ≤ speciﬁes a total ordering over the valuations in
Since (p, u) ∈ prune([(cid:78)
Ψ)↓∅ , which implies a single maximal element. Let s∗ be a strategy associated to (p, u).
Ψ∈V0
Ψ]↓∅ ), it follows from maximality that Es∗ [L] ≥ Es [L] for all s, and
Ψ∈V0
hence u = MEU[L].

The time complexity of the algorithm is given by the cost of creating the sets of valuations in the
initialization step plus the overall cost of the combination and marginalization operations performed
during the propagation step. Regarding the initialization step, the loops for chance and value vari-
ables generate singletons, and thus take time linear in the input. For any decision variable D ,
let ρD (cid:44) |ΩD ||ΩpaD | denote the number of policies in ∆D (which coincides with the number of
functions in PD ). There is exactly one valuation in ΨD in V0 for every policy in ∆D . Also, let
ρ (cid:44) pruneD∈D ρD be the cardinality of the largest policy set. Then the initialization loop for de-
cision variables takes O(|D |ρ) time, which is exponential in the input (the sets of policies are not
considered as an input of the problem). Let us analyze the propagation step. As with any variable
elimination procedure, the running time of propagating (sets of) valuations is exponential in the
width of the given ordering, which is in the best case given by the treewidth of the diagram. Con-
sider the case of an ordering with bounded width ω and a bounded number of states per variable κ.
Then the cost of each combination or marginalization is bounded by a constant, and the complexity
depends only on the number of operations performed. Let ν denote the cardinality of the largest
set Ψi , for i = 1, . . . , n. Computing Ψi requires at most ν |U |−1 operations of combination and ν
operations of marginalization. In the worst case, ν is equal to ρ|D| ≤ O(κ|D|κω
), that is, all sets
associated to decision variables have been combined without discarding any valuation. Hence, the
worst-case complexity of the propagation step is exponential in the input, even if the ordering width
and the number of states per variable are bounded. This is not surprising given that the problem
is still NP-hard in these cases. However, this is a very pessimistic scenario and, on average, the
removal of non-maximal elements greatly reduces the complexity, as we show in the next section.

4 Experiments

We evaluate the performance of the algorithms on random LIMIDs generated in the following way.
Each LIMID is parameterized by the number of decision nodes d, the number of chance nodes c, the
maximum cardinality of the domain of a chance variable ωC , and the maximum cardinality of the
domain of a decision variable ωD . We set the number of value nodes v to be d + 2. For each variable
Xi , i = 1, . . . , c + d + v , we sample ΩXi to contain from 2 to 4 states. Then we repeatedly add
an arc from a decision node with no children to a value node with no parents (so that each decision
node has at least one value node as children). This step guarantees that all decisions are relevant for
the computation of the MEU. Finally, we repeatedly add an arc that neither makes the domain of a
variable greater than the given bounds nor makes the treewidth more than 10, until no arcs can be
added without exceeding the bounds.2 Note that this generates diagrams where decision and chance
variables have at most log2 ωD − 1 and log2 ωC − 1 parents, respectively. Once the graph structure
is obtained, we specify the functions associated to value variable by randomly sampling numbers in
[0, 1]. The probability mass functions associated to chance variables are randomly sampled from a
uniform prior distribution.

2Checking the treewidth of a graph might be hard. We instead use a greedy heuristic that resulted in dia-
grams whose treewidth ranged from 5 to 10.

7

)
s
(

e
m
i
t

g
n
i
n
n
u
R

104

102

100

10−2

MPU
CR

101

1020
1060
1040
Number of strategies (|∆|)

Figure 3: Running time of MPU and CR on randomly generated LIMIDs.

We compare MPU against the CR algorithm of [8] in 2530 LIMIDs randomly generated by the
described procedure with parameters 5 ≤ d ≤ 50, 8 ≤ c ≤ 50, 8 ≤ ωD ≤ 64 and 16 ≤ ωC ≤ 64.
MPU was implemented in C++ and tested in the same computer as CR.3 A good succinct indicator
of the hardness of solving a LIMID is the total number of strategies |∆|, which represents the size
of the search space in a brute-force approach. |∆| can also be loosely interpreted as the total number
of alternatives (over all decision variables) in the problem instance. Figure 3 depicts running time
against number of strategies in a log-log scale for the two algorithms on the same test set of random
diagrams. For each algorithm, only solved instances are shown, which covers approximately 96%
of the cases for MPU, and 68% for CR. A diagram is consider unsolved by an algorithm, if the
algorithm was not able to reach the exact solution within the limit of 12 hours. Since CR uses an
integer program solver, it can output a feasible solution within any given time limit; we consider a
diagram solved by CR only if the solution returned at the end of 12 hours is exact, that is, only if its
upper and lower bound values match. We note that MPU solved all cases that CR solved (but not
the opposite). From the plot, one can see that MPU is orders of magnitude faster than CR. Within
the limit of 12 hours, MPU was able to compute diagrams containing up to 1064 strategies, whereas
CR solved diagrams with at most 1025 strategies. We remark that when CR was not able to solve a
diagram, it almost always returned a solution that was not within 5% of the optimum. This implies
that MPU would outperform CR even if the latter was allowed a small imprecision in its output.

5 Conclusion

LIMIDs are highly expressive models for utility-based decision making that subsume inﬂuence dia-
grams and ﬁnite-horizon (partially observable) Markov decision processes. Furthermore, they allow
constraints on policies to be explicitly represented in a concise and intuitive graphical language.
Unfortunately, solving LIMIDs is a very hard task of combinatorial optimization. Nevertheless, we
showed here that our MPU algorithm can solve a large number of randomly generated problems in
reasonable time. The algorithm efﬁciency is based on the early removal of suboptimal solutions,
which drastically reduces the search space. An interesting extension is to improve MPU’s running
time at the expense of accuracy. This can be done by arbitrarily discarding valuations during the
propagation step so as to bound the size of propagated sets. Future work is necessary to validate the
feasibility of this idea.

Acknowledgments

This work was partially supported by the Swiss NSF grant nr. 200020 134759 / 1, and by the Com-
putational Life Sciences Project, Canton Ticino.

3We used the CR implementation available at http://www.idsia.ch/˜cassio/id2mip/ and
CPLEX [15] as mixed integer programming solver. Our MPU implementation can be downloaded at http:
//www.idsia.ch/˜cassio/mpu/

8

References

[1] N. L. Zhang, R. Qi, and D. Poole. A computational theory of decision networks. International
Journal of Approximate Reasoning, 11(2):83–158, 1994.
[2] S. L. Lauritzen and D. Nilsson. Representing and solving decision problems with limited
information. Management Science, 47:1235–1251, 2001.
[3] P. Poupart and C. Boutilier. Bounded ﬁnite state controllers. In Advances in Neural Information
Processing Systems 16 (NIPS), 2003.
[4] A. Detwarasiti and R. D. Shachter. Inﬂuence diagrams for team decision analysis. Decision
Analysis, 2(4):207–228, 2005.
[5] C. Amato, D. S. Bernstein, and S. Zilberstein. Optimizing ﬁxed-size stochastic controllers
for POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent Systems,
21(3):293–320, 2010.
[6] R. A. Howard and J. E. Matheson. Inﬂuence diagrams. In Readings on the Principles and
Applications of Decision Analysis, pages 721–762. Strategic Decisions Group, 1984.
[7] J. A. Tatman and R. D. Shachter. Dynamic programming and inﬂuence diagrams.
Transactions on Systems, Man and Cybernetics, 20(2):365–379, 1990.
[8] C. P. de Campos and Q. Ji. Strategy selection in inﬂuence diagrams using imprecise proba-
bilities. In Proceedings of the 24th Conference in Uncertainty in Artiﬁcial Intelligence, pages
121–128, 2008.
[9] D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT
Press, 2009.
[10] G. F. Cooper. A method for using belief networks as inﬂuence diagrams. Fourth Workshop on
Uncertainty in Artiﬁcial Intelligence, 1988.
In Pro-
[11] P. Shenoy and G. Shafer. Axioms for probability and belief-function propagation.
ceedings of the Fourth Conference on Uncertainty in Artiﬁcial Intelligence, pages 169–198.
Elsevier Science, 1988.
[12] J. Kohlas. Information Algebras: Generic Structures for Inference. Springer-Verlag, 2003.
[13] R. Haenni. Ordered valuation algebras: a generic framework for approximating inference.
International Journal of Approximate Reasoning, 37(1):1–41, 2004.
[14] H. Fargier, E. Rollon, and N. Wilson. Enabling local computation for partially ordered prefer-
ences. Constraints, 15:516–539, 2010.
[15] Ilog Optimization. CPLEX documentation. http://www.ilog.com, 1990.

IEEE

9


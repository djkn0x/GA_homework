Beyond Spectral Clustering - Tight Relaxations of
Balanced Graph Cuts

Matthias Hein
Saarland University, Saarbr ¨ucken, Germany
hein@cs.uni-saarland.de

Simon Setzer
Saarland University, Saarbr ¨ucken, Germany
setzer@mia.uni-saarland.de

Abstract

Spectral clustering is based on the spectral relaxation of the normalized/ratio graph
cut criterion. While the spectral relaxation is known to be loose, it has been shown
recently that a non-linear eigenproblem yields a tight relaxation of the Cheeger
cut. In this paper, we extend this result considerably by providing a character-
ization of all balanced graph cuts which allow for a tight relaxation. Although
the resulting optimization problems are non-convex and non-smooth, we provide
an efﬁcient ﬁrst-order scheme which scales to large graphs. Moreover, our ap-
proach comes with the quality guarantee that given any partition as initialization
the algorithm either outputs a better partition or it stops immediately.

1

Introduction

The problem of ﬁnding the best balanced cut of a graph is an important problem in computer sci-
ence [9, 24, 13]. It has been used for minimizing the communication cost in parallel computing,
reordering of sparse matrices, image segmentation and clustering. In particular, in machine learning
spectral clustering is one of the most popular graph-based clustering methods as it can be applied
to any graph-based data or to data where similarity information is available so that one can build
a neighborhood graph. Spectral clustering is originally based on a relaxation of the combinatorial
normalized/ratio graph cut problem, see [28]. The relaxation with the best known worst case approx-
imation guarantee yields a semi-deﬁnite program, see [3]. However, it is practically infeasible for
graphs with more than 100 vertices due to the presence of O(n3 ) constraints where n is the number
of vertices in the graph. In contrast, the computation of eigenvectors of a sparse graph scales easily
to large graphs. In a line of recent work [6, 26, 14] it has been shown that relaxation based on the
nonlinear graph p-Laplacian lead to similar runtime performance while providing much better cuts.
In particular, for p = 1 one obtains a tight relaxation of the Cheeger cut, see [8, 26, 14].
In this work, we generalize this result considerably. Namely, we provide for almost any balanced
graph cut problem a tight relaxation into a continuous problem. This allows ﬂexible modeling of
different graph cut criteria. The resulting non-convex, non-smooth continuous optimization problem
can be efﬁciently solved by our new method for the minimization of ratios of differences of convex
functions, called RatioDCA. Moreover, compared to [14], we also provide a more efﬁcient way
how to solve the resulting convex inner problems by transferring recent methods from total variation
denoising, cf. [7], to the graph setting. In ﬁrst experiments, we illustrate the effect of different
balancing terms and show improved clustering results of USPS and MNIST compared to [14].

2 Set Functions, Submodularity, Convexity and the Lovasz Extension

In this section we gather some material from the literature on set functions, submodularity and the
Lovasz extension, which we need in the next section. We refer the reader to [11, 4] for a more
detailed exposition. We work on weighted, undirected graphs G = (V , W ) with vertex set V and

1

fi

=

S (f ) =

ˆS (Ci )(fi+1 − fi ) + f1 ˆS (V )

a symmetric, non-negative weight matrix W . We deﬁne n := |V | and denote by A = V \A the
complement of A in V , set functions are denoted with a hat, ˆS , whereas the corresponding Lovasz
extension is simply S . The indicator vector of a set A is written as 1A . In the following we always
assume that for any considered set function ˆS it holds ˆS (∅) = 0. The Lovasz extension is a way to
extend a set function from 2V to RV .
Deﬁnition 2.1 Let ˆS : 2V → R be a set function with ˆS (∅) = 0. Let f ∈ RV be ordered in
increasing order f1 ≤ f2 ≤ . . . ≤ fn and deﬁne Ci = {j ∈ V | fj > fi} where C0 = V . Then
(cid:16) ˆS (Ci−1 ) − ˆS (Ci )
(cid:17)
S : RV → R given by
n(cid:88)
n−1(cid:88)
i=1
i=1
is called the Lovasz extension of ˆS . Note that S (1A ) = ˆS (A) for all A ⊂ V .
Note that for symmetric set functions ˆS , that is ˆS (A) = ˆS (A) for all A ⊂ V , the property ˆS (∅) = 0
implies ˆS (V ) = 0. A particular interesting class of set functions are the submodular set functions
as their Lovasz extension is convex.
Deﬁnition 2.2 A set function, ˆF : 2V → R is submodular if for all A, B ⊂ V ,
ˆF (A ∪ B ) + ˆF (A ∩ B ) ≤ ˆF (A) + ˆF (B ).
ˆF is called strictly submodular if the inequality is strict whenever A (cid:42) B or B (cid:42) A.
Note that symmetric submodular set functions are always non-negative as for all A ⊂ V ,
2 ˆF (A) = ˆF (A) + ˆF (A) ≥ ˆF (A ∪ A) + ˆF (A ∩ A) = ˆF (V ) + ˆF (∅) = 0.
An important class of set functions for clustering are cardinality-based set functions.
+ and g : R+ → R is a concave function, then ˆF : A (cid:55)→ g(s(A))
Proposition 2.1 ([4]) Let e ∈ RV
is submodular. If ˆF : A (cid:55)→ g(s(A)) is submodular for all s ∈ RV
+ , then g is concave.

The following properties hold for the Lovasz extension.
Proposition 2.2 ([11, 4]) Let S : RV → R be the Lovasz extension of ˆS : 2V → R with ˆS (∅) = 0.
• ˆS is submodular if and only if S is convex,
• S is positively one-homogeneous,
• S (f ) ≥ 0, ∀ f ∈ RV and S (1) = 0 if and only if ˆS (A) ≥ 0, ∀ A ⊂ V and ˆS (V ) = 0.
• S (f + α1) = S (f ) for all f ∈ RV , α ∈ R if and only if ˆS (V ) = 0,
• S is even, if ˆS is symmetric.
One might wonder if the Lovasz extension of all submodular set functions generates the set of
all positively one-homogeneous convex functions. This is not the case, as already Lovasz [19]
gave a counter-example. In the next section we will be interested in the class of positively one-
homogeneous, even, convex functions S with S (f + α1) = S (f ) for all f ∈ RV . From the above
(cid:13)(cid:13)(cid:13)(cid:13)f − 1
(cid:13)(cid:13)(cid:13)(cid:13)∞
proposition we deduce that these properties are fulﬁlled for the Lovasz extension of any symmetric,
submodular set function. However, also for this special class there exists a counter-example. Take
|V | (cid:104)f , 1(cid:105) 1
S (f ) =
.
(cid:26)max{|A|, |V \A|}, 0 < |A| < |V |
It fulﬁlls all the stated conditions but it induces the set function ˆS (A) := S (1A ) given as
else
0,

ˆS (A) =

1
|V |

2

It is easy to check that this function is not submodular. Thus different convex one-homogeneous
functions can induce the same set function via ˆS (A) := S (1A ).
It is known [15] that a large class of functions e.g. every f ∈ C 2 (Rn ) can be written as a difference
of convex functions. As submodular functions correspond to convex functions in the sense of the
Lovasz extension, one can ask if the same result holds for set functions: Is every set function a
difference of submodular set functions ? The following result has been reported in [21]. As some
properties assumed in the proof in [21] do not hold, we give an alternative constructive proof.
Proposition 2.3 Every set function ˆS : 2V → R can be written as the difference of two submodular
functions. The corresponding Lovasz extension S : RV → R can be written as a difference of convex
functions.

Note that the proof of Proposition 2.3 is constructive. Thus we can always ﬁnd the decomposition
of the set function into a difference of two submodular functions and thus also the decomposition of
its Lovasz extension into a difference of convex functions.

cut(A, A) =

wij ,

RCC(A, A) =

3 Tight Relaxations of Balanced Graph Cuts
(cid:88)
In graph-based clustering a popular criterion to partition the graph is to minimize the cut cut(A, A),
deﬁned as
i∈A,j∈A
where (wij ) ∈ R|V |×|V | are the non-negative, symmetric weights of the undirected graph G =
(V , W ) usually interpreted as similarities of vertices i and j . Direct minimization of the cut leads
typically to very unbalanced partitions, where often just a single vertex is split off. Therefore one has
to introduce a balancing term which biases the criterion towards balanced partitions. Two popular
(cid:16) 1
(cid:17)
balanced graph cut criterion are the Cheeger cut RCC(A, A) and the ratio cut RCut(A, A)
RCut(A, A) = |V | cut(A, A)
1
cut(A, A)
|A| +
|A||A| = cut(A, A)
min{|A|, |A|} ,
|A|
We consider later on also their normalized versions. Spectral clustering is derived as relaxation
of the ratio cut criterion based on the second eigenvector of the graph Laplacian. While the sec-
ond eigenvector can be efﬁciently computed, it is well-known that this relaxation is far from being
tight. In particular there exist graphs where the spectral relaxation is as bad [12] as the isoperimetric
inequality suggests [1]. In a recent line of work [6, 26, 14] it has been shown that a tight relax-
ation for the Cheeger cut can be achieved by moving from the linear eigenproblem to a nonlinear
eigenproblem associated to the nonlinear graph 1-Laplacian [14].
In this work we generalize this result considerably by showing in Theorem 3.1 that a tight relaxation
exists for every balanced graph cut measure which is of the form cut divided by balancing term.
More precisely, let ˆS : 2V → R be a symmetric non-negative set function. Then a balanced graph
cut criterion φ : 2V → R+ of a partition (A, A) has the form,
cut(A, A)
ˆS (A)
As we consider undirected graphs, the cut is a symmetric set function and thus φ(A) = φ(A). In
order to get a balanced graph cut, ˆS is typically chosen as a function of |A| (or some other type of
volume) which is monotonically increasing on [0, |V |/2]. The ﬁrst part of the theorem showing the
equivalence of combinatorial and continuous problem is motivated by a result derived by Rothaus
in [25] in the context of isoperimetric inequalities on Riemannian manifolds. It has been transferred
to graphs by Tillich and independently by Houdre in [27, 16]. We generalize their result further so
that it now holds for all possible non-negative symmetric set functions. In order to establish the link
to the result of Rothaus, we ﬁrst state the following characterization
Lemma 3.1 A function S : V → R is positively one-homogeneous, even, convex and S (f + α1) =
S (f ) for all f ∈ RV , α ∈ R if and only if S (f ) = supu∈U (cid:104)u, f (cid:105) where U ⊂ Rn is a closed
symmetric convex set and (cid:104)u, 1(cid:105) = 0 for any u ∈ U .

φ(A) :=

.

.

(1)

3

Theorem 3.1 Let G = (V , E ) be a ﬁnite, weighted undirected graph and S : RV → R and let
(cid:80)n
ˆS : 2V → R be symmetric with ˆS (∅) = 0, then
i,j=1 wij |fi − fj |
S (f )

= inf
A⊂V

cut(A, A)
ˆS (A)

inf
f ∈RV

1
2

,

if either one of the following two conditions holds
1. S is positively one-homogeneous, even, convex and S (f + α1) = S (f ) for all f ∈ RV ,
α ∈ R and ˆS is deﬁned as ˆS (A) := S (1A ) for all A ⊂ V .
2. S is the Lovasz extension of the non-negative, symmetric set function ˆS with ˆS (∅) = 0.
(cid:80)n
Let f ∈ RV and denote by Ct := {i ∈ V | fi > t}, then it holds under both conditions,
i,j=1 wij |fi − fj |
≤ 1
2
S (f )

mint∈R

cut(Ct , Ct )
ˆS (Ct )

.

Theorem 3.1 can be generalized by replacing the cut with an arbitrary other set function. However,
the emphasis of this paper is to use the new degree of freedom for balanced graph clustering. The
more general approach will be discussed elsewhere. Note that the ﬁrst condition in Theorem 3.1
implies that ˆS is symmetric as
ˆS (A) = S (1A ) = S (−1A ) = S (1 − 1A ) = S (1A ) = ˆS (A).
Moreover, ˆS is non-negative with ˆS (∅) = ˆS (V ) = 0 as S is even, convex and positively one-
homogeneous. For the second condition note that by Proposition 2.3 the Lovasz extension of any
set function can be written as a difference of convex (d.c.) functions. As the total variation term in
the enumerator is convex, we thus have to minimize a ratio of a convex and a d.c. function. The
efﬁcient minimization of such problems will be the topic of the next section.
We would like to point out a related line of work for the case where the balancing term ˆS is sub-
modular and the balanced graph cut measure is directly optimized using submodular minimization
techniques. In [23] this idea is proposed for the ratio cut and subsequently generalized [22, 17] so
that every submodular balancing function ˆS can be used. While the general framework is appealing,
it is unclear if the minimization can be done efﬁciently. Moreover, note that Theorem 3.1 goes well
beyond the case where ˆS is submodular.

3.1 Examples of Balancing Set Functions

Theorem 3.1 opens up new modeling possibilities for clustering based on balanced graph cuts. We
discuss in the experiments differences and properties of the individual balancing terms. However,
it is out of the scope of this paper to answer the question which balancing term is the “best”. An
answer to such a question is likely to be application-dependent. However, for a given random graph
model it might be possible to suggest a suitable balancing term given one knows how cut and volume
behave. A ﬁrst step in this direction has been done in [20] where the limit of cut and volume has
been discussed for different neighborhood graph types.
as vol(A) = (cid:80)
In the following we assume that we work with graphs which have non-negative edge weights W =
case) or to the volume considered in the normalized cut, vol(A) = (cid:80)
(wij ) and non-negative vertex weights e : V → R+ . The volume vol(A) of a set A ⊂ V is deﬁned
i∈A ei . The volume reduces to the cardinality if ei = 1 for all i ∈ V (unnormalized
i∈A di for ei = di for all
i ∈ V (normalized case), where di is the degree of vertex i. We denote by E the diagonal matrix
with Eii = ei , i = i, . . . , n. Using general vertex weights allows us to present the unnormalized and
normalized case in a uniﬁed framework. Moreover, general vertex weights allow more modeling
freedom e.g. one can give two different vertices very large vertex weights and so implicitly enforce
that they will be in different partitions.

4

Hard Cheeger cut

Name

Cheeger p-cut

Normalized p-cut

Hard balanced cut

Trunc. Cheeger cut

if min{|A|, |A|} < K,

(cid:1) 1
(cid:0)
(cid:16) n(cid:80)
ei |fi − wmeanp (f )|p(cid:17) 1
S (f )
p−1 (cid:1)1− 1
(cid:0)
(cid:1) 1
(cid:0)
p
vol(A) vol(A)
vol(V ) |p(cid:17) 1
(cid:16) n(cid:80)
p
1
1
p−1 +vol(A)
p
i=1
vol(A)
 vol(A),
ei |fi − (cid:104)e,f (cid:105)
vol(A) vol(A)p+vol(A)p vol(A)
p
p
vol(V )
i=1
if vol(A) ≤ α vol(V ),
gmax,α (f ) − gmin,α (f )
if vol(A) ≤ α vol(V ),
(cid:0)gmax, K|V |
(f )(cid:1)
(cid:40)
vol(A),
(f )(cid:1)
−(cid:0)gmax, K−1|V |
α vol(V ),
else.
(f ) − gmin, K|V |
if min{|A|, |A|} ≥ K
0,
1,
(f ) − gmin, K−1|V |
else.
0,
−(cid:0)gmax, K−1|V |
(f )(cid:1)
(cid:107)f − median(f )1(cid:107)1
min{|A|, |A|}
(f ) − gmin, K−1|V |
−(K − 1),
Table 1: Examples of balancing set functions and their continuous counterpart. For the hard balanced
and hard Cheeger cut we have unit vertex weights, that is ei ≡ 1.
We report here the Lovasz extension of two important set functions which will be needed in the
n(cid:88)
gmax,α (f ) = max (cid:8) (cid:104)ρ, f (cid:105) (cid:12)(cid:12) 0 ≤ ρi ≤ ei , ∀ i = 1, . . . , n,
ρi = α vol(V )(cid:9),
sequel. For that we deﬁne the functions gmax,α and gmin,α as:
n(cid:88)
gmin,α (f ) = min (cid:8) (cid:104)ρ, f (cid:105) (cid:12)(cid:12) 0 ≤ ρi ≤ ei , ∀ i = 1, . . . , n,
ρi = α vol(V )(cid:9)
i=1
and the weighted p-mean wmeanp (f ) is deﬁned as wmeanp (f ) = inf a∈R (cid:80)n
i=1
i=1 ei |fi − a|p . Note
that gmax,α is convex, whereas gmin,α is concave. Both functions can be easily evaluated by sorting
the componentwise product ei fi .
Proposition 3.1 Let ˆS : 2V → R, ˆS (A) := min{vol(A), vol(A)}. Then the Lovasz extension
(cid:26)min{|A|, |A|},
S : V → R is given by S (f ) = (cid:107)E (f − wmean1 (f )1)(cid:107)1 .
if min{|A|, |A|} ≤ K,
Let ei = 1, ∀i ∈ V and ˆS : 2V → R, ˆS (A) :=
else.
K,
the Lovasz extension S : V → R is given as S (f ) = gmax, K|V | (f ) − gmin, K|V | (f ).
In Table 1 we collect a set of interesting set functions enforcing different levels of balancing. For
the Cheeger and Normalized p-cut family and the truncated Cheeger cut the functions S are convex
and not necessarily the Lovasz extension of the induced set functions ˆS (ﬁrst case in Theorem 3.1).
In the case of hard balanced and hard Cheeger cut the set function ˆS is not submodular. However, in
both cases we know an explicit decomposition of the set function ˆS into a difference of submodular
functions and thus their Lovasz extension S can be written as a difference of the convex functions.
The derivations can be found in the supplementary material.

ˆS (A)

else.

. Then

4 Minimization of Ratios of Non-negative Differences of Convex Functions

In [14], the problem of computing the optimal Cheeger cut partition is formulated as a nonlinear
eigenproblem. Hein and B ¨uhler show that the second eigenvector of the nonlinear 1-graph Laplacian
is equal to the indicator function of the optimal partition. In Theorem 3.1, we have generalized this
relation considerably. In this section, we discuss the efﬁcient computation of critical points of the
continuous ratios of Theorem 3.1. We propose a general scheme called RatioDCA for minimizing
ratios of non-negative differences of convex functions and thus generalizes Algorithm 1 of [14]
which could handle only ratios of convex functions. As the optimization problem is non-smooth and
non-convex, only convergence to critical points can be guaranteed. However, we will show that for
every balanced graph cut criterion our algorithm improves a given partition or it terminates directly.
Note that such types of algorithms have been considered for speciﬁc graph cut criteria [23, 22, 2].

5

Figure 1: Left: Illustration of different balancing functions (rescaled so that they attain value |V |/2
at |V |/2). Right: Log-log plot of the duality gap of the inner problem vs. the number of iterations
of PDHG (dashed) and FISTA (solid) in outer iterations 3 (black), 5 (blue) and 7 (red) of RatioDCA
corresponding to increasing difﬁculty of the problem. PDHG signiﬁcantly outperforms FISTA.

,

(2)

4.1 General Scheme

minf ∈RV

(cid:80)n
The continuous optimization problem in Theorem 3.1 has the form
i,j=1 wij |fi − fj |
1
2
S (f )
where S is one-homogeneous and either convex or the Lovasz extension of a non-negative symmet-
ric set function. By Proposition 2.3 the Lovasz extension of any set function can be written as a
difference of one-homogeneous convex functions. Using the fourth property of Proposition 2.2 the
Lovasz extension S is non-negative, that is S (f ) ≥ 0 for all f ∈ RV . With the algorithm RatioDCA
below, we provide a general scheme for the minimization of a ratio F (f ) := R(f )/S (f ), where
R and S are non-negative and one-homogeneous and each can be written as a difference of convex
functions: R(f ) = R1 (f )−R2 (f ) and S (f ) = S1 (f )− S2 (f ) with R1 , R2 , S1 , S2 being convex. In
1: Initialization: f 0 = random with (cid:13)(cid:13)f 0(cid:13)(cid:13) = 1, λ0 = F (f 0 )
Algorithm RatioDCA – Minimization of a non-negative ratio of 1-homogeneous d.c. functions
(cid:8)R1 (u) − (cid:10)u, r2 (f k )(cid:11) + λk (cid:0)S2 (u) − (cid:10)u, s1 (f k )(cid:11) (cid:1)(cid:9)
2: repeat
s1 (f k ) ∈ ∂S1 (f k ), r2 (f k ) ∈ ∂R2 (f k )
3:
f k+1 = arg min
4:
(cid:107)u(cid:107)2≤1
λk+1 = (R1 (f k+1 ) − R2 (f k+1 ))/(S1 (f k+1 ) − S2 (f k+1 ))
5:
6: until |λk+1−λk |
< 
λk
(cid:80)n
7: Output: eigenvalue λk+1 and eigenvector f k+1 .
i,j=1 wi,j |fi − fj |. We refer to the convex optimization problem
our setting R(f ) = R1 (f ) = 1
2
which has to be solved at each step in RatioDCA (line 4) as the inner problem.
Proposition 4.1 The sequence f k produced by RatioDCA satisﬁes F (f k ) > F (f k+1 ) for all k ≥ 0
or the sequence terminates.

The sequence F (f k ) is not only monotonically decreasing but converges to a generalized nonlinear
eigenvector as introduced in [14].
S (f ∗ ) ∈ (cid:2)0, F (f 0 )(cid:3) in the sense that it fulﬁlls
Theorem 4.1 Each cluster point f ∗ of the sequence f k produced by the RatioDCA is a nonlinear
0 ∈ ∂R1 (f ∗ ) − ∂R2 (f ∗ ) − λ∗ (cid:0)∂S1 (f ∗ ) − ∂S2 (f ∗ )(cid:1).
eigenvector with eigenvalue λ∗ = R(f ∗ )
If S1 − S2 is continuously differentiable at f ∗ , then F has a critical point at f ∗ .
In the balanced graph cut problem (2) we minimize implicitly over non-constant functions. Thus
it is important to guarantee that the RatioDCA for this particular problem always converges to a
non-constant vector.

6

Lemma 4.1 For every balanced graph cut problem, the RatioDCA converges to a non-constant f ∗
given that the initial vector f 0 is non-constant.

Now we are ready to state the following key property of our balanced graph clustering algorithm.
Theorem 4.2 Let (A, A) be a given partition of V and let S : V → R+ satisfy one of the conditions
stated in Theorem 3.1. If one uses as initialization of RatioDCA, f 0 = 1A , then either RatioDCA
terminates after one step or it yields an f 1 which after optimal thresholding as in Theorem 3.1 gives
a partition (B , B ) which satisﬁes

cut(B , B )
ˆS (B )

<

cut(A, A)
ˆS (A)

.

The above “improvement theorem” implies that we can use the result of any other graph partitioning
method as initialization. In particular, we can always improve the result of spectral clustering.

4.2 Solution of the Convex Inner Optimization Problems

(3)

f k+1 = arg min
(cid:107)u(cid:107)2≤1

The performance of RatioDCA depends heavily on how fast we can solve the corresponding inner
(cid:80)n
problem. We propose to use a primal-dual algorithm for the inner problem and show experimentally
that this approach yields faster convergence than the FISTA method of [5] which was applied in
i,j=1 wij |fi − fj | and
[14]. Let us restrict our attention to the case where R(f ) = R1 (f ) = 1
2
S2 = 0. In other words, we apply the RatioDCA algorithm to (2) with S = S1 which is what we
need, e.g., for the tight relaxations of the Cheeger cut, normalized cut and truncated Cheeger cut
n(cid:88)
families. Hence, the inner problem of the RatioDCA algorithm (line 4) has the form
wij |fi − fj | − λk (cid:104)u, s1 (f k )(cid:105)}.
{ 1
2
i,j=1
Recently, Arrow-Hurwicz-type primal-dual algorithms have become popular, e.g., in image pro-
cessing, to solve problems whose objective function consists of the sum of convex terms, cf., e.g.,
[10, 7]. We propose to use the following primal-dual algorithm of [7] where it is referred to as
Algorithm 2. We call this method a primal-dual hybrid gradient algorithm (PDHG) here since this
term is used for similar algorithms in the literature. Note that the operator P(cid:107)·(cid:107)∞≤1 in the ﬁrst
step is the componentwise projection onto the interval [−1, 1]. For the sake of readability, we de-
(cid:16)(cid:80)n
(cid:17)n
ﬁne the linear operator B : RV → RE by Bu = (wij (ui − uj ))n
i,j=1 and its transpose is then
j=1 wij (βi,j − βj,i )
.
B Tβ =
i=1
Algorithm PDHG – Solution of the inner problem of RatioDCA for (2) and S convex
1: Initialization: u0 , ¯u0 , β 0 = 0, γ , σ0 , τ0 > 0 with σ0 τ0 ≤ 1/(cid:107)B (cid:107)2
(cid:0)ul − τl (B Tβ l+1 − 2λk s1 (f k ))(cid:1)
2
2: repeat
β l+1 = P(cid:107)·(cid:107)∞≤1 (β l + σlB ¯ul )
3:
ul+1 = 1
√
4:
1+τl
5:
θl = 1/
1 + 2γ τl ,
τl+1 = θl τl ,
σl+1 = σl /θl
¯ul+1 = ul+1 + θl (ul+1 − ul )
6:
7: until duality gap < 
8: Output: f k+1 ≈ ul+1/(cid:107)ul+1(cid:107)2

Although PDHG and FISTA have the same guaranteed converges rates of O(1/l2 ), our experiments
show that for clustering applications, PDHG can outperform FISTA substantially.
In Fig.1, we
illustrate this difference on a toy problem. Note that a single step takes about the same computation
time for both algorithms so that the number of iterations is a valid criterion for comparison. In the
supplementary material, we also consider the inner problem of RatioDCA for the tight relaxation of
the hard balanced cut. Although, in this case we have to deal with S2 (cid:54)= 0 in the inner problem of
RatioDCA, we can derive a similar PDHG method since the objective function is still the a sum of
convex terms.

7

5 Experiments

In a ﬁrst experiment, we study the inﬂuence of the different balancing criteria on the obtained clus-
tering. The data is a Gaussian mixture in R20 where the projection onto the ﬁrst two dimensions
is shown in Figure 2 - the remaining 18 dimensions are just noise. The distribution of the 2000
points is [1200,600,200]. A symmetric k-NN-graph with k = 20 is built with Gaussian weights
2(cid:107)x−y(cid:107)2
−
max{σ2
} where σx,k is the k-NN distance of point x. For better interpretation, we report
,σ2
x,k
y,k

e

Figure 2: From left to right: Cheeger 1-cut, Normalized 1-cut, truncated Cheeger cut (TCC), hard
balanced cut (HBC), hard Cheeger cut (HCC). The criteria are the normalized ones, i.e., the vertex
weights are ei = di .
all resulting partitions with respect to all balanced graph cut criteria, cut and the size of the largest
component in the following table. The parameter for truncated, hard Cheeger cut and hard balanced
cut is set to K = 200. One observes that the normalized 1-cut results in a less balanced partition but
with a much smaller cut than the Cheeger 1-cut, which is itself less balanced than the hard Cheeger
cut. The latter is fully balanced but has an even higher cut. The truncated Cheeger cut has a smaller
cut than the hard balanced cut but its partition is not feasible. Note that the hard balanced cut is
similar to the normalized 1-cut but achieves smaller cut at the prize of a larger maximal component.
Thus, the example nicely shows how the different balance criterion inﬂuence the ﬁnal partition.
Criterion \ Obj.
max{|A|, |A|}
Cut
Ch. 1-cut N. 1-cut TCC200 HBC200 HCC200
1301
Cheeger 1-cut
408.4
0.817
408.4
2.042
0.079
0.099
1775
Norm. 1-cut
178.3
0.892
0.075
0.132
178.3
6.858
∞
∞
1945
Trunc. Ch. cut
153.6
0.768
0.263
0.513
1785
Hard bal. cut
175.4
10.96
175.4
0.877
0.076
0.134
Hard Ch. cut
1000
639.2
0.119
0.115
3.196
639.2
0.798
Next we perform unnormalized 1-spectral clustering on the full USPS, normal and extended1
multicut version of the normalized 1-cut, given as RCut(C1 , . . . , CM ) = (cid:80)M
MNIST-datasets (resp. 9298, 70000 and 630000 points) in the same setting as in [14] with no
vertex weights, that is ei = 1, ∀i ∈ V . As clustering criterion for multi-partitioning we use the
cut(Ci ,Ci )
. We
|Ci |
i=1
successively subdivide clusters until the desired number of clusters (M = 10) is reached. This re-
cursive partitioning scheme is used for all methods. In [14] the Cheeger 1-cut has been used which
is not compatible with the multi-cut criterion. We expect that using the normalized 1-cut for the
bipartitioning steps we should get better results. The results of the other methods for USPS and
MNIST (normal) are taken from [14]. Each bipartitioning step is initialized randomly. Out of 100
obtained multi-partitionings we report the results of the best clustering with respect to the multi-cut
criterion. The next table shows the obtained RCut and errors.
S.&B.[26]
Ch. 1-cut[14]
N. 1-cut
Vertices/Edges
USPS
0.6629
0.6661
0.6663
9K/272K
0.1309
0.1349
0.1301
0.1545
0.1507
0.1499
MNIST (Normal)
0.1318
0.1244
0.1236
70K/1043K
–
0.0997
0.0996
MNIST (Ext)
630K/9192K
0.1180
0.1223
–

Standard spectral
0.8180
0.1686
0.2252
0.1883
0.1594
0.2297

Rcut
Error
Rcut
Error
Rcut
Error

1.1-SCl [6]
0.6676
0.1308
0.1529
0.1293
–
–

We see for all datasets improvements in the obtained cut. Also a slight decrease in the obtained
error can be observed. The improvements are not so drastic as the clustering is already very good.
The problem is that for both datasets one digit is split (0) and two are merged (4 and 9) resulting in
seemingly large errors. Similar results hold for the extended MNIST dataset. Note that the resulting
error is comparable to recently reported results on semi-supervised learning [18].
1The extended MNIST dataset is generated by translating each original input image of MNIST by one pixel
(8 directions).

8

References
[1] N. Alon and V. D. Milman. λ1 , isoperimetric inequalities for graphs, and superconcentrators. J. Combin.
Theory Ser. B, 38(1):73–88, 1985.
[2] R. Andersen and K. Lang. An algorithm for improving graph partitions. In Proc. of the 19th ACM-SIAM
Symposium on Discrete Algorithms (SODA 2008), pages 651–660, 2008.
[3] S. Arora, J. R. Lee, and A. Naor. Expander ﬂows, geometric embeddings and graph partitioning. In Proc.
36th Annual ACM Symp. on Theory of Computing (STOC), pages 222–231. ACM, 2004.
[4] F. Bach. Convex analysis and optimization with submodular functions, 2010. arXiv:1010.4207v2.
[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM J. Imaging Sciences, 2:183–202, 2009.
[6] T. B ¨uhler and M. Hein. Spectral clustering based on the graph p-Laplacian. In L. Bottou and M. Littman,
editors, Proc. of the 26th Int. Conf. on Machine Learning (ICML), pages 81–88. Omnipress, 2009.
[7] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to
imaging. Journal of Mathematical Imaging and Vision, 40(1):120–145, 2011.
[8] F. Chung. Spectral Graph Theory. AMS, Providence, RI, 1997.
[9] W. E. Donath and A. J. Hoffman. Lower bounds for the partitioning of graphs. IBM J. Res. Develop.,
17:420–425, 1973.
[10] E. Esser, X. Zhang, and T. F. Chan. A general framework for a class of ﬁrst order primal-dual algorithms
for convex optimization in imaging science. SIAM J. Imaging Sciences, 3(4):1015–1046, 2010.
[11] S. Fujishige. Submodular functions and optimization, volume 58 of Annals of Discrete Mathematics.
Elsevier B. V., Amsterdam, second edition, 2005.
[12] Stephen Guattery and Gary L. Miller. On the quality of spectral separators. SIAM Journal on Matrix
Analysis and Applications, 19:701–719, 1998.
[13] L. Hagen and A. B. Kahng. Fast spectral methods for ratio cut partitioning and clustering. Proc. IEEE
Intl. Conf. on Computer-Aided Design, pages 10–13, November 1991.
[14] M. Hein and T. B ¨uhler. An inverse power method for nonlinear eigenproblems with applications in 1-
In Advances in Neural Information Processing Systems 23 (NIPS
spectral clustering and sparse pca.
2010), pages 847–855, 2010.
[15] J.-B. Hiriart-Urruty. Generalized differentiability, duality and optimization for problems dealing with
differences of convex functions. In Convexity and duality in optimization, pages 37–70. 1985.
[16] C. Houdr ´e. Mixed and Isoperimetric Estimates on the Log-Sobolev Constants of Graphs and Markov
Chains. Combinatorica, 21:489–513, 2001.
[17] Y. Kawahara, K. Nagano, and Y. Okamoto. Submodular fractional programming for balanced clustering.
Pattern Recognition Letters, 32:235–243, 2011.
[18] W. Liu, J. He, and S.-F. Chang. Large graph construction for scalable semi-supervised learning. In Proc.
of the 27th Int. Conf. on Machine Learning (ICML), 2010.
In Mathematical programming: the state of the art
[19] L. Lov ´asz. Submodular functions and convexity.
(Bonn, 1982), pages 235–257. Springer, Berlin, 1983.
[20] M. Maier, U. von Luxburg, and M. Hein.
Inﬂuence of graph construction on graph-based clustering
measures. In Advances in Neural Information Processing Systems 21 (NIPS), pages 1025 – 1032, 2009.
[21] M. Narasimhan and J. Bilmes. A submodular-supermodular procedure with applications to discriminative
structure learning. In 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2005.
[22] M. Narasimhan and J. Bilmes. Local search for balanced submodular clusterings. In 20th International
Joint Conference on Artiﬁcial Intelligence (IJCAI), 2007.
[23] S. B. Patkar and H. Narayanan. Improving graph partitions using submodular functions. Discrete Appl.
Math., 131(2):535–553, 2003.
[24] A. Pothen, H. D. Simon, and K.-P. Liou. Partitioning sparse matrices with eigenvectors of graphs. SIAM
Journal on Matrix Analysis and Applications, 11:430 – 452, 1990.
[25] O. S. Rothaus. Analytic inequalities, Isoperimetric Inequalities and Logarithmic Sobolev Inequalities.
Journal of Functional Analysis, 64:296–313, 1985.
[26] A. Szlam and X. Bresson. Total variation and Cheeger cuts. In Proceedings of the 27th International
Conference on Machine Learning, pages 1039–1046. Omnipress, 2010.
[27] J.-P. Tillich. Edge isoperimetric inequalities for product graphs. Discrete Mathematics, 213:291–320,
2000.
[28] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17:395–416, 2007.

9


The Fast Convergence of Boosting

Matus Telgarsky
Department of Computer Science and Engineering
University of California, San Diego
9500 Gilman Drive, La Jolla, CA 92093-0404
mtelgars@cs.ucsd.edu

Abstract

This manuscript considers the convergence rate of boosting under a large class of
losses, including the exponential and logistic losses, where the best previous rate
of convergence was O(exp(1/2 )). First, it is established that the setting of weak
learnability aids the entire class, granting a rate O(ln(1/)). Next, the (disjoint)
conditions under which the inﬁmal empirical risk is attainable are characterized
in terms of the sample and weak learning class, and a new proof is given for the
known rate O(ln(1/)). Finally, it is established that any instance can be de-
composed into two smaller instances resembling the two preceding special cases,
yielding a rate O(1/), with a matching lower bound for the logistic loss. The
principal technical hurdle throughout this work is the potential unattainability of
the inﬁmal empirical risk; the technique for overcoming this barrier may be of
general interest.

1

Introduction

Boosting is the task of converting inaccurate weak learners into a single accurate predictor. The
existence of any such method was unknown until the breakthrough result of Schapire [1]: under
a weak learning assumption, it is possible to combine many carefully chosen weak learners into a
majority of majorities with arbitrarily low training error. Soon after, Freund [2] noted that a single
majority is enough, and that O(ln(1/)) iterations are both necessary and sufﬁcient to attain accu-
racy . Finally, their combined effort produced AdaBoost, which attains the optimal convergence
rate (under the weak learning assumption), and has an astonishingly simple implementation [3].
It was eventually revealed that AdaBoost was minimizing a risk functional, speciﬁcally the expo-
nential loss [4]. Aiming to alleviate perceived deﬁciencies in the algorithm, other loss functions
were proposed, foremost amongst these being the logistic loss [5]. Given the wide practical suc-
cess of boosting with the logistic loss, it is perhaps surprising that no convergence rate better than
O(exp(1/2 )) was known, even under the weak learning assumption [6]. The reason for this de-
ﬁciency is simple: unlike SVM, least squares, and basically any other optimization problem con-
sidered in machine learning, there might not exist a choice which attains the minimal risk! This
reliance is carried over from convex optimization, where the assumption of attainability is generally
made, either directly, or through stronger conditions like compact level sets or strong convexity [7].
Convergence rate analysis provides a valuable mechanism to compare and improve of minimiza-
tion algorithms. But there is a deeper signiﬁcance with boosting: a convergence rate of O(ln(1/))
means that, with a combination of just O(ln(1/)) predictors, one can construct an -optimal clas-
siﬁer, which is crucial to both the computational efﬁciency and statistical stability of this predictor.
The contribution of this manuscript is to provide a tight convergence theory for a large class of
losses, including the exponential and logistic losses, which has heretofore resisted analysis. The goal
is a general analysis without any assumptions (attainability of the minimum, or weak learnability),

1

however this manuscript also demonstrates how the classically understood scenarios of attainability
and weak learnability can be understood directly from the sample and the weak learning class.
The organization is as follows. Section 2 provides a few pieces of background: how to encode the
weak learning class and sample as a matrix, boosting as coordinate descent, and the primal objective
function. Section 3 then gives the dual problem, max entropy. Given these tools, section 4 shows
how to adjust the weak learning rate to a quantity which is useful without any assumptions. The ﬁrst
step towards convergence rates is then taken in section 5, which demonstrates that the weak learning
rate is in fact a mechanism to convert between the primal and dual problems.
The convergence rates then follow: section 6 and section 7 discuss, respectively, the conditions
under which classical weak learnability and (disjointly) attainability hold, both yielding the rate
O(ln(1/)), and ﬁnally section 8 shows how the general case may be decomposed into these two,
and the conﬂicting optimization behavior leads to a degraded rate of O(1/). The last section will
also exhibit an Ω(1/) lower bound for the logistic loss.

1.1 Related Work

The development of general convergence rates has a number of important milestones in the past
decade. The ﬁrst convergence result, albeit without any rates, is due to Collins et al. [8]; the work
considered the improvement due to a single step, and as its update rule was less aggressive than the
line search of boosting, it appears to imply general convergence. Next, Bickel et al. [6] showed a
rate of O(exp(1/2 )), where the assumptions of bounded second derivatives on compact sets are
also necessary here.
Many extremely important cases have also been handled. The ﬁrst is the original rate of O(ln(1/))
for the exponential loss under the weak learning assumption [3]. Next, R ¨atsch et al. [9] showed, for
a class of losses similar to those considered here, a rate of O(ln(1/)) when the loss minimizer is
attainable. The current manuscript provides another mechanism to analyze this case (with the same
rate), which is crucial to being able to produce a general analysis. And, very recently, parallel to this
work, Mukherjee et al. [10] established the general convergence under the exponential loss, with a
rate of Θ(1/). The same matrix, due to Schapire [11], was used to show the lower bound there as
for the logistic loss here; their upper bound proof also utilized a decomposition theorem.
It is interesting to mention that, for many variants of boosting, general convergence rates were
known. Speciﬁcally, once it was revealed that boosting is trying to be not only correct but also
have large margins [12], much work was invested into methods which explicitly maximized the
margin [13], or penalized variants focused on the inseparable case [14, 15]. These methods generally
impose some form of regularization [15], which grants attainability of the risk minimizer, and allows
standard techniques to grant general convergence rates. Interestingly, the guarantees in those works
cited in this paragraph are O(1/2 ).

2 Setup

A view of boosting, which pervades this manuscript, is that the action of the weak learning class
1 ⊆ (X × Y )m
upon the sample can be encoded as a matrix [9, 15]. Let a sample S := {(xi , yi )}m
and a weak learning class H be given. For every h ∈ H, let S |h denote the projection onto S
induced by h; that is, S |h is a vector of length m, with coordinates (S |h )i = yih(xi ). If the set
of all such columns {S |h : h ∈ H} is ﬁnite, collect them into the matrix A ∈ Rm×n . Let ai
denote the ith row of A, corresponding to the example (xi , yi ), and let {hj }n
1 index the set of weak
learners corresponding to columns of A. It is assumed, for convenience, that entries of A are within
[−1, +1]; relaxing this assumption merely scales the presented rates by a constant.
The setting considered in this manuscript is that this ﬁnite matrix can be constructed. Note that this
can encode inﬁnite classes, so long as they map to only k < ∞ values (in which case A has at most
km columns). As another example, if the weak learners are binary, and H has VC dimension d,
then Sauer’s lemma grants that A has at most (m + 1)d columns. This matrix view of boosting is
thus similar to the interpretation of boosting performing descent on functional space, but the class
complexity and ﬁnite sample have been used to reduce the function class to a ﬁnite object [16, 5].

2

Routine BOO S T.
Input Convex function f ◦ A.
Output Approximate primal optimum λ.
1. Initialize λ0 := 0n .
2. For t = 1, 2, . . ., while ∇(f ◦ A)(λt−1 ) (cid:54)= 0n :
(a) Choose column jt := argmaxj |∇(f ◦ A)(λt−1 )(cid:62)ej |.
(b) Line search: αt apx. minimizes α (cid:55)→ (f ◦ A)(λt−1 + αejt ).
(c) Update λt := λt−1 + αtejt .
3. Return λt−1 .

Figure 1: l1 steepest descent [17, Algorithm 9.4] applied to f ◦ A.

(f ◦ A)(λ) =: ¯fA ,

(2.1)

To make the connection to boosting, the missing ingredient is the loss function. Let G0 denote the
set of loss functions g satisfying: g is twice continuously differentiable, g (cid:48)(cid:48) > 0 (which implies
strict convexity), and limx→∞ g(x) = 0. (A few more conditions will be added in section 5 to prove
convergence rates, but these properties sufﬁce for the current exposition.) Crucially, the exponential
loss exp(−x) from AdaBoost and the logistic loss ln(1 + exp(−x)) are in G0 (and the eventual G).
Boosting determines some weighting λ ∈ Rn of the columns of A, which correspond to weak
learners in H. The (unnormalized) margin of example i is thus (cid:104)ai , λ(cid:105) = e(cid:62)
i Aλ, where ei is an
indicator vector. Since the prediction on xi is 1[(cid:104)ai , λ(cid:105) ≥ 0], it follows that Aλ > 0m (where 0m is
m(cid:88)
m(cid:88)
the zero vector) implies a training error of zero. As such, boosting solves the minimization problem
g((cid:104)ai , λ(cid:105)) = inf
g(e(cid:62)
where f : Rm → R is the convenience function f (x) = (cid:80)
f (Aλ) = inf
i Aλ) = inf
inf
λ∈Rn
λ∈Rn
λ∈Rn
λ∈Rn
i=1
i=1
i g((x)i ), and in the present problem
denotes the (unnormalized) empirical risk. ¯fA will denote the optimal objective value.
The inﬁmum in eq. (2.1) may well not be attainable. Suppose there exists λ(cid:48) such that Aλ(cid:48) > 0m
(theorem 6.1 will show that this is equivalent to the weak learning assumption). Then
f (Aλ) ≤ inf {f (Aλ) : λ = cλ(cid:48) , c > 0} = inf
0 ≤ inf
f (c(Aλ(cid:48) )) = 0.
λ∈Rn
c>0
On the other hand, for any λ ∈ Rn , f (Aλ) > 0. Thus the inﬁmum is never attainable when weak
learnability holds.
The template boosting algorithm appears in ﬁg. 1, formulated in terms of f ◦ A to make the connec-
m(cid:88)
tion to coordinate descent as clear as possible. To interpret the gradient terms, note that
i=1
which is the expected correlation of hj with the target labels according to an unnormalized dis-
tribution with weights g (cid:48) ((cid:104)ai , λ(cid:105)). The stopping condition ∇(f ◦ A)(λ) = 0m means: either the
distribution is degenerate (it is exactly zero), or every weak learner is uncorrelated with the target.
As such, eq. (2.1) represents an equivalent formulation of boosting, with one minor modiﬁcation:
the column (weak learner) selection has an absolute value. But note that this is the same as closing
H under complementation (i.e., for any h ∈ H, there exists h(cid:48) with h(x) = −h(cid:48) (x)), which is
assumed in many theoretical treatments of boosting.
In the case of the exponential loss with binary weak learners, the line search step has a conve-
nient closed form; but for other losses, or even for the exponential loss but with conﬁdence-rated
predictors, there may not be a closed form. Moreover, this univariate search problem may lack a
minimizer. To produce the eventual convergence rates, this manuscript utilizes a step size minimiz-
ing an upper bounding quadratic (which is guaranteed to exist); if instead a standard iterative line
search guarantee were used, rates would only degrade by a constant factor [17, section 9.3.1].

(∇(f ◦ A)(λ))j = (A(cid:62)∇f (Aλ))j =

g (cid:48) ((cid:104)ai , λ(cid:105))hj (xi )yi ,

3

As a ﬁnal remark, consider the rows {ai }m
1 of A as a collection of m points in Rn . Due to the form
of g , BOO ST is therefore searching for a halfspace, parameterized by a vector λ, which contains
all of the points. Sometimes such a halfspace may not exist, and g applies a smoothly increasing
penalty to points that are farther and farther outside it.

3 Dual Problem

This section provides the convex dual to eq. (2.1). The relevance of the dual to convergence rates is
as follows. First, although the primal optimum may not be attainable, the dual optimum is always
attainable—this suggests a strategy of mapping the convergence strategy to the dual, where there
exists a clear notion of progress to the optimum. Second, this section determines the dual feasible
set—the space of dual variables or what the boosting literature typically calls unnormalized weights.
Understanding this set is key to relating weak learnability, attainability, and general instances.
Before proceeding, note that the dual formulation will make use of the Fenchel conjugate h∗ (φ) =
supx∈dom(h) (cid:104)x, φ(cid:105) − h(x), a concept taking a central place in convex analysis [18, 19]. Interest-
ingly, the Fenchel conjugates to the exponential and logistic losses are respectively the Boltzmann-
Shannon and Fermi-Dirac entropies [19, Commentary, section 3.3], and thus the dual is explicitly
Theorem 3.1. For any A ∈ Rm×n and g ∈ G0 with f (x) = (cid:80)
performing entropy maximization (cf. lemma C.2). As a ﬁnal piece of notation, denote the kernel of
a matrix B ∈ Rm×n by Ker(B ) = {φ ∈ Rn : Bφ = 0m}.
i g((x)i ),
inf {f (Aλ) : λ ∈ Rn} = sup {−f ∗ (−φ) : φ ∈ ΦA } ,
Lastly, f ∗ (φ) = (cid:80)m
where ΦA := Ker(A(cid:62) )∩Rm
+ is the dual feasible set. The dual optimum ψA is unique and attainable.
i=1 g∗ ((φ)i ).
+ ), and, for any j , 0 = (φ(cid:62)A)j = (cid:80)m
+ has a strong interpretation. Suppose ψ ∈ ΦA ; then ψ
The dual feasible set ΦA = Ker(A(cid:62) ) ∩ Rm
is a nonnegative vector (since ψ ∈ Rm
i=1 φi yihj (xi ). That
is to say, every nonzero feasible dual vector provides a (an unnormalized) distribution upon which
every weak learner is uncorrelated! Furthermore, recall that the weak learning assumption states that
under any weighting of the input, there exists a correlated weak learner; as such, weak learnability
necessitates that the dual feasible set contains only the zero vector.
There is also a geometric interpretation. Ignoring the constraint, −f ∗ attains its maximum at some
rescaling of the uniform distribution (for details, please see lemma C.2). As such, the constrained
dual problem is aiming to write the origin as a high entropy convex combination of the points {ai }m
1 .

(3.2)

4 A Generalized Weak Learning Rate

The weak learning rate was critical to the original convergence analysis of AdaBoost, providing a
handle on the progress of the algorithm. Recall that the quantity appeared in the denominator of the
convergence rate, and a weak learning assumption critically provided that this quantity is nonzero.
This section will generalize the weak learning rate to a quantity which is always positive, without
any assumptions.
Note brieﬂy that this manuscript will differ slightly from the norm in that weak learning will be a
purely sample-speciﬁc concept. That is, the concern here is convergence, and all that matters is the
sample S = {(xi , yi )}m
1 , as encoded in A; it doesn’t matter if there are wild points outside this
sample, because the algorithm has no access to them.
This distinction has the following implication. The usual weak learning assumption states that there
exists no uncorrelating distribution over the input space. This of course implies that any training
sample S used by the algorithm will also have this property; however, it sufﬁces that there is no
distribution over the input sample S which uncorrelates the weak learners from the target.
Returning to task, the weak learning assumption posits the existence of a constant, the weak learning
rate γ , which lower bounds the correlation of the best weak learner with the target for any distribu-

4

=

.

max
j∈[n]

inf
φ∈Rm
+ \{0m }

inf
φ∈Rm
+ \{0m }

(cid:107)A(cid:62)φ(cid:107)∞
(cid:107)φ(cid:107)1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
tion. Stated in terms of the matrix A,
(cid:107)A(cid:62)φ(cid:107)∞
(cid:107)φ − 0m (cid:107)1
0 < γ = inf
(φ)i yihj (xi )
φ∈Rm
+
(cid:107)φ(cid:107)=1
i=1
(4.1)
The only way this quantity can be positive is if φ (cid:54)∈ Ker(A(cid:62) ) ∩ Rm
+ = ΦA , meaning the dual
feasible set is exactly {0m }. As such, one candidate adjustment is to simply replace {0m} with the
dual feasible set:
(cid:107)A(cid:62)φ(cid:107)∞
γ (cid:48) := inf
inf ψ∈ΦA (cid:107)φ − ψ(cid:107)1
.
+ \ΦA
φ∈Rm
Indeed, by the forthcoming proposition 4.3, γ (cid:48) > 0 as desired. Due to technical considerations
which will be postponed until the various convergence rates, it is necessary to tighten this deﬁnition
with another set.
(cid:26)
(cid:27)
Deﬁnition 4.2. For a given matrix A ∈ Rm×n and set S ⊆ Rm , deﬁne
(cid:107)A(cid:62)φ(cid:107)∞
: φ ∈ S \ Ker(A(cid:62) )
inf ψ∈S∩Ker(A(cid:62) ) (cid:107)φ − ψ(cid:107)1
Crucially, for the choices of S pertinent here, this quantity is always positive.
Proposition 4.3. Let A (cid:54)= 0m×n and polyhedron S be given. If S ∩ Ker(A(cid:62) ) (cid:54)= ∅ and S has
nonempty interior, γ (A, S ) ∈ (0, ∞).
To simplify discussion, the following projection and distance notation will be used in the sequel:
C (x)(cid:107)p ,
C (x) = (cid:107)x − Pp
C (x) ∈ Argmin
(cid:107)y − x(cid:107)p ,
Dp
Pp
y∈C
with some arbitrary choice made when the minimizer is not unique.

γ (A, S ) := inf

.

♦

5 Prelude to Convergence Rates: Three Alternatives

The pieces are in place to ﬁnally sketch how the convergence rates may be proved. This section
identiﬁes how the weak learning rate γ (A, S ) can be used to convert the standard gradient guarantees
into something which can be used in the presence of no attainable minimum. To close, three basic
optimization scenarios are identiﬁed, which lead to the following three sections on convergence
rates. But ﬁrst, it is a good time to deﬁne the ﬁnal loss function class.
Deﬁnition 5.1. Every g ∈ G satisﬁes the following properties. First, g ∈ G0 . Next, for any x ∈ Rm
satisfying f (x) ≤ f (Aλ0 ), and for any coordinate (x)i , there exist constants η > 0 and β > 0 such
that g (cid:48)(cid:48) ((x)i ) ≤ ηg((x)i ) and g((x)i ) ≤ −β g (cid:48) ((x)i ).
♦
The exponential loss is in this class with η = β = 1 since exp(·) is a ﬁxed point with respect to
the differentiation operator. Furthermore, as is veriﬁed in remark F.1 of the full version, the logistic
loss is also in this class, with η = 2m /(m ln(2)) and β ≤ 1 + 2m . Intuitively, η and β encode
how similar some g ∈ G is to the exponential loss, and thus these parameters can degrade radically.
However, outside the weak learnability case, the other terms in the bounds here will also incur a
penalty of the form em for the exponential loss, and there is some evidence that this is unavoidable
(see the lower bounds in Mukherjee et al. [10] or the upper bounds in R ¨atsch et al. [9]).
Next, note how the standard guarantee for coordinate descent methods can lead to guarantees on the
progress of the algorithm in terms of dual distances, thanks to γ (A, S ).
Proposition 5.2. For any t, A (cid:54)= 0m×n , S ⊇ {−∇f (Aλt )} with γ (A, S ) > 0, and g ∈ G,
S∩Ker(A(cid:62) ) (−∇f (Aλt ))2
f (Aλt+1 ) − ¯fA ≤ f (Aλt ) − ¯fA − γ (A, S )2D1
2ηf (Aλt )
Proof. The stopping condition grants −∇f (Aλt ) (cid:54)∈ Ker(A(cid:62) ). Thus, by deﬁnition of γ (A, S ),
(cid:107)A(cid:62)φ(cid:107)∞
(cid:107)A(cid:62)∇f (Aλt )(cid:107)∞
≤
S∩Ker(A(cid:62) ) (−∇f (Aλt ))
D1
D1
S∩Ker(A(cid:62) ) (φ)

inf
φ∈S\Ker(A(cid:62) )

γ (A, S ) =

.

.

5

(a) Weak learnability.
(c) General case.
(b) Attainability.
Figure 2: Viewing the rows {ai}m
1 of A as points in Rn , boosting seeks a homogeneous halfspace,
parameterized by a normal λ ∈ Rn , which contains all m points. The dual, on the other hand, aims
to express the origin as a high entropy convex combination of the rows. The convergence rate and
dynamics of this process are controlled by A, which dictates one of the three above scenarios.

Combined with a standard guarantee of coordinate descent progress (cf. lemma F.2),
S∩Ker(A(cid:62) ) (−∇f (Aλt ))2
f (Aλt ) − f (Aλt+1 ) ≥ (cid:107)A(cid:62)∇f (Aλt )(cid:107)2∞
≥ γ (A, S )2D1
2ηf (Aλt )
2ηf (Aλt )
Subtracting ¯fA from both sides and rearranging yields the statement.

.

Recall the interpretation of boosting closing section 2: boosting seeks a halfspace, parameterized by
λ ∈ Rn , which contains the points {ai }m
1 . Progress onward from proposition 5.2 will be divided
into three cases, each distinguished by the kind of halfspace which boosting can reach.
These cases appear in ﬁg. 2. The ﬁrst case is weak learnability: positive margins can be attained
on each example, meaning a halfspace exists which strictly contains all points. Boosting races to
push all these margins unboundedly large, and has a convergence rate O(ln(1/)). Next is the case
that no halfspace contains the points within its interior: either any such halfspace has the points on
its boundary, or no such halfspace exists at all (the degenerate choice λ = 0n ). This is the case of
attainability: boosting races towards ﬁnite margins at the rate O(ln(1/)).
The ﬁnal situation is a mix of the two: there exists a halfspace with some points on the boundary,
some within its interior. Boosting will try to push some margins to inﬁnity, and keep others ﬁnite.
These two desires are at odds, and the rate degrades to O(1/). Less metaphorically, the analysis
will proceed by decomposing this case into the previous two, applying the above analysis in parallel,
and then stitching the result back together. It is precisely while stitching up that an incompatibility
arises, and the rate degrades. This is no artifact: a lower bound will be shown for the logistic loss.

6 Convergence Rate under Weak Learnability

To start this section, the following result characterizes weak learnability, including the earlier rela-
tionship to the dual feasible set (speciﬁcally, that it is precisely the origin), and, as analyzed by many
authors, the relationship to separability [1, 9, 15].
Theorem 6.1. For any A ∈ Rm×n and g ∈ G the following conditions are equivalent:
∃λ ∈ Rn (cid:5) Aλ ∈ Rm
++ ,
f (Aλ) = 0,
inf
λ∈Rn
ψA = 0m ,
ΦA = {0m}.

(6.2)
(6.3)

(6.4)
(6.5)

The equivalence means the presence of any of these properties sufﬁces to indicate weak learnability.
The last two statements encode the usual distributional version of the weak learning assumption.

6

The ﬁrst encodes the fact that there exists a homogeneous halfspace containing all points within
its interior; this encodes separability, since removing the factor yi from the deﬁnition of ai will
place all negative points outside the halfspace. Lastly, the second statement encodes the fact that the
empirical risk approaches zero.
(cid:18)
(cid:19)t
Theorem 6.6. Suppose Aλ > 0m and g ∈ G; then γ (A, Rm
+ ) > 0, and for all t,
1 − γ (A, Rm
+ )2
f (Aλt ) − ¯fA ≤ f (Aλ0 )
2β 2 η
+ ∩ Ker(A(cid:62) ) = ΦA = {0m }, which combined with g ≤ −β g (cid:48) gives
Proof. By theorem 6.1, Rm
(−∇f (Aλt )) = inf
(cid:107) − ∇f (Aλt ) − ψ(cid:107)1 = (cid:107)∇f (Aλt )(cid:107)1 ≥ f (Aλt )/β .
D1
ψ∈ΦA
ΦA
+ ⊇ −∇f (Rm ) (whereby
Plugging this and ¯fA = 0 (again by theorem 6.1) along with polyhedron Rm
(cid:18)
(cid:19)
+ ) > 0 by proposition 4.3 since ψA ∈ Rm
γ (A, Rm
+ ) into proposition 5.2 gives
f (Aλt+1 ) ≤ f (Aλt ) − γ (A, Rm
1 − γ (A, Rm
+ )2 f (Aλt )
+ )2
2β 2 η
2β 2 η
and recursively applying this inequality yields the result.

= f (Aλt )

.

,

Since the present setting is weak learnability, note by (4.1) that the choice of polyhedron Rm
+ grants
that γ (A, Rm
+ ) is exactly the original weak learning rate. When specialized for the exponential loss
(where η = β = 1), the bound becomes (1 − γ (A, Rm
+ )2/2)t , which exactly recovers the bound of
(cid:16)− γ (f ,A)2 t
≤ (cid:16)
(cid:17)t ≤ exp
(cid:17)
Schapire and Singer [20], although via different analysis.
In general, solving for t in the expression  = f (Aλt )− ¯fA
1 − γ (f ,A)2
f (Aλ0 )− ¯fA
2β 2 η
2β 2 η
reveals that t ≤ 2β 2 η
γ (A,S )2 ln(1/) iterations sufﬁce to reach error . Recall that β and η , in the case of
the logistic loss, have only been bounded by quantities like 2m . While it is unclear if this analysis
of β and η was tight, note that it is plausible that the logistic loss is slower than the exponential loss
in this scenario, as it works less in initial phases to correct minor margin violations.

7 Convergence Rate under Attainability
Theorem 7.1. For any A ∈ Rm×n and g ∈ G, the following conditions are equivalent:
∀λ ∈ Rn (cid:5) Aλ (cid:54)∈ Rm
+ \ {0m },
f ◦ A has minimizers,
ψA ∈ Rm
++ ,
ΦA ∩ Rm
++ (cid:54)= ∅.
Interestingly, as revealed in (7.4) and (7.5), attainability entails that the dual has fully interior points,
and furthermore that the dual optimum is interior. On the other hand, under weak learnability,
eq. (6.4) provided that the dual optimum has zeros at every coordinate. As will be made clear in
section 8, the primal and dual weights have the following dichotomy: either the margin (cid:104)ai , λ(cid:105) goes
to inﬁnity and (ψA )i goes to zero, or the margin stays ﬁnite and (ψA )i goes to some positive value.
Theorem 7.6. Suppose A (cid:54)= 0m×n , g ∈ G, and the inﬁmum of eq. (2.1) is attainable. Then there
exists a (compact) tightest axis-aligned retangle C containing the initial level set, and f is strongly
(cid:19)t
(cid:18)
convex with modulus c > 0 over C . Finally, γ (A, −∇f (C )) > 0, and for all t,
1 − cγ (A, −∇f (C ))2
f (Aλt ) − ¯fA ≤ (f (0m ) − ¯fA )
ηf (Aλ0 )

(7.2)
(7.3)
(7.4)
(7.5)

.

In other words, t ≤
ηf (Aλ0 )
 ) iterations sufﬁce to reach error . The appearance of a
cγ (A,−∇f (C ))2 ln( 1
modulus of strong convexity c (i.e., a lower bound on the eigenvalues of the Hessian of f ) may seem
surprising, and sketching the proof illuminates its appearance and subsequent function.

7

When the inﬁmum is attainable, every margin (cid:104)ai , λ(cid:105) converges to some ﬁnite value. In fact, they all
remain bounded: (7.2) provides that no halfspace contains all points, so if one margin becomes pos-
itive and large, another becomes negative and large, giving a terrible objective value. But objective
values never increase with coordinate descent. To ﬁnish the proof, strong convexity (i.e., quadratic
lower bounds in the primal) grants quadratic upper bounds in the dual, which can be used to bound
the dual distance in proposition 5.2, and yield the desired convergence rate. This approach fails
under weak learnability—some primal weights grow unboundedly, all dual weights shrink to zero,
and no compact set contains all margins.

8 General Convergence Rate

(8.4)

(8.5)

The ﬁnal characterization encodes two principles: the rows of A may be partitioned into two matri-
ces A0 , A+ which respectively satisfy theorem 6.1 and theorem 7.1, and that these two subproblems
affect the optimization problem essentially independently.
Theorem 8.1. Let A0 ∈ Rz×n , A+ ∈ Rp×n , and g ∈ G be given. Set m := z + p, and A ∈ Rm×n
to be the matrix obtained by stacking A0 on top of A+ . The following conditions are equivalent:
(∃λ ∈ Rn (cid:5) A0λ ∈ Rz
++ ∧ A+λ = 0p ) ∧ (∀λ ∈ Rn (cid:5) A+λ (cid:54)∈ Rp
+ \ {0p}),
(8.2)
(cid:104) ψA0
(cid:105)
f (A0λ) = 0) ∧ f ◦ A+ has minimizers,
f (A+λ)) ∧ ( inf
(8.3)
f (Aλ) = inf
( inf
λ∈Rn
λ∈Rn
λ∈Rn
with ψA0 = 0z ∧ ψA+ ∈ Rp
ψA =
++ ,
ψA+
(ΦA0 = {0z }) ∧ (ΦA+ ∩ Rp
++ (cid:54)= ∅) ∧ (ΦA = ΦA0 × ΦA+ ).
To see that any matrix A falls into one of the three scenarios here, ﬁx a loss function g , and recall
from theorem 3.1 that ψA is unique. In particular, the set of zero entries in ψA exactly speciﬁes
which of the three scenarios hold, the current scenario allowing for simultaneous positive and zero
entries. Although this reasoning made use of ψA , note that it is A which dictates the behavior: in
fact, as is shown in remark I.1 of the full version, the decomposition is unique.
Returning to theorem 8.1, the geometry of ﬁg. 2c is provided by (8.2) and (8.5). The analysis
will start from (8.3), which allows the primal problem to be split into two pieces, which are then
individually handled precisely as in the preceding sections. To ﬁnish, (8.5) will allow these pieces
to be stitched together.
++ \ {0m}, and the notation from
+ \ Rm
Theorem 8.6. Suppose A (cid:54)= 0m×n , g ∈ G, ψA ∈ Rm
theorem 8.1. Set w := supt (cid:107)∇f (A+λt ) + P1
(−∇f (A+λt ))(cid:107)1 . Then w < ∞, and there exists
ΦA+
+ × −∇f (C+ ))2 /((β + w/(2c))2 η)(cid:9)(cid:1) .
2f (Aλ0 )/ (cid:0)(t + 1) min (cid:8)1, γ (A, Rz
a tightest cube C+ so that C+ ⊇ {x ∈ Rp : f (x) ≤ f (Aλ0 )}, and let c > 0 be the modulus of
+ × −∇f (C+ )) > 0, and for all t, f (Aλt ) − ¯fA ≤
strong convexity of f over C+ . Then γ (A, Rz
(In the case of the logistic loss, w ≤ supx∈Rm (cid:107)∇f (x)(cid:107)1 ≤ m.)
As discussed previously, the bounds deteriorate to O(1/) because the ﬁnite and inﬁnite margins
(cid:34)−1 +1
(cid:35)
sought by the two pieces A0 , A+ are in conﬂict. For a beautifully simple, concrete case of this,
consider the following matrix, due to Schapire [11]:
+1 −1
+1 +1
The optimal solution here is to push both coordinates of λ unboundedly positive, with margins
approaching (0, 0, ∞). But pushing any coordinate λi too quickly will increase the objective value,
rather than decreasing it. In fact, this instance will provide a lower bound, and the mechanism of the
proof shows that the primal weights grow extremely slowly, as O(ln(t)).
Theorem 8.7. Using the logistic loss and exact line search, for any t ≥ 1, f (Sλt ) − ¯fS ≥ 1/(8t).

S :=

.

Acknowledgement

The author thanks Sanjoy Dasgupta, Daniel Hsu, Indraneel Mukherjee, and Robert Schapire for
valuable conversations. The NSF supported this work under grants IIS-0713540 and IIS-0812598.

8

References
[1] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, July
1990.
[2] Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation,
121(2):256–285, 1995.
[3] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.
[4] Leo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517,
October 1999.
[5] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statis-
tical view of boosting. Annals of Statistics, 28, 1998.
[6] Peter J. Bickel, Yaacov Ritov, and Alon Zakai. Some theory for generalized boosting algo-
rithms. Journal of Machine Learning Research, 7:705–732, 2006.
[7] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex
differentiable minimization. Journal of Optimization Theory and Applications, 72:7–35, 1992.
[8] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and
Bregman distances. Machine Learning, 48(1-3):253–285, 2002.
[9] Gunnar R ¨atsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of leveraging.
In NIPS, pages 487–494, 2001.
[10] Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost.
In COLT, 2011.
[11] Robert E. Schapire. The convergence rate of AdaBoost. In COLT, 2010.
[12] Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A
new explanation for the effectiveness of voting methods. In ICML, pages 322–330, 1997.
[13] Gunnar R ¨atsch and Manfred K. Warmuth. Maximizing the margin with boosting. In COLT,
pages 334–350, 2002.
[14] Manfred K. Warmuth, Karen A. Glocer, and Gunnar R ¨atsch. Boosting algorithms for maxi-
mizing the soft margin. In NIPS, 2007.
[15] Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear
In COLT, pages 311–322,
separability: New relaxations and efﬁcient boosting algorithms.
2008.
[16] Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Functional gradient
techniques for combining hypotheses. In A.J. Smola, P.L. Bartlett, B. Sch ¨olkopf, and D. Schu-
urmans, editors, Advances in Large Margin Classiﬁers, pages 221–246, Cambridge, MA, 2000.
MIT Press.
[17] Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.
[18] Jean-Baptiste Hiriart-Urruty and Claude Lemar ´echal. Fundamentals of Convex Analysis.
Springer Publishing Company, Incorporated, 2001.
[19] Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer
Publishing Company, Incorporated, 2000.
[20] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated
predictions. Machine Learning, 37(3):297–336, 1999.
[21] George B. Dantzig and Mukund N. Thapa. Linear Programming 2: Theory and Extensions.
Springer, 2003.
[22] Adi Ben-Israel. Motzkin’s transposition theorem, and the related theorems of Farkas, Gordan
and Stiemke. In M. Hazewinkel, editor, Encyclopaedia of Mathematics, Supplement III. 2002.

9


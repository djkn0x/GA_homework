!"#$%&’()*+",+-.,&/&.0&+(.+12)&.)+3(/(04%&)+5%%"026".+

327(8+9".)2:;+32.(&%+<"*+
=>?@;+!2#A/(8:&B+

CDD+
E"$(0+#"8&%F+2/&+$"G&/,H%+)""%F+,"/+&’$%"/(.:+%2/:&+82)2+F&)F+2.8+,"/+#2I(.:+
(.,&/&.0&F+2A"H)+)4&+0".)&.)+",+8"0H#&.)F+

3"0H#&.)F+

$"%(60F+JKLKK+
$/&F(8&.)+JKKMN+
"A2#2+JKKMK+
G2F4(.:)".+JKKON+
/&%(:(".+JKKDK+
T+

E"$(0F+

/&%(:(".+JKNKK+
4(.8H+JKKMU+
VH8(2F#+JKKOK+
&)4(0F+JKKWN+
AH884(F#+JKKLD+
T+

F$"/)F+JKLKN+
A2F&A2%%+JKLKK+
F"00&/+JKKNN+
A2FI&)A2%%+JKKNK+
,"")A2%%+JKKXN+
T+

βt = ￿ p(w | z = t) ￿

5%#"F)+2%%+HF&F+",+)"$(0+#"8&%F+=&J:J;+,"/+H.FH$&/7(F&8+%&2/.(.:;+(.,"/#26".+
/&)/(&72%;+0%2FF(P026".B+/&QH(/&+!"#$%$&’&()*+&,-.".,*.R+

>&G+8"0H#&.)+

C42)+(F+)4(F+8"0H#&.)+2A"H)S+

C"/8F+GL;+T;+G>+

3(F)/(AH6".+",+)"$(0F+

θ

G&2)4&/+
P.2.0&+
F$"/)F+

JNK+
JXM+
JKL+

!"#$%&’()*+",+-.,&/&.0&+(.+12)&.)+3(/(04%&)+5%%"026".+

327(8+9".)2:;+32.(&%+<"*+
=>?@;+!2#A/(8:&B+

CDD+

C&+F)H8*+)4&+0"#$%&’()*+",+$/"A2A(%(F60+(.,&/&.0&+(.+12)&.)+3(/(04%&)+5%%"026".+

Input: new document with words w1:N

topic-word distributions βt , t = 1, 2, . . . , T and Dirichlet hyper-parameters α1:T

\&.&/267&+#"8&%+

(cid:7555)!

θ ∼ Dirichlet(α1:T )
++++++++++++++++++++++++++++++++++++++++++++!4""F&+2+8(F)/(AH6".+"7&/+)4&+E+)"$(0F++

∼
(cid:7556)! Z"/+&204+G"/8+(;+
++++++++++++++++++++++++++++++++++++++++++++!4""F&+2+)"$(0+,"/+([)4+G"/8+
zi | θ ∼ θ
| ∼

++++++++++++++++++++++++++++++++++++++++++++++++++++92#$%&+2+G"/8+
wi | zi ∼ βzi

Y"$H%2/+(.,&/&.0&+$/"A%&#F+

1. Maximize p(z1:N | w1:N ). ←− Discrete. Classiﬁcation.

2. Maximize p(θ | w1:N ). ←− Dimensionality reduction, IR

3. Sample from p(θ | w1:N ). ←− Useful for learning in EM.

4. Sample from p(z1:N | w1:N ). ←− Useful for learning in EM.

!"#$%&’()*+",+-.,&/&.0&+(.+12)&.)+3(/(04%&)+5%%"026".+

327(8+9".)2:;+32.(&%+<"*+
=>?@;+!2#A/(8:&B+

Maximize p(z1:N | w1:N )

]2(.+<&FH%)F+

For any α

CDD+

9+1#!&*(+&,+:;<+%((&=,4.,1+ 5#4!’.6&12+

7,18&)#,+

]"F)+0"##".+
F&a.:++

9#2%%+

12/:&+

^2F*+

Z(/F)+04""F&+)"$(0+F(_&F;+
)4&.+#2)04+G"/8F+)"+)"$(0F+

>Y‘42/8+

<&8H06".+,/"#+F&)+$20I(.:+

Maximize p(θ | w1:N )

/&"&*0’.1+02!."3!%"%4.1."(+ 5#4!’.6&12+

7,18&)#,+

]"F)+0"##".+
F&a.:++

αt ≥ 1

αt < 1

^2F*+

]2’(#(_(.:+0".027&+,H.06".+

>Y‘42/8+

<&8H06".+,/"#+F&)+0"7&/+

Sample from p(θ | w1:N )

/&"&*0’.1+02!."3!%"%4.1."(+ 5#4!’.6&12+

7,18&)#,+

αt ≥ 1

αt ≈ 0

^2F*+

1":‘0".027&+8(F)/(AH6".+

>Y‘42/8+

<&8H06".+,/"#+F&)+0"7&/+

Practical Variational Inference 
for Neural Networks

Alex Graves

CIFAR Junior Fellow
University of Toronto
Canada

Method

• Instead of learning neural network weights, we learn the 
mean and variance of a separate Gaussian for each 
weight: adaptive weight noise
• The loss is the number of bits to transmit the errors plus 
the number of bits to transmit the weights: optimisation 
= compression
• The more information the weights store about the 
training data, the more they cost to send: no overﬁtting
• Can interpret as MDL or stochastic variational inference

Advantages

• Applies to any differentiable log-loss model (previous 
variational methods for neural networks were limited to 
very simple architectures) 
• No validation set required (as long as the training data is 
compressed)
• The weight costs tell you how important each weight is 
to the network
• Can prune the network by removing weights with high 
probability at zero

Results

• Outperformed other regularisers for phoneme 
recognition on TIMIT with a complex neural network

ay aa nx er m ay m aa m

Regulariser

Error Rate

L2
L1
Weight noise
Adaptive weight noise

27.4%
26.0%
25.4%
23.8%

• Allowed many weights to be pruned with little impact 
(even improvement!) on performance

Weights Pruned Error Rate

22.6%
54.8%
69.1%
88.5%

24.0%
23.5%
23.7%
24.5%

Weight matrix at different pruning 
thresholds: black=prune, white=keep

Multilinear Subspace Regression: 
An Or thogonal Tensor Decomposition Approach

Qibin Zhao1, Cesar F. Caiafa2, Danilo P. Mandic3, Liqing Zhang4, Tonio Ball5, Andreas 
Schulze-Bonhage5, and Andrzej Cichocki1 

1 Brain Science Institute, RIKEN, Japan
2 IAR, CONICET, Argentina 
3 Imperial College, UK
4 Shanghai Jiao Tong University, China
5 Albert-Ludwigs-University, Germany

NIPS 2011

Presented by Qibin Zhao

POSTER:  W043

LABSP: http://www.bsp.brain.riken.jp/

1

Multilinear regression and applications

! Tensor representation of multidimensional data 

• EEG, ECoG (spatial, temporal, frequency, epoch,...)

• Physical meaning - ease of interpretation 

! From multivariate to multi-way array processes - partial least squares (PLS)

Prediction of ECoG from 
scalp EEG recorded simultaneously
 

X = Scalp EEEG

Tensor regression model

Y = Intracranial ECoG

! Standard PLS applied on matricization of both X 
and Y

•
Small sample size problem
• Overﬁtting problem (high dimension of subspace basis)
•

Lack of physical interpretation for loadings

l
e
n
n
a
h
c
 
#

e p o c h

time-frequency

2

Proposed approach

Brain data Behavior data

Objective function

￿
￿
￿

, . . . , P(N −1) ]]

2

￿
￿
￿

+

￿
￿
￿

Y − [[D; t, Q(1)

, . . . , Q(M −1) ]]

2

￿
￿
￿

X − [[G; t, P(1)

min
{P(n) , Q(m) }
{P(n)T P(n) } = ILn+1 ,

s. t.

{Q(m)T Q(m) } = IKm+1 ,

X

Y

T

Latent variable

Extension of PLS to higher-order 
tensor data - HOPLS
• Goal: to predict a tensor Y from a 
tensor X
• Approach: to extract the common 
latent variables

Properties:
• Flexible multilinear regression 
framework
• Projection on tensor subspace 
basis
• Efficient optimization algorithm 
using HOOI on the n-mode cross-
covariance tensor

3

Key advantages
Small sample size

Robustness against overfitting and noise

HOPLS: better prediction 
performance and 
enhanced robustness to 
noise 

Stability of the performance of 
HOPLS, NPLS and PLS for a 
varying number of latent 
vectors under different noise 
conditions

POSTER:  W043

4

Sparse Filtering 
Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar & Andrew Y. Ng 

Extract 
Features 

Classifier 

Labeled 
Training Examples 

Unlabeled Examples 

Learned Features 

Sparse Filtering 
Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar & Andrew Y. Ng 

Why Sparse Filtering? 
 
• Easy, fast approach to feature learning 
 
• No hyper-parameters that need tuning 
 
• Easy to evaluate objective function 
 
• Minimal data preprocessing required 
 
• Trains well with off-the-shelf 
optimization toolboxes  
(e.g., L-BFGS). 

(cid:857) 
(cid:857) 
(cid:857) 
(cid:857) 

Classifier 

Features (2) 

Features (1) 

Input 

Sparse Filtering 
Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar & Andrew Y. Ng 

Examples 
x1  x2  x3  x4  (cid:857)  xm 
1.5  (cid:857) 
4 
0 
2 
0.5 

0 

0 

2.5 

0 

(cid:857) 

0 

(cid:857) 

3.2 

0 

1.6  0.3 

... 

1 

(cid:857) 

4 

0.5 

0 

1 

(cid:857) 

3 

 
s
e
u
l
a
V
 
e
r
u
t
a
e
F

f1 
f2 
(cid:857) 

f99 
(cid:857) 
fn 

Sparse Filtering  
Objective Function 
 
1. Normalize across rows 
 
2. Normalize across columns 
 
3. Cost Function =  
Sum of the normalized 
entries 

Sparse Filtering 
Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar & Andrew Y. Ng 

1st Layer Visualizations (STL Dataset) 

2nd Layer Visualizations (Natural Images) 

Evaluated sparse filtering features on natural images,  
image classification (STL Dataset), audio classification (TIMIT). 
  
Results comparable to state-of-the-art and fast! 

code available at http://cs.stanford.edu/~jngiam/ 

Directed Graph Embedding: an Algorithm based on
Continuous Limits of Laplacian-type Operators
Dominique Perrault-Joncas, Marina Meil˘a
University of Washington

Problem
Embed directed graph in euclidean
space

AND

Capture the directionality of the
graph

Model Schematic

Model

Observed

Recovered

Our
Algorithm

Directed Graph

Embedding
Sampling density
Vector field

Artiﬁcial Data

Model

Observed

Recovered

5000x5000 Asymmetric
adjacency matrix

Embedding
Sampling density
Vector ﬁeld

Main Contributions

1 Manifold-based generative model for directed graphs with
weighted edges.
2 Asymptotic results for diﬀusion operators constructed from
the directed graphs.
3 Natural algorithm for estimating the model.
4 Real Data:

Noise Thresholds for Spectral 
Clustering 

Sivaraman Balakrishnan 
Poster: W056 

  Min Xu                              Akshay Krishnamurthy                   Aarti Singh 
!
School of Computer Science!
Carnegie Mellon University!

Traditional analyses of Spectral 
Clustering 
!! k-way spectral clustering 

 

!! Compute L = D – W,                     

      smallest k eigenvectors of L 

!! Embed each data point i into k-dim space 

!! Run k-means on embedded data points  

 

  
High-level justification: Connection to graph cut, random walks 
on graph, electric network theory, Laplace-Beltrami operator on 
manifold – don’t translate to cluster recovery guarantees 

Perturbation Analysis: Rohe et. al. (2010) and McSherry (2001) 
– spectral algorithms for planted partition (structured random 
graph) model (constant block similarities, low rank)  

Jordan, Weiss (2001), Huang, Yan, Jordan, Taft (2009) – 
l2
eigenvectors are stable in    -norm (Davis-Kahan Theorem) 
under small similarity perturbations 

 

Our contributions – 1/2 

!! Study hierarchical spectral clustering and traditional k-way 
spectral clustering 

!! Characterization of general similarity conditions under which 
true eigenvectors reflect cluster structure, including 
eigenvectors of hierarchically-structured high-rank 
matrices 
 
l∞
!! Stability of eigenvectors in      -norm under sub-Gaussian 
perturbation 

!! Precise characterization of total clustering error of k-way 
and hierarchical spectral clustering 

!! As a function of noise variance, number of objects, size of 
clusters and within v/s between cluster similarity gap 

Our contributions – 2/2 

!! Information theoretic (minimax) optimality of signal-to-noise thresholds 
!! Minimax lower bound: No clustering method can succeed if 
σ = ω ￿γ￿ log n
n ￿
σ
     - Noise std. dev. of perturbation,    - number of objects 
n
γ /σ
     - Gap between inter and intra cluster similarity,        - SNR  
γ

 

!! Ratio min-cut (combinatorial) achieves this rate up to constants 
!! Spectral clustering succeeds if 
σ = o ￿γ
n ￿
4￿ log n
!! Price of computational efficiency: ratio min-cut (combinatorial)
outperforms spectral clustering (efficient) 

!! Remarks: 

!! Conjecture rate can be improved under different conditions on noise 


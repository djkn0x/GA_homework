Learning in Hilbert vs. Banach Spaces: A Measure
Embedding Viewpoint

Bharath K. Sriperumbudur
Gatsby Unit
University College London
bharath@gatsby.ucl.ac.uk

Kenji Fukumizu
The Institute of Statistical
Mathematics, Tokyo
fukumizu@ism.ac.jp

Gert R. G. Lanckriet
Dept. of ECE
UC San Diego
gert@ece.ucsd.edu

Abstract

The goal of this paper is to investigate the advantages and disadvantages of learn-
ing in Banach spaces over Hilbert spaces. While many works have been carried
out in generalizing Hilbert methods to Banach spaces, in this paper, we consider
the simple problem of learning a Parzen window classiﬁer in a reproducing kernel
f embedding prob-
Banach space (RKBS) —which is closely related to the notion o
ability measures into an RKBS—in order to carefully underst
and its pros and cons
over the Hilbert space classiﬁer. We show that while this gen eralization yields
richer distance measures on probabilities compared to its Hilbert space counter-
part, it however suffers from serious computational drawback limiting its practi-
cal applicability, which therefore demonstrates the need for developing efﬁcient
learning algorithms in Banach spaces.

1 Introduction

Kernel methods have been popular in machine learning and pattern analysis for their superior per-
formance on a wide spectrum of learning tasks. They are broadly established as an easy way to
construct nonlinear algorithms from linear ones, by embedding data points into reproducing kernel
Hilbert spaces (RKHSs) [1, 14, 15]. Over the last few years, generalization of these techniques to
Banach spaces has gained interest. This is because any two Hilbert spaces over a common scalar
ﬁeld with the same dimension are isometrically isomorphic w hile Banach spaces provide more va-
riety in geometric structures and norms that are potentially useful for learning and approximation.

To sample the literature, classiﬁcation in Banach spaces, m ore generally in metric spaces were stud-
ied in [3, 22, 11, 5]. Minimizing a loss function subject to a regularization condition on a norm
in a Banach space was studied by [3, 13, 24, 21] and online learning in Banach spaces was con-
sidered in [17]. While all these works have focused on theoretical generalizations of Hilbert space
methods to Banach spaces, the practical viability and inherent computational issues associated with
the Banach space methods has so far not been highlighted. The goal of this paper is to study the
advantages/disadvantages of learning in Banach spaces in comparison to Hilbert space methods, in
particular, from the point of view of embedding probability measures into these spaces.

The concept of embedding probability measures into RKHS [4, 6, 9, 16] provides a powerful and
straightforward method to deal with high-order statistics of random variables. An immediate appli-
cation of this notion is to problems of comparing distributions based on ﬁnite samples: examples
include tests of homogeneity [9], independence [10], and conditional independence [7]. Formally,
suppose we are given the set P (X ) of all Borel probability measures de ﬁned on the topological
space X , and the RKHS (H, k) of functions on X with k as its reproducing kernel (r.k.). If k is
measurable and bounded, then we can embed P in H as
P 7→ ZX

k(·, x) dP(x).

(1)

1

Given the embedding in (1), the RKHS distance between the embeddings of P and Q de ﬁnes a
pseudo-metric between P and Q as
γk (P, Q) := (cid:13)(cid:13)(cid:13)(cid:13)
k(·, x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)H
k(·, x) dP(x) − ZX
ZX
It is clear that when the embedding in (1) is injective, then P and Q can be distinguished based
k(·, x) dP(x) and RX
on their embeddings RX
k(·, x) dQ(x). [18] related RKHS embeddings to the
problem of binary classiﬁcation by showing that γk (P, Q) is the negative of the optimal risk associ-
ated with the Parzen window classiﬁer in H. Extending this classiﬁer to Banach space and studying
the highlights/issues associated with this generalization will throw light on the same associated with
more complex Banach space learning algorithms. With this motivation, in this paper, we consider
the generalization of the notion of RKHS embedding of probability measures to Banach spaces—in
particular reproducing kernel Banach spaces (RKBSs) [24] —
and then compare the properties of the
RKBS embedding to its RKHS counterpart.

(2)

.

(3)

To derive RKHS based learning algorithms, it is essential to appeal to the Riesz representation
theorem (as an RKHS is de ﬁned by the continuity of evaluation functionals), which establishes the
existence of a reproducing kernel. This theorem hinges on the fact that a notion of inner product can
be de ﬁned on Hilbert spaces. In this paper, as in [24], we deal with RKBSs that are uniformly Fr ´echet
differentiable and uniformly convex (called as s.i.p. RKBS) as many Hilbert space arguments—mos
t
importantly the Riesz representation theorem —can be carri
ed over to such spaces through the notion
of semi-inner-product (s.i.p.) [12], which is a more general structure than an inner product. Based
on Zhang et al. [24], who recently developed RKBS counterparts of RKHS based algorithms like
regularization networks, support vector machines, kernel principal component analysis, etc., we
provide a review of s.i.p. RKBS in Section 3. We present our main contributions in Sections 4 and
5. In Section 4, ﬁrst , we derive an RKBS embedding of P into B′ as
P 7→ ZX
K (·, x) dP(x),
where B is an s.i.p. RKBS with K as its reproducing kernel (r.k.) and B′ is the topological dual of
B. Note that (3) is similar to (1), but more general than (1) as K in (3) need not have to be positive
de ﬁnite (pd), in fact, not even symmetric (see Section 3; als o see Examples 2 and 3). Based on (3),
we de ﬁne
K (·, x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)B′
γK (P, Q) := (cid:13)(cid:13)(cid:13)(cid:13)
K (·, x) dP(x) − ZX
ZX
a pseudo-metric on P (X ), which we show to be the negative of the optimal risk associated with the
Parzen window classiﬁer in B′ . Second, we characterize the injectivity of (3) in Section 4.1 wherein
we show that the characterizations obtained for the injectivity of (3) are similar to those obtained for
(1) and coincide with the latter when B is an RKHS. Third, in Section 4.2, we consider the empirical
estimation of γK (P, Q) based on ﬁnite random samples drawn i.i.d. from P and Q and study its
consistency and the rate of convergence. This is useful in applications like two-sample tests (also in
binary classiﬁcation as it relates to the consistency of the Parzen window classiﬁer) where different
P and Q are to be distinguished based on the ﬁnite samples drawn from them and it is important that
the estimator is consistent for the test to be meaningful. We show that the consistency and the rate
of convergence of the estimator depend on the Rademacher type of B′ . This result coincides with
the one obtained for γk when B is an RKHS.
The above mentioned results, while similar to results obtained for RKHS embeddings, are signiﬁ-
cantly more general, as they apply RKBS spaces, which subsume RKHSs. We can therefore expect
to obtain “richer ” metrics
γK than when being restricted to RKHSs (see Examples 1 –3). On th e
other hand, one disadvantage of the RKBS framework is that γK (P, Q) cannot be computed in a
closed form unlike γk (see Section 4.3). Though this could seriously limit the practical impact of
the RKBS embeddings, in Section 5, we show that closed form expression for γK and its empirical
estimator can be obtained for some non-trivial Banach spaces (see Examples 1 –3). However, the
critical drawback of the RKBS framework is that the computation of γK and its empirical estima-
tor is signiﬁcantly more involved and expensive than the RKH S framework, which means a simple
kernel algorithm like a Parzen window classiﬁer, when gener alized to Banach spaces suffers from
a serious computational drawback, thereby limiting its practical impact. Given the advantages of
learning in Banach space over Hilbert space, this work, therefore demonstrates the need for the

,

2

development of efﬁcient algorithms in Banach spaces in orde r to make the problem of learning in
Banach spaces worthwhile compared to its Hilbert space counterpart. The proofs of the results in
Sections 4 and 5 are provided in the supplementary material.

2 Notation

We introduce some notation that is used throughout the paper. For a topological space X , C (X )
(resp. Cb (X )) denotes the space of all continuous (resp. bounded continuous) functions on X . For
a locally compact Hausdorff space X , f ∈ C (X ) is said to vanish at in ﬁnity if for every ǫ > 0 the
set {x : |f (x)| ≥ ǫ} is compact. The class of all continuous f on X which vanish at in ﬁnity is
denoted as C0 (X ). For a Borel measure µ on X , Lp(X , µ) denotes the Banach space of p-power
(p ≥ 1) µ-integrable functions. For a function f de ﬁned on Rd , ˆf and f ∨ denote the Fourier
and inverse Fourier transforms of f . Since ˆf and f ∨ on Rd can be de ﬁned in L1 , L2 or more
generally in distributional senses, they should be treated in the appropriate sense depending on the
context. In the L1 sense, the Fourier and inverse Fourier transforms of f ∈ L1 (Rd ) are de ﬁned as:
ˆf (y ) = (2π)−d/2 RRd f (x) e−ihy ,xi dx and f ∨ (y ) = (2π)−d/2 RRd f (x) eihy ,xi dx, where i denotes
the imaginary unit √−1. φP := RRd eih·,xi dP(x) denotes the characteristic function of P.
3 Preliminaries: Reproducing Kernel Banach Spaces

In this section, we brie ﬂy review the theory of RKBSs, which w as recently studied by [24] in the
context of learning in Banach spaces. Let X be a prescribed input space.
De ﬁnition 1 (Reproducing kernel Banach space). An RKBS B on X is a re ﬂexive Banach space of
functions on X such that its topological dual B′ is isometric to a Banach space of functions on X
and the point evaluations are continuous linear functionals on both B and B′ .
Note that if B is a Hilbert space, then the above de ﬁnition of RKBS coincide s with that of an RKHS.
Let (·, ·)B be a bilinear form on B × B′ wherein (f , g ∗)B := g ∗ (f ), f ∈ B, g ∗ ∈ B′ . Theorem 2 in
[24] shows that if B is an RKBS on X , then there exists a unique function K : X × X → C called
the reproducing kernel (r.k.) of B, such that the following hold:
(a1 ) K (x, ·) ∈ B, K (·, x) ∈ B′ , x ∈ X ,
(a2 ) f (x) = (f , K (·, x))B , f ∗ (x) = (K (x, ·), f ∗ )B , f ∈ B, f ∗ ∈ B′ , x ∈ X .
Note that K satisﬁes K (x, y ) = (K (x, ·), K (·, y ))B and therefore K (·, x) and K (x, ·) are reproduc-
ing kernels for B and B′ respectively. When B is an RKHS, K is indeed the r.k. in the usual sense.
Though an RKBS has exactly one r.k., different RKBSs may have the same r.k. (see Example 1) un-
like an RKHS, where no two RKHSs can have the same r.k (by the Moore-Aronszajn theorem [4]).
Due to the lack of inner product in B (unlike in an RKHS), it can be shown that the r.k. for a general
RKBS can be any arbitrary function on X × X for a ﬁnite set X [24]. In order to have a substitute for
inner products in the Banach space setting, [24] considered RKBS B that are uniformly Fr ´echet dif-
ferentiable and uniformly convex (referred to as s.i.p. RKBS) as it allows Hilbert space arguments to
be carried over to B—most importantly, an analogue to the Riesz representation
theorem holds (see
Theorem 3) —through the notion of
semi-inner-product (s.i.p.) introduced by [12]. In the following,
we ﬁrst present results related to general s.i.p. spaces and then consider s.i.p. RKBS.
De ﬁnition 2 (S.i.p. space). A Banach space B is said to be uniformly Fr ´echet differentiable if for
all f , g ∈ B, limt∈R,t→0 kf +tgkB−kf kB
exists and the limit is approached uniformly for f , g in the
t
unit sphere of B. B is said to be uniformly convex if for all ǫ > 0, there exists a δ > 0 such that
kf + gkB ≤ 2 − δ for all f , g ∈ B with kf kB = kgkB = 1 and kf − gkB ≥ ǫ. B is called an
s.i.p. space if it is both uniformly Fr ´echet differentiable and uniformly convex.
Note that uniform Fr ´echet differentiability and uniform c onvexity are properties of the norm associ-
ated with B. [8, Theorem 3] has shown that if B is an s.i.p. space, then there exists a unique function
[·, ·]B : B × B → C, called the semi-inner-product such that for all f , g , h ∈ B and λ ∈ C:
(a3 ) [f + g , h]B = [f , h]B + [g , h]B ,
(a4 ) [λf , g ]B = λ[f , g ]B , [f , λg ]B = λ[f , g ]B ,
(a5 ) [f , f ]B =: kf k2
B > 0 for f 6= 0,

3

(a6 ) (Cauchy-Schwartz) |[f , g ]B |2 ≤ kf k2
B ,
Bkgk2
= Re([g,f ]B )
and limt∈R,t→0 kf +tgkB−kf kB
, f , g ∈ B, f 6= 0, where Re(α) and α represent the
t
kf kB
real part and complex conjugate of a complex number α. Note that s.i.p. in general do not satisfy
conjugate symmetry, [f , g ]B = [g , f ]B for all f , g ∈ B and therefore is not linear in the second
argument, unless B is a Hilbert space, in which case the s.i.p. coincides with the inner product.
Suppose B is an s.i.p. space. Then for each h ∈ B, f 7→ [f , h]B de ﬁnes a continuous linear
functional on B, which can be identiﬁed with a unique element h∗ ∈ B′ , called the dual function of
h. By this de ﬁnition of h∗ , we have h∗ (f ) = (f , h∗ )B = [f , h]B , f , h ∈ B. Using the structure of
s.i.p., [8, Theorem 6] provided the following analogue in B to the Riesz representation theorem of
Hilbert spaces.
Theorem 3 ([8]). Suppose B is an s.i.p. space. Then
(a7 ) (Riesz representation theorem) For each g ∈ B′ , there exists a unique h ∈ B such that
g = h∗ , i.e., g (f ) = [f , h]B , f ∈ B and kgkB′ = khkB .
(a8 ) B′ is an s.i.p. space with respect to the s.i.p. de ﬁned by [h∗ , f ∗ ]B′ := [f , h]B , f , h ∈ B
and kh∗kB′ := [h∗ , h∗ ]1/2
B′ .
For more details on s.i.p. spaces, we refer the reader to [8]. A concrete example of an s.i.p. space
is as follows, which will prove to be useful in Section 5. Let (X , A , µ) be a measure space and
B := Lp (X , µ) for some p ∈ (1, +∞). It is an s.i.p. space with dual B′ := Lq (X , µ) where
p−1 . For each f ∈ B, its dual element in B′ is f ∗ = f |f |p−2
q = p
. Consequently, the semi-inner-
kf kp−2
Lp (X ,µ)
product on B is
[f , g ]B = g ∗ (f ) = RX
f g |g |p−2 dµ
kgkp−2
Lp(X ,µ)
Having introduced s.i.p. spaces, we now discuss s.i.p. RKBS which was studied by [24]. Using the
Riesz representation for s.i.p. spaces (see (a7 )), Theorem 9 in [24] shows that if B is an s.i.p. RKBS,
then there exists a unique r.k. K : X × X → C and a s.i.p. kernel G : X × X → C such that:
(a9 ) G(x, ·) ∈ B for all x ∈ X , K (·, x) = (G(x, ·))∗ , x ∈ X ,
(a10 ) f (x) = [f , G(x, ·)]B , f ∗(x) = [K (x, ·), f ]B for all f ∈ B, x ∈ X .
It is clear that G(x, y ) = [G(x, ·), G(y , ·)]B , x, y ∈ X . Since s.i.p. in general do not satisfy conju-
gate symmetry, G need not be Hermitian nor pd [24, Section 4.3]. The r.k. K and the s.i.p. kernel
G coincide when span{G(x, ·) : x ∈ X } is dense in B, which is the case when B is an RKHS [24,
Theorems 2, 10 and 11]. This means when B is an RKHS, then the conditions (a9 ) and (a10 ) reduce
to the well-known reproducing properties of an RKHS with the s.i.p. reducing to an inner product.

(4)

.

4 RKBS Embedding of Probability Measures

In this section, we present our main contributions of deriving and analyzing the RKBS embedding
of probability measures, which generalize the theory of RKHS embeddings. First, we would like to
remind the reader that the RKHS embedding in (1) can be derived by choosing F = {f : kf kH ≤ 1}
in
f ∈F (cid:12)(cid:12)(cid:12)(cid:12)
f dQ(cid:12)(cid:12)(cid:12)(cid:12) .
ZX
f dP − ZX
γF (P, Q) = sup
See [19, 20] for details. Similar to the RKHS case, in Theorem 4, we show that the RKBS embed-
dings can be obtained by choosing F = {f : kf kB ≤ 1} in γF (P, Q). Interestingly, though B does
not have an inner product, it can be seen that the structure of semi-inner-product is sufﬁcient enough
to generate an embedding similar to (1).
Theorem 4. Let B be an s.i.p. RKBS de ﬁned on a measurable space X with G as the s.i.p. kernel
and K as the reproducing kernel with both G and K being measurable. Let F = {f : kf kB ≤ 1}
and G be bounded. Then
γK (P, Q) := γF (P, Q) = (cid:13)(cid:13)(cid:13)(cid:13)
K (·, x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)B′
K (·, x) dP(x) − ZX
ZX
4

(5)

.

Based on Theorem 4,
it is clear that P can be seen as being embedded into B′ as P 7→
RX
K (·, x) dP(x) and γK (P, Q) is the distance between the embeddings of P and Q. Therefore,
we arrive at an embedding which looks similar to (1) and coincides with (1) when B is an RKHS.
Given these embeddings, two questions that need to be answered for these embeddings to be practi-
cally useful are: (⋆) When is the embedding injective? and (⋆⋆) Can γK (P, Q) in (5) be estimated
consistently and computed efﬁciently from ﬁnite random sam ples drawn i.i.d. from P and Q? The
signiﬁcance of ( ⋆) is that if (3) is injective, then such an embedding can be used to differentiate
between different P and Q, which can then be used in applications like two-sample tests to differen-
tiate between P and Q based on samples drawn i.i.d. from them if the answer to (⋆⋆) is afﬁrmative.
These questions are answered in the following sections.

Before that, we show how these questions are important in binary classiﬁcation. Following [18], it
can be shown that γK is the negative of the optimal risk associated with a Parzen window classiﬁer
in B′ , that separates the class-conditional distributions P and Q (refer to the supplementary material
for details). This means that if (3) is not injective, then the maximum risk is attained for P 6= Q, i.e.,
distinct distributions are not classiﬁable. Therefore, th e injectivity of (3) is of primal importance in
applications. In addition, the question in (⋆⋆) is critical as well, as it relates to the consistency of the
Parzen window classiﬁer.

4.1 When is (3) injective?

The following result provides various characterizations for the injectivity of (3), which are similar
(but more general) to those obtained for the injectivity of (1) and coincide with the latter when B is
an RKHS.
Theorem 5 (Injectivity of γK ). Suppose B is an s.i.p. RKBS de ﬁned on a topological space X with
K and G as its r.k. and s.i.p. kernel respectively. Then the following hold:
(a) Let X be a Polish space that is also locally compact Hausdorff. Suppose G is bounded and
K (x, ·) ∈ C0 (X ) for all x ∈ X . Then (3) is injective if B is dense in C0 (X ).
(b) Suppose the conditions in (a) hold. Then (3) is injective if B is dense in Lp (X , µ) for any Borel
probability measure µ on X and some p ∈ [1, ∞).
Since it is not easy to check for the denseness of B in C0 (X ) or Lp (X , µ), in Theorem 6, we present
an easily checkable characterization for the injectivity of (3) when K is bounded continuous and
translation invariant on Rd . Note that Theorem 6 generalizes the characterization (see [19, 20]) for
the injectivity of RKHS embedding (in (1)).
Theorem 6 (Injectivity of γK for translation invariant K ). Let X = Rd . Suppose K (x, y ) =
ψ(x − y ), where ψ : Rd → R is of the form ψ(x) = RRd eihx,ωi dΛ(ω ) and Λ is a ﬁnite complex-
valued Borel measure on Rd . Then (3) is injective if supp(Λ) = Rd . In addition if K is symmetric,
then the converse holds.
Remark 7. If ψ in Theorem 6 is a real-valued pd function, then by Bochner’s theorem, Λ has to be
real, nonnegative and symmetric, i.e., Λ(dω ) = Λ(−dω ). Since ψ need not be a pd function for K
to be a real, symmetric r.k. of B, Λ need not be nonnegative. More generally, if ψ is a real-valued
function on Rd , then Λ is conjugate symmetric, i.e., Λ(dω ) = Λ(−dω ). An example of a translation
invariant, real and symmetric (but not pd) r.k. that satisﬁe s the conditions of Theorem 6 can be
obtained with ψ(x) = (4x6 + 9x4 − 18x2 + 15) exp(−x2 ). See Example 3 for more details.
4.2 Consistency Analysis

Consider a two-sample test, wherein given two sets of random samples, {Xj }m
j=1 and {Yj }n
j=1
drawn i.i.d. from distributions P and Q respectively, it is required to test whether P = Q or not.
Given a metric, γK on P (X ), the problem can equivalently be posed as testing for γK (P, Q) = 0 or
not, based on {Xj }m
j=1 and {Yj }n
j=1 , in which case, γK (P, Q) is estimated based on these random
samples. For the test to be meaningful, it is important that this estimate of γK is consistent. [9]
showed that γK (Pm , Qn) is a consistent estimator of γK (P, Q) when B is an RKHS, where Pm :=
m Pm
n Pn
j=1 δXj , Qn := 1
1
j=1 δYj and δx represents the Dirac measure at x ∈ X . Theorem 9
generalizes the consistency result in [9] by showing that γK (Pm , Qn) is a consistent estimator of

5

γK (P, Q) and the rate of convergence is O(m(1−t)/t + n(1−t)/t ) if B′ is of type t, 1 < t ≤ 2. Before
we present the result, we de ﬁne the type of a Banach space, B [2, p. 303].
De ﬁnition 8 (Rademacher type of B). Let 1 ≤ t ≤ 2. A Banach space B is said to be of t-
Rademacher (or, more shortly, of type t) if there exists a constant C ∗ such that for any N ≥ 1
B )1/t ≤ C ∗ (PN
j=1 ⊂ B: (Ek PN
B )1/t , where {̺j }N
j=1 kfj kt
j=1 ̺j fj kt
and any {fj }N
j=1 are
i.i.d. Rademacher (symmetric ±1-valued) random variables.
Clearly, every Banach space is of type 1. Since having type t′ for t′ > t implies having type t, let us
de ﬁne t∗ (B) := sup{t : B has type t}.
Theorem 9 (Consistency of γK (Pm , Qn)). Let B be an s.i.p. RKBS. Assume ν := sup{pG(x, x) :
i.i.d.∼ P
x ∈ X } < ∞. Fix δ ∈ (0, 1). Then with probability 1−δ over the choice of samples {Xj }m
j=1
i.i.d.∼ Q, we have
and {Yj }n
j=1
|γK (Pm , Qn) − γK (P, Q)| ≤ 2C ∗ν (cid:16)m
t (cid:17) + p18ν 2 log(4/δ)(cid:16)m− 1
2 (cid:17),
1−t
2 + n− 1
where t = t∗ (B′ ) and C ∗ is some universal constant.
It is clear from Theorem 9 that if t∗ (B′ ) ∈ (1, 2], then γK (Pm , Qn ) is a consistent estimator of
γK (P, Q). In addition, the best rate is obtained if t∗ (B′ ) = 2, which is the case if B is an RKHS. In
Section 5, we will provide examples of s.i.p. RKBSs that satisfy t∗ (B′ ) = 2.

1−t
t + n

:=

4.3 Computation of γK (P, Q)
We now consider the problem of computing γK (P, Q) and γK (Pm , Qn ). De ﬁne λ∗P
RX
K (·, x) dP(x). Consider
(a3 )
(a5 )
= [λ∗P , λ∗P − λ∗Q ]B′ − [λ∗Q , λ∗P − λ∗Q ]B′
K (P, Q) = kλ∗P − λ∗Q k2
γ 2
= [λ∗P − λ∗Q , λ∗P − λ∗Q ]B′
B′
= h ZX
K (·, x) dP(x), λ∗P − λ∗Q iB′ − h ZX
K (·, x) dQ(x), λ∗P − λ∗Q iB′
[K (·, x), λ∗P − λ∗Q ]B′ dP(x) − ZX
= ZX
(∗)
[K (·, x), λ∗P − λ∗Q ]B′ dQ(x)
= ZX hK (·, x), ZX
K (·, y ) d(P − Q)(y )iB′
d(P − Q)(x),
where (∗) is proved in the supplementary material. (6) is not reducible as the s.i.p. is not linear in
the second argument unless B is a Hilbert space. This means γK (P, Q) is not representable in terms
of the kernel function, K (x, y ) unlike in the case of B being an RKHS, in which case the s.i.p. in
(6) reduces to an inner product providing
K (P, Q) = Z ZX
γ 2
K (x, y ) d(P − Q)(x) d(P − Q)(y ).
Since this issue holds for any P, Q ∈ P (X ), it also holds for Pm and Qn , which means γK (Pm , Qn )
cannot be computed in a closed form in terms of the kernel, K (x, y ) unlike in the case of an RKHS
where γK (Pm , Qn ) can be written as a simple V-statistic that depends only on K (x, y ) computed
at {Xj }m
j=1 and {Yj }n
j=1 . This is one of the main drawbacks of the RKBS approach where the
s.i.p. structure does not allow closed form representations in terms of the kernel K (also see [24]
where regularization algorithms derived in RKBS are not solvable unlike in an RKHS), and therefore
could limit its practical viability. However, in the following section, we present non-trivial examples
of s.i.p. RKBSs for which γK (P, Q) and γK (Pm , Qn ) can be obtained in closed forms.

(6)

5 Concrete Examples of RKBS Embeddings

In this section, we present examples of RKBSs and then derive the corresponding γK (P, Q) and
γK (Pm , Qn ) in closed forms. To elaborate, we present three examples that cover the spectrum:
Example 1 deals with RKBS (in fact a family of RKBSs induced by the same r.k.) whose r.k. is pd,
Example 2 with RKBS whose r.k. is not symmetric and therefore not pd and Example 3 with RKBS
whose r.k. is symmetric but not pd. These examples show that the Banach space embeddings result
in richer metrics on P (X ) than those obtained through RKHS embeddings.

6

Example 1 (K is positive de ﬁnite) . Let µ be a ﬁnite nonnegative Borel measure on Rd . Then for
any 1 < p < ∞ with q = p
p−1
u(t)eihx,ti dµ(t) : u ∈ Lp (Rd , µ), x ∈ Rd(cid:27) ,
p (Rd ) := (cid:26)fu (x) = ZRd
Bpd
is an RKBS with K (x, y ) = G(x, y ) = (µ(Rd ))(p−2)/p RRd e−ihx−y ,ti dµ(t) as the r.k. and
γK (P, Q) = (cid:13)(cid:13)(cid:13) ZRd
eihx,·i d(P − Q)(x)(cid:13)(cid:13)(cid:13)Lq (Rd ,µ)
= kφP − φQ kLq (Rd ,µ) .
First note that K is a translation invariant pd kernel on Rd as it is the Fourier transform of a
nonnegative ﬁnite Borel measure, µ, which follows from Bochner’s theorem. Therefore, though the
s.i.p. kernel and the r.k. of an RKBS need not be symmetric, the space in (7) is an interesting example
of an RKBS, which is induced by a pd kernel. In particular, it can be seen that many RKBSs (Bpd
p (Rd )
for any 1 < p < ∞) have the same r.k (ignoring the scaling factor which can be made one for any
p by choosing µ to be a probability measure). Second, note that Bpd
p is an RKHS when p = q = 2
and therefore (8) generalizes γk (P, Q) = kφP − φQ kL2 (Rd ,µ) . By Theorem 6, it is clear that γK in
(8) is a metric on P (Rd ) if and only if supp(µ) = Rd . Refer to the supplementary material for an
interpretation of Bpd
p (Rd ) as a generalization of Sobolev space [23, Chapter 10].
Example 2 (K is not symmetric). Let µ be a ﬁnite nonnegative Borel measure such that its moment-
generating function, i.e., Mµ (x) := RRd ehx,ti dµ(t) exists. Then for any 1 < p < ∞ with q = p
p−1
u(t)ehx,ti dµ(t) : u ∈ Lp (Rd , µ), x ∈ Rd(cid:27)
p (Rd ) := (cid:26)fu (x) = ZRd
Bns
is an RKBS with K (x, y ) = G(x, y ) = (Mµ (qx))(p−2)/p Mµ (x(q − 1) + y ) as the r.k. Suppose
P and Q are such that MP and MQ exist. Then γK (P, Q) = k RRd ehx,·i d(P − Q)(x)kLq (Rd ,µ) =
kMP − MQkLq (Rd ,µ) , which is the weighted Lq distance between the moment-generating functions
of P and Q. It is easy to see that if supp(µ) = Rd , then γK (P, Q) = 0 ⇒ MP = MQ a.e. ⇒ P =
Q, which means γK is a metric on P (Rd ). Note that K is not symmetric (for q 6= 2) and therefore
is not pd. When p = q = 2, K (x, y ) = Mµ (x + y ) is pd and Bns
p (Rd ) is an RKHS.
Example
3
de ﬁnite) .
(K is
symmetric
but
not
positive
Let
=
ψ(x)
Ae−x2 (cid:0)4x6 + 9x4 − 18x2 + 15(cid:1) with A := (1/243) (cid:0)4π2/25(cid:1)1/6 . Then
2 (R), x ∈ R(cid:27)
(R) := (cid:26)fu (x) = ZR
(x − t)2 e− 3(x−t)2
Bsnpd
3
u(t) dt : u ∈ L
2
3
2
is an RKBS with r.k. K (x, y ) = g (x, y ) = ψ(x − y ). Clearly, ψ and therefore K are not pd
34992√2 (cid:0)x6 − 39x4 + 216x2 − 324(cid:1) is not nonnegative at
(though symmetric on R) as bψ(x) = −e− x2
4
every x ∈ R. Refer to the supplementary material for the derivation of K and bψ . In addition,
γK (P, Q) = k RR θ(· − x) d(P − Q)(x)kLq (R) = k(bθ (φP − φQ ))∨ kLq (R) , where θ(t) = t2 e− 3
2 t2 .
Since supp(bθ) = R, we have γK (P, Q) = 0 ⇒ (bθ (φP − φQ ))∨ = 0 ⇒ bθ (φP − φQ ) = 0 ⇒ φP = φQ
a.e., which implies P = Q and therefore γK is a metric on P (R).
So far, we have presented different examples of RKBSs, wherein we have demonstrated the nature
of the r.k., derived the Banach space embeddings in closed form and studied the conditions under
which it is injective. These examples also show that the RKBS embeddings result in richer distance
measures on probabilities compared to those obtained by the RKHS embeddings—an advantage
gained by moving from Hilbert to Banach spaces. Now, we consider the problem of computing
γK (Pm , Qn ) in closed form and its consistency. In Section 4.3, we showed that γK (Pm , Qn ) does
not have a nice closed form expression unlike in the case of B being an RKHS. However, in the
following, we show that for K in Examples 1 –3, γK (Pm , Qn ) has a closed form expression for
certain choices of q . Let us consider the estimation of γK (P, Q):
K (Pm , Qn ) = (cid:13)(cid:13)(cid:13)(cid:13)
b(x, ·) d(Pm − Qn )(x)(cid:13)(cid:13)(cid:13)(cid:13)
ZX
= ZX (cid:12)(cid:12)(cid:12) ZX
b(x, t) d(Pm − Qn )(x)(cid:12)(cid:12)(cid:12)
q
γ q
Lq (X ,µ)
= ZX (cid:12)(cid:12)(cid:12)
b(Yj , t)(cid:12)(cid:12)(cid:12)
mXj=1
nXj=1
q
1
1
b(Xj , t) −
dµ(t),
m
n
7

dµ(t)

(9)

(7)

(8)

q

+

.

(10)

γ 2
K (Pm , Qn ) =

where b(x, t) = eihx,ti in Example 1, b(x, t) = ehx,ti in Example 2 and b(x, t) = θ(x − t) with
q = 3 and µ being the Lebesgue measure in Example 3. Since the duals of RKBSs considered
in Examples 1 –3 are of type min(q , 2) for 1 ≤ q ≤ ∞ [2, p. 304], by Theorem 9, γK (Pm , Qn )
max(1−q,−1)
max(1−q,−1)
) for q ∈
estimates γK (P, Q) consistently at a convergence rate of O(m
min(q,2) + n
min(q,2)
(1, ∞), with the best rate of O(m−1/2 + n−1/2 ) attainable when q ∈ [2, ∞). This means for
q ∈ (2, ∞), the same rate as attainable by the RKHS can be achieved. Now, the problem reduces
to computing γK (Pm , Qn ). Note that (9) cannot be computed in a closed form for all q —see the
discussion in the supplementary material about approximating γK (Pm , Qn). However, when q = 2,
(9) can be computed very efﬁciently in closed form (in terms o f K ) as a V-statistic [9], given by
mXj,l=1
nXl=1
mXj=1
nXj,l=1
K (Xj , Xl )
K (Yj , Yl )
− 2
m2
n2
More generally, it can be shown that if q = 2s, s ∈ N, then (9) reduces to
A(x1 ,...,xq )
z
}|
{
· · · ZX
K (Pm , Qn ) = ZX
ZX
qYj=1
sYj=1
q
γ q
d(Pm − Qn )(xj )
b(x2j−1 , t)b(x2j , t) dµ(t)
for which closed form computation is possible for appropriate choices of b and µ. Refer to
the supplementary material for the derivation of (11). For b and µ as in Example 1, we have
j=1 x2j (cid:17), while for b and µ as in Example 2,
p K (cid:16)Ps
j=1 x2j−1 , Ps
2−p
A(x1 , . . . , xq ) = (µ(Rd ))
we have A(x1 , . . . , xq ) = Mµ (Pq
j=1 xj ). By appropriately choosing θ and µ in Example 3, we
can obtain a closed form expression for A(x1 , . . . , xq ), which is proved in the supplementary mate-
rial. Note that choosing s = 1 in (11) results in (10). (11) shows that γ q
K (Pm , Qn) can be computed
in a closed form in terms of A at a complexity of O(mq ), assuming m = n, which means the least
complexity is obtained for q = 2. The above discussion shows that for appropriate choices of q ,
i.e., q ∈ (2, ∞), the RKBS embeddings in Examples 1 –3 are useful in practice a s γK (Pm , Qn) is
consistent and has a closed form expression. However, the drawback of the RKBS framework is that
the computation of γK (Pm , Qn ) is more involved than its RKHS counterpart.

K (Xj , Yl )
mn

(11)

6 Conclusion & Discussion

With a motivation to study the advantages/disadvantages of generalizing Hilbert space learning algo-
rithms to Banach spaces, in this paper, we generalized the notion of RKHS embedding of probability
measures to Banach spaces, in particular RKBS that are uniformly Fr ´echet differentiable and uni-
formly convex —note that this is equivalent to generalizing
a RKHS based Parzen window classiﬁer
to RKBS. While we showed that most of results in RKHS like injectivity of the embedding, con-
sistency of the Parzen window classiﬁer, etc., nicely gener alize to RKBS yielding richer distance
measures on probabilities, the generalized notion is less attractive in practice compared to its RKHS
counterpart because of the computational disadvantage associated with it. Since most of the existing
literature on generalizing kernel methods to Banach spaces deal with more complex algorithms than
a simple Parzen window classiﬁer that is considered in this p aper, we believe that most of these
algorithms may have limited practical applicability, though they are theoretically appealing. This,
therefore raises an important open problem of developing computationally efﬁcient Banach space
based learning algorithms.

Acknowledgments

The authors thank the anonymous reviewers for their constructive comments that improved the pre-
sentation of the paper. Part of the work was done while B. K. S. was a Ph. D. student at UC
San Diego. B. K. S. and G. R. G. L. acknowledge support from the National Science Foundation
(grants DMS-MSPA 0625409 and IIS-1054960). K. F. was supported in part by JSPS KAKENHI
(B) 22300098.

References
[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.

8

[2] B. Beauzamy. Introduction to Banach spaces and their Geometry. North-Holland, The Netherlands, 1985.
[3] K. Bennett and E. Bredensteiner. Duality and geometry in svm classi ﬁer. In Proc. 17th International
Conference on Machine Learning, pages 57–64, 2000.
[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.
Kluwer Academic Publishers, London, UK, 2004.
[5] R. Der and D. Lee. Large-margin classi ﬁcation in Banach s paces. In JMLR Workshop and Conference
Proceedings, volume 2, pages 91–98. AISTATS, 2007.
[6] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with repro-
ducing kernel Hilbert spaces. Journal of Machine Learning Research, 5:73–99, 2004.
[7] K. Fukumizu, A. Gretton, X. Sun, and B. Sch ¨olkopf. Kernel measures of conditional dependence. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems
20, pages 489–496, Cambridge, MA, 2008. MIT Press.
[8] J. R. Giles. Classes of semi-inner-product spaces. Trans. Amer. Math. Soc., 129:436–446, 1967.
[9] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch ¨olkopf, andA. Smola. A kernel method for the two sample
problem. In B. Sch ¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing
Systems 19, pages 513–520. MIT Press, 2007.
[10] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch ¨olkopf, and A. J. Smola. A kernel statistical test of
independence. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 585–592. MIT Press, 2008.
[11] M. Hein, O. Bousquet, and B. Sch ¨olkopf. Maximal margin classi ﬁcation for metric spaces.
System Sci., 71:333–359, 2005.
[12] G. Lumer. Semi-inner-product spaces. Trans. Amer. Math. Soc., 100:29–43, 1961.
[13] C. A. Micchelli and M. Pontil. A function representation for learning in Banach spaces. In Conference
on Learning Theory, 2004.
[14] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[15] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press,
UK, 2004.
[16] A. J. Smola, A. Gretton, L. Song, and B. Sch ¨olkopf. A Hilbert space embedding for distributions. In
Proc. 18th International Conference on Algorithmic Learning Theory, pages 13–31. Springer-Verlag,
Berlin, Germany, 2007.
[17] K. Sridharan and A. Tewari. Convex games in Banach spaces. In Conference on Learning Theory, 2010.
[18] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Sch ¨olkopf. Kernel choice
and classi ﬁability for RKHS embeddings of probability dist ributions.
In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems
22, pages 1750–1758. MIT Press, 2009.
[19] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. G. Lanckriet, and B. Sch ¨olkopf. Injective Hilbert
space embeddings of probability measures. In R. Servedio and T. Zhang, editors, Proc. of the 21st Annual
Conference on Learning Theory, pages 111–122, 2008.
[20] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch ¨olkopf, and G. R. G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:1517–1561,
2010.
[21] H. Tong, D.-R. Chen, and F. Yang. Least square regression with ℓp -coefﬁcient regularization. Neural
Computation, 22:3221–3235, 2010.
[22] U. von Luxburg and O. Bousquet. Distance-based classi ﬁ cation with Lipschitz functions. Journal for
Machine Learning Research, 5:669–695, 2004.
[23] H. Wendland. Scattered Data Approximation. Cambridge University Press, Cambridge, UK, 2005.
[24] H. Zhang, Y. Xu, and J. Zhang. Reproducing kernel Banach spaces for machine learning. Journal of
Machine Learning Research, 10:2741–2775, 2009.

J. Comput.

9


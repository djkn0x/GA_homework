Algorithms and hardness results
for parallel large margin learning

Philip M. Long
Google
plong@google.com

Rocco A. Servedio
Columbia University
rocco@cs.columbia.edu

Abstract

We study the fundamental problem of learning an unknown large-margin half-
space in the context of parallel computation.
Our main positive result is a parallel algorithm for learning a large-margin half-
space that is based on interior point methods from convex optimization and fast
parallel algorithms for matrix computations. We show that this algorithm learns
an unknown γ -margin halfspace over n dimensions using poly(n, 1/γ ) processors
and runs in time ˜O(1/γ ) + O(log n). In contrast, naive parallel algorithms that
learn a γ -margin halfspace in time that depends polylogarithmically on n have
Ω(1/γ 2 ) runtime dependence on γ .
Our main negative result deals with boosting, which is a standard approach to
learning large-margin halfspaces. We give an information-theoretic proof that in
the original PAC framework, in which a weak learning algorithm is provided as an
oracle that is called by the booster, boosting cannot be parallelized: the ability to
call the weak learner multiple times in parallel within a single boosting stage does
not reduce the overall number of successive stages of boosting that are required.

1

Introduction

In this paper we consider large-margin halfspace learning in the PAC model: there is a target half-
space f (x) = sign(w · x), where w is an unknown unit vector, and an unknown probability distribu-
tion D over the unit ball Bn = {x ∈ Rn : (cid:107)x(cid:107)2 ≤ 1} which has support on {x ∈ Bn : |w · x| ≥ γ }.
(Throughout this paper we refer to such a combination of target halfspace f and distribution D
as a γ -margin halfspace.) The learning algorithm is given access to labeled examples (x, f (x))
where each x is independently drawn from D , and it must with high probability output a hypothesis
h : Rn → {−1, 1} that satisﬁes Prx∼D [h(x) (cid:54)= f (x)] ≤ ε. Learning a large-margin halfspace is
a fundamental problem in machine learning; indeed, one of the most famous algorithms in machine
learning is the Perceptron algorithm [25] for this problem. PAC algorithms based on the Percep-
tron [17] run in poly(n, 1
ε ) time, use O( 1
εγ 2 ) labeled examples in Rn , and learn an unknown
γ , 1
n-dimensional γ -margin halfspace to accuracy 1 − ε.
A motivating question: achieving Perceptron’s performance in parallel? The last few years have
witnessed a resurgence of interest in highly efﬁcient parallel algorithms for a wide range of compu-
tational problems in many areas including machine learning [33, 32]. So a natural goal is to develop
an efﬁcient parallel algorithm for learning γ -margin halfspaces that matches the performance of the
Perceptron algorithm. A well-established theoretical notion of efﬁcient parallel computation is that
an efﬁcient parallel algorithm for a problem with input size N is one that uses poly(N ) processors
and runs in parallel time polylog(N ), see e.g. [12]. Since the input to the Perceptron algorithm is a
γ ) labeled examples in Rn , we naturally arrive at the following:
sample of poly( 1
ε , 1

1

Algorithm
naive parallelization of Perceptron
naive parallelization of [27]
polynomial-time linear programming [2]
This paper

Number of processors
poly(n, 1/γ )
poly(n, 1/γ )
1
poly(n, 1/γ )

Running time
˜O(1/γ 2 ) + O(log n)
˜O(1/γ 2 ) + O(log n)
poly(n, log(1/γ ))
˜O(1/γ ) + O(log n)

Table 1: Bounds on various parallel algorithms for learning a γ -margin halfspace over Rn .

Main Question: Is there a learning algorithm that uses poly(n, 1
ε ) processors
γ , 1
and runs in time poly(log n, log 1
γ , log 1
ε ) to learn an unknown n-dimensional γ -
margin halfspace to accuracy 1 − ε?
(See [31] for a detailed deﬁnition of parallel learning algorithms; here we only recall that an efﬁ-
cient parallel learning algorithm’s hypothesis must be efﬁciently evaluatable in parallel.) As Freund
[10] has largely settled how the resources required by parallel algorithms scale with the accuracy
parameter  (see Lemma 6 below), our focus in this paper is on γ and n, leading to the following:

Main Question (simpliﬁed): Is there a learning algorithm that uses poly(n, 1
γ )
processors and runs in time poly(log n, log 1
γ ) to learn an unknown n-dimensional
γ -margin halfspace to accuracy 9/10?

This question, which we view as a fundamental open problem, inspired the research reported here.
Prior results. Table 1 summarizes the running time and number of processors used by various par-
allel algorithms to learn a γ -margin halfspace over Rn . The naive parallelization of Perceptron in
the ﬁrst line of the table is an algorithm that runs for O(1/γ 2 ) stages; in each stage it processes
all of the O(1/γ 2 ) examples simultaneously in parallel, identiﬁes one that causes the Perceptron
algorithm to update its hypothesis vector, and performs this update. We do not see how to obtain
parallel time bounds better than O(1/γ 2 ) from recent analyses of other algorithms based of gra-
dient descent (such as [7, 8, 4]), some of which use assumptions incomparable in strength to the
γ -margin condition studied here. The second line of the table corresponds to a similar naive par-
allelization of the boosting-based algorithm of [27] that achieves Perceptron-like performance for
learning a γ -margin halfspace. It boosts for O(1/γ 2 ) stages over a O(1/γ 2 )-size sample; using one
processor for each coordinate of each example, the running time bound is ˜O(1/γ 2 ) · log n, using
poly(n, 1/γ ) processors. (For both this algorithm and the Perceptron the time bound can be im-
proved to ˜O(1/γ 2 ) + O(log n) as claimed in the table by using an initial random projection step;
we explain how to do this in Section 2.) The third line of the table, included for comparison, is
simply a standard sequential algorithm for learning a halfspace based on polynomial-time linear
programming executed on one processor, see e.g. [2, 14].
Efﬁcient parallel algorithms have been developed for some simpler PAC learning problems such
as learning conjunctions, disjunctions, and symmetric Boolean functions [31]. [6] gave efﬁcient
parallel PAC learning algorithms for some geometric constant-dimensional concept classes.
In terms of negative results for parallel learning, [31] shows that (under a complexity-theoretic
assumption) there is no parallel algorithm using poly(n) processors and polylog(n) time that
constructs a halfspace hypothesis that is consistent with a given linearly separable data set of n-
dimensional labeled examples. This does not give a negative answer to the Main Question for several
reasons: the Main Question allows any hypothesis representation (that can be efﬁciently evaluated in
parallel), allows the number of processors to grow inverse polynomially with the margin parameter
γ , and allows the ﬁnal hypothesis to err on up to (say) 5% of the points in the data set.
Our results. Our main positive result is a parallel algorithm that uses poly(n, 1/γ ) processors
to learn γ -margin halfspaces in parallel time ˜O(1/γ ) + O(log n) (see Table 1). We believe ours
is the ﬁrst algorithm that runs in time polylogarithmic in n and subquadratic in 1/γ . Our analy-
sis can be modiﬁed to establish similar positive results for other formulations of the large-margin
learning problem, including ones (see [28]) that have been tied closely to weak learnability (these
modiﬁcations are not presented due to space constraints). In contrast, our main negative result is an

2

information-theoretic argument that suggests that such positive parallel learning results cannot be
obtained by boosting alone. We show that if the weak learner must be called as an oracle, boosting
cannot be parallelized: any parallel booster must perform Ω(1/γ 2 ) sequential stages of boosting a
“black-box” γ -advantage weak learner in the worst case. This extends an earlier lower bound of
Freund [10] for standard (sequential) boosters that can only call the weak learner once per stage.

2 A parallel algorithm for learning γ -margin halfspaces over Bn

Our parallel algorithm is an amalgamation of existing tools from high-dimensional geometry, con-
vex optimization, parallel algorithms for linear algebra, and learning theory. Roughly speaking the
algorithm works as follows: given a data set of m = ˜O(1/γ 2 ) labeled examples from Bn × {−1, 1},
it begins by randomly projecting the examples down to d = ˜O(1/γ 2 ) dimensions. This essentially
preserves the geometry so the resulting d-dimensional labeled examples are still linearly separable
with margin Θ(γ ). The algorithm then uses a variant of a linear programming algorithm of Renegar
√
[24, 21] which, roughly speaking, solves linear programs with m constraints to high accuracy using
(essentially)
m stages of Newton’s method. Within Renegar’s algorithm we employ fast parallel
algorithms for linear algebra [22] to carry out each stage of Newton’s method in polylog(1/γ ) par-
allel time steps. This sufﬁces to learn the unknown halfspace to high constant accuracy (say 9/10);
to get a 1 − ε-accurate hypothesis we combine the above procedure with Freund’s approach [10]
for boosting accuracy that was mentioned in the introduction. The above sketch omits many details,
including crucial issues of precision in solving the linear programs to adequate accuracy. In the rest
of this section we address the necessary details in full and prove the following theorem:
Theorem 1 There is a parallel algorithm with the following performance guarantee: Let f , D deﬁne
an unknown γ -margin halfspace over Bn as described in the introduction. The algorithm is given as
input , δ > 0 and access to labeled examples (x, f (x)) that are drawn independently from D . It runs
in O(((1/γ )polylog(1/γ )+ log(n)) log(1/)+ log log(1/δ)) time, uses poly(n, 1/γ , 1/, log(1/δ))
processors, and with probability 1− δ it outputs a hypothesis h satisfying Prx∼D [h(x) (cid:54)= f (x)] ≤ ε.
We assume that the value of γ is “known” to the algorithm, since otherwise the algorithm can use a
standard “guess and check” approach trying γ = 1, 1/2, 1/4, etc., until it ﬁnds a value that works.
We ﬁrst describe the tools from the literature that are used in the algorithm.
Random projection. We say that a random projection matrix is a matrix A chosen uniformly from
{−1, 1}n×d . Given such an A and a unit vector w ∈ Rn (recall that the target halfspace f is
√
f (x) = sign(w · x)), let w(cid:48) denote (1/
d)wA. After transformation by A the distribution D over
Bn is transformed to a distribution D (cid:48) over Rd in the natural way: a draw x(cid:48) from D (cid:48) is obtained by
√
making a draw x from D and setting x(cid:48) = (1/
d)xA. We will use the following lemma from [1]:
Lemma 1 [1] Let f (x) = sign(w · x) and D deﬁne a γ -margin halfspace as de-
(cid:104)(cid:12)(cid:12)(cid:12) w(cid:48)
(cid:107)w(cid:48) (cid:107) · x(cid:48) (cid:12)(cid:12)(cid:12) < γ /2 or (cid:107)x(cid:48)(cid:107)2 > 2
(cid:105) ≤ γ 4 .
For d = O((1/γ 2 ) log(1/γ )), a random n × d projection
scribed in the introduction.
matrix A will with probability 99/100 induce D (cid:48) and w(cid:48) as described above such that
Prx(cid:48)∼D(cid:48)
(cid:16)
(cid:17)
Let F be the convex barrier function F (u) = (cid:80)d
Convex optimization. We recall some tools we will use from convex optimization over Rd [24, 3].
i=1 log
(we specify the values
1
be the Hessian of F at u, let ||v||u = (cid:112)vT H (u)v, and let n(u) = −H (u)−1 g(u) be the Newton
(ui−ai )(bi−ui )
− 1
. Let H (u)
ai < bi below). Let g(u) be the gradient of F at u; note that g(u)i = 1
bi−ui
ui−ai
step at u. For a linear subspace L of Rd , let F|L be the restriction of F to L, i.e. the function that
evaluates to F on L and ∞ everywhere else.
We will apply interior point methods to approximately solve problems of the following form, where
a1 , ..., ad , b1 , ..., bd ∈ [−2, 2], |bi − ai | ≥ 2 for all i, and L is a subspace of Rd :
minimize − u1 such that u ∈ L and ai ≤ xi ≤ bi for all i.
Let z ∈ Rd be the minimizer, and let opt be the optimal value of (1).

(1)

3

The algorithm we analyze minimizes Fη (u) def= −ηu1 + F|L (u) for successively larger values of η .
Let z(η) be the minimizer of Fη , let optη = Fη (z(η)), and let nη (u) be its Newton step. (To keep
the notation clean, the dependence on L is suppressed from the notation.)
As in [23], we periodically round intermediate solutions to keep the bit complexity under control.
The analysis of such rounding in [23] requires a problem transformation which does not preserve
the large-margin condition that we need for our analysis, so we give a new analysis, using tools
from [24], and a simpler algorithm. It is easier to analyze the effect of the rounding on the quality
of the solution than on the progress measure used in [24]. Fortunately, [3] describes an algorithm
that can go from an approximately optimal solution to a solution with a good measure of progress
while controlling the bit complexity of the output. The algorithm repeatedly ﬁnds the direction of
the Newton step, and then performs a line search to ﬁnd the approximately optimal step size.

Lemma 2 ([3, Section 9.6.4]) There is an algorithm Abt with the following property. Suppose for
any η > 0, Abt is given u with rational components such that Fη (u) − optη ≤ 2. Then after
constantly many iterations of Newton’s method and back-tracking line search, Abt returns an u+ that
(i) satisﬁes ||nη (u+ )||u+ ≤ 1/9; and (ii) has rational components that have bit-length bounded by a
polynomial in d, the bit length of u, and the bit length of the matrix A such that L = {v : Av = 0}.1

We analyze the following variant of the usual central path algorithm for linear programming, which
we call Acpr . It takes as input a precision parameter α and outputs the ﬁnal u(k) .
• Set η1 = 1, β = 1 + 1
√
√
√
and  =
d210d/α+2d+1 )(cid:101) .
1
(cid:100)2
8
2d
d(5d/α+
• Given u as input, run Abt starting with u to obtain u(1) such that ||nη1 (u(1) )||u(1) ≤ 1/9.
log(β ) (cid:101) perform the following steps (i)–(iv): (i) set ηk = β ηk−1 ;
• For k from 2 to 1 + (cid:100) log(4d/α)
(ii) set w(k) = u(k−1) + nηk (u(k−1) ) (i.e. do one step of Newton’s method); (iii) form r(k)
by rounding each component of w(k) to the nearest multiple of , and then projecting back
onto L; (iv) Run Abt starting with r(k) to obtain u(k) such that ||nηk (u(k) )||u(k) ≤ 1/9.
The following lemma, implicit2 in [3, 24], bounds the quality of the solutions in terms of the progress
measure ||nηk (u)||u .
u and −u1−opt ≤ 4d
≤ ||nηk (u)||2
Lemma 3 If u ∈ L and ||nηk (u)||u ≤ 1/9, then Fηk (u)−optηk
ηk
The following key lemma shows that rounding intermediate solutions does not do too much harm:
Lemma 4 For any k , if Fηk (w(k) ) ≤ optηk
+ 1/9, then Fηk (r(k) ) ≤ optηk
+ 1.
Proof: Fix k , and note that ηk = β k−1 ≤ 5d/α. We henceforth drop k from all notation.
First, we claim that
{|ai − wi |, |bi − wi |} ≥ 2−2η−2d−1/9 .
κ = min
(2)
(cid:1) − 2d − η .
1/9 ≤ η + 1/9. But minimizing each term of Fη separately, we get Fη (w) ≥ log (cid:0) 1
i
Let m = ((a1 + b1 )/2, ..., (ad + bd )/2). Since Fη (w) ≤ optη + 1/9, we have Fη (w) ≤ Fη (m) +
κ
Combining this with the previous inequality and solving for κ yields (2).
√
Since ||w − r|| ≤ 
d, recalling that  ≤
√
√
, we have
1
d210d/α+2d+1 )
√
d(5d/α+
2
{|ai − ri |, |bi − ri |} ≥ 2−2η−2d−1/9 − 
d ≥ 2−2η−2d−1 .
min
(3)
i
1We note for the reader’s convenience that λ(u) in [3] is the same as our ||n(u+ )||u+ . The analysis on pages
503-505 of [3] shows that a constant number of iterations sufﬁce. Each step is a projection of H (u)−1 g(u)
onto L, which can be seen to have bit-length bounded by a polynomial in the bit-length of u. Composing
polynomials constantly many times yields a polynomial, which gives the claimed bit-length bound for u+ .
2The ﬁrst inequality is (9.50) from [3]. The last line of p. 46 of [24] proves that ||nηk (u)||u ≤ 1/9 implies
||u − z(η)||z(η) ≤ 1/5 from which the second inequality follows by (2.14) of [24], using the fact that ϑ = 2d
(proved on page 35 of [24]).

.

4

(cid:16)

Now, deﬁne ψ : R → R by ψ(t) = Fη

(cid:17)
w + t r−w||r−w||
(cid:90) ||r−w||
. We have
|ψ (cid:48) (t)|.
ψ (cid:48) (t)dt ≤ ||r − w|| max
Fη (r) − Fη (w) = ψ(||r − w||) − ψ(0) =
(4)
t
0
Let S be the line segment between w and r. Since for each t ∈ [0, ||r − w||] the value ψ (cid:48) (t) is a
directional derivative of Fη at some point of S , (4) implies that, for the gradient gη of Fη ,
Fη (r) − Fη (w) ≤ ||w − r|| max{||gη (s)|| : s ∈ S }.
(5)
However (3) and (2) imply that min{|ai − si |, |bi − si |} ≥ 2−2η−2d−1 for all s ∈ S . Recalling that
√
, this means that ||gη (s)|| ≤ η +
− 1
g(u)i = 1
d22η+2d+1 so that applying (5) we get
√
√
bi−ui
ui−ai
d22η+2d+1 ). Since ||w − r|| ≤ 
Fη (r) − Fη (w) ≤ ||w − r||(η +
d, we have Fη (r) − Fη (w) ≤
√
√
√
√
d22η+2d+1 ) ≤ 
d210d/α+2d+1 ) ≤ 1/2, and the lemma follows.
d(η +
d(5d/α +


Fast parallel linear algebra: inverting matrices. We will use an algorithm due to Reif [22]:

Lemma 5 ([22]) There is a polylog(d, L)-time, poly(d, L)-processor parallel algorithm which,
given as input a d × d matrix A with rational entries of total bit-length L, outputs A−1 .
Learning theory: boosting accuracy. The following is implicit in the analysis of Freund [10].
Lemma 6 ([10]) Let D be a distribution over (unlabeled) examples. Let A be a parallel learning
algorithm such that for all D (cid:48) with support(D (cid:48) ) ⊆ support(D), given draws (x, f (x)) from D (cid:48) , with
probability 9/10 A outputs a hypothesis with accuracy 9/10 (w.r.t. D (cid:48) ) using P processors in T
time. Then there is a parallel algorithm B that with probability 1 − δ constructs a (1 − ε)-accurate
hypothesis (w.r.t. D) in O(T log(1/)+ log log(1/δ)) time using poly(P , 1/, log(1/δ)) processors.

2.1 Proof of Theorem 1

As described at the start of this section, due to Lemma 6, it sufﬁces to prove the lemma in the case
that  = 1/10 and δ = 1/10. We assume w.l.o.g. that γ = 1/integer.
Rd is obtained by (i) rounding each xi to the nearest integer multiple of 1/(4(cid:100)(cid:112)n/γ (cid:101)); then (ii)
The algorithm ﬁrst selects an n × d random projection matrix A where d = O(log(1/γ )/γ 2 ).
This deﬁnes a transformation ΦA : Bn → Rd as follows: given x ∈ Bn , the vector ΦA (x) ∈
√
i to the nearest multiple of 1/(8(cid:100)d/γ (cid:101)).
setting x(cid:48) = (1/2
d)xA; and ﬁnally (iii) rounding each x(cid:48)
Given x it is easy to compute ΦA (x) using O(n log(1/γ )/γ 2 ) processors in O(log(n/γ )) time. Let
D (cid:48) denote the distribution over Rd obtained by applying ΦA to D . Across all coordinates D (cid:48) is
√
supported on rational numbers with the same poly(1/γ ) common denominator. By Lemma 1, with
(cid:104)|x(cid:48) · (w(cid:48)/(cid:107)w(cid:48)(cid:107))| < γ (cid:48) def= γ /8 or (cid:107)x(cid:48)(cid:107)2 > 1
(cid:105) ≤ γ 4 .
d)wA, D (cid:48) ) satisﬁes
probability 99/100 over A, the target-distribution pair (w(cid:48) = (1/
Pr
x(cid:48)∼D(cid:48)
The algorithm next draws m = c log(1/γ )/γ 2 labeled training examples (ΦA (x), f (x)) from
D (cid:48) ; this can be done in O(log(n/γ )) time using O(n) · poly(1/γ ) processors as noted above.
It then applies Acpr to ﬁnd a d-dimensional halfspace h that classiﬁes all m examples correctly
(more on this below). By (6), with probability at least (say) 29/30 over the random draw of
(ΦA (x1 ), ym ), ..., (ΦA (xm ), ym ), we have that yt (w(cid:48) · ΦA (xt )) ≥ γ and ||ΦA (xt )|| ≤ 1 for all
t = 1, . . . , m. Now the standard VC bound for halfspaces [30] applied to h and D (cid:48) implies that
since h classiﬁes all m examples correctly, with overall probability at least 9/10 its accuracy is at
least 9/10 with respect to D (cid:48) , i.e. Prx∼D [h(ΦA (x)) (cid:54)= f (x)] ≤ 1/10. So the hypothesis h ◦ ΦA has
accuracy 9/10 with respect to D with probability 9/10 as required by Lemma 6.
It remains to justify the above claim about Acpr classifying all examples correctly, and analyze the
running time. More precisely we show that given m = O(log(1/γ )/γ 2 ) training examples in Bd
with rational components that all have coordinates with a common denominator that is poly(1/γ )
and are separable with a margin γ (cid:48) = γ /8, Acpr can be used to construct a d-dimensional halfspace
that classiﬁes them all correctly in ˜O(1/γ ) parallel time using poly(1/γ ) processors.

(6)

5

m , ym ) ∈ Bd × {−1, 1} satisfying the above conditions, we will apply algo-
Given (x(cid:48)
1 , y1 ), ..., (x(cid:48)
rithm Acpr to the following linear program, called LP, with α = γ (cid:48) /2: “minimize −s such that
yt (v · x(cid:48)
t ) − st = s and 0 ≤ st ≤ 2 for all t ∈ [m]; −1 ≤ vi ≤ 1 for all i ∈ [d]; and −2 ≤ s ≤ 2.”
Intuitively, s is the minimum margin over all examples, and st is the difference between each exam-
ple’s margin and s. The subspace L is deﬁned by the equality constraints yt (v · x(cid:48)
t ) − st = s.
Our analysis will conclude by applying the following lemma, with an initial solution of s = −1,
v = 0, and st = 1 for all t. (Note that u1 corresponds to s.)
Lemma 7 Given any d-dimensional linear program in the form (1), and an initial solution u ∈ L
such that min{|ui − ai |, |ui − bi |} ≥ 1 for all i, Algorithm Acpr approximates the optimal solution
√
to an additive ±α. It runs in
d · polylog(d/α) parallel time and uses poly(1/α, d) processors.

The LP constraints enforce that all examples are classiﬁed correctly with a margin of at least s. The
feasible solution in which v is w(cid:48)/||w(cid:48) ||, s equals γ (cid:48) and st = yt (v · x(cid:48)
t ) − s shows that the optimum
solution of LP has value at most −γ (cid:48) . So approximating the optimum to an additive ±α = ±γ (cid:48)/2
ensures that all examples are classiﬁed correctly, and it is enough to prove Lemma 7.
Proof of Lemma 7: First, we claim that, for all k , ||nηk (u(k) )||u(k) ≤ 1/9; given this, since the
ﬁnal value of ηk is at least 4d/α, Lemma 3 implies that the solution is α-close to optimal. We induct
on k . For k = 1, since initially mini {|ui − ai |, |ui − bi |} ≥ 1, we have F (u) ≤ 0, and, since η1 = 1
and u1 ≥ −1 we have Fη1 (u) ≤ 1 and optη1 ≥ −1. So we can apply Lemma 2 to get the base case.
Now, for the induction step, suppose ||nηk (u(k) )||u(k) ≤ 1/9. It then follows3 from [24, page 46]
that ||nηk+1 (w(k+1) )||w(k+1) ≤ 1/9. Next, Lemmas 3 and 4 imply that Fηk+1 (r(k+1) ) − optηk+1 ≤
1. Then Lemma 2 gives ||nηk+1 (u(k+1) )||u(k+1) ≤ 1/9 as required.
Next, we claim that the bit-length of all intermediate solutions is at most poly(d, 1/γ ). This holds for
r(k) , and follows for u(k) and w(k) because each of them is obtained from some r(k) by performing
a constant number of operations each of which blows up the bit length at most polynomially (see
Lemma 2). Since each intermediate solution has polynomial bit length, the matrix inverses can be
√
computed in polylog(d, 1/γ ) time using poly(d, 1/γ ) processors, by Lemma 5. The time bound
d log(d/α)) iterations.
then follows from the fact that there are at most O(

3 Lower bound for parallel boosting in the oracle model

Boosting is a widely used method for learning large-margin halfspaces. In this section we consider
the question of whether boosting algorithms can be efﬁciently parallelized. We work in the original
PAC learning setting [29, 16, 26] in which a weak learning algorithm is provided as an oracle that is
called by the boosting algorithm, which must simulate a distribution over labeled examples for the
weak learner. Our main result for this setting is that boosting is inherently sequential; being able to
to call the weak learner multiple times in parallel within a single boosting stage does not reduce the
overall number of sequential boosting stages that are required. In fact we show this in a very strong
sense, by proving that a boosting algorithm that runs arbitrarily many copies of the weak learner in
parallel in each stage cannot save even one stage over a sequential booster that runs the weak learner
just once in each stage. This lower bound is unconditional and information-theoretic.
Below we ﬁrst deﬁne the parallel boosting framework and give some examples of parallel boosters.
We then state and prove our lower bound on the number of stages required by parallel boosters. A
consequence of our lower bound is that Ω(log(1/ε)/γ 2 ) stages of parallel boosting are required in
order to boost a γ -advantage weak learner to achieve classiﬁcation accuracy 1 − ε no matter how
many copies of the weak learner are used in parallel in each stage.
Our deﬁnition of weak learning is standard in PAC learning, except that for our discussion it sufﬁces
to consider a single target function f : X → {−1, 1} over a domain X.

Deﬁnition 1 A γ -advantage weak learner L is an algorithm that is given access to a source of inde-
pendent random labeled examples drawn from an (unknown and arbitrary) probability distribution
3Noting that ϑ ≤ 2d [24, page 35].

6

P over labeled examples {(x, f (x))}x∈X . L must4 return a weak hypothesis h : X → {−1, 1} that
satisﬁes Pr(x,f (x))←P [h(x) = f (x)] ≥ 1/2 + γ . Such an h is said to have advantage γ w.r.t. P .
We ﬁx P to henceforth denote the initial distribution over labeled examples, i.e. P is a distribution
over {(x, f (x))}x∈X where the marginal distribution PX may be an arbitrary distribution over X.
Intuitively, a boosting algorithm runs the weak learner repeatedly on a sequence of carefully chosen
distributions to obtain a sequence of weak hypotheses, and combines the weak hypotheses to obtain a
ﬁnal hypothesis that has high accuracy under P . We give a precise deﬁnition below, but ﬁrst we give
some intuition to motivate our deﬁnition. In stage t of a parallel booster the boosting algorithm may
run the weak learner many times in parallel using different probability distributions. The probability
weight of a labeled example (x, f (x)) under a distribution constructed at the t-th stage of boosting
may depend on the values of all the weak hypotheses from previous stages and on the value of
f (x), but may not depend on any of the weak hypotheses generated by any of the calls to the weak
learner in stage t. No other dependence on x is allowed, since intuitively the only interface that
the boosting algorithm should have with each data point is through its label and the values of the
weak hypotheses from earlier stages. We further observe that since the distribution P is the only
source of labeled examples, a booster should construct the distributions at each stage by somehow
“ﬁltering” examples (x, f (x)) drawn from P based only on the value of f (x) and the values of the
weak hypotheses from previous stages. We thus deﬁne a parallel booster as follows:

Deﬁnition 2 (Parallel booster) A T -stage parallel boosting algorithm with N -fold parallelism is
deﬁned by T N functions {αt,k }t∈[T ],k∈[N ] and a (randomized) Boolean function h, where αt,k :
{−1, 1}(t−1)N +1 → [0, 1] and h : {−1, 1}T N → {−1, 1}. In the t-th stage of boosting the weak
learner is run N times in parallel. For each k ∈ [N ], the distribution Pt,k over labeled examples
that is given to the k-th run of the weak learner is as follows: a draw from Pt,k is made by drawing
(x, f (x)) from P and accepting (x, f (x)) as the output of the draw from Pt,k with probability
px = αt,k (h1,1 (x), . . . , ht−1,N (x), f (x)) (and rejecting it and trying again otherwise). In stage t,
for each k ∈ [N ] the booster gives the weak learner access to Pt,k as deﬁned above and the weak
learner generates a hypothesis ht,k that has advantage at least γ w.r.t. Pt,k .
After T stages, T N weak hypotheses {ht,k }t∈[T ],k∈[N ] have been obtained from the weak learner.
The ﬁnal hypothesis of the booster is H (x) := h(h1,1 (x), . . . , hT ,N (x)), and its accuracy is
minht,k Pr(x,f (x))←P [H (x) = f (x)], where the min is taken over all sequences of T N weak hy-
potheses subject to the condition that each ht,k has advantage at least γ w.r.t. Pt,k .

The parameter N above corresponds to the number of processors that the parallel booster is using;
we get a sequential booster when N = 1. Many of the most common PAC-model boosters in the
(cid:0)T
(cid:1) (cid:0) 1
2 + γ (cid:1)j (1/2 − γ )T −j
pothesis that is guaranteed to have error at most vote(γ , T ) def= (cid:80)(cid:98)T /2(cid:99)
literature are sequential boosters, such as [26, 10, 9, 11, 27, 5] and others.
In [10] Freund gave a
boosting algorithm and showed that after T stages of boosting, his algorithm generates a ﬁnal hy-
j=0
j
(see Theorem 2.1 of [10]). Freund also gave a matching lower bound by showing (see his Theo-
rem 2.4) that any T -stage sequential booster must have error at least as large as vote(γ , T ), and so
consequently any sequential booster that generates a (1 − ε)-accurate ﬁnal hypothesis must run for
Ω(log(1/ε)/γ 2 ) stages. Our Theorem 2 below extends this lower bound to parallel boosters.
Several parallel boosting algorithms have been given in the literature, including branching pro-
gram [20, 13, 18, 19] and decision tree [15] boosters. All of these boosters take Ω(log(1/ε)/γ 2 )
stages to learn to accuracy 1 − ε; our theorem below implies that any parallel booster must run for
Ω(log(1/ε)/γ 2 ) stages no matter how many parallel calls to the weak learner are made per stage.

Theorem 2 Let B be any T -stage parallel boosting algorithm with N -fold parallelism. Then for
any 0 < γ < 1/2, when B is used to boost a γ -advantage weak learner the resulting ﬁnal hypothesis
may have error as large as vote(γ , T ) (see the discussion after Deﬁnition 2).

We emphasize that Theorem 2 holds for any γ and any N that may depend on γ in an arbitrary way.

4The usual deﬁnition of a weak learner would allow L to fail with probability δ. This probability can be made
exponentially small by running L multiple times so for simplicity we assume there is no failure probability.

7

The theorem is proved as follows: ﬁx any 0 < γ < 1/2 and ﬁx B to be any T -stage parallel
boosting algorithm. We will exhibit a target function f and a distribution P over {(x, f (x))x∈X ,
and describe a strategy that a weak learner W can use to generate weak hypotheses ht,k that each
have advantage at least γ with respect to the distributions Pt,k . We show that with this weak learner
W , the resulting ﬁnal hypothesis H that B outputs will have accuracy at most 1 − vote(γ , T ).
We begin by describing the desired f and P . The domain X of f is X = Z × Ω, where Z =
{−1, 1}n and Ω is the set of all ω = (ω1 , ω2 , . . . ) where each ωi belongs to {−1, 1}. The target
function f is simply f (z , ω) = z . The distribution P = (P X , P Y ) over {(x, f (x))}x∈X is deﬁned
as follows. A draw from P is obtained by drawing x = (z , ω) from P X and returning (x, f (x)). A
draw of x = (z , ω) from P X is obtained by ﬁrst choosing a uniform random value in {−1, 1} for z ,
and then choosing ωi ∈ {−1, 1} to equal z with probability 1/2 + γ independently for each i. Note
that under P , given the label z = f (x) of a labeled example (x, f (x)), each coordinate ωi of x is
correct in predicting the value of f (x, z ) with probability 1/2 + γ independently of all other ωj ’s.
We next describe a way that a weak learner W can generate a γ -advantage weak hypothesis each
time it is invoked by B . Fix any t ∈ [T ] and any k ∈ [N ]. When W is invoked with Pt,k it replies as
follows (recall that for x ∈ X we have x = (z , ω) as described above): (i) if Pr(x,f (x))←Pt,k [ωt =
f (x)] ≥ 1/2 + γ then the weak hypothesis ht,k (x) is the function “ωt ,” i.e. the (t + 1)-st coordinate
of x. Otherwise, (ii) the weak hypothesis ht,k (x) is “z ,” i.e.
the ﬁrst coordinate of x. (Note that
since f (x) = z for all x, this weak hypothesis has zero error under any distribution.)
It is clear that each weak hypothesis ht,k generated as described above indeed has advantage at least
γ w.r.t. Pt,k , so the above is a legitimate strategy for W . The following lemma will play a key role:
Lemma 8 If W never uses option (ii) then Pr(x,f (x))←P [H (x) (cid:54)= f (x)] ≥ vote(γ , T ).
Proof: If the weak learner never uses option (ii) then H depends only on variables ω1 , . . . , ωT and
hence is a (randomized) Boolean function over these variables. Recall that for (x = (z , ω), f (x) =
z ) drawn from P , each coordinate ω1 , . . . , ωT independently equals z with probability 1/2 + γ .
Hence the optimal (randomized) Boolean function H over inputs ω1 , . . . , ωT that maximizes the
accuracy Pr(x,f (x))←P [H (x) = f (x)] is the (deterministic) function H (x) = Maj(ω1 , . . . , ωT ) that
outputs the majority vote of its input bits. (This can be easily veriﬁed using Bayes’ rule in the usual
“Naive Bayes” calculation.) The error rate of this H is precisely the probability that at most (cid:98)T /2(cid:99)
“heads” are obtained in T independent (1/2 + γ )-biased coin tosses, which equals vote(γ , T ).

Thus it sufﬁces to prove the following lemma, which we prove by induction on t:
Lemma 9 W never uses option (ii) (i.e. Pr(x,f (x))←Pt,k [ωt = f (x)] ≥ 1/2 + γ always).
Proof: Base case (t = 1). For any k ∈ [N ], since t = 1 there are no weak hypotheses from
previous stages, so the value of px is determined by the bit f (x) = z (see Deﬁnition 2). Hence P1,k
is a convex combination of two distributions which we call D1 and D−1 . For b ∈ {−1, 1}, a draw
of (x = (z , ω); f (x) = z ) from Db is obtained by setting z = b and independently setting each
coordinate ωi equal to z with probability 1/2 + γ . Thus in the convex combination P1,k of D1 and
D−1 , we also have that ω1 equals z (i.e. f (x)) with probability 1/2 + γ . So the base case is done.
Inductive step (t > 1). Fix any k ∈ [N ]. The inductive hypothesis and the weak learner’s strategy
together imply that for each labeled example (x = (z , ω), f (x) = z ), since hs,(cid:96) (x) = ωs for
s < t, the rejection sampling parameter px = αt,k (h1,1 (x), . . . , ht−1,N (x), f (x)) is determined
by ω1 , . . . , ωt−1 and z and does not depend on ωt , ωt+1 , .... Consequently the distribution Pt,k
over labeled examples is some convex combination of 2t distributions which we denote Db , where b
ranges over {−1, 1}t corresponding to conditioning on all possible values for ω1 , . . . , ωt−1 , z . For
each b = (b1 , . . . , bt ) ∈ {−1, 1}t , a draw of (x = (z , ω); f (x) = z ) from Db is obtained by setting
z = bt , setting (ω1 , . . . , ωt−1 ) = (b1 , . . . , bt−1 ), and independently setting each other coordinate
ωj (j ≥ t) equal to z with probability 1/2 + γ . In particular, because ωt is conditionally independent
of ω1 , ..., ωt−1 given z , Pr(ωt = z |ω1 = b1 , ..., ωt−1 = bt−1 ) = Pr(ωt = z ) = 1/2 + γ . Thus
in the convex combination Pt,k of the different Db ’s, we also have that ωt equals z (i.e. f (x)) with
probability 1/2 + γ . This concludes the proof of the lemma and the proof of Theorem 2.

8

ICML,

References
[1] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random projection.
In Proc. 40th FOCS, pages 616–623, 1999.
[2] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the Vapnik-Chervonenkis
dimension. Journal of the ACM, 36(4):929–965, 1989.
[3] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge, 2004.
[4] J. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel coordinate descent for l1-regularized loss
minimization. ICML, 2011.
[5] Joseph K. Bradley and Robert E. Schapire. Filterboost: Regression and classiﬁcation on large datasets.
In NIPS, 2007.
[6] N. Bshouty, S. Goldman, and H.D. Mathias. Noise-tolerant parallel learning of geometric concepts. Inf.
and Comput., 147(1):89 – 110, 1998.
[7] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, adaboost and bregman
distances. Machine Learning, 48(1-3):253–285, 2002.
[8] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction.
2011.
[9] C. Domingo and O. Watanabe. MadaBoost: a modiﬁed version of AdaBoost. In Proc. 13th COLT, pages
180–189, 2000.
[10] Y. Freund. Boosting a weak learning algorithm by majority. Inf. and Comput., 121(2):256–285, 1995.
[11] Y. Freund. An adaptive version of the boost-by-majority algorithm. Mach. Learn., 43(3):293–318, 2001.
[12] R. Greenlaw, H.J. Hoover, and W.L. Ruzzo. Limits to Parallel Computation: P-Completeness Theory.
Oxford University Press, New York, 1995.
[13] A. Kalai and R. Servedio. Boosting in the presence of noise. Journal of Computer & System Sciences,
71(3):266–290, 2005.
[14] N. Karmarkar. A new polynomial time algorithm for linear programming. Combinat., 4:373–395, 1984.
[15] M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In
Proceedings of the Twenty-Eighth Annual Symposium on Theory of Computing, pages 459–468, 1996.
[16] M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge,
MA, 1994.
[17] N. Littlestone. From online to batch learning. In Proc. 2nd COLT, pages 269–284, 1989.
[18] P. Long and R. Servedio. Martingale boosting. In Proc. 18th Annual COLT, pages 79–94, 2005.
[19] P. Long and R. Servedio. Adaptive martingale boosting. In Proc. 22nd NIPS, pages 977–984, 2008.
[20] Y. Mansour and D. McAllester. Boosting using branching programs. Journal of Computer & System
Sciences, 64(1):103–112, 2002.
[21] Y. Nesterov and A. Nemirovskii. Interior Point Polynomial Methods in Convex Programming: Theory
and Applications. Society for Industrial and Applied Mathematics, Philadelphia, 1994.
[22] John H. Reif. O(log2 n) time efﬁcient parallel factorization of dense, sparse separable, and banded matri-
ces. SPAA, 1994.
[23] J. Renegar. A polynomial-time algorithm, based on Newton’s method, for linear programming. Mathe-
matical Programming, 40:59–93, 1988.
[24] James Renegar. A mathematical view of interior-point methods in convex optimization. Society for
Industrial and Applied Mathematics, 2001.
[25] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and organization in the brain.
Psychological Review, 65:386–407, 1958.
[26] R. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990.
[27] R. Servedio. Smooth boosting and learning with malicious noise. JMLR, 4:633–648, 2003.
[28] S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: New
relaxations and efﬁcient boosting algorithms. Machine Learning, 80(2):141–163, 2010.
[29] L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.
[30] V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.
[31] J. S. Vitter and J. Lin. Learning in parallel. Inf. Comput., 96(2):179–202, 1992.
[32] DIMACS 2011 Workshop. Parallelism: A 2020 Vision. 2011.
[33] NIPS 2009 Workshop. Large-Scale Machine Learning: Parallelism and Massive Datasets. 2009.

9


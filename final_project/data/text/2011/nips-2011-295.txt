MAP Inference for
Bayesian Inverse Reinforcement Learning

Jaedeug Choi and Kee-Eung Kim
bDepartment of Computer Science
Korea Advanced Institute of Science and Technology
Daejeon 305-701, Korea
jdchoi@ai.kaist.ac.kr, kekim@cs.kaist.ac.kr

Abstract

The difﬁculty in inverse reinforcement learning (IRL) aris es in choosing the best
reward function since there are typically an inﬁnite number of reward functions
that yield the given behaviour data as optimal. Using a Bayesian framework, we
address this challenge by using the maximum a posteriori (MAP) estimation for
the reward function, and show that most of the previous IRL algorithms can be
modeled into our framework. We also present a gradient metho d for the MAP es-
timation based on the (sub)differentiability of the posterior distribution. We show
the effectiveness of our approach by comparing the performa nce of the proposed
method to those of the previous algorithms.

1

Introduction

The objective of inverse reinforcement learning (IRL) is to determine the decision making agent’s
underlying reward function from its behaviour data and the m odel of environment [1]. The signi ﬁ-
cance of IRL has emerged from problems in diverse research ar eas. In animal and human behaviour
studies [2], the agent’s behaviour could be understood by the reward function since the reward func-
tion reﬂects the agent’s objectives and preferences. In rob otics [3], IRL provides a framework for
making robots learn to imitate the demonstrator’s behaviou r using the inferred reward function.
In other areas related to reinforcement learning, such as ne uroscience [4] and economics [5], IRL
addresses the non-trivial problem of ﬁnding an appropriate
reward function when building a com-
putational model for decision making.

In IRL, we generally assume that the agent is an expert in the problem domain and hence it be-
haves optimally in the environment. Using the Markov decision process (MDP) formalism, the IRL
problem is deﬁned as ﬁnding the reward function that the expe
rt is optimizing given the behaviour
data of state-action histories and the environment model of state transition probabilities. In the last
decade, a number of studies have addressed IRL in a direct (reward learning) and indirect (policy
learning by inferring the reward function, i.e., apprenticeship learning) fashions. Ng and Russell [6]
proposed a sufﬁcient and necessary condition on the reward f unctions that guarantees the optimality
of the expert’s policy and formulated a linear programming (LP) problem to ﬁnd the reward func-
tion from the behaviour data. Extending their work, Abbeel and Ng [7] presented an algorithm for
ﬁnding the expert’s policy from its behaviour data with a per formance guarantee on the learned pol-
icy. Ratliff et al. [8] applied the structured max-margin optimization to IRL and proposed a method
for ﬁnding the reward function that maximizes the margin bet ween the expert’s policy and all other
policies. Neu and Szepesv ´ari [9] provided an algorithm for ﬁnding the policy that mini mizes the
deviation from the behaviour. Their algorithm uni ﬁes the di rect method that minimizes a loss func-
tion of the deviation and the indirect method that ﬁnds an opt
imal policy from the learned reward
function using IRL. Syed and Schapire [10] proposed a method to ﬁnd a policy that improves the
expert’s policy using a game-theoretic framework. Ziebart et al. [11] adopted the principle of the

1

maximum entropy for learning the policy whose feature expec tations are constrained to match those
of the expert’s behaviour. In addition, Neu and Szepesv ´ari [12] provided a (non-Bayesian) uni ﬁed
view for comparing the similarities and differences among p revious IRL algorithms.

IRL is an inherently ill-posed problem since there may be an inﬁnite number of reward functions
that yield the expert’s policy as optimal. Previous approaches summarized above employ various
preferences on the reward function to address the non-uniqu eness. For example, Ng and Russell [6]
search for the reward function that maximizes the differenc e in the values of the expert’s policy and
the second best policy. More recently, Ramachandran and Amir [13] presented a Bayesian approach
formulating the reward preference as the prior and the behav iour compatibility as the likelihood, and
proposed a Markov chain Monte Carlo (MCMC) algorithm to ﬁnd t he posterior mean of the reward
function.

In this paper, we propose a Bayesian framework subsuming mos t of the non-Bayesian IRL algo-
rithms in the literature. This is achieved by searching for t he maximum-a-posteriori (MAP) reward
function, in contrast to computing the posterior mean. We show that the posterior mean can be prob-
lematic for the reward inference since the loss function is integrated over the entire reward space,
even including those inconsistent with the behaviour data. Hence, the inferred reward function can
induce a policy much different from the expert’s policy. The MAP estimate, however, is more ro-
bust in the sense that the objective function (the posterior probability in our case) is evaluated on
a single reward function. In order to ﬁnd the MAP reward funct
ion, we present a gradient method
using the differentiability result of the posterior, and show the effectiveness of our approach through
experiments.

2 Preliminaries

2.1 MDPs

A Markov decision process (MDP) is deﬁned as a tuple
hS, A, T , R, γ , αi: S is the ﬁnite set of
states; A is the ﬁnite set of actions; T is the state transition function where T (s, a, s′ ) denotes the
probability P (s′ |s, a) of changing to state s′ from state s by taking action a; R is the reward function
where R(s, a) denotes the immediate reward of executing action a in state s, whose absolute value
is bounded by Rmax ; γ ∈ [0, 1) is the discount factor; α is the initial state distribution where
α(s) denotes the probability of starting in state s. Using matrix notations, the transition function is
denoted as an |S ||A| × |S | matrix T , and the reward function is denoted as an |S ||A|-dimensional
vector R.

A policy is deﬁned as a mapping π : S → A. The value of policy π is the expected discounted
return of executing the policy and deﬁned as V π = E [P∞
t=0 γ tR(st , at )|α, π ] where the initial
state s0 is determined according to initial state distribution α and action at is chosen by policy π
in state st . The value function of policy π for each state s is computed by V π (s) = R(s, π(s)) +
γ Ps′∈S T (s, π(s), s′ )V π (s′ ) such that the value of policy π is calculated by V π = Ps α(s)V π (s).
Similarly, the Q-function is deﬁned as Qπ (s, a) = R(s, a) + γ Ps′∈S T (s, a, s′ )V π (s′ ). We can
rewrite the equations for the value function and the Q-function in matrix notations as

V π = Rπ + γT π V π , Qπ
a = Ra + γT aV π

(1)

where T π is an |S | × |S | matrix with the (s, s′ ) element being T (s, π(s), s′ ), T a is an |S | × |S |
matrix with the (s, s′ ) element being T (s, a, s′ ), Rπ is an |S |-dimensional vector with the s-th
element being R(s, π(s)), Ra is an |S |-dimensional vector with the s-th element being R(s, a), and
Qπ
a is an |S |-dimensional vector with the s-th element being Qπ (s, a).
An optimal policy π∗ maximizes the value function for all the states, and thus should satisfy
the Bellman optimality equation: π is an optimal policy if and only if for all s ∈ S , π(s) ∈
argmaxa∈A Qπ (s, a). We denote V ∗ = V π∗ and Q∗ = Qπ∗ .
When the state space is large, the reward function is often linearly parameterized: R(s, a) =
Pd
: S × A → R and the weight vector w =
i=1 wiφi (s, a) with the basis functions φi
[w1 , w2 , · · · , wd ]. Each basis function φi has a corresponding basis value V π
i of policy π : V π
i =
E [P∞
t=0 γ tφi (st , at )|α, π ].

2

We also assume that the expert’s behaviour is given as the set X of M trajectories executed by
the expert’s policy πE , where the m-th trajectory is an H -step sequence of state-action pairs:
H )}. Given the set of trajectories, the value and the basis value
{(sm
1 , am
1 ), (sm
2 , am
2 ), · · · , (sm
H , am
of the expert’s policy πE can be empirically estimated by
ˆV E = 1
ˆV E
m=1 PH
M PM
m=1 PH
M PM
i = 1
h=1 γ h−1φi (sm
h=1 γ h−1R(sm
h , am
h , am
h ),
h ).
In addition, we can empirically estimate the expert’s policy ˆπE and its state visitation frequency ˆµE
from the trajectories:
ˆπE (s, a) = PM
m=1 PH
1(sm
h =s∧am
h =a)
h=1
m=1 PH
PM
h=1
In the rest of the paper, we use the notation f (R) or f (x; R) for function f in order to be explicit
that f is computed using reward function R. For example, the value function V π (s; R) denotes the
value of policy π for state s using reward function R.

1
m=1 PH
M H PM
h=1

h =s) .
1(sm

,

ˆµE (s) =

1(sm
h =s)

2.2 Reward Optimality Condition

Ng and Russell [6] presented a necessary and sufﬁcient condi
tion for reward function R of an MDP
to guarantee the optimality of policy π : Qπ
a (R) ≤ V π (R) for all a ∈ A. From the condition,
we obtain the following corollary (although it is a succinct reformulation of the theorem in [6], the
proof is provided in the supplementary material).

Corollary 1 Given an MDP\R hS, A, T , γ , αi, policy π is optimal if and only if reward function R
satis ﬁes

hI − (I A − γT )(I − γT π )−1E π i R ≤ 0,
where E π is an |S | × |S ||A| matrix with the (s, (s′ , a′ )) element being 1 if s = s′ and π(s′ ) = a′ ,
and I A is an |S ||A| × |S | matrix constructed by stacking the |S | × |S | identity matrix |A| times.

(2)

We refer to Equation (2) as the reward optimality condition w.r.t. policy π . Since the linear in-
equalities deﬁne the region of the reward functions that yie ld policy π as optimal, we refer to the
region bounded by Equation (2) as the reward optimality region w.r.t. policy π . Note that there ex-
ist inﬁnitely many reward functions in the reward optimalit y region even including constant reward
functions (e.g. R = c1 where c ∈ [−Rmax , Rmax ]). In other words, even when we are presented
with the expert’s policy, there are inﬁnitely many reward fu nctions to choose from, including the de-
generate ones. To resolve this non-uniqueness in solutions, IRL algorithms in the literature employ
various preferences on reward functions.

2.3 Bayesian framework for IRL (BIRL)

Ramachandran and Amir [13] proposed a Bayesian framework fo r IRL by encoding the reward
function preference as the prior and the optimality conﬁden ce of the behaviour data as the likelihood.
We refer to their work as BIRL.

Assuming the rewards are i.i.d., the prior in BIRL is computed by
P (R) = Qs∈S,a∈A P (R(s, a)).
Various distributions can be used as the prior. For example, the uniform prior can be used if we have
no knowledge about the reward function other than its range, and a Gaussian or a Laplacian prior
can be used if we prefer rewards to be close to some speci ﬁc val ues.

(3)

The likelihood in BIRL is deﬁned as an independent exponenti al distribution analogous to the soft-
max function:

P (X |R) =

M
Y
m=1

H
Y
h=1

h |sm
P (am
h , R) =

M
Y
m=1

H
Y
h=1

h , am
exp(βQ∗ (sm
h ; R))
Pa∈A exp(βQ∗ (sm
h , a; R))

(4)

3

)
X
|
)
5
s
(
R
,
)
1
s
(
R
(
P

0.04

0.02

0
1

0.5

R(s5 )

0

0

0.2

0.4

R(s1 )

0.6

0.8

1

(b)
(a)
Figure 1: (a) 5-state chain MDP. (b) Posterior for R(s1 ) and R(s5 ) of the 5-state chain MDP.

where β is a parameter that is equivalent to the inverse of temperatu re in the Boltzmann distribution.

The posterior over the reward function is then formulated by combining the prior and the likelihood,
using Bayes theorem:

P (R|X ) ∝ P (X |R)P (R).

(5)

BIRL uses a Markov chain Monte Carlo (MCMC) algorithm to compute the posterior mean of the
reward function.

3 MAP Inference in Bayesian IRL

In the Bayesian approach to IRL, the reward function can be determined using different estimates,
such as the posterior mean, median, or maximum-a-posterior (MAP). The posterior mean is com-
monly used since it can be shown to be optimal under the mean square error function. However,
the problem with the posterior mean in Bayesian IRL is that the error is integrated over the entire
space of reward functions, even including inﬁnitely many re wards that induce policies inconsistent
with the behaviour data. This can yield a posterior mean rewa rd function with an optimal policy
again inconsistent with the data. On the other hand, the MAP does not involve an objective function
that is integrated over the reward function space; it is simp ly a point that maximizes the posterior
probability. Hence, it is more robust to inﬁnitely many inco nsistent reward functions. We present a
simple example that compares the posterior mean and the MAP reward function estimation.

Consider an MDP with 5 states arranged in a chain, 2 actions, and the discount factor 0.9. As shown
in Figure 1(a), we denote the leftmost state as s1 and the rightmost state as s5 . Action a1 moves to
the state on the right with probability 0.6 and to the state on the left with probability 0.4. Action a2
always moves to state s1 . The true reward of each state is [0.1, 0, 0, 0, 1], hence the optimal policy
chooses a1 in every state. Suppose that we already know R(s2 ), R(s3 ), and R(s4 ) which are all 0,
and estimate R(s1 ) and R(s5 ) from the behaviour data X which contains optimal actions for all the
states. We can compute the posterior P (R(s1 ), R(s5 )|X ) using Equations (3), (4), and (5) under the
assumption that 0 ≤ R ≤ 1 and priors P (R(s1 )) being N (0.1, 1), and P (R(s5 )) being N (1, 1).
The reward optimality region can be also computed using Equa tion (2).

Figure 1(b) presents the posterior distribution of the rewa rd function. The true reward, the MAP
reward, and the posterior mean reward are marked with the bla ck star, the blue circle, and the red
cross, respectively. The black solid line is the boundary of the reward optimality region. Although
the prior mean is set to the true reward, the posterior mean is outside the reward optimality region.
An optimal policy for the posterior mean reward function cho oses action a2 rather than action a1
in state s1 , while an optimal policy for the MAP reward function is identical to the true one. The
situation gets worse when using the uniform prior. An optima l policy for the posterior mean reward
function chooses action a2 in states s1 and s2 , while an optimal policy for the MAP reward function
is again identical to the true one.

In the rest of this section, we additionally show that most of the IRL algorithms in the literature can
be cast as searching for the MAP reward function in Bayesian IRL. The main insight comes from
the fact that these algorithms try to optimize an objective function consisting of a regularization term
for the preference on the reward function and an assessment t erm for the compatibility of the reward
function with the behaviour data. The objective function is naturally formulated as the posterior in
a Bayesian framework by encoding the regularization into the prior and the data compatibility into
the likelihood. In order to subsume different approaches used in the literature, we generalize the

4

Table 1: IRL algorithms and their equivalent f (X ; R) and prior for the Bayesian formulation. q ∈
{1, 2} is for representing L1 or L2 slack penalties.
Previous algorithm
Ng and Russell’s IRL from sampled trajectories [6]
MMP without the loss function [8]
MWAL [10]
Policy matching [9]
MaxEnt [11]

Prior
Uniform
Gaussian
Uniform
Uniform
Uniform

f (X ; R)

fV
(fV )q
fG
fJ
fE

likelihood in Equation (4) to the following:

P (X |R) ∝ exp(β f (X ; R))

where β is a parameter for scaling the likelihood and f (X ; R) is a function which will be deﬁned
appropriately to encode the data compatibility assessment used in each IRL algorithm. We then have
the following result (the proof is provided in the supplemen tary material):

Theorem 1 IRL algorithms listed in Table 1 are equivalent to computing the MAP estimates with
the prior and the likelihood using f (X ; R) deﬁned as follows:
• fG (X ; R) = mini hV π∗ (R)
i i
− ˆV E
• fV (X ; R) = ˆV E (R) − V ∗ (R)
i
• fJ (X ; R) = − Ps,a ˆµE (s) (J (s, a; R) − ˆπE (s, a))2
• fE (X ; R) = log PMaxEnt (X |T , R)
where π∗ (R) is an optimal policy induced by the reward function R, J (s, a; R) is a smooth
mapping from reward function R to a greedy policy such as the soft-max function, and PMaxEnt
is the distribution on the behaviour data (trajectory or path) satisfying the principle of maximum
entropy.

The MAP estimation approach provides a rich framework for ex plaining the previous non-Bayesian
IRL algorithms in a uni ﬁed manner, as well as encoding variou s types of a priori knowledge into the
prior distribution. Note that this framework can exploit th e insights behind apprenticeship learning
algorithms even if they do not explicitly learn a reward func tion (e.g., MWAL [10]).

4 A Gradient Method for Finding the MAP Reward Function

We have proposed a unifying framework for Bayesian IRL and suggested that the MAP estimate can
be a better solution to the IRL problem. We can then reformula te the IRL problem into the posterior
optimization problem, which is ﬁnding RMAP that maximizes the (log unnormalized) posterior:

RMAP = argmaxR P (R|X ) = argmaxR [log P (X |R) + log P (R)]
Before presenting a gradient method for the optimization problem, we show that the generalized
likelihood is differentiable almost everywhere.

The likelihood is deﬁned for measuring the compatibility of
the reward function R with the be-
haviour data X . This is often accomplished using the optimal value function V ∗ or the optimal
Q-function Q∗ w.r.t. R. For example, the empirical value of X is compared with V ∗ [6, 8], X
is directly compared to the learned policy (e.g. the greedy policy from Q∗ ) [9], or the probability
of following the trajectories in X is computing using Q∗ [13]. Thus, we generally assume that
P (X |R) = g(X , V ∗ (R)) or g(X , Q∗ (R)) where g is differentiable w.r.t. V ∗ or Q∗ . The remain-
ing question is the differentiability of V ∗ and Q∗ w.r.t. R, which we address in the following two
theorems (The proofs are provided in the supplementary mate rial.):

Theorem 2 V ∗ (R) and Q∗ (R) are convex.

Theorem 3 V ∗ (R) and Q∗ (R) are differentiable almost everywhere.
Theorems 2 and 3 relate to the previous work on gradient metho ds for IRL. Neu and Szepesv ´ari [9]
showed that Q∗ (R) is Lipschitz continuous, and except on a set of measure zero (almost every-
where), it is Fr ´echet differentiable by Rademacher’s theorem. We have obtained the same result

5

based on the reward optimality region, and additionally identi ﬁed the condition for which V ∗ (R)
and Q∗ (R) are non-differentiable (refer to the proof for details). Ra tliff et al. [8] used a subgra-
dient of their objective function because it involves differentiating V ∗ (R). Using Theorem 3 for
computing the subgradient of their objective function yields an identical result.

Assuming a differentiable prior, we can compute the gradien t of the posterior using the result in The-
orem 3 and the chain rule. If the posterior is convex, we will ﬁ nd the MAP reward function. Other-
wise, as in [9], we will obtain a locally optimal solution. In the next section, we will experimentally
show that the locally optimal solutions are nonetheless better than the posterior mean in practice.
This is due to the property that they are generally found with in the reward optimality region w.r.t.
the policy consistent with the behaviour data.
The gradient method uses the update rule Rnew ← R + δt∇RP (R|X ) where δt is an appropriate
step-size (or learning rate). Since computing ∇RP (R|X ) involves computing an optimal policy
for the current reward function and a matrix inversion, cach ing these results helps reduce repetitive
computation. The idea is to compute the reward optimality region for checking whether we can
reuse the cached result. If Rnew is inside the reward optimality region of an already visited reward
function R′ , they share the same optimal policy and hence the same ∇RV π (R) or ∇RQπ (R).
Given policy π , the reward optimality region is deﬁned by H π = I − (I A − γT )(I − γT π )−1E π ,
and we can reuse the cached result if H π · Rnew ≤ 0. The gradient method using this idea is
presented in Algorithm 1.

Algorithm 1 Gradient method for MAP inference in Bayesian IRL
Input: MDP\R, behaviour data X , step-size sequence {δt }, number of iterations N
1: Initialize R
2: π ← solveMDP(R)
3: H π ← computeRewardOptRgn(π )
4: Π ← {hπ , H π i}
5: for t = 1 to N do
6: Rnew ← R + δt∇R P (R|X )
if isNotInRewardOptRgn(Rnew , H π ) then
7:
hπ , H π i ← ﬁndRewardOptRgn( Rnew , Π)
8:
if isEmpty(hπ , H π i) then
9:
10:
π ← solveMDP(Rnew )
H π ← computeRewardOptRgn(π )
11:
12:
Π ← Π ∪ {hπ , H π i}
end if
13:
end if
14:
15: R ← Rnew
16: end for

5 Experimental Results

The ﬁrst set of experiments was conducted in N × N gridworlds [7]. The agent can move west,
east, north, or south, but with probability 0.3, it fails and moves in a random direction. The grids
are partitioned into M × M non-overlapping regions, so there are ( N
M )2 regions. The basis function
is deﬁned by a 0-1 indicator function for each region. The lin early parameterized reward function
is determined by the weight vector w sampled i.i.d. from a zero mean Gaussian prior with variance
0.1 and |wi | ≤ 1 for all i. The discount factor is set to 0.99.
We compared the performance of our gradient method to those o f other IRL algorithms in the liter-
ature: Maximum Margin Planning (MMP) [8], Maximum Entropy (MaxEnt) [11], Policy Matching
with natural gradient (NatPM) and the plain gradient (PlainPM) [9], and Bayesian Inverse Rein-
forcement Learning (BIRL) [13]. We executed our gradient me thod for ﬁnding MAP using three
different choices of the likelihood: B denotes the BIRL likelihood, and E and J denote the likelihood
with fE and fJ , respectively. For the Bayesian IRL algorithms (BIRL and MAP), two types of the
prior are prepared: U denotes the uniform prior and G denotes the true Gaussian prior. We evaluated
the performance of the algorithms using the difference between V ∗ (the value of the expert’s policy)
and V L (the value of the optimal policy induced by the learned weight wL measured on the true
weight w∗ ) and the difference between w∗ and wL using L2 norm.

6

Table 2: Results in the gridworld problems.
k w ∗ − wL k2
V ∗ − V L

24 × 24
|S |
144
576
36
dim(w )
6.84 16.83
NatPM
3.04
PlainPM 3.77
6.63 16.60
6.05 11.98 22.11
MaxEnt
MMP
0.85
1.26
2.38
n.a.
5.67
3.27
BIRL-U
1.36
BIRL-G
0.86
n.a.
8.46 13.87
MAP-B-U 4.45
2.40
1.30
MAP-B-G 0.83
MAP-E-G 0.83
1.22
2.33
2.30
1.10
0.48
MAP-J-G

32 × 32
256 1024
64
8.88 21.25
3.50
5.21
9.05 17.36
7.91 15.48 25.52
0.83
1.61
3.17
n.a.
7.89
3.78
0.98
1.71
n.a.
5.68 10.50 18.21
3.17
1.62
0.94
0.76
1.53
3.13
3.11
1.51
0.65

24 × 24
576
144
36
8.74
8.97
2.49
0.51
0.67
0.15
0.33
0.60
0.60
10.74 16.32 13.72
n.a.
0.80
1.38
n.a.
0.54
2.21
0.13
1.06
0.57
0.40
0.45
0.16
0.19
0.44
0.42
0.37
0.42
0.17

32 × 32
64
256 1024
1.08 12.84 10.97
1.91
1.28
0.41
2.91
0.95
2.22
13.58 10.59
8.87
n.a.
2.24
0.35
n.a.
0.90
0.50
2.17
1.34
1.63
0.87
0.77
0.41
0.43
1.29
1.88
0.38
1.21
0.90

3

2

1

2
k

L
w
−
∗
w
k

0

15

10

5

L
V
−
∗
V

0

0

5

4

3

2

20

10

L
V
−
∗
V

 

BIRL
MAP−B

2
k

L
w
−
∗
w
k

1

0

20
60
40
CPU time (sec)

800
80
(b)
(a)
Figure 2: CPU timing results of BIRL and MAP-B in 24 × 24 gridworld problem. (a) dim(w) = 36.
(b) dim(w) = 144.

200
600
400
CPU time (sec)

200
600
400
CPU time (sec)

20
60
40
CPU time (sec)

800

80

0

 
0

We used training data with 10 trajectories of 50 time steps, collected from the simulated runs of
the expert’s policy. Table 2 shows the average performance over 10 training data. Most of the
algorithms found the weight that induces an optimal policy whose performance is as good as that
of the expert’s policy (i.e., small V ∗ − V L ) except for MMP and NatPM. The poor performance of
MMP was due to the small size in the training data, as already noted in [14]. The poor performance
of NatPM may be due to the ineffectiveness of pseudo-metric in high dimensional reward spaces,
since PlainPM was able produce good performance. Regarding the learned weights, the algorithms
using the true prior (MMP, BIRL, and the variants of MAP) found the weight close to the true one
(i.e., small ||w∗ − wL ||2 ). Comparing BIRL and MAP-B is especially meaningful since they share
the same prior and likelihood. The only difference was in computing the mean versus MAP from the
posterior. MAP-B was consistently better than BIRL in terms of both ||w∗ − wL ||2 and V ∗ − V L .
Finally, we note that the correct prior yields small ||w∗ − wL ||2 and V ∗ − V L when we compare
PlainPM, MaxEnt, BIRL-U, and MAP-B-U (uniform prior) to MAP-J-G, MAP-E-G, BIRL-G, and
MAP-B-G (Gaussian prior), respectively.

Figure 2 compares the CPU timing results of the MCMC algorithm in BIRL and the gradient method
in MAP-B for the 24×24 gridworld with 36 and 144 basis functions. BIRL takes much longer
CPU time to converge than MAP-B since the former takes much larger number of iterations to
converge, and in addition, each iteration requires solving an MDP with a sampled reward function.
The CPU time gap gets larger as we increase the dimension of the reward function. Caching the
optimal policies and gradients sped up the gradient method by factors of 1.5 to 4.2 until convergence,
although not explicitly shown in the ﬁgure.

The second set of experiments was performed on a simpli ﬁed ca r race problem, modi ﬁed from [14].
The racetrack is shown in Figure 3. The shaded and white cells indicate the off-track and on-track
locations, respectively. The state consists of the location and velocity of the car. The velocities in the
vertical and horizontal directions are represented as 0, 1, or 2, and the net velocity is computed as
the squared sum of directional velocities. The net velocity is regarded as high if greater than 2, zero
if 0, and low otherwise. The car can increase, decrease, or ma intain one of the directional velocities.
The control of the car succeeds with p=0.9 if the net velocity is low, but p=0.6 if high. If the control
fails, the velocity is maintained, and if the car attempts to move outside the racetrack, it remains in
the previous location with velocity 0. The basis functions are 0-1 indicator functions for the goal
locations, off-track locations, and 3 net velocity values (zero, low, high) while the car is on track.
Hence, there are 3150 states, 5 actions, and 5 basis functions. The discount factor is set to 0.99.

7

Table 3: True and learned weights in the car race problem.
Velocity while on track
Goal
Off-track
Low
0.00
-0.12±0.02
-0.13±0.01

Zero
0.00
-0.04±0.01
-0.03±0.01

0.00
-0.20±0.03
-0.19±0.02

1.00
0.96±0.02
1.00±0.00

Fast expert
BIRL
MAP-B

High
0.10
0.32±0.02
0.29±0.01

Table 4: Statistics of the policies simulated in the car race problem.
Avg. steps in velocity
Avg. steps in locations
Avg. steps
On-track
Off-track
to goal
Low
3.40
17.85
1.56
20.41
32.98±6.42
2.13±0.60
29.85±6.03
4.34±0.79
3.38±0.18
22.09±1.71
1.68±0.26
24.77±1.92

Zero
2.01
3.33±0.52
2.70±0.16

High
12.44
22.18±4.84
16.01±1.48

Fast expert
BIRL
MAP-B

We designed two experts. The slow expert prefers low velocity and avoids the off-track locations,
w = [1, −0.1, 0, 0.1, 0]. The fast expert prefers high velocity, w = [1, 0, 0, 0, 0.1]. We compared the
posterior mean and the MAP using the prior P (w1 )=N (1, 1) and P (w2 )=P (w3 )=P (w4 )=P (w5 )=
N (0, 1) assuming that we do not know the experts’ preference on the locations nor the velocity, but
we know the experts’ ultimate goal is to reach one of the goal locations. We used BIRL for the
posterior mean and MAP-B for the MAP estimation, hence using the identical prior and likelihood.

We used 10 training data, each consisting of 5 trajectories. We omit
the results regarding the slow expert since both BIRL and MAP-
B successfully found the weight similar with the true one, which
induced the slow expert’s policy as optimal. However for the fast
expert, MAP-B was signi ﬁcantly better than BIRL. 1 Table 3 shows
the true and learned weights, and Table 4 shows some statistics
characterizing the expert’s and learned policies. The policy from
Figure 3: Racetrack.
BIRL tends to remain in high speed on the track for signi ﬁcant
ly
more steps than the one from MAP-B since BIRL converged to a larger ratio of w5 to w1 .

S

S

G

G

G

G

6 Conclusion

We have argued that, when using a Bayesian framework for learning reward functions in IRL, the
MAP estimate is preferable over the posterior mean. Experim ental results conﬁrmed the effec-
tiveness of our approach. We have also shown that the MAP estimation approach subsumes non-
Bayesian IRL algorithms in the literature, and allows us to incorporate various types of a priori
knowledge about the reward functions and the measurement of the compatibility with behaviour
data.

We proved that the generalized posterior is differentiable almost everywhere, and proposed a gradi-
ent method to ﬁnd a locally optimal solution to the MAP estima tion. We provided the theoretical
result equivalent to the previous work on gradient methods for non-Bayesian IRL, but used a differ-
ent proof based on the reward optimality region.

Our work could be extended in a number of ways. For example, the IRL algorithm for partially
observable environments in [15] mostly relies on Ng and Russell [6]’s heuristics for MDPs, but our
work opens up new opportunities to leverage the insight behind other IRL algorithms for MDPs.

Acknowledgments

This work was supported by National Research Foundation of Korea (Grant# 2009-0069702) and the
Defense Acquisition Program Administration and the Agency for Defense Development of Korea
(Contract# UD080042AD)

1All the results in Table 4 except for the average number of steps in the off -track locations are statistically
signiﬁcant at the 95% conﬁdence level.

8

References
[1] S. Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of COLT,
1998.
[2] P. R. Montague and G. S. Berns. Neural economics and the biological substrates of valuation. Neuron,
36(2), 2002.
[3] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration.
Robotics and Autonomous Systems, 57(5), 2009.
[4] Y. Niv. Reinforcement learning in the brain. Journal of Mathematical Psychology, 53(3), 2009.
[5] E. Hopkins. Adaptive learning models of consumer behavior. Journal of Economic Behavior and Orga-
nization, 64(3–4), 2007.
[6] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement lear ning. In Proceedings of ICML, 2000.
[7] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinfo rcement learning. In Proceedings of
ICML, 2004.
[8] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In Proceedings of ICML,
2006.
[9] G. Neu and C. Szepesv ´ari. Apprenticeship learning using inverse reinforcement learning and gradient
methods. In Proceedings of UAI, 2007.
[10] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proceedings of
NIPS, 2008.
[11] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning.
In Proceedings of AAAI, 2008.
[12] G. Neu and C. Szepesv ´ari. Training parsers by inverse reinforcement learning. Machine Learning, 77(2),
2009.
[13] D. Ramachandran and E. Amir. Bayesian inverse reinforcemen t learning. In Proceedings of IJCAI, 2007.
[14] A. Boularias and B. Chaib-Draa. Bootstrapping apprenticeship learning. In Proceedings of NIPS, 2010.
[15] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. In Proceedings
of IJCAI, 2009.

9


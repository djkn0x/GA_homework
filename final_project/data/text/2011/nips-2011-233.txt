Priors over Recurrent Continuous Time Processes

Alexandre Bouchard-C ˆot ´e
Ardavan Saeedi
Department of Statistics
University of British Columbia

Abstract

We introduce the Gamma-Exponential Process (GEP), a prior over a large fam-
ily of continuous time stochastic processes. A hierarchical version of this prior
(HGEP; the Hierarchical GEP) yields a useful model for analyzing complex time
series. Models based on HGEPs display many attractive properties: conjugacy,
exchangeability and closed-form predictive distribution for the waiting times, and
exact Gibbs updates for the time scale parameters. After establishing these prop-
erties, we show how posterior inference can be carried efﬁciently using Particle
MCMC methods [1]. This yields a MCMC algorithm that can resample entire se-
quences atomically while avoiding the complications of introducing slice and stick
auxiliary variables of the beam sampler [2]. We applied our model to the problem
of estimating the disease progression in multiple sclerosis [3], and to RNA evo-
lutionary modeling [4]. In both domains, we found that our model outperformed
the standard rate matrix estimation approach.

1

Introduction

The application of non-parametric Bayesian techniques to time series has been an active ﬁeld in the
recent years, and has led to many successful continuous time models. Examples include Depen-
dent Dirichlet Processes (DDP) [5], Ornstein-Uhlenbeck Dirichlet Processes [6], and stick-breaking
autoregressive processes [7]. One property of these models is that they are forgetful, meaning that
the effect of an observation at time t on a prediction at time t + s will decrease as s → ∞. More
formally, DDPs and their cousins can be viewed as priors over transient processes (see Section A of
the Supplementary Material).
In some situations, emphasizing the short term trends is desirable, for example for the analysis of
ﬁnancial time series. However, in other situations, this behavior does not use the data optimally.
As a concrete example of the type of time series we are interested in, consider the problem of
modeling the progression of recurrent diseases such as multiple sclerosis. Recurrent diseases are
characterized by alternations between relapse and remission periods, and patients can undergo this
cycle repeatedly. In multiple sclerosis research, measuring the effect of drugs in the presence of
these complex cycles is challenging, and is one of the applications that motivated this work.
The data available to infer the disease progression typically takes the form of summary measure-
ments taken at different points in time for each patient. We model these measurements as being
conditionally independent given a continuous time non-parametric latent process. The main options
available for this type of situation are currently limited to parametric Bayesian models [8], or to
non-Bayesian models [9].
In this work, we propose a family of models, Gamma-Exponential Processes (GEPs), that ﬁlls this
gap. GEPs are based on priors over recurrent, inﬁnite rate matrices specifying a jump process in a
latent space.
It is informative to start by a preview of what the predictive distributions look like in GEP models.
Indeed, an advantage of GEPs is that they have simple predictive distributions, a situation remi-

1

niscent of the theory of Dirichlet Processes, in which the simple predictive distributions (given by
the Chinese Restaurant Process (CRP)) were probably an important factor behind their widespread
adoption in Bayesian non-parametric statistics.
Suppose that the hidden state at the current time step is θ , and that we are interested in the distribution
over the waiting time t before the next jump to a different hidden state (we will come back to the
predictive distribution over what this next state is in Section 3, showing that it has the form of a
CRP). Let t1 , t2 , . . . , tn denote the previous, distinct waiting times at θ . The predictive distribution
is then speciﬁed by the following density over the positive reals:

f (t) =

(α0 + n)(β0 + T )(α0+n)
(β0 + T + t)α0+n+1

,

where T is the sum over the ti ’s, and α0 , β0 are parameters. It can be checked that this yields an
exchangeable distribution over the sequences of waiting times at θ (if forms a telescoping product—
see the proof of Proposition 5 in the Supplementary Material). By de Finetti’s theorem, there is
therefore a mixing prior distribution. We identify this prior in Section 3, and use it to build a
powerful hierarchical model in Section 4. As we will see, this hierarchical model displays many
attractive properties: conjugacy, exchangeability and closed-form predictive distributions for the
waiting times, and exact Gibbs updates for the time scale parameters. Moreover it admits efﬁcient
inference algorithms, described in Section 5.
In addition to the connection to DDPs mentioned above, our models are also related to the inﬁ-
nite Hidden Markov Model (iHMM) [10] and to the more general Sticky-HDP-HMM [11], which
are both based on priors over discrete time processes. While continuous-time analogues of these
discrete time processes can be constructed by subordination, we discuss in Section C of the Sup-
plementary Material the differences and advantages of GEPs compared to these subordinations. A
similar argument holds for factorial extensions of the inﬁnite HMM [12].
Gamma (Moran) Processes [13], a building block for our process, have been used in non-parametric
Bayesian statistics, but in different contexts, for example in survival analysis [14], spatial statistics
[15], and for modeling count data [16].1 Note also that the gamma-exponential process introduced
here is unrelated to the exponential-gamma process [18].

2 Background and notation

While our process can be deﬁned on continuous state spaces, the essential ideas can be described
over countable state spaces. We therefore focus in this section on reviewing Continuous Time
Markov Processes (CTMPs) over a countably inﬁnite state space.
These CTMPs can be characterized by an inﬁnite matrix qi,j where the off-diagonal entries are non-
negative and each row sums to zero (i.e. the diagonal entries are negative and with magnitude equal
to the sum of the off-diagonal row entries). Samples from these processes take the form of a list
of pairs of states and waiting times X = (θn , Jn )N
n=1 (see Figure 1(a)). We will call each pair of
that form a (hidden) event. Typically, only a function Y of the events is available. For example,
measurements could be taken at ﬁxed or random time intervals. We will come back to the partially
observed sequences setup in Section 5.
To simulate a sequence of events given parameters Q = (qi,j ), we use the standard Doob-Gillespie
algorithm: conditioning on the current state having index i, θN = i, the waiting time before the next
jump is exponentially distributed JN +1 |(θN = i) ∼ Exp(−qi,i ), and the index j (cid:54)= i of the next
state θN +1 is selected independently with probability proportional to p(j ) = qi,j 1[i (cid:54)= j ].
The goal of this work is to develop priors on such inﬁnite rate matrices that are both ﬂexible and easy
to work with. To do that, we ﬁrst note that the off-diagonal elements of each row i can be viewed
as a positive measure µi . Note that the normalization of this measure in not equal to one in general.
We will denote the normalization constant of measures by ||µ|| and the normalized measures by
¯µ = µ/||µ||.

1The terminology “Moran Gamma Process” is from Kingman (e.g. in [17]). It is the same process as the
Gamma process used in e.g. [15], except that we have one more degree of freedom in the parameterization (the
rate; this is because ours is not destructively normalized).

2

(a)

(b)

Figure 1: (a) An illustration of our notation for samples from CTMPs. We assume the state space (Ω) is
countable. The notation for the observations Y (t1 ), . . . , Y (tG ) is described in Section 5. (b) Graphical model
for the hierarchical model of Section 4. For simplicity we only show a single J and θ .

To get a conjugate family, we will base our priors on Moran Gamma Processes (MGPs) [13], a
family of measure-valued probability distributions. MGPs have three parameters: (1) A positive
real number α0 > 0, called the concentration or shape parameter, (2) A probability distribution
P0 : FΩ → [0, 1] called the base probability distribution, (3) A positive real number β0 > 0, called
the rate parameter. Alternatively, the ﬁrst two parameters can be grouped into a single ﬁnite base
measure parameter H0 = α0P0 .
Recall that by the Kolmogorov consistency theorem, in order to guarantee the existence of a stochas-
tic process on a probability space (Ω(cid:48) , FΩ(cid:48) ), it is enough to provide a consistent deﬁnition of what
the marginals of this stochastic process are. As the name suggest, in the case of a Moran Gamma
process, the marginals are gamma distributions:
Deﬁnition 1 (Moran Gamma Process). Let H0 , β0 be of the types listed above. We say that µ :
FΩ(cid:48) → (FΩ → [0, ∞)) is distributed according to the Moran Gamma process distribution, denoted
by µ ∼ MGP(H0 , β0 ), if for all measurable partitions of Ω, (A1 , . . . , AK ), we have: 2
(µ(A1 ), µ(A2 ), . . . , µ(AK )) ∼ Gamma(H0 (A1 ), β0 ) × · · · × Gamma(H0 (Ak ), β0 ).

3 Gamma-Exponential Process

We can now describe the basic version of our model, the Gamma-Exponential Process (GEP). In the
next section, we will move to a hierarchical version of this model.
In GEPs, the rows of a rate matrix Q are obtained by a transformation of iid samples from an MGP,
and the states are then generated from Q with the Doob-Gillespie algorithm described in the previous
section. In this section we show that this model is conjugate and has a closed form expression for
the predictive distribution.
Let H0 be a base measure on a countable support Ω with (cid:107)H0(cid:107) < ∞. We will relax the countable
base measure support assumption in the next section. The GEP is formally deﬁned as follows:
(cid:12)(cid:12)X, {µθ }θ∈Ω ∼ ¯µθN
iid∼ MGP(H0 , β0 ) ∀θ ∈ Ω
µθ
(cid:12)(cid:12)X, {µθ }θ∈Ω ∼ Exp ((cid:107)µθN (cid:107))
To understand the connection with the Doob-Gillespie process, note that a rate matrix can be ob-
tained by arbitrarily ordering Ω = θ(1) , θ(2) , . . . , and setting:3 qi,j = µθ(i) ({θ(j ) }) if i (cid:54)= j , and
2We use the rate parameterization for the gamma density throughout.
3Note that the GEP as deﬁned above can generate self-transitions, but conditioning on the parameters, the
jump waiting times are still exponential. However for computing predictive distributions, it will be simpler to
allow positive self-transitions rates.

JN +1

θN +1

3

J(cid:2870)θ(cid:2869)J(cid:2871)θ(cid:2870)θ(cid:3029)(cid:3032)(cid:3034)t=0J(cid:2869)=0. . . J(cid:3015)θ(cid:3015)(cid:2879)(cid:2869)t∗Y(t(cid:2869))Y(t(cid:2870)). . . tY(t(cid:3008)(cid:2879)(cid:2870))θ(cid:3015)XYY(t(cid:3008)(cid:2879)(cid:2869))Y(t(cid:3008))Ω(cid:107)µθ(i) (cid:107) ( ¯µθ(i) ({i}) − 1) otherwise. In order to model the initial distribution without cluttering the
notation, we assume there is a special state θbeg always present at the beginning of the sequence, and
only at the beginning. In other words, we always condition on (θ0 = θbeg ) and (θn (cid:54)= θbeg , n > 0),
and drop these conditioning events from the notation. Similarly, we are going to consider distribu-
tion over inﬁnite sequences in the notation that follows, but if the goal is to model ﬁnite sequences,
an additional special state θend (cid:54)= θbeg can be introduced. We would then condition on (θN +1 = θend )
and (θn (cid:54)= θend , n ∈ {1, . . . , N }), and set the total rate for the row corresponding to θend to zero.
Next, we show that the posterior of each row, µθ |X , is also MGP distributed with updated param-
eters. We assume that all the states are observed for now, and treat the partially observed case in
Section 5.
The sufﬁcient statistics for the parameters of µθ |X are the empirical transition measures and waiting
N(cid:88)
N(cid:88)
times:
n=1
n=1
Proposition 2. The Gamma-Exponential Process (GEP) is a conjugate family, µθ |X ∼
MGP (µ(cid:48)
θ , β (cid:48)
θ ) , where µ(cid:48)
θ = Fθ + H0 and β (cid:48)
θ = Tθ + β0 .
Note that the µ(cid:48)
θ are unnormalized versions of the posterior parameters of a Dirichlet process. This
connexion with the Dirichlet process is used in the proof below, and also implies that samples from
GEPs have countable support even when Ω is uncountable (i.e. the chain will always visit a random
countable subset of Ω). For the proof of proposition 2, we will need the following elementary
lemma:
Lemma 3. If V ∼ Beta(a, b) and W ∼ Gamma(a + b, c) are independent, then V W ∼
Gamma(a, c).

1[θn−1 = θ ] δθn ,

1[θn−1 = θ] Jn .

Fθ =

Tθ =

See for example [19] for a survey of standard beta-gamma algebra results such as the one stated in
this lemma. We now prove the proposition:

Proof. Fix an arbitrary state θ and drop the index for simplicity (this is without loss of generality
since the rows are iid): let µ = µθ , µ(cid:48) = µ(cid:48)
θ , and β (cid:48) = β (cid:48)
θ .
Let (A1 , . . . , AK ) be a measurable partition of Ω. By the Kolmogorov consistency theorem, it is
enough to show that for all such partition,
) × · · · × Gamma(µ
(µ(A1 ), µ(A2 ), . . . , µ(AK ))|X ∼ Gamma(µ
Assume for simplicity that K = 2 (the argument can be generalized to K > 2 without difﬁculties),
and let Γ1 = µ(A1 ), Γ0 = (cid:107)µ(cid:107). By elementary properties of Gamma distributed vectors, if we let
V = Γ1/Γ0 , W = Γ0 , then V ∼ Beta(H0 (A1 ), H0 (A2 )), W ∼ Gamma(α0 , β0 ), and V , W are in-
dependent (both conditionally on X and unconditionally). By beta-multinomial conjugacy, we also
have (V |X ) = (V |θ1 , . . . , θN ) ∼ Beta(µ(cid:48) (A1 ), µ(cid:48) (A2 )), and by gamma-exponential conjugacy,
we have W |X ∼ Gamma((cid:107)µ(cid:48) (cid:107), β (cid:48) ).
Using the lemma with a = µ(cid:48) (A1 ), b = µ(cid:48) (A2 ), c = β (cid:48) , we ﬁnally get that (µ(A1 )|X ) =
(V W |X ) ∼ Gamma(µ(cid:48) (A1 ), β (cid:48) ), which concludes the proof.

(Ak ), β

(A1 ), β

).

(cid:48)

(cid:48)

(cid:48)

(cid:48)

We now turn to the task of ﬁnding an expression for the predictive distribution, (θN +1 , JN +1 )|X .
We will need the following family of densities (see Section F for more information):
Deﬁnition 4 (Translated Pareto). Let α > 0, β > 0. We say that a random variable T is translated-
Pareto, denoted T ∼ TP(α, β ), if it has density:
1[t > 0]αβα
(t + β )α+1 .
Proposition 5. The predictive distribution of the GEP is given by:
(θN +1 , JN +1 )|X ∼ ¯µ
θN × TP((cid:107)µ
θN (cid:107), β
(cid:48)
(cid:48)

(cid:48)
θN ).

f (t) =

(2)

(1)

4

Proof. By Proposition 2, it is enough to show that if µ ∼ MGP(H0 , β0 ), θ |µ ∼ ¯µ, and J |µ ∼
Exp((cid:107)µ(cid:107)), then (θ , J ) ∼ ¯µ × TP(α0 , β0 ), where α0 = (cid:107)H0(cid:107).
Note ﬁrst that we have (J |θ) d= J by the fact that the minimum and argmin of independent expo-
(cid:90)
nential random variables are independent. To get the distribution of J , we need to show that the
following integral is proportional to Equation (1):
(cid:90)
xα0−1 exp(−β0x) · x exp(−xt) dx
p(t) ∝
x>0
x>0

xα0 exp (−(β0 + t)x) dx =

Γ(α0 + 1)
(β0 + t)α0+1

Hence J ∼ TP(α0 , β0 ).

=

As a sanity check, and to connect this result with the discussion in the introduction, it is instructive
to directly check that these predictive distributions are indeed exchangeable (see Section B for the
proof):
Proposition 6. Let Jj (θ,1) , Jj (θ,2) , . . . , Jj (θ,K ) be the subsequence of waiting times following state
θ . Then the random variables Jj (θ,1) , Jj (θ,2) , . . . , Jj (θ,K ) are exchangeable. Moreover, the joint
density of a sequence of waiting times (Jj (θ,1) = j1 , Jj (θ,2) = j2 , . . . , Jj (θ,K ) = jK ) is given by:
1[jk > 0, k ∈ {1, . . . , K }](α0 )K βα0
0
(β0 + j1 + · · · + jK )α0+K
p(j1 , j2 , . . . , jK ) =
where the Pochhammer symbol (x)n is deﬁned as (x)n = x(x + 1) · · · (x + n − 1).

(3)

4 Hierarchical GEP

In this section, we present a hierarchical version of the GEP, where the rows of the random rate
matrix are exchangeable rather than iid. Informally, the motivation behind this construction is to
have the rows share information on what states are frequently visited.
As with Hierarchical Dirichlet Processes (HDPs) [20], the hierarchical construction is especially
important when Ω is uncountable. For such spaces, since each GEP sample has a random countable
support, any two independent GEP samples will have disjoint supports with probability one. There-
fore, GEP alone cannot be used to construct recurrent processes when Ω is uncountable. Fortunately,
the hierarchical model introduced in this section addresses this issue: it yields a recurrent prior over
continuous time jump processes over both countable and uncountable spaces Ω (see Section A).
(cid:12)(cid:12)X, {µθ }θ∈Ω ∼ ¯µθN
(cid:26) θN +1
(cid:26) µ0
The hierarchical process is constructed by making the base measure parameter of the rows shared
(cid:12)(cid:12)X, {µθ }θ∈Ω ∼ Exp((cid:107)µθN (cid:107)).
and random. Formally, the model has the following form:
∼ MGP(H0 , γ0 )
µθ |µ0
iid∼ MGP(µ0 , β0 )
JN +1
In order to get a tractable predictive distribution, we introduce a set of auxiliary variables. These
auxiliary variables can be compared to the variables used in the Chinese Restaurant Franchise (CRF)
metaphor [20] to indicate when new tables are created in a given restaurant. In the HGEP, a restau-
rant can be understood as a row in the rate matrix, and tables, as groups of transitions to the same
destination state. These auxiliary variables will be denoted by An , where the event An = 1 means
informally that the n-th transition creates a new table. The variable takes value An = 0 otherwise.
See Section D in the Supplementary Material for a review of the CRF construction and a formal
deﬁnition of the auxiliary variables An .
rants that share a given dish, G = (cid:80)N
We augment the sufﬁcient statistics with empirical counts for the number of tables across all restau-
n=1 An δθn , and introduce one additional auxiliary variable, the
normalization of the top level random measure, (cid:107)µ0(cid:107). This latter auxiliary variable has no equivalent
in CRFs. As in the previous section, the normalization of the lower level random measures (cid:107)µθ (cid:107)
will be marginalized. Finally, we let:
θ = Fθ + (cid:107)µ0 (cid:107) ¯µ
(cid:48)(H)
(cid:48)(cid:48)
(cid:48)(cid:48)
= G + H0
µ
µ
(cid:48)(H)
where ¯µ
can be recognized as the mean parameter of the predictive distribution of the HDP.
θ
We use the superscript (H) to disambiguate from the non-hierarchical case. The main result of this
section is (see Section B for the proof):

5

(a) β0 = 10, (cid:107)H0 (cid:107) = 5, γ0 = 100

(b) β0 = 100, (cid:107)H0 (cid:107) = 5, γ0 = 10

(c) β0 = 10, (cid:107)H0 (cid:107) = 500, γ0 = 10

(d) β0 = 1, (cid:107)H0 (cid:107) = 5000, γ0 = 1

Figure 2: Qualitative behavior of the prior
(θN +1 , JN +1 )(cid:12)(cid:12)(X, {An }N
Proposition 7. The predictive distribution of the Hierarchical GEP (HGEP) is given by:
(cid:107), β
× TP((cid:107)µ
n=1 , (cid:107)µ0 (cid:107)) ∼ ¯µ
(cid:48)(H)
(cid:48)(H)
(cid:48)
θN ).
θN
θN
To resample the auxiliary variable (cid:107)µ0(cid:107), a gamma-distributed Gibbs kernel can be used (see Sec-
tion E of the Supplementary Material).

5

Inference on partially observed sequences

In this section, we describe how to approximate expectations under the posterior distribution of
GEPs, E[h(X )|Y ], for a test function h on the hidden events X given observations Y . An example
of function h on these events is to interpolate the progression of the disease in a patient with Multiple
Sclerosis (MS) between two medical visits. We start by describing the form of the observations Y .
Note that in most applications, the sequence of states is not directly nor fully observed. First, instead
of observing the random variables θ , inference is often carried from X -valued random variables Yn
distributed according to a parametric family P indexed by the states θ of the chain, P = {Lθ :
(cid:110)
(cid:111)
FX → [0, 1], θ ∈ Ω}. Second, the measurements are generally available only for a ﬁnite set of
N : (cid:80)N +1
times T . To specify the random variables in question, we will need a notation for the event index at
(see Figure 1, where I (t∗ ) = N − 1), and for the
a given time t, I (t) = min
n=1 Jn > t
individual observations, Y (t)|X ∼ LθI (t) . The set of all observed random variable is then deﬁned
as Y = (Y (t1 ), Y (t2 ), . . . , Y (tG ) : tg < tg+1 , {ti} = T ) .
For simplicity, we assume in this section that P is a conjugate family with respect to H0 . Non-
conjugate models can be handled by incorporating the auxiliary variables of Algorithm 8 in [21].
We will describe inference on the model of Section 3. Extension to hierarchical models is direct (by
keeping track of an additional sufﬁcient statistic G, as well as the auxiliary variables An , (cid:107)µ0(cid:107)).
In general, there may be several exchangeable sequences from which we want to learn a model. For
example, we learned a model for MS disease progression by using time series from several patients.4
(cid:16)
i } = T (k)(cid:17)
We denote the number of time series by K , each of the form
Y (k) =
g+1 , {t(k)
2 ), . . . , Y (k) (t(k)
1 ), Y (k) (t(k)
Y (k) (t(k)
g < t(k)
G ) : t(k)
At a high-level, our inference algorithm works by resampling the hidden events X (k) for one se-
quence k given the sufﬁcient statistics of the other sequences, (F (\k)
, T (\k)
). This is done using a
θ
θ
Sequential Monte Carlo (SMC) algorithm to construct a proposal over sequences of hidden events.
Each particle in our SMC algorithm is a sequence of states and waiting times for the current se-
quence k . By using a Particle MCMC (PMCMC) method [1], we then compute an acceptance ratio

, k ∈ {1, . . . , K }.

4Even in cases where there is a single long sequence, we recommend for efﬁciency reasons to partition the
sequence into subsequences. In this case our proposal can be viewed as a block update.

6

02004006008000510152025TimeState020040060080024681012TimeState02004006008000100200300400TimeState0200400600800050100150TimeStateName
Synthetic
MS
RNA

# sequences
1000
72
1000

Datasets
# datapoints
10000
384
6167

# heldout
878
31
508

Results (mean error)
EM HGEP
# characters Baseline
0.404
0.446
0.703
4
0.277
0.355
0.516
3
0.426
4
0.648
0.596

Table 1: Summary statistics and mean error results for the experiments. All experiments were repeated 5
times.

that makes this proposal a valid MCMC move. As we will see shortly, the acceptance is simply given
by a ratio of marginal likelihood estimators, which can be computed directly from the unnormalized
particle weights.
Formally, the proposal is based on M particles propagated from generation g = 0 up to generation
G, where G is equal to the number of measurements in the current sequence, G = |Y (k) |. Each
particle Xm,g , m ∈ {1, . . . , M } consists of a list of hidden events indexed by n, containing both
(hidden) states and waiting times: Xm,g = (θm,n , Jm,n )Nm,g
n=1 . The pseudocode for the SMC algo-
rithm, used for constructing the proposals, is presented in Figure 4 of the Supplementary Material.
The next step is to compute an acceptance probability for a proposed sequence of states X (k)∗ .
weights πg (described in Figure 4 of the Supplementary Material) as follows: L(k) = (cid:81)G
At each MCMC iteration, we assume that we store the value of the data likelihood estimates for
the accepted state sequences. These data likelihood estimates are computed from the unnormalized
g=1 (cid:107)πg (cid:107).
(cid:110)
1, L(k)∗ /L(k)(cid:111)
Let L(k) be the estimate for the previously accepted sequence of states for observed sequence k , and
let L(k)∗
be the estimate for the current MCMC iteration. The acceptance probability for the new
. If it is accepted, we set L(k) = L(k)∗ .
sequence is given by min

6 Experiments

In this section we present the results of our experiments. First, we demonstrate the behavior of state
trajectories and sojourn times sampled from the prior to give a qualitative idea of the range of time
series that can be captured by our model. Second, we evaluate quantitatively our model by apply-
ing it to three held-out tasks: synthetic, Multiple Sclerosis (MS) patients, and RNA evolutionary
datasets.

6.1 Qualitative behavior of the prior

We can distinguish at least four types of prior behaviors in the HGEP when considering different
values for the parameters β0 , (cid:107)H0 (cid:107) and γ0 . We sampled a sequence of length T = 800 and present
the state-time plots. Figure 2(a) shows a sequence with short sojourn times and high volatility of
states, whereas Figure 2(b) depicts longer sojourn times with much less volatility. Figures 2(c) and
2(d) illustrate the effect of hyperparameter (cid:107)H0(cid:107). In Figure 2(c) we can see creation of many new
states and a sparse transition matrix. Likewise, in Figure 2(d) the high tendency to create new states
is present, but we have longer sojourn times. See Section H of the supplementary material for a
more detailed account of the interpretation and quantitative effect of the parameters.

6.2 Quantitative evaluation

In this section, we use a simple likelihood model for discrete observations (described in Section G
of the supplementary material) to evaluate our method on three held-out tasks. Note that even when
the observations are discrete, non-parametric models are still useful for better explaining the data
using latent variables [22].
We considered three evaluation datasets obtained by holding out each observed datapoint with a
10% probability (see Table 1). We then reconstructed the observations at these held-out times, and
measured the mean error. For HGEP, reconstruction was done by using the Bayes estimator approx-
imated from 1000 posterior samples (one after each scan through all the time series). We repeated

7

Synthetic

RNA

MS

Figure 3: Mean reconstruction error on the held-out data as a function of the number of Gibbs scans. Lower
is better. The standard maximum likelihood estimate learned with EM outperformed our model in the simple
synthetic dataset, but the trend was reversed in the more complex real world datasets.

all experiments 5 times with different random seeds. We compared against the standard maximum
likelihood rate matrix estimator learned by EM described in [23]. We also report in Table 1 the mean
error for a simpler maximum likelihood estimate ignoring the sequential information (returning the
most common observation deterministically). See Section G of the supplementary material for de-
tailed instructions for replicating the following three results.5 Refer also to Figure 3, where we show
error as a function of the number of scans.
Synthetic: We used an Erd ¨os-R ´enyi model to generate a random sparse matrix of size 10 × 10,
which we perturbed with uniform noise to get a random rate matrix. Both HGEP and the EM-learned
maximum likelihood outperformed the baseline. In contrast to the next two tasks, the EM approach
slightly outperformed the HGEP model here. We believe this is because the synthetic data was not
sufﬁciently rich to highlight the advantages of HGEPs. However, we compared our results with
iHMM after discretizing time. We observed that iHMM had an error rate of 0.47, underperforming
both EM and HGEP.
MS disease progression: This dataset, obtained from a phase III clinical trial, tracks the progression
of MS in 72 patients over 3 years. The observed state of a patient at a given time is binned into three
categories as customary in the MS literature [3]. Both HGEP and EM outperformed the baseline by
a large margin, and our HGEP model outperformed EM with a relative error reduction of 22%.
RNA evolution: In this task, we used the dataset from [4] containing aligned 16S ribosomal RNA
of species from the three domains of life. As a preprocessing, we constructed a rooted phylogenetic
tree from a sample of 30 species, and performed ancestral reconstruction using a standard CTMC
model and all the sampled taxa in the tree. We then considered the time series consisting of paths
from one modern leaf to the root. The task is to reconstruct held-out nucleotides using only the data
in this path. Again, both HGEP and EM outperformed the baseline, and our model outperformed
EM with a relative error reduction of 29%.

7 Conclusion

We have introduced a method for non-parametric Bayesian modeling of recurrent, continuous time
processes. The model has attractive properties and we show that the posterior computations can be
done efﬁciently using a sampler based on particle MCMC methods. Most importantly, our experi-
ments show that the model is useful for analyzing complex real world time series.

Acknowledgments

We would like to thank Arnaud Doucet, John Petkau and the anonymous reviewers for helpful
comments. This work was supported by a NSERC Discovery Grant and the WestGrid cluster.

5The code used to run these experiments is available at
http://www.stat.ubc.ca/˜bouchard/GEP/

8

02004006008000.40.50.60.70.8lHGEPEMllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll02004006008000.400.500.60lHGEPEMllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll02004006008000.250.350.45lHGEPEMllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllReferences
[1] C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal Of The
Royal Statistical Society Series B, 2010.
[2] J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov
model. In ICML, 2008.
[3] M. Mandel. Estimating disease progression using panel data. Biostatistics, 2010.
[4] J.J. Cannone, S. Subramanian, M.N. Schnare, J.R. Collett, L.M. D’Souza, Y. Du, B. Feng, N. Lin, L.V.
Madabusi, K.M. Muller, N. Pande, Z. Shang, N. Yu, and R.R. Gutell. The comparative RNA web (CRW)
site: An online database of comparative sequence and structure information for ribosomal, intron, and
other RNAs. BioMed Central Bioinformatics, 2002.
[5] S.N. MacEachern. Dependent nonparametric processes. In Section on Bayesian Statistical Science, Amer-
ican Statistical Association, 1999.
[6] J.E. Grifﬁn. The Ornstein-Uhlenbeck Dirichlet process and other time-varying processes for Bayesian
nonparametric inference. Journal of Statistical Planning and Inference, 2008.
[7] J.E. Grifﬁn and M.F.J. Steel. Stick-breaking autoregressive processes. Journal of Econometrics, 2011.
[8] M. F. J. Steel. The New Palgrave Dictionary of Economics, chapter Bayesian time series analysis. Pal-
grave Macmillan, 2008.
[9] S. Heiler. A survey on nonparametric time series analysis. CoFE Discussion Paper 99-05, Center of
Finance and Econometrics, University of Konstanz, 1999.
[10] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The inﬁnite hidden Markov model.
Learning. MIT Press, 2002.
[11] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. An hdp-hmm for systems with state persistence.
In Proceedings of the International Conference on Machine Learning, 2008.
[12] J. Van Gael, Y. W. Teh, and Z. Ghahramani. The inﬁnite factorial hidden Markov model. In NIPS’08,
2008.
[13] P.A.P. Moran. The Theory of Storage. Methuen, 1959.
[14] M. Friesl. Estimation in the Koziol-Green model using a gamma process prior. Austrian Journal of
Statistics, 2008.
[15] V. Rao and Y. W. Teh. Spatial normalized gamma processes. In Advances in Neural Information Process-
ing Systems, 2009.
[16] L. Kuo and S. K. Ghosh. Bayesian nonparametric inference for nonhomogeneous Poisson processes.
Technical report, University of Connecticut, Department of Statistics, 1997.
[17] J. F. C. Kingman. Poisson Processes. The Clarendon Press Oxford University Press, 1993.
[18] M. Schroder. Risk-neutral parameter shifts and derivatives pricing in discrete time. The Journal of
Finance, 2004.
[19] D. Dufresne. G distributions and the beta-gamma algebra. Electronic Journal of Probability, 2010.
[20] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the
American Statistical Association, 2004.
[21] R. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical report, U of T,
2000.
[22] P. Liang, S. Petrov, M. I. Jordan, and D. Klein. The inﬁnite PCFG using hierarchical Dirichlet processes.
In Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP/CoNLL), 2007.
[23] A. Hobolth and J.L. Jensen. Statistical inference in evolutionary models of DNA sequences via the EM
algorithm. Statistical applications in Genetics and Molecular Biology, 2005.
[24] L. Mateiu and B. Rannala.
Inferring complex DNA substitution processes on phylogenies using uni-
formization and data augmentation. Syst. Biol., 2006.

In Machine

9


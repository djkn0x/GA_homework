Learning Sparse Representations of High
Dimensional Data on Large Scale Dictionaries

Zhen James Xiang Hao Xu
Peter J. Ramadge
Department of Electrical Engineering, Princeton University
Princeton, NJ 08544, USA
{zxiang,haoxu,ramadge}@princeton.edu

Abstract

Learning sparse representations on data adaptive dictionaries is a state-of-the-art
method for modeling data. But when the dictionary is large and the data dimen-
sion is high, it is a computationally challenging problem. We explore three aspects
of the problem. First, we derive new, greatly improved screening tests that quickly
identify codewords that are guaranteed to have zero weights. Second, we study
the properties of random projections in the context of learning sparse representa-
tions. Finally, we develop a hierarchical framework that uses incremental random
projections and screening to learn, in small stages, a hierarchically structured dic-
tionary for sparse representations. Empirical results show that our framework can
learn informative hierarchical sparse representations more efﬁciently.

Introduction
1
Consider approximating a p-dimensional data point x by a linear combination x ≈ Bw of m (pos-
sibly linearly dependent) codewords in a dictionary B = [b1 , b2 , . . . , bm ]. Doing so by imposing
the additional constraint that w is a sparse vector, i.e., x is approximated as a weighted sum of only
a few codewords in the dictionary, has recently attracted much attention [1]. As a further reﬁnement,
when there are many data points xj , the dictionary B can be optimized to make the representations
wj as sparse as possible. This leads to the following problem. Given n data points in Rp organized as
matrix X = [x1 , x2 , . . . , xn ] ∈ Rp×n , we want to learn a dictionary B = [b1 , b2 , . . . , bm ] ∈ Rp×m
and sparse representation weights W = [w1 , w2 , . . . , wn ] ∈ Rm×n so that each data point xj is
well approximated by Bwj with wj a sparse vector:
(cid:107)X − BW(cid:107)2
F + λ(cid:107)W(cid:107)1
1
min
2
B,W
(cid:107)bi (cid:107)2
2 ≤ 1,
∀i = 1, 2, . . . , m.
s.t.
Here (cid:107)·(cid:107)F and (cid:107)·(cid:107)1 denote the Frobenius norm and element-wise l1 -norm of a matrix, respectively.
There are two advantages to this representation method. First, the dictionary B is adapted to the
data. In the spirit of many modern approaches (e.g. PCA, SMT [2], tree-induced bases [3, 4]), rather
than ﬁxing B a priori (e.g. Fourier, wavelet, DCT), problem (1) assumes minimal prior knowledge
and uses sparsity as a cue to learn a dictionary adapted to the data. Second, the new representation w
is obtained by a nonlinear mapping of x. Algorithms such as Laplacian eigenmaps [5] and LLE [6],
also use nonlinear mappings x (cid:55)→ w. By comparison, l1 -regularization enjoys a simple formula-
tion with a single tuning parameter (λ). In many other approaches (including [2–4]), although the
codewords in B are cleverly chosen, the new representation w is simply a linear mapping of x,
e.g. w = B†x. In this case, training a linear model on w cannot learn nonlinear structure in the
data. As a ﬁnal point, we note that the human visual cortex uses similar mechanisms to encode
visual scenes [7] and sparse representation has exhibited superior performance on difﬁcult computer
vision problems such as face [8] and object [9] recognition.

(1)

1

The challenge, however, is that solving the non-convex optimization problem(1) is computationally
expensive. Most state-of-the-art algorithms solve (1) by iteratively optimizing W and B. For a ﬁxed
B, optimizing W requires solving n, p-dimensional, lasso problems of size m. Using LARS [10]
with a Cholesky-based implementation, each lasso problem has a computation cost of O(mpκ +
mκ2 ), where κ is the number of nonzero coefﬁcients [11]. For a ﬁxed W, optimizing B is a
least squares problem of pm variables and m constraints. In an efﬁcient algorithm [12], the dual
formulation has only m variables but still requires inverting m × m matrices (O(m3 ) complexity).
To address this challenge, we examine decomposing a large dictionary learning problem into a set
of smaller problems. First (§2), we explore dictionary screening [13, 14], to select a subset of code-
words to use in each Lasso optimization. We derive two new screening tests that are signiﬁcantly
better than existing tests when the data points and codewords are highly correlated, a typical scenario
in sparse representation applications [15]. We also provide simple geometric intuition for guiding
the derivation of screening tests. Second (§3), we examine projecting data onto a lower dimensional
space so that we can control information ﬂow in our hierarchical framework and solve sparse repre-
sentations with smaller p. We identify an important property of the data that’s implicitly assumed in
sparse representation problems (scale indifference) and study how random projection preserves this
property. These results are inspired by [16] and related work in compressed sensing. Finally (§4), we
develop a framework for learning a hierarchical dictionary (similar in spirit to [17] and DBN [18]).
To do so we exploit our results on screening and random projection and impose a zero-tree like struc-
tured sparsity constraint on the representation. This constraint is similar to the formulation in [19].
The key difference is that we learn the sparse representation stage-wise in layers and use the exact
zero-tree sparsity constraint to utilize the information in previous layers to simplify the computation,
whereas [19] uses a convex relaxation to approximate the structured sparsity constraint and learns
the sparse representation (of all layers) by solving a single large optimization problem. Our idea of
using incremental random projections is inspired by the work in [20, 21]. Finally, unlike [12] (that
addresses the same computational challenge), we focus on a high level reorganization of the compu-
tations rather than improving basic optimization algorithms. Our framework can be combined with
all existing optimization algorithms, e.g. [12], to attain faster results.

|wi |.

min
w1 ,w2 ,...,wm

2 Reducing the Dictionary By Screening
In this section we assume that all data points and codewords are normalized: (cid:107)xj (cid:107)2 = (cid:107)bi (cid:107)2 =
1, 1 ≤ j ≤ n, 1 ≤ i ≤ m (we discuss the implications of this assumption in §3). When B is ﬁxed,
ﬁnding the optimal W in (1) requires solving n subproblems. The j th subproblem ﬁnds wj for
xj . For notational simplicity, in this section we drop the index j and denote x = xj , w = wj =
m(cid:88)
(cid:107)x − m(cid:88)
[w1 , w2 , . . . , wm ]T . Each subproblem is then of the form:
wibi (cid:107)2
2 + λ
i=1
i=1
To address the challenge of solving (2) for large m, we ﬁrst explore simple screening tests that
identify and discard codewords bi guaranteed to have optimal solution ˜wi = 0. El Ghaoui’s SAFE
rule [13] is an example of a simple screening test. We introduce some simple geometric intuition for
screening and use this to derive new tests that are signiﬁcantly better than existing tests for the type
of problems of interest here. To this end, it will help to consider the dual problem of (2):
2 − λ2
(cid:107)2
(cid:107)θ − x
(cid:107)x(cid:107)2
1
max
2
2
λ
2
θ
|θT bi | ≤ 1 ∀i = 1, 2, . . . , m.
s.t.
As is well known (see the supplemental material), the optimal solution of the primal problem ˜w =
(cid:26) {sign ˜wi }
m(cid:88)
[ ˜w1 , ˜w2 , . . . , ˜wm ]T and the optimal solution of the dual problem ˜θ are related through:
if ˜wi (cid:54)= 0,
bi ∈
[−1, 1]
if ˜wi = 0.
i=1
The dual formulation gives useful geometric intuition. Since (cid:107)x(cid:107)2 = (cid:107)bi (cid:107)2 = 1, x and all bi lie on
the unit sphere S p−1 (Fig.1(a)). For y on S p−1 , P (y) = {z : zT y = 1} is the tangent hyperplane

˜wibi + λ˜θ ,

x =

(2)

(3)

(4)

1
2

T

˜θ

2

(a)

(b)

(c)

(d)

Figure 1: (a) Geometry of the dual problem. (b) Illustration of a sphere test. (c) The solid red, dotted blue and
solid magenta circles leading to sphere tests ST1/SAFE, ST2, ST3, respectively. (d) The thresholds in ST2 and
ST1/SAFE when λmax = 0.8 (top) and λmax = 0.9 (bottom). A higher threshold yields a better test.

of S p−1 at y and H (y) = {z : zT y ≤ 1} is the corresponding closed half space containing the
origin. The constraints in (3) indicate that feasible θ must be in H (bi ) and H (−bi ) for all i. To
ﬁnd ˜θ that maximizes the objective in (3), we must ﬁnd a feasible θ closest to x/λ. By (4), if ˜θ is
not on P (bi ) or P (−bi ), then ˜wi = 0 and we can safely discard bi from problem (2).
Let λmax = maxi |xT bi | and b∗ ∈ {±bi }m
i=1 be selected so that λmax = xT b∗ . Note that
θ = x/λmax is a feasible solution for (3). λmax is also the largest λ for which (2) has a nonzero
solution. If λ > λmax , then x/λ itself is feasible, making it the optimal solution. Since it is not on
any hyperplane P (bi ) or P (−bi ), ˜wi = 0, i = 1, . . . , m. Hence we assume that λ ≤ λmax .
These observations can be used for screening as follows. If we know that ˜θ is within a region R,
then we can discard those bi for which the tangent hyperplanes P (bi ) and P (−bi ) don’t intersect
R, since by (4) the corresponding ˜wi will be 0. Moreover, if the region R is contained in a closed
ball (e.g. the shaded blue area in Fig.1(b)) centered at q with radius r , i.e., {θ : (cid:107)θ − q(cid:107)2 ≤ r},
then one can discard all bi for which |qT bi | is smaller than a threshold determined by the common
tangent hyperplanes of the spheres (cid:107)θ − q(cid:107)2 = r and S p−1 . This “sphere test” is made precise in
the following lemma (All lemmata are proved in the supplemental material).
Lemma 1. If the solution ˜θ of (3) satisﬁes (cid:107)˜θ − q(cid:107)2 ≤ r , then |qT bi | < (1 − r) ⇒ ˜wi = 0.
El Ghaoui’s SAFE rule [13] is a sphere test of the simplest form. To see this, note that x/λmax is a
feasible point of (3), so the optimal θ cannot be further away from x/λ than x/λmax . Therefore we
have the constraint : (cid:107)˜θ − x/λ(cid:107)2 ≤ 1/λ − 1/λmax (solid red ball in Fig.1(c)). Plugging in q = x/λ
and r = 1/λ − 1/λmax into Lemma 1 yields El Ghaoui’s SAFE rule:
If |xT bi | < λ − 1 + λ/λmax , then ˜wi = 0.
Sphere Test # 1 (ST1/SAFE):
Note that the SAFE rule is weakest when λmax is large, i.e., when the codewords are very similar to
the data points, a frequent situation in applications [15]. To see that there is room for improvement,
consider the constraint: θT b∗ ≤ 1. This puts ˜θ in the intersection of the previous closed ball (solid
red) and H (b∗ ). This is indicated by the shaded green region in Fig. 1(c). Since this intersection is
small when λmax is large, a better test results by selecting R to be the shaded green region. However,
to simplify the test, we relax R to a closed ball and use the sphere test of Lemma 1. Two relaxations,
the solid magenta ball and the dotted blue ball in Fig. 1(c), are detailed in the following lemma.
(cid:107)θ − (x/λ − (λmax/λ − 1)b∗ (cid:107)2 ≤ (cid:112)1/λ2
Lemma 2. If θ satisﬁes (a) (cid:107)θ − x/λ(cid:107)2 ≤ 1/λ − 1/λmax and (b) θT b∗ ≤ 1, then θ satisﬁes
(cid:107)θ − x/λmax(cid:107)2 ≤ 2(cid:112)1/λ2
max − 1(λmax/λ − 1),
and
max − 1(λmax/λ − 1).
x/λmax and r = 2(cid:112)1/λ2
By Lemma 2, since ˜θ satisﬁes (a) and (b), it satisﬁes (5) and (6). We start with (6) because of its
similarity to the closed ball constraint used to derive ST1/SAFE (solid red ball). Plugging q =
max − 1(λmax/λ − 1) into Lemma 1 yields our ﬁrst new test:

(5)
(6)

3

FeasibleRegion0xSp−1b*P(b*)b1P(b1)b2P(b2)x/λmax→x/λθ0Sp−1qSp−10xb*x/λx/λmax0.60.8100.20.40.60.8(cid:104) / (cid:104)maxDiscarding Threshold(cid:104)max = 0.8  ST2, our new test.ST1/SAFE0.60.8100.20.40.60.8(cid:104) / (cid:104)maxDiscarding Threshold(cid:104)max = 0.9  ST2, our new test.ST1/SAFEIf |xT bi | < λmax (1 − 2(cid:112)1/λ2
max − 1(λmax/λ − 1)), then ˜wi = 0.
Sphere Test # 2 (ST2):
Since ST2 and ST1/SAFE both test |xT bi | against thresholds, we can compare the tests by plotting
their thresholds. We do so for λmax = 0.8, 0.9 in Fig.1(d). The thresholds must be positive and
large to be useful. ST2 is most useful when λmax is large. Indeed, we have the following lemma:
√
Lemma 3. When λmax >
3/2, if ST1/SAFE discards bi , then ST2 also discards bi .
(cid:112)1/λ2
Finally, to use the closed ball constraint (5), we plug in q = x/λ − (λmax/λ − 1)b∗ and r =
max − 1(λmax/λ − 1) into Lemma 1 to obtain a second new test:
If |xT bi − (λmax − λ)bT∗ bi | < λ(1 − (cid:112)1/λ2
Sphere Test # 3 (ST3):
max − 1(λmax/λ − 1)), then ˜wi = 0.
ST3 is slightly more complex. It requires ﬁnding b∗ and computing a weighted sum of inner prod-
ucts. But ST3 is always better than ST2 since its sphere lies strictly inside that of ST2:
Lemma 4. Given any x, b∗ and λ, if ST2 discards bi , then ST3 also discards bi .
√
3/2 ≈ 0.866, ST2

To summarize, ST3 completely outperforms ST2, and when λmax is larger than
completely outperforms ST1/SAFE. Empirical comparisons are given in §5.
By making two passes through the dictionary, the above tests can be efﬁciently implemented on
large-scale dictionaries that can’t ﬁt in memory. The ﬁrst pass holds x, u, bi ∈ Rp in memory at
once and computes u(i) = xT bi . By simple bookkeeping, after pass one we have b∗ and λmax .
The second pass holds u, b∗ , bi in memory at once, computes bT∗ bi and executes the test.

3 Random Projections of the Data
In §4 we develop a framework for learning a hierarchical dictionary and this involves the use of
random data projections to control information ﬂow to the levels of the hierarchy. The motivation for
using random projections will become clear, and is speciﬁcally discussed, in §4. Here we lay some
groundwork by studying basic properties of random projections in learning sparse representations.
We ﬁrst revisit the normalization assumption (cid:107)xj (cid:107)2 = (cid:107)bi(cid:107)2 = 1, 1 ≤ j ≤ n, 1 ≤ i ≤ m in §2. The
assumption that all codewords are normalized: (cid:107)bi (cid:107)2 = 1, is necessary for (1) to be meaningful,
otherwise we can increase the scale of bi and inversely scale the ith row of W to lower the loss. The
assumption that all data points are normalized: (cid:107)xj (cid:107)2 = 1, warrants a more careful examination.
To see this, assume that the data {xj }n
j=1 are samples from an underlying low dimensional smooth
manifold X and that one desires a correspondence between codewords and local regions on X . Then
we require the following scale indifference (SI) property to hold:
Deﬁnition 1. X satisﬁes the SI property if ∀x1 , x2 ∈ X , with x1 (cid:54)= x2 , and ∀γ (cid:54)= 0, x1 (cid:54)= γx2 .
Intuitively, SI means that X doesn’t contain points differing only in scale and it implies that points
x1 , x2 from distinct regions on X will use different codewords in their representation. SI is usually
implicitly assumed [9, 15] but it will be important for what follows to make the condition explicit. SI
is true in many typical applications of sparse representation. For example, for image signals when
we are interested in the image content regardless of image luminance. When SI holds we can indeed
normalize the data points to S p−1 = {x : (cid:107)x(cid:107)2 = 1}.
Since a random projection of the original data doesn’t preserve the normalization (cid:107)xj (cid:107)2 = 1, it’s
important for the random projection to preserve the SI property so that it is reasonable to renormalize
the projected data. We will show that this is indeed the case under certain assumptions. Suppose
we use a random projection matrix T ∈ Rd×p , with orthonormal rows, to project the data to Rd
(d < p) and use TX as the new data matrix. Such T can be generated by running the Gram-
Schmidt procedure on d, p-dimensional random row vectors with i.i.d. Gaussian entries. It’s known
≤ (1 + )(cid:112)d/p.
(1 − )(cid:112)d/p ≤ (cid:107)Tx1 − Tx2(cid:107)2
that for certain sets X , with high probability random projection preserves pairwise distances:
(7)
(cid:107)x1 − x2(cid:107)2
For example, when X contains only κ-sparse vectors, we only need d ≥ O(κ ln(p/κ)) and when X
is a K -dimensional Riemannian submanifold, we only need d ≥ O(K ln p) [16]. We will show that
when the pairwise distances are preserved as in (7), the SI property will also be preserved:

4

Theorem 1. Deﬁne S (X ) = {z : z = γx, x ∈ X , |γ | ≤ 1}. If X satisﬁes SI and ∀(x1 , x2 ) ∈
S (X ) × S (X ) (7) is satisﬁed, then T (X ) = {z : z = Tx, x ∈ X } also satisﬁes SI.
Proof. If T (X ) doesn’t satisfy SI, then by Deﬁnition 1, ∃(x1 , x2 ) ∈ X × X , γ /∈ {0, 1} s.t.:
(cid:107)Tx1 − γTx2 (cid:107)2/((1 − )(cid:112)d/p) = 0. So x1 = γx2 . This contradicts the SI property of X .
Tx1 = γTx2 . Without loss of generality we can assume that |γ | ≤ 1 (otherwise we can exchange
the positions of x1 and x2 ). Since x1 and γx2 are both in S (X ), using (7) gives that (cid:107)x1 − γx2(cid:107)2 ≤
For example, if X contains only κ-sparse vectors, so does S (X ). If X is a Riemannian submanifold,
so is S (X ). Therefore applying random projections to these X will preserve SI with high probability.
For the case of κ-sparse vectors, under some strong conditions, we can prove that random projection
always preserves SI. (Proofs of the theorems below are in the supplemental material.)
Theorem 2. If X satisﬁes SI and has a κ-sparse representation using dictionary B, then the pro-
jected data T (X ) satisﬁes SI if (2κ − 1)M (TB) < 1, where M (·) is matrix mutual coherence.

Combining (7) with Theorem 1 or 2 provides an important insight: the projected data TX contains
rough information about the original data X and we can continue to use the formulation (1) on TX
to extract such information. Actually, if we do this for a Riemannian submanifold X , then we have:
Theorem 3. Let the data points lie on a K -dimensional compact Riemannian submanifold X ⊂ Rp
with volume V , conditional number 1/τ , and geodesic covering regularity R (see [16]). Assume
that in the optimal solution of (1) for the projected data (replacing X with TX), data points Tx1
and Tx2 have nonzero weights on the same set of κ codewords. Let wj be the new representation
of xj and µi = (cid:107)Txj − Bwj (cid:107)2 be the length of the residual (j = 1, 2). With probability 1 − ρ:
2 ≤ (p/d)(1 + 1 )(1 + 2 )((cid:107)w1 − w2(cid:107)2
(cid:107)x1 − x2 (cid:107)2
1 + 2µ2
2 + 2µ2
2 )
(cid:107)x1 − x2 (cid:107)2
2 ≥ (p/d)(1 − 1 )(1 − 2 )((cid:107)w1 − w2(cid:107)2
2 ,
with 1 = O(( K ln(N V Rτ −1 ) ln(1/ρ)
)0.5−η ) (for any small η > 0) and 2 = (κ − 1)M (B).
d
Therefore the distances between the sparse representation weights reﬂect the original data point
distances. We believe a similar result should also hold when X contains only κ-sparse vectors.

(8)

4 Learning a Hierarchical Dictionary

Our hierarchical framework decomposes a large dictionary learning problem into a sequence of
smaller hierarchically structured dictionary learning problems. The result is a tree of dictionaries.
High levels of the tree give course representations, deeper levels give more detailed representations,
and the codewords at the leaves form the ﬁnal dictionary. The tree is grown top-down in l levels
by reﬁning the dictionary at the previous level to give the dictionary at the next level. Random data
projections are used to control the information ﬂow to different layers. We also enforce a zero-tree
constraint on the sparse representation weights so that the zero weights in the previous level will
force the corresponding weights in the next level to be zero. At each stage we combine this zero-tree
constraint with our new screening tests to reduce the size of Lasso problems that must be solved.
In detail, we use l random projections Tk ∈ Rdk×p (1 ≤ k ≤ l) to extract information incrementally
from the data in l stages. Each Tk has orthonormal rows and the rows of distinct Tk are orthogonal.
At level k we learn a dictionary Bk ∈ Rdk×mk and weights Wk ∈ Rmk×n by solving a small sparse
representation problem similar to (1):

min
Bk ,Wk
s.t.

(cid:107)TkX − BkWk (cid:107)2
F + λk (cid:107)Wk (cid:107)1
1
2
2 ≤ 1,
∀i = 1, 2, . . . , mk .
(cid:107)b(k)
i (cid:107)2
Here b(k)
is the ith column of matrix Bk and mk is assumed to be a multiple of mk−1 , so the
i
number of codewords mk increases with k . We solve (9) for level k = 1, 2, . . . , l sequentially.
An additional constraint is required to enforce a tree structure. Denote the ith element of the j th
column of Wk by w(k)
(i) and organize the weights at level k > 1 in mk−1 groups, one per level
j

(9)

5

(10)

(rmk−1 + i) = 0.

k − 1 codeword. The ith group has mk /mk−1 weights: {w(k)
(rmk−1 + i), 0 ≤ r < mk /mk−1},
j
and has weight w(k−1)
(i) as its parent weight. To enforce a tree structure we require that a child
weight is zero if its parent weight is zero. So for every level k ≥ 2, data point j (1 ≤ j ≤ n), group i
j
(1 ≤ i ≤ mk−1 ), and weight w(k)
(rmk−1 + i) (0 ≤ r < mk /mk−1 ), we enforce:
j
w(k−1)
(i) = 0 ⇒ w(k)
j
j
This imposed tree structure is analogous to a “zero-tree” in EZW wavelet compression [22]. In
addition, (10) means that the weights of the previous layer select a small subset of codewords to
j , (10) reduces the number of codewords from
enter the Lasso optimization. When solving for wk
mk to (mk /mk−1 )(cid:107)w(k−1)
(cid:107)0 , a considerable reduction since w(k−1)
is sparse. Thus the screening
rules in §2 and the imposed screening rule (10) work together to reduce the effective dictionary size.
j
j
The tree structure in the weights introduces a similar hierarchical tree structure in the dictionaries
rmk−1+i , 0 ≤ r < mk /mk−1 } are the children of codeword b(k−1)
{Bk }l
k=1 : the codewords {b(k)
. This
i
tree structure provides a heuristic way of updating Bk . When k > 1, the mk codewords in layer k
are naturally divided into mk−1 groups, so we can solve Bk by optimizing each group sequentially.
rmk−1+i ]mk /mk−1−1
This is similar to block coordinate descent. For i = 1, 2, . . . , mk−1 , let B(cid:48) = [b(k)
r=0
denote the codewords in group i. Let W(cid:48) be the submatrix of W containing only the (rmk−1 + i)th
rows of W, r = 0, 1, . . . , mk /mk−1 − 1. W(cid:48) is the weight matrix for B(cid:48) . Denote the remaining
codewords and weights by B(cid:48)(cid:48) and W(cid:48)(cid:48) . For all mk−1 groups in random order, we ﬁx B(cid:48)(cid:48) and
update B(cid:48) by solving (1) for data matrix TkX − B(cid:48)(cid:48)W(cid:48)(cid:48) . This reduces the complexity from O(mq
k )
k /mq−1
k−1 ) where O(mq ) is the complexity of updating a dictionary with size m. Since q ≥ 3,
to O(mq
this offers big computational savings but might yield a suboptimal solution of (9).
After ﬁnalizing Wk
and Bk , we can solve an unconstrained QP to ﬁnd Ck =
arg minC(cid:107)X − CWk (cid:107)2
F . Ck is useful for visualization purposes; it represents the points on the
original data manifold corresponding to Bk .
In principle, our framework can use any orthogonal projection matrix Tk . We choose random pro-
jections because they’re simple and, more importantly, because they provide a mechanism to control
the amount of information extracted at each layer. If all Tk are randomly generated independently
of X, then on average, the amount of information in TkX is proportional to dk . This allows us to
control the ﬂow of information to each layer so that we avoid using all the information in one layer.

5 Experiments

We tested our framework on: (a) the COIL rotational image data set [23], (b) the MNIST digit
classiﬁcation data set [24], and (c) the extended Yale B face recognition data set [25] [26]. The basic
sparse representation problem (1) is solved using the toolbox provided in [12] to iteratively optimize
B and W until an iteration results in a loss function reduction of less than 0.01%.
COIL Rotational Image Data Set: This is intended as a small scale illustration of our frame-
work. We use the 72, 128x128 color images of object No. 80 rotating around a circle in 15 degree-
increments (18 images shown in Fig.2(a)). We ran the traditional sparse representation algorithm to
compare the three screening tests in §2. The dictionary size is m = 16 and we vary λ. As shown
in Fig.2(c), ST3 discards a larger fraction of codewords than ST2 and ST2 discards a larger fraction
than ST1/SAFE. We ran the same algorithms on 200 random data projections and the results are
almost identical. The average λmax for these two situations is 0.98.
Next we test our hierarchical framework using two layers. We set (d2 , m2 ) = (200, 16) so that
the second layer solves a problem of the same scale as in the previous paragraph. We demonstrate
how the result of the ﬁrst layer, with (d1 , m1 , λ1 ) = (100, 4, 0.5), helps the second layer discard
more codewords when the tree constraint (10) is imposed. Fig.2(b) illustrates this constraint: the 16
second layer codewords are organized in 4 groups of 4 (only 2 groups shown). The weight on any
codeword in a group has to be zero if the parent codeword in the ﬁrst layer has weight zero. This
imposed constraint discards many more codewords in the screening stage than any of the three tests
in §2. (Fig.2(d)). Finally, the illustrated codewords and weights in Fig.2(b) are the actual values in

6

Figure 2: (a): Example images of the data set. (b): Illustration of a two layer hierarchical sparse representa-
tion. (c): Comparison of the three screening tests for sparse representation. (d): Screening performance in the
second layer of our hierarchical framework using combinations of screening criteria. The imposed constraint
(10) helps to discard signiﬁcantly more codewords when λ is small.

Figure 3: Left: MNIST: The tradeoff between classiﬁcation accuracy and average encoding time for various
sparse representation methods. Our hierarchical framework yields better performance in less time. The average
encoding time doesn’t apply to baseline methods. The performance of traditional sparse representation is
consistent with [9]. Right: Face Recognition: The recognition rate (top) and average encoding time (bottom)
for various methods. Traditional sparse representation has the best accuracy and is very close to a similar
method SRC in [8] (SRC’s recognition rate is cited from [8] but data on encoding time is not available). Our
hierarchical framework achieves a good tradeoff between the accuracy and speed. Using PCA projections in
our framework yields worse performance since these projections do not spread information across the layers.

C2 and W2 when λ2 = 0.4 (the marked point in Fig.2(d)). The sparse representation gives a multi-
resolution representation of the rotational pattern: the ﬁrst layer encodes rough orientation and the
second layer reﬁnes it.
The next two experiments evaluate the performance of sparse representation by (1) the accuracy of
a classiﬁcation task using the columns in W (or in [WT
l ]T for our framework) as
2 , . . . , WT
1 , WT
features, and (2) the average encoding time required to obtain these weights for a testing data point.
This time is highly correlated with the total time needed for iterative dictionary learning. We used
linear SVM (liblinear [27]) with parameters tuned by 10-fold cross-validations on the training set.

7

00.20.40.60.81020406080100(cid:104)Average percentage of discarded codewords in the prescreening.  Use constraint (13) and our new boundUse our new boundUse constraint (13) and El Ghaoui et al. 2010Use El Ghaoui et al. 201000.20.40.60.81020406080100(cid:104)Average percentage of discarded codewords in the prescreening.  Use our new bound on the origianl dataUse our new bound on the projected dataUse El Ghaoui et al. 2010 on the original dataUse El Ghaoui et al. 2010 on the projected data!"#$%&'()*#&&+*,-./&'()*#&&0(1&021&0,1&0/1&00.20.40.60.81020406080λAverage % of codewords discardedLearning non−hierarchical sparse representation  ST3, original dataST3, projected dataST2, original dataST2, projected dataST1/SAFE, original dataST1/SAFE, projected data00.20.40.60.81020406080λAverage % of codewords discardedLearning the second layer sparse representation  (10) + ST3ST3 only(10) + ST2ST2 only(10) + ST1/SAFEST1/SAFE only23510203091929394959697Average encoding time for a testing image (ms)Classification accuracy (%) on testing set  Traditional sparse representation:m=64, with 6 different λ settingsm=128, with 6 λ (same as above)m=192, with 6 λm=256, with 6 λm=512, with 6 λOur hierarchical framework:m1=32, m2=512, with 6 λm1=64, m2=2048, with 6 λm1=16, m2=256, m3=4096, with 6 λBaseline: the same linear classifierusing 250 principal componentsusing original pixel values32(0.1%)64(0.2%)128(0.4%)256(0.8%)5060708090100# of random projections (percentage of image size) to useRecognition rate (%) on testing set  Traditional sparse representationOur hierarchical frameworkOur framework with PCA projectionsLinear classifierWright et al., 2008, SRC32(0.1%)64(0.2%)128(0.4%)256(0.8%)020406080# of random projections (percentage of image size) to useAverage encoding time (ms)  Traditional sparse representationOur hierarchical frameworkOur framework with PCA projectionsLinear classifierMNIST Digit Classiﬁcation: This data set contains 70,000 28x28 hand written digit images (60,000
training, 10,000 testing). We ran the traditional sparse representation algorithm for dictionary size
m ∈ {64, 128, 192, 256} and λ ∈ Λ = {0.06, 0.08, 0.11, 0.16, 0.23, 0.32}. In Fig.3 left panel,
each curve contains settings with the same m but with different λ. Points to the right correspond to
smaller λ values (less sparse solutions and more difﬁcult computation). There is a tradeoff between
speed (x-axis) and classiﬁcation performance (y-axis). To see where our framework stands, we tested
the following settings: (a) 2 layers with (d1 , d2 ) = (200, 500), (m1 , m2 ) = (32, 512), λ1 = 0.23
and λ2 ∈ Λ, (b) (m1 , m2 ) = (64, 2048) and everything else in (a) unchanged, (c) 3 layers with
(d1 , d2 , d3 ) = (100, 200, 400), (m1 , m2 , m3 ) = (16, 256, 4096), (λ1 , λ2 ) = (0.16, 0.11) and λ3 ∈
Λ. The plot shows that compared to the traditional sparse representation, our hierarchical framework
achieves roughly a 1% accuracy improvement given the same encoding time and a roughly 2X
speedup given the same accuracy. Using 3 layers also offers competitive performance but doesn’t
outperform the 2 layer setting.
Face Recognition: For each of 38 subjects we used 64 cropped frontal face views under differing
lighting conditions, randomly divided into 32 training and 32 testing images. This set-up mirrors
In this experiment we start with the random projected data (p ∈ {32, 64, 128, 256}
that in [8].
random projections of the original 192x128 data) and use this data as follows: (a) learn a traditional
non-hierarchical sparse representation, (b) our framework, i.e., sample the data in two stages using
orthogonal random projections and learn a 2 layer hierarchical sparse representation, (c) use PCA
projections to replace random projections in (b), (d) directly apply a linear classiﬁer without ﬁrst
learning a sparse representation. For (a) we used m = 1024, λ = 0.030 for p = 32, 64 and
λ = 0.029 for p = 128, 256 (tuned to yield the same average sparsity for different p). For (b) we
used (m1 , m2 ) = (32, 1024), (d1 , d2 ) = ( 3
8 p), λ1 = 0.02 and λ2 the same as λ in (a). For (c)
8 p, 5
we used the same setting in (b) except random projection matrices T1 , T2 in our framework are now
set to the PCA projection matrices (calculate SVD X = USVT with singular values in descending
order, then use the ﬁrst d1 columns of U as the rows in T1 and the next d2 columns of U as the rows
in T2 ). The results in Fig.3 right panel suggest that our framework strikes a good balance between
speed and accuracy. The PCA variant of our framework has worse performance because the ﬁrst
8 p projections contain too much information, leaving the second layer too little information (which
3
also drags down the speed for lack of sparsity and structure). This reinforces our argument at the end
of §4 about the advantage of random projections. The fact that a linear SVM performs well given
enough random projections suggests this data set does not have a strong nonlinear structure.
Finally, at any iteration, the average λmax for all data points ranges from 0.76 to 0.91 in all settings in
the MNIST experiment and ranges from 0.82 to nearly 1 in the face recognition experiment (except
for the second layer in the PCA variant, in which average λmax can be as low as 0.54). As expected,
λmax is large, a situation that favors our new screening tests (ST2, ST3).

6 Conclusion

Our theoretical results and algorithmic framework effectively make headway on the computational
challenge of learning sparse representations on large size dictionaries for high dimensional data
The new screening tests greatly reduce the size of the lasso problems to be solved and the tests are
proven, both theoretically and empirically, to be much more effective than the existing ST1/SAFE
test. We have shown that under certain conditions, random projection preserves the scale indiffer-
ence (SI) property with high probability, thus providing an opportunity to learn informative sparse
representations with data fewer dimensions. Finally, the new hierarchical dictionary learning frame-
work employs random data projections to control the ﬂow of information to the layers, screening
to eliminate unnecessary codewords, and a tree constraint to select a small number of candidate
codewords based on the weights leant in the previous stage. By doing so, it can deal with large m
and p simultaneously. The new framework exhibited impressive performance on the tested data sets,
achieving equivalent classiﬁcation accuracy with less computation time.

Acknowledgements

This research was partially supported by the NSF grant CCF-1116208. Zhen James Xiang thanks
Princeton University for support through the Charlotte Elizabeth Procter honoriﬁc fellowship.

8

In Advances in

References
[1] M. Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image
Processing. Springer, 2010.
[2] G. Cao and C.A. Bouman. Covariance estimation for high dimensional data vectors using the sparse
matrix transform. In Advances in Neural Information Processing Systems, 2008.
[3] A.B. Lee, B. Nadler, and L. Wasserman. Treelets An adaptive multi-scale basis for sparse unordered data.
The Annals of Applied Statistics, 2(2):435–471, 2008.
[4] M. Gavish, B. Nadler, and R.R. Coifman. Multiscale wavelets on trees, graphs and high dimensional data:
Theory and applications to semi supervised learning. In International Conference on Machine Learning,
2010.
[5] M. Belkin and P. Niyogi. Using manifold stucture for partially labeled classiﬁcation.
Neural Information Processing Systems, pages 953–960, 2003.
[6] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323, 2000.
[7] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by
V1? Vision research, 37(23):3311–3325, 1997.
[8] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse representa-
tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210–227, 2008.
[9] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Advances in Neural
Information Processing Systems, volume 3, 2009.
[10] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, pages
407–451, 2004.
[11] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding.
The Journal of Machine Learning Research, 11:19–60, 2010.
[12] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. In Advances in Neural
Information Processing Systems, volume 19, page 801, 2007.
[13] L.E. Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Arxiv
preprint arXiv:1009.3515, 2010.
[14] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R.J. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Arxiv preprint arXiv:1011.2234, 2010.
[15] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. Huang, and S. Yan. Sparse representation for computer vision
and pattern recognition. Proceedings of the IEEE, 98(6):1031–1044, 2010.
[16] R.G. Baraniuk and M.B. Wakin. Random projections of smooth manifolds. Foundations of Computational
Mathematics, 9(1):51–77, 2007.
[17] Y. Lin, T. Zhang, S. Zhu, and K. Yu. Deep coding network. In Advances in Neural Information Processing
Systems, 2010.
[18] G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-
tation, 18(7):1527–1554, 2006.
[19] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary
learning. In International Conference on Machine Learning, 2010.
[20] M.B. Wakin, D.L. Donoho, H. Choi, and R.G. Baraniuk. Highresolution navigation on non-differentiable
image manifolds. In IEEE International Conference on Acoustics, Speech and Signal Processing, vol-
ume 5, pages 1073–1076, 2005.
[21] M.F. Duarte, M.A. Davenport, M.B. Wakin, JN Laska, D. Takhar, K.F. Kelly, and RG Baraniuk. Mul-
tiscale random projections for compressive classiﬁcation. In IEEE International Conference on Image
Processing, volume 6, 2007.
[22] J.M. Shapiro. Embedded image coding using zerotrees of wavelet coefﬁcients. IEEE Transactions on
Signal Processing, 41(12):3445–3462, 2002.
[23] S.A. Nene, S.K. Nayar, and H. Murase. Columbia object image library (coil-100). Techn. Rep. No.
CUCS-006-96, dept. Comp. Science, Columbia University, 1996.
[24] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278 –2324, nov 1998.
[25] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone models for
face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 23(6):643–660, 2002.
[26] K.C. Lee, J. Ho, and D.J. Kriegman. Acquiring linear subspaces for face recognition under variable
lighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 684–698, 2005.
[27] R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J. Lin. LIBLINEAR: A library for large linear
classiﬁcation. The Journal of Machine Learning Research, 9:1871–1874, 2008.

9


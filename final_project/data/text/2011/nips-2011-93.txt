Multiple Instance Learning on Structured Data

1Dan Zhang, 2Yan Liu, 1Luo Si, 3 Jian Zhang, 4Richard D. Lawrence
1. Computer Science Department, Purdue University, West Lafayette, IN 47906
2. Computer Science Department, University of Southern California, Los Angeles, CA 90089
3. Statistics Department, Purdue University, West Lafayette, IN 47906
4. Machine Learning Group, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598
1 {zhang168, lsi}@cs.purdue.edu, 2 yanliu.cs@usc.edu, 3 jian.zhang@gmail.com , 4 ricklawr@us.ibm.com

Abstract

Most existing Multiple-Instance Learning (MIL) algorithms assume data instances
and/or data bags are independently and identically distributed. But there often
exists rich additional dependency/structure information between instances/bags
within many applications of MIL. Ignoring this structure information limits the
performance of existing MIL algorithms. This paper explores the research prob-
lem as multiple instance learning on structured data (MILSD) and formulates a
novel framework that considers additional structure information.
In particular,
an effective and efﬁcient optimization algorithm has been proposed to solve the
original non-convex optimization problem by using a combination of Concave-
Convex Constraint Programming (CCCP) method and an adapted Cutting Plane
method, which deals with two sets of constraints caused by learning on instances
within individual bags and learning on structured data. Our method has the nice
convergence property, with speciﬁed precision on each set of constraints. Experi-
mental results on three different applications, i.e., webpage classiﬁcation, market
targeting, and protein fold identiﬁcation, clearly demonstrate the advantages of
the proposed method over state-of-the-art methods.

1

Introduction

Multiple Instance Learning (MIL) is a variation of the classical learning methods for problems with
incomplete knowledge on the instances (or examples) [4]. In a MIL problem, the labels are assigned
to bags, i.e., a set of instances, rather than individual instances [1, 4, 5, 13]. MIL has been widely
employed in areas such as text mining [1], drug design [4], and localized content based image
retrieval (LCBIR) [13].
One major assumption of most existing MIL methods is that instances (and bags) are independently
and identically distributed. But in many applications, the dependencies between instances/bags nat-
urally exist and if incorporated in models, they can potentially improve the prediction performance
signiﬁcantly. For example, in business analytics, big corporations often analyze the websites of dif-
ferent companies to look for potential partnerships. Since not all of the webpages in a website are
useful, we can treat the whole website of a speciﬁc company as a bag, and each webpage in this
website is considered as an instance. The hyperlinks between webpages provide important infor-
mation on various relationships between these companies (e.g. supply-demand or joint-selling) and
more partner companies can be identiﬁed if we follow the hyperlinks of existing partners. Another
example is protein fold identiﬁcation [15], whose goal is to predict protein fold with low conser-
vation in primary sequence, e.g., Thioredoxin-fold (Trx-fold). MIL algorithms have been applied
to identify new Trx-fold proteins, where each protein sequence is considered as a bag, and some of
its subsequences are instances. The relational information between protein sequences, such as same
organism locations or similar species origins, can be used to help the prediction tasks.

1

Several recent methods have been proposed to model the dependencies between instances in each
bag [11, 12, 21]. However, none of them takes into consideration the relational structure between
bags or between instances across bags. Furthermore, most of existing MIL research only uses con-
tent similarity for modeling structures between instances in each bag, but does not consider other
types of relational structure information (e.g. hyperlink) among instances or bags. While much re-
search work [16, 22] for traditional single instance learning has demonstrated that additional struc-
ture information (e.g., hyperlink) can be very useful, we believe this is similar for MIL.
Generally speaking, we summarize three scenarios of the structure information in MIL: (1) the
relational structures are on the instance level. For example, in the business partner example, the hy-
perlinks between different webpages can be considered as the relational structure between instances
(either in the same bag or across bags). (2) the structure information is available on the bag level.
For example, in protein fold identiﬁcation task, we can consider the phylogenetic tree to capture the
evolutionary dependencies between protein sequences . (3) the structure information is available on
both instance level and bag level. We refer these three scenarios of learning problems collectively
as multiple instance learning on structured data (MILSD).
In this paper, we propose a general framework that address all three structure learning scenarios
for MIL. The model consists of a regularization term that conﬁnes the capacity of the classiﬁer,
a term that penalizes the difference between the predicted labels of the bags and their true labels,
and a graph regularization term based on the structure information. The corresponding optimization
problem is non-convex. But we show that it can be expressed as the difference between two convex
functions. Then, we employ an iterative method – Constrained Concave-Convex Procedure (CCCP)
[14, 19] to solve this problem. To make the proposed method scalable to large datasets, the Cutting
Plane method [8] is adapted to solve the subproblems derived from each CCCP iteration. The novelty
of the proposed variant of Cutting Plane method lies in modeling dual sets of constraints, i.e., one
from modeling instances in individual bags, and the other from the structure information, and its
ability to control the precisions (i.e., 1 and 2 in Table 1) on different sets of constraints separately.
The reason why we need to control precisions separately is that since different sets of constraints
normally are derived from various sources and have different forms, their characteristics, as well as
the required optimization precisions, are very likely to be diverse. Furthermore, we prove an upper
bound of the convergence rate of the proposed optimization method, which is a signiﬁcant result
given our optimization scheme for dual constraint sets can also be applied to many other learning
problems. Experiments on three applications demonstrate the advantages of the proposed research.

2 Methodology

2.1 Problem Statement and Notation
Suppose we are given a set of n labeled bags {(Bi , Yi ), i = 1, 2, · · · , n}, u unlabeled bags
{Bi , i = n + 1, n + 2, · · · , n + u}, and a directed or undirected graph G = (V , E ) that depicts
the structure between either bags or instances. Here, the instances in the bag Bi are denoted as
{Bi1 , Bi2 , ..., Bini } ∈ X , where ni is the total number of instances in this bag and Yi ∈ {−1, 1}.
Each node v ∈ V corresponds to either a bag or an instance in either the labeled set or the unlabeled
set, and the j -th edge ej = (p, q) ∈ E represents a link from node p to node q . The task is to
learn a classiﬁer w1 based on labeled, unlabeled bags, and the predeﬁned structure graph so that the
unlabeled bags can be correctly classiﬁed. The soft label for the instance x can be estimated by:
f (x) = wT Bij . The soft label of the bag Bi can be modeled as: f (Bi ) = maxj∈Bi wT Bij , and if
f (Bi ) > 0, this bag would be labeled as positive and otherwise negative.

2.2 Formulation

Our motivation is that labeled bags should be correctly classiﬁed and the soft labels of the bags or
instances deﬁned on the graph G should be as smooth as possible. Speciﬁcally, a pair of nodes
linked by an edge tend to possess the same label and therefore the nodes lying on a densely linked
subgraph are likely to have the same labels [20]. The general formulation of MILSD is given as:

1Without loss of generality, in this paper, we only consider linear classiﬁers. Here, the bias of the classiﬁer
is absorbed by the feature vectors. The kernel version [3] of the proposed method can be easily derived.

2

minw Hr (w) + Hd (w) + HG (w), where Hr (w) is a regularization term based on w, and depicts
the capacity of this classiﬁer. One of the possible options, which is also the one used in this paper,
(cid:80)n
is (cid:107)w(cid:107)2 . Hd (w) penalizes the difference between the estimated bag labels and the given labels. In
this paper, without loss of generality, the hinge loss is used [3]. So, given a classiﬁer w, Hd (w) is
(cid:12)(cid:12)(cid:12)(cid:12) f (vp )√
(cid:12)(cid:12)(cid:12)(cid:12),
i=1 max{0, 1−maxj∈Bi YiwT Bij }, where C is the trade off parameter. HG (w)
(cid:80)
calculated as: C
n
is a graph regularization term based on the given graph G that enforces the smoothness on the soft
− f (vq )√
labels of nodes in the given graph, which can be deﬁned as: µ|E |
(p,q)∈E w(p, q)
d(p)
d(q)
where vp and vq are two nodes in the graph. w(p, q) is a weight function that measures the weight
on the edge (p, q). d(p) and d(q) are the outgoing degrees for the node vp and vq respectively
[20]. |E | is the number of edges in graph G. Depending on which one of the three scenarios the
graph is deﬁned, we name the formulation where the graph is deﬁned on instances as I-MILSD, the
formulation where the graph is deﬁned on bags as B-MILSD, and the formulation where the graph
(cid:12)(cid:12)(cid:12)(cid:12), where xp
(cid:12)(cid:12)(cid:12)(cid:12) wT xp√
is deﬁned on both bags and instances as BI-MILSD. In particular,
(cid:80)
− wT xq√
1. For I-MILSD, HG (w) can be deﬁned as µ|E |
(p,q)∈E w(p, q)
d(q)
d(p)
(cid:12)(cid:12)(cid:12)(cid:12) maxj∈Bp wT Bpj
2. For B-MILSD, HG (w) is deﬁned as µ|E | × (cid:80)
and xq are two instances.
√
√
− maxj∈Bq wT Bqj
(p,q)∈E w(p, q)
d(p)
d(q)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) maxj∈Bp wT Bpj
is deﬁned. Then, the B-MILSD problem can be formulated as follows:
|E | × (cid:88)
n(cid:88)
(cid:112)d(p)
(cid:112)d(q)
− maxj∈Bq wT Bqj
µ
C
1
min
n
2
w
(p,q)∈E
i=1
s.t. ∀i ∈ {1, 2, . . . , n},
wT Bij ≥ 1 − ξi ,
(1)
Yi max
j∈Bi

(cid:107)w(cid:107)2 +

ξi +

w(p, q)

(cid:12)(cid:12)(cid:12)(cid:12)

where ξi is the hinge loss and µ is the trade-off parameter for the graph term. The formu-
lation proposed in [1] is a special case of the proposed formulation, with µ equals zero.

3. The deﬁnition of HG (w) in BI-MILSD can be considered as a combination of previous
two formulations.

In the following sections, our focus will be on the more challenging problem as B-MILSD, while I-
MILSD and BI-MILSD can be solved in a similar way, since the HG (w) in I-MILSD is convex and
the formulation of BI-MILSD can be considered as a combination of the B-MILSD and I-MILSD.

2.3 Optimization Procedure with CCCP and Multi-Constraint Cutting Plane

The formulation in problem (1) combines both the goodness-of-ﬁt for labeled bags and the structure
information embedded in the graph. However, since both HG (w) and the constraints in problem
(1) are non-convex, the global optimal solution of this problem cannot be attained. To solve this
problem, the constrained concave-convex procedure (CCCP) is used. It is an optimization method
that deals with the concave convex objective function with concave convex constraints [14]. In this
paper, without loss of generality, we only assume w(p, q) to be a canonical weight function. To em-
ploy CCCP, ﬁrst of all, for each edge (p, q), a non-negative loss variable ζ(p,q) is introduced. Then,
problem (1) can be solved iteratively. In particular, given an initial point w(0) , CCCP iteratively
computes w(t+1) from w(t) 2 by replacing maxj∈Bi wT Bij with its ﬁrst order Taylor expansions
at w(t) , and solving the resulting quadratic programming problem as follows, until convergence
(u(t)
i = arg maxj∈{1,...,ni } (w(t) )T Bij ).

2The superscript t is used to denote that the result is obtained from the t-th CCCP iteration. For example,
w(t) is the optimized classiﬁer from the t-th CCCP iteration step.

3

n(cid:88)
C
1
min
w,ξi≥0,ζ(p,q)≥0
n
2
i=1
s.t. ∀i ∈ {1, 2, . . . , n},

(cid:107)w(cid:107)2 +

(cid:88)
(p,q)∈E
YiwT B
iu

µ
|E |

ξi +

ζ(p,q)

(2)

∀(p, q) ∈ E ,

(t)
i
∀k ∈ {1, 2, . . . , np },

≥ 1 − ξi
wT Bpk(cid:112)d(p)
q(cid:112)d(q)
− wT B
≤ ζ(p,q)
(t)
qu
p(cid:112)d(p)
(cid:112)d(q)
− wT B
wT Bq(k−np )
∀k ∈ {np + 1, . . . , np + nq },
(t)
pu
The problem (2) can be directly solved as a standard quadratic programming problem [2]. However,
in many real world applications, the number of the labeled bags as well as the number of links
between bags are huge.
In this case, we would need to ﬁnd a way that can solve this problem
efﬁciently.
Instead of directly solving this optimization problem, we employ the Cutting Plane
method [8], which has shown its effectiveness and efﬁciency in solving similar tasks recently [6].
But different from the method employed in [6], in this paper, we need to deal with two sets of
constraints, rather than just one constraint set, with speciﬁed precisions separately. A new way to
adapt the Cutting Plane method is devised here. Problem (2) is equivalently transformed to the
following form:

≤ ζ(p,q) .

(3)

τjk (

) +

wT

≤ ζ ,

)

(cid:33)

ci − ξ

τj (k+np ) (

(cid:107)w(cid:107)2 + C ξ + µζ

1
min
w,ξ≥0,ζ≥0
2
s.t. ∀c ∈ {0, 1}n ,

n(cid:88)
n(cid:88)
≥ 1
1
ciYiB
np+nq(cid:88)
(cid:92)
n
n
iu
i=1
i=1
∀(τ ∈ {0, 1}|E |×(np+nq ) )
τjk ≤ 1)
(∀ej ∈ E ,
(cid:32) np(cid:88)
|E |(cid:88)
nq(cid:88)
q(cid:112)d(q)
Bpk(cid:112)d(p)
Bqk(cid:112)d(q)
p(cid:112)d(p)
k=1
− B
− B
wT
(t)
(t)
qu
pu
|E |
j=1
k=1
k=1
where, ej = (p, q), τ is a matrix with |E | rows and a varying number of columns: for the j -th row
of τ , it has np + nq columns (possible constraints). For each edge, at most one constraint could be
(cid:80)n
(cid:80)
activated for each feasible τ .
Theorem 1: Any solution w∗ of problem (2) is also a solution to problem (3) (and vice versa) with
ξ ∗ = 1
(p,q)∈E ζ ∗
i=1 ξ ∗
i and ζ ∗ = 1|E |
3 .
(p,q)
n
Proof: Please refer to the supplemental materials in the author’s homepage.
The beneﬁt of making this transformation is that, as we shall see later, during each Cutting Plane it-
eration at most two constraints will be added and therefore the ﬁnal solution would be extremely
sparse, with the number of non-zero dual variables independent of the number of training ex-
amples. Now the problem turns to how to solve the problem (3) efﬁciently, which is convex,
but contains two sets of exponential number of constraints due to the large number of feasible c
and τ . We present a novel adaption of the Cutting Plane method that can handle the two sets
of constraints simultaneously. More speciﬁcally, the main motivation of the method proposed
i.e., Ω1 and Ω2 from constraint sets in Eq.(3).
here is to ﬁnd two small subsets of constraints,
i=1 ci − (ξ + ε1 ) and ∀(τ ∈ {0, 1}|E |×(np+nq ) ) (cid:84)(∀ej ∈ E , (cid:80)np+nq
n wT (cid:80)n
(cid:80)n
With these two sets of selected constraints, the solution of the corresponding relaxed problem
satisﬁes all the constraints from problem (3) up to two precisions 1 and 2 , i.e., ∀c ∈ {0, 1}n :
(cid:18)(cid:80)np
(cid:19)
τjk ≤
≥ 1
1|E | wT (cid:80)|E |
) + (cid:80)nq
1
i=1 ciYiB
(t)
k=1
n
iu
i
p√
k=1 τj (k+np ) ( Bqk√
q√
k=1 τjk ( Bpk√
− B
− B
≤ (ζ + ε2 ).
(t)
(t)
It
pu
qu
1):
)
j=1
d(p)
d(q)
d(q)
d(p)
indicates that the two remaining sets of constraints (that are not added to Ω1 and Ω2 ) will not be
violated up to two precisions ε1 and ε2 respectively, and therefore they do not need to be added to
Ω1 and Ω2 explicitly.
3 the subscript ∗ denotes the optimal value of the corresponding variable.

(t)
i

4

,

),

∗

)

< 1

(t)
iu
i

cts
i =

if Yi (wts )T B

4
2 iteratively, which starts from two empty sets Ωt0
1 and Ωt
The proposed method constructs Ωt
1
and Ωt0
2 respectively. During the s-th Cutting Plane iteration, based on the wts , the most violated
(cid:40) 1,
constraint for Ωts
1 can be computed as:
0,

otherwise
(cid:40)
 1,
(cid:92)
and the most violated constraint for Ωts
2 can be computed as:
q(cid:112)d(q)
Bpk(cid:112)d(p)
Bq(k−np )(cid:112)d(q)
− B
(t)
qu
k∈{1,...,np }(wts )T (
max
(max
if (k = k
0, otherwise,
(cid:26)
(5)
p√
√
q√
maxk∈{1,...,np } (wts )T ( Bpk√
− B
− B
k∗ = arg maxk
Bq(k−np )
(t)
(t)
pu
qu
), maxk∈{np+1,...,np+nq } (wts )T (
d(q)
d(p)
d(q)
d(p)
After calculating these two sets of most violated constraints, the two stopping conditions can be
(cid:33)
(cid:32)
n(cid:88)
n(cid:88)
computed:
(wts )T
≥ 1
n
n
i=1
i=1
np+nq(cid:88)
Bpk(cid:112)d(p)
k=np+1

i − (ξ ts + ε1 )
cts
Bqk(cid:112)d(q)

H ts
1 =
 np(cid:88)
|E |(cid:88)
j=1
k=1

cts
i YiB
q(cid:112)d(q)
− B
(t)
qu

,
p(cid:112)d(p)
− B
(t)
pu

k∈{np+1,...,np+nq }(wts )T (
max

(wts )T
|E | ×

τ ts
j (k−np ) (

(6)
 ≤ (ζ ts + ε2 )).
(7)

τ ts
jk (

iu

(t)
i

) +

)

(4)

(cid:41)

)

> 0)

p(cid:112)d(p)
− B
(t)
pu
(cid:27)

)

.

τ ts
jk =

H ts
2 =(

1
min
w,ξ≥0,ζ≥0
2
s.t. ∀c ∈ Ωts
1 ,

The Cutting Plane iteration will terminate if both conditions H ts
1 and H ts
2 are true. Otherwise, cts
will be added to Ωts
1 if H ts
1 is false, and τ ts will be added to Ωts
2 if H ts
2 is false. Then, the new
optimization problem turns to:
(cid:107)w(cid:107)2 + C ξ + µζ
n(cid:88)
n(cid:88)
≥ 1
ci − ξ
(cid:32) np(cid:88)
wT
ciYiB
|E |(cid:88)
nq(cid:88)
(t)
n
iu
i
Bqk(cid:112)d(q)
q(cid:112)d(q)
Bpk(cid:112)d(p)
i=1
i=1
− B
(t)
qu
j=1
k=1
k=1
This optimization problem can be solved efﬁciently through the dual form [2].

p(cid:112)d(p)
− B
(t)
pu

∀τ ∈ Ωts
2 ,

τj (k+np ) (

wT
|E |

≤ ζ .

(cid:33)

τjk (

1
n

) +

(8)

)

2.4 Analysis and Discussions

2 (cid:107)w(t)(cid:107)2+C ξ (t)+µζ (t) . The
The whole algorithm of B-MILSD is described in Table 1. Here, J t = 1
convergence of the proposed method is guaranteed. Given an initial w, the outer CCCP iteration has
already been proved to converge to a local optimal solution [14]. The ﬁnal solution can be improved
by running this algorithm several times and picking the solution with the smallest J (t) value. We
will show that the Cutting Plane iterations with two different sets of constraints converge in a ﬁxed
number of steps through the following two theorems.
Theorem 2: For each Cutting Plane iteration described in Table 1, the objective function of (8) will
be increased by at least κ = min{ C 1
2)R2 }, where R2 = maxi,j B2
2
2 , 2
√
(1+2 )2
8R2 , µ2
ij .
2 ,
16R2 ,
1
2
(24+16
Sketch of Proof: The detailed proof of this theorem can be found in the supplemental materials.
Here, we only brieﬂy outline the way how we proved it. In each Cutting Plane iteration described
in Table 1, there are three possibilities for updating the constraints. In each case, we will ﬁnd a
feasible direction for increasing the objective function. A line search method will then be used to

4Here, ts denotes the s-th Cutting Plane iteration for solving the problem from the t-th CCCP iteration.

5

Table 1: The description of B-MILSD
Input: 1. Labeled bags: {(Bi , Yi ), i = 1, 2, · · · , n}; 2. Unlabeled Bags: {Bi , i = n + 1, 2, · · · , n + u}; 3. A graph G which represents the
relationship between these bags. (The graph can be built solely on the labeled bags, or on an union of both the labeled bags and the unlabeled bags); 4. parameters:
loss weight C and µ, CCCP solution precision δ , Cutting Plane solution precision for constraint1: 1 , Cutting Plane solution precision for constraint2: 2 .
Output: The classiﬁer w.
CCCP Iterations:
1. Initialize w0 ,t=0,∆J = 103 , J −1 = 103 .
2. while ∆J/J t−1 > δ do
2 = φ and s = −1.
t0
t0
3. Derive problem (2). Set the constraint set Ω
1 = φ, Ω
Cutting Plane Iterations:
repeat
s = s + 1.
Get (w(ts ) , ξ(ts ) , ζ (ts ) ) by solving (8).
Compute the most constraints, i.e., cts , and τ ts by Eq.(4) and Eq.(5).
(cid:83) cts , if H ts
Compute the stopping criteria, i.e., H ts
1 , and H ts
2 by Eq.(6) and Eq.(7).
t(s+1)
Update Ωts
= Ωts
1 is false. Otherwise, Ω
1 by Ω
(cid:86) H ts
1
1
t(s+1)
= Ωts
false. Otherwise, Ω
2 .
2
while H ts
2 is false
10.
1
11.
t = t + 1.
12. w(t) = w(t−1)s , ξ(t) = ξ(t−1)s , and ζ (t) = ζ ((t−1)s .
13. ∆J = J t−1 − J t .
14.end while

(cid:83) τ ts if H ts
2 is

1 . Update Ωts
= Ωts
2 by Ω

4.
5.
6.
7.
8.
9.

t(s+1)
1

ts+1
2

= Ωts
2

ﬁnd the optimal increment, which serves as the lower bound for each updating. (1) H ts
1 is false.
H ts
2 is true. cts is added to Ωts
1 . The minimal improvement of the objective function for problem
(8) after this constraint is added would be min{ C 1
8R2 }.
2 , 2
(2) H ts
is true. H ts
is false. Ωts
1
1
2
2
16R2 }.
In this case, the minimal increment will be min{ µ2
2
(3)
is updated by appending τ ts .
2 ,
2
2 are false. The most violated constraints are added to both Ωts
Both H ts
1 and H ts
1 and Ωts
2 . We
proved that the minimal increment is min{ min{C,µ}(1+2 )
2)R2 }. By integrating all of
√
(1+2 )2
,
2
(24+16
these three cases, it is clear that for each Cutting Plane iteration, the minimal increment is κ =
2 } ≤ min{C,µ}(1+2 )
min{ C 1
2)R2 }, since min{ C 1
2 , 2
2
√
(1+2 )2
2 , µ2
8R2 , µ2
.
2 ,
16R2 ,
1
2
2
(24+16
Theorem 3: The proposed Cutting Plane iteration terminates after at most C
κ steps, where, κ =
2)R2 }, and R2 = maxi,j B2
min{ C 1
2
2 , 2
√
(1+2 )2
8R2 , µ2
ij .
2 ,
16R2 ,
1
2
(24+16
Proof: w = 0, ξ = 1, ζ = 0 is a feasible solution for problem (3). Therefore, the objective function
of (3) is upper bounded by C , and should be lower bounded by 0. Given the conclusion from
κ steps.
Theorem 2, it is clear that the Cutting Plane iteration will terminate within C
The Cutting Plane method has already been employed in several previous works. In [6, 7, 17], the
authors adapted the Cutting Plane method to accelerate structural SVM related algorithms. However,
these works do not explicitly consider the case when several different sets of constraints with speci-
ﬁed precisions are involved. The novelty of the proposed method lies on its ability to control these
optimization precisions separately, and meanwhile it still enjoys the sparseness of the ﬁnal solution
with respect to the number of dual variables, which is brought by slack variable transformation.
In [18], the authors solved the problem of structural SVM with latent variables by employing CCCP
and the bundle method [9]. MIL problem itself can be considered as a special case of latent variable
problem. But the major limitation of [18] is that they cannot incorporate the relational information
into the formulation, and therefore cannot be used here. Furthermore, [18] does not consider dual
sets of constraints in optimization, which is less appropriate than the proposed optimization method.

3 Experiments

3.1 Webpage Classiﬁcation

In webpage classiﬁcation, each webpage can be considered as a bag, and its corresponding passages
represent its instances [1]. The hyperlinks between different webpages are treated as the additional
relational structure/links between different bags. WebKB5 dataset is used in experiments. There are
5 http://www.cs.cmu.edu/∼webkb/

6

Figure 1: Classiﬁcation and CPU Time Comparisons

(a)

(e)

(b)

(c)

(d)

(f)

(g)

(h)

in total 8280 webpages in this dataset. The webpages without any incoming and outgoing links are
deleted, and 6883 webpages are left. The three most frequently appeared categories, i.e., student,
course, and faculty, are used for classiﬁcation, where each sub-dataset contains all of the web-
pages/bags from one of the three categories, and the same number of the negative bags randomly
sampled from the remaining six categories in WebKB. The hyperlinks between these webpages are
used as the structure/link information. The tf-idf (normalized term frequency and log inverse docu-
ment frequency) [10] features are extracted for each passage, and the stop words are removed. We
use porter as the stemmer.
In the proposed method, C and µ are set by 5-fold cross validation through the grids 2[−5:5] and
[0, 0.01, 0.1, 1] respectively on the training set. To show the effects of the structure information on
the performance of MIL methods, we compare the proposed method with the instance-based multi-
ple instance support vector machine (I-miSVM) as well as the bag-based multiple instance support
vector machine (B-miSVM) [1]. The formulation of these two methods can be considered as a spe-
cial case of the proposed method with µ equals zero, and they are two different heuristic iterative
ways of implementing the same formulation. Their parameters are also set by 5 fold cross valida-
tions. Link-Content Matrix Factorization (LC-MF) is a non-MIL matrix factorization method [22],
which has been shown to outperform several alternatives, including SVM. We conduct experiments
with LC-MF based on the single instances that we extract for the same set of examples, and the
corresponding links. Similar to [22], the number of latent factors is set to be 50. After computing
the latent factors, a linear SVM is trained on the training set with the hinge loss parameter C being
determined by using 5-fold cross validation. For each experiment, a ﬁxed ratio of bags are chosen
as the training set, while the remaining examples are treated as the testing set. The average results
over 20 independent runs are reported on the training ratios [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5].
The classiﬁcation results are reported in Fig.1(a)(b)(c) and the CPU time comparison results are
announced in Fig.1(e)(f)(g). In Table 2, we further report the performances when the training ratio
equals 0.2. From these experimental results, it is clear that the performance of the proposed method
is better than the other comparison methods in accuracy and its CPU time is comparatively low.

3.2 Market Targeting

Market targeting is a popular topic for big corporations.
Its basic objective is to automatically
identify potential partners. One of the feasible market targeting strategy is to analyze the websites of
the potential partners. But usually not all of the webpages are useful for partner identiﬁcation. So,
it is better to formulate it as a MIL problem, in which each website is considered as a bag, and its

7

00.10.20.30.40.50.650.70.750.80.850.90.951Training RatioAccuracyCourse  B−MILSDLC−MFI−miSVMB−miSVM00.10.20.30.40.50.750.80.850.90.95Training RatioAccuracy  B−MILSDLC−MFI−miSVMB−miSVMFaculty00.10.20.30.40.50.70.750.80.850.9Training RatioAccuracyStudent  B−MILSDLC−MFI−miSVMB−miSVM0.10.20.30.40.50.10.150.20.250.30.350.40.450.5Training RatioAUCASCOT  B−MILSDLC−MFI−miSVMB−miSVM00.10.20.30.40.5020040060080010001200Training RatioTime (in Seconds)Course  B−MILSDLC−MFI−miSVMB−miSVM00.10.20.30.40.50100200300400500600700Training RatioTime (in Seconds)Faculty  B−MILSDLC−MFI−miSVMB−miSVM00.10.20.30.40.5020040060080010001200Training RatioTime (in Seconds)Student  B−MILSDLC−MFI−miSVMB−miSVM0.10.20.30.40.5050100150200Training RatioTime (in Seconds)ASCOT  B−MILSDLC−MFI−miSVMB−miSVMassociated webpages are considered as instances. Two related companies may be connected through
hyperlinks in some of their webpages.
We obtained a dataset (ASCOT) from a big international corporation. In ASCOT, the webpages of
around 225 companies are crawled. 25 of the companies/bags are labeled as positive, since they
are partners of this corporation, while the remaining 200 companies/bags are treated as negative
ones. For each company, the webpages with less than 100 unique words are removed and at most
50 webpages with the largest number of unique words6 are selected as instances. The hyperlinks
between webpages of different companies are treated as the structure information. For each experi-
ment, we ﬁx the training ratio of positive and negative bags, while the remaining bags are considered
as the testing set. The averaged results over 20 independent runs are reported on the training ratios
[0.1, 0.2, 0.3, 0.4, 0.5]. The parameters for different methods are tuned in the same way as on We-
bKB. But for the ratios 0.1 and 0.2, we use 3-fold cross validation due to the lack of positive bags.
For LC-MF, experiments are conducted on the instances which are the averages of the instances in
each bag. Because of the extremely imbalanced nature of this dataset, the Area Under Curve (AUC)
is used as the measure criteria.
The corresponding results are reported in Fig.1(d) and Fig.1(h). In Table 2, we report the perfor-
mances when the training ratio equals 0.2. On this dataset, B-MILSD performs much better than
the comparison methods, especially when the ratio of training examples is low. This is because the
hyperlink information helps a lot when the content information is rare in MIL, and the MIL setting
is useful to eliminate the useless instances especially when the supervised information is scare.

3.3 Protein Fold Identiﬁcation

In protein fold identiﬁcation [15], the low conservation of primary sequence in protein superfamilies
such as Thioredoxin-fold (Trx-fold) makes conventional modeling methods, such as Hidden Markov
Models difﬁcult to use. MIL can be used to identify new Trx-fold proteins naturally, in which each
protein sequence is considered as a bag, and some of its subsequences are considered as instances.
Here, we use a benchmark protein dataset7 . In each protein’s primary sequence, ﬁrst of all, the
primary sequence motif (typically CxxC) are found. Then, a window of size 214 around it are
extracted and aligned. These windows are then mapped to a 8-dimensional feature space. The
similarities between different proteins are estimated by using clustalw8 . If the score between a pair
of proteins exceed 25, then we consider there exists a link between them.
Following the experiment setting in [15], we conduct 5 fold cross validation to test the performances.
The averaged classiﬁcation accuracies and CPU Running Time are reporeted in Table 2. From the
comparison methods, we can see that on this dataset, the proposed method is both efﬁcient and
effective. Its CPU running time is almost 10 – 100 times faster than the comparison methods.

Table 2: Performance Comparisons
Measure
B-miSVM
I-miSVM
LC-MF
95.9
94.3
94.5
AUC
Time (seconds)
95.6
23.9
648.5
Accuracy (%)
93.4
93.3
95.3
591.6
29.7
360.6
Time (seconds)
89.1
89.5
91.7
526.3
41.2
540.4

ASCOT

Protein

B-MILSD
0.350
76.0
96.2
1.7

LC-MF
0.248
56.4
95.2
16.9

I-miSVM
0.264
20.9
92.2
160.3

B-miSVM
0.230
20.7
82.7
73.8

Course

Faculty

Student

Measure
Accuracy (%)
Time (seconds)
Accuracy (%)
Time (seconds)
Accuracy (%)
Time (seconds)

B-MILSD
97.2
49.1
95.2
73.9
92.7
245.7

4 Conclusions

This paper presents a novel machine learning problem – multiple instance learning on structured
data (MILSD) for incorporating additional structure information into multiple instance learning.
In particular, a general framework of MILSD is proposed for dealing with the additional structure
information in different scenarios. An effective and efﬁcient optimization method is proposed for
MILSD by combining the CCCP method and a new multi-constraint Cutting Plane method. Some
theoretical results are proved to justify the methodology that we employed to handle multi-sets of

6Still, we use porter as the stemmer and have removed the stop words.
7 http://cse.unl.edu/∼ qtao/datasets/mil dataset Trx protein.html
8 http://www.ebi.ac.uk/Tools/msa/clustalw2/

8

constraints with the Cutting Plane method. The experimental results on three different applications
clearly demonstrate the advantages of the proposed method. For future work, we plan to adapt the
current framework to solve multi-view multiple instance learning on structured data.
Acknowledgement: The work of Dan Zhang and Luo Si was partially supported by NSF research
grant IIS-0746830, CNS-1012208, IIS-1017837, and the Center for Science of Information (CSoI),
an NSF Science and Technology Center, under grant agreement CCF-0939370. The work of Yan Liu
was partially sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under
the Anomaly Detection at Multiple Scales (ADAMS) program, Agreement Number W911NF-11-
C-0200. The authors would also like to express their sincere thanks to Prof. S.V.N. Vishwanathan
and the anonymous reviewers for their constructive suggestions.

References
[1] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance
learning. In NIPS, 2003.
[2] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Press, 2004.
[3] B.Scholkopf and A.Smola. Learning with Kernels. MITPress, Cambridge, MA, 2002.
[4] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. Solving the multiple instance problem
with axis-parallel rectangles. In Artiﬁcial Intelligence, 1998.
[5] T. G ¨artner, P.A. Flach, A. Kowalczyk, and A.J. Smola. Multi–instance kernels. In ICML, 2002.
[6] T. Joachims. Training linear SVMs in linear time. In KDD, 2006.
[7] T. Joachims, T. Finley, and C.N.J. Yu. Cutting-plane training of structural SVMs. Machine
Learning, 2009.
[8] JE Kelley Jr. The cutting-plane method for solving convex programs. JSIAM, 1960.
[9] Krzysztof C. Kiwiel. Proximity control in bundle methods for convex nondifferentiable mini-
mization. Math. Program., 46:105–122, 1990.
[10] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. Introduction to Informa-
tion Retrieval. Cambridge University Press, 2008.
[11] Amy McGovern and David Jensen. Identifying predictive structures in relational data using
multiple instance learning. In ICML, 2003.
[12] G.J. Qi, X.S. Hua, Y. Rui, T. Mei, J. Tang, and H.J. Zhang. Concurrent multiple instance
learning for image categorization. In CVPR, 2007.
[13] R. Rahmani and S.A. Goldman. MISSL: Multiple-instance semi-supervised learning. In ICML,
2006.
[14] A.J. Smola, SVN Vishwanathan, and T. Hofmann. Kernel methods for missing variables. In
AISTATS, 2005.
[15] Qingping Tao, Stephen D. Scott, N. V. Vinodchandran, and Thomas Takeo Osugi. Svm-based
generalized multiple-instance learning via approximate box counting. In ICML, 2004.
[16] Benjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin markov networks.
NIPS, 2003.
[17] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured
and interdependent output variables. JMLR, 2006.
[18] Chun-Nam John Yu and T. Joachims. Learning structural svms with latent variables. In ICML,
2009.
[19] A. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 2003.
[20] D. Zhou, J. Huang, and B. Scholkopf. Learning from labeled and unlabeled data on a directed
graph. In ICML, 2005.
[21] Z-H Zhou, Y-Y Sun, and Y-F Li. Multi-instance learning by treating instances as non i.i.d.
samples. In ICML, 2009.
[22] S-H Zhu, K. Yu, Y. Chi, and Y-H Gong. Combining content and link classiﬁcation using matrix
factorization. In SIGIR, 2007.

In

9


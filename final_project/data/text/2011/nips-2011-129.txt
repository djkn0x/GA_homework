Global Solution of Fully-Observed
Variational Bayesian Matrix Factorization
is Column-Wise Independent

Shinichi Nakajima
Nikon Corporation
Tokyo, 140-8601, Japan
nakajima.s@nikon.co.jp

Masashi Sugiyama
Tokyo Institute of Technology
Tokyo 152-8552, Japan
sugi@cs.titech.ac.jp

Derin Babacan
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
dbabacan@illinois.edu

Abstract

Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the
posterior distribution of factorized matrices by assuming matrix-wise indepen-
dence of the two factors. A recent study on fully-observed VBMF showed that,
under a stronger assumption that the two factorized matrices are column-wise in-
dependent, the global optimal solution can be analytically computed. However,
it was not clear how restrictive the column-wise independence assumption is. In
this paper, we prove that the global solution under matrix-wise independence is
actually column-wise independent, implying that the column-wise independence
assumption is harmless. A practical consequence of our theoretical ﬁnding is that
the global solution under matrix-wise independence (which is a standard setup)
can be obtained analytically in a computationally very efﬁcient way without any
iterative algorithms. We experimentally illustrate advantages of using our analytic
solution in probabilistic principal component analysis.

1 Introduction

The goal of matrix factorization (MF) is to approximate an observed matrix by a low-rank one. In
this paper, we consider fully-observed MF where the observed matrix has no missing entry1 . This
formulation includes classical multivariate analysis techniques based on singular-value decomposi-
tion such as principal component analysis (PCA) [9] and canonical correlation analysis [10].
In the framework of probabilistic MF [20, 17, 19], posterior distributions of factorized matrices are
considered. Since exact inference is computationally intractable, the Laplace approximation [3],
the Markov chain Monte Carlo sampling [3, 18], and the variational Bayesian (VB) approximation
[4, 13, 16, 15] were used for approximate inference in practice. Among them, the VB approximation
seems to be a popular choice due to its high accuracy and computational efﬁciency.
In the original VBMF [4, 13], factored matrices are assumed to be matrix-wise independent, and a
local optimal solution is computed by an iterative algorithm. A simpliﬁed variant of VBMF (sim-
pleVBMF) was also proposed [16], which assumes a stronger constraint that the factored matrices

1This excludes the collaborative ﬁltering setup, which is aimed at imputing missing entries of an observed
matrix [12, 7].

1

are column-wise independent. A notable advantage of simpleVBMF is that the global optimal solu-
tion can be computed analytically in a computationally very efﬁcient way [15].
Intuitively, it is suspected that simpleVBMF only possesses weaker approximation ability due to its
stronger column-wise independence assumption. However, it was reported that no clear performance
degradation was observed in experiments [14]. Thus, simpleVBMF would be a practically useful
approach. Nevertheless, the inﬂuence of the stronger column-wise independence assumption was
not elucidated beyond this empirical evaluation.
The main contribution of this paper is to theoretically show that the column-wise independence
assumption does not degrade the performance. More speciﬁcally, we prove that a global optimal
solution of the original VBMF is actually column-wise independent. Thus, a global optimal so-
lution of the original VBMF can be obtained by the analytic-form solution of simpleVBMF—no
computationally-expensive iterative algorithm is necessary. We show the usefulness of the analytic-
form solution through experiments on probabilistic PCA.

2 Formulation

In this section, we ﬁrst formulate the problem of probabilistic MF, and then introduce the VB ap-
proximation and its simpliﬁed variant.

¶

,

(1)

∥Y − BA

⊤∥2
(cid:181)
Fro

2.1 Probabilistic Matrix Factorization

(cid:181)
The probabilistic MF model is given as follows [19]:
− 1
p(Y |A, B ) ∝ exp
⊤ ¢¶
(cid:181)
⊤ ¢¶
¡
¡
2σ2
p(A) ∝ exp
− 1
p(B ) ∝ exp
− 1
−1
−1
(2)
tr
tr
,
BC
B B
,
A A
AC
2
2
where Y ∈ RL×M is an observed matrix, A ∈ RM ×H and B ∈ RL×H are parameter matrices to be
estimated, and σ2 is the noise variance. Here, we denote by ⊤ the transpose of a matrix or vector, by
∥ · ∥Fro the Frobenius norm, and by tr(·) the trace of a matrix. We assume that the prior covariance
matrices CA and CB are diagonal and positive deﬁnite, i.e.,
for cah , cbh > 0, h = 1, . . . , H.
CA = diag(c2
),
CB = diag(c2
)
b1 , . . . , c2
a1 , . . . , c2
aH
bH
Without loss of generality, we assume that the diagonal entries of the product CACB are arranged
≥ cah′ cbh′ for any pair h < h
′ .
in the non-increasing order, i.e., cah cbh
·⊤ ∈ RL×H .
‡eb1 , . . . , ebL
Throughout the paper, we denote a column vector of a matrix by a bold smaller letter, and a row
A = (a1 , . . . , aH ) = (ea1 , . . . , eaM )
vector by a bold smaller letter with a tilde, namely,
⊤ ∈ RM ×H , B = (b1 , . . . , bH ) =
2.2 Variational Bayesian Approximation

The Bayes posterior is written as
p(A, B |Y ) = p(Y |A, B )p(A)p(B )
(3)
,
Z (Y )
where Z (Y ) = 〈p(Y |A, B )〉p(A)p(B ) is the marginal likelihood. Here, 〈·〉p denotes the expecta-
tion over the distribution p. Since the Bayes posterior (3) is computationally intractable, the VB
approximation was proposed [4, 13, 16, 15].
(cid:192)
¿
(cid:192)
¿
Let r(A, B ), or r for short, be a trial distribution. The following functional with respect to r is called
the free energy:
F (r|Y ) =
log r(A, B )
p(A, B |Y )
=
log
r(A,B )

r(A, B )
p(Y |A, B )p(A)p(B )

− log Z (Y ).
r(A,B )

(4)

2

In the last equation, the ﬁrst term is the Kullback-Leibler (KL) distance from the trial distribution
to the Bayes posterior, and the second term is a constant. Therefore, minimizing the free energy (4)
amounts to ﬁnding the distribution closest to the Bayes posterior in the sense of the KL distance. In
the VB approximation, the free energy (4) is minimized over some restricted function space.
A standard constraint for the MF model is matrix-wise independence [4, 13], i.e.,
rVB (A, B ) = rVB
A (A)rVB
B (B ).
(5)
This constraint breaks off the entanglement between the parameter matrices A and B , and leads to
a computationally-tractable iterative algorithm. Using the variational method, we can show that,
MY
LY
ebbl , ΣB ),
NH (eam ; ebam , ΣA )
NH (ebl ;
under the constraint (5), the VB posterior minimizing the free energy (4) is written as
rVB (A, B ) =
·−1
‡ bB
‡eba1 , . . . , ebaM
·⊤
m=1
l=1
⊤ bB + LΣB + σ2C
⊤ bB
bA =
where the parameters satisfy
(cid:181)ebb1 , . . . ,
¶⊤
·−1
‡ bA
−1
ebbL
ΣA
bB =
⊤ bA + M ΣA + σ2C
= Y bA
= Y
,
σ2 ,
A
−1
ΣB
ΣB = σ2
(7)
σ2 ,
.
trix Σ . Iteratively updating the parameters bA, ΣA , bB , and ΣB by Eqs.(6) and (7) until convergence
B
Here, Nd (·; µ, Σ ) denotes the d-dimensional Gaussian distribution with mean µ and covariance ma-
gives a local minimum of the free energy (4).
·
‡
When the noise variance σ2 is unknown, it can also be estimated based on the free energy minimiza-
⊤ bB + LΣB )
⊤ bA + M ΣA )( bB
( bA
⊤ bB bA
tion. The update rule for σ2 is given by
∥Y ∥2
Fro − tr(2Y
⊤ ) + tr
LM

σ2 =
.
= ∥bbh∥2 /L + (ΣB )hh .
Furthermore, in the empirical Bayesian scenario, the hyperparameters CA and CB are also estimated
= ∥bah∥2 /M + (ΣA )hh ,
from data. In this scenario, CA and CB are updated in each iteration by the following formulas:
c2
c2
ah
bh
2.3 SimpleVB Approximation

ΣA = σ2

(9)

(6)

(8)

(bh ).

(10)

r simpleVB
ah

(ah )

r simpleVB
bh

r simpleVB (A, B ) =

A simpliﬁed variant, called the simpleVB approximation, assumes column-wise independence of
HY
HY
each matrix [16, 15], i.e.,
h=1
h=1
This constraint restricts the covariances ΣA and ΣB to be diagonal, and thus necessary memory stor-
HY
age and computational cost are substantially reduced [16]. The simpleVB posterior can be written
IM )NL (bh ; bbh , σ2
NM (ah ; bah , σ2
as
r simpleVB (A, B ) =
ah
bh
0@Y −
1A⊤ bbh ,
h=1
‡
X
where the parameters satisfy
∥bbh∥2 + Lσ2
bbh′ ba
bah =
0@Y −
1A bah ,
⊤
h′
X
¡∥bah∥2 + M σ2
bh
h′ ̸=h
bbh =
bbh′ ba
⊤
h′
ah
h′ ̸=h

·−1
,
¢−1

−2
+ σ2 c
bh

−2
+ σ2 c
ah

σ2
bh

= σ2

σ2
ah

= σ2

σ2
ah
σ2

σ2
bh
σ2

.

(12)

IL ),

(11)

3

Here, Id denotes the d-dimensional identity matrix. Iterating Eqs.(11) and (12) until convergence,
we can obtain a local minimum of the free energy. Eqs.(8) and (9) are similarly applied if the noise
variance σ2 is unknown and in the empirical Bayesian scenario, respectively.
A recent study has derived the analytic solution for simpleVB when the observed matrix has no
missing entry [15]. This work made simpleVB more attractive, because it did not only provide sub-
stantial reduction of computation costs, but also guaranteed the global optimality of the solution.
However, it was not clear how restrictive the column-wise independence assumption is, beyond its
experimental success [14]. In the next section, we theoretically show that the column-wise indepen-
dence assumption is actually harmless.

3 Analytic Solution of VBMF under Matrix-wise Independence

Under the matrix-wise independence constraint (5), the free energy (4) can be written as
F = 〈log r(A) + log r(B ) − log p(Y |A, B )p(A)p(B )〉
r(A)r(B )
|CA |
|CB |
∥Y ∥2
‡ bA
·
‡ bB
·
n
|ΣA | + L
log σ2 + M
= LM
⊤ bA + M ΣA
⊤ bB + LΣB
|ΣB | +
log
log
2σ2 + const.
2
2
2
··o
· ‡ bB
‡
‡ bA
−2 bA
⊤ bB +
⊤ bA + M ΣA
⊤ bB + LΣB
1
−1
−1
+ C
+
tr
C
2
A
B
Note that Eqs.(6) and (7) together form the stationarity condition of Eq.(13) with respect to bA, bB ,
−2
⊤
+σ
(13)
.
Y
ΣA , and ΣB .
Below, we show that a global solution of ΣA and ΣB is diagonal. When the product CACB is non-
′ ), the global solution is unique and diagonal.
degenerate (i.e., cah cbh > cah′ cbh′ for any pair h < h
On the other hand, when CACB is degenerate, the global solutions are not unique because arbitrary
rotation in the degenerate subspace is possible without changing the free energy. However, still one
of the equivalent solutions is always diagonal.

Theorem 1 Diagonal ΣA and ΣB minimize the free energy (13).

The basic idea of our proof is that, since minimizing the free energy (13) with respect to A, B , ΣA ,
and ΣB is too complicated, we focus on a restricted space written in a particular form that includes
the optimal solution. From necessary conditions for optimality, we can deduce that the solutions ΣA
and ΣB are diagonal.
Below, we describe the outline of the proof for non-degenerate CACB . The complete proof for
general cases is omitted because of the page limit.
∗
∗
∗
∗
bA = A
(Sketch of proof of Theorem 1) Assume that (A
B ) is a minimizer of the free energy
A , Σ
, B
, Σ
(13), and consider the following set of parameters speciﬁed by an H × H orthogonal matrix Ω :
bB = B
−1/2
−1/2
−1/2
∗
∗
⊤
⊤
C 1/2
ΣA = C 1/2
C 1/2
A Ω
A Ω
C
AC
A Σ
A ,
A ΩC
A ,
⊤ is invariant with respect to Ω , and ( bA, bB , ΣA , ΣB ) = (A
Note that bB bA
−1/2
−1/2
−1/2
⊤
∗
⊤
∗
B C 1/2
A ΩC 1/2
C 1/2
ΣB = C
.
C
A Ω
A Σ
,
C
A Ω
A
A
n
o
∗
∗
∗
¡
¢
B ) holds if
A , Σ
, Σ
, B
Ω = IH . Then, as a function of Ω , the free energy (13) can be simpliﬁed as
1
−1
−1
∗ + LΣ
∗⊤
∗
⊤
B ΩC 1/2
C 1/2
F (Ω ) =
+ const.
tr
C
A C
B
B
A Ω
2
B
A
∗
∗
∗
∗
This is necessarily minimized at Ω = IH , because we assumed that (A
B ) is a mini-
, Σ
A , Σ
, B
∗ + LΣ
∗⊤
∗
mizer. We can show that F (Ω ) is minimized at Ω = IH only if B
B is diagonal. This
B
∗
implies that Σ
A (see Eq.(6)) should be diagonal.
bA = A
Similarly, we consider another set of parameters speciﬁed by an H × H orthogonal matrix Ω
′ :
bB = B
−1/2
−1/2
−1/2
∗
′⊤
′
∗
′⊤
C 1/2
C 1/2
AC 1/2
ΣA = C
C
B Ω
,
B Ω
B Σ
C
B Ω
,
B
B
−1/2
−1/2
−1/2
∗
′⊤
′
∗
′⊤
C 1/2
ΣB = C 1/2
C 1/2
C
B Ω
B ,
C
B Ω
B Σ
B C
B Ω
B .

∗

4

F (Ω

o

′⊤

n
¡
¢
′ , the free energy (13) can be expressed as
Then, as a function of Ω
1
−1
−1
∗ + M Σ
′ ) =
′
∗⊤
∗
C 1/2
C 1/2
+ const.
tr
A
A C
C
B Ω
A
B Ω
2
A
B
∗
′ = IH only if A
∗⊤
∗ + M Σ
∗
Similarly, this is minimized at Ω
A is diagonal. Thus, Σ
B should be
A
⁄
diagonal (see Eq.(7)).
The result that ΣA and ΣB become diagonal would be natural because we assumed the independent
Gaussian prior on A and B : the fact that any Y can be decomposed into orthogonal components may
imply that the observation Y cannot convey any preference for singular-component-wise correlation.
Note, however, that Theorem 1 does not necessarily hold when the observed matrix has missing
entries.
Theorem 1 implies that the stronger column-wise independence constraint (10) does not degrade
approximation accuracy, and the VB solution under matrix-wise independence (5) essentially agrees
with the simpleVB solution. Consequently, we can obtain a global analytic solution for VB, by
combining Theorem 1 above with Theorem 1 in [15]:
Corollary 1 Let γh (≥ 0) be the h-th largest singular value of Y , and let ωah and ω bh be the
LX
associated right and left singular vectors:
⊤
Let bγh be the second largest real solution of the following quartic equation with respect to t:
Y =
.
γhω bh ω
ah
h=1
!
ˆ
p
fh (t) := t4 + ξ3 t3 + ξ2 t2 + ξ1 t + ξ0 = 0,
where the coefﬁcients are deﬁned by
(L − M )2γh
ˆ
!2
ξ2 = −
2σ4
(L2 + M 2 )η2
¶
¶ (cid:181)
(cid:181)
ξ3γh +
h
ξ0 ,
,
c2
c2
LM
LM
ah
bh
− σ4
1 − σ2L
vuuut (L + M )σ2
ξ0 =
η2
,
vuutˆ
h
γ 2
c2
!2 − LM σ4 .
c2
ah
h
bh
eγh =
+ σ4
+ σ4
+
2
2c2
2c2
c2
c2
ah
ah
‰bγh
bh
bh
HX
if γh > eγh ,
bU VB ≡ 〈BA
⊤ 〉rVB (A,B ) = bB bA
, where bγVB
bγVB
Then, the global VB solution under matrix-wise independence (5) can be expressed as
⊤
⊤ =
h =
h ω bh ω
0
ah
otherwise.
h=1
Theorem 1 holds also in the empirical Bayesian scenario, where the hyperparameters (CA , CB )
are also estimated from observation. Accordingly, the empirical VB solution also agrees with the
empirical simpleVB solution, whose analytic-form is given in Corollary 5 in [15]. Thus, we obtain
(
the global analytic solution for empirical VB:
HX
bU EVB =
, where bγ EVB
bγ EVB
Corollary 2 The global empirical VB solution under matrix-wise independence (5) is given by
and ∆h ≤ 0,
˘γVB
if γh > γ
⊤
h
h ω bh ω
h
0
ah
h
otherwise.
h=1
(cid:181)
¶
q
√
√
L +
·
‡
‡
¡−2γh ˘γVB
− (L + M )σ2 )2 − 4LM σ4
1
(γ 2
,
2LM
h
1
γh
h + LM ˘c2
h + 1
Lσ2 ˘γVB
h
σ2

M )σ,
·
− (L + M )σ2 +

(L + M )σ2
2

γh
M σ2 ˘γVB
h + 1
+ L log
∆h = M log
is the VB solution for cah cbh = ˘ch .
and ˘γVB
h

1 − σ2M
γ 2
h

Here,
γ

= (

h
˘c2
h =

+

,

ξ1 = ξ3

h =
η2

(16)

(17)

,

(18)

¢

(14)

(15)

ξ3 =

Let

γ 2
h .

γ 2
h

=

+

5

by using Eq.(17) and Corollary 1. Otherwise, bγ EVB
When we calculate the empirical VB solution, we ﬁrst check if γh > γ
holds. If it holds, we
h
compute ˘γVB
= 0. Finally, we check if
∆h ≤ 0 holds by using Eq.(18).
h
h
When the noise variance σ2 is unknown, it is optimized by a naive 1-dimensional search to minimize
the free energy [15]. To evaluate the free energy (13), we need the covariances ΣA and ΣB , which
neither Corollary 1 nor Corollary 2 provides. The following corollary, which gives the complete
information on the VB posterior, is obtained by combining Theorem 1 above with Corollary 2 in
[15]:
HY
HY
NL (bh ; bbh , σ2
Corollary 3 The VB posteriors under matrix-wise independence (5) are given by
NM (ah ; bah , σ2
A (A) =
B (B ) =
IM ),
rVB
rVB
where, for bγVB
ah
bh
qbγVB
qbγVB
h=1
h=1
bδ
bδh · ωah ,
bbh = ±
bah = ±
being the solution given by Corollary 1,
p
− ¡bη2
¢
h
− σ2 (M − L))2 + 4M σ2 bη2
(bη2
· ω bh ,
−1
bδ
2M (bγVB
h
h
h
− σ2 (M − L)
p
− ¡bη2
¢
+
h + σ2 (M − L))2 + 4Lσ2 bη2
(bη2
=
h
h
h
σ2
−1
−2
ah
h + σ2 c
ah )
bδh + σ2 c
2L(bγVB
h
h + σ2 (M − L)
q
+
h
(M − L)(γh − bγVB
(M − L)2 (γh − bγVB
−2
)
bh
h
h )2 + 4σ4LM
(
h ) +
c2
c2
if γh > eγh ,
ah
bh
−2
2σ2M c
ah
η2
h
σ4
otherwise.
c2
c2
ah
bh

=
σ2
bh
bδh =
bη2
h =

IL ),

,

,

,

Note that the ratio cah /cbh is arbitrary in empirical VB, so we can ﬁx it to, e.g., cah /cbh = 1 without
loss of generality [15].

4 Experimental Results

In this section, we ﬁrst introduce probabilistic PCA as a probabilistic MF model. Then, we show
experimental results on artiﬁcial and benchmark datasets, which illustrate practical advantages of
using our analytic solution.
In probabilistic PCA [20], the observation y ∈ RL is assumed to be driven by a latent vector ea ∈ RH
4.1 Probabilistic PCA
y = B ea + ε.
in the following form:
Here, B ∈ RL×H speciﬁes the linear relationship between ea and y , and ε ∈ RL is a Gaussian noise
from the latent vectors {ea1 , . . . , eaM }, and each latent vector is subject to ea ∼ NH (0, IH ). Then,
subject to NL (0, σ2 IL ). Suppose that we are given M observed samples {y1 , . . . , yM
} generated
the probabilistic PCA model is written as Eqs.(1) and (2) with CA = IH .
If we apply Bayesian inference, the intrinsic dimension H is automatically selected without prede-
termination [4, 14]. This useful property is called automatic dimensionality selection (ADS).

4.2 Experiment on Artiﬁcial Data

We compare the iterative algorithm and the analytic solution in the empirical VB scenario with
unknown noise variance, i.e., the hyperparameters (CA , CB ) and the noise variance σ2 are also

6

(a) Free energy

(b) Computation time

(c) Estimated rank

Figure 1: Experimental results for Artiﬁcial1 dataset, where the data dimension is L = 100, the
∗ = 20.
number of samples is M = 300, and the true rank is H

(a) Free energy

(b) Computation time

Figure 2: Experimental results for Artiﬁcial2 dataset (L = 70, M = 300, and H

(c) Estimated rank
∗ = 40).

estimated from observation. We use the full-rank model (i.e., H = min(L, M )), and expect the
∗ .
ADS effect to automatically ﬁnd the true rank H
Figure 1 shows the free energy, the computation time, and the estimated rank over iterations for an
∗ = 20. We randomly created true
artiﬁcial (Artiﬁcial1) dataset with L = 100, M = 300, and H
∗ follows N1 (0, 1). An
∗ ∈ RM ×H
∗ ∈ RL×H
∗ and B
∗
∗
matrices A
and B
so that each entry of A
observed matrix Y was created by adding a noise subject to N1 (0, 1) to each entry of B
∗
∗⊤ .
way: bA and bB are randomly created so that each entry follows N1 (0, 1). Other variables are set to
A
The iterative algorithm consists of the update rules (6)–(9). Initial values were set in the following
ΣA = ΣB = CA = CB = IH and σ2 = 1. Note that we rescale Y so that ∥Y ∥2
Fro/(LM ) = 1,
before starting iteration. We ran the iterative algorithm 10 times, starting from different initial
points, and each trial is plotted by a solid line in Figure 1. The analytic solution consists of applying
rank bH = H
Corollary 2 combined with a naive 1-dimensional search for noise variance σ2 estimation [15]. The
analytic solution is plotted by the dashed line. We see that the analytic solution estimates the true
∗ = 20 immediately (∼ 0.1 sec on average over 10 trials), while the iterative algorithm
does not converge in 60 sec.
Figure 2 shows experimental results on another artiﬁcial dataset (Artiﬁcial2) where L = 70, M =
∗ = 40. In this case, all the 10 trials of the iterative algorithm are trapped at local
300, and H
minima. We empirically observed a tendency that the iterative algorithm suffers from the local
∗ is large (close to H ).
minima problem when H

4.3 Experiment on Benchmark Data

Figures 3 and 4 show experimental results on the Satellite and the Spectf datasets available from the
UCI repository [1], showing similar tendencies to Figures 1 and 2. We also conducted experiments
on various benchmark datasets, and found that the iterative algorithm typically converges slowly,
and sometimes suffers from the local minima problem, while our analytic-form gives the global
solution immediately.

7

0501001502002501.822.22.42.62.8IterationF/(LM)  AnalyticIterative050100150200250020406080IterationTime(sec)  AnalyticIterative050100150200250020406080100IterationbH  AnalyticIterative0501001502002502.62.833.2IterationF/(LM)  AnalyticIterative050100150200250051015202530IterationTime(sec)  AnalyticIterative050100150200250020406080IterationbH  AnalyticIterative(a) Free energy

(b) Computation time

(c) Estimated rank

Figure 3: Experimental results for the Sat dataset (L = 36, M = 6435).

(a) Free energy

(b) Computation time

(c) Estimated rank

Figure 4: Experimental results for the Spectf dataset (L = 44, M = 267).

5 Conclusion and Discussion

In this paper, we have analyzed the fully-observed variational Bayesian matrix factorization (VBMF)
under matrix-wise independence. We have shown that the VB solution under matrix-wise indepen-
dence essentially agrees with the simpliﬁed VB (simpleVB) solution under column-wise indepen-
dence. As a consequence, we can obtain the global VB solution under matrix-wise independence
analytically in a computationally very efﬁcient way.
Our analysis assumed uncorrelated priors. With correlated priors, the posterior is no longer uncor-
related and thus it is not straightforward to obtain a global solution analytically. Nevertheless, there
exists a situation where an analytic solution can be easily obtained: Suppose there exists an H × H
′
⊤ and C
′
−1 )⊤
−1 are diagonal.
A = T CAT
B = (T
non-singular matrix T such that both of C
CB T
We can show that the free energy (13) is invariant under the following transformation for any T :
CA → T CAT
ΣA → T ΣAT
A → AT
⊤
⊤
⊤
,
,
B → BT
ΣB → (T
CB → (T
−1 .
−1 ,
−1 )T ΣB T
−1 ,
−1 )⊤
Accordingly, the following procedure gives the global solution analytically: the analytic solution
′
′
given the diagonal (C
B ) is ﬁrst computed, and the above transformation is then applied.
A , C
We have demonstrated the usefulness of our analytic solution in probabilistic PCA. On the other
hand, robust PCA has gathered a great deal of attention recently [5], and its Bayesian variant has
been proposed [2]. We expect that our analysis can handle more structured sparsity, in addition to
the current low-rank inducing sparsity. Extension of the current work along this line will allow us to
give more theoretical insights into robust PCA and provide computationally efﬁcient algorithms.
Finally, a more challenging direction is to handle priors correlated over rows of A and B . This
allows us to model correlations in the observation space, and capture, e.g., short-term correlation
in time-series data and neighboring pixels correlation in image data. Analyzing such a situation, as
well as missing value imputation and tensor factorization [11, 6, 8, 21] is our important future work.

CB T

,

Acknowledgments

The authors thank anonymous reviewers for helpful comments. Masashi Sugiyama was supported
by the FIRST program. Derin Babacan was supported by a Beckman Postdoctoral Fellowship.

8

0501001502002502.533.544.55IterationF/(LM)  AnalyticIterative0501001502002500100200300400500IterationTime(sec)  AnalyticIterative05010015020025001020304050IterationbH  AnalyticIterative05010015020025033.544.55IterationF/(LM)  AnalyticIterative0501001502002500510152025IterationTime(sec)  AnalyticIterative050100150200250010203040IterationbH  AnalyticIterativeReferences
[1] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[2] D. Babacan, M. Luessi, R. Molina, and A. Katsaggelos. Sparse Bayesian methods for low-rank matrix
estimation. arXiv:1102.5288v1 [stat.ML], 2011.
[3] C. M. Bishop. Bayesian principal components. In Advances in NIPS, volume 11, pages 382–388, 1999.
[4] C. M. Bishop. Variational principal components. In Proc. of ICANN, volume 1, pages 514–509, 1999.
[5] E.-J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? CoRR, abs/0912.3599,
2009.
[6] J. D. Carroll and J. J. Chang. Analysis of individual differences in multidimensional scaling via an n-way
generalization of ’eckart-young’ decomposition. Psychometrika, 35:283–319, 1970.
[7] S. Funk. Try this at home. http://sifter.org/˜simon/journal/20061211.html, 2006.
[8] R. A. Harshman. Foundations of the parafac procedure: Models and conditions for an ”explanatory”
multimodal factor analysis. UCLA Working Papers in Phonetics, 16:1–84, 1970.
[9] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of Educa-
tional Psychology, 24:417–441, 1933.
[10] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3–4):321–377, 1936.
[11] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455–500,
2009.
[12] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl. Grouplens: Applying
collaborative ﬁltering to Usenet news. Communications of the ACM, 40(3):77–87, 1997.
[13] Y. J. Lim and T. W. Teh. Variational Bayesian approach to movie rating prediction. In Proceedings of
KDD Cup and Workshop, 2007.
[14] S. Nakajima, M. Sugiyama, and D. Babacan. On Bayesian PCA: Automatic dimensionality selection and
analytic solution. In Proceedings of 28th International Conference on Machine Learning (ICML2011),
Bellevue, WA, USA, Jun. 28–Jul.2 2011.
[15] S. Nakajima, M. Sugiyama, and R. Tomioka. Global analytic solution for variational Bayesian matrix fac-
torization. In J. Lafferty, C. K. I. Williams, R. Zemel, J. Shawe-Taylor, and A. Culotta, editors, Advances
in Neural Information Processing Systems 23, pages 1759–1767, 2010.
[16] T. Raiko, A. Ilin, and J. Karhunen. Principal component analysis for large scale problems with lots of
missing values. In J. Kok, J. Koronacki, R. Lopez de Mantras, S. Matwin, D. Mladenic, and A. Skowron,
editors, Proceedings of the 18th European Conference on Machine Learning, volume 4701 of Lecture
Notes in Computer Science, pages 691–698, Berlin, 2007. Springer-Verlag.
[17] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation,
11:305–345, 1999.
[18] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte
Carlo. In International Conference on Machine Learning, 2008.
[19] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In J. C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1257–1264, Cam-
bridge, MA, 2008. MIT Press.
[20] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society, 61:611–622, 1999.
[21] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279–311, 1996.

9


Iterative Learning for Reliable Crowdsourcing
Systems

David R. Karger
Devavrat Shah
Sewoong Oh
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology

Abstract

Crowdsourcing systems, in which tasks are electronically distributed to numerous
“information piece-workers”, have emerged as an effective paradigm for human-
powered solving of large scale problems in domains such as image classiﬁcation,
data entry, optical character recognition, recommendation, and proofreading. Be-
cause these low-paid workers can be unreliable, nearly all crowdsourcers must
devise schemes to increase conﬁdence in their answers, typically by assigning
each task multiple times and combining the answers in some way such as ma-
jority voting. In this paper, we consider a general model of such crowdsourcing
tasks, and pose the problem of minimizing the total price (i.e., number of task as-
signments) that must be paid to achieve a target overall reliability. We give a new
algorithm for deciding which tasks to assign to which workers and for inferring
correct answers from the workers’ answers. We show that our algorithm signiﬁ-
cantly outperforms majority voting and, in fact, is asymptotically optimal through
comparison to an oracle that knows the reliability of every worker.

1

Introduction

Background. Crowdsourcing systems have emerged as an effective paradigm for human-powered
problem solving and are now in widespread use for large-scale data-processing tasks such as image
classiﬁcation, video annotation, form data entry, optical character recognition, translation, recom-
mendation, and proofreading. Crowdsourcing systems such as Amazon Mechanical Turk provide a
market where a “taskmaster” can submit batches of small tasks to be completed for a small fee by
any worker choosing to pick them up. For example, a worker may be able to earn a few cents by indi-
cating which images from a set of 30 are suitable for children (one of the beneﬁts of crowdsourcing
is its applicability to such highly subjective questions).
Since typical crowdsourced tasks are tedious and the reward is small, errors are common even among
workers who make an effort. At the extreme, some workers are “spammers”, submitting arbitrary
answers independent of the question in order to collect their fee. Thus, all crowdsourcers need
strategies to ensure the reliability of answers. Because the worker crowd is large, anonymous, and
transient, it is generally difﬁcult to build up a trust relationship with particular workers.1 It is also
difﬁcult to condition payment on correct answers, as the correct answer may never truly be known
and delaying payment can annoy workers and make it harder to recruit them for your future tasks.
Instead, most crowdsourcers resort to redundancy, giving each task to multiple workers, paying
them all irrespective of their answers, and aggregating the results by some method such as majority
voting. For such systems there is a natural core optimization problem to be solved. Assuming the
e-mail: {karger,swoh,devavrat}@mit.edu. This work was supported in parts by AFOSR com-
plex networks project, MURI on network tomography, and National Science Foundation.
1For certain high-value tasks, crowdsourcers can use entrance exams to “prequalify” workers and block
spammers, but this increases the cost and still provides no guarantee that the prequaliﬁed workers will try hard.

1

taskmaster wishes to achieve a certain reliability in their answers, how can they do so at minimum
cost (which is equivalent to asking how they can do so while asking the fewest possible questions)?
Several characteristics of crowdsourcing systems make this problem interesting. Workers are neither
persistent nor identiﬁable; each batch of tasks will be solved by a worker who may be completely
new and who you may never see again. Thus one cannot identify and reuse particularly reliable
workers. Nonetheless, by comparing one worker’s answer to others’ on the same question, it is
possible to draw conclusions about a worker’s reliability, which can be used to weight their answers
to other questions in their batch. However, batches must be of manageable size, obeying limits on
the number of tasks that can be given to a single worker. Another interesting aspect of this problem
is the choice of task assignments. Unlike many inference problems which makes inferences based
on a ﬁxed set of signals, our algorithm can choose which signals to measure by deciding which
questions to ask which workers.
In the following, we ﬁrst deﬁne a formal model that captures these aspects of the problem. We will
then describe a scheme for deciding which tasks to assign to which workers and introduce a novel
iterative algorithm to infer the correct answers from the workers’ responses.
Setup. We model a set of m tasks {ti }i∈[m] as each being associated with an unobserved ‘correct’
answer si ∈ {±1}. Here and after, we use [N ] to denote the set of ﬁrst N integers. In the earlier im-
age categorization example, each task corresponds to labeling an image as suitable for children (+1)
or not (−1). We assign these tasks to n workers from the crowd, which we denote by {wj }j∈[n] .
When a task is assigned to a worker, we get a possibly inaccurate answer from the worker. We
use Aij ∈ {±1} to denote the answer if task ti is assigned to worker wj . Some workers are more
diligent or have more expertise than others, while some other workers might be spammers. We
choose a simple model to capture this diversity in workers’ reliability: we assume that each worker
wj is characterized by a reliability pj ∈ [0, 1], and that they make errors randomly on each question
(cid:26) si with probability pj ,
they answer. Precisely, if task ti is assigned to worker wj then
−si with probability 1 − pj ,
and Aij = 0 if ti is not assigned to wj . The random variable Aij is independent of any other
event given pj . (Throughout this paper, we use boldface characters to denote random variables and
random matrices unless it is clear from the context.) The underlying assumption here is that the
error probability of a worker does not depend on the particular task and all the tasks share an equal
level of difﬁculty. Hence, each worker’s performance is consistent across different tasks.
We further assume that the reliability of workers {pj }j∈[n] are independent and identically dis-
tributed random variables with a given distribution on [0, 1]. One example is spammer-hammer
model where, each worker is either a ‘hammer’ with probability q or is a ‘spammer’ with probability
1 − q . A hammer answers all questions correctly, in which case pj = 1, and a spammer gives
random answers, in which case pj = 1/2. Given this random variable pj , we deﬁne an important
parameter q ∈ [0, 1], which captures the ‘average quality’ of the crowd: q ≡ E[(2pj − 1)2 ]. A
value of q close to one indicates that a large proportion of the workers are diligent, whereas q close
to zero indicates that there are many spammers in the crowd. The deﬁnition of q is consistent with
use of q in the spammer-hammer model. We will see later that our bound on the error rate of our
inference algorithm holds for any distribution of pj but depends on the distribution only through
this parameter q . It is quite realistic to assume the existence of a prior distribution for pj . The model
is therefore quite general: in particular, it is met if we simply randomize the order in which we up-
load our task batches, since this will have the effect of randomizing which workers perform which
batches, yielding a distribution that meets our requirements. On the other hand, it is not realistic
to assume that we know what the prior is. To execute our inference algorithm for a given number
of iterations, we do not require any knowledge of the distribution of the reliability. However, q is
necessary in order to determine how many times a task should be replicated and how many iterations
we need to run to achieve certain reliability.
Under this crowdsourcing model, a taskmaster ﬁrst decides which tasks should be assigned to which
workers, and then estimates the correct solutions {si }i∈[m] once all the answers {Aij } are submit-
ted. We assume a one-shot scenario in which all questions are asked simultaneously and then an
estimation is performed after all the answers are obtained. In particular, we do not allow allocating

Aij =

2

tasks adaptively based on the answers received thus far. Then, assigning tasks to nodes amounts to
designing a bipartite graph G({ti }i∈[m] ∪ {wj }j∈[n] , E ) with m task and n worker nodes. Each
edge (i, j ) ∈ E indicates that task ti was assigned to worker wj .
Prior Work. A naive approach to identify the correct answer from multiple workers’ responses is to
use majority voting. Majority voting simply chooses what the majority of workers agree on. When
there are many spammers, majority voting is error-prone since it weights all the workers equally.
We will show that majority voting is provably sub-optimal and can be signiﬁcantly improved upon.
To infer the answers of the tasks and also the reliability of workers, Dawid and Skene [1, 2] proposed
an algorithm based on expectation maximization (EM) [3]. This approach has also been applied in
classiﬁcation problems where the training data is annotated by low-cost noisy ‘labelers’ [4, 5]. In
[6] and [7], this EM approach has been applied to more complicated probabilistic models for image
labeling tasks. However, the performance of these approaches are only empirically evaluated, and
there is no analysis that proves performance guarantees. In particular, EM algorithms require an
initial starting point which is typically randomly guessed. The algorithm is highly sensitive to this
initialization, making it difﬁcult to predict the quality of the resulting estimate. The advantage of
using low-cost noisy ‘labelers’ has been studied in the context of supervised learning, where a set
of labels on a training set is used to ﬁnd a good classiﬁer. Given a ﬁxed budget, there is a trade-
off between acquiring a larger training dataset or acquiring a smaller dataset but with more labels
per data point. Through extensive experiments, Sheng, Provost and Ipeirotis [8] show that getting
repeated labeling can give considerable advantage.

Contributions. In this work, we provide a rigorous treatment of designing a crowdsourcing sys-
tem with the aim of minimizing the budget to achieve completion of a set of tasks with a certain
reliability. We provide both an asymptotically optimal graph construction (random regular bipartite
graph) and an asymptotically optimal algorithm for inference (iterative algorithm) on that graph.
As the main result, we show that our algorithm performs as good as the best possible algorithm.
The surprise lies in the fact that the optimality of our algorithm is established by comparing it with
the best algorithm, one that is free to choose any graph, regular or irregular, and performs optimal
estimation based on the information provided by an oracle about reliability of the workers. Previous
approaches focus on developing inference algorithms assuming that a graph is already given. None
of the prior work on crowdsourcing provides any systematic treatment of the graph construction.
To the best of our knowledge, we are the ﬁrst to study both aspects of crowdsourcing together and,
more importantly, establish optimality.
Another novel contribution of our work is the analysis technique. The iterative algorithm we in-
troduce operates on real-valued messages whose distribution is a priori difﬁcult to analyze. To
overcome this challenge, we develop a novel technique of establishing that these messages are sub-
Gaussian (see Section 3 for a deﬁnition) using recursion, and compute the parameters in a closed
form. This allows us to prove the sharp result on the error rate, and this technique could be of
independent interest for analyzing a more general class of algorithms.

2 Main result

Under the crowdsourcing model introduced, we want to design algorithms to assign tasks and es-
timate the answers. In what follows, we explain how to assign tasks using a random regular graph
and introduce a novel iterative algorithm to infer the correct answers. We state the performance
Task allocation. Assigning tasks amounts to designing a bipartite graph G(cid:0){ti }i∈[m] ∪
guarantees for our algorithm and provide comparisons to majority voting and an oracle estimator.
{wj }j∈[n] , E (cid:1), where each edge corresponds to a task-worker assignment. The taskmaster makes a
choice of how many workers to assign to each task (the left degree l) and how many tasks to assign
to each worker (the right degree r). Since the total number of edges has to be consistent, the number
of workers n directly follows from ml = nr . To generate an (l, r)-regular bipartite graph we use
a random graph generation scheme known as the conﬁguration model in random graph literature
[9, 10]. In principle, one could use arbitrary bipartite graph G for task allocation. However, as we
show later in this paper, random regular graphs are sufﬁcient to achieve order-optimal performance.

3

Inference algorithm. We introduce a novel iterative algorithm which operates on real-valued task
messages {xi→j }(i,j )∈E and worker messages {yj→i }(i,j )∈E . The worker messages are initialized
as independent Gaussian random variables. At each iteration, the messages are updated according
of the answers weighted by each worker’s reliability: ˆsi = sign((cid:80)
to the described update rule, where ∂ i is the neighborhood of ti . Intuitively, a worker message yj→i
represents our belief on how ‘reliable’ the worker j is, such that our ﬁnal estimate is a weighted sum
j∈∂ i Aij yj→i ).
Output: Estimation ˆs(cid:0){Aij }(cid:1)
Iterative Algorithm
Input: E , {Aij }(i,j )∈E , kmax
1: For all (i, j ) ∈ E do
j→i with random Zij ∼ N (1, 1) ;
i→j ← (cid:80)
Initialize y (0)
2: For k = 1, . . . , kmax do
j→i ← (cid:80)
j (cid:48)∈∂ i\j Aij (cid:48) y (k−1)
For all (i, j ) ∈ E do x(k)
3: For all i ∈ [m] do xi ← (cid:80)
j (cid:48)→i
For all (i, j ) ∈ E do
4: Output estimate vector ˆs(cid:0){Aij }(cid:1) = [sign(xi )] .
i(cid:48)∈∂ j\i Ai(cid:48) j x(k)
y (k)
i(cid:48)→j ;
j∈∂ i Aij y (kmax−1)
;
j→i
While our algorithm is inspired by the standard Belief Propagation (BP) algorithm for approximat-
ing max-marginals [11, 12], our algorithm is original and overcomes a few critical limitations of the
standard BP. First, the iterative algorithm does not require any knowledge of the prior distribution of
pj , whereas the standard BP requires the knowledge of the distribution. Second, there is no efﬁcient
way to implement standard BP, since we need to pass sufﬁceint statistics (or messages) which under
our general model are distributions over the reals. On the otherhand, the iterative algorithm only
passes messages that are real numbers regardless of the prior distribution of pj , which is easy to im-
plement. Third, the iterative algorithm is provably asymptotically order-optimal. Density evolution,
explained in detail in Section 3, is a standard technique to analyze the performance of BP. Although
we can write down the density evolution for the standard BP, we cannot analyze the densities, ana-
lytically or numerically. It is also very simple to write down the density evolution equations for the
iterative algorithm, but it is not trivial to analyze the densities in this case either. We develop a novel
technique to analyze the densities and prove optimality of our algorithm.

;

2.1 Performance guarantee

We state the main analytical result of this paper: for random (l, r)-regular bipartite graph based task
assignments with our iterative inference algorithm, the probability of error decays exponentially in
lq , up to a universal constant and for a broad range of the parameters l, r and q . With a reasonable
choice of l = r and both scaling like (1/q) log(1/), the proposed algorithm is guarantted to achieve
error less than  for any  ∈ (0, 1/2). Further, an algorithm independent lower bound that we
establish suggests that such an error dependence on lq is unavoidable. Hence, in terms of the task
allocation budget, our algorithm is order-optimal. The precise statements follow next. Let µ =
(cid:16)
(cid:17) 1 − (1/q2ˆl ˆr)k−1
E[2pj − 1] and recall q = E[(2pj − 1)2 ]. To lighten the notation, let ˆl ≡ l − 1 and ˆr ≡ r − 1. Deﬁne
k ≡
1
2q
ρ2
+
3 +
1 − (1/q2ˆl ˆr)
µ2 (q2ˆl ˆr)k−1
q ˆr
ρ2∞ = (cid:0)3 + (1/q ˆr)(cid:1)q2ˆl ˆr/(q2ˆl ˆr − 1) .
For q2ˆl ˆr > 1, let ρ2∞ ≡ limk→∞ ρ2
k such that
Then we can show the following bound on the probability of making an error.
Theorem 2.1. For ﬁxed l > 1 and r > 1, assume that m tasks are assigned to n = ml/r workers
according to a random (l, r)-regular graph drawn from the conﬁguration model. If the distribution
of the worker reliaiblity satisfy µ ≡ E[2pj − 1] > 0 and q2 > 1/(ˆl ˆr), then for any s ∈ {±1}m , the
(cid:1)(cid:17) ≤ e−lq/(2ρ2
P(cid:16)
m(cid:88)
(cid:0){Aij }(i,j )∈E
estimates from k iterations of the iterative algorithm achieve
si (cid:54)= ˆsi
k ) .
i=1

lim sup
m→∞

(1)

.

1
m

4

si (cid:54)= ˆsi

lim sup
m→∞

lim sup
k→∞

As we increase k , the above bound converges to a non-trivial limit.
P(cid:16)
(cid:1)(cid:17) ≤ e−lq/(2ρ2∞ ) .
m(cid:88)
(cid:0){Aij }(i,j )∈E
Corollary 2.2. Under the hypotheses of Theorem 2.1,
i=1
One implication of this corollary is that, under a mild assumption that r ≥ l, the probabiilty of
error is upper bounded by e−(1/8)(lq−1) . Even if we ﬁx the value of q = E[(2pj − 1)2 ], different
√
q ]. Surprisingly, the asymptotic
distributions of pj can have different values of µ in the range of [q ,
bound on the error rate does not depend on µ. Instead, as long as q is ﬁxed, µ only affects how fast
the algorithm converges (cf. Lemma 2.3).
Notice that the bound in (2) is only meaningful when it is less than a half, whence ˆl ˆrq2 > 1 and
lq > 6 log(2) > 4. While as a task master the case of ˆl ˆrq2 < 1 may not be of interest, for the purpose
of completeness we comment on the performance of our algorithm in this regime. Speciﬁcally, we
empirically observe that the error rate increases as the number of iterations k increases. Therefore,
it makes sense to use k = 1. In which case, the algorithm essentially boils down to the majority
P(cid:16)
(cid:1)(cid:17) ≤ e−lµ2 /4 .
m(cid:88)
(cid:0){Aij }(i,j )∈E
rule. We can prove the following error bound. The proof is omitted due to a space constraint.
i=1

lim sup
m→∞

1
m

si (cid:54)= ˆsi

1
m

(2)

(3)

2.2 Discussion

Here we make a few comments relating to the execution of the algorithm and the interpretation of
the main results. First, the iterative algorithm is efﬁcient with runtime comparable to the simple
majority voting which requires O(ml) operations.
Lemma 2.3. Under the hypotheses of Theorem 2.1, the total computational cost sufﬁcient to achieve
the bound in Corollary 2.2 up to any constant factor in the exponent is O(ml log(q/µ2 )/ log(q2ˆl ˆr)).
By deﬁnition, we have q ≤ µ ≤ √
√
√
q . The runtime is the worst when µ = q , which happens under
the spammer-hammer model, and it is the best when µ =
q which happens if pj = (1 +
q)/2
deterministically. There exists a (non-iterative) polynomial time algorithm with runtime independent
of q for computing the estimate which achieves (2), but in practice we expect that the number
of iterations needed is small enough that the iterative algorithm will outperform this non-iterative
algorithm. Detailed proof of Lemma 2.3 will be skipped here due to a space constraint.
Second, the assumption that µ > 0 is necessary. If there is no assumption on µ, then we cannot
distinguish if the responses came from tasks with {si }i∈[m] and workers with {pj }j∈[n] or tasks
with {−si }i∈[m] and workers with {1 − pj }j∈[n] . Statistically, both of them give the same output.
In the case when we know that µ < 0, we can use the same algorithm changing the sign of the ﬁnal
output and get the same performance guarantee.
Third, our algorithm does not require any information on the distribution of pj . Further, unlike
other EM based algorithms, the iterative algorithm is not sensitive to initialization and with random
initialization converges to a unique estimate with high probability. This follows from the fact that
the algorithm is essentially computing a leading eigenvector of a particular linear operator.

2.3 Relation to singular value decomposition

The leading singular vectors are often used to capture the important aspects of datasets in matrix
form. In our case, the leading left singular vector of A can be used to estimate the correct answers,
where A ∈ {0, ±1}m×n is the m × n adjacency matrix of the graph G weighted by the submitted
answers. We can compute it using power iteration: for u ∈ Rm and v ∈ Rn , starting with a
(cid:88)
(cid:88)
randomly initialized v , power iteration iteratively updates u and v according to
j∈∂ i
i∈∂ j

Aij vj , and for all j, vj =

for all i, ui =

Aij ui

.

5

It is known that normalized u converges exponentially to the leading left singular vector. This update
rule is very similar to that of our iterative algorithm. But there is one difference that is crucial in the
analysis: in our algorithm we follow the framework of the celebrated belief propagation algorithm
[11, 12] and exclude the incoming message from node j when computing an outgoing message
to j . This extrinsic nature of our algorithm and the locally tree-like structure of sparse random
graphs [9, 13] allow us to perform asymptotic analysis on the average error rate. In particular, if we
use the leading singular vector of A to estimate s, such that si = sign(ui ), then existing analysis
techniques from random matrix theory does not give the strong performance guarantee we have.
These techniques typically focus on understanding how the subspace spanned by the top singular
vector behaves. To get a sharp bound, we need to analyze how each entry of the leading singular
vector is distributed. We introduce the iterative algorithm in order to precisely characterize how
each of the decision variable xi is distributed. Since the iterative algorithm introduced in this paper
is quite similar to power iteration used to compute the leading singular vectors, this suggests that
our analysis may shed light on how to analyze the top singular vectors of a sparse random matrix.

2.4 Optimality of our algorithm

As a taskmaster, the natural core optimization problem of our concern is how to achieve a certain
reliability in our answers with minimum cost. Since we pay equal amount for all the task assign-
ments, the cost is proportional to the total number of edges of the graph G. Here we compute the
total budget sufﬁcient to achieve a target error rate using our algorithm and show that this is within a
constant factor from the necessary budget to achieve the given target error rate using any graph and
the best possible inference algorithm. The order-optimality is established with respect to all algo-
rithms that operate in one-shot, i.e. all task assignments are done simultaneously, then an estimation
is performed after all the answers are obtained. The proofs of the claims in this section are skipped
here due to space limitations.
P(si (cid:54)= ˆsi ). We will show that Ω(cid:0)(1/q) log(1/)(cid:1) assignments per task is neces-
(1/m) (cid:80)
Formally, consider a scenario where there are m tasks to complete and a target accuracy  ∈ (0, 1/2).
To measure accuracy, we use the average probability of error per task denoted by dm (s, ˆs) ≡
i∈[m]
sary and sufﬁcient to achieve the target error rate: dm (s, ˆs) ≤ . To establish this fundamental limit,
we use the following minimax bound on error rate. Consider the case where nature chooses a set of
correct answers s ∈ {±1}m and a distribution of the worker reliability pj ∼ f . The distribution f
is chosen from a set of all distributions on [0, 1] which satisfy Ef [(2pj − 1)2 ] = q . We use F (q) to
denote this set of distributions. Let G (m, l) denote the set of all bipartite graphs, including irregular
graphs, that have m task nodes and ml total number of edges. Then the minimax error rate achieved
by the best possible graph G ∈ G (m, l) using the best possible inference algorithm is at least
dm (s, ˆsG,ALGO ) ≥ (1/2)e−(lq+O(lq2 )) ,

sup
s,f ∈F (q)
where ˆsG,ALGO denotes the estimate we get using graph G for task allocation and algorithm ALGO
for inference. This minimax bound is established by computing the error rate of an oracle esitimator,
which makes an optimal decision based on the information provided by an oracle who knows how
reliable each worker is. Next, we show that the error rate of majority voting decays signiﬁcantly
slower: the leading term in the error exponent scales like −lq2 . Let ˆsMV be the estimate produced
by majority voting. Then, for q ∈ (0, 1), there exists a numerical constant C1 such that
dm (s, ˆsMV ) = e−(C1 lq2+O(lq4+1)) .
sup
inf
G∈G (m,l)
s,f ∈F (q)
The lower bound in (4) does not depend on how many tasks are assigned to each worker. However,
our main result depends on the value of r . We show that for a broad range of parameters l, r , and q
our algorithm achieves optimality. Let ˆsIter be the estimate given by random regular graphs and the
iterative algorithm. For ˆlq ≥ C2 , ˆrq ≥ C3 and C2C3 > 1, Corollary 2.2 gives
dm (s, ˆsIter ) ≤ e−C4 lq .
lim
m→∞ sup
s,f ∈F (q)
This is also illustrated in Figure 1. We ran numerical experiments with 1000 tasks and 1000 work-
ers from the spammer-hammer model assigned according to random graphs with l = r from the
conﬁguration model. For the left ﬁgure, we ﬁxed q = 0.3 and for the right ﬁgure we ﬁxed l = 25.

inf
ALGO,G∈G (m,l)

(4)

(5)

(6)

6

PError

PError

l

q

Figure 1: The iterative algorithm improves over majority voting and EM algorithm [8].

Now, let ∆LB be the minimum cost per task necessary to achieve a target accuracy  ∈ (0, 1/2)
using any graph and any possible algorithm. Then (4) implies ∆LB ∼ (1/q) log(1/), where x ∼ y
indicates that x scales as y . Let ∆Iter be the minimum cost per task sufﬁcient to achieve a target
accuracy  using our proposed algorithm. Then from (6) we get ∆Iter ∼ (1/q) log(1/). This
establishes the order-optimality of our algorithm.
It is indeed surprising that regular graphs are
sufﬁcient to achieve this optimality. Further, let ∆Ma jority be the minimum cost per task necessary
to achieve a target accuracy  using the Majority voting. Then ∆Ma jority ∼ (1/q2 ) log(1/), which
signiﬁcantly more costly than the optimal scaling of (1/q) log(1/) of our algorithm.

3 Proof of Theorem 2.1
(1/m) (cid:80)
By symmetry, we can assume all si ’s are +1. If I is a random integer drawn uniformly in [m], then
I ≤ 0), where x(k)
P(si (cid:54)= ˆsi ) ≤ P(x(k)
denotes the decision variable for task i after
i∈[m]
i
k iterations of the iterative algorithm. Asymptotically, for a ﬁxed k , l and r , the local neighborhood
I ≤ 0), we use a standard proba-
of xI converges to a regular tree. To analyze limm→∞ P(x(k)
bilistic analysis technique known as ‘density evolution’ in coding theory or ‘recursive distributional
equations’ in probabilistic combinatorics [9, 13]. Precisely, we use the following equality.
I ≤ 0) = P( ˆx(k) ≤ 0) ,
P(x(k)
lim
m→∞
where ˆx(k) is deﬁned through density evolution equations (8) and (9) in the following.
Density evolution. In the large system limit as m → ∞, the (l, r)-regular random graph locally
converges in distribution to a (l, r)-regular tree. Therefore, for a randomly chosen edge (i, j ), the
messages xi→j and yj→i converge in distribution to x and ypj deﬁned in the following density
evolution equations (8). Here and after, we drop the superscript k denoting the iteration number
whenever it is clear from the context. We initialize yp with a Gaussian distribution independent of
(cid:88)
(cid:88)
p ∼ N (1, 1). Let d= denote equality in distribution. Then, for k ∈ {1, 2, . . .},
p: y(0)
zpi ,iy(k−1)
x(k) d=
d=
zp,j x(k)
y(k)
p
pi ,i
j
i∈[l−1]
j∈[r−1]
where xj ’s, pi ’s, and yp,i ’s are independent copies of x, p, and yp , respectively. Also, zp,i ’s
and zp,j ’s are independent copies of zp . p ∈ [0, 1] is a random variable distributed according to
the distribution of the worker’s quality. zp,j ’s and xj ’s are independent. zpi ,i ’s and ypi ,i ’s are
conditionally independent conditioned on pi . Finally, zp is a random variable which is +1 with
probability p and −1 with probability 1 − p. Then, for a randomly chosen I , the decision variable
(cid:88)
x(k)
converges in distribution to
I
i∈[l]
Analyzing the density. Our strategy to provide an upper bound on P( ˆx(k) ≤ 0) is to show
that ˆx(k) is sub-Gaussian with appropriate parameters and use the Chernoff bound. A random

zpi ,iy(k−1)
pi ,i

(8)

ˆx(k) d=

.

,

,

(7)

(9)

7

 1e-05 0.0001 0.001 0.01 0.1 1 0 5 10 15 20 25 30Majority VotingEM AlgorithmIterative AlgorithmLower Bound 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4Majority VotingEM AlgorithmIterative AlgorithmLower Bound,

(12)

(13)

(10)

variable z with mean m is said to be sub-Gaussian with parameter σ if for all λ ∈ R the fol-
lowing inequality holds: E[eλz ] ≤ emλ+(1/2)σ2 λ2 . Deﬁne σ2
k ≡ 2ˆl(ˆl ˆr)k−1 + µ2ˆl3 ˆr(3q ˆr +
1)(qˆl ˆr)2k−4 (1 − (1/q2ˆl ˆr)k−1 )/(1 − (1/q2ˆl ˆr)) and mk ≡ µˆl(qˆl ˆr)k−1 for k ∈ Z. We will ﬁrst
show that, x(k) is sub-Gaussian with mean mk and parameter σ2
k for a regime of λ we are interested
in. Precisely, we will show that for |λ| ≤ 1/(2mk−1 ˆr),
] ≤ emk λ+(1/2)σ2
E[eλx(k)
k λ2
.
By deﬁnition, due to distributional independence, we have E[eλˆx(k)
] = E[eλx(k)
](l/ˆl) . Therefore, it
] ≤ e(l/ˆl)mk λ+(l/2ˆl)σ2
follows from (10) that ˆx(k) satisﬁes E[eλˆx(k)
k λ2 . Applying the Chernoff bound
P(cid:0) ˆx(k) ≤ 0(cid:1) ≤ E(cid:2)eλˆx(k) (cid:3) ≤ e−l m2
with λ = −mk /(σ2
k ), we get
k /(2 ˆl σ2
(11)
k ) ,
k ) ≤ µ2ˆl2 (qˆl ˆr)2k−3 /(3µ2 qˆl3 ˆr2 (qˆl ˆr)2k−4 ) = 1/(3ˆr), it is easy to check that
Since mkmk−1/(σ2
|λ| ≤ 1/(2mk−1 ˆr). Substituting (11) in (7), this ﬁnishes the proof of Theorem 2.1.
Now we are left to prove that x(k) is sub-Gaussian with appropriate parameters. We can write down
(cid:16)Ep
(cid:104)
(cid:105)(cid:17)ˆl
E(cid:2)eλx(k) (cid:3) =
a recursive formula for the evolution of the moment generating functions of x and yp as
(cid:16)
pE(cid:2)eλx(k) (cid:3) + ¯pE(cid:2)e−λx(k) (cid:3)(cid:17) ˆr
|p] + ¯pE[e−λy(k−1)
|p]
E(cid:2)eλy(k)
p (cid:3) =
pE[eλy(k−1)
p
p
,
where ¯p = 1 − p and ¯p = 1 − p. We can prove that these are sub-Gaussian using induction.
First, for k = 1, we show that x(1) is sub-Gaussian with mean m1 = µˆl and parameter σ2
1 = 2ˆl,
where µ ≡ E[2p − 1]. Since yp is initialized as Gaussian with unit mean and variance, we have
E[eλy(0)
p ] = eλ+(1/2)λ2 regardless of p. Substituting this into (12), we get for any λ, E[eλx(1)
] =
(E[p]eλ + (1 − E[p])e−λ )ˆl e(1/2)ˆlλ2 ≤ eˆlµλ+ˆlλ2 , where the inequality follows from the fact that
aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z2 for any z ∈ R and a ∈ [0, 1] (cf. [14, Lemma A.1.5]).
] ≤ emk λ+(1/2)σ2
k λ2 for |λ| ≤ 1/(2mk−1 ˆr), we show that E[eλx(k+1)
] ≤
Next, assuming E[eλx(k)
k+1 λ2 for |λ| ≤ 1/(2mk ˆr), and compute appropriate mk+1 and σ2
emk+1 λ+(1/2)σ2
k+1 . Substituting
] ≤ emk λ+(1/2)σ2
p ] ≤ (pemk λ + ¯pe−mk λ ) ˆr e(1/2) ˆrσ2
the bound E[eλx(k)
k λ2 in (13), we get E[eλy(k)
k λ2 .
eλx(k+1) (cid:105) ≤ (cid:16)Ep
E(cid:104)
(cid:104)
p(pemk λ + ¯pe−mk λ ) ˆr + ¯p(pe−mk λ + ¯pemk λ ) ˆr (cid:105)(cid:17)ˆl
Further applying this bound in (12), we get
e(1/2)ˆl ˆrσ2
k λ2
To bound the ﬁrst term in the right-hand side, we use the next key lemma.
(cid:104)
p(pez + ¯pe−z ) ˆr + ¯p(¯pez + pe−z ) ˆr (cid:105) ≤ eq ˆrz+(1/2)(3q ˆr2+ ˆr)z2
Lemma 3.1. For any |z | ≤ 1/(2ˆr) and p ∈ [0, 1] such that q = E[(2p − 1)2 ], we have
Ep
.
(cid:1)λ2 , for |λ| ≤ 1/(2mk ˆr). In the regime where qˆl ˆr ≥
] ≤ eqˆl ˆrmk λ+(1/2)(cid:0)(3qˆl ˆr2+ˆl ˆr)m2
For the proof, we refer to the journal version of this paper. Applying this inequality to (14) gives
E[eλx(k+1)
k+ˆl ˆrσ2
k
1 as per our assumption, mk is non-decreasing in k . At iteration k , the above recursion holds for
|λ| ≤ min{1/(2m1 ˆr), . . . , 1/(2mk−1 ˆr)} = 1/(2mk−1 ˆr). Hence, we get a recursion for mk and
σk such that (10) holds for |λ| ≤ 1/(2mk−1 ˆr):
k + ˆl ˆrσ2
k+1 = (3qˆl ˆr2 + ˆl ˆr)m2
mk+1 = qˆl ˆrmk ,
σ2
k .
1 = 2ˆl, we have mk = µˆl(qˆl ˆr)k−1 for k ∈ {1, 2, . . .} and
1 ak−1 + bck−2 (cid:80)k−2
With the initialization m1 = µˆl and σ2
k−1 + bck−2 for k ∈ {2, 3, . . .}, with a = ˆl ˆr , b = µ2ˆl2 (3qˆl ˆr2 + ˆl ˆr), and c = (qˆl ˆr)2 . After
σ2
k = aσ2
(cid:96)=0 (a/c)(cid:96) . For ˆl ˆrq2 (cid:54)= 1, we have a/c (cid:54)= 1,
some algebra, it follows that σ2
k = σ2
1 ak−1 + bck−2 (1 − (a/c)k−1 )/(1 − a/c). This ﬁnishes the proof of (10).
whence σ2
k = σ2

.(14)

8

References
[1] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–
28, 1979.
[2] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective
labelling of venus images. In Advances in neural information processing systems, pages 1085–
1092, 1995.
[3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):pp.
1–38, 1977.
[4] R. Jin and Z. Ghahramani. Learning with multiple labels. In Advances in neural information
processing systems, pages 921–928, 2003.
[5] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
from crowds. J. Mach. Learn. Res., 99:1297–1322, August 2010.
[6] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more:
In Advances in Neural
Optimal integration of labels from labelers of unknown expertise.
Information Processing Systems, volume 22, pages 2035–2043, 2009.
[7] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds.
In Advances in Neural Information Processing Systems, pages 2424–2432, 2010.
[8] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data
mining using multiple, noisy labelers. In Proceeding of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining, KDD ’08, pages 614–622. ACM, 2008.
[9] T. Richardson and R. Urbanke. Modern Coding Theory. Cambridge University Press, march
2008.
[10] B. Bollob ´as. Random Graphs. Cambridge University Press, January 2001.
[11] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann Publ., San Mateo,
Califonia, 1988.
[12] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its gen-
eralizations, pages 239–269. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,
2003.
Information, Physics, and Computation. Oxford University
[13] M. Mezard and A. Montanari.
Press, Inc., New York, NY, USA, 2009.
[14] N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley, 2008.

9


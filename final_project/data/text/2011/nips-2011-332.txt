A Non-Parametric Approach to
Dynamic Programming

Oliver B. Kroemer1,2
Jan Peters1,2
1 Intelligent Autonomous Systems, Technische Universität Darmstadt
2Robot Learning Lab, Max Planck Institute for Intelligent Systems
{kroemer,peters}@ias.tu-darmstadt.de

Abstract

In this paper, we consider the problem of policy evaluation for continuous-
state systems. We present a non-parametric approach to policy evaluation,
which uses kernel density estimation to represent the system. The true
form of the value function for this model can be determined, and can be
computed using Galerkin’s method. Furthermore, we also present a uniﬁed
view of several well-known policy evaluation methods.
In particular, we
show that the same Galerkin method can be used to derive Least-Squares
Temporal Diﬀerence learning, Kernelized Temporal Diﬀerence learning, and
a discrete-state Dynamic Programming solution, as well as our proposed
method. In a numerical evaluation of these algorithms, the proposed ap-
proach performed better than the other methods.

1
Introduction
Value functions are an essential concept for determining optimal policies in both optimal con-
trol [1] and reinforcement learning [2, 3]. Given the value function of a policy, an improved
policy is straightforward to compute. The improved policy can subsequently be evaluated
to obtain a new value function. This loop of computing value functions and determining
better policies is known as policy iteration. However, the main bottleneck in policy iteration
is the computation of the value function for a given policy. Using the Bellman equation, only
two classes of systems have been solved exactly: tabular discrete state and action problems
[4] as well as linear-quadratic regulation problems [5]. The exact computation of the value
function remains an open problem for most systems with continuous state spaces [6]. This
paper focuses on steps toward solving this problem.
As an alternative to exact solutions, approximate policy evaluation methods have been
developed in reinforcement learning. These approaches include Monte Carlo methods, tem-
poral diﬀerence learning, and residual gradient methods. However, Monte Carlo methods
are well-known to have an excessively high variance [7, 2], and tend to overﬁt the value
function to the sampled data [2]. When using function approximations, temporal diﬀerence
learning can result in a biased solution[8]. Residual gradient approaches are biased unless
multiple samples are taken from the same states [9], which is often not possible for real
continuous systems.
In this paper, we propose a non-parametric method for continuous-state policy evaluation.
The proposed method uses a kernel density estimate to represent the system in a ﬂexible
manner. Model-based approaches are known to be more data eﬃcient than direct methods,
and lead to better policies [10, 11]. We subsequently show that the true value function
for this model has a Nadaraya-Watson kernel regression form [12, 13]. Using Galerkin’s
pro jection method, we compute a closed-form solution for this regression problem. The

1

resulting method is called Non-Parametric Dynamic Programming (NPDP), and is a stable
as well as consistent approach to policy evaluation.
The second contribution of this paper is to provide a uniﬁed view of several sample-based
algorithms for policy evaluation, including the NPDP algorithm. In Section 3, we show how
Least-Squares Temporal Diﬀerence learning (LSTD) in [14], Kernelized Temporal Diﬀerence
learning (KTD) in [15], and Discrete-State Dynamic Programming (DSDP) in [4, 16] can
all be derived using the same Galerkin pro jection method used to derive NPDP. In Section
4, we compare these methods using empirical evaluations.
In reinforcement learning, the uncontrolled system is usually represented by a Markov De-
cision Process (MDP). An MDP is deﬁned by the following components: a set of states
S; a set of actions A; a transition distribution p(s(cid:48) |a, s), where s(cid:48) ∈ S is the next state
given action a ∈ A in state s ∈ S; a reward function r , such that r(s, a) is the immediate
maximize the discounted rewards that are obtained; i.e., max (cid:80)∞
reward obtained for performing action a in state s; and a discount factor γ ∈ [0, 1) on future
rewards. Actions a are selected according to the stochastic policy π(a|s). The goal is to
t=0γ tr(st , at ). The term
system will refer jointly to the agent’s policy and the MDP.
The value of a state V (s), for a speciﬁc policy π , is deﬁned as the expected discounted sum
t=0γ tr(st , at )(cid:12)(cid:12) s0 = s, π(cid:9) .
V (s) = E (cid:8) (cid:80)∞
of rewards that an agent will receive after visiting state s and executing policy π ; i.e.,
´
´
By using the Markov property, Eq. (1) can be rewritten as the Bel lman equation
Sπ (a|s) p (s(cid:48) |s, a) [r (s, a) + γV π (s(cid:48) )] ds(cid:48)da.
(2)
V (s) =
A
The advantage of using the Bellman equation is that it describes the relationship between the
value function at one state s and its immediate follow-up states s(cid:48) ∼ p(s(cid:48) |s, a). In contrast,
the direct computation of Eq. (1) relies on the rewards obtained from entire tra jectories.

(1)

2 Non-Parametric Model-based Dynamic Programming
We begin describing the NPDP approach by introducing the kernel density estimation frame-
work used to represent the system. The true value function for this model has a kernel
regression form, which can be computed by using Galerkin’s pro jection method. We subse-
quently discuss some of the properties of this algorithm, including its consistency.

2.1 Non-Parametric System Modeling
The dynamics of a system are compactly represented by the joint distribution p(s, a, s(cid:48) ).
Using Bayes rule and marginalization, one can compute the transition probabilities
´
p(s(cid:48) |s, a) and the current policy π(a|s) from this joint distribution; e.g. p(s(cid:48) |s, a) =
p(s, a, s(cid:48) )/
p(s, a, s(cid:48) )ds(cid:48) . Rather than assuming that certain prior information is given,
we will focus on the problem where only sampled information of the system is available.
Hence, the system’s joint distribution is modeled from a set of n samples obtained from the
real system. The ith sample includes the current state si ∈ S, the selected action ai ∈ A,
i ∈ S, as well as the immediate reward ri ∈ R. The state space S
and the follow-up state s(cid:48)
and the action space A are assumed to be continuous.
We propose using kernel density estimation to represent the joint distribution [17, 18]
in a non-parametric manner. Unlike parametric models, non-parametric approaches
n−1(cid:80)n
use the collected data as features, which leads to accurate representations of arbitrary
functions [19]. The system’s joint distribution is therefore modeled as p(s, a, s(cid:48) ) =
i=1ψi (s(cid:48) ) ϕi (a) φi (s), where ψi (s(cid:48) ) = ψ (s(cid:48) , s(cid:48)
i ), ϕi (a) = ϕ (a, ai ), and φi (s) =
φ (s, si ) are symmetric kernel functions. In practice, the kernel functions ψ and φ will often
´
be the same. To ensure a valid probability density, each kernel must integrate to one; i.e.,
φi (s) ds = 1, ∀i, and similarly for ψ and ϕ. As an additional constraint, the kernel must
always be positive; i.e., ψi (s(cid:48) ) ϕi (a) φi (s) ≥ 0, ∀s ∈ S. This representation implies a fac-
torization into separate ψi (s(cid:48) ), ϕi (a), and φi (s) kernels. As a result, an individual sample
cannot express correlations between s(cid:48) , a, and s. However, the representation does allow
multiple samples to express correlations between these components in p(s, a, s(cid:48) ).

2

The reward function r(s, a) must also be represented. Given the kernel density estimate
(cid:80)n
representation, the expected reward for a state-action pair is denoted as [12]
(cid:80)n
k=1 rk ϕk (a) φk (s)
i=1ϕi (a) φi (s)
Having speciﬁed the model of the system dynamics and rewards, the next step is to derive
the corresponding value function.

r(s, a) = E [r |s, a] =

.

p (s) V (s) =

V (s) =

A

p (s(cid:48) , s) V (s(cid:48) ) ds(cid:48) ,

2.2 Resulting Solution
In this section, we propose an approach to computing the value function for the continuous
model speciﬁed in Section 2.1. Every policy has a unique value function, which fulﬁlls the
Bellman equation, Eq. (2), for all states [2, 20]. Hence, the goal is to solve the Bellman
equation for the entire state space, and not just at the sampled states. This goal can be
achieved by using the Galerkin pro jection method to compute the value function for the
model [21].
The Galerkin method involves ﬁrst pro jecting the integral equation into the space spanned
by a set of basis functions. The integral equation is then solved in this pro jected space. To
´
´
´
´
begin, the Bellman equation, Eq. (2), is rearranged as
Ap (s(cid:48) |s, a) γV (s(cid:48) ) π (a|s) dads(cid:48) ,
Sπ (a|s) r (s, a) p (s(cid:48) |s, a) ds(cid:48)da +
ˆ
ˆ
S
p (s(cid:48) , s) V (s(cid:48) ) ds(cid:48) .
p (a, s) r (s, a) da + γ
(3)
S
A
Before applying the Galerkin method, we derive the exact form of the value function. Ex-
(cid:80)n
(cid:80)n
panding the reward function and joint distributions, as deﬁned in Section 2.1, gives
ˆ
ˆ
(cid:80)n
i=1 riϕi (a) φi (s)
p (s) V (s) = n−1
n−1(cid:80)n
n−1(cid:80)n
k=1ϕk (a) φk (s)
ˆ
ˆ
A
S
j=1ϕj (a) φj (s)
i=1ψi (s(cid:48) ) φi (s) V (s(cid:48) ) ds(cid:48) ,
i=1 riϕi (a) φi (s) da + γ
p (s) V (s) = n−1(cid:80)n
i=1 riφi (s) + n−1(cid:80)n
ˆ
p (s) V (s) =
A
S
ψi (s(cid:48) ) φi (s) V (s(cid:48) ) ds(cid:48) ,
Therefore, p(s)V (s) = n−1(cid:80)n
i=1γ
n−1(cid:80)n
S
i=1 θiφi (s), where θ are value weights. Given that p(s) =
j=1φj (s), the true value function of the kernel density estimate system has a
(cid:80)n
Nadaraya-Watson kernel regression [12, 13] form
(cid:80)n
i=1 θiφi (s)
j=1φj (s)
Having computed the true form of the value function, the Galerkin pro jection method can be
used to compute the value weights θ . The pro jection is performed by taking the expectation
of the integral equation with respect to each of the n basis function φi . The resulting n
(cid:17)
φ (s) n−1 (cid:16)
simultaneous equations can be written as the vector equation
ˆ
ˆ
ˆ
ˆ
φ (s) n−1φ (s)T rds+γ
φ (s)T ψ (s(cid:48) )
V (s(cid:48) )ds(cid:48)ds,
φ (s) p(s)V (s)ds =
S
S
S
S
where the ith elements of the vectors are given by [r ]i = ri , [φ (s)]i = φi (s), and [ψ (s(cid:48) )]i =
ψi (s(cid:48) ). Expanding the value functions gives
(cid:17) φ (s(cid:48) )T θ
(cid:16)
ˆ
ˆ
ˆ
ˆ
(cid:80)n
φ (s) φ (s)T rds + γ
φ (s) φ (s)T θds =
i=1φi (s(cid:48) )
S
S
S ((cid:80)n
´
´
C θ = C r + γC λθ ,
i=1φi (s(cid:48) ))−1ψ (s(cid:48) ) φ (s(cid:48) )T ds(cid:48) is a stochastic
where C =
S φ (s) φ (s)T ds, and λ =
matrix; i.e., a transition matrix. The matrix C can become singular if two basis functions

φ (s)T ψ (s(cid:48) )

V (s) =

.

S

S

φ (s)

ds(cid:48)ds,

3

da + γ

(4)

Algorithm 1 Non-Parametric Dynamic Programming
Input:
n system samples:
state si , next state s(cid:48)
i
Kernel functions:
φi (sj ) = φ (si , sj ), and ψi
Discount factor:
0 ≤ γ < 1
Output:
Value function:

, and reward ri
(cid:1) = ψ (cid:0)s(cid:48)
(cid:0)s(cid:48)
i , s(cid:48)
j
j
(cid:80)n
(cid:80)n
i=1 θi φi (s)
j=1 φj (s)

V (s) =

(cid:1)

Computation:
Reward vector:
[r ]i = ri
(cid:80)n
´
Transition matrix:
φj (s(cid:48) )ψi (s(cid:48) )
[λ]i,j =
S
k=1 φk (s(cid:48) )
Value weights:
θ = (I − γλ)−1r

ds(cid:48)

are coincident. In such cases, there exists an inﬁnite set of solutions for θ . However, all of
the solutions result in identical values. The NPDP algorithm uses the solution given by
θ = (I − γλ)−1r ,
which always exists for any stochastic matrix λ. Thus, the derivation has shown that the
exact value function for the model in Section 2.1 has a Nadaraya-Watson kernel regression
form, as shown in Eq. (4), with weights θ given by Eq. (5). The non-parametric dynamic
programming algorithm is summarized in Alg. 1. The NPDP algorithm ultimately requires
only the state information s and s(cid:48) , and not the actions a. In Section 3, we will show how
this form of derivation can also be used to derive the LSTD, KTD, and DSDP algorithms.

(5)

2.3 Properties of the NPDP Algorithm
In this section, we discuss some of the key properties of the proposed NPDP algorithm,
including precision, accuracy, and computational complexity. Precision refers to how close
the predicted value function is to the true value function of the model, while accuracy refers
to how close the model is to the true system.
One of the key contributions of this paper is providing the true form of the value function for
policy evaluation with the non-parametric model described in Section 2.1. The parameters
of this value function can be computed precisely by solving Eq. (5). Even if λ is evaluated
numerically, a high level of precision can still be obtained.
As a non-parametric method, the accuracy of the NPDP algorithm depends on the number
of samples obtained from the system. It is important that the model, and thus the value
function, converges to that of the true system as the number of samples increases; i.e., that
the model is statistically consistent. In fact, kernel density estimation can be proven to have
almost sure convergence to the true distribution for a wide range of kernels [22].
the Neumann series; i.e., θ = (cid:80)∞
Given that λ is a stochastic matrix and 0 ≤ γ < 1, it is well-known that the inversion of
(I − γλ) is well-deﬁned [16]. The inversion can therefore also be expanded according to
i=0 [γλ]ir . Similar to other kernel-based policy evaluation
methods [23, 24], NPDP has a computational complexity of O(n3 ) when performed naively.
However, by taking advantage of sparse matrix computations, this complexity can be reduced
to O(nz ), where z is the number of non-zero elements in (I − γλ).

3 Relation to Existing Methods

The second contribution of this paper is to provide a uniﬁed view of Least Squares Temporal
Diﬀerence learning (LSTD), Kernelized Temporal Diﬀerence learning (KTD), Discrete-State
Dynamic Programming (DSDP), and the proposed Non-Parametric Dynamic Programming
(NPDP). In this section, we utilize the Galerkin methodology from Section 2.2 to re-derive
the LSTD, KTD, and DSDP algorithms, and discuss how these methods compare to NPDP.
A numerical comparison is given in Section 4.

4

S

ˆφ (s) p (s, s(cid:48) ) ˆφ (s(cid:48) )T ˆθds(cid:48)ds,

S

A

S

ˆφ (sk ) ˆφ (s(cid:48)
k )T ˆθ ,

3.1 Least Squares Temporal Diﬀerence Learning
basis functions ˆφi (s), see [14]. Hence, V (s) = (cid:80)m
The LSTD algorithm allows the value function V (s) to be represented by a set of m arbitrary
ˆφi (s) = ˆφ (s)T ˆθ , where ˆθ is a vector
ˆθi
p (s, a, s(cid:48) ) = n−1 (cid:80)n
i=1
of coeﬃcients learned during policy evaluation, and [ ˆφ (s)]i = ˆφi (s). In order to re-derive
the LSTD policy evaluation, the joint distribution is represented as a set of delta functions
i=1 δi (s, a, s(cid:48) ), where δi (s, a, s(cid:48) ) is a Dirac delta function centered on
(si , ai , s(cid:48)
i ). Using Galerkin’s method, the integral equation is pro jected into the space of
the basis functions ˆφ (s). Thus, Eq. (3) becomes
ˆ
ˆ
ˆ
ˆ
ˆφ (s) p (s, a) r (s, a) dsda+γ
ˆφ (s) p (s) ˆφ (s)T ˆθds =
n(cid:88)
n(cid:88)
n(cid:88)
ˆφ (si ) ˆφ (si )T ˆθ =
r (sj , aj ) ˆφ (sj ) + γ
(cid:16) ˆφ (si )T − γ ˆφ (s(cid:48)
i )T (cid:17) ˆθ =
n(cid:88)
n(cid:88)
i=1
j=1
k=1
ˆφ (si )
and thus A ˆθ = b, where A = (cid:80)n
(cid:80)n
j=1
i=1
ˆφ (si ) ( ˆφ (si )T − γ ˆφ (s(cid:48)
i )T ) and b =
i=1
j=1 r (sj , aj ) ˆφ (sj ). The ﬁnal weights are therefore given by
ˆθ = A−1b.
This equation is also solved by LSTD, including the incremental updates of A and b as
new samples are acquired [14]. Therefore, LSTD can be seen as computing the transitions
between the basis functions using a Monte Carlo approach. However, Monte Carlo methods
rely on large numbers of samples to obtain accurate results.
A key disadvantage of the LSTD method is the need to select a speciﬁc set of basis functions.
The computed value function will always be a pro jection of the true value function into the
space of these basis functions [8]. If the true value function does not lie within the space of
these basis functions, the resulting approximation may be arbitrarily inaccurate, regardless
of the number of acquired samples. However, using predeﬁned basis functions only requires
inverting an m × m matrix, which results in a lower computational complexity than NPDP.
The LSTD may also need to be regularized, as the inversion of A becomes ill-posed if the
basis functions are too densely spaced. Regularization has a similar eﬀect to changing the
transition probabilities of the system [25].

r (sj , aj ) ˆφ (sj ) ,

3.2 Kernelized Temporal Diﬀerence Learning Methods
The proposed approach is of course not the ﬁrst to use kernels for policy evaluation. Meth-
ods such as kernelized least-squares temporal diﬀerence learning [24] and Gaussian process
temporal diﬀerence learning [23] have also employed kernels in policy evaluation. Taylor
and Parr demonstrated that these methods diﬀer mainly in their use of regularization [15].
The uniﬁed view of these methods is referred to as Kernelized Temporal Diﬀerence learning.
The KTD approach assumes that the reward and value functions can be represented by
kernelized linear least-squares regression; i.e., r(s) = k(s)T K −1r and V (s) = k(s)T ˆθ ,
as p (s, a, s(cid:48) ) = n−1 (cid:80)n
where [k(s)]i = k(s, si ), [K ]ij = k(si , sj ), [r ]i = ri , and ˆθ is a weight vector. In order to
derive KTD using Galerkin’s method, it is necessary to again represent the joint distribution
i=1 δi (s, a, s(cid:48) ). The Galerkin method pro jects the integral equation
into the space of the Kronecker delta functions [ˇδ(s)]i = ˇδi (s, ai , s(cid:48)
i ), where ˇδi (s, a, s(cid:48) ) = 1
if s(cid:48) = s(cid:48)
, a = ai , and s = si ; otherwise ˇδi (s, a, s(cid:48) ) = 0. Thus, Eq. (3) becomes
ˆ
ˆ
ˆ
i

ˇδ (s) p (s, s(cid:48) ) k(s(cid:48) )T ˆθds(cid:48)ds,

ˇδ (s) p (s) k(s)T ˆθds =

S

ˇδ (s) p (s) r (s) ds + γ

S

S

5

ˇδ(sk )k(s(cid:48)
k )T ˆθ ,

By substituting p(s, a, s(cid:48) ) and applying the sifting property of delta functions, this equation
n(cid:88)
n(cid:88)
n(cid:88)
becomes
ˇδ(sj )k(sj )T K −1r + γ
ˇδ(si )k(si )T ˆθ =
i=1
j=1
k=1
and thus K ˆθ = r + γK (cid:48) ˆθ , where [K (cid:48) ]ij = k(s(cid:48)
i , sj ). The value function weights are therefore
ˆθ = (K − γK (cid:48) )−1r ,
which is identical to the solution found by the KTD approach [15]. In this manner, the
KTD approach computes a weighting ˆθ such that the diﬀerence in the value at si and the
discounted value at s(cid:48)
equals the observed empirical reward ri . Thus, only the ﬁnite set
i
of sampled states are regarded for policy evaluation. Therefore, some KTD methods, e.g.
Gaussian process temporal diﬀerence learning [23], require that the samples are obtained
from a single tra jectory to ensure that s(cid:48)
i = si+1 .
A key diﬀerence between KTD and NPDP is the representation of the value function V (s).
The form of the value function is a direct result of the representation used to embody the
state transitions. In the original paper [15], the KTD algorithm represents the transitions
by using linear kernelized regression ˆk(s(cid:48) ) = k(s)T K −1K (cid:48) , where [ˆk(s(cid:48) )]i = E[k(s(cid:48) , si )].
The value function V (s) = k(s)T ˆθ is the correct form for this transition model. However,
the transition model does not explicitly represent a conditional distribution and can lead
to inaccurate predictions. For example, consider two samples that start at s1 = 0 and
s2 = 0.75 respectively, and both transition to s(cid:48) = 0.75. For clarity, we use a box-cart
kernel with a width of one k(si , sj ) = 1 iﬀ (cid:107)si − sj (cid:107) ≤ 0.5 and 0 otherwise. Hence, K = I
In the region 0.25 ≤ s ≤ 0.5, where the two
and each row of K ’ corresponds to (0, 1).
kernels overlap, the transition model would then predict ˆk(s) = k(s)T K −1K (cid:48) = [ 0
2 ].
This prediction is however impossible as it requires that E[k(s(cid:48) , s2 )] > maxs k(s, s2 ).
In
comparison, NPDP would predict the distribution ψ(s(cid:48) ) ≡ ψ1 (s(cid:48) ) ≡ ψ2 (s(cid:48) ) for all states in
the range −0.5 ≤ s ≤ 1.25.
Similar as for LSTD, the matrix (K − γK (cid:48) ) may become singular and thus not be invertible.
As a result, KTD usually needs to be regularized [15]. Given that KTD requires inverting
an n × n matrix, this approach has a computational complexity similar to NPDP.
3.3 Discrete-State Dynamic Programming
The standard tabular DSDP approach can also be derived using the Galerkin method.
Given a system with q discrete states, the value function has the form V (s) = ˇδ(s)T v ,
q−1δ(s)T P δ(s(cid:48) ), where P is a stochastic matrix (cid:80)q
where ˇδ(s) is a vector of q Kronecker delta functions centered on the discrete states. The
corresponding reward function is r(s) = ˇδ(s)T ¯r . The joint distribution is given by p(s(cid:48) , s) =
q−1 (cid:80)q
j=1 [P ]ij = 1, ∀i and hence p(s) =
i=1 δi (s). Galerkin’s method pro jects the integral equation into the space of the
ˆ
ˆ
ˆ
states ˇδ(s). Thus, Eq. (3) becomes
ˇδ (s) p (s) ˇδ(s)T vds =
ˇδ (s) p (s) ˇδ(s)T ¯rds + γ
ˆ

S
ˇδ (s) δ(s)T P δ(s(cid:48) )ˇδ(s(cid:48) )T vds(cid:48)ds,
S
v = ¯r + γP v ,
v = (I − γP )−1 ¯r ,
(6)
which is the same computation used by DSDP [16]. The DSDP and NPDP methods actually
use similar models to represent the system. While NPDP uses a kernel density estimation,
the DSDP algorithm uses a histogram representation. Hence, DSDP can be regarded as a
special case of NPDP for discrete state systems.
The DSDP algorithm has also been the basis for continuous-state policy evaluation algo-
rithms [26, 27]. These algorithms ﬁrst use the sampled states as the discrete states of an
MDP and compute the corresponding values. The computed values are then generalized,
under a smoothness assumption, to the rest of the state-space using local averaging. Unlike
these methods, NPDP explicitly performs policy evaluation for a continuous set of states.

ˇδ (s) p (s, s(cid:48) ) ˇδ(s(cid:48) )T vds(cid:48)ds,

S

S

I v = I ¯r + γ

6

4 Numerical Evaluation

In this section, we compare the diﬀerent policy evaluation methods discussed in the previous
section, with the proposed NPDP method, on an illustrative benchmark system.

4.1 Benchmark Problem and Setup
In order to compare the LSTD, KTD, DSDP, and NPDP approaches, we evaluated the
methods on a discrete-time continuous-state system. A standard linear-Gaussian system
was used for the benchmark problem, with transitions given by s(cid:48) = 0.95s + ω where ω is
Gaussian noise N (µ = 0, σ = 0.025). The initial states are restricted to the range 0.95 to 1.
The reward functions consist of three Gaussians, as shown by the black line in Fig. 1.
The KTD method was implemented using a Gaussian kernel function and regularization.
The LSTD algorithm was implemented using 15 uniformly-spaced normalized Gaussian
basis functions, and did not require regularization. The DSDP method was implemented
by discretizing the state-space into 10 equally wide regions. The NPDP method was also
implemented using Gaussian kernels.
The hyper-parameters of all four methods, including the number of basis functions for
LSTD and DSDP, were carefully tuned to achieve the best performance. As a performance
base-line, the values of the system in the range 0 < s < 1 were computed using a Monte
Carlo estimate based on 50000 tra jectories. The policy evaluations performed by the tested
methods were always based on only 500 samples; i.e. 100 times less samples than the base-
line. The experiment was run 500 times using independent sets of 500 samples. The samples
were not drawn from the same tra jectory.

4.2 Results
The performance of the diﬀerent methods were compared using three performance measures.
´
Two of the performance measures are based on the weighted Mean Squared Error (MSE)
´
0 W (s) (V (s) − V (cid:63) (s))2 ds where V (cid:63) is the true value function and W (s) ≥ 0,
[2] E(V ) =
1
for all states, is a weighting distribution
0 W (s)ds = 1. The ﬁrst performance measure
1
Eunif corresponds to the MSE where W (s) = 1 for all states in the range zero to one. The
second performance measure Esamp corresponds to the MSE where W (s) = n−1Σn
i=1 δi (s)
respectively. Thus, Esamp is an indicator of the accuracy in the space of the samples, while
Eunif is an indicator of how well the computed value function generalizes to the entire state
space. The third performance measure Emax is given by the maximum error in the value
function. This performance measure is the basis of a bound on the overall value function
approximation [20].
The results of the experiment are shown in Table 1. The performance measures were aver-
aged over the 500 independent trials of the experiment. For all three performance measures,
the NPDP algorithm achieved the highest levels of performance, while the DSDP approach
consistently led to the worst performance.

Emax
Esamp
Eunif
1.4971 ± 0.0309
0.7185 ± 0.0321
NPDP 0.5811 ± 0.0333
1.5591 ± 0.0382
0.8932 ± 0.0412
LSTD 0.6898 ± 0.0443
2.5329 ± 0.0391
0.8681 ± 0.0270
0.7585 ± 0.0460
KTD
2.9985 ± 0.0449
2.1548 ± 0.1082
1.6979 ± 0.0332
DSDP
Table 1: Each row corresponds to one of the four tested algorithms for policy evaluation.
The columns indicate the performance of the approaches during the experiment. The per-
formance indexes include the mean squared error evaluated uniformly over the zero to one
range, the mean squared error evaluated at the 500 sampled points, and the maximum error.
The results are averaged over 500 trials. The standard errors of the means are also given.

7

Figure 1: Value functions obtained by the evaluated methods. The black lines show the
reward function. The blue lines show the value function computed from the tra jectories of
50,000 uniformly sampled points. The LSTD, KTD, DSDP, and NPDP methods evaluated
the policy using only 500 points. The presentation was divided into two plots for improved
clarity

4.3 Discussion
The LSTD algorithm achieved a relatively low Eunif value, which indicates that the tuned
basis functions could accurately represent the true value function. However, the performance
of LSTD is sensitive to the choice of basis functions and the number of samples per basis
function. Using 20 basis functions instead of 15 reduces the performance of LSTD to Eunif =
2.8705 and Esamp = 1.0256 as a result of overﬁtting. The KTD method achieved the second
best performance for Esamp , as a result of using a non-parametric representation. However,
the value tended to drop in sparsely-sampled regions, which lead to relatively high Eunif
and Emax values. The discretization of states for DSDP is generally a disadvantage when
modeling continuous systems, and resulted in poor overall performance for this evaluation.
The NPDP approach out-performed the other methods in all three performance measures.
The performance of NPDP could be further improved by using adaptive kernel density
estimation [28] to locally adapt the kernels’ bandwidths according to the sampling density.
However, all methods were restricted to using a single global bandwidth for the purpose of
this comparison.

5 Conclusion
This paper presents two key contributions to continuous-state policy evaluation. The ﬁrst
contribution is the Non-Parametric Dynamic Programming algorithm for policy evaluation.
The proposed method uses a kernel density estimate to generate a consistent representation
of the system.
It was shown that the true form of the value function for this model is
given by a Nadaraya-Watson kernel regression. The NPDP algorithm provides a solution for
calculating the value function. As a kernel-based approach, NPDP simultaneously addresses
the problems of function approximation and policy evaluation.
The second contribution of this paper is providing a uniﬁed view of Least-Squares Temporal
Diﬀerence learning, Kernelized Temporal Diﬀerence learning, and discrete-state Dynamic
Programming, as well as NPDP. All four approaches can be derived from the Bellman
equation using the Galerkin pro jection method. These four approaches were also evaluated
and compared on an empirical problem with a continuous state space and non-linear reward
function, wherein the NPDP algorithm out-performed the other methods.

Acknowledgements
The pro ject receives funding from the European Community’s Seventh Framework Pro-
gramme under grant agreement n° ICT- 248273 GeRT and n° 270327 Complacs.

8

00.10.20.30.40.50.60.70.80.91024681012StateValue  True ValueRewardLSTDKTD00.10.20.30.40.50.60.70.80.91024681012StateValue  True ValueRewardDSDPNPDPReferences
[1] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, Vol. II. Athena Scientiﬁc,
2007.
[2] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. 1998.
[3] H. Maei, C. Szepesvari, S. Bhatnagar, D. Precup, D. Silver, and R. Sutton. Convergent
temporal-diﬀerence learning with arbitrary smooth function approximation. In NIPS, pages
1204–1212, 2009.
[4] Richard Bellman. Bottleneck problems and dynamic programming. Proceedings of the National
Academy of Sciences of the United States of America, 39(9):947–951, 1953.
[5] R.E. Kalman. Contributions to the theory of optimal control, 1960.
[6] Warren B. Powell. Approximate Dynamic Programming: Solving the Curses of Dimensionality
(Wiley Series in Probability and Statistics). Wiley-Interscience, 2007.
[7] Rémi Munos. Geometric Variance Reduction in Markov Chains: Application to Value Function
and Gradient Estimation. Journal of Machine Learning Research, 7:413–427, 2006.
[8] Ralf Schoknecht. Optimality of reinforcement learning algorithms with linear function approx-
imation. In NIPS, pages 1555–1562, 2002.
[9] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
ICML, 1995.
[10] Christopher G. Atkeson and Juan C. Santamaria. A Comparison of Direct and Model-Based
Reinforcement Learning. In ICRA, pages 3557–3564, 1997.
[11] H. Bersini and V. Gorrini. Three connectionist implementations of dynamic programming for
optimal control: A preliminary comparative analysis. In Nicrosp, 1996.
[12] E. Nadaraya. On estimating regression. Theory of Prob. and Appl., 9:141–142, 1964.
[13] G. Watson. Smooth regression analysis. Sankhya, Series, A(26):359–372, 1964.
[14] Justin A. Boyan. Least-squares temporal diﬀerence learning.
In ICML, pages 49–56, San
Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.
[15] Taylor, Gavin and Parr, Ronald. Kernelized value function approximation for reinforcement
learning. In ICML, pages 1017–1024, New York, NY, USA, 2009. ACM.
[16] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc,
1996.
[17] Murray Rosenblatt. Remarks on Some Nonparametric Estimates of a Density Function. The
Annals of Mathematical Statistics, 27(3):832–837, September 1956.
[18] Emanuel Parzen. On Estimation of a Probability Density Function and Mode. The Annals of
Mathematical Statistics, 33(3):1065–1076, 1962.
[19] G. S. Kimeldorf and G. Wahba. Some results on Tchebycheﬃan spline functions. Journal of
Mathematical Analysis and Applications, 33(1):82–95, 1971.
[20] Rémi Munos. Error bounds for approximate policy iteration. In ICML, pages 560–567, 2003.
[21] Kendall E. Atkinson. The Numerical Solution of Integral Equations of the Second Kind. Cam-
bridge University Press, 1997.
[22] Dominik Wied and Rafael Weissbach. Consistency of the kernel density estimator: a survey.
Statistical Papers, pages 1–21, 2010.
[23] Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with Gaussian processes.
In ICML, pages 201–208, New York, NY, USA, 2005. ACM.
[24] Xin Xu, Tau Xie, Dewen Hu, and Xicheng Lu. Kernel least-squares temporal diﬀerence learn-
ing. International Journal of Information Technology, 11:54–63, 1997.
[25] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares tem-
poral diﬀerence learning. In ICML, pages 521–528. ACM, 2009.
[26] Nicholas K. Jong and Peter Stone. Model-based function approximation for reinforcement
learning. In AAMAS, May 2007.
[27] Dirk Ormoneit and Śaunak Sen. Kernel-Based reinforcement learning. Machine Learning,
49(2):161–178, November 2002.
[28] B. W. Silverman. Density estimation: for statistics and data analysis. London, 1986.

9


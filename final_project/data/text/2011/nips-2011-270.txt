Large-Scale Sparse Principal Component Analysis
with Application to Text Data

Youwei Zhang
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
Berkeley, CA 94720
zyw@eecs.berkeley.edu

Laurent El Ghaoui
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
Berkeley, CA 94720
elghaoui@eecs.berkeley.edu

Abstract

Sparse PCA provides a linear combination of small number of features that maxi-
mizes variance across data. Although Sparse PCA has apparent advantages com-
pared to PCA, such as better interpretability, it is generally thought to be compu-
tationally much more expensive. In this paper, we demonstrate the surprising fact
that sparse PCA can be easier than PCA in practice, and that it can be reliably
applied to very large data sets. This comes from a rigorous feature elimination
pre-processing result, coupled with the favorable fact that features in real-life data
typically have exponentially decreasing variances, which allows for many features
to be eliminated. We introduce a fast block coordinate ascent algorithm with much
better computational complexity than the existing ﬁrst-order ones. We provide ex-
perimental results obtained on text corpora involving millions of documents and
hundreds of thousands of features. These results illustrate how Sparse PCA can
help organize a large corpus of text data in a user-interpretable way, providing an
attractive alternative approach to topic models.

1

Introduction

The sparse Principal Component Analysis (Sparse PCA) problem is a variant of the classical PCA
problem, which accomplishes a trade-off between the explained variance along a normalized vector,
and the number of non-zero components of that vector.
Sparse PCA not only brings better interpretation [1], but also provides statistical regularization [2]
when the number of samples is less than the number of features. Various researchers have proposed
different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor
rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6]. Other algorithms
include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized
power method by [10]. These algorithms are based on non-convex formulations, and may only
converge to a local optimum. The (cid:96)1 -norm based semideﬁnite relaxation DSPCA, as introduced
in [1], does guarantee global convergence and as such, is an attractive alternative to local methods.
In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS
DSPCA, as developed in [1], has a computational complexity of O(n4√
and SPCA algorithms, often underperform DSPCA. However, the ﬁrst-order algorithm for solving
log n), with n the number of

1

features, which is too high for many large-scale data sets. At ﬁrst glance, this complexity estimate
indicates that solving sparse PCA is much more expensive than PCA, since we can compute one
principal component with a complexity of O(n2 ).
In this paper we show that solving DSPCA is in fact computationally easier than PCA, and hence can
be applied to very large-scale data sets. To achieve that, we ﬁrst view DSPCA as an approximation
to a harder, cardinality-constrained optimization problem. Based on that formulation, we describe
a safe feature elimination method for that problem, which leads to an often important reduction in
problem size, prior to solving the problem. Then we develop a block coordinate ascent algorithm,
with a computational complexity of O(n3 ) to solve DSPCA, which is much faster than the ﬁrst-
order algorithm proposed in [1]. Finally, we observe that real data sets typically allow for a dramatic
reduction in problem size as afforded by our safe feature elimination result. Now the comparison
between sparse PCA and PCA becomes O(ˆn3 ) v.s. O(n2 ) with ˆn (cid:28) n, which can make sparse
PCA surprisingly easier than PCA.
In Section 2, we review the (cid:96)1 -norm based DSPCA formulation, and relate it to an approximation to
the (cid:96)0 -norm based formulation and highlight the safe feature elimination mechanism as a powerful
pre-processing technique. We use Section 3 to present our fast block coordinate ascent algorithm.
Finally, in Section 4, we demonstrate the efﬁciency of our approach on two large data sets, each one
containing more than 100,000 features.
Notation. R(Y ) denotes the range of matrix Y , and Y † its pseudo-inverse. The notation log refers
to the extended-value function, with log x = −∞ if x ≤ 0.

2 Safe Feature Elimination
Primal problem. Given a n × n positive-semideﬁnite matrix Σ, the “sparse PCA” problem intro-
duced in [1] is :
Tr ΣZ − λ(cid:107)Z (cid:107)1 : Z (cid:23) 0, Tr Z = 1
φ = max
(1)
Z
where λ ≥ 0 is a parameter encouraging sparsity. Without loss of generality we may assume that
Σ (cid:31) 0.
Problem (1) is in fact a relaxation to a PCA problem with a penalty on the cardinality of the variable:
xT Σx − λ(cid:107)x(cid:107)0 : (cid:107)x(cid:107)2 = 1
ψ = max
(2)
x
Where (cid:107)x(cid:107)0 denotes the cardinality (number of non-zero elemements) in x. This can be seen by ﬁrst
Tr ΣZ − λ(cid:112)(cid:107)Z (cid:107)0 : Z (cid:23) 0, Tr Z = 1, Rank(Z ) = 1
writing problem (2) as:
where (cid:107)Z (cid:107)0 is the cardinality (number of non-zero elements) of Z . Since (cid:107)Z (cid:107)1 ≤ (cid:112)(cid:107)Z (cid:107)0 (cid:107)Z (cid:107)F =
max
(cid:112)(cid:107)Z (cid:107)0 , we obtain the relaxation
Z
Tr ΣZ − λ(cid:107)Z (cid:107)1 : Z (cid:23) 0, Tr Z = 1, Rank(Z ) = 1
max
Z
Further drop the rank constraint, leading to problem (1).
By viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage
the safe feature elimination theorem ﬁrst presented in [6, 12] for problem (2):
n(cid:88)
Theorem 2.1 Let Σ = AT A, where A = (a1 , . . . , an ) ∈ Rm×n . We have
i ξ )2 − λ)+ .
((aT
i=1
An optimal non-zero pattern corresponds to indices i with λ < (aT
i ξ )2 at optimum.
i ξ )2 ≤ λ for every ξ , (cid:107)ξ(cid:107)2 = 1. Hence,
We observe that the i-th feature is absent at optimum if (aT
we can safely remove feature i ∈ {1, . . . , n} if
Σii = aT
i ai < λ

ψ = max
(cid:107)ξ(cid:107)2=1

(3)

2

A few remarks are in order. First, if we are interested in solving problem (1) as a relaxation to prob-
lem (2), we ﬁrst calculate and rank all the feature variances, which takes O(nm) and O(n log(n))
respectively. Then we can safely eliminate any feature with variance less than λ. Second, the
elimination criterion above is conservative. However, when looking for extremely sparse solutions,
applying this safe feature elimination test with a large λ can dramatically reduce problem size and
lead to huge computational savings, as will be demonstrated empirically in Section 4. Third, in
practice, when PCA is performed on large data sets, some similar variance-based criteria is rou-
tinely employed to bring problem sizes down to a manageable level. This purely heuristic practice
has a rigorous interpretation in the context of sparse PCA, as the above theorem states explicitly the
features that can be safely discarded.

3 Block Coordinate Ascent Algorithm
of O(n4√
The ﬁrst-order algorithm developed in [1] to solve problem (1) has a computational complexity
log n). With a theoretical convergence rate of O( 1
 ), the DSPCA algorithm does not
converge fast in practice. In this section, we develop a block coordinate ascent algorithm with better
dependence on problem size (O(n3 )), that in practice converges much faster.

Failure of a direct method. We seek to apply a “row-by-row” algorithm by which we update each
row/column pair, one at a time. This algorithm appeared in the speciﬁc context of sparse covariance
estimation in [13], and extended to a large class of SDPs in [14]. Precisely, it applies to problems of
the form
f (X ) − β log det X : L ≤ X ≤ U, X (cid:31) 0,
min
(4)
X
where X = X T is a n × n matrix variable, L, U impose component-wise bounds on X , f is convex,
and β > 0.
However, if we try to update the row/columns of Z in problem (1), the trace constraint will imply that
we never modify the diagonal elements of Z . Indeed at each step, we update only one diagonal ele-
ment, and it is entirely ﬁxed given all the other diagonal elements. The row-by-row algorithm does
not directly work in that case, nor in general for SDPs with equality constraints. The authors in [14]
propose an augmented Lagrangian method to deal with such constraints, with a complication due to
the choice of appropriate penalty parameters. In our case, we can apply a technique resembling the
augmented Lagrangian technique, without this added complication. This is due to the homogeneous
nature of the objective function and of the conic constraint. Thanks to the feature elimination result
min := min1≤i≤n Σii .
(Thm. 2.1), we can always assume without loss of generality that λ < σ2

Direct augmented Lagrangian technique. We can express problem (1) as
Tr ΣX − λ(cid:107)X (cid:107)1 − 1
1
(Tr X )2 : X (cid:23) 0.
2 φ2 = max
(5)
2
X
This expression results from the change of variable X = γZ , with Tr Z = 1, and γ ≥ 0. Optimizing
over γ ≥ 0, and exploiting φ > 0 (which comes from our assumption that λ < σ2
min ), leads to the
result, with the optimal scaling factor γ equal to φ. An optimal solution Z ∗ to (1) can be obtained
from an optimal solution X ∗ to the above, via Z ∗ = X ∗ /φ. (In fact, we have Z ∗ = X ∗ / Tr(X ∗ ).)
To apply the row-by-row method to the above problem, we need to consider a variant of it, with a
strictly convex objective. That is, we address the problem
Tr ΣX − λ(cid:107)X (cid:107)1 − 1
max
2
X
where β > 0 is a penalty parameter. SDP theory ensures that if β = /n, then a solution to the
above problem is -suboptimal for the original problem [15].
Optimizing over one row/column. Without loss of generality, we consider the problem of updat-
(cid:18) S
(cid:18) Y
(cid:19)
(cid:19)
ing the last row/column of the matrix variable X . Partition the latter and the covariance matrix S
as
yT
sT

(Tr X )2 + β log det X,

: X (cid:31) 0,

s
σ

,

(6)

X =

y
x

, Σ =

3

(7)

ψ := max
x,y

: y ∈ R(Y ).

where Y , S ∈ R(n−1)×(n−1) , y , s ∈ Rn−1 , and x, σ ∈ R. We are considering the problem above,
where Y is ﬁxed, and (y , x) ∈ Rn is the variable. We use the notation t := Tr Y .
The conic constraint X (cid:31) 0 translates as yT Y † y ≤ x, y ∈ R(Y ), where R(Y ) is the range of the
(cid:18) 2(yT s − λ(cid:107)y(cid:107)1 ) + (σ − λ)x − 1
(cid:19)
matrix Y . We obtain the sub-problem
2 (t + x)2
+β log(x − yT Y † y)
Simplifying the sub-problem. We can simplify the above problem, in particular, avoid the step of
forming the pseudo-inverse of Y , by taking the dual of problem (7).
Using the conjugate relation, valid for every η > 0:
zη − log z ,
log η + 1 = min
(cid:0)z (x − yT Y † y) − log z(cid:1)
z>0
and with f (x) := (σ − λ)x − 1
2 (t + x)2 , we obtain
2(yT s − λ(cid:107)y(cid:107)1 ) + f (x) + β min
ψ + β = max
y∈R(Y )
z>0
2(yT s − λ(cid:107)y(cid:107)1 − β zyT Y † y) + max
(f (x) + β zx) − β log z
max
= min
y∈R(Y )
x
z>0
h(z ) + 2g(z )
= min
z>0
where, for z > 0, we deﬁne
:= −β log z + max
(f (x) + β zx)
h(z )
x
((σ − λ + β z )x − 1
= −β log z + max
(t + x)2 )
2
x
((σ − λ − t + β z )x − 1
= − 1
2 t2 − β log z + max
2 x2 )
x
1
= − 1
(σ − λ − t + β z )2
2 t2 − β log z +
2
with the following relationship at optimum:
x = σ − λ − t + β z .

(8)

In addition,

g(z )

:= max
y∈R(Y )

(yT Y † y)

yT s − λ(cid:107)y(cid:107)1 − β z
2
yT v − β z
yT s + min
v : (cid:107)v(cid:107)∞≤λ
2
(yT (s + v) − β z
2
(yT Y † y))

max
y∈R(Y )

(yT u − β z
2

(yT Y † y)

(yT Y † y))

max
y∈R(Y )

min
v : (cid:107)v(cid:107)∞≤λ

=

=

=

min
u : (cid:107)u−s(cid:107)∞≤λ
with the following relationship at optimum:

=

min
u : (cid:107)u−s(cid:107)∞≤λ

max
y∈R(Y )
1
2β z

uT Y u.

Y u.

y =

1
β z
2 t2 , and c := σ−λ−t,
Putting all this together, we obtain the dual of problem (7): with ψ (cid:48) := ψ+β+ 1
we have
1
1
(c + β z )2 : z > 0, (cid:107)u − s(cid:107)∞ ≤ λ.
ψ (cid:48) = min
2
β z
u,z
Since β is small, we can avoid large numbers in the above, with the change of variable τ = β z :
1
1
ψ (cid:48) − β log β = min
uT Y u − β log τ +
(c + τ )2 : τ > 0, (cid:107)u − s(cid:107)∞ ≤ λ.
2
τ
u,τ

uT Y u − β log z +

(10)

(9)

4

(11)

Solving the sub-problem. Problem (10) can be further decomposed into two stages.
First, we solve the box-constrained QP
uT Y u : (cid:107)u − s(cid:107)∞ ≤ λ,
R2 := min
u
(cid:18) s1
(cid:18) y1
(cid:18) η
(cid:19)
(cid:19)
(cid:19)
using a simple coordinate descent algorithm to exploit sparsity of Y . Without loss of generality, we
consider the problem of updating the ﬁrst coordinate of u. Partition u, Y and s as
ˆyT
u =
, Y =
, s =
,
ˆY
ˆu
ˆs
ˆy
Where, ˆY ∈ R(n−2)×(n−2) , ˆu, ˆy , ˆs ∈ Rn−2 , y1 , s1 ∈ R are all ﬁxed, while η ∈ R is the variable.
We obtain the subproblem
y1 η2 + (2 ˆyT ˆu)η : (cid:107)η − s1(cid:107) ≤ λ
min

η
for which we can solve for η analytically using the formula given below.
− ˆyT ˆu
if (cid:107)s1 + ˆyT ˆu
(cid:107) ≤ λ, y1 > 0,
y1
y1
< s1 − λ, y1 > 0 or if ˆyT ˆu > 0, y1 = 0,
s1 − λ if − ˆyT ˆu
y1
s1 + λ if − ˆyT ˆu
> s1 + λ, y1 > 0 or if ˆyT ˆu <= 0, y1 = 0.
y1
Next, we set τ by solving the one-dimensional problem:
1
R2
− β log τ +
min
2
τ
τ >0
The above can be reduced to a bisection problem over τ , or by solving a polynomial equation of
degree 3.

(c + τ )2 .

η =

(12)

(13)

Obtaining the primal variables. Once the above problem is solved, we can obtain the primal
variables y , x, as follows. Using formula (9), with β z = τ , we set y = 1
τ Y u. For the diagonal
element x, we use formula (8): x = c + τ = σ − λ − t + τ .

Algorithm summary. We summarize the above derivations in Algorithm 1. Notation: for any
symmetric matrix A ∈ Rn×n , let A\i\j denote the matrix produced by removing row i and column
j . Let Aj denote column j (or row j ) with the diagonal element Aj j removed.

Convergence and complexity. Our algorithm solves DSPCA by ﬁrst casting it to problem (6),
which is in the general form (4). Therefore, the convergence result from [14] readily applies and
hence every limit point that our block coordinate ascent algorithm converges to is the global opti-
mizer. The simple coordinate descent algorithm solving problem (11) only involves a vector product
and can take sparsity in Y easily. To update each column/row takes O(n2 ) and there are n such
columns/rows in total. Therefore, our algorithm has a computational complexity of O(K n3 ), where
K is the number of sweeps through columns. In practice, K is ﬁxed at a number independent of
compared to O(n4√
problem size (typically K = 5). Hence our algorithm has better dependence on the problem size
log n) required of the ﬁrst order algorithm developed in [1].
Fig 1 shows that our algorithm converges much faster than the ﬁrst order algorithm. On the left, both
algorithms are run on a covariance matrix Σ = F T F with F Gaussian. On the right, the covariance
matrix comes from a ”spiked model” similar to that in [2], with Σ = uuT + V V T /m, where u ∈ Rn
is the true sparse leading eigenvector, with Card(u) = 0.1n, V ∈ Rn×m is a noise matrix with
Vij ∼ N (0, 1) and m is the number of observations.

4 Numerical Examples

In this section, we analyze two publicly available large data sets, the NYTimes news articles data
and the PubMed abstracts data, available from the UCI Machine Learning Repository [16]. Both

5

Algorithm 1 Block Coordinate Ascent Algorithm
Input: The covariance matrix Σ, and a parameter ρ > 0.
1: Set X (0) = I
2: repeat
for j = 1 to n do
3:
Let X (j−1) denote the current iterate. Solve the box-constrained quadratic program
4:
\j\j u : (cid:107)u − Σj (cid:107)∞ ≤ λ
uT X (j−1)

R2 := min
u
using the coordinate descent algorithm
Solve the one-dimensional problem
− β log τ +

min
τ >0

5:

6:

(Σj j − λ − Tr X (j−1)
\j\j + τ )2

R2
1
τ
2
using a bisection method, or by solving a polynomial equation of degree 3.
\j\j = X (j−1)
First set X (j )
\j\j , and then set both X (j ) ’s column j and row j using
X (j−1)
1
X (j )
\j\j u
j =
τ
j j = Σj j − λ − Tr X (j−1)
X (j )
\j\j + τ

end for
7:
8:
Set X (0) = X (n)
9: until convergence

Figure 1: Speed comparisons between Block Coordinate Ascent and First-Order

text collections record word occurrences in the form of bag-of-words. The NYTtimes text collection
contains 300, 000 articles and has a dictionary of 102, 660 unique words, resulting in a ﬁle of size 1
GB. The even larger PubMed data set has 8, 200, 000 abstracts with 141, 043 unique words in them,
giving a ﬁle of size 7.8 GB. These data matrices are so large that we cannot even load them into
memory all at once, which makes even the use of classical PCA difﬁcult. However with the pre-
processing technique presented in Section 2 and the block coordinate ascent algorithm developed
in Section 3, we are able to perform sparse PCA analysis of these data, also thanks to the fact that
variances of words decrease drastically when we rank them as shown in Fig 2. Note that the feature
elimination result only requires the computation of each feature’s variance, and that this task is easy
to parallelize.
By doing sparse PCA analysis of these text data, we hope to ﬁnd interpretable principal components
that can be used to summarize and explore the large corpora. Therefore, we set the target cardinality
for each principal component to be 5. As we run our algorithm with a coarse range of λ to search for

6

010020030040050060070080010−1100101102103104105Problem SizeCPU Time (seconds)  Block Coordinate AscentFirst Order010020030040050060070080010−1100101102103104105Problem SizeCPU Time (seconds)  Block Coordinate AscentFirst OrderFigure 2: Sorted variances of 102,660 words in NYTimes (left) and 141,043 words in PubMed (right)

a solution with the given cardinality, we might end up accepting a solution with cardinality close,
but not necessarily equal to, 5, and stop there to save computational time.
The top 5 sparse principal components are shown in Table 1 for NYTimes and in Table 2 for PubMed.
Clearly the ﬁrst principal component for NYTimes is about business, the second one about sports,
the third about U.S., the fourth about politics and the ﬁfth about education. Bear in mind that the
NYTimes data from UCI Machine Learning Repository “have no class labels, and for copyright
reasons no ﬁlenames or other document-level metadata” [16]. The sparse principal components still
unambiguously identify and perfectly correspond to the topics used by The New York Times itself to
classify articles on its own website.

Table 1: Words associated with the top 5 sparse principal components in NYTimes
5th PC (4 words)
4th PC (4 words)
3rd PC (5 words)
2nd PC (5 words)
1st PC (6 words)
school
president
official
point
million
program
campaign
government
play
percent
children
bush
united states
team
business
company
season
u s
administration
student
market
attack
game
companies

After the pre-processing steps, it takes our algorithm around 20 seconds to search for a range of λ
and ﬁnd one sparse principal component with the target cardinality (for the NYTimes data in our
current implementation on a MacBook laptop with 2.4 GHz Intel Core 2 Duo processor and 2 GB
memory).

Table 2: Words associated with the top 5 sparse principal components in PubMed
5th PC (4 words)
4th PC (4 words)
3rd PC (5 words)
2nd PC (5 words)
1st PC (5 words)
year
tumor
human
effect
patient
infection
mice
expression
level
cell
treatment
activity
receptor
cancer
age
children
maligant
concentration
protein
binding
disease
rat
carcinoma
child

A surprising ﬁnding is that the safe feature elimination test, combined with the fact that word vari-
ances decrease rapidly, enables our block coordinate ascent algorithm to work on covariance matri-
ces of order at most n = 500, instead of the full order (n = 102660) covariance matrix for NYTimes,
so as to ﬁnd a solution with cardinality of around 5. In the case of PubMed, our algorithm only needs
to work on covariance matrices of order at most n = 1000, instead of the full order (n = 141, 043)

7

024681012x 10410−610−510−410−310−210−1100Word IndexVariance051015x 10410−610−510−410−310−210−1100Word IndexVariancecovariance matrix. Thus, at values of the penalty parameter λ that target cardinality of 5 commands,
we observe a dramatic reduction in problem sizes, about 150 ∼ 200 times smaller than the original
sizes respectively. This motivates our conclusion that sparse PCA is in a sense, easier than PCA
itself.

5 Conclusion

The safe feature elimination result, coupled with a fast block coordinate ascent algorithm, allows
to solve sparse PCA problems for very large scale, real-life data sets. The overall method works
especially well when the target cardinality of the result is small, which is often the case in applica-
tions where interpretability by a human is key. The algorithm we proposed has better computational
complexity, and in practice converges much faster than, the ﬁrst-order algorithm developed in [1].
Our experiments on text data also show that the sparse PCA can be a promising approach towards
summarizing and organizing a large text corpus.

References
[1] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct formulation of sparse
PCA using semideﬁnite programming. SIAM Review, 49(3), 2007.
[2] A.A. Amini and M. Wainwright. High-dimensional analysis of semideﬁnite relaxations for
sparse principal components. The Annals of Statistics, 37(5B):2877–2921, 2009.
[3] I. T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal
of Applied Statistics, 22:29–35, 1995.
[4] J. Cadima and I. T. Jolliffe. Loadings and correlations in the interpretation of principal com-
ponents. Journal of Applied Statistics, 22:203–214, 1995.
[5] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: exact and greedy
algorithms. Advances in Neural Information Processing Systems, 18, 2006.
[6] A. d’Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component
analysis. Journal of Machine Learning Research, 9:1269–1294, 2008.
[7] I. T. Jolliffe, N.T. Trendaﬁlov, and M. Uddin. A modiﬁed principal component technique based
on the LASSO. Journal of Computational and Graphical Statistics, 12:531–547, 2003.
[8] H. Zou, T. Hastie, and R. Tibshirani. Sparse Principal Component Analysis. Journal of Com-
putational & Graphical Statistics, 15(2):265–286, 2006.
[9] Haipeng Shen and Jianhua Z. Huang. Sparse principal component analysis via regularized low
rank matrix approximation. J. Multivar. Anal., 99:1015–1034, July 2008.
[10] M. Journ ´ee, Y. Nesterov, P. Richt ´arik, and R. Sepulchre. Generalized power method for sparse
principal component analysis. arXiv:0811.4724, 2008.
[11] Y. Zhang, A. d’Aspremont, and L. El Ghaoui. Sparse PCA: Convex relaxations, algorithms
and applications. In M. Anjos and J.B. Lasserre, editors, Handbook on Semide
nite, Cone and Polynomial Optimization: Theory, Algorithms, Software and Applications.
Springer, 2011. To appear.
[12] L. El Ghaoui. On the quality of a semideﬁnite programming bound for sparse principal com-
ponent analysis. arXiv:math/060144, February 2006.
[13] O.Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum
likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning
Research, 9:485–516, March 2008.
[14] Zaiwen Wen, Donald Goldfarb, Shiqian Ma, and Katya Scheinberg. Row by row methods for
semideﬁnite programming. Technical report, Dept of IEOR, Columbia University, 2009.
[15] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
New York, NY, USA, 2004.
[16] A. Frank and A. Asuncion. UCI machine learning repository, 2010.

8


Decoding Visual Stimuli from Neural Responses
Niru Maheswaranathan, Benjamin Naecker, Hannah Payne
CS 229: Machine Learning, Fall 2011

Abstract

A central question in neuroscience is how populations of neurons encode information about the external world and
how that information is transformed to drive behavior. Here we apply machine learning approaches to shed light on
these questions. Using data recorded from two brain areas (V4, a visual processing area, and the frontal eye ﬁeld, which
controls saccadic eye movements) in an awake behaving macaque monkey, we trained learning algorithms to decode
information about a visual stimulus or the location of a planned eye movement. The algorithms were quite successful,
indicating that both sensory information and motor plans may be read out effectively from multi-electrode recordings.
Furthermore, by performing feature selection, we are able investigate which channels encoded the most information
about the stimulus.

Background

Early studies of information processing in the brain char-
acterized the response of individual neurons to sensory
inputs. Advances in both recording and computing tech-
nologies now allow for examination of how populations of
neurons encode such information. Machine learning algo-
rithms ranging from support vector machines to Gaussian
process factor analysis have been applied effectively to
the analysis of neural data [2, 3, 6]. An important practi-
cal application of such algorithms is in the deployment of
brain machine interfaces (BMI), in which neural signals
are decoded to drive electronic devices and artiﬁcial limbs.
While early BMI efforts required that the human brain
learn what mental states drove certain actions, modern
BMI decodes the normal activity of the brain. Other ap-
plications of machine learning to neuroscience include
analysis of functional brain imaging data, such as for lie
detection [1] or for decoding a visual scene [4]. In addi-
tion, neuroscientists can also apply machine learning to
determine which features of neural activity carry the most
information about a stimulus or behavior, in order to gain
insight into how the brain efﬁciently encodes information.
Our data was recorded from visual cortical areas of an
awake behaving macaque monkey. We focused on visual
areas because the areas are generally well-understood,
and because stimuli are well-deﬁned which provides for
relatively easy incorporation into machine learning meth-
ods. Visual processing begins in the retina, where precise
patterns of output signals ("spikes") encode relatively sim-
ple visual features, such as luminance and contrast. This
information is then transferred to the visual cortex, where
it radiates out to many areas responsible for processing
speciﬁc types of visual information. Area V4 is one such
area, known to be selective for simple features, such as
orientation and basic shape, and modulated by top-down
attentional control [5]. In addition to recordings from area
V4, we also analyzed data from an area of the frontal cor-
tex known as the frontal eye ﬁeld (FEF), an area involved
in both attentional control and the planning of precise,
rapid eye movements known as saccades [7]. Figure 1a

shows where these areas are in the brain. Our data is
courtesy of Nick Steinmetz, from the Tirin Moore lab here
at Stanford.

The activity in V4 and FEF was recorded with a linear
array electrode, which is able to record from 16 channels
simultaneously in an awake behaving monkey. For data
recorded from V4, the monkey maintained ﬁxation at the
center of a computer screen, on which an oriented grating
appeared in the region of space for which that particular
part of V4 was most sensitive (Figure 1b). During record-
ings from FEF, the monkey maintained central ﬁxation
(“Target On”) while a target in one of six trained locations
appeared for a short time, and then disappeared (“Target
Off ”). The monkey then maintained ﬁxation until they
were given a signal to act ("Go-Cue"), at which time the
monkey made a controlled eye movement ("Saccade") to
the previous location of the target (Figure 1c). Example
recordings for the V4 and FEF data are shown in Fig-
ure 1d and 1e, respectively. These show the ﬁring rates
recorded from two randomly selected channels as a func-
tion of time. Note the subtle differences between the two
channels, which makes it difﬁcult to analyze such data.

We trained a series of algorithms to predict from the
spiking neural data either the orientation of the grating
(for data from V4) or the endpoint of the saccade trajectory
(for data from FEF). We also performed a series of model
analyses to explore the robustness of our trained algo-
rithms. Our results indicate that support vector machines
consistently outperform other algorithms. In addition, we
found that despite having 16 separate channels of record-
ings, a small number of them usually proved far more
informative than the remainder. A ﬁnal interesting re-
sult is that on trials for which our algorithms incorrectly
predicted the endpoint of the saccade, the animal’s reac-
tion time was signiﬁcantly longer than on trials that the
algorithm correctly predicted, suggesting that the neu-
ral activity on these trials was more noisy or somehow
sub-optimal than on correct trials.

Decoding Neural Activity

N. Maheswaranathan, B. Naecker, H. Payne

Figure 1: Experimental setup - see text for description.

V4 Experiment

First, we looked at neural recordings from visual area V4.
We pre-processed the data by computing the average ﬁr-
ing rate of the neurons recorded from a given channel over
the 0.5 s stimulus presentation. This left us with a data
set of 16 features (corresponding to each channel) and
around 1400 examples. We randomly chose ten percent of
these to be testing examples, and used the rest as training
examples. We ﬁrst trained three different machine learn-
ing algorithms to predict what orientation stimulus was
shown to the monkey based on of the activity in V4. For
our support vector machine (SVM) implementation, we
used a radial basis function (RBF) gaussian kernel with
parameters chosen via grid search and cross-validation.
The results of the grid search are shown in Figure 2b.
Figure 2a shows comparison of the accuracy of these al-
gorithms across three separate days of recording. We see
that the generalized linear model (GLM) performs poorly,
while multinomial logistic regression (MLR) and the SVM
perform reasonably well. Since there are eight possible
stimulus orientations, chance accuracy corresponds to
12.5%. The large change in accuracy over different days
suggests that the electrode placement has a signiﬁcant
impact on learning algorithm performacne. The electrode
is removed and re-inserted into the monkey’s brain each

day. Thus, poor electrode placement is the most likely
reason for the varying accuracy.
Figure 2d shows that as we vary the number of train-
ing examples, the learning algorithm (MLR) performance
increases and saturates around 80%. Here, 138 exam-
ples of data were randomly selected for testing, and the
number of selected training examples was varied. This
indicates that having more training data would not dras-
tically improve performance. We also performed feature
selection to determine which channels were most impor-
tant for the learning algorithm. Figure 2c shows these
results. All pairwise combinations of two channels (for
the third day of recording, since it performed the best as
shown in Fig. 2a) were removed from the data set and the
accuracy of a MLR model was computed. Color indicates
the accuracy with the given channels removed. We see
that channels 13 and 14 have the largest impact on per-
formance, reducing accuracy by around 10% individually
and around 20% when both are removed. The linear array
electrode used for recording is inserted depth-wise, per-
pendicular to cortex. Therefore, channels 13 and 14 are
most likely picking up signals from the deeper layers of
cortex (layers 4-5). It would be interesting to test exactly
why these channels are the most useful, and if this is a
robust phenomenon.
Up to this point, all of our analyses focused on using

2 of 5

00.511.522.530.050.10.150.20.250.30.350.4Time (s)Mean Firing Rate  Direction 1Direction 2FEFV4V4 Experiment: Predicting orientation0 secTarget On1 secTarget Off2 secGo Cue3 secFEF Experiment: Eye movements −0.100.10.20.30.40.50.600.050.10.150.20.250.30.35Time (s)Mean Firing Rate  θ = 0θ = 90ABDECDecoding Neural Activity

N. Maheswaranathan, B. Naecker, H. Payne

Figure 2: (a) Accuracy of each model. (b) Feature selection. Red line indicates accuracy with all features included. (c)
Parameter search for radial basis function SVM. (d) Accuracy vs. # of training examples.

features generated by averaging over the entire stimulus
presentation (0.5 s). We wondered if by splitting the half
a second period into distinct bins, each of which is an
independent feature, we could improve the algorithms’
performance. However, such an analysis is subject to over-
ﬁtting. Figure 3a and 3b show the training (dashed lines)
and generalization (solid lines) error as the number of
bins was increased (again with 10% of the data randomly
selected for testing) for MLR and SVM algorithms, respec-
tively. We see that the training accuracy quickly rises

to 100%, while the generalization error drops off rather
quickly. This means that we are overﬁtting the training
data when splitting the recordings into discrete time bins,
which suggests that there is not much additional infor-
mation contained by the temporal structure of the ﬁring
rates. However, for the SVM (Figure 3b), it appears that
splitting the data into a few bins (<10) may help perfor-
mance, although it is difﬁcult to say for sure given our
error bounds.

FEF Experiment

We next examined neural data from the frontal eye ﬁeld
(FEF) in macaque monkeys. We examined neural ﬁring
rates aligned with the four phases: Target On, Target Off,
Go Cue, and Saccade (Figure 1c). The total experiment

lasted three seconds, therefore there was some overlap
between Go Cue and Saccade-aligned periods.
We applied several machine learning algorithms to
decode the desired target location from neural data. Fea-
tures were the average ﬁring rate during each of these
four periods across all 16 electrodes. Using these fea-

3 of 5

A−10120.511.522.533.5log2(g)log2(c)  Accuracy77.57878.57979.58080.58181.58282.5BChannelChannelLogistic regressionGLMSVM0102030405060708090Accuracy  Day 1Day 2Day 3CDDecoding Neural Activity

N. Maheswaranathan, B. Naecker, H. Payne

Figure 3: Generalization/training error vs. bin size for MLR (a) and SVM (b)

tures, the best accuracy was 74% using a support vector
machine with a Gaussian radial basis function (Figure
4a). Since there are six possible targets, chance accuracy
would be 16.7%. SVM performed far better than both lo-
gistic regression (47% accuracy) and a generalized linear
model (56 % accuracy). To optimize the SVM, a parameter
search was used to ﬁnd the best kernel width (g) and slack
parameter (c) (Figure 4c).
Interestingly, the monkey’s reaction times were slower
on trials that were incorrectly classiﬁed by the learning
algorithm (Figure 5c). This difference was signiﬁcant for
both SVM (p = .014, one-tailed t-test) and logistic regres-
sion (p = .0049). The fact that slow trials were harder
to classify could indicate that the neural trajectories on
these trials were less reliable than those with fast reac-
tion times - if the monkey was not as conﬁdent in making
the saccade, this could be reﬂected in the neural response.
To investigate which phases of the experiment con-
tained the most information about the target location,
we excluded each of the four phases in turn from the
analysis. Not surprisingly, removing Target On impaired
performance the most, reducing accuracy from 76% to
66%. Removing Saccade-aligned information had the next
highest impact, followed by Go Cue and Target Off. In-
terestingly, even though excluding Target Off data had
very little impact in the algorithms performance, the mon-
key clearly still remembered the target location during
this period, indicating that the memory was likely stored
elsewhere than FEF, and that FEF may play a larger
role in detecting the location of a new target, and later in
initiating a saccade to the desired location.
With further improvement in recording and feature

selection, SVM should be a robust technique for decod-
ing neural data, with applications in the ﬁeld of neural
prosthetics, for example to decode desired movement tra-
jectories. Although machine learning algorithms do not
directly probe biological mechanisms, they can reveal in-
teresting features, such as increasing reaction time with
variation in neural activity, which would not otherwise be
apparent.

Conclusions

We have showed that it is possible to decode visual stimuli
using multi-unit recordings from macaque cortex. Using
feature selection, our algorithm gives evidence that infor-
mation about a visual orientation is best encoded in deep
cortical layers, and that information about present and fu-
ture saccadic eye movements is encoded in the frontal eye
ﬁeld (FEF). Furthermore, for the V4 experiment, given the
amount of data we had available, it appears as if training
algrotihms on the temporal structure of ﬁring rates after
stimulus does not improve performance. Finally, for the
FEF experiment, we observe that the monkey’s reaction
time is correlated with our machine learning algorithm
performance, suggesting that the motor regions respon-
sible for coordinating saccadic eye movements depend on
robust encoding in FEF.

Acknowledgements

We would like to thank Nick Steinmetz for guidance and
supplying us with experimental data. We would also like
to thank the TAs and Professor Ng for their support.

4 of 5

1001011020102030405060708090100Number of binsAccuracy  Day 1 (training)Day 2 (training)Day 3 (training)Day 1 (generalization)Day 2 (generalization)Day 3 (generalization)1001011021030102030405060708090100Number of binsAccuracy  Day 1 (training)Day 2 (training)Day 3 (training)Day 1 (generalization)Day 2 (generalization)Day 3 (generalization)ABDecoding Neural Activity

N. Maheswaranathan, B. Naecker, H. Payne

Figure 4: FEF results. (a) Accuracy of each model. (b) Feature selection. Red line indicates accuracy with all features
included. (c) Parameter search for radial basis function SVM.

Figure 5: FEF results. (a) Actual saccade trajectories. (b) Target classiﬁcations predicted from neural data. (c) Reaction
times slower on trials incorrectly classiﬁed by SVM.

References
[1] C Davatzikos, K Ruparel, Y Fan, D G Shen, M Acharyya, J W Loughead, R C Gur, and D D Langleben, Classifying spatial
patterns of brain activity with machine learning methods: application to lie detection., NeuroImage 28 (2005), no. 3, 663–8.
[2] Arnulf B a Graf, Adam Kohn, Mehrdad Jazayeri, and J Anthony Movshon, Decoding the activity of neuronal populations in
macaque primary visual cortex., Nature neuroscience 14 (2011), no. 2, 239–45.
[3] Y Kamitani, Decoding seen and attended motion directions from activity in the human visual cortex, Current Biology 16 (2006),
no. 11, 1096–1102.
[4] Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant, Identifying natural images from human brain activity.,
Nature 452 (2008), no. 7185, 352–5.
[5] C J McAdams and J H Maunsell, Effects of attention on the reliability of individual neurons in monkey visual cortex., Neuron 23
(1999), no. 4, 765–73.
[6] Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen I Ryu, Krishna V Shenoy, and Maneesh Sahani, Gaussian-process
factor analysis for low-dimensional single-trial analysis of neural population activity., Journal of neurophysiology 102 (2009),
no. 1, 614–35.
[7] Huihui Zhou and Robert Desimone, Feature-Based Attention in the Frontal Eye Field and Area V4 during Visual Search., Neuron
70 (2011), no. 6, 1205–17.

5 of 5

a b c MLR SVM Reaction times (s) a b c 
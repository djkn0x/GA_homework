Automated Patent Classiﬁcation

CS229/CS229A: Machine Learning

Ian Christopher • Sydney Lin • Sigurd Spieckermann
December 17, 2011

Abstract

The goal of this pro ject is to automate the process of classifying patents under the hierarchical
International Patent Classiﬁcation System (IPC). In this age of innovation, the war over intellectual
property (IP) becomes common between companies and even individuals. Although a patent
protects the ownership of IP, its application process is costly and slow. Moreover, most patent
lawyers nowadays manually classify patent applications based on their knowledge, experience and
individual research. Therefore, automation on patent classiﬁcation not only helps to reduce human
error that might lead to expensive cost, but also accelerate the application process.

1

Introduction

Our system is aiming to categorize a query
patent under a ﬁve-leveled hierarchical classiﬁca-
tion structure of diﬀerent sections, classes, sub-
classes, groups and main groups or subgroups. In
the training step, our classiﬁcation algorithm an-
alyzes the abstract, title, author (plus company,
if applicable) and citations of a given patent and
learns patterns between these features and its
assigned class(es).
In a fair amount of cases,
patents can fall under diﬀerent categories and it
may be diﬃcult for a human expert to accurately
classify a patent.
Due to our inexperience with patent patent
law, we met with a patent lawyer in the area
in order to get a better understanding of com-
mon practice and computer-aided tools in this
business. After the conversation with him, we
were surprised that for new patents applica-
tions, most patent lawyers mostly rely on their

knowledge and research on the World Intellectual
Property1 Organization website with key words.
This increases our motivation on applying ma-
chine learning to solve the multi-class classiﬁca-
tion problem.

2 Background

Text classiﬁcation is a well-known area of pattern
recognition and information retrieval. Though
there has been a suﬃcient research in the area,
we tried to keep our eﬀort largely original. That
being said we did use a number of publications
to guide us when results started to stall or there
were too many possible next steps to choose
from.
There were a number of basic text classiﬁca-
tion papers that we read to better understand
the space.
[SL01] was a well cited survey of

1 http://www.wipo.int

1

sorts into hierarchical text classiﬁcation. Their
metrics for classiﬁcation performance was espe-
[RS02] used neural networks
cially interesting.
to address the problem.
[WL09] used support
vector machines and produced competitive re-
sults. [QHLZ11] helped introduce the use of la-
tent features and their importance to the prob-
lem.
[XXYY08] had very good results using a
two step classiﬁcation scheme for each patent
that involved pruning the hierarchy before ac-
tually classifying a document. We even found
a paper that was also trying to classify patents
using text [CIW] but it ignored the hierarchical
nature of the problem.
In addition to browsing text classiﬁcation pa-
pers, we considered other types of hierarchical
classiﬁcation problems. In particular [ima] pro-
vided background of a classiﬁcation competi-
tion on the Imagenet data set and algorithms
that seemed more successful on image data.
Google research’s [SZYW10] provided informa-
tion about classifying videos.

3 Patent Data

Our data source is bulk downloads of patent
data from Google Patents, which originates from
the United States Patent and Trademark Of-
ﬁce (USPTO). Google hosts a number of diﬀer-
ent data sets within the patent legal space in-
cluding patent applications, patent grants, and
maintenance events related to the sets. We use
weekly bibliographic data on granted patents,
which Google provides for all patents since 1976.

3.1 Structure

One of the challenges of dealing with this data
is the sheer size of the data sets. A typical
week in 2010 comprises several thousand granted
patents. Each of these weekly sets takes typi-
cally under 10 MBytes compressed and approxi-

Figure 1: Granted Patents: Frequency of Types

mately 90 MBytes uncompressed in size. Conse-
quently, a single year of patents takes up about
4.5 GBytes of space. Relatively speaking this is
not a massive amount of data but on commod-
ity hardware it is a nontrivial amount as we are
intending to use many years of data. It is also
worth noting that the number of patents each
year roughly increases as time moves on so ear-
lier data sets are smaller. Another issue with
Google’s data turns out to be the inconsistency
in its structural representation across diﬀerent
years. More speciﬁcally, the data over the last
ten years is provided according to two diﬀerent
XML schemata and one preﬁx-based format.

3.2 Content

There exist diﬀerent types of patents—utility
patents, design patents, reissue patents, statu-
tory invention registrations, and plant (like
corn/beans/trees) patents—with diﬀerent prop-
erties regarding their information schemata.
Utility patents are the most common types of
patents and the ones we are focusing on, but
the data set contains patents of all types. They
include the information that we anticipate to
be meaningful
features as opposed to other
types—in particular design patents that make

2

19901995200020052010101102103104105Year#PatentsGrantedPatentsUtilityDesignPlantReissueSIRthe largest fraction of the non-utility patents,
but merely consist of very few key words and
no classes. We wonder about their usefulness,
but do not dive deeper into this question.
Further, there exist various classiﬁcation sys-
tem in diﬀerent parts of the world besides the In-
ternational Patent Classiﬁcation System (IPC).
In general, there is no one-to-one mapping be-
tween the diﬀerent systems, but United States
patents are classiﬁed by a national classiﬁcation
system as well as by the IPC. This has lasting ef-
fects on the rest of our work as we decide to use
the IPC as the basis of our classiﬁcation eﬀort
due to its deeper hierarchy.

3.3 Preprocessing

The ﬁrst step in handling the data is to parse
it into a more appropriate format. The data
set provided by Google contains a signiﬁcant
amount of redundant information represented in
the notoriously verbose XML format. As a re-
sult, the data set is largely space-ineﬃcient for
our purposes and hence wastes storage capacity
as well as computation time in subsequent pro-
cessing steps. We decided to convert the data
into a comma-separated ﬁle format that only
contained information relevant to our problem.
This step resulted in a compression ratio of the
order ten to one making storage of patent data
across many years much more feasible.
In the second step, we preprocess the patent
text data in multiple ways to improve the per-
formance of our learning algorithms.

• Stop-word and punctuation removal
Stop-words do not contribute to the richness
of information of a text and punctuation is
diﬃcult to handle properly by machines. By
removing both, the next is normalized and
better suited for our purposes. We remove
punctuation by means of the string-library

3

in Python that contains a complete set of
punctuation characters and replace all oc-
currences of those in our text data. Stop-
words are identiﬁed and removed by means
of the Natural Language Toolkit Develop-
ment (NLTK) library in Python using the
English corpus.

• Stemming In order to achieve invariance
with respect to inﬂected forms, we reduce
words to their stem using the Porter stem-
mer provided in the NLTK library.

• Mutual
information and frequency
count Only a subset of words in text cor-
pora are often indicative of the content of a
text.
In order to retain the most relevant
words and thereby limit the dimensional-
ity of our dictionary, we combine the mu-
tual information (MI) metric and the fre-
quency counts words by intersecting both
sets, which are each sorted in descend-
ing order of their values. Although MI is
only deﬁned to relate words with one par-
ticular label—in our case patent classes—
, we obtain an overall scalar metric that
attempts to generalize the relevance of a
word to all classes by summing the quan-
tities for a word with respect to a partic-
ular class over all classes. However, it is
prone to give high weight to rare words that
are greatly nonuniformly distributed across
classes while they are in fact not particularly
indicative in general.

• Latent Semantic Analysis Our above-
described steps only address
cosmetic
changes and statistical ﬁltering. By apply-
ing Latent Semantic Analysis (LSA) to our
bag-of-words matrix, we attempt to iden-
tify similar semantic meanings of words and
pro ject onto a lower-dimensional subspace
of abstracted semantics. This step will

prove vastly beneﬁcial in later sections of
this report.

optimization algorithm. The GPU implementa-
tion of the Neural Network uses a gradient de-
scent optimization method.

In the third step, we generate additional fea-
tures by computing the joint probabilities be-
tween an assignee as well as the assignees country
of origin and each class we are considering in our
classiﬁcation task. Finally, we construct the the
binary-valued matrix of classes, that a patent is
categorized by and export all data to a MAT-
LAB data ﬁle using the SciPy Python library.

4 Classiﬁcation

Our ﬁrst goal is to accurately classify patents
into the ﬁrst level of the classiﬁcation hierar-
chy. In the second step, we consider two levels of
the hierarchy and ﬂatten out the tree structure,
hence, we attempt to classify to approximately
150 diﬀerent subclasses. Third, we perform a hi-
erarchical classiﬁcation task by training a model
for each level and passing patents with active
predictions on to the next level. In these steps,
we compare a number of diﬀerent learning algo-
rithms:

• Logistic Regression
• Linear L2-regularized L2-loss soft-margin
Support Vector Machine

• Multi-layer Feedforward Neural Network (+
GPU implementation)

We implemented all algorithms, except for the
SVM, ourselves in MATLAB and the GPU Neu-
ral Network in C++/CUDA by means of the
NVIDIA linear algebra library CUBLAS and the
open source library Thrust which is the CUDA-
equivalent of the STL in C++. The cost func-
tion of the Logistic Regression algorithm and the
Neural Network are optimized using the L-BFGS

4.1 Algorithms

All algorithms except for the Neural Network are
binary classiﬁers. In order to achieve classiﬁca-
tion with multiple simultaneous activations, we
follow the one-vs-all methodology and learn a
model for each class separately. Neural Networks
are naturally capable of performing a multi-class
multiple activations classiﬁcation task. In con-
sequence of only few patents being assigned to a
certain class our data quite skewed which espe-
cially diminishes the performance of the binary
classiﬁers. In order to account for the data skew,
we up-sample the patents with positive class la-
bel giving a noteworthy improvement. In order
to comprehensively assess the performance of our
algorithms, we utilize four common metrics: ac-
curacy, precision, recall and F-Score. The ac-
curacy alone may be misleading in some cases,
e.g. when the data we train on is skewed. The
other three metrics give a better insight into the
actual performance and are in fact still relevant
after up-sampling the data because this step only
introduces balance artiﬁcially.
During early stages of testing our various al-
gorithms on a 10 weeks data set of patents from
2011, we observe that the bag-of-words feature
matrix with raw frequency counts is subopti-
mal because documents with a larger text corpus
cause larger values in their corresponding row.
In order to account for this issue, we normalize
the document word frequencies and see a signif-
icant improvement in the classiﬁcation quality.

4.1.1 Logistic Regression

In a ﬁrst step, we start with a one-vs-all regular-
ized Logistic Regression classiﬁer as a baseline

4

because it is simple to implement and because it
provides a good basis for the later evaluation of
our more advanced learning algorithms. Logistic
Regression performs well for its implementation
complexity and performs best for a bag-of-words
feature matrix reduced to 200 dimensions using
LSA. The result of this setting yields an F-Score
of approximately 0.9 on the ﬁrst level of our test
set after training the classiﬁer with L-BFGS. In
the following steps, we use the Logistic Regres-
sion implementation to verify the performance of
the other learning algorithms.
Because the regularization term does not im-
prove the F-score, we suspect logistic regression
might under ﬁt the data set. Consequently, we
implement a weighted logistic regression to im-
prove the F-score. At early stage of developing
algorithm, we deﬁned accuracy with the assump-
tion that there is only one active label. However,
the accuracy was below what we had hoped for.
We stopped further work on this direction be-
cause the low accuracy was very likely to be a
result of overﬁtting the data set.

4.1.2 Backpropagation Neural Network

Neural Networks are capable of classify patents
into multiple active classes simultaneously which
makes them an attractive algorithm to use. In a
ﬁrst implementation in MATLAB, we implement
a fully vectorized Backpropagation Neural Net-
work with one hidden layer and choose the num-
ber of hidden units close to the number of out-
put units. Coincidentally this architecture gives
us some of our best results compared to others
architectures with a single hidden layer. Beyond
adjusting the number of hidden units in a sin-
gle hidden layer, we take the following steps to
arrive at our ﬁnal version of this algorithm:

• Normalized Bag-of-Words The nor-
malized bag-of-words feature matrix yields

an F-Score of approximately 0.9 on the test
set and slightly above that for the training
set after tuning the network parameters.

• Latent Semantic Analysis A further
approach to improve on the classiﬁcation
quality drives us towards applying LSA to
our bag-of-words in order to capture se-
mantic similarity between words. However,
our scores for both, training and test set,
drop by about 15% which is indicative of
a bias problem. Diﬀerent conﬁgurations of
the single hidden layer Neural Network do
not seem to improve results.

• Latent Semantic Analysis + 2 Hidden
Layers We extend our implementation to
handle multiple hidden layers in order to re-
alize a more complex model. The number
and sizes of the hidden layers are speciﬁed
by a vector whose entries denote the num-
ber of hidden units for each layer. A 100-
dimensional feature space using 200 hidden
units for the ﬁrst, 20 units for the second
hidden layer and a regularization parameter
of 0.3 give best results across all our tests.

The convergence plot shown in Figure 2 gives a
better understanding of the relationship of the
number of iterations and the performance of the
algorithm. Overall, we notice that the cost func-
tions of our networks are diﬃcult to optimize and
even though we use the L-BFGS algorithm, we
often terminate early in local minima and have
to repeat the optimization process with a new
set of random initial values or slightly modiﬁed
network parameters.

4.1.3 Backpropagation Neural Network
(GPU)

Heavy training of the Neural Network with two
hidden layers requires several thousand itera-
tions of the L-BFGS algorithm. Unfortunately

5

Thrust library. We also implement a few special-
ized kernels in order to combine smaller opera-
tions in one kernel launch.
Our GPU implementation of the Backpropa-
gation Neural Network approximately yields a
factor 20 speed-up over the vectorized MATLAB
implementation that we modify to use gradi-
ent descent as well for fair comparison. How-
ever, we notice that the gradient descent opti-
mization algorithm is vastly inferior to the L-
BFGS algorithm and is in fact unable to opti-
mize the cost function enough to train the Neu-
ral Network. We make this observation for both
implementations—MATLAB and C++/CUDA.
Nevertheless, we appreciate the speed-up and re-
fer to future work to implement an advanced op-
timization algorithm on the GPU.

4.1.4 Linear L2-regularized L2-loss soft-
margin Support Vector Machine

Support Vector Machines are a typical learning
algorithm for text classiﬁcation in many of the
papers we looked at so we decide to apply them
to our problem. Conveniently linear support vec-
tor machines are available through the popular
library liblinear so testing them is a relatively
quick and easy task.
Though plugging the SVM library into our
code is simple and compare to the Neural Net-
work there are fewer parameters to tweak.
It
turns out that we get best results for a normal-
ized bag-of-words matrix reduced through LSA
to a 750-dimensional feature space plus the as-
signee and assignees country of origin features as
described above. The performance of the SVM
is very good and compares to our optimal results
using the Neural Network. In terms of execution
time, it is noticeably faster although we train one
model per class in consequence of the one-vs-all
method we use. We also tried to use nonlin-
ear SVMs through the library but unfortunately

Figure 2: Neural Network Convergence Plot (1
Level)

this takes hours to run on the cluster we are us-
ing. Luckily our group has some experience with
general purpose GPU programming, so we de-
cide to port the Neural Network to CUDA. For
simplicity and feasibility in the context of this
pro ject, we implement the gradient descent opti-
mization algorithm to minimize the cost function
instead of L-BFGS.

One of our tools for this implementation is
the Thrust Library, which provides a high level
interface for GPU programming. ”High-level”
here is relative term as the code is still low level.
Regardless it helps speed up development with
matrix multiplication methods and a number of
other high level utilities.

In our implementation, matrices are repre-
sented using Thrust vectors and managed by a
custom matrix class which handles the dimen-
sions and wraps required linear algebra opera-
tions. Matrix multiplications are generally exe-
cuted using CUBLAS whereas element-wise ma-
trix multiplications and reduction operations—
e.g.
required to compute the cost function—
utilize optimized CUDA implementations of the

6

05001,0001,50012345#IterationsValueofCostFunctionConvergencePlotthey do not seem to perform well and training is
slow.

ers. Nonetheless we try to develop methods that
would scale to more levels if we choose to do so.

We approach the hierarchy problem in two dis-
tinct ways. Because we are trying to classify each
patent to at least one leaf node, we ﬂatten the
hierarchy to a single level that just consists of
leaf nodes. The other approach is to classify
recursively on each level of the hierarchy until
we reach the bottom of the tree. Ultimately we
are left with comparable results between the two
which actually seem to agree with a few results
we found in literature before implementing.

The ﬂat approach is the easiest to implement
because it only means changing the labels ma-
trix in our preprocessing. After this we can just
run algorithms for dealing with just the ﬁrst level
on the data set. As such, this approach is often
called for training for each of the one hundred
plus labels on the second level. Consequently
this approach becomes much slower than the ﬁrst
level code so we are unable to optimize parame-
ters as well.

The standard recursive approach is slightly
more diﬃcult to implement but does not take too
long. We actually have a number of diﬀerent ver-
sions of this approach depending on classiﬁcation
parameters and how often we want to run latent
semantic analysis (top level, every level, etc.).
Because many of the classiﬁcation algorithms are
one vs all, we actually have more train runs here
than in the ﬂat approach, but here the training
sets are smaller as we only train on patents that
can be classiﬁed by the current node.

(a) Logistic Regression

(b) Linear SVM

Figure 3: Learning Curves

4.1.5 Hierarchy Discussion

5 Results

After getting suﬃcient results on the ﬁrst layer
of the hierarchy, we move on to tackle the hier-
archy. Taking baby steps, we decide to concen-
trate on just the ﬁrst two levels until we have
good enough accuracy to move on to the oth-

6 Future Work

There are a number of ways that we could move
forward with our work in the future.

7

01234·1040.10.150.20.25#TrainingExamplesError(1-F-score)LearningCurveTrainingSetTestSet01234·10400.10.2#TrainingExamplesError(1-F-score)LearningCurveTrainingSetTestSet(a) Classiﬁcation on 1 Level

Accuracy
Logistic Regression 0.973908 / 0.973689
Neural Network
0.984336 / 0.983452
Linear SVM
0.986899 / 0.981362

Precision
0.848143 / 0.847178
0.985644 / 0.982583
0.922070 / 0.901788

Recall
0.962301 / 0.861350
0.902030 / 0.898394
0.990729 / 0.973069

F-Score
0.901623 / 0.900660
0.941985 / 0.938604
0.955167 / 0.936073

(b) Classiﬁcation on 2 Levels

Neural Network
Linear SVM (ﬂat)
Linear SVM

Accuracy
0.995011 / 0.994597
0.998595 / 0.988762

Precision
0.957578 / 0.933084
0.892609 / 0.508447
0.6160

Recall
0.594714 / 0.573634
0.998661 / 0.818083
0.8590

F-Score
0.733735 / 0.710483
0.942662 / 0.627128
0.6968

Table 1: Results for Training/Test Set

lationships, titles, etc.).

• Hierarchy pruning A number of the
more successful papers we read, used a two
step classiﬁcation approach. During the
ﬁrst step a lightweight similarity metric was
used to prune the tree, leaving only plau-
sible categories remaining. After it would
classify in this pruned hierarchy. Of course
this would mean more training, but the re-
sults might be worth it.

• GPU optimization techniques At the
moment, we use gradient descent in our
GPU neural network propagation algo-
rithm. We use this algorithm due to its
implementation simplicity but we might be
able to use a more powerful optimization al-
gorithm to speed up our results there.
In
particular, BFGS seems like a prime candi-
date to implement.

• Classify further down the hierarchy
Much of our time has been spent on clas-
sifying on just the ﬁrst level. We held oﬀ
on classifying on the second level until we
had suﬃciently accurate results because we
were worried that otherwise we would get
bad results.

• Larger Datasets The size of the raw
XML data is one the order of several giga-
bytes per year. Though we can eﬀectively
compress these ﬁles by picking out pertinent
metadata from XML, this is still a large set
without the help of a database if we want
to hold decades worth of data without a
database. A larger dataset would help us
with the uncommon classes and dive deeper
in to the classiﬁcation hierarchy (which has
over sixty thousand leaf nodes).

• Additional features There were a num-
ber of ﬁelds in the raw XML data that we
ignored. Though most of them do not seem
useful, one in particular could be very help-
ful; patent citations. Unfortunately only
using only a year of data makes it hard
to resolve these citations, but if we had a
database we could construct numerous fea-
tures from them (citation classes, graph re-

8

References

[CIW]

[ima]

Ioana Costantea, Radu Ioan, and Bot Gert Wanka. Patent document classiﬁcation
based on mutual information feature selection.

Large Scale Visual Recognition Challenge 2010.
challenges/LSVRC/2010/pascal_ilsvrc.pdf.

http://www.image-net.org/

[QHLZ11] X. Qiu, X. Huang, Z. Liu, and J. Zhou. Hierarchical text classiﬁcation with latent concepts,
2011. http://www.aclweb.org/anthology/P/P11/P11-2105.pdf.

[RS02]

[SL01]

Miguel E. Ruiz and Padmini Srinivasan. Hierarchical text categorization using neural networks.
Information Retrieval, 5:87–118, 2002. 10.1023/A:1012782908347.

Aixin Sun and Ee-Peng Lim. Hierarchical text classiﬁcation and evaluation. In Data Mining,
2001. ICDM 2001, Proceedings IEEE International Conference on, pages 521 –528, 2001.

[SZYW10] Y. Song, M. Zhao, J. Yagnik, and X. Wu. Taxonomic classication for web-based videos. 2010.

[WL09]

X L Wang and B L Lu. Improved hierarchical svms for large-scale hierarchical text classiﬁcation
challenge. Large scale hierarchical text classiﬁcation, (60903119), 2009.

[XXYY08] G.R. Xue, D. Xing, Q. Yang, and Y. Yu. Deep classiﬁcation in large-scale text hierarchies.
In Proceedings of the SIGIR conference on Research and development in information retrieval.
ACM Press, 2008.

9


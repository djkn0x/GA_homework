Sparse Features for PCA-Like Linear Regression

Christos Boutsidis
Mathematical Sciences Department
IBM T. J. Watson Research Center
Yorktown Heights, New York
cboutsi@us.ibm.com

Petros Drineas
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180
drinep@cs.rpi.edu

Malik Magdon-Ismail
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180
magdon@cs.rpi.edu

Abstract

Principal Components Analysis (PCA) is often used as a feature extraction proce-
dure. Given a matrix X ∈ Rn×d , whose rows represent n data points with respect
to d features, the top k right singular vectors of X (the so-called eigenfeatures),
are arbitrary linear combinations of all available features. The eigenfeatures are
very useful in data analysis, including the regularization of linear regression. En-
forcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations
of only a small number of actual features (as opposed to all available features), can
promote better generalization error and improve the interpretability of the eigen-
features. We present deterministic and randomized algorithms that construct such
sparse eigenfeatures while provably achieving in-sample performance comparable
to regularized linear regression. Our algorithms are relatively simple and practi-
cally efﬁcient, and we demonstrate their performance on sev eral data sets.

1 Introduction

Least-squares analysis was introduced by Gauss in 1795 and has since has bloomed into a staple of
the data analyst. Assume the usual setting with n tuples (x1 , y1), . . . , (xn , yn ) in Rd , where xi are
points and yi are targets. The vector of regression weights w∗ ∈ Rd minimizes (over all w ∈ Rd )
the RMS in-sample error
E (w) = vuut
n
Xi=1
(xi · w − yi )2 = kXw − yk2 .
In the above, X ∈ Rn×d is the data matrix whose rows are the vectors xi (i.e., Xij = xi [j ]); and,
y ∈ Rn is the target vector (i.e., y[i] = yi ). We will use the more convenient matrix formulation1,
namely given X and y, we seek a vector w∗ that minimizes kXw − yk2 . The minimal-norm vector
w∗ can be computed via the Moore-Penrose pseudo-inverse of X: w∗ = X+y. Then, the optimal
in-sample error is equal to:

E (w∗ ) = ky − XX+yk2 .
1For the sake of simplicity, we assume d ≤ n and rank (X) = d in our exposition; neither assumption is
necessary.

1

When the data is noisy and X is ill-conditioned, X+ becomes unstable to small perturbations and
over ﬁtting can become a serious problem. Practitioners dea l with such situations by regularizing
the regression. Popular regularization methods include, for example, the Lasso [28], Tikhonov
regularization [17], and top-k PCA regression or truncated SVD regularization [21]. In general,
such methods are encouraging some form of parsimony, thereby reducing the number of effective
degrees of freedom available to ﬁt the data. Our focus is on to p-k PCA regression which can be
viewed as regression onto the top-k principal components, or, equivalently, the top-k eigenfeatures.
The eigenfeatures are the top-k right singular vectors of X and are arbitrary linear combinations
of all available input features. The question we tackle is whether one can efﬁciently extract
sparse
eigenfeatures (i.e., eigenfeatures that are linear combinations of only a small number of the available
features) that have nearly the same performance as the top-k eigenfeatures.
Basic notation. A, B , . . . are matrices; a, b, . . . are vectors; i, j, . . . are integers; In is the n × n
identity matrix; 0m×n is the m × n matrix of zeros; ei is the standard basis (whose dimensionality
will be clear from the context). For vectors, we use the Euclidean norm k · k2 ; for matrices, the
Frobenius and the spectral norms: kXk2
F = Pi,j X2
ij and kXk2 = σ1 (X), i.e., the largest singular
value of X .
Top-k PCA Regression. Let X = UΣV T be the singular value decomposition of X, where U
(resp. V) is the matrix of left (resp. right) singular vectors of X with singular values in the diagonal
matrix Σ. For k ≤ d, let Uk , Σk , and Vk contain only the top-k singular vectors and associated
singular values. The best rank-k reconstruction of X in the Frobenius norm can be obtained from
this truncated singular value decomposition as Xk = UkΣk V T
k . The k right singular vectors in Vk
are called the top-k eigenfeatures. The projections of the data points onto the top k eigenfeatures are
obtained by projecting the xi ’s onto the columns of Vk to obtain Fk = XVk = UΣV T Vk = UkΣk .
Now, each data point (row) in Fk only has k dimensions. Each column of Fk contains a particular
eigenfeature’s value for every data point and is a linear combination of the columns of X.

The top-k PCA regression uses Fk as the data matrix and y as the target vector to produce regression
k = F+
k y. The in-sample error of this k-dimensional regression is equal to
weights w∗
kyk2 = ky − Uk U T
k U T
k yk2 = ky − UkΣkΣ−1
ky − Fkw∗
k k2 = ky − Fk F+
k yk2 .
k are k-dimensional and cannot be applied to X, but the equivalent weights Vkw∗
The weights w∗
k
can be applied to X and they have the same in-sample error with respect to X:
k k2 = ky − Uk U T
E (Vkw∗
k ) = ky − XVkw∗
k k2 = ky − Fkw∗
k yk2 .
k as the top-k PCA regression weights (the dimension will
k and Vkw∗
Hence, we will refer to both w∗
k to refer to both
make it clear which one we are talking about) and, for simplicity, we will overload w∗
these weight vectors (the dimension will make it clear which). In practice, k is chosen to measure
the “effective dimension ” of the data, and, typically,
k ≪ rank(X) = d. One way to choose k is so
that kX − Xk kF ≪ σk (X) (the “energy ” in the
k-th principal component is large compared to the
energy in all smaller principal components). We do not argue the merits of top-k PCA regression;
we just note that top-k PCA regression is a common tool for regularizing regression.
Problem Formulation. Given X ∈ Rn×d , k (the number of target eigenfeatures for top-k PCA
regression), and r > k (the sparsity parameter), we seek to extract a set of at most k sparse eigenfea-
tures ˆVk which use at most r of the actual dimensions. Let ˆFk = X ˆVk ∈ Rn×k denote the matrix
whose columns are the k extracted sparse eigenfeatures, which are a linear combination of a set of at
most r actual features. Our goal is to obtain sparse features for which the vector of sparse regression
k y results in an in-sample error ky − ˆFk ˆF+
weights ˆwk = ˆF+
k yk2 that is close to the top-k PCA
regression error ky − Fk F+
k yk2 . Just as with top-k PCA regression, we can de ﬁne the equivalent
d-dimensional weights ˆVk ˆwk ; we will overload ˆwk to refer to these weights as well.
Finally, we conclude by noting that while our discussion above has focused on simple linear regres-
sion, the problem can also be de ﬁned for multiple regression , where the vector y is replaced by a
matrix Y ∈ Rn×ω , with ω ≥ 1. The weight vector w becomes a weight matrix, W, where each
column of W contains the weights from the regression of the corresponding column of Y onto the
features. All our results hold in this general setting as well, and we will actually present our main
contributions in the context of multiple regression.

2

2 Our contributions

Recall from our discussion at the end of the introduction that we will present all our results in the
general setting, where the target vector y is replaced by a matrix Y ∈ Rn×ω . Our ﬁrst theorem
argues that there exists a polynomial-time deterministic algorithm that constructs a feature matrix
ˆFk ∈ Rn×k , such that each feature (column of ˆFk ) is a linear combination of at most r actual
features (columns) from X and results in small in-sample error . Again, this should be contrasted
with top-k PCA regression, which constructs a feature matrix Fk , such that each feature (column of
Fk ) is a linear combination of all features (columns) in X. Our theorems argue that the in-sample
error of our features is almost as good as the in-sample error of top-k PCA regression, which uses
dense features.
Theorem 1 (Deterministic Feature Extraction). Let X ∈ Rn×d and Y ∈ Rn×ω be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and Y.
For any r > k , there exists an algorithm that constructs a feature matrix ˆFk = X ˆVk ∈ Rn×k , such
that every column of ˆFk is a linear combination of (the same) at most r columns of X , and
k kF +  1 + r 9k
r ! kX − Xk kF
Y − X ˆWk (cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)
= kY − ˆFk ˆF+
k YkF ≤ kY − XW∗
kYk2 .
σk (X)
(σk (X) is the k-th singular value of X.) The running time of the proposed algorithm is T (Vk ) +
O (cid:0)ndk + nrk2 (cid:1), where T (Vk ) is the time required to compute the matrix Vk , the top-k right sin-
gular vectors of X .
Theorem 1 says that one can construct k features with sparsity O(k) and obtain a comparble regres-
sion error to that attained by the dense top-k PCA features, up to additive term that is proportional
to ∆k = kX − Xk kF /σk (X).
To construct the features satisfying the guarantees of the above theorem, we ﬁrst employ the Al-
gorithm DSF-Select (see Table 1 and Section 4.3) to select r columns of X and form the matrix
C ∈ Rn×r . Now, let ΠC ,k (Y) denote the best rank-k approximation (with respect to the Frobenius
norm) to Y in the column-span of C. In other words, ΠC ,k (Y) is a rank-k matrix that minimizes
kY − ΠC ,k (Y) kF over all rank-k matrices in the column-span of C. Efﬁcient algorithms are kn own
for computing ΠC,k (X) and have been described in [2]. Given ΠC ,k (Y), the sparse eigenfeatures
can be computed efﬁciently as follows: ﬁrst, set Ψ = C+ΠC ,k (Y). Observe that
CΨ = CC+ΠC ,k (Y) = ΠC ,k (Y).
The last equality follows because CC+ projects onto the column span of C and ΠC ,k (Y) is already
in the column span of C . Ψ has rank at most k because ΠC ,k (Y) has rank at most k . Let the
ψ and set ˆFk = CUψ Σψ ∈ Rn×k . Clearly, each column of ˆFk is a
SVD of Ψ be Ψ = Uψ Σψ V T
linear combination of (the same) at most r columns of X (the columns in C). The sparse features
themselves can also be obtained because ˆFk = X ˆVk , so ˆVk = X+ ˆFk .
To prove that ˆFk are a good set of sparse features, we ﬁrst relate the regressi on error from using ˆFk
to how well ΠC ,k (Y) approximates Y.
ψ kF ≥ kY − ˆFk ˆF+
= kY − ˆFk V T
kY − ΠC ,k (Y)kF = kY − CΨkF = kY − CUψ Σψ V T
k YkF .
ψ kF
The last inequality follows because ˆF+
k Y are the optimal regression weights for the features ˆFk . The
reverse inequality also holds because ΠC ,k (Y) is the best rank-k approximation to Y in the column
span of C. Thus,

kY − ˆFk ˆF+
k YkF = kY − ΠC ,k (Y)kF .
The upshot of the above discussion is that if we can ﬁnd a matri x C consisting of columns of X for
which kY − ΠC ,k (Y)kF is small, then we immediately have good sparse eigenfeatures. Indeed, all
that remains to complete the proof of Theorem 1 is to bound kY − ΠC ,k (Y)kF for the columns C
returned by the Algorithm DSF-Select.
Our second result employs the Algorithm RSF-Select (see Table 2 and Section 4.4) to select r
columns of X and again form the matrix C ∈ Rn×r . One then proceeds to construct ΠC ,k (Y) and
ˆFk as described above. The advantage of this approach is simplicity, better efﬁciency and a slightly
better error bound, at the expense of logarithmically worse sparsity.

3

Theorem 2 (Randomized Feature Extraction). Let X ∈ Rn×d and Y ∈ Rn×ω be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and
Y. For any r > 144k ln(20k), there exists a randomized algorithm that constructs a feature matrix
ˆFk = X ˆVk ∈ Rn×k , such that every column of ˆFk is a linear combination of at most r columns
of X, and, with probability at least .7 (over random choices made in the algorithm),
k kF + r 36k ln(20k)
(cid:13)(cid:13)(cid:13)
Y − X ˆWk (cid:13)(cid:13)(cid:13)F
= kY − ˆFk ˆF+
k YkF ≤ kY − XW∗
r
The running time of the proposed algorithm is T (Vk ) + O(dk + r log r).

kX − Xk kF
σk (X)

kYk2 .

3 Connections with prior work

A variant of our problem is the identiﬁcation of a matrix C con sisting of a small number (say r)
columns of X such that the regression of Y onto C (as opposed to k features from C) gives small in-
sample error. This is the sparse approximation problem, where the number of non-zero weights in the
regression vector is restricted to r. This problem is known to be NP-hard [25]. Sparse approximation
has important applications and many approximation algorithms have been presented [29, 9, 30];
proposed algorithms are typically either greedy or are based on convex optimization relaxations of
the objective. An important difference between sparse approximation and sparse PCA regression is
that our goal is not to minimize the error under a sparsity constraint, but to match the top-k PCA
regularized regression under a sparsity constraint. We argue that it is possible to achieve a provably
accurate sparse PCA-regression, i.e., use sparse features instead of dense ones.

If X = Y (approximating X using the columns of X), then this is the column-based matrix recon-
struction problem, which has received much attention in existing literature [16, 18, 14, 26, 5, 12, 20].
In this paper, we study the more general problem where X 6= Y, which turns out to be considerably
more difﬁcult.

Input sparseness is closely related to feature selection and automatic relevance determination. Re-
search in this area is vast, and we refer the reader to [19] for a high-level view of the ﬁeld. Again,
the goal in this area is different than ours, namely they seek to reduce dimensionality and improve
out-of-sample error. Our goal is to provide sparse PCA features that are almost as good as the ex-
act principal components. While it is de ﬁnitely the case tha t many methods outperform top-k PCA
regression, especially for d ≫ n, this discussion is orthogonal to our work.
The closest result to ours in prior literature is the so-called rank-revealing QR (RRQR) factoriza-
tion [8]. The authors use a QR-like decomposition to select exactly k columns of X and compare
k . They show that
their sparse solution vector ˆwk with the top-k PCA regularized solution w∗
k − ˆwk k2 ≤ pk(n − k) + 1 kX − Xk k2
kw∗
σk (X)
where ∆ = 2 k ˆwk k2 + ky − Xw∗
k k2 /σk (X). This bound is similar to our bound in Theorem 1,
but only applies to r = k and is considerably weaker. For example, pk(n − k) + 1 kX − Xk k2 ≥
√k kX − Xk kF ; note also that the dependence of the above bound on 1/σk (X) is generally worse
than ours.
The importance of the right singular vectors in matrix reconstruction problems (including PCA)
has been heavily studied in prior literature, going back to work by Jolliffe in 1972 [22]. The idea of
sampling columns from a matrix X with probabilities that are derived from V T
k (as we do in Theorem
2) was introduced in [15] in order to construct coresets for regression problems by sampling data
points (rows of the matrix X) as opposed to features (columns of the matrix X ). Other prior work
including [15, 13, 27, 6, 4] has employed variants of this sampling scheme; indeed, we borrow
proof techniques from the above papers in our work. Finally, we note that our deterministic feature
selection algorithm (Theorem 1) uses a sparsiﬁcation tool d eveloped in [2] for column based matrix
reconstruction. This tool is a generalization of algorithms originally introduced in [1].

∆,

4

4 Our algorithms

Our algorithms emerge from the constructive proofs of Theorems 1 and 2. Both algorithms necessi-
tate access to the right singular vectors of X , namely the matrix Vk ∈ Rd×k . In our experiments, we
used PROPACK [23] in order to compute Vk iteratively; PROPACK is a fast alternative to the exact
SVD. Our ﬁrst algorithm ( DSF-Select) is deterministic, while the second algorithm (RSF-Select)
is randomized, requiring logarithmically more columns to guarantee the theoretical bounds. Prior
to describing our algorithms in detail, we will introduce useful notation on sampling and rescaling
matrices as well as a matrix factorization lemma (Lemma 3) that will be critical in our proofs.

4.1 Sampling and rescaling matrices

Let C ∈ Rn×r contain r columns of X ∈ Rn×d . We can express the matrix C as C = XΩ, where
the sampling matrix Ω ∈ Rd×r is equal to [ei1 , . . . , eir ] and ei are standard basis vectors in Rd . In
our proofs, we will make use of S ∈ Rr×r , a diagonal rescaling matrix with positive entries on the
diagonal. Our column selection algorithms return a sampling and a rescaling matrix, so that XΩS
contains a subset of rescaled columns from X. The rescaling is benign since it does not affect the
span of the columns of C = XΩ and thus the quantity of interest, namely ΠC ,k (Y).

4.2 A structural result using matrix factorizations

We now present a matrix reconstruction lemma that will be the starting point for our algorithms.
Let Y ∈ Rn×ω be a target matrix and let X ∈ Rn×d be the basis matrix that we will use in order
to reconstruct Y . More speciﬁcally, we seek a
sparse reconstruction of Y from X, or, in other
words, we would like to choose r ≪ d columns from X and form a matrix C ∈ Rn×r such that
kY − ΠC ,k (Y)kF is small. Let Z ∈ Rd×k be an orthogonal matrix (i.e., Z T Z = Ik ), and express the
matrix X as follows:
X = HZT + E,
where H is some matrix in Rn×k and E ∈ Rn×d is the residual error of the factorization. It is easy
to prove that the Frobenius or spectral norm of E is minimized when H = XZ . Let Ω ∈ Rd×r and
S ∈ Rr×r be a sampling and a rescaling matrix respectively as de ﬁned i n the previous section, and
let C = XΩ ∈ Rn×r . Then, the following lemma holds (see [3] for a detailed proof).
Lemma 3 (Generalized Column Reconstruction). Using the above notation, if the rank of the matrix
Z TΩS is equal to k , then
kY − ΠC ,k (Y)kF ≤ kY − HH+ YkF + kEΩS(Z TΩS)+ H+ YkF .
We now parse the above lemma carefully in order to understand its implications in our setting. For
our goals, the matrix C essentially contains a subset of r features from the data matrix X. Recall that
ΠC ,k (Y) is the best rank-k approximation to Y within the column space of C; and, the difference
Y − ΠC ,k (Y) measures the error from performing regression using sparse eigenfeatures that are
constructed as linear combinations of the columns of C. Moving to the right-hand side of eqn. (1),
the two terms re ﬂect a tradeoff between the accuracy of the re construction of Y using H and the
error E in approximating X by the product HZT . Ideally, we would like to choose H so that Y can
be accurately approximated and, at the same time, the matrix X is approximated by the product HZ T
with small residual error E . In general, these two goals might be competing and a balance must be
struck. Here, we focus on one extreme of this trade off, namely choosing Z so that the (Frobenius)
norm of the matrix E is minimized. More speciﬁcally, since Z h as rank k , the best choice for HZT in
order to minimize kEkF is Xk ; then, E = X − Xk . Using the SVD of Xk , namely Xk = UkΣk V T
k ,
we apply Lemma 3 setting H = UkΣk and Z = Vk . The following corollary is immediate.
Lemma 4 (Generalization of Lemma 7 in [2]). Using the above notation, if the rank of the matrix
V T
kΩS is equal to k , then
k U T
k YkF + k(X − Xk )ΩS(V T
kY − ΠC ,k (Y)kF ≤ kY − Uk U T
kΩS)+Σ−1
k YkF .
Our main results will follow by carefully choosing Ω and S in order to control the right-hand side of
the above inequality.

(1)

5

Algorithm: DSF-Select

1: Input: X , k , r .
2: Output: r columns of X in C.
3: Compute Vk and
E = X − Xk = X − XVk V T
k .
4: Run DetSampling to construct sam-
pling and rescaling matrices Ω and S:

[Ω, S] = DetSampling(V T
k , E, r).

5: Return C = XΩ.

Algorithm: DetSampling
1: Input: V T = [v1 , . . . , vd ], A = [a1 , . . . , ad ], r .
2: Output: Sampling and rescaling matrices [Ω, S].
3: Initialize B0 = 0k×k , Ω = 0d×r , and S = 0r×r .
4: for τ = 1 to r − 1 do
√rk .
5:
Set Lτ = τ −
Pick index i ∈ {1, 2, ..., n} and t such that
6:
1
t ≤ L(vi , Bτ −1 , Lτ ).
U (ai ) ≤
Update Bτ = Bτ −1 + tvivT
i .
7:
Set Ωiτ = 1 and Sτ τ = 1/√t.
8:
9: end for
10: Return Ω and S.

Table 1: DSF-Select: Deterministic Sparse Feature Selection

4.3 DSF-Select: Deterministic Sparse Feature Selection

DSF-Select deterministically selects r columns of the matrix X to form the matrix C (see Table 1
and note that the matrix C = XΩ might contain duplicate columns which can be removed without
any loss in accuracy). The heart of DSF-Select is the subroutine DetSampling, a near-greedy
algorithm which selects columns of V T
k iteratively to satisfy two criteria: the selected columns should
form an approximately orthogonal basis for the columns of V T
k so that (V T
kΩS)+ is well-behaved;
and EΩS should also be well-behaved. These two properties will allow us to prove our results via
Lemma 4. The implementation of the proposed algorithm is quite simple since it relies only on
standard linear algebraic operations.
DetSampling takes as input two matrices: V T ∈ Rk×d (satisfying V T V = Ik ) and A ∈ Rn×d . In
order to describe the algorithm, it is convenient to view these two matrices as two sets of column
vectors, V T = [v1 , . . . , vd ] (satisfying Pd
i = Ik ) and A = [a1 , . . . , ad ]. In DSF-Select
i=1 vivT
we set V T = V T
k and A = E = X − Xk . Given k and r, the algorithm iterates from τ = 0 up to
τ = r − 1 and its main operation is to compute the functions φ(L , B) and L(v, B , L) that are de ﬁned
as follows:

φ (L, B) =

,

L (v, B , L) =

k
vT (B − (L + 1) Ik )−2
1
v
Xi=1
φ (L + 1, B) − φ (L , B) − vT (B − (L + 1) Ik )−1
λi − L
In the above, B ∈ Rk×k is a symmetric matrix with eigenvalues λ1 , . . . , λk and L ∈ R is a parameter.
We also de ﬁne the function U (a) for a vector a ∈ Rn as follows:
U (a) =  1 − r k
r ! aT a
kAk2
F
At every step τ , the algorithm selects a column ai such that U (ai ) ≤ L(vi , Bτ −1 , Lτ ); note that
Bτ −1 is a k × k matrix which is also updated at every step of the algorithm (see Table 1). The
existence of such a column is guaranteed by results in [1, 2].

v.

.

It is worth noting that in practical implementations of the proposed algorithm, there might exist
multiple columns which satisfy the above requirement. In our implementation we chose to break
such ties arbitrarily. However, more careful and informed choices, such as breaking the ties in a way
that makes maximum progress towards our objective, might result in considerable savings. This is
indeed an interesting open problem.

The running time of our algorithm is dominated by the search for a column which satisﬁes
U (ai ) ≤ L(vi , Bτ −1 , Lτ ). To compute the function L, we ﬁrst need to compute φ(Lτ , Bτ −1 ) (which
necessitates the eigenvalues of Bτ −1 ) and then we need to compute the inverse of Bτ −1 − (L + 1) Ik .
These computations need O(k3 ) time per iteration, for a total of O(rk3 ) time over all r iterations.
Now, in order to compute the function L for each vector vi for all i = 1, . . . , d, we need an additional

6

Algorithm: RSF-Select

1: Input: X , k , r .
2: Output: r columns of X in C.
3: Compute Vk .
4: Run RandSampling to construct sam-
pling and rescaling matrices Ω and S:

[Ω, S ] = RandSampling(V T
k , r).

5: Return C = XΩ.

Algorithm: RandSampling
1: Input: V T = [v1 , . . . , vd ] and r .
2: Output: Sampling and rescaling matrices [Ω, S].
3: For i = 1, ..., d compute probabilities

pi =

1
k kvik2
2 .
4: Initialize Ω = 0d×r and S = 0r×r .
5: for τ = 1 to r do
Select an index iτ ∈ {1, 2, ..., d} where the
6:
probability of selecting index i is equal to pi .
Set Ωiτ τ = 1 and Sτ τ = 1/√rpiτ .
7:
8: end for
9: Return Ω and S .

Table 2: RSF-Select: Randomized Sparse Feature Selection

O(dk2 ) time per iteration; the total time for all r iterations is O(drk2 ). Next, in order to compute
the function U , we need to compute aT
i ai (for all i = 1, . . . , d) which necessitates O(nnz(A)) time,
where nnz(A) is the number of non-zero elements of A. In our setting, A = E ∈ Rn×d , so the
overall running time is O(drk2 + nd). In order to get the ﬁnal running time we also need to account
for the computation of Vk and E .
The theoretical properties of DetSampling were analyzed in detail in [2], building on the original
analysis of [1]. The following lemma from [2] summarizes important properties of Ω.
Lemma 5 ([2]). DetSampling with inputs V T and A returns a sampling matrix Ω ∈ Rd×r and a
rescaling matrix S ∈ Rr×r satisfying
k(V TΩS)+k2 ≤ 1 − r k
r
We apply Lemma 5 with V = VT
k and A = E and we combine it with Lemma 4 to conclude the
proof of Theorem 1; see [3] for details.

kAΩSkF ≤ kAkF .

;

4.4 RSF-Select: Randomized Sparse Feature Selection

RSF-Select is a randomized algorithm that selects r columns of the matrix X in order to form the
matrix C (see Table 2). The main differences between RSF-Select and DSF-Select are two: ﬁrst,
RSF-Select only needs access to V T
k and, second, RSF-Select uses a simple sampling procedure in
order to select the columns of X to include in C . This sampling procedure is described in algorithm
RandSampling and essentially selects columns of X with probabilities that depend on the norms of
k . Thus, RandSampling ﬁrst computes a set of probabilities that are proportional
the columns of V T
to the norms of the columns of V T
k and then samples r columns of X in r independent identical trials
with replacement, where in each trial a column is sampled according to the computed probabilities.
Note that a column could be selected multiple times. In terms of running time, and assuming that
the matrix Vk that contains the top k right singular vectors of X has already been computed, the
proposed algorithm needs O(dk) time to compute the sampling probabilities and an additional O(d+
r log r) time to sample r columns from X . Similar to Lemma 5, we can prove analogous properties
for the matrices Ω and S that are returned by algorithm RandSampling. Again, combining with
Lemma 4 we can prove Theorem 2; see [3] for details.

5 Experiments

The goal of our experiments is to illustrate that our algorithms produce sparse features which per-
form as well in-sample as the top-k PCA regression. It turns out that the out-of-sample performance
is comparable (if not better in many cases, perhaps due to the sparsity) to top-k PCA-regression.

7

Data

(n; d)

Arcene

I-sphere

LibrasMov

Madelon

HillVal

Spambase

(100;10,000)

(351;34)

(45;90)

(2,000;500)

(606;100)

(4601;57)

k = 5, r = k + 1
ˆwDSF
ˆwRSF
k
k
0.91
0.88
0.98
0.94
0.55
0.52
0.53
0.57
3.1
2.9
3.7
3.6
0.98
0.98
0.98
0.98
0.66
0.67
0.68
0.67
0.31
0.30
0.30
0.30

ˆwrnd
k
1.0
1.0
0.57
0.57
3.7
3.7
1.0
1.0
0.68
0.68
0.28
0.38

w∗
k

0.93
0.99
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.30
0.30

k = 5, r = 2k
ˆwDSF
ˆwRSF
k
k
0.86
0.89
0.98
0.97
0.52
0.51
0.54
0.55
2.6
2.4
3.6
3.3
0.97
0.97
0.98
0.98
0.65
0.67
0.69
0.67
0.3
0.3
0.3
0.3

ˆwrnd
k
1.0
1.0
0.56
0.56
3.6
3.6
1.0
1.0
0.69
0.69
0.25
0.35

w∗
k

0.93
1.0
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.3
0.3

Table 3: Comparison of DSF-select and RSF-select with top-k PCA. The top entry in each cell
is the in-sample error, and the bottom entry is the out-sample error. In bold is the method achieving
the best out-sample error.

Compared to top-k PCA, our algorithms are efﬁcient and work well in practice, e ven better than the
theoretical bounds suggest.

We present our ﬁndings in Table 3 using data sets from the UCI m achine learning repository. We
used a ﬁve-fold cross validation design with 1,000 random sp lits: we computed regression weights
using 80% of the data and estimated out-sample error in the remaining 20% of the data. We set k = 5
in the experiments (no attempt was made to optimize k). Table 3 shows the in- and out-sample error
k ; r-sparse features regression using DSF-select, ˆwDSF
for four methods: top-k PCA regression, w∗
;
k
r-sparse features regression using RSF-select, ˆwRSF
; r-sparse features regression using r random
k
columns, ˆwrnd
k .

6 Discussion

The top-k PCA regression constructs “features” without looking at th e targets – it is target-agnostic.
So are all the algorithms we discussed here, as our goal was to compare with top-k PCA. However,
there is unexplored potential in Lemma 3. We only explored one extreme choice for the factorization,
namely the minimization of some norm of the matrix E . Other choices, in particular non-target-
agnostic choices, could prove considerably better. Such investigations are left for future work.

As mentioned when we discussed our deterministic algorithm, it will often be the case that in some
steps of the greedy selection process, multiple columns could satisfy the criterion for selection. In
such a situation, we are free to choose any one; we broke ties arbitrarily in our implementation,
and even as is, the algorithm performed as well or better than top-k PCA. However, we expect that
breaking the ties so as to optimize the ultimate objective would yield considerable additional bene ﬁt;
this would also be non-target-agnostic.

Acknowledgments

This work has been supported by two NSF CCF and DMS grants to Petros Drineas and Malik
Magdon-Ismail.

References
[1] J. Batson, D. Spielman, and N. Srivastava. Twice-ramanujan sparsi ﬁers. In Proceedings of ACM STOC,
pages 255–262, 2009.
[2] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column based matrix reconstruction. In
Proceedings of IEEE FOCS, 2011.
[3] C. Boutsidis, P. Drineas, and M. Magdon-Ismail.
manuscript, 2011.
[4] C. Boutsidis and M. Magdon-Ismail.
arXiv:1109.5664v1, 2011.

Deterministic feature selection for k-means clustering.

Sparse features for PCA-like linear regression.

8

In Proceedings of ACM

[5] C. Boutsidis, M. W. Mahoney, and P. Drineas. An improved approximation algorithm for the column
subset selection problem. In Proceedings of ACM -SIAM SODA, pages 968–977, 2009.
[6] C. Boutsidis, M. W. Mahoney, and P. Drineas. Unsupervised feature selection for the k-means clustering
problem. In Proceedings of NIPS, 2009.
[7] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal components. Applied
Statistics, 22:203–214, 1995.
[8] T. Chan and P. Hansen. Some applications of the rank revealing QR factorization. SIAM Journal on
Scienti ﬁc and Statistical Computing , 13:727–741, 1992.
[9] A. Das and D. Kempe. Algorithms for subset selection in linear regression.
STOC, 2008.
[10] A. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. Sampling algorithms and coresets for
Lp regression. In Proceedings of ACM-SIAM SODA, 2008.
[11] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA
using semideﬁnite programming. In Proceedings of NIPS, 2004.
[12] A. Deshpande and L. Rademacher. Efﬁcient volume sampli ng for row/column subset selection. In Pro-
ceedings of ACM STOC, 2010.
[13] P. Drineas, R. Kannan, and M. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating
matrix multiplication. SIAM Journal of Computing, 36(1):132–157, 2006.
[14] P. Drineas, M. Mahoney, and S. Muthukrishnan. Polynomial time algorithm for column-row based
relative-error low-rank matrix approximation. Technical Report 2006-04, DIMACS, March 2006.
[15] P. Drineas, M. Mahoney, and S. Muthukrishnan. Sampling algorithms for ℓ2 regression and applications.
In Proceedings of ACM-SIAM SODA, pages 1127–1136, 2006.
[16] G. Golub. Numerical methods for solving linear least squares problems. Numerische Mathematik, 7:206–
216, 1965.
[17] G. Golub, P. Hansen, and D. O’Leary. Tikhonov regularization and total least squares. SIAM Journal on
Matrix Analysis and Applications, 21(1):185–194, 2000.
[18] M. Gu and S. Eisenstat. Efﬁcient algorithms for computi ng a strong rank-revealing QR factorization.
SIAM Journal on Scienti ﬁc Computing , 17:848–869, 1996.
[19] I. Guyon and A. Elisseeff. Special issue on variable and feature selection. Journal of Machine Learning
Research, 3, 2003.
[20] N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: probabilistic algorithms for
constructing approximate matrix decompositions. SIAM Review, 2011.
[21] P. Hansen. The truncated SVD as a method for regularization. BIT Numerical Mathematics, 27(4):534–
553, 1987.
[22] I. Jolliffe. Discarding variables in Principal Component Analysis: asrti ﬁcial data.
21(2):160–173, 1972.
[23] R. Larsen.
PROPACK: A software package for the symmetric eigenvalue problem and sin-
gular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization.
http://soi.stanford.edu/∼rmunk/∼PROPACK/.
[24] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: exact and greedy algorithms.
In Proceedings of NIPS, 2005.
[25] B. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227–
234, 1995.
[26] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric functional
analysis. Journal of the ACM, 54, 2007.
[27] N. Srivastava and D. Spielman. Graph sparsi ﬁcations by effective resistances. In Proceedings of ACM
STOC, pages 563–568, 2008.
[28] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
pages 267–288, 1996.
[29] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information
Theory, 50(10):2231–2242, 2004.
[30] T. Zhang. Generating a d-dimensional linear subspace efﬁciently. In Adaptive forward-backward greedy
algorithm for sparse learning with linear models, 2008.

Applied Statistics,

9


Resampling Detection for Digital Image Forensics
John Ho, Derek Ma, and Justin Meyer

1

Abstract—A virtually unavoidable consequence of ma-
nipulations on digital images are statistical correlations
introduced between the pixels. These correlations may not
be visible to a human, but can be detected by statistical
techniques. This paper presents a machine learning based
approach to image resampling detection based on the
detector by Popescu and Farid. We investigate ways
to improve robustness and detection accuracy by using
supervised learning techniques.

I . IN TRODUC T ION
With the proliferation of digital images and pow-
erful image editing tools such as Photoshop, it has
become increasingly easy to manipulate images to
alter content and meaning. Digital image forensics
seeks to authenticate image based on statistical
patterns left on an image by tampering. In [1],
Popescu and Farid introduced a forgery detector by
based on the correlations introduced between pixels
by resampling; that is, the operation of stretching,
shrinking, or rotating an image. Since objects in
images are often on different scales, resampling is
necessary to create a visually convincing forgery.
However, resampling imposes periodic correlations
between pixels that otherwise do not exist. These
correlations can estimated by a learning algorithm.
In order to determine if the image has been
resampled, the found correlations must be passed
to a classiﬁer. The classiﬁer by Popescu and Farid
characterized resampling by constructing a database
of synthetic maps for different resampling ratios.
This has the advantage of being conceptually sim-
ple, but requires an exhaustive database and does not
take into account the fact that natural variations can
be learned. In this paper, we show that a supervised
classiﬁer can improve the detector’s accuracy and
allow it to work in a much more general setting.
The paper is organized as follows. Section II
describes the basis of correlations introduced by
resampling and the EM procedure used to determine
the correlation between pixels. Section III presents

a supervised learning approach to classiﬁcation. Fi-
nally, Section IV presents the results of our classiﬁer
and compares it with the performance by Popescu
and Farid’s classifer.

I I . D ET EC T ING R E SAM PL ING CORR E LAT ION S

A. Resampling
Resampling causes certain pixels be a linear com-
bination of its neighbors. These pixels are correlated
with its neighbors and will appear periodically in
the resampled image. For each pixel, we deﬁne its
neighbors to be all pixels within a window of length
2N + 1.
Resampling by a factor p/q can be represented
by the linear equation (cid:126)y = Ap/q (cid:126)x, where p and q
are integers, (cid:126)y the resampled signal, (cid:126)x the original
signal, and Ap/q ∈ Rn×m the resampling matrix.
For example, consider the case of p/q = 2. The

 .
resampling matrix is given by
0
1
0
0
0.5 0.5
1
0
0
0.5 0.5
0
0
0
1
...
It can be seen that the odd samples do not change
while the even samples are linearly dependent on
their neighbors. We have

A2/1 =

. . .

. . .

y2i−1 = xi

y2i = 0.5xi + 0.5xi+1

for i = 1, . . . , m. This implies that

y2i−1 = 0.5y2i−1 + 0.5y2i+1 ,

(1)

Submitted November 17, 2011 for CS229 Machine Learning, Prof.
Andrew Ng, Stanford University.

so, for this case, every even sample is exactly the
same linear combination of its neighbors.

2

Fig. 1.
Image, probability map, and periodicity map for (a) upsampling, (b) downsampling, and (c) rotation. The classiﬁer must be able to
distinguish between natural peaks in the image and those introduced by resampling.

yi =

αk yi+k

(2)

To generalize this to a signal resampled by an
N(cid:88)
arbitrary p/q it is necessary to determine when
k=−N
where αk is the set of weights across the neighbor-
hood.
Let (cid:126)ai be the ith row of the resampling matrix
N(cid:88)
Ap/q . It can be shown that (2) holds when
k=−N
That is, a resampled pixel yi is correlated with
its neighbors whenever its corresponding row of
the resampling matrix ai can be written as a linear
combination of its neighboring rows [1][2]. This
will lead to periodic correlations between resampled
pixels since the resampling matrix is periodic.

αk(cid:126)ai+k .

(cid:126)ai =

(3)

B. Expectation-Maximization
the
is known,
If the resampling matrix Ap/q
problem is simply that of ﬁnding a set of weights (cid:126)α
that satisﬁes (3) for a subset of rows. However, in
practice, the sampling factor p/q and interpolation
method are not known. Popescu and Farid de-
scribe an expectation-maximization (EM) algorithm
to learn the weights (cid:126)α.

Let M1 be the set of samples that are corre-
lated to their neighbors and M2 the set of samples
not correlated to their neighbors. EM is used to
simultaneously estimate (cid:126)α and a soft assignment
Pr{yi ∈ M1} for each pixel yi since neither is
known. The output of the EM algorithm is a set
of coefﬁcients (cid:126)α and a probability map P . The
complete algorithm is given in Appendix A.
For faster computation of the probability map,
the EM algorithm can be applied to only a random
subset of the blocks in the image to train the weights
(cid:126)α. Once the weights are weights are found, P can
be quickly computed everywhere. We found that this
does not signiﬁcantly change the probability maps.
Random block sampling was used throughout our
experiments.

C. Periodicity
Neighboring pixels can be naturally correlated
based on the statistics of the natural image. How-
ever, it is unlikely that these correlations are peri-
odic. To detect periodicity, Fourier transform of the
probability map is taken. We refer to the magni-
tude of the transform as the periodicity map. The
periodicity map is typically high-pass ﬁltered in
order to suppress the natural DC peak. In [1], a
peak detection algorithm is also described in order
to enhance the peaks and suppress energy due to
natural correlations in the image.

0%5%10%15%0%5%10%15%0o5o10o15o(a)(b)(c)I I I . SU PERV I S ED L EARN ING
Given a periodicity map, we want to determine if
the image has been resampled or non-resampled. In
[1], an exhaustive database of synthetic maps was
generated for various resampling ratios. A periodic-
ity map was then classiﬁed by k -nearest neighbors
(kNN) with k = 1 for a similarity score. While this
yields reasonable accuracy, the kNN classiﬁer does
not work in a general setting with arbitrary block
sizes and resampling ratios.
We train a support vector machine (SVM) to
classify a periodicity map as resampled or non-
resampled. Figure 1 shows the probability and pe-
riodicity map for different images and resampling
ratios. There are distinct peaks that appear when an
image has been resampled. The classiﬁer must be
able to distinguish between natural peaks in the pe-
riodicity map from those introduced by resampling.
We extract
the following 5 features from the
periodicity map:
1) n largest coefﬁcients. Higher peaks tend to
indicate resampling. The coefﬁcients ci are
sorted by magnitude and aggregated to a sin-
n(cid:88)
m(cid:112)|ci |.
gle feature
i=1
2) n largest coefﬁcients after peak detection.
The peak detection algorithm used in [1] is
applied to the periodicity map. The sorted
n(cid:88)
m(cid:112)|c(cid:48)
coefﬁcients c(cid:48)
i are aggregated by
i |.
i=1
3) n largest coefﬁcient distance from center.
Peaks in natural
images tend to be con-
centrated in low frequencies. The positions
(cid:113)
n(cid:88)
(ui , vi ) are used to compute
i + v 2
u2
i .
i=1
4) n largest coefﬁcient to local energy ratio.
The ratios ri are computed for a local rect-
(cid:32) n(cid:88)
(cid:33)m
angular window of width W and aggregated
by
i=1

100 ri

f2 =

/100.

(7)

f3 =

(4)

(5)

(6)

f1 =

f4 =

3

TABLE I
TRA IN ING R E SAM PL ING RAT E S .

Start

Step Size

Upsampling ratio
Downsampling ratio
Rotation (◦ )

1.05
0.5
1

0.03
0.03
1

End

1.5
0.95
45

5) Peak to total energy ratio. The ratio R was
scaled by

f5 = (104R)m/102 .
Since all the features are non-negative, they are
scaled to [0, 1]. We selected n = 4 coefﬁcients and
a scaling factor of m = 2. LIBSVM was used with
the radial basis kernel [3].

(8)

A. Experiment Setup
For the test images, we use the uncompressed
Kodak PhotoCD set of 24 images. These images
are not compressed and are true color; that is, free
of demosaicking, which can also introduce linear
correlations [4]. For simplicity, we extract the green
color channel from the image to obtain a grayscale
test image. We used a 128× 128 window size, which
is signiﬁcantly smaller than 512 × 512 used in [1].
Smaller block sizes can better localize tampering,
but result in peaks that are less distinct.
For each resampling operation, we ﬁrst randomly
select an image on which to apply resampling
and then randomly select a window within the
resampled image. The resampling ratios used in
training are shown in Table I. In order to obtain non-
resampled samples, the images are divided into non-
overlapping blocks. We used 80% of the images for
training and 20% for testing. Since false positives
are highly undesirable in a forensic setting,
the
parameters of the classiﬁer are set to have no false
positives over the training set.
We also implemented the kNN classiﬁer in [1]
for comparison. The database consists of synthetic
maps using the same resampling ratios in Table I.

IV. R E SU LT S
A. Classiﬁcation Results
The overall recall and precision rates for our clas-
siﬁer are shown in Table II. There is only one false
positive (< 1%) over the test set. The performance
of the SVM classiﬁer is compared to the kNN

4

TABLE II
R ECA LL AND PREC I S ION RAT E FOR SVM C LA S S I FIER .

Non-resampled Resampled

Recall
(%)

Precision
(%)

119/120
99.16%

119/144
82.639%

369/394
93.66%

369/370
99.730%

classiﬁer over a wide range of resampling ratios in
Figure 2. The threshold of the synthetic classiﬁer
was set
to have a similar < 1% false positive
rate. Our classiﬁer clearly outperforms kNN at low
resampling rates, which compared to results in [1]
suffers from the smaller block. It has comparable
accuracy to kNN at large resampling ratios and can
detect some highly downsampled images where the
kNN classiﬁer has zero recall.
Figure 3 shows the training set features projected
onto the ﬁrst two principal components. The up-
sampled and rotated data are clearly separable from
the non-resampled data, but the down-sampling data
appears to be clustered. This is due to fact that
downsampling peaks in the periodicity map are
indistinct and can be buried in the natural image
correlations. Interestingly, it also appears that up-
sampled and rotated images can be distinguished
from one another.

B. Detection Example
Figure 4 shows how the SVM classiﬁer could be
used to perform tamper detection in a more general
setting. A beach scene contains an image of Prof.
Andrew Ng of Stanford University that has been
upsampled and inserted into the foreground. Several
regions in the image are selected over which the
periodicity maps are computed. The features are
extracted from the periodicity maps and passed to
our classiﬁer, which determines if the region has
been resampled. The classiﬁer successfully distin-
guishes the resampled and non-resampled regions.
If not computationally prohibitive, a sliding window
could be used to perform detection at every location
in the image to enable automatic detection without
user input.

V. CONC LU S ION
In this project,
the Popescu and Farid’s EM
algorithm for learning correlations between pixels

Fig. 2.
Recall rate for the SVM classiﬁer (black solid) and the
kNN (dashed gray) classiﬁer. The false positive rate for both cases
are < 1%.

Fig. 3. Training set features projected onto the ﬁrst two principal
components γ1 and γ2 . The upsampled and rotated data is clearly
separable from the non-resampled data even in low dimension, but
downsampled data is difﬁcult to separate.

01020304050607080020406080100Upsampling[%]%051015202530354045020406080100Downsampling[%]%051015202530354045020406080100Rotation[◦]%−0.500.51−0.2−0.100.10.20.30.40.50.60.7γ1γ2  Non−resampledUpsampledDownsampledRotated5

Fig. 4.
(a) Tampered image and (b) probability map and periodicity map for four blocks. The classiﬁer correctly determines that the red
block has been resampled and the remaining blocks are not resampled.

matrix with diagonal elements equal to Pr{yi |yi ∈
M1}.
The EM steps are iterated until ||(cid:126)α(i)−(cid:126)α(i−1) || < 
where (cid:126)α(i) is the set of weights obtained from the
ith iteration.

R E F ER ENC E S
[1] A. Popescu and H. Farid, “Exposing digital forgeries by detecting
traces of resampling,” Signal Processing, IEEE Transactions on,
vol. 53, no. 2, pp. 758 – 767, feb. 2005.
[2] M. Kirchner, “Fast and reliable resampling detection by spectral
analysis of ﬁxed linear predictor residue,” in Proceedings of the
10th ACM Workshop on Multimedia and Security, ser. Sec ’08.
New York, NY, USA: ACM, 2008, pp. 11–20.
[3] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support
vector machines,” ACM Transactions on Intelligent Systems and
Technology, vol. 2, pp. 27:1–27:27, 2011, software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
[4] A. Popescu and H. Farid, “Exposing digital forgeries in color
ﬁlter array interpolated images,” Signal Processing, IEEE Trans-
actions on, vol. 53, no. 10, pp. 3948 – 3959, oct. 2005.

was implemented and tested for a smaller block
size. An SVM classiﬁer was trained to determine if
the correlations found by the EM algorithm result
from resampling. This classiﬁer was shown to have
better performance than the kNN classiﬁer at low
resampling rates and does not require and exhaustive
database of synthetic maps.

A P P END IX A
The expectation step consists of assigning
(cid:80)2
Pr{yi |yi ∈ M1}Pr{yi ∈ M1}
Pr{yi ∈ M1 |yi} =
k=1 Pr{yi |yi ∈ Mk }
(9)
where Pr{yi ∈ M1} = Pr{yi ∈ M2} = 1/2.
The model M1 is assumed be normally distributed
(cid:32)
(cid:33)
− (yi − (cid:80)N
Pr{yi |yi ∈ M1} =
k=−N (cid:126)αk yi+k )2
√
1
2σ 2
2π
(10)
and M2 uniformly distributed Pr{yi |yi ∈ M2} =
1/N where N is the range of the pixel values.
The maximization step reduces to ﬁnding the
parameters (cid:126)α by weighted least-squares
(cid:126)α = (Y T W Y )−1Y T W (cid:126)y
(11)
where the rows of Y are the blocks in the image
with the center value removed and W is a diagonal

exp

σ

(a)(b)
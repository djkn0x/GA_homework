Unifying Framework for Fast Learning Rate of
Non-Sparse Multiple Kernel Learning

Taiji Suzuki
Department of Mathematical Informatics
The University of Tokyo
Tokyo 113-8656, Japan
s-taiji@stat.t.u-tokyo.ac.jp

Abstract

In this paper, we give a new generalization error bound of Multiple Kernel Learn-
ing (MKL) for a general class of regularizations. Our main target in this paper is
dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regu-
larization instead of ℓ1 -mixed-norm regularization. According to the recent nu-
merical experiments, the sparse regularization does not necessarily show a good
performance compared with dense type regularizations. Motivated by this fact,
this paper gives a general theoretical tool to derive fast learning rates that is ap-
plicable to arbitrary mixed-norm-type regularizations in a unifying manner. As
a by-product of our general result, we show a fast learning rate of ℓp -MKL that
is tightest among existing bounds. We also show that our general learning rate
achieves the minimax lower bound. Finally, we show that, when the complexities
of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type
regularization shows better learning rate compared with sparse ℓ1 regularization.

1 Introduction

Multiple Kernel Learning (MKL) proposed by [20] is one of the most promising methods that adap-
tively select the kernel function in supervised kernel learning. A kernel method is widely used and
several studies have supported its usefulness [25]. However the performance of kernel methods
critically relies on the choice of the kernel function. Many methods have been proposed to deal
with the issue of kernel selection. [23] studied hyperkrenels as a kernel of kernel functions. [2]
considered DC programming approach to learn a mixture of kernels with continuous parameters.
Some studies tackled a problem to learn non-linear combination of kernels as in [4, 9, 34]. Among
them, learning a linear combination of ﬁnite candidate kernels with non-negative coefﬁcients is the
most basic, fundamental and commonly used approach. The seminal work of MKL by [20] con-
sidered learning convex combination of candidate kernels. This work opened up the sequence of
the MKL studies. [5] showed that MKL can be reformulated as a kernel version of the group lasso
[36]. This formulation gives an insight that MKL can be described as a ℓ1 -mixed-norm regularized
method. As a generalization of MKL, ℓp -MKL that imposes ℓp -mixed-norm regularization has been
proposed [22, 14]. ℓp -MKL includes the original MKL as a special case as ℓ1 -MKL. Another direc-
tion of generalizing MKL is elasticnet-MKL [26, 31] that imposes a mixture of ℓ1 -mixed-norm and
ℓ2 -mixed-norm regularizations. Recently numerical studies have shown that ℓp -MKL with p > 1
and elasticnet-MKL show better performances than ℓ1 -MKL in several situations [14, 8, 31]. An
interesting perception here is that both ℓp -MKL and elasticnet-MKL produce denser estimator than
the original ℓ1 -MKL while they show favorable performances. One motivation of this paper is to
give a theoretical justiﬁcation to these generalized dense type MKL methods in a unifying manner.

1

√
M
In the pioneering paper of [20], a convergence rate of MKL is given as
n , where M is the number
of given kernels and n is the number of samples. [27] gave improved learning bound utilizing the
pseudo-dimension of the given kernel class. [35] gave a convergence bound utilizing Rademacher
chaos and gave some upper bounds of the Rademacher chaos utilizing the pseudo-dimension of the
√
kernel class. [8] presented a convergence bound for a learning method with L2 regularization on the
1− 1
p ∨
for 1 ≤ p ≤ 2. [15]
√
kernel weight. [10] gave the convergence rate of ℓp -MKL as M
log(M )
n
gave a similar convergence bound with improved constants. [16] generalized this bound to a variant
of the elasticnet type regularization and widened the effective range of p to all range of p ≥ 1 while
in the existing bounds 1 ≤ p ≤ 2 was imposed. One concern about these bounds is that all bounds
√
introduced above are “global” bounds in a sense that the bounds are applicable to all candidates of
estimators. Consequently all convergence rate presented above are of order 1/
n with respect to
the number n of samples. However, by utilizing the localization techniques including so-called local
Rademacher complexity [6, 17] and peeling device [32], we can derive a faster learning rate. Instead
of uniformly bounding all candidates of estimators, the localized inequality focuses on a particular
estimator such as empirical risk minimizer, thus can gives a sharp convergence rate.

Localized bounds of MKL have been given mainly in sparse learning settings [18, 21, 19], and there
are only few studies for non-sparse settings in which the sparsity of the ground truth is not assumed.
Recently [13] gave a localized convergence bound of ℓp -MKL. However, their analysis assumed a
strong condition where RKHSs have no-correlation to each other.

In this paper, we show a uniﬁed framework to derive fast convergence rates of MKL with various
regularization types. The framework is applicable to arbitrary mixed-norm regularizations includ-
ing ℓp -MKL and elasticnet-MKL. Our learning rate utilizes the localization technique, thus is tighter
than global type learning rates. Moreover our analysis does not require no-correlation assumption
as in [13]. We apply our general framework to some examples and show our bound achieves the
minimax-optimal rate. As a by-product, we obtain a tighter convergence rate of ℓp -MKL than exist-
ing results. Finally, we show that dense type regularizations can outperforms sparse ℓ1 regularization
when the complexities of the RKHSs are not uniformly same.

2 Preliminary

In this section, we give the problem formulation, the notations and the assumptions required for the
convergence analysis.

2.1 Problem Formulation
Suppose that we are given n i.i.d. samples {(xi , yi )}n
i=1 distributed from a probability distribution
P on X × R where X is an input space. We denote by (cid:5) the marginal distribution of P on X . We
are given M reproducing kernel Hilbert spaces (RKHS) {Hm }M
m=1 each of which is associated with
a kernel km . We consider a mixed-norm type regularization with respect to an arbitrary given norm
∑
∥ψ of the vector (∥fm ∥Hm )M
∥·∥ψ , that is, the regularization is given by the norm ∥(∥fm ∥Hm )M
∥ψ
for fm ∈ Hm (m = 1, . . . , M )
. For notational simplicity, we write ∥f ∥ψ = ∥(∥fm ∥Hm )M
∗
m=1
m=1
∑
m=1 fm (fm ∈ Hm ).
m=1
M
for f =
m=1 fm (fm ∈
(
)2
M
The general formulation of MKL that we consider in this paper ﬁts a function f =
Hm ) to the data by solving the following optimization problem:
yi − M∑
M∑
n∑
fm (xi )
m=1
m=1
i=1
We call this “ ψ -norm MKL”. This formulation covers many practically used MKL methods (e.g.,
ℓp -MKL, elasticnet-MKL, variable sparsity kernel learning (see later for their deﬁnitions)), and is
solvable by a ﬁnite dimensional optimization procedure due to the representer theorem [12]. In this
We assume that the mixed-norm ∥(∥fm ∥Hm )M
m=1 ∥  satisﬁes the triangular inequality with respect to
∗
m=1 , that is, ∥(∥fm + f
m ∥Hm )M
m=1 ∥  ≤ ∥(∥fm ∥Hm )M
m=1 ∥  + ∥(∥f
m ∥Hm )M
m=1 ∥  . To satisfy this
′
′
(fm )M
condition, it is sufﬁcient if the norm is monotone, i.e., ∥a∥  ≤ ∥a + b∥  for all a, b ≥ 0.

arg min
fm∈Hm (m=1,...,M )

^fm =

^f =

+ λ(n)
1

∥f ∥2
ψ .

(1)

1
n

2

paper, we focus on the regression problem (the squared loss). However the discussion presented
∑
here can be generalized to Lipschitz continuous and strongly convex losses [6].
Example 1: ℓp -MKL The ﬁrst motivating example of ψ -norm MKL is ℓp -MKL [14] that employs
ℓp -norm for 1 ≤ p ≤ ∞ as the regularizer: ∥f ∥ψ = ∥(∥fm∥Hm )M
∥ℓp = (
∥fm ∥pHm
1
M
p .
)
m=1
m=1
If p is strictly greater than 1 (p > 1), the solution of ℓp -MKL becomes dense. In particular, p = 2
corresponds to averaging candidate kernels with uniform weight [22]. It is reported that ℓp -MKL
with p greater than 1, say p = 4
3 , often shows better performance than the original sparse ℓ1 -MKL
∑
[10].
∑
Example 2: Elasticnet-MKL The second example is elasticnet-MKL [26, 31] that employs mix-
ture of ℓ1 and ℓ2 norms as the regularizer: ∥f ∥ψ = τ ∥f ∥ℓ1 + (1 − τ )∥f ∥ℓ2 = τ
∥fm ∥Hm +
M
(1 − τ )(
2 with τ ∈ [0, 1]. Elasticnet-MKL shares the same spirit with ℓp -MKL in
∥fm∥2Hm
m=1
M
) 1
m=1
a sense that it bridges sparse ℓ1 -regularization and dense ℓ2 -regularization. An efﬁcient optimization
method for elasticnet-MKL is proposed by [30].
{∑
} 1
∑Mj
Example 3: Variable Sparsity Kernel Learning Variable Sparsity Kernel Learning (VSKL) pro-
groups {Hj,k }Mj
′
′
posed by [1] divides the RKHSs into M
) and imposes a mixed
k=1 , (j = 1, . . . , M
norm regularization ∥f ∥ψ = ∥f ∥(p,q) =
q where 1 ≤ p, q , and
∥fj,k ∥pHj,k
′
q
M
)
j=1 (
p
fj,k ∈ Hj,k . An advantageous point of VSKL is that by adjusting the parameters p and q , various
k=1
levels of sparsity can be introduced, that is, the parameters can control the level of sparsity within
group and between groups. This point is beneﬁcial especially for multi-modal tasks like object
categorization.

∗

(X ) =

2.2 Notations and Assumptions
Here, we prepare notations and assumptions that are used in the analysis. Let H⊕M = H1 ⊕ · · · ⊕
HM . Throughout the paper, we assume the following technical conditions (see also [3]).
∑
Assumption 1. (Basic Assumptions)
M ) ∈ H⊕M such that E[Y |X ] = f
∗
∗
∗
∗
M
(A1) There exists f
m (X ),
1 , . . . , f
m=1 f
= (f
and the noise ϵ := Y − f
(X ) is bounded as |ϵ| ≤ L.
∗
(A2) For each m = 1, . . . , M , Hm is separable (with respect to the RKHS norm) and
supX ∈X |km (X, X )| < 1.
The ﬁrst assumption in (A1) ensures the model H⊕M is correctly speciﬁed, and the technical as-
sumption |ϵ| ≤ L allows ϵf to be Lipschitz continuous with respect to f . The noise boundedness
∫
can be relaxed to unbounded situation as in [24], but we don’t pursue that direction for simplicity.
Let an integral operator Tkm : L2 ((cid:5)) → L2 ((cid:5)) corresponding to a kernel function km be
km (·, x)f (x)d(cid:5)(x).
Tkm f =
It is known that this operator is compact, positive, and self-adjoint (see Theorem 4.27 of [28]). Thus
it has at most countably many non-negative eigenvalues. We denote by µℓ,m be the ℓ-th largest
eigenvalue (with possible multiplicity) of the integral operator Tkm . Then we assume the following
assumption on the decreasing rate of µℓ,m .
Assumption 2. (Spectral Assumption) There exist 0 < sm < 1 and 0 < c such that
µℓ,m ≤ cℓ
(∀ℓ ≥ 1, 1 ≤ ∀m ≤ M ),
− 1
(A3)
sm ,
where {µℓ,m }∞
ℓ=1 is the spectrum of the operator Tkm corresponding to the kernel km .
It was shown that the spectral assumption (A3) is equivalent to the classical covering number as-
sumption [29]. Recall that the ϵ-covering number N (ϵ, BHm , L2 ((cid:5))) with respect to L2 ((cid:5)) is the
minimal number of balls with radius ϵ needed to cover the unit ball BHm in Hm [33]. If the spectral
assumption (A3) holds, there exists a constant C that depends only on s and c such that
log N (ε, BHm , L2 ((cid:5))) ≤ C ε
−2sm ,

3

(2)

Table 1: Summary of the constants we use in this article.
The number of samples.
n
M The number of candidate kernels.
sm The spectral decay coefﬁcient; see (A3).
κM The smallest eigenvalue of the design matrix (see Eq. (3)).

.

(3)

0 < C

κM := sup

, ∀fm ∈ Hm (m = 1, . . . , M )

and the converse is also true (see [29, Theorem 15] and [28] for details). Therefore, if sm is large,
the RKHSs are regarded as “complex”, and if
sm is small, the RKHSs are “simple ”.
An important class of RKHSs where sm is known is Sobolev space. (A3) holds with sm = d
2α for
Sobolev space of α-times continuously differentiability on the Euclidean ball of Rd [11]. Moreover,
for α-times continuously differentiable kernels on a closed Euclidean ball in Rd , that holds for sm =
d
2α [28, Theorem 6.26]. According to Theorem 7.34 of [28], for Gaussian kernels with compact
support distribution, that holds for arbitrary small 0 < sm . The covering number of Gaussian
{
}
kernels with unbounded support distribution is also described in Theorem 7.34 of [28].
(cid:12)(cid:12)(cid:12) κ ≤ ∥ ∑
Let κM be deﬁned as follows:
∑
m=1 fm ∥2
κ ≥ 0
M
L2 ((cid:5))
∥fm ∥2
M
m=1
L2 ((cid:5))
κM represents the correlation of RKHSs. We assume all RKHSs are not completely correlated to
each other.
Assumption 3. (Incoherence Assumption) κM is strictly bounded from below; there exists a con-
stant C0 > 0 such that
−1
(A4)
∑
0 < κM .
This condition is motivated by the incoherence condition [18, 21] considered in sparse MKL settings.
∗
∗
M
m of the ground truth. [3] also
This ensures the uniqueness of the decomposition f
m=1 f
=
assumed this condition to show the consistency of ℓ1 -MKL.
Finally we give a technical assumption with respect to ∞-norm.
Assumption 4. (Embedded Assumption) Under the Spectral Assumption, there exists a constant
C1 > 0 such that
∥fm ∥sm
∥fm ∥∞ ≤ C1 ∥fm ∥1−smHm
(A5)
L2 ((cid:5)) .
This condition is met when the input distribution (cid:5) has a density with respect to the uniform distri-
bution on X that is bounded away from 0 and the RKHSs are continuously embedded in a Sobolev
space W α,2 (X ) where sm = d
2α , d is the dimension of the input space X and α is the “smoothness”
of the Sobolev space. Many practically used kernels satisfy this condition (A5). For example, the
RKHSs of Gaussian kernels can be embedded in all Sobolev spaces. Therefore the condition (A5)
seems rather common and practical. More generally, there is a clear characterization of the condi-
tion (A5) in terms of real interpolation of spaces. One can ﬁnd detailed and formal discussions of
interpolations in [29], and Proposition 2.10 of [7] gives the necessary and sufﬁcient condition for
the assumption (A5).

Constants we use later are summarized in Table 1.

3 Convergence Rate Analysis of   -norm MKL
Here we derive the learning rate of ψ -norm MKL in a most general setting. We suppose that the
number of kernels M can increase along with the number of samples n. The motivation of our
analysis is summarized as follows:
• Give a unifying frame work to derive a sharp convergence rate of ψ -norm MKL.
• (homogeneous complexity) Show the convergence rate of some examples using our general
frame work, and prove its minimax-optimality under conditions that the complexities sm
of all RKHSs are same.

4

r

r

sm r

n

, (4)

• (inhomogeneous complexity) Discuss how the dense type regularization outperforms the
sparse type regularization, when the complexities sm of all RKHSs are not uniformly same.
√
√
n) for t > 0, and, for given positive reals {rm}M
(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(
) 1
(
)M
M∑
Now we deﬁne η(t) := ηn (t) = max(1,
t, t/
m=1
and given n, we deﬁne α1 , α2 , β1 , β2 as follows:
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
)M
(
) 1
(
, α2 := α2 ({rm }) = 3
α1 := α1 ({rm}) = 3
sm r1−sm
−2sm
2
m√
M∑
,
m
n
n
ψ∗
m=1
m=1
(1−sm )2
− 2sm (3−sm )
β1 := β1 ({rm }) = 3
, β2 := β2 ({rm }) = 3
2
1+sm
1+sm
m
m
1
2
1+sm
1+sm
n
m=1
ψ∗
m=1
(note that α1 , α2 , β1 , β2 implicitly depends on the reals {rm }M
m=1 ). Then the following theorem
(
)2
(
)2
gives the general form of the learning rate of ψ -norm MKL.
Theorem 1. Suppose Assumptions 1-4 are satisﬁed. Let {rm }M
m=1 be arbitrary positive reals that
≤ 1
′
that satisfy log(M )√
can depend on n, and assume λ(n)
[(
]
β2
α2
. Then for all n and t
+
1 =
)2
(
)2
)
(
√
α1
β1
n
max{α2
}η(t
) ≤ 1
12 and for all t ≥ 1, we have
′
1 , M log(M )
and 4ϕ
n
1 , β 2
κM
n
∥ ^f − f
≤ 24η(t)2ϕ2
∗∥2
M log(M )
α2
1 + β 2
α2
1 +
L2 ((cid:5))
κM
n
α1
with probability 1 − exp(−t) − exp(−t
′
The proof will be given in Appendix D in the supplementary material. One can also ﬁnd an outline
of the proof in Appendix A in the supplementary material.
The statement of Theorem 1 itself is complicated. Thus we will show later concrete learning rates on
some examples such as ℓp -MKL. The convergence rate (5) depends on the positive reals {rm}M
m=1 ,
(
{
[(
]
})
but the choice of {rm }M
)2
(
)2
m=1 are arbitrary. Thus by minimizing the right hand side of Eq. (5), we
obtain tight convergence bound as follows:
∥ ^f − f
∗∥2
L2 ((cid:5)) = Op
1 + β 2
α2
min
1 +
.
{rm }M
[(
]
(
)2
)2
m=1 :
rm>0
1 + β 2
(a) := α2
1 and the third term (b) :=
There is a trade-off between the ﬁrst two terms
ψ , that is, if we take {rm}m large, then the term (a) becomes small and
∥f
∗∥2
β2
α2
+
β1
α1
the term (b) becomes large, on the other hand, if we take {rm}m small, then it results in large (a)
and small (b). Therefore we need to balance the two terms (a) and (b) to obtain the minimum in
Eq. (6).

M log(M )
n

∗∥2
ψ +

∗∥2
ψ .

α2
α1

β2
β1

β2
β1

+ 4

+

∥f

(5)

).

+

∥f

(6)

We discuss the obtained learning rate in two situations, (i) homogeneous complexity situation, and
(ii) inhomogeneous complexity situation:
(i) (homogeneous) All sm s are same: there exists 0 < s < 1 such that sm = s (∀m) (Sec.3.1).
such that sm ̸= sm′ (Sec.3.2).
′
(ii) (inhomogeneous) All sm s are not same: there exist m, m

3.1 Analysis on Homogeneous Settings

Here we assume all sm s are same, say sm = s for all m (homogeneous setting). If we further restrict
the situation as all rm s are same (rm = r (∀m) for some r ), then the minimization in Eq. (6) can
be easily carried out as in the following lemma. Let 1 be the M -dimensional vector each element of
⊤ ∈ RM , and ∥ · ∥ψ∗ be the dual norm of the ψ -norm
†
which is 1: 1 := (1, . . . , 1)
.
)
(
Lemma 2. When sm = s (∀m) with some 0 < s < 1 and n ≥ (∥1∥ψ∗ ∥f
∗∥ψ /M )
4s
1−s , the bound
(6) indicates that
∥ ^f − f

∗∥ψ )
1+s (∥1∥ψ∗ ∥f
∗∥2
L2 ((cid:5)) = Op
− 1
a | ∥a∥  ≤ 1}.
{b
The dual of the norm ∥ · ∥  is deﬁned as ∥b∥ ∗ := supa
⊤

M log(M )
n

M 1− 2s
1+s n

2s
1+s +

(7)

†

.

5

1+s M 1− 2s
− 1
p(1+s) R

n

.

(9)

.

(8)

2s
1+s
p

The proof is given in Appendix G.1 in the supplementary material. Lemma 2 is derived by assuming
rm = r (∀m), which might make the bound loose. However, when the norm ∥ · ∥ψ is isotropic
(whose deﬁnition will appear later), that restriction ( rm = r (∀m)) does not make the bound loose,
that is, the upper bound obtained in Lemma 2 is tight and achieves the minimax optimal rate (the
minimax optimal rate is the one that cannot be improved by any estimator). In the following, we
investigate the general result of Lemma 2 through some important examples.
∑
Convergence Rate of ℓp -MKL Here we derive the convergence rate of ℓp -MKL (1 ≤ p ≤ ∞)
where ∥f ∥ψ =
m=1 (∥fm ∥pHm
p (for p = ∞, it is deﬁned as maxm ∥fm ∥Hm ). It is well known
(∑
) 1
1
M
)
that the dual norm of ℓp -norm is given as ℓq -norm where q is the real satisfying 1
p + 1
q = 1. For
p . Then substituting ∥f
∗∥ψ = Rp and ∥1∥ψ∗ =
∥f
∥pHm
∗
M
notational simplicity, let Rp :=
)
(
m
m=1
∥1∥ℓq = M
q = M 1− 1
1
p into the bound (7), the learning rate of ℓp -MKL is given as
∗∥2
∥ ^f − f
L2 ((cid:5)) =Op
1+s M 1− 2s
− 1
M log(M )
2s
1+s
p +
n
p(1+s) R
n
)
(
If we further assume n is sufﬁciently large so that n ≥ M
−2
2
1+s
s , the leading term is the
p R
p (log M )
ﬁrst term, and thus we have
∥ ^f − f

∗∥2
L2 ((cid:5)) = Op
Note that as the complexity s of RKHSs becomes small the convergence rate becomes fast. It is
− 1
known that n
1+s is the minimax optimal learning rate for single kernel learning. The derived
rate of ℓp -MKL is obtained by multiplying a coefﬁcient depending on M and Rp to the optimal
rate of single kernel learning. To investigate the dependency of Rp to the learning rate, let us
∥Hm )M
consider two extreme settings, i.e., sparse setting (∥f
∗
m=1 = (1, 0, . . . , 0) and dense setting
∥Hm )M
(∥f
∗
m
m=1 = (1, . . . , 1) as in [15].
m
∥Hm )M
• (∥f
∗
m=1 = (1, 0, . . . , 0): Rp = 1 for all p. Therefore the convergence rate
m
1+s M 1− 2s
− 1
p(1+s) is fast for small p and the minimum is achieved at p = 1. This means
n
that ℓ1 regularization is preferred for sparse truth.
• (∥f
∥Hm )M
− 1
∗
1
m=1 = (1, . . . , 1): Rp = M
p , thus the convergence rate is M n
1+s for all
m
p. Interestingly for dense ground truth, there is no dependency of the convergence rate
on the parameter p (later we will show that this is not the case in inhomogeneous settings
(Sec.3.2)). That is, the convergence rate is M times the optimal learning rate of single
− 1
kernel learning (n
1+s ) for all p. This means that for the dense settings, the complexity of
solving MKL problem is equivalent to that of solving M single kernel learning problems.
∑
∑
Comparison with Existing Bounds Here we compare the bound for ℓp -MKL we derived above
with the existing bounds. Let Hℓp (R) be the ℓp -mixed norm ball with radius R: Hℓp (R) := {f =
p ≤ R}. [10, 16, 15] gave “global” type bounds for
m=1 fm | (
∥fm ∥pHm
R(f ) ≤ bR(f ) + C
1
M
M
ℓp -MKL as
)
√
m=1
where R(f ) and bR(f ) is the population risk and the empirical risk. First observation is that the
1− 1
p ∨
R for all f ∈ Hℓp (R),
√
M
log(M )
(10)
n
bounds by [10] and [15] are restricted to the situation 1 ≤ p ≤ 2. On the other hand, our analysis
and that of [16] covers all p ≥ 1. Second, since our bound is specialized to the regularized risk
minimizer ^f deﬁned at Eq. (1) while the existing bound (10) is applicable to all f ∈ Hℓp (R), our
bound is sharper than theirs for sufﬁciently large n. To see this, suppose n ≥ M
−2
2
p , then we
p R
p(1+s) ≤ n
1+s M 1− 2s
2 M 1− 1
− 1
− 1
have n
p . Moreover we should note that s can be large as long as
Spectral Assumption (A3) is satisﬁed. Thus the bound (10) is formally recovered by our analysis by
})
{
(
approaching s to 1.
Recently [13] gave a tighter convergence rate utilizing the localization technique as ∥ ^f −f
∗∥2
L2 ((cid:5)) =
1+s M 1− 2s
Op
− 1
′
2s
p′ (1+s) R
minp′≥p
p
, under a strong condition κM = 1 that imposes all
p′−1 n
1+s
p′

6

RKHSs are completely uncorrelated to each other. Comparing our bound with their result, there are
′
′
not minp′≥p and p
p′−1 , then the minimum of minp′≥p
p′−1 in our bound (if there is not the term p
′
= p, thus our bound is tighter), moreover our analysis doesn’t need the strong
is attained at p
assumption κM = 1.
( ∥a∥ℓ∞
{
)}
Convergence Rate of Elasticnet-MKL Elasticnet-MKL employs a mixture of ℓ1 and ℓ2 norm as
the regularizer: ∥f ∥ψ = τ ∥f ∥ℓ1 + (1 − τ )∥f ∥ℓ2 where τ ∈ [0, 1]. Then its dual norm is given
∥a−b∥ℓ2
by ∥b∥ψ∗ = mina∈RM
. Therefore by a simple calculation, we have
(
)
1−τ
max
,
√
τ
∥1∥ψ∗ =
√
M
. Hence Eq. (7) gives the convergence rate of elasticnet-MKL as
1−τ +τ
M
L2 ((cid:5)) = Op
∥ ^f − f
∗∥2
(τ ∥f
∗∥ℓ1 + (1 − τ )∥f
∗∥ℓ2 )
1− s
√
M
1+s
(1−τ +τ
M )
Note that, when τ = 0 or τ = 1, this rate is identical to that of ℓ2 -MKL or ℓ1 -MKL obtained in
Eq. (8) respectively.

2s
1+s + M log(M )
n

− 1
1+s

2s
1+s

n

.

3.1.1 Minimax Lower Bound

{
}
(cid:12)(cid:12) ∥f ∥ψ ≤ R
∑
In this section, we show that the derived learning rate (7) achieves the minimax-learning rate on the
ψ -norm ball
Hψ (R) :=
M
,
m=1 fm
f =
when the norm is isotropic. We say the ψ -norm ∥ · ∥ψ is isotropic when there exits a universal
constant (cid:22)c such that
m (∀m)),
′∥ψ (if 0 ≤ bm ≤ b
∥b∥ψ ≤ ∥b
(cid:22)cM = (cid:22)c∥1∥ℓ1
≥ ∥1∥ψ∗ ∥1∥ψ ,
′
(11)
(note that the inverse inequality M ≤ ∥1∥ψ∗ ∥1∥ψ of the ﬁrst condition always holds by the deﬁ-
nition of the dual norm). Practically used regularizations usually satisfy this isotropic property. In
fact, ℓp -MKL, elasticnet-MKL and VSKL satisfy the isotropic property with (cid:22)c = 1.
We derive the minimax learning rate in a simpler situation. First we assume that each RKHS is same
fm (x(1) , . . . , x(M ) ) = ~fm (x(m) ), ~fm ∈ eH} where eH is an RKHS shared by all Hm . Thus
as others. That is, the input vector is decomposed into M components like x = (x(1) , . . . , x(M ) )
∑
m=1 are M i.i.d. copies of a random variable ~X , and Hm = {fm | fm (x) =
where {x(m)}M
a member of the common RKHS eH. We denote by ek the kernel associated with the RKHS eH.
f ∈ H⊕M is decomposed as f (x) = f (x(1) , . . . , x(M ) ) =
~fm (x(m) ) where each ~fm is
M
m=1
In addition to the condition about the upper bound of spectrum (Spectral Assumption (A3)), we
assume that the spectrum of all the RKHSs Hm have the same lower bound of polynomial rate.
′
Assumption 5. (Strong Spectral Assumption) There exist 0 < s < 1 and 0 < c, c
such that
(1 ≤ ∀ℓ),
s ≤ ~µℓ ≤ cℓ
− 1
′
− 1
(A6)
s ,
ℓ
c
where { ~µℓ}∞
ℓ=1 is the spectrum of the integral operator T~k corresponding to the kernel ~k . In partic-
Without loss of generality, we may assume that E[f ( ~X )] = 0 (∀f ∈ eH). Since each fm receives
ular, the spectrum of Tkm also satisﬁes µℓ,m ∼ ℓ
s (∀ℓ, m).
− 1
i.i.d. copy of ~X , Hm s are orthogonal to each other:
) )] = 0 (∀fm ∈ Hm , ∀fm′ ∈ Hm′ , ∀m ̸= m
′
′
E[fm (X )fm′ (X )] = E[ ~fm (X (m) ) ~fm′ (X (m
).
We also assume that the noise {ϵi }n
i=1 is an i.i.d. normal sequence with standard deviation σ > 0.
Under the assumptions described above, we have the following minimax L2 ((cid:5))-error.
]
[
Theorem 3. Suppose R > 0 is given and n > (cid:22)c2M 2
ψ∗ is satisﬁed. Then the minimax-learning rate
R2 ∥1∥2
on Hψ (R) for isotropic norm ∥ · ∥ψ is lower bounded as
1+s (∥1∥ψ∗ R)
≥ CM 1− 2s
∗∥2
∥ ^f − f
− 1
E
max
min
1+s n
f ∗∈Hψ (R)
L2 ((cid:5))
^f
where inf is taken over all measurable functions of n samples {(xi , yi )}n
i=1 .

2s
1+s ,

(12)

7

The proof will be given in Appendix F in the supplementary material. One can see that the con-
vergence rate derived in Eq. (7) achieves the minimax rate on the ψ -norm ball (Theorem 3) up
to M log(M )
that is negligible when the number of samples is large. This means that the ψ -norm
n
regularization is well suited to make the estimator included in the ψ -norm ball.

‡

. In

.

r

(1−s)2
1+s

1

− 2s(3−s)
1+s
1

r

α1 = 3

3.2 Analysis on Inhomogeneous Settings
In the previous section (analysis on homogeneous settings), we have not seen any theoretical justiﬁ-
cation supporting the fact that dense MKL methods like ℓ 4
-MKL can outperform the sparse ℓ1 -MKL
3
[10]. In this section, we show dense type regularizations can outperform the sparse regularization
such that sm ̸= sm′ ). For simplicity, we focus on
′
in inhomogeneous settings (there exists m, m
ℓp -MKL, and discuss the relation between the learning rate and the norm parameter p.
) 1
(
(
) 1
Let us consider an extreme situation where s1 = s for some 0 < s < 1 and sm = 0 (m > 1)
this situation, we have
1 +M −1
, α2 = 3 sr1−s
−2s
2
1√
2
n , β1 = 3
n

+M −1
, β2 = 3 sr
2
1
n
n
1+s
1+s
for all p. Note that these α1 , α2 , β1 and β2 have no dependency on p. Therefore the learning bound
∗∥ℓ∞ ≤ ∥f
(6) is smallest when p = ∞ because ∥f
∗∥ℓp for all 1 ≤ p < ∞. In particular, when
(∥f
∥Hm )M
m=1 = 1, we have ∥f
∗∥ℓ1 = M ∥f
∗∥ℓ∞ and thus obviously the learning rate of ℓ∞ -MKL
∗
m
given by Eq. (6) is faster than that of ℓ1 -MKL. In fact, through a bit cumbersome calculation, one
2s
can check that ℓ∞ -MKL can be M
1+s times faster than ℓ1 -MKL in a worst case. This indicates
that, when the complexities of RKHSs are inhomogeneous, the generalization abilities of dense type
regularizations (e.g., ℓ∞ -MKL) can be better than the sparse type regularization (ℓ1 -MKL). In real
settings, it is likely that one uses various types of kernels and the complexities of RKHSs become
inhomogeneous. As mentioned above, it has been often reported that ℓ1 -MKL is outperformed by
dense type MKL such as ℓ 4
-MKL in numerical experiments [10]. Our theoretical analysis explains
3
well this experimental results.
4 Conclusion
We have shown a uniﬁed framework to derive the learning rate of MKL with arbitrary mixed-norm-
type regularization. To analyze the general result, we considered two situations: homogeneous
settings and inhomogeneous settings. We have seen that the convergence rate of ℓp -MKL obtained in
homogeneous settings is tighter and require less restrictive condition than existing results. We have
also shown the convergence rate of elasticnet-MKL, and proved the derived learning rate is minimax
optimal. Furthermore, we observed that our bound well explains the favorable experimental results
for dense type MKL by considering the inhomogeneous settings. This is the ﬁrst result that strongly
justiﬁes the effectiveness of dense type regularizations in MKL.
Acknowledgement This work was partially supported by MEXT Kakenhi 22700289 and the Ai-
hara Project, the FIRST program from JSPS, initiated by CSTP.

References

[1] J. Aﬂalo, A. Ben-Tal, C. Bhattacharyya, J. S. Nath, and S. Raman. Variable sparsity kernel learning.
Journal of Machine Learning Research, 12:565–592, 2011.
[2] A. Argyriou, R. Hauser, C. A. Micchelli, and M. Pontil. A DC-programming algorithm for kernel selec-
tion. In the 23st ICML, pages 41–48, 2006.
[3] F. R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning
Research, 9:1179–1225, 2008.
[4] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances in
Neural Information Processing Systems 21, pages 105–112, 2009.
[5] F. R. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm.
In the 21st ICML, pages 41–48, 2004.
[6] P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statistics,
33:1487–1537, 2005.
‡
In our assumption sm should be greater than 0. However we formally put sm = 0 (m > 1) for simplicity
of discussion. For rigorous discussion, one might consider arbitrary small sm ≪ s.

8

In the 27th

In

Journal of Mathemati-

[7] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, Boston, 1988.
[8] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In UAI 2009, 2009.
[9] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels. In Advances
in Neural Information Processing Systems 22, pages 396–404, 2009.
[10] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels.
ICML, pages 247–254, 2010.
[11] D. E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential Operators. Cambridge
University Press, Cambridge, 1996.
[12] G. S. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions.
cal Analysis and Applications, 33:82–95, 1971.
[13] M. Kloft and G. Blanchard. The local rademacher complexity of ℓp -norm multiple kernel learning, 2011.
arXiv:1103.0790.
[14] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. M ¨uller, and A. Zien. Efﬁcient and accurate ℓp -norm
multiple kernel learning. In Advances in Neural Information Processing Systems 22, pages 997–1005,
2009.
[15] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. lp -norm multiple kernel learning. Journal of Machine
Learning Research, 12:953–997, 2011.
[16] M. Kloft, U. R ¨uckert, and P. L. Bartlett. A unifying view of multiple kernel learning. In ECML/PKDD,
2010.
[17] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals
of Statistics, 34:2593–2656, 2006.
[18] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In COLT, pages
229–238, 2008.
[19] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics, 38(6):3660–
3695, 2010.
[20] G. Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with
semi-deﬁnite programming.
Journal of Machine Learning Research, 5:27–72, 2004.
[21] L. Meier, S. van de Geer, and P. B ¨uhlmann. High-dimensional additive modeling. The Annals of Statistics,
37(6B):3779–3821, 2009.
[22] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine
Learning Research, 6:1099–1125, 2005.
[23] C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine
Learning Research, 6:1043–1071, 2005.
[24] G. Raskutti, M. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel
classes via convex programming. Technical report, 2010. arXiv:1008.3654.
[25] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[26] J. Shawe-Taylor. Kernel learning for novelty detection. In NIPS 2008 Workshop on Kernel Learning:
Automatic Selection of Optimal Kernels, Whistler, 2008.
[27] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels.
COLT, pages 169–183, 2006.
[28] I. Steinwart. Support Vector Machines. Springer, 2008.
[29] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT,
2009.
[30] T. Suzuki and R. Tomioka. Spicymkl: A fast algorithm for multiple kernel learning with thousands of
kernels. Machine Learning, 85(1):77–108, 2011.
[31] R. Tomioka and T. Suzuki. Sparsity-accuracy trade-off in MKL. In NIPS 2009 Workshop: Understanding
Multiple Kernel Learning Methods, Whistler, 2009.
[32] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
[33] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to
Statistics. Springer, New York, 1996.
[34] M. Varma and B. R. Babu. More generality in efﬁcient multiple kernel learning. In the 26th ICML, pages
1065–1072, 2009.
[35] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In COLT, 2009.
[36] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of The
Royal Statistical Society Series B, 68(1):49–67, 2006.

9


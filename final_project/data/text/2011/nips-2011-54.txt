Sparse recovery by thresholded
non-negative least squares

Martin Slawski and Matthias Hein
Department of Computer Science
Saarland University
Campus E 1.1, Saarbr ¨ucken, Germany
{ms,hein}@cs.uni-saarland.de

Abstract

Non-negative data are commonly encountered in numerous ﬁelds, making non-
negative least squares regression (NNLS) a frequently used tool. At least rela-
tive to its simplicity, it often performs rather well in practice. Serious doubts
about its usefulness arise for modern high-dimensional linear models. Even in
this setting − unlike ﬁrst intuition may suggest − we show that for a broad class
of designs, NNLS is resistant to overﬁtting and works excellently for sparse re-
covery when combined with thresholding, experimentally even outperforming (cid:96)1 -
regularization. Since NNLS also circumvents the delicate choice of a regulariza-
tion parameter, our ﬁndings suggest that NNLS may be the method of choice.

1

Introduction

Consider the linear regression model
y = X β ∗ + ε,
(1)
where y is a vector of observations, X ∈ Rn×p a design matrix, ε a vector of noise and β ∗ a vector
of coefﬁcients to be estimated. Throughout this paper, we are concerned with a high-dimensional
setting in which the number of unknowns p is at least of the same order of magnitude as the number
of observations n, i.e. p = O(n) or even p (cid:29) n, in which case one cannot hope to recover the
target β ∗ if it does not satisfy one of various kinds of sparsity constraints, the simplest being that
β ∗ is supported on S = {j : β ∗
j (cid:54)= 0}, |S | = s < n. In this paper, we additionally assume that
β ∗ is non-negative, i.e. β ∗ ∈ Rp
+ . This constraint is particularly relevant, since non-negative data
occur frequently, e.g. in the form pixel intensity values of an image, time measurements, histograms
or count data, economical quantities such as prices, incomes and growth rates. Non-negativity
constraints emerge in numerous deconvolution and unmixing problems in diverse ﬁelds such as
acoustics [1], astronomical imaging [2], computer vision [3], genomics [4], proteomics [5] and
spectroscopy [6]; see [7] for a survey. Sparse recovery of non-negative signals in a noiseless setting
(ε = 0) has been studied in a series of recent papers [8, 9, 10, 11]. One important ﬁnding of this body
of work is that non-negativity constraints alone may sufﬁce for sparse recovery, without the need to
employ sparsity-promoting (cid:96)1 -regularization as usually. The main contribution of the present paper
is a transfer of this intriguing result to a more realistic noisy setup, contradicting the well-established
paradigm that regularized estimation is necessary to cope with high dimensionality and to prevent
over-adaptation to noise. More speciﬁcally, we study non-negative least squares (NNLS)
1
(cid:107)y − X β (cid:107)2
with minimizer (cid:98)β and its counterpart after hard thresholding (cid:98)β (λ),
min
β(cid:23)0
(cid:40) (cid:98)βj ,
2
n
(cid:98)βj > λ,
(cid:98)βj (λ) =
0,
otherwise, j = 1, . . . , p,

(3)

(2)

1

S by (cid:98)S (λ) = {j : (cid:98)βj (λ) > 0}. Classical work on the problem [12] gives a positive answer for
where λ ≥ 0 is a threshold, and state conditions under which it is possible to infer the support
ﬁxed p, while in case one follows the modern statistical trend, one would add a regularizer to (2) in
order to encourage sparsity: the most popular approach is (cid:96)1 -regularized least squares (lasso, [13]),
which is easy to implement and comes with strong theoretical guarantees with regard to prediction
and estimation of β ∗ in the (cid:96)2 -norm over a broad range of designs (see [14] for a review). On the
other hand, the rather restrictive ’irrepresentable condition’ on the design is essentially necessary in
order to infer the support S from the sparsity pattern of the lasso [15, 16]. In view of its tendency
to assign non-zero weights to elements of the off-support S c = {1, . . . , p} \ S , several researchers,
e.g. [17, 18, 19], suggest to apply hard thresholding to the lasso solution to achieve support recovery.
In light of this, thresholding a non-negative least squares solution, provided it is close to the target
w.r.t. the (cid:96)∞ -norm, is more attractive for at least two reasons: ﬁrst, there is no need to carefully
tune the amount of (cid:96)1 -regularization prior to thresholding; second, one may hope to detect relatively
small non-zero coefﬁcients whose recovery is negatively affected by the bias of (cid:96)1 -regularization.

Outline. We ﬁrst prove a bound on the mean square prediction error of the NNLS estimator,
demonstrating that it may be resistant to overﬁtting. Section 3 contains our main results on sparse
recovery with noise. Experiments providing strong support of our theoretical ﬁndings are presented
in Section 4. Most of the proofs as well as technical deﬁnitions are relegated to the supplement.
Notation. Let J, K be index sets. For a matrix A ∈ Rn×m , AJ denotes the matrix one obtains by
extracting the columns corresponding to J . For j = 1, . . . , m, Aj denotes the j -th column of A.
The matrix AJK is the sub-matrix of A by extracting rows in J and columns in K . For v ∈ Rm , vJ
is the sub-vector corresponding to J . The identity matrix is denoted by I and vectors of ones by 1.
The symbols (cid:22) (≺), (cid:23) ((cid:31)) denote entry-wise (strict) inequalities. Lower and uppercase c’s denote
positive universal constants (not depending on n, p, s) whose values may differ from line to line.

2 Prediction error and uniqueness of the solution

Assumptions. We here ﬁx what is assumed throughout the paper unless stated otherwise. Model
(1) is assumed to hold. The matrix X is assumed to be non-random and scaled s.t. (cid:107)Xj (cid:107)2
2 = n ∀j .
We assume that ε has i.i.d. zero-mean sub-Gaussian entries with parameter σ > 0, cf. supplement.
n (cid:107)X β ∗−X (cid:98)β (cid:107)2
In the following, the quantity of interest is the mean squared prediction error (MSE) 1
2 .
NNLS does not necessarily overﬁt.
It is well-known that the MSE of ordinary least squares (OLS)
as well as that of ridge regression in general does not vanish unless p/n → 0. Can one do better with
let a design matrix (cid:101)X be given and set X = [ (cid:101)X − (cid:101)X ] by concatenating (cid:101)X and − (cid:101)X columnwise.
non-negativity constraints ? Obviously, the answer is negative for general X . To make this clear,
The non-negativity constraint is then vacuous in the sense that X (cid:98)β = X (cid:98)β ols , where (cid:98)β ols is any OLS
solution. However, non-negativity constraints on β can be strong when coupled with the following
n X (cid:62)X .
condition imposed on the Gram matrix Σ = 1
Self-regularizing property. We call a design self-regularizing with universal constant κ ∈ (0, 1] if
β(cid:62)Σβ ≥ κ(1(cid:62)β )2 ∀β (cid:23) 0.
(4)
The term ’self-regularizing’ refers to the fact that the quadratic form in Σ restricted to the non-
negative orthant acts like a regularizer arising from the design itself. Let us consider two examples:
(1) If Σ (cid:23) κ0 > 0, i.e. all entries of the Gram matrix are at least κ0 , then (4) holds with κ = κ0 .
(2) If the Gram matrix is entry-wise non-negative and if the set of predictors indexed by {1, . . . , p}
XBb (cid:23) κ0 , then
n X (cid:62)
B(cid:88)
B(cid:88)
can be partitioned into subsets B1 , . . . , BB such that min1≤b≤B 1
Bb
(1(cid:62)βBb )2 ≥ κ0B (1(cid:62)β )2 .
b=1
b=1
In particular, this applies to design matrices whose entries Xij = φj (ui ) contain the function eval-
uations of non-negative functions {φj }p
j=1 traditionally used for data smoothing such as splines,
Gaussians and related ’localized’ functions at points {ui }n
i=1 in some ﬁxed interval, see Figure 1.

β(cid:62)Σβ ≥

X (cid:62)
Bb

XBb βBb ≥ κ0

min
β(cid:23)0

β(cid:62)
Bb

1
n

2

For self-regularizing designs, the MSE of NNLS can be controlled as follows.
(cid:114) 2 log p
Theorem 1. Let Σ fulﬁll the self-regularizing property with constant κ. Then, with probability no
(cid:107)X β ∗ − X (cid:98)β (cid:107)2
less than 1 - 2/p, the NNLS estimator obeys
1
log p
8σ2
2 ≤ 8σ
(cid:107)β ∗ (cid:107)1 +
.
MSE, which is of the order O((cid:112)log(p)/n (cid:107)β ∗ (cid:107)1 ), may vanish as n → ∞ even if the number of
n
κ
n
n
κ
The statement implies that for self-regularizing designs, NNLS is consistent in the sense that its
predictors p scales up to sub-exponentially in n. It is important to note that exact sparsity of β ∗ is
not needed for Theorem 1 to hold. The rate is the same as for the lasso if no further assumptions on
the design are made, a result that is essentially obtained in the pioneering work [20].

Figure 1: Block partitioning of 15 Gaussians
into B = 5 blocks. The right part shows the
corresponding pattern of the Gram matrix.

Figure 2: A polyhedral cone in R3 and
its intersection with the simplex (right).
The point y is contained in a face (bold)
with normal vector w , whereas y (cid:48) is not.

Uniqueness of the solution. Considerable insight can be gained by looking at the NNLS problem
(2) from the perspective of convex geometry. Denote by C = X Rp
+ the polyhedral cone generated
by the columns {Xj }p
j=1 of X , which are henceforth assumed to be in general position in Rn . As
visualized in Figure 2, sparse recovery by non-negativity constraints can be analyzed by studying the
face lattice of C [9, 10, 11]. For F ⊆ {1, . . . , p}, we say that XF R|F |
+ is a face of C if there exists a
separating hyperplane with normal vector w passing through the origin such that (cid:104)Xj , w(cid:105) > 0, j /∈
F , (cid:104)Xj , w(cid:105) = 0, j ∈ F . Sparse recovery in a noiseless setting (ε = 0) can then be characterized
concisely by the following statement which can essentially be found in prior work [9, 10, 11, 21].
+ is a face of C and the
Proposition 1. Let y = X β ∗ , where β ∗ (cid:23) 0 has support S , |S | = s. If XS Rs
columns of X are in general position in Rn , then the constrained linear system X β = y sb.t. β (cid:23) 0,
has β ∗ as its unique solution.
S + δS ) + XS c δS c = y . Multiplying both sides by w(cid:62) yields (cid:80)
there exists a w ∈ Rn s.t. (cid:104)Xj , w(cid:105) = 0, j ∈
+ is a face of C ,
Proof. By deﬁnition, since XS Rs
(cid:104)Xj , w(cid:105) > 0, j ∈ S c . Assume that there is a second solution β ∗ + δ, δ (cid:54)= 0. Expand
S,
j∈S c (cid:104)Xj , w(cid:105) δj = 0. Since
XS (β ∗
S c = 0, feasibility requires δj ≥ 0, j ∈ S c . All inner products within the sum are positive,
β ∗
concluding that δS c = 0. General position implies δS = 0.
Corollary 1. In the setting of Theorem 1, if (cid:107)β ∗ (cid:107)1 = o((cid:112)n/ log(p)), then the NNLS solution (cid:98)β is
Given Theorem 1 and Proposition 1, we turn to uniqueness in the noisy case.
+ , then X (cid:98)β , the projection of y on C , is contained in its
unique with high probability.
Proof. Suppose ﬁrst that y /∈ C = X Rp
1 implies that (cid:98)β is unique. If y were already contained in C , one would have y = X (cid:98)β and hence
boundary, i.e. in a lower-dimensional face. Using general position of the columns of X , Proposition
(cid:107)X β ∗ − X (cid:98)β (cid:107)2
1
1
1
(cid:107)ε(cid:107)2
(cid:107)X β ∗ − y(cid:107)2
2 =
2 =
2 = O(1), with high probability,
(5)
n (cid:107)X β ∗ − X (cid:98)β (cid:107)2
n
n
n
using concentration of measure of the norm of the sub-Gaussian random vector ε. With the assumed
scaling for (cid:107)β ∗ (cid:107)1 , 1
2 = o(1) in view of Theorem 1, which contradicts (5).

3

f1f15B1B2B3B4B5yy'w(6)

(8)

(9)

3 Sparse recovery in the presence of noise
Proposition 1 states that support recovery requires XS Rs
+ to be a face of X Rp
+ , which is equivalent
+ from the rest of C . For the noisy case, mere
to the existence of a hyperplane separating XS Rs
separation is not enough − a quantiﬁcation is needed, which is provided by the following two inco-
herence constants that are of central importance for our main result. Both are speciﬁc to NNLS and
have not been used previously in the literature on sparse recovery.
(cid:98)τ (S ) = max
Deﬁnition 1. For some ﬁxed S ⊂ {1, . . . , p}, the separating hyperplane constant is deﬁned as
τ
τ ,w
1√
1√
S c w (cid:23) τ 1, (cid:107)w(cid:107)2 ≤ 1,
X (cid:62)
X (cid:62)
S w = 0,
sb.t.
n
n
1√
(cid:107)XS θ − XS c λ(cid:107)2 ,
duality=
min
(7)
θ∈Rs , λ∈T p−s−1
where T m−1 = {v ∈ Rm : v (cid:23) 0, 1(cid:62) v = 1} denotes the simplex in Rm , i.e. (cid:98)τ (S ) equals the
n
distance of the subspace spanned by {Xj }j∈S and the convex hull of {Xj }j∈S c .
S the orthogonal projections on the subspace spanned by {Xj }j∈S and its
We denote by ΠS and Π⊥
orthogonal complement, respectively, and set Z = Π⊥
(cid:98)τ 2 (S ) = min
S XS c . One can equivalently express (7) as
λ(cid:62) 1
Z (cid:62)Z λ.
λ∈T p−s−1
n
The second incoherence constant we need can be traced back to the KKT optimality conditions of
S XS c , (cid:98)ω(S ) is deﬁned as
(cid:13)(cid:13)(cid:13)∞ , V (F ) = {v ∈ R|F | : (cid:107)v(cid:107)∞ = 1, v (cid:23) 0}.
(cid:13)(cid:13)(cid:13) 1
the NNLS problem. The role of the following quantity is best understood from (13) below.
Deﬁnition 2. For some ﬁxed S ⊂ {1, . . . , p} and Z = Π⊥
(cid:98)ω(S ) =
Z (cid:62)
min
min
F ZF v
In the supplement, we show that i) (cid:98)ω(S ) > 0 ⇔ (cid:98)τ (S ) > 0 ⇔ XS Rs
v∈V (F )
∅(cid:54)=F ⊆{1,...,p−s}
n
(cid:98)ω(S ) ≤ 1, with equality if {Xj }j∈S and {Xj }j∈S c are orthogonal and 1
+ is a face of C , and ii)
n X (cid:62)
S c XS c is entry-wise non-
(cid:80)
n X (cid:62)X by σjk , 1 ≤ j, k ≤ p, our main result additionally
negative. Denoting the entries of Σ = 1
SS v(cid:13)(cid:13)∞ , φmin (S ) = minv : (cid:107)v(cid:107)2=1 (cid:107)ΣSS v(cid:107)2 .
(cid:13)(cid:13)Σ−1
involves the constants
µ(S ) = maxj∈S maxk∈S c |σjk |,
k∈S c |σjk |, βmin (S ) = minj∈S β ∗
µ+ (S ) = maxj∈S
j ,
K (S ) = maxv : (cid:107)v(cid:107)∞=1
Theorem 2. Consider the thresholded NNLS estimator (cid:98)β (λ) deﬁned in (3) with support (cid:98)S (λ).
(cid:113) 2 log p
(cid:114) 2 log p
(i) If λ > 2σbτ 2 (S )
and
βmin (S ) > (cid:101)λ, (cid:101)λ = λ(1 + K (S )µ(S )) +
n
(cid:113) 2 log p
,
n
(cid:114) 2 log p
(ii) or if λ > 2σbω(S )
and
βmin (S ) > (cid:101)λ, (cid:101)λ = λ(1 + K (S )µ+ (S )) +
n
2σ
then (cid:107) (cid:98)β (λ) − β ∗ (cid:107)∞ ≤ (cid:101)λ and (cid:98)S (λ) = S with probability no less than 1 − 10/p.
{φmin (S )}1/2
n
Remark. The concept of a separating functional as in (6) is also used to show support recovery for
the lasso [15, 16] as well as for orthogonal matching pursuit [22, 23]. The ’irrepresentable condition’
employed in these works requires the existence of a separation constant γ (S ) > 0 such that
hence {Xj }j∈S and {Xj }j∈S c are separated by the functional | (cid:10)·, XS (X (cid:62)
S )(cid:11) |.
|X (cid:62)
S )| ≤ 1−γ (S ), while |X (cid:62)
S )| = 1, j ∈ S,
j XS (X (cid:62)
S XS )−1 sign(β ∗
j XS (X (cid:62)
S XS )−1 sign(β ∗
max
j∈S c
S XS )−1 sign(β ∗
In order to prove Theorem 2, we need two lemmas ﬁrst. The ﬁrst one is immediate from the
KKT optimality conditions of the NNLS problem.

2σ
{φmin (S )}1/2

(10)

,

4

|Z (cid:62)
j ξ |.
(11)

Lemma 1. (cid:98)β is a minimizer of (2) if and only if there exists F ⊆ {1, . . . , p} such that
j (y − X (cid:98)β ) = 0, and (cid:98)βj > 0, j ∈ F ,
j (y − X (cid:98)β ) ≤ 0, and (cid:98)βj = 0, j ∈ F c .
1
1
X (cid:62)
X (cid:62)
The next lemma is crucial, since it permits us to decouple (cid:98)βS from (cid:98)βS c .
n
n
(cid:107)ΠS y − XS β (P 2) − ΠS XS c (cid:98)β (P 1) (cid:107)2
Lemma 2. Consider the two non-negative least squares problems
1
1
S (ε − XS c β (P 1) )(cid:107)2
(cid:107)Π⊥
(P 2) : min
(P 1) : min
with minimizers (cid:98)β (P 1) of (P 1) and (cid:98)β (P 2) of (P 2), respectively. If (cid:98)β (P 2) (cid:31) 0, then setting (cid:98)βS =
β (P 2)(cid:23)0
β (P 1)(cid:23)0
2
2
n
n
(cid:98)β (P 2) and (cid:98)βS c = (cid:98)β (P 1) yields a minimizer (cid:98)β of the non-negative least squares problem (2).
Proof of Theorem 2. The proofs of parts (i) and (ii) overlap to a large extent. Steps speciﬁc to one of
Step 1: Controlling (cid:107) (cid:98)β (P 1)(cid:107)1 via (cid:98)τ 2 (S ), controlling (cid:107) (cid:98)β (P 1) (cid:107)∞ via (cid:98)ω(S ).
the two parts are preceded by ’(i)’ or ’(ii)’. Consider problem (P 1) of Lemma 2.
S ε, since (cid:98)β (P 1) is a minimizer, it satisﬁes
(i) With ξ = Π⊥
Z (cid:62)Z (cid:98)β (P 1) ≤ (cid:107) (cid:98)β (P 1) (cid:107)1M , M = max
2 ⇒ ( (cid:98)β (P 1) )(cid:62) 1
(cid:107)ξ − Z (cid:98)β (P 1) (cid:107)2
2
2 ≤ 1
1
(cid:107)ξ(cid:107)2
1≤j≤(p−s)
As observed in (8), (cid:98)τ 2 (S ) = minλ∈T p−s−1 λ(cid:62) 1
n
n
n
n
(cid:26)
(cid:27)
n Z (cid:62)Z λ, s.t. the l.h.s. can be lower bounded via
1 = (cid:98)τ 2 (S )(cid:107) (cid:98)β (P 1)(cid:107)2
(cid:107) (cid:98)β (P 1) (cid:107)2
Z (cid:62)Z (cid:98)β (P 1) ≥
( (cid:98)β (P 1) )(cid:62) 1
λ(cid:62) 1
Z (cid:62)Z λ
min
1 .
Combining (11) and (12), we have (cid:107) (cid:98)β (P 1)(cid:107)1 ≤ 1bτ 2 (S ) M .
λ∈T p−s−1
n
n
(cid:98)β (P 1) = 0) such that (cid:98)β (P 1)
(ii) In view of Lemma 1, there exists a set F ⊆ {1, . . . , p − s} (we may assume F (cid:54)= ∅, otherwise
F ξ , ⇒ (cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) 2
(cid:13)(cid:13)(cid:13)∞
F ZF (cid:98)β (P 1)
F ZF (cid:98)β (P 1)
F c = 0 and such that
(cid:13)(cid:13)(cid:13)∞ , V (F ) = {v ∈ R|F | : (cid:107)v(cid:107)∞ = 1, v (cid:23) 0}
(cid:107) (cid:98)β (P 1)(cid:107)∞ ≤ (cid:13)(cid:13)(cid:13) 2
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) 1
1
2
Z (cid:62)
Z (cid:62)
Z (cid:62)
Z (cid:62)
=
F =
F ξ
F
n
n
n
n
(cid:107) (cid:98)β (P 1) (cid:107)∞ ≤ (cid:13)(cid:13)(cid:13) 2
(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)∞
⇒ min
Z (cid:62) ξ
Z (cid:62)
⇒ (cid:98)ω(S )(cid:107) (cid:98)β (P 1) (cid:107)∞ =
F ZF v
v∈V (F )
n
n
Z (cid:62) ξ
= M ,
min
min
ZF ZF v
∅(cid:54)=F ⊆{1,...,p−s}
v∈V (F )
n
n
where we have used Deﬁnition 2. We conclude that (cid:107) (cid:98)β (P 1)(cid:107)∞ ≤ Mbω(S ) .
(13)
Step 2: Back-substitution into (P2). Equipped with the bounds just derived, we insert (cid:98)β (P 1) into
problem (P 2) of Lemma 2, and show that in conjunction with the assumptions made for the mini-
(cid:107)ΠS y − XS β (P 2) − ΠS XS c (cid:98)β (P 1)(cid:107)2
mum support coefﬁcient βmin (S ), the ordinary least squares estimator corresponding to (P 2)
1
¯β (P 2) = argmin
has only positive components. Lemma 2 then yields ¯β (P 2) = (cid:98)β (P 2) = (cid:98)βS . Using the closed form
2
n
β (P 2)
S + ΠS ε − ΠS XS c (cid:98)β (P 1) ) = β ∗
SS ΣSS c (cid:98)β (P 1) .
expression for the ordinary least squares estimator, one obtains
1
1
S ε − Σ−1
Σ−1
Σ−1
SS X (cid:62)
S (XS β ∗
SS X (cid:62)
¯β (P 2) =
SS ΣSS c (cid:98)β (P 1) (cid:107)∞ . We have
S +
n
n
(cid:40)
S ε(cid:107)∞ and (cid:107)Σ−1
It remains to control the deviation terms M = (cid:107) 1
n Σ−1
SS X (cid:62)
µ(S )(cid:107) (cid:98)β (P 1)(cid:107)1
SS v(cid:107)∞(cid:107)ΣSS c (cid:98)β (P 1) (cid:107)∞
SS ΣSS c (cid:98)β (P 1) (cid:107)∞ ≤ max
µ+ (S )(cid:107) (cid:98)β (P 1)(cid:107)∞ for (ii).
for (i),
(10)≤ K (S )·
(cid:107)Σ−1
(cid:107)Σ−1
v : (cid:107)v(cid:107)∞=1
(14)
Step 3: Putting together the pieces. The two random terms M and M are maxima of a ﬁnite collec-
tion of sub-Gaussian random variables, which can be controlled using standard techniques. Since

(12)

5

√
n(cid:107)2 ≤ {φmin (S )}−1/2 for all j , the sub-Gaussian parameters
(cid:107)Zj (cid:107)2 ≤ (cid:107)Xj (cid:107)2 and (cid:107)e(cid:62)
j Σ−1
SS X (cid:62)
(cid:113) 2 log p
(cid:113) 2 log p
√
n and σ/({φmin (S )}1/2√
S /
n), respectively. It follows
of these collections are upper bounded by σ/
that the two events {M ≤ 2σ
n } and {M ≤
n } both hold with probability
2σ
{φmin (S )}1/2
no less than 1 − 10/p, cf. supplement. Subsequently, we work conditional on these two events. For
(cid:114) 2 log p
(cid:26)µ(S )
the choice of λ made for (i) and (ii), respectively, it follows that
2σ
for (i),
(cid:107)β ∗ − ¯β (P 2)(cid:107)∞ ≤
+ λK (S ) ·
{φmin (S )}1/2
and hence, using the lower bound on βmin (S ), that ¯β (P 2) = (cid:98)βS (cid:31) 0 and thus also that (cid:98)β (P 1) = (cid:98)βS c .
µ+ (S )
for (ii),
n
Subsequent thresholding with the respective choices made for λ yields the assertion. 2
literature, for which thresholded NNLS achieves an (cid:96)∞ -error of the optimal order O((cid:112)log(p)/n).
In the sequel, we apply Theorem 2 to speciﬁc classes of designs commonly studied in the
We here only provide sketches, detailed derivations are relegated to the supplement.
Example 1: Power decay. Let the entries of the Gram matrix Σ be given by σjk = ρ|j−k| , 1 ≤
j, k ≤ p, 0 ≤ ρ < 1, so that the {Xj }p
j=1 form a Markov random ﬁeld in which Xj is conditionally
structure implies that all entries of Z (cid:62)Z are non-negative, such that, using the deﬁnition of (cid:98)ω(S ),
independent of {Xk }k /∈{j−1,j,j+1} given {Xj−1 , Xj+1}, cf. [24]. The conditional independence
(cid:12)(cid:12)(cid:12) = min
(cid:12)(cid:12)(cid:12) 1
(cid:88)
(cid:98)ω(S ) ≥ min
1
1
min{(Z (cid:62)Z )jk , 0},
(Z (cid:62)Z )j j +
Z (cid:62)
min
j Z v
1≤j≤p−s
v(cid:23)0,(cid:107)v(cid:107)∞=1
1≤j≤(p−s)
n
n
n
the sum on the r.h.s. vanishes, thus one computes (cid:98)ω(S ) ≥ min1≤j≤(p−s)
k (cid:54)=j
n (Z (cid:62)Z )j j ≥ 1 − 2ρ2
1
1+ρ2
for all S . For the remaining constants in (10), one can show that Σ−1
SS is a band matrix of bandwidth
no more than 3 for all choices of S such that φmin (S ) and K (S ) are uniformly lower and upper
bounded, respectively, by constants depending on ρ only. By the geometric series formula, µ+ (S ) ≤
(cid:107) (cid:98)β (λ) − β ∗ (cid:107)∞ ≤ Cρσ(cid:112)2 log(p)/n.
1−ρ . In total, for a constant Cρ > 0 depending on ρ only, one obtains an (cid:96)∞ -error of the form
ρ
(15)
Example 2: Equi-correlation. Suppose that σjk = ρ, 0 < ρ < 1, for all j (cid:54)= k , and σj j = 1 for
n Z (cid:62)Z is of the same regular structure with diagonal
all j . For any S , one computes that the matrix 1
entries all equal to 1 − δ and off-diagonal entries all equal to ρ − δ , where δ = ρ2 s/(1 + (s − 1)ρ).
Therefore, using (8), the separating hyperplane constant (7) can be computed in closed form:
(cid:98)τ 2 (S ) =
(1 − ρ)ρ
1 − ρ
= O(s−1 ).
+
(s − 1)ρ + 1
p − s
≤ ((s − 1)ρ + 1)2σ(cid:112)2 log(p)/n
(cid:107) (cid:98)βS c (cid:107)1 ≤ 2σ(cid:112)2 log(p)/n
Arguing as in (12) in the proof of Theorem 2, this allows one to show that with high probability,
(cid:98)τ 2 (S )
(17)
(1 − ρ)ρ
.
On the other hand, using the same reasoning as in Example 1, (cid:98)ω(S ) ≥ 1 − δ = cρ > 0, say.
(cid:113) 2 log p
Choosing the threshold λ = 2σbω(S )
as in part (ii) of Theorem 2 and combining the strong
n
(cid:96)1 -bound (17) on the off-support coefﬁcients with a slight modiﬁcation of the bound (14) together
with φmin (S ) = 1 − ρ yields again the desired optimal bound of the form (15).

(16)

Random designs. So far, the design matrix X has been assumed to be ﬁxed. Consider the follow-
ing ensemble of random matrices
Ens+ = {X = (xij ), {xij , 1 ≤ i ≤ n, 1 ≤ j ≤ p} i.i.d. from a sub-Gaussian distribution on R+}.
Among others, the class of sub-Gaussian distributions on R+ encompasses all distributions on a
bounded set on R+ , e.g. the family of beta distributions (with the uniform distribution as spe-
cial case) on [0, 1], Bernoulli distributions on {0, 1} or more generally distributions on counts

6

{0, 1, . . . , K }, for some positive integer K . The ensemble Ens+ is well amenable to analysis,
since after suitable re-scaling the corresponding population Gram matrix Σ∗ = E[ 1
n X (cid:62)X ] has
√
equi-correlation structure (Example 2): denoting the mean of the entries and their squares by µ and
correlation with ρ = µ2 /µ2 . As shown above, the incoherence constant (cid:98)τ 2 (S ), which gives rise to a
µ2 , respectively, we have Σ∗ = (µ2 − µ2 )I + µ211(cid:62) such that re-scaling by 1/
µ2 leads to equi-
strong bound on (cid:107) (cid:98)βS c (cid:107)1 , scales favourably and can be computed in closed form. For random designs
from random matrix theory, we show that the deviation is moderate, of the order O((cid:112)log(p)/n).
from Ens+ , one additionally has to take into account the deviation between Σ and Σ∗ . Using tools
n X (cid:62)X (cid:3) = ρI + (1 − ρ)11(cid:62) for
Theorem 3. Let X be a random matrix from Ens+ , scaled s.t. E (cid:2) 1
some ρ ∈ (0, 1). Fix an S ⊂ {1, . . . , p}, |S | ≤ s. Then there exists constants c, c1 , c2 , c3 , C, C (cid:48) > 0
such that for all n ≥ C log(p)s2 , (cid:98)τ 2 (S ) ≥ cs−1 − C (cid:48)(cid:112)log(p)/n
with probability no less than 1 − 3/p − exp(−c1n) − 2 exp(−c2 log p) − exp(−c3 log1/2 (p)s).

4 Experiments
Setup. We randomly generate data y = X β ∗ + ε, where ε has i.i.d. standard Gaussian entries. We
consider two choices for the design X . For one set of experiments, the rows of X are drawn i.i.d.
from a Gaussian distribution whose covariance matrix has the power decay structure of Example 1
with parameter ρ = 0.7. For the second set, we pick a representative of the class Ens+ by drawing
j = b · βmin (S )(1 + Uj ), j ∈ S , where βmin (S ) = Cρσ(cid:112)2 log(p)/n,
each entry of X uniformly from [0, 1] and re-scaling s.t. the population Gram matrix Σ∗ has equi-
correlation structure with ρ = 3/4. The target β ∗ is generated by selecting its support S uniformly
at random and then setting β ∗
using upper bounds for the constant Cρ as used for Examples 1 and 2; the {Uj }j∈S are drawn i.i.d.
uniformly from [0, 1], and b is a parameter controlling the signal strength. The experiments can be
divided into two parts. In the ﬁrst part, the parameter b is kept ﬁxed while the aspect ratio p/n of X
and the fraction of sparsity s/n vary. In the second part, s/n is ﬁxed to 0.2, while p/n and b vary.
When not ﬁxed, s/n ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3}. The grid used for b is chosen speciﬁc to
the designs, calibrated such that the sparse recovery problems are sufﬁciently challenging. For the
design from Ens+ , p/n ∈ {2, 3, 5, 10}, whereas for power decay p/n ∈ {1.5, 2, 2.5, 3, 3.5, 4}, for
reasons that become clear from the results. Each conﬁguration is replicated 100 times for n = 500.

Comparison. Across these runs, we compare the probability of ’success’ of thresholded NNLS
(cid:98)β (µ) of minβ(cid:23)0
(tNNLS), non-negative lasso (NN(cid:96)1 ), thresholded non-negative lasso (tNN(cid:96)1 ) and orthogonal match-
ing pursuit (OMP, [22, 23]). For a regularization parameter µ ≥ 0, NN(cid:96)1 is deﬁned as a minimizer
n (cid:107)y − X β (cid:107)2
2 + µ1(cid:62)β . We also compare against the ordinary lasso (replacing 1(cid:62)β
1
by (cid:107)β (cid:107)1 and removing the non-negativity constraint); since its performance is mostly nearly equal,
ability. ’Success’ is deﬁned as follows. For tNNLS, we have ’success’ if minj∈S (cid:98)βj > maxj∈S c (cid:98)βj ,
partially considerably worse than that of its non-negative counterpart (see the bottom right panel of
Figure 4 for an example), the results are not shown in the remaining plots for the sake of better read-
i.e. there exists a threshold that permits support recovery. For NN(cid:96)1 , we set (cid:98)µ = 2(cid:107)X (cid:62) ε/n(cid:107)∞ ,
which is the empirical counterpart to µ0 = 2(cid:112)2 log(p)/n, the choice for the regularization param-
the whole set of solutions { (cid:98)β (µ), µ ≥ (cid:98)µ} using the non-negative lasso modiﬁcation of LARS [26]
eter advocated in [14] to achieve the optimal rate for estimating β ∗ in the (cid:96)2 -norm, and compute
{ (cid:98)β (µ) : µ ∈ [µ0 ∧ (cid:98)µ, µ0 ∨ (cid:98)µ]} and check whether minj∈S (cid:98)βj (µ) > maxj∈S c (cid:98)βj (µ) holds for one
and check whether the sparsity pattern of one of these solutions recovers S . For tNN(cid:96)1 , we inspect
of these solutions. For OMP, we check whether the support S is recovered in the ﬁrst s steps. Note
that, when comparing tNNLS and tNN(cid:96)1 , the lasso is given an advantage, since we optimize over a
range of solutions.
Remark: We have circumvented the choice of the threshold λ, which is crucial in practice. In a
Theorem 2, the s largest coefﬁcients of (cid:98)β are those of the support. Given a suitable data-driven
speciﬁc application [5] the threshold is chosen in a signal-dependent way allowing domain experts
to interpret λ as signal-to-noise ratio. Alternatively, one can exploit that under the conditions of
estimate for s e.g. that proposed in [25], λ can be chosen automatically.

7

Figure 3: Comparison of thresholded NNLS (red) and thresholded non-negative lasso (blue) for the
experiments with constant s/n, while b (abscissa) and p/n (symbols) vary.

Figure 4: Top: Comparison of thresholded NNLS (red) and the thresholded non-negative lasso
(blue) for the experiments with constant b, while s/n (abscissa) and p/n (symbols) vary. Bottom
left: Non-negative lasso without thresholding (blue) and orthogonal matching pursuit (magenta).
Bottom right: Thresholded non-negative lasso (blue) and thresholded ordinary lasso (green).

Results. The approaches NN(cid:96)1 and OMP are not competitive − both work only with rather mod-
erate levels of sparsity, with a breakdown at s/n = 0.15 for power decay as displayed in the bottom
left panel of Figure 4. For the second design, the results are even worse. This is in accordance with
the literature where thresholding is proposed as remedy [17, 18, 19]. Yet, for a wide range of con-
ﬁgurations, tNNLS visibly outperforms tNN(cid:96)1 , a notable exception being power decay with larger
values for p/n. This is in contrast to the design from Ens+ , where even p/n = 10 can be handled.
This difference requires further research.
Conclusion. To deal with higher levels of sparsity, thresholding seems to be inevitable. Threshold-
ing the biased solution obtained by (cid:96)1 -regularization requires a proper choice of the regularization
parameter and is likely to be inferior to thresholded NNLS with regard to the detection of small sig-
nals. The experimental results provide strong support for the central message of the paper: even in
high-dimensional, noisy settings, non-negativity constraints can be unexpectedly powerful when in-
teracting with ’self-regularizing ’properties of the design. While this has previously been observed
empirically, our results provide a solid theoretical understanding of this phenomenon. A natural
question is whether this ﬁnding can be transferred to other kinds of ’simple constraints’ (e.g. box
constraints) that are commonly imposed.

8

0.10.150.20.250.30.3500.20.40.60.81power decaybProb. of Success  p/n= 1.5p/n= 2.0p/n= 2.5p/n= 3.0p/n= 3.5p/n= 4.00.30.350.40.450.50.550.600.20.40.60.81Ens+bProb. of Success  p/n= 2.0p/n= 3.0p/n= 5.0p/n= 10.00.050.10.150.20.250.300.20.40.60.81power decays/nProb. of Success  p/n= 1.5p/n= 2.0p/n= 2.5p/n= 3.0p/n= 3.5p/n= 4.00.050.10.150.20.250.300.20.40.60.81Ens+s/nProb. of Success  p/n= 2.0p/n= 3.0p/n= 5.0p/n= 10.000.050.10.1500.20.40.60.81power decay, w/o thresholdings/nProb. of Success  p/n= 1.5p/n= 2.0p/n= 2.5p/n= 3.0p/n= 3.5p/n= 4.00.050.10.150.20.250.300.20.40.60.81power decay, non−negative lasso vs. ordinary lassos/nProb. of Success  p/n= 1.5p/n= 2.0p/n= 2.5p/n= 3.0p/n= 3.5p/n= 4.0References
[1] Y. Lin, D. Lee, and L. Saul. Nonnegative deconvolution for time of arrival estimation. In ICASSP, 2004.
[2] J. Bardsley and J. Nagy. Covariance-preconditioned iterative methods for nonnegatively constrained as-
tronomical imaging. SIAM Journal on Matrix Analysis and Applications, 27:1184–1198, 2006.
[3] A. Szlam and. Z. Guo and S. Osher. A split Bregman method for non-negative sparsity penalized least
squares with applications to hyperspectral demixing. In IEEE International Conference on Image Pro-
cessing, 2010.
[4] L. Li and T. Speed. Parametric deconvolution of positive spike trains. The Annals of Statistics, 28:1279–
1301, 2000.
[5] M. Slawski and M. Hein. Sparse recovery for Protein Mass Spectrometry data. In NIPS workshop on
practical applications of sparse modelling, 2010.
[6] D. Donoho, I. Johnstone, J. Hoch, and A. Stern. Maximum entropy and the nearly black object. Journal
of the Royal Statistical Society Series B, 54:41–81, 1992.
[7] D. Chen and R. Plemmons. Nonnegativity constraints in numerical analysis. In Symposium on the Birth
of Numerical Analysis, 2007.
[8] A. Bruckstein, M. Elad, and M. Zibulevsky. On the uniqueness of nonnegative sparse solutions to under-
determined systems of equations. IEEE Transactions on Information Theory, 54:4813–4820, 2008.
[9] D. Donoho and J. Tanner. Counting the faces of randomly-projected hypercubes and orthants, with appli-
cations. Discrete and Computational Geometry, 43:522–541, 2010.
[10] M. Wang and A. Tang. Conditions for a Unique Non-negative Solution to an Underdetermined System.
In Proceedings of Allerton Conference on Communication, Control, and Computing, 2009.
[11] M. Wang, W. Xu, and A. Tang. A unique nonnegative solution to an undetermined system: from vectors
to matrices. IEEE Transactions on Signal Processing, 59:1007–1016, 2011.
[12] C. Liew. Inequality Constrained Least-Squares Estimation. Journal of the American Statistical Associa-
tion, 71:746–751, 1976.
[13] R. Tibshirani. Regression shrinkage and variable selection via the lasso. Journal of the Royal Statistical
Society Series B, 58:671–686, 1996.
[14] S. van de Geer and P. B ¨uhlmann. On the conditions used to prove oracle results for the Lasso. The
Electronic Journal of Statistics, 3:1360–1392, 2009.
[15] P. Zhao and B. Yu. On model selection consistency of the lasso. Journal of Machine Learning Research,
7:2541–2567, 2006.
[16] M. Wainwright.
Sharp thresholds for noisy and high-dimensional recovery of sparsity using (cid:96)1 -
constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55:2183–2202,
2009.
[17] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. The
Annals of Statistics, 37:246–270, 2009.
[18] T. Zhang. Some Sharp Performance Bounds for Least Squares Regression with L1 Regularization. The
Annals of Statistics, 37:2109–2144, 2009.
[19] S. Zhou. Thresholding procedures for high dimensional variable selection and statistical estimation. In
NIPS, 2009.
[20] E. Greenshtein and Y. Ritov. Persistence in high-dimensional linear predictor selection and the virtue of
overparametrization. Bernoulli, 6:971–988, 2004.
[21] D. Donoho and J. Tanner. Sparse nonnegative solution of underdetermined linear equations by linear
programming. Proceedings of the National Academy of Science, 102:9446–9451, 2005.
[22] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information
Theory, 50:2231–2242, 2004.
[23] T. Zhang. On the Consistency of Feature Selection using Greedy Least Squares Regression. Journal of
Machine Learning Research, 10:555–568, 2009.
[24] H. Rue and L. Held. Gaussian Markov Random Fields. Chapman and Hall/CRC, Boca Raton, 2001.
[25] C. Genovese, J. Jin, and L. Wasserman. Revisiting Marginal Regression. Technical report, Carnegie
Mellon University, 2009. http://arxiv.org/abs/0911.4080.
[26] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least Angle Regression. The Annals of Statistics,
32:407–499, 2004.

9


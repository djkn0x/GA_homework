On U -processes and clustering performance

St ´ephan Cl ´emenc¸ on∗
LTCI UMR Telecom ParisTech/CNRS No. 5141
Institut Telecom, Paris, 75634 Cedex 13, France
stephan.clemencon@telecom-paristech.fr

Abstract

Many clustering techniques aim at optimizing empirical criteria that are of the
form of a U -statistic of degree two. Given a measure of dissimilarity between
pairs of observations, the goal is to minimize the within cluster point scatter over
a class of partitions of the feature space. It is the purpose of this paper to deﬁne
a general statistical framework, relying on the theory of U -processes, for study-
ing the performance of such clustering methods. In this setup, under adequate
√
assumptions on the complexity of the subsets forming the partition candidates, the
excess of clustering risk is proved to be of the order OP (1/
n). Based on recent
results related to the tail behavior of degenerate U -processes, it is also shown how
to establish tighter rate bounds. Model selection issues, related to the number of
clusters forming the data partition in particular, are also considered.

1

Introduction

In cluster analysis, the objective is to segment a dataset into subgroups, such that data points in the
same subgroup are more similar to each other (in a sense that will be speciﬁed) than to those in
other subgroups. Given the wide range of applications of the clustering paradigm, numerous data
segmentation procedures have been introduced in the machine-learning literature (see Chapter 14 in
[HTF09] and Chapter 8 in [CFZ09] for recent overviews of ”off-the-shelf” clustering techniques).
Whereas the design of clustering algorithms is still receiving much attention in machine-learning
(see [WT10] and the references therein for instance), the statistical study of their performance,
with the notable exception of the celebrated K -means approach, see [Har78, Pol81, Pol82, BD04]
and more recently [BDL08] in the functional data analysis setting, may appear to be not sufﬁciently
well-documented in contrast, as pointed out in [vLBD05, BvL09]. Indeed, in the K -means situation,
the speciﬁc form of the criterion (and of its expectation, the clustering risk), as well as that of the
cells deﬁning the clusters and forming a partition of the feature space (Voronoi cells), permits to
use, in a straightforward manner, results of the theory of empirical processes in order to control
the performance of empirical clustering risk minimizers. Unfortunately, this center-based approach
does not carry over into more general situations, where the dissimilarity measure is not a square
hilbertian norm anymore, unless one loses the possibility to interpret the clustering criterion as a
function of pairwise dissimilarities between the observations (cf K -medians).
It is the goal of this paper to establish a general statistical framework for investigating clustering
performance. The present analysis is based on the observation that many statistical criteria for
measuring clustering accuracy are (symmetric) U -statistics (of degree two), functions of a matrix
of dissimilarities between pairs of data points. Such statistics have recently received a good deal of
attention in the machine-learning literature, insofar as empirical performance measures of predictive
rules in problems such as statistical ranking (when viewed as pairwise classiﬁcation), see [CLV08],
or learning on graphs ([BB06]), are precisely functionals of this type, generalizing sample mean
statistics. By means of uniform deviation results for U -processes, the Empirical Risk Minimization
∗ http://www.tsi.enst.fr/∼clemenco/.

1

√
paradigm (ERM) can be extended to situations where natural estimates of the risk are U -statistics.
In this way, we establish here a rate bound of order OP (1/
n) for the excess of clustering risk
of empirical minimizers under adequate complexity assumptions on the cells forming the partition
candidates (the bias term is neglected in the present analysis). A linearization technique, combined
with sharper tail results in the case of degenerate U -processes is also used in order to show that
tighter rate bounds can be obtained. Finally, it is shown how to use the upper bounds established
in this analysis in order to deal with the problem of automatic model selection, that of selecting the
number of clusters in particular, through complexity penalization.
The paper is structured as follows.
In section 2, the notations are set out, a formal description
of cluster analysis, from the ”pairwise dissimilarity” perspective, is given and the main theoretical
concepts involved in the present analysis are brieﬂy recalled. In section 3, an upper bound for the
performance of empirical minimization of the clustering risk is established in the context of general
dissimilarity measures. Section 4 shows how to reﬁne the rate bound previously obtained by means
of a recent inequality for degenerate U -processes, while section 5 deals with automatic selection of
the optimal number of clusters. Technical proofs are deferred to the Appendix section.

2 Theoretical background

In this section, after a brief description of the probabilistic framework of the study, the general
formulation of the clustering objective, based on the notion of dissimilarity between pairs of obser-
vations, is recalled and the connection of the problem of investigating clustering performance with
the theory of U -statistics and U -processes is highlighted. Concepts pertaining to this theory and
involved in the subsequent analysis are next recalled.

2.1 Probabilistic setup and ﬁrst notations

Here and throughout, (X1 , . . . , Xn ) denotes a sample of i.i.d. random vectors, valued in a high-
dimensional feature space X , typically a subset of the euclidian space Rd with d >> 1, with com-
mon probability distribution µ(dx). With no loss of generality, we assume that the feature space X
denoted by I{E }, the usual lp norm on Rd by ||x||p = ((cid:80)d
coincides with the support of the distribution µ(dx). The indicator function of any event E will be
i=1 |xi |p )1/p when 1 ≤ p < ∞ and by
||x||∞ = max1≤i≤d |xi | in the case p = ∞, with x = (x1 , . . . , xd ) ∈ Rd . When well-deﬁned, the
expectation and the variance of a r.v. Z are denoted by E[Z ] and Var(Z ) respectively. Finally, we
denote by x+ = max(0, x) the positive part of any real number x.

2.2 Cluster analysis

D(Xi , Xj ) · I{(Xi , Xj ) ∈ C 2
k },

The goal of clustering techniques is to partition the data (X1 , . . . , Xn ) into a given ﬁnite number
of groups, K << n say, so that the observations lying in a same group are more similar to each
other than to those in other groups. When equipped with a (borelian) measure of dissimilarity
D : X 2 → R∗
K(cid:88)
(cid:88)
+ , the clustering task can be rigorously cast as the problem of minimizing the criterion
(cid:99)Wn (P ) =
2
n(n − 1)
1≤i<j≤n
k=1
over all possible partitions P = {Ck :
1 ≤ k ≤ K } of the feature space X . The quantity
(1) is generally called the intra-cluster similarity or the within cluster point scatter. The function
D aiming at measuring dissimilarity between pairs of observations, we suppose that it fulﬁlls the
following properties:
• (SYMM E TRY ) For all (x, x(cid:48) ) ∈ X 2 , D(x, x(cid:48) ) = D(x(cid:48) , x)
• (S E PARAT ION ) For all (x, x(cid:48) ) ∈ X 2 : D(x, x(cid:48) ) = 0, ⇔ x = x(cid:48)
Typical choices for the dissimilarity measure are of the form D(x, x(cid:48) ) = Φ(||x− x(cid:48) ||p ), where p ≥ 1
and Φ : R+ → R+ is a nondecreasing function such that Φ(0) = 0 and Φ(t) > 0 for all t > 0. This
includes the so-termed ”standard K -means” setup, where the dissimilarity measure coincides with

(1)

2

(2)

W (P ) =

the square euclidian norm (in this case, p = 2 and Φ(t) = t2 for t ≥ 0). Notice that the expectation
K(cid:88)
E (cid:2)D(X, X (cid:48) ) · I{(X, X (cid:48) ) ∈ C 2
k }(cid:3) ,
of the r.v. (1) is equal to the following quantity:
k=1
where (X, X (cid:48) ) denotes a pair of independent r.v.’s drawn from µ(dx). It will be referred to as the
clustering risk of the partition P , while its statistical counterpart (1) will be called the empirical
clustering risk. Optimal partitions of the feature space X are deﬁned as those that minimize W (P ).
1/(n(n − 1)) · (cid:80)
(cid:80)
Remark 1 (MAX IM I ZAT ION FORMU LAT ION ) It is well-known that minimizing the empirical clus-
tering risk (1) is equivalent to maximizing the between-cluster scatter point, which is given by
i, j D(Xi , Xj ) · I{(Xi , Xj ) ∈ Ck × Cl }, the sum of these two statistics being
k (cid:54)=l
independent from the partition P = {Ck : 1 ≤ k ≤ K } considered.
Here we consider minimizers of the empirical risk (cid:99)Wn over Π, i.e. partitions (cid:98)P ∗
Suppose we are given a (hopefully sufﬁciently rich) class Π of partitions of the feature space X .
(cid:16) (cid:98)P ∗
(cid:17)
(cid:99)Wn (P ) .
(cid:99)Wn
n in Π such that
= minP ∈Π
n
The design of practical algorithms for computing (approximately) empirical clustering risk minimiz-
ers is beyond the scope of this paper (refer to [HTF09] for an overview of ”off-the-shelf” clustering
methods). Here, focus is on the performance of such empirically deﬁned rules.

(3)

2.3 U -statistics and U -processes

The subsequent analysis crucially relies on the fact that the quantity (1) that one seeks to optimize
is a U -statistic. For clarity’s sake, we recall the deﬁnition of this class of statistics, generalizing
sample means.

(4)

Deﬁnition 1 (U - STAT I S T IC O F D EGR EE TWO . ) Let X1 ,
. . . , Xn be independent copies of a
random vector X drawn from a probability distribution µ(dx) on the space X and K : X 2 → R be
(cid:88)
a symmetric function such that K(X1 , X2 ) is square integrable. By deﬁnition, the functional
2
K(Xi , Xj ).
Un =
n(n − 1)
1≤i<j≤n
is a (symmetric) U -statistic of degree two, with kernel K.
It is said to be degenerate when
The statistic (4) is a natural (unbiased) estimate of the quantity θ = (cid:82) (cid:82) K(x, x(cid:48) )µ(dx)µ(dx(cid:48) ). The
K(1) (x) def= E[K(x, X )] = 0 with probability one for all x ∈ X , non degenerate otherwise.
class of U -statistics is very large and include most dispersion measures, including the variance or the
Gini mean difference (with K(x, x(cid:48) ) = (x− x(cid:48) )2 and K(x, x(cid:48) ) = |x− x(cid:48) | respectively, (x, x(cid:48) ) ∈ R2 ),
as well as the celebrated Wilcoxon location test statistic (with K(x, x(cid:48) ) = I{x + x(cid:48) > 0} for
(x, x(cid:48) ) ∈ R2 in this case). Although the dependence structure induced by the summation over all
pairs of observations makes its study more difﬁcult than that of basic sample means, this estimator
has nice properties. It is well-known folklore in mathematical statistics that it is the most efﬁcient
estimator among all unbiased estimators of the parameter θ (i.e.
that with minimum variance),
see [vdV98]. Precisely, when non degenerate, it is asymptotically normal with limiting variance
4 ·Var(K(1) (X )) (refer to Chapter 5 in [Ser80] for an account of asymptotic analysis of U -statistics).
As shall be seen in section 4, the reduced variance property of U -statistics is crucial, when it comes
to establish tight rate bounds.
K(cid:88)
Going back to the U -statistic of degree two (1) estimating (2), observe that its symmetric kernel is:
k }.
D(x, x(cid:48) ) · I{(x, x(cid:48) ) ∈ C 2
∀(x, x(cid:48) ) ∈ X 2 , KP (x, x(cid:48) ) =
k=1
Assuming that E[D2 (X1 , X2 ) · I{(X1 , X2 ) ∈ C 2
. . . , K } and placing
k }] < ∞ for all k ∈ {1,
ourselves in the situation where K ≥ 1 is less than X ’s cardinality, the U -statistic (1) is always non

(5)

3

degenerate, except in the (sole) case where X is made of K elements exactly and all P ’s cells are
(cid:90)
singletons. Indeed, for all x ∈ X , denoting by k(x) the index of {1, . . . , K } such that x ∈ Ck(x) ,
we have:
K(1)P (x) def= E[KP (x, X )] =
D(x, x(cid:48) )µ(dx(cid:48) ).
(6)
x(cid:48)∈Ck(x)
As µ’s support coincides with X and the separation property is fulﬁlled by D , the quantity
n{(cid:99)Wn (P ) − W (P )} is equal to 4 · Var(D(X, Ck(X ) ), where we set D(x, C ) =
above is zero iff Ck(x) = {x}.
(cid:82)
In the non degenerate case, notice ﬁnally that the asymptotic
√
variance of
x(cid:48)∈X D(x, x(cid:48) )µ(dx(cid:48) ) for all x ∈ X and any measurable set C ⊂ X .
By deﬁnition, a U -process is a collection of U -statistics, one may refer to [dlPG99] for an account
of the theory of U -processes. Echoing the role played by the theory of empirical processes in the
(cid:111)
(cid:110)(cid:99)Wn (P ) − W (P ) : P ∈ Π
study of the ERM principle in binary classiﬁcation, the control of the ﬂuctuations of the U -process
indexed by a set Π of partition candidates will naturally lie at the heart of the present analysis. As
shall be seen below, this can be achieved mainly by the means of the Hoeffding representations of
U -statistics, see [Hoe48].

3 A bound for the excess of clustering risk

(7)

AK,n =

sup
C∈P , P ∈ΠK

1
(cid:98)n/2(cid:99)

iD(Xi , Xi+(cid:98)n/2(cid:99) ) · I{(Xi , Xi+(cid:98)n/2(cid:99) ) ∈ C 2}

Here we establish an upper bound for the performance of an empirical minimizer of the clustering
risk over a class ΠK of partitions of X with K ≥ 1 cells, K being ﬁxed here and supposed to be
smaller than X ’s cardinality. We denote by W ∗
K the clustering risk minimum over all partitions of
X with K cells. The following global suprema of empirical Rademacher averages, characterizing
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)n/2(cid:99)(cid:88)
the complexity of the cells forming the partition candidates, shall be involved in the subsequent rate
analysis: ∀n ≥ 2,
i=1
where  = (i )i≥1 is a Rademacher chaos, independent from the Xi ’s, see [Kol06].
√
The following theorem reveals that the clustering performance of the empirical minimizer (3) is of
the order OP (1/
n), when neglecting the bias term (depending on the richness of ΠK solely).
Theorem 1 Consider a class ΠK of partitions with K ≥ 1 cells and suppose that:
• there exists B < ∞ such that for all P in ΠK , any C in P , sup(x,x(cid:48) )∈C 2 D(x, x(cid:48) ) ≤ B ,
• the expectation of the Rademacher average AK,n is of the order O(n−1/2 ).
Let δ > 0. For any empirical clustering risk minimizer (cid:98)P ∗
(cid:114) 2 log(1/δ)
(cid:18)
(cid:19)
n , we have with probability at least 1 − δ :
∀n ≥ 2, W ( (cid:98)P ∗
(cid:19)
(cid:18)
W (P ) − W ∗
K ≤ 4K E[AK,n ] + 2BK
n ) − W ∗
infP ∈ΠK
K
n
≤ c(B , δ) · K√
W (P ) − W ∗
infP ∈ΠK
+
K
n
for some constant c(B , δ) < ∞, independent from n and K .
The key for proving (8) is to express the U -statistic Wn (P ) in terms of sums of i.i.d. r.v.’s, as that
(cid:88)
(cid:98)n/2(cid:99)(cid:88)
involved in the Rademacher average (7):
σ∈Sn
i=1

KP (Xi , Xi+(cid:98)n/2(cid:99) ),

Wn (P ) =

1
n!

1
(cid:98)n/2(cid:99)

(8)

(9)

+

,

4

where the average is taken over Sn , the symmetric group of order n. The main point lies in the fact
that standard techniques in empirical process theory can be then used to control Wn (P ) − W (P )
uniformly over ΠK under adequate hypotheses, see the proof in the Appendix for technical details.
We underline that, naturally, the complexity assumption is also a crucial ingredient of the result
stated above, and more generally to clustering consistency results, see Example 1 in [BvL09]. We
also point out that the ERM approach is by no means the sole method to obtain error bounds in the
clustering context. Just like in binary classiﬁcation (see [KN02]), one may use a notion of stability
of a clustering algorithm to establish such results, see [vL09, ST09] and the references therein. Refer
to [vLBD06, vLBD08] for error bounds proved through the stability approach. Before showing how
the bound for the excess of risk stated above can be improved, a few remarks are in order.

Remark 2 (ON TH E COM P LEX I TY A S SUM P T ION . ) We point out that standard entropy metric argu-
ments can be used in order to bound the expected value of the Rademacher average An , see [BBL05]
E[AK,n ] ≤ c(cid:112)V /n for some universal constant c < ∞. This covers a wide variety of situations,
for instance. In particular, if the set of functions FΠK = {(x, x(cid:48) ) ∈ X 2 (cid:55)→ D(x, x(cid:48) ) · I{(x, x(cid:48) ) ∈
C 2} : C ∈ P , P ∈ ΠK } is a VC major class with ﬁnite VC dimension V (see [Dud99]), then
including the case where D(x, x(cid:48) ) = ||x − x(cid:48) ||β
p and the class of sets {C : C ∈ P , P ∈ ΠK } is of
ﬁnite VC dimension.

Remark 3 (K -M EAN S . )
the dissimilarity measure is
In the standard K -means approach,
D(x, x(cid:48) ) = ||x − x(cid:48) ||2
2 and partition candidates are indexed by a collection c of distinct ”centers”
c1 , . . . , cK in X : Pc = {C1 , . . . , CK } with Ck = {x ∈ X : ||x − ck ||2 = min1≤l≤K ||x − cl ||2 }
for 1 ≤ k ≤ K (with adequate distance-tie breaking). One may easily check that for this speciﬁc
collection of partitions ΠK and this choice for the dissimilarity measure, the class FΠK is a VC
major class with ﬁnite VC dimension, see section 19.1 in [DGL96] for instance. Additionally, it
should be noticed than in most practical clustering procedures, center candidates are picked in a
data-driven fashion, being taken as the averages of the observations lying in each cluster/cell. In this
respect, the M -estimation problem formulated here can be considered to a certain extent as closer to
what is actually achieved by K -means clustering techniques in practice, than the usual formulation
of the K -means problem (as an optimization problem over c = (c1 , . . . , cK ) namely).
(ωi )1≤i≤d in a coordinatewise manner, leading to (cid:98)D(x, x(cid:48) ) = (cid:80)d
Remark 4 (W E IGH TED C LU ST ER ING CR I TER IA . ) Notice that, in practice, the measure D involved
i )2/(cid:98)σ2
in (1) may depend on the data. For scaling purpose, one could assign data-dependent weights ω =
where (cid:98)σ2
i=1 (xi − x(cid:48)
i for instance,
i denotes the sample variance related to the i-th coordinate. Although the criterion reﬂecting
between the latter and the U -statistic (1) with D(x, x(cid:48) ) = (cid:80)d
the performance is not a U -statistic anymore, the theory we develop here can be straightforwardly
used for investigating clustering accuracy in such a case. Indeed, it is easy to control the difference
i=1 (xi − x(cid:48)
i )2 /σ2
i , the σ2
i ’s denoting
the theoretical variances of µ’s marginals, under adequate moment assumptions.

4 Tighter bounds for empirical clustering risk minimizers

We now show that one may reﬁne the rate bound established above, by considering another repre-
sentation of the U -statistic (1), its Hoeffding’s decomposition (see [Ser80]): for all partition P ,
(cid:80)C∈P H(1)C (Xi ) being a simple average of i.i.d r.v.’s with, for (x, x(cid:48) ) ∈ X 2 ,
Ln (P ) = (1/n) (cid:80)n
Wn (P ) − W (P ) = 2Ln (P ) + Mn (P ),
(10)
i=1
where D(C , C ) = (cid:82)
HC (x, x(cid:48) ) = D(x, x(cid:48) ) · I{(x, x(cid:48) ) ∈ C 2} and H(1)C (x) = D(x, C ) · I{x ∈ C } − D(C , C ),
degenerate U -statistic based on the Xi ’s with kernel given by: (cid:80)C∈P H(2)C (x, x(cid:48) ), where
x∈C D(x, C )µ(dx) and E[HC (x, X )] = D(x, C ) · I{x ∈ C }, and Mn (P ) being a
H(2)C (x, x(cid:48) ) = HC (x, x(cid:48) ) − H(1)C (x) − H(1)C (x(cid:48) ) − D(C , C ),
order OP ((cid:112)1/n), while the second term is of the order OP (1/n). Hence, provided this holds true
for all (x, x(cid:48) ) ∈ X 2 . The leading term in (10) is the (centered) sample mean 2Ln (P ), of the
5

1
n

M =

sup
C∈P , P ∈ΠK

Z =

sup
C∈P , P ∈ΠK

sup
C∈P , P ∈ΠK

iαj H(2)C (Xi , Xj ),

n(cid:88)
uniformly over P , the main contribution to the rate bound should arise from the quantity
|2Ln (P )| ≤ 2K
|(1/n)
H(1)C (Xi ) − D(C , C )|,
sup
sup
C∈P , P ∈ΠK
P ∈ΠK
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
i=1
which thus leads to consider the following suprema of empirical Rademacher averages:
iD(Xi , C ) · I{Xi ∈ C }
RK,n =
sup
C∈P , P ∈ΠK
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) , U =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
i=1
(cid:88)
(cid:88)
This supremum clearly has smaller mean and variance than (7). We also introduce the quantities:
α: P
i j H(2)C (Xi , Xj )
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
sup
j α2
i,j
i,j
j
iH(2)C (Xi , Xj )
sup
1≤j≤n
i
Theorem 2 Consider a class ΠK of partitions with K cells and suppose that:
• there exists B < ∞ such that sup(x,x(cid:48) )∈C 2 D(x, x(cid:48) ) ≤ B for all P ∈ ΠK , C ∈ P .
Let δ > 0. For any empirical clustering risk minimizer (cid:98)P ∗
(cid:114) log(2/δ)
(cid:18)
(cid:19)
n , with probability at least 1 − δ : ∀n ≥ 2,
W ( (cid:98)P ∗
n ) − W ∗
K ≤ 4K E[RK,n ] + 2BK
W (P ) − W ∗
infP ∈ΠK
+ K κ(n, δ) +
K
n
(cid:17)
(cid:16)E[Z ] + (cid:112)log(1/δ)E[U ] + (n + E[M ])/ log(1/δ)
where we set for some universal constant C < ∞, independent from n, N and K :
κ(n, δ) = C
/n2 .
The result above relies on the moment inequality for degenerate U -processes proved in [CLV08].
where Λn (P ) = (cid:99)Wn (P ) − W ∗
Remark 5 (LOCAL I ZAT ION . ) The same argument can be used to decompose Λn (P ) − Λ(P ),
K is an estimate of the excess of risk Λ(P ) = W (P ) − W ∗
K , and, by
functions {(cid:80)C∈P D(x, C ) · I{x ∈ C } − (cid:80)C∗∈P ∗ D(x, C ∗ ) · {x ∈ C ∗ } : P ∈ ΠK }, following in
means of concentration inequalities, to obtain next a sharp upper bound that involves the modulus
of continuity of the variance of the Rademacher average indexed by the convex hull of the set of
the footsteps or recent advances in binary classiﬁcation, see [Kol06] and subsection 5.3 in [BBL05].
Owing to space limitations, this will be dealt with in a forthcoming article.

(11)

, (12)

(13)

5 Model selection - choosing the number of clusters

A crucial issue in data segmentation is to determine the number K of cells that exhibits the most the
clustering phenomenon in the data. A variety of automatic procedures for choosing a good value for
K have been proposed in the literature, based on data splitting, resampling or sampling techniques
([PFvN89, TWH01, ST08]). Here we consider a complexity regularization method that avoids to
have recourse to such techniques and uses a data-dependent penalty term based on the analysis
carried out above.
. . . of collections of partitions of the feature space X
Suppose that we have a sequence Π1 , Π2 ,
such that, for all K ≥ 1, the elements of ΠK are made of K cells and fulﬁll the assumptions of
+ (cid:112)(2B log K )/n
Theorem 1. In order to avoid overﬁtting, consider the (data-driven) complexity penalty given by
27BK log K
pen(n, K ) = 3K E [AK,n ] +
and the minimizer (cid:98)P bK ,n of the penalized empirical clustering risk, with
n
(cid:110)(cid:99)Wn ( (cid:98)PK,n ) + pen(n, K )
(cid:111)
(cid:99)Wn (P ).
(cid:98)K = arg min
and (cid:99)Wn ( (cid:98)PK,n ) = minP ∈ΠK
K≥1
6

(14)

obtained with the help of an oracle, revealing the value of the index K that minimizes E[ (cid:98)PK,n ]−W ∗ ,
The next result shows that the partition thus selected nearly achieves the performance that would be
with W ∗ = inf P W (P ).
Theorem 3 (AN ORAC LE IN EQUAL I TY ) Suppose that, for all K ≥ 1, the assumptions of Theorem
(cid:33)
(cid:32)
(cid:114) 2
(cid:105) − W ∗ ≤ min
E (cid:104)
1 are fulﬁlled. Then, we have:
W ( (cid:98)P bK ,n )
2B
K≥1
n
Of course, the penalty could be slightly reﬁned using the results of Section 4. Due to space limita-
tions, such an analysis is not carried out here and is left to the reader.

K − W ∗ + pen(n, K )} + π2
{W ∗
6

18B
n

(15)

+

.

6 Conclusion

Whereas, until now, the theoretical analysis of clustering performance was mainly limited to the
K -means situation (but not only, cf [BvL09] for instance), this paper establishes bounds for the
success of empirical clustering risk minimization in a general ”pairwise dissimilarity” framework,
relying on the theory of U -processes. The excess of risk of empirical minimizers of the clustering
risk is proved to be of the order OP (n−1/2 ) under mild assumptions on the complexity of the cells
forming the partition candidates. It is also shown how to reﬁne slightly this upper bound through
a linearization technique and the use of recent inequalities for degenerate U -processes. Although
the improvement displayed here can appear as not very signiﬁcant at ﬁrst glance, our approach
suggests that much sharper data-dependent bounds could be established this way. To the best of
our knowledge, the present analysis is the ﬁrst to state results of this nature. As regards complexity
regularization, while focus is here on the choice of the number of clusters, the argument used in this
paper also paves the way for investigating more general model selection issues, including choices
related to the geometry/complexity of the cells of the partition considered.

Appendix - Technical proofs

(cid:19)

(cid:18)

W (P ) − W ∗
K

Proof of Theorem 1
|(cid:99)Wn (P ) − W (P )| +
(cid:99)W ( (cid:98)Pn ) − W ∗
We may classically write:
(cid:18)
(cid:19)
K ≤ 2 sup
infP ∈ΠK
P ∈ΠK
W (P ) − W ∗
|Un (C ) − u(C )| +
≤ 2K
infP ∈ΠK
sup
(16)
,
K
C∈P , P ∈ΠK
where Un (C ) denotes the U -statistic with kernel HC (x, x(cid:48) ) = D(x, x(cid:48) ) · I{(x, x(cid:48) ) ∈ C 2 } based on
the sample X1 , . . . , Xn and u(C ) its expectation. Therefore, mimicking the argument of Corollary
3 in [CLV08], based on the so-termed ﬁrst Hoeffding’s representation of U -statistics (see Lemma
A.1 in [CLV08]), we may straightforwardly derive the lemma below.
(cid:114) 2 log(1/δ)
Proposition 1 (UN I FORM DEV IAT ION S ) Suppose that Theorem 1’s assumptions are fulﬁlled. Let
δ > 0. With probability at least 1 − δ , we have: ∀n ≥ 2,
|Un (C ) − u(C )| ≤ 2E[AK,n ] + B
n

sup
C∈P , P ∈ΠK

.

(17)

PROO F. The argument follows in the footsteps of Corollary 3’s proof in [CLV08]. It is based on the
so-termed ﬁrst Hoeffding’s representation of U -statistics (9), which provides an immediate control
of the moment generating function of the supremum supC |Un (C ) − u(C )| by that of the norm of an
empirical process, namely supC |An (C ) − u(C )|, where, for all C ∈ P and P ∈ ΠK :
(cid:98)n/2(cid:99)(cid:88)
1
An (C ) =
D(Xi , Xi+(cid:98)n/2(cid:99) ) · I{(Xi , Xi+(cid:98)n/2(cid:99) ) ∈ C 2 }.
(cid:98)n/2(cid:99)
i=1

7

.

(18)

(cid:20)
(cid:19)(cid:21)
(cid:18)
(cid:19)(cid:21)
(cid:20)
(cid:18)
Lemma 1 (see Lemma A.1 in [CLV08]) Let Ψ : R → R be convex and nondecreasing. We have:
λ · sup
|Un (C ) − u(C )|
≤ E
|An (C ) − u(C )|
λ · sup
E
exp
exp
C
C
(cid:20)
(cid:18)
(cid:19)(cid:21)
Now, using standard symmetrization and randomization tricks, one obtains that: ∀λ > 0,
λ · sup
|An (C ) − u(C )|
≤ E [exp (2λ · AK,n )] .
E
exp
(19)
C
the value of AK,n cannot change by more than 2B/n when one of the
Observing that
(i , Xi , Xi+(cid:98)n/2(cid:99) )(cid:48) s is changed, while the others are kept ﬁxed, the standard bounded differences
(cid:18)
(cid:19)
inequality argument applies and yields:
2λ · E[AK,n ] + λ2B 2
E [exp (2λ · AK,n )] ≤ exp
(20)
.
2n
Next, Markov’s inequality with λ = (t − 2E[AK,n ])/B 2 gives: P{supC |An (C ) − u(C )| > t} ≤
exp(−n(t − 2E[AK,n ])2/(2B 2 )). The desired result is then immediate. (cid:3)
The rate bound is ﬁnally established by combining bounds (16) and (17).

Proof of Theorem 2 (Sketch of)

The theorem can be proved by using the decomposition (10), applying the argument above in order
to control supP |Ln (P )| and the lemma below to handle the degenerate part. The latter is based on a
recent moment inequality for degenerate U -processes, proved in [CLV08]. Due to space limitations,
technical details are left to the reader.

(cid:35)

Lemma 2 (see Theorem 11 in [CLV08]) Suppose that Theorem 2’s assumptions are fulﬁlled. There
exists a universal constant C < ∞ such that for all δ ∈ (0, 1), we have with probability at least
1 − δ : ∀n ≥ 2,
|Mn (P )| ≤ K κ(n, δ).

sup
P ∈ΠK
Proof of Theorem 3
E (cid:104)
(cid:105) − W ∗ ≤ E (cid:104)
(cid:105) − W ∗ + pen(K, n)
The proof mimics the argument of Theorem 8.1 in [BBL05]. We thus obtain that: ∀K ≥ 1,
W ( (cid:98)P bK ,n )
W ( (cid:98)PK,n )
(cid:34)(cid:18)
(cid:19)
+ (cid:88)
{W (P ) − (cid:99)Wn (P )} − pen(n, k)
E
sup
P ∈Πk
k≥1
(cid:20)
(cid:21)
Reproducing the argument of Theorem 1’s proof, one may easily show that: ∀k ≥ 1,
{W (P ) − (cid:99)Wn (P )}
≤ 2kE[Ak,n ].
E
sup
{W (P ) − (cid:99)Wn (P )} ≥ pen(n, k) + 2δ} is bounded by
P ∈Πk
(cid:27)
(cid:20)
(cid:26)
(cid:21)
Thus, for all k ≥ 1, the quantity P{supP ∈Πk
+ (cid:112)(2B log k)/n + δ
{W (P ) − (cid:99)Wn (P )}
{W (P ) − (cid:99)Wn (P )} ≥ E
(cid:27)
(cid:26)
P
sup
P ∈Πk
3kE [Ak,n ] ≤ 2kE[Ak,n ] − 27Bk log k
+ P
n
By virtue of the bounded differences inequality (jumps being bounded by 2B/n), the ﬁrst term is
bounded by exp(−nδ2/(2B 2 ))/k2 , while the second term is bounded by, exp(−nδ/(9Bk))/k3 as
(cid:35)
(cid:34)(cid:18)
(cid:19)
≤ (2B(cid:112)2/n + 18B/n)/k2 .
shown by Lemma 8.2 in [BBL05] (see the third inequality therein). Integrating over δ , one obtains:
{W (P ) − (cid:99)Wn (P )} − pen(n, k)
sup
P ∈Πk
+
Summing next the bounds thus obtained over k leads to the oracle inequality stated in the theorem.

sup
P ∈Πk

− δ

.

.

+

E

8

[BD04]

[BvL09]

[CLV08]

References
G. Biau and L. Bleakley. Statistical Inference on Graphs. Statistics & Decisions, 24:209–232, 2006.
[BB06]
[BBL05] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classiﬁcation: A Survey of Some Recent
Advances. ESAIM: Probability and Statistics, 9:323–375, 2005.
S. Ben-David. A framework for statistical clustering with a constant time approximation algorithms
for k-median clustering. In Proceedings of COLT’04, Lecture Notes in Computer Science, Volume
3120/2004, 415-426, 2004.
[BDL08] G. Biau, L. Devroye, and G. Lugosi. On the Performance of Clustering in Hilbert Space. IEEE
Trans. Inform. Theory, 54(2):781–790, 2008.
S. Bubeck and U. von Luxburg. Nearest neighbor clustering: A baseline method for consistent
clustering with arbitrary objective functions. Journal of Machine Learning Research, 10:657–698,
2009.
[CFZ09] B. Clarke, E. Fokou ´e, and H.. Zhang. Principles and Theory for Data-Mining and Machine-
Learning. Springer, 2009.
S. Cl ´emenc¸ on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of U-statistics.
The Annals of Statistics, 36(2):844–874, 2008.
[DGL96] L. Devroye, L. Gy ¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer,
1996.
[dlPG99] V. de la Pena and E. Gin ´e. Decoupling: from Dependence to Independence. Springer, 1999.
R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.
[Dud99]
J.A. Hartigan. Asymptotic distributions for clustering criteria. The Annals of Statistics, 6:117–131,
[Har78]
1978.
[Hoe48] W. Hoeffding. A class of statistics with asymptotically normal distribution. Ann. Math. Stat.,
19:293–325, 1948.
[HTF09] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning (2nd ed.), pages
520–528. Springer, 2009.
S. Kutin and P. Niyogi. Almost-everywhere algorithmic stability and generalization error. In Pro-
ceedings of the of the 18th Conference in Uncertainty in Artiﬁcial Intelligence, 2002.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization (with
discussion). The Annals of Statistics, 34:2593–2706, 2006.
[PFvN89] R. Peck, L. Fisher, and J. van Ness. Bootstrap conﬁdence intervals for the number of clusters in
cluster analysis. J. Am. Stat. Assoc., 84:184–191, 1989.
D. Pollard. Strong consistency of k-means clustering. The Annals of Statistics, 9:135–140, 1981.
D. Pollard. A central limit theorem for k-means clustering. The Annals of Probability, 10:919–926,
1982.
R.J. Serﬂing. Approximation theorems of mathematical statistics. Wiley, 1980.
O. Shamir and N. Tishby. Model selection and stability in k-means clustering. In in Proceedings of
the 21rst Annual Conference on Learning Theory, 2008.
O. Shamir and N. Tishby. On the reliability of clustering stability in the large sample regime. In
Advances in Neural Information Processing Systems 21, 2009.
[TWH01] R. Tibshirani, G. Walther, and T. Hastie. Estimating the number of clusters in a data set via the gap
statistic. J. Royal Stat. Soc., 63(2):411–423, 2001.
[vdV98] A. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.
U. von Luxburg. Clustering stability: An overview. Foundations and Trends in Machine Learning,
[vL09]
2(3):235–274, 2009.
[vLBD05] U. von Luxburg and S. Ben-David. Towards a statistical theory of clustering. In Pascal workshop
on Statistics and Optimization of Clustering, 2005.
[vLBD06] U. von Luxburg and S. Ben-David. A sober look at clustering stability. In Proceedings of the 19th
Conference on Learning Theory, 2006.
[vLBD08] U. von Luxburg and S. Ben-David. Relating clustering stability to properties of cluster boundaries.
In Proceedings of the 21th Conference on Learning Theory, 2008.
D. M. Witten and R. Tibshirani. A framework for feature selection in clustering. J. Amer. Stat.
Assoc., 105(490):713–726, 2010.

[Pol81]
[Pol82]

[Ser80]
[ST08]

[ST09]

[KN02]

[Kol06]

[WT10]

9


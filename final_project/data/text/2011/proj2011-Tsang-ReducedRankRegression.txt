CS 229 Final Pro ject
Reduced Rank Regression

Name : Ka Wai Tsang
SID : 005589301

1. Introduction
Given m observations of the predictors Xi ∈ Rp and the corresponding responses Yi ∈ Rn ,
let Y = [Y1 , Y2 , . . . , Ym ]T and X = [X1 , X2 , . . . , Xm ]T . Suppose X and Y are related by

(1.1)
Y = X A + E
where A is an unknown p × n matrix of coeﬃcients and E is an unobserved m × n random
noise matrix with independent mean zero and variance σ 2 . We want to ﬁnd an estimate ˆA
such that ||Y − X ˆA|| is small. If we use standard least square estimation directly to estimate
A in (1.1) without adding any constraints, then it is just the same as regressing each response
on the predictors separately. In this way, we actually ignore the possibility that the responses
may be correlated among themselves. Besides, when there are many attributes (p is large)
and many diﬀerent kinds of responses (n is large), the number of unknowns can be larger
than the sample size m. We may then need much more eﬀort to collect more samples to
increase m or the least square method simply cannot be applied.
To address this problem, one popular way to handle it is reduced rank regression. Let r(M )
be the rank of a matrix M . If we expect r(A) = r < min (p, n) or A can be well approximated
by a low rank matrix, we can write A as a product of two matrices with rank r, see [1]. That
is A = BrCr , Br ∈ Rp×r and Cr ∈ Rr×n which have total r × (n + p) unknowns needed to be
estimated. It can be much less than m if r(A) is very small. The model (1.1) then become

Y = (X Br )Cr + E

It can be interpreted as instead of p attributes, Y actually only depends on r factors. Each
factor is a linear combination of the attributes. In another words, this model says that the
attributes are correlated which is often the case in many real situations.
It is quite often that too many attributes are considered when we build a model. If we believe
that some attributes are actually not important to determine a response or we are only
interested to those really important attributes, then we would like A to be row sparse. Let
J (M ) be the index set of the non-zero rows of a matrix M and |J (M )| is the corresponding
cardinality. We would like to have |J (A)| (or |J ( ˆA)|) small so that we can only consider
those attributes in J (A) in future prediction. A common way to make the estimate ˆA sparse
is adding appropriate norm penalty, such as zero-norm or 1-norm.
In this pro ject, I am going to introduce three reduced rank regression methods, see [2], which
can give an estimate ˆA with r( ˆA) and |J ( ˆA)| small.

(2.1)

2. Method 1
Step 1. We ﬁrst ﬁnd an estimate ˆA1 such that
F + µr(B )}
{||Y − X B ||2
ˆA1 = arg min
B∈Rp×n
√
√
with µ = 2σ 2 (
p)2 , see [3]. In this step, we add a penalty term µr(B ), so we
n +
would expect ˆA1 is a low rank matrix. Let ˆk = r( ˆA1 ).
1

2

(2.2)

Step 2. Use the ˆk computed before, ﬁnd ˆA such that
||Y − X B ||2
F + λ||B ||2,1}
{ 1
ˆA = arg
p(cid:80)
min
B∈Rp×n ,r(B )≤k
2
i ||2 , bT
||bT
where ||B ||2,1 =
i
i=1
parameter that can be estimated by cross validation method. Notice that if the
constraint r(B ) ≤ k is removed, then ˆA is a group Lasso estimator. Lasso method
will output sparsity in the estimation.

is the i-th row of B . Here k = ˆk , λ is a regularization

3. Method 2
Step 1. For each pair (k , λ), f or1 ≤ k ≤ p and a range of values λ, we ﬁnd an estimate ˆAk,λ
such that it is the minimizer of (2.2).
Step 2. Among the ˆAk,λ computed above, choose the one ˆA such that
{||Y − X ˆAk,λ ||2
F + 2σ 2 (2n + |J ( ˆAk,λ |)|r( ˆAk,λ )|)}
ˆA = arg min
ˆAk,λ

(3.1)

This method is called selective reduced rank regression introduced in [2]. We observe
that the penalty term here penalizes both r( ˆAk,λ ) and |J ( ˆAk,λ )|.
It is an penalty
designed for selecting estimators with the best bias-variance trade-oﬀ relative to a
list of possible candidates.

(4.1)

4. Method 3
Step 1. We ﬁrst ﬁnd an estimate ˆA1 such that
{ 1
||Y − X B ||2
F + λ||B ||2,1}
ˆA1 = arg min
B∈Rp×n
2
The result is an group lasso estimator and so will be a sparse matrix. Let ˆX be the
predictor matrix that only contains the attribute columns selected here, that is the
columns that the corresponding rows in ˆA1 are non-zero.
Step 2. Find ˆA such that

(4.2)

{||Y − ˆX B ||2
F + µr(B )}
ˆA = arg min
B∈Rq×n
√
√
with q = |J ( ˆA1 )|,µ = 2σ 2 (
q)2 as before.

n +

5. Simulation

We ﬁrst make up some examples to see if these methods work. The setting I use here
is the similar as that in [2] so that I can compare the results. We construct the matrix of
(cid:20) bB0B1
(cid:21)
dependent variables X with rows i.i.d. from a multivariate normal distribution MVN(0,Σ),
with Σj k = ρ|j−k| , ρ > 0, 1 ≤ j, k ≤ p. Set the coeﬃcient matrix
A =
0
where b > 0, B0 is a J × r matrix and B1 is a r × n matrix. All entries in B0 and B1 are
i.i.d. N (0, 1). Therefore, almost for sure J = J (A) and r = r(A). Generate E ∈ Rm×n such
that Eij ∼ N (0, 1). Then set Y = X A + E .

3

To evaluate how good an approximation ˆA is, for the same setting (m, |J |, p, n, r,
rho and b) we repeat the methods 50 times and then we look at: 1. mean square error (MSE)
||X A − X ˆA||2
F /(mn) using test data at each run; 2. the mean number of predictors (| ˆJ |) and
mean rank estimate ( ˆR); and maybe the most important 3. how many correct(or wrong)
predictors were selected in comparison to the correct coeﬃcient matrix A. It is measured by
the missing rate (M = |J − ˆJ |/|J |) and false chosen (F C = | ˆJ − J |/(p − |J |)) rate. A good
approximation should have low M and F A.
For the setting (m = 30, |J | = 15, p = 100, n10, r = 2, ρ = 0.1 and b = 2), the simulation
results showed that the performances of these methods are close to each other. It is not
surprising because as the main components of these methods actually are similar. All these
methods successfully estimated the rank of the correct coeﬃcient matrix and selected most
(around 70%) of actual related features

Method 1
Method 2
Method 3

MSE
42.0
38.7
44.3

ˆJ
22.2
21.9
22.6

ˆR
2
2
2

M
0.30
0.27
0.29

FC
0.14
0.13
0.14

6. Vector Autoregressive Model

= (1 yt−1 yt−2 . . . yt−d )

To apply these methods in real applications, I have collected the interest rate swaps for 1-,
2-, 3-, 4-, 5-, 7-, 10-, 30-year from Jan 2009 to Sep 2011 [4]. My problem here is to estimate
the future values of the swaps using the past values. Consider a vector autoregressive (V AR)
model


yt = α0 + yt−1A1 + · · · + yt−dAd + t
α0
A1
A2
...
Ad
where yt ∈ Rn contains n swaps’ value at day t, Ai ∈ Rn×n are coeﬃcient matrices and
t ∈ Rn are i.i.d. with mean 0 and covariance matrix Σ. Suppose we use m + d days to
approximate Ai , then we have Y = X A + E . Y ∈ Rm×n , X ∈ Rm×p and A ∈ Rp×n where
p = 1 + d × n. The i-th row of Y is yt−i and the corresponding row of X is (1 yt−i−1 . . . yt−i−d ).
Notice that we have the term σ 2 in the above methods. For the case the covariance matrix
Σ = σI , [3] suggests an unbiased estimator
F /(mn − pn)
S 2 = ||Y − P Y ||2
where P is the orthogonal pro jection matrix on the column space of X .
In order to satisfy the assumptions of the V AR model, I ﬁrst diﬀerentiate the times series in
the time dimension. By augmented Dickey-Fuller test, no unit-root is present in each time
series with 95% conﬁdence level.
However, the performance of the methods in this application is not good. It may be because
:

(6.1)

(1) The above methods tend to generate an estimate with more zero rows which means
the corresponding columns should not be selected in the model. In autoregressive

4

model, people expect the values today depend more on the values right before today
than the days further before. Before the experiment, I thought most of the zero rows
would appear at the bottom of the estimate matrix ˆA which are corresponding to the
dates farthest away from the current day t so that the methods can tell me how many
days (d) should be used to predict the future values. However, the result showed that
the number of zero rows at the top of ˆA is roughly the same as that at the bottom.
I think the problem is that if we want to ﬁnd out how many days should be used in
the V AR model, then all the swaps’ value at the same day should be consider as one
group, either all the swaps in this day should be chosen or none of them.
(2) Obviously the values of the interest rate swaps with diﬀerent maturities are corre-
lated. The assumption that the variance matrix E = σI may be too strong for this
application and so (6.1) is not a good estimate.

To make the methods gave me something make sense, I only considered AR(1) model,
that is d = 1. Therefore, selecting variables is now nothing about selecting approximate
number of regression days in the model, but selecting the interest rate swaps that contain
most of the information about the swaps’ values tomorrow.

The procedure of this experiment is as follow:
(1) Divide the data set into one training set (Jan 2009 to Dec 2010) and one test set
(Jan 2011 to Sep 2011).
(2) Use the training set to compute ˆA by the above methods.
|, where
t − ˆy (i)
ˆA and error term |y (i)
(3) For t in test set, compute prediction ˆyt = yt−1
t
yt = (y (1)
, . . . , y (n)
, y (2)
) is a vector of actual diﬀerentiated interest rate swaps at day t.
t
t
t
The table below shows the results of those three methods. ”mean(| ˆyt |)i” is the mean of
the estimated diﬀerentiated interest rates swaps using method i. From the table, we can see
that the results are very poor. I have modiﬁed the methods such as ﬁxing the matrix to be
full rank and increasing d to see if these help improve the results. It helps a little bit but
still cannot give reasonable estimates.

10-yr 30-yr
7-yr
5-yr
4-yr
3-yr
2-yr
1-yr
mean(|yt |)
0.009 0.020 0.032 0.053 0.060 0.063 0.068 0.066
mean(| ˆyt |)1 0.001 0.004 0.006 0.009 0.009 0.008 0.007 0.006
mean(| ˆyt |)2 0.001 0.004 0.006 0.009 0.009 0.009 0.007 0.007
mean(| ˆyt |)3 0.001 0.006 0.008 0.011 0.012 0.010 0.008 0.007

The reason may be the methods are based on the assumption that the actual coeﬃcient
matrix is of low rank and sparse. Clearly, as swaps are correlated, the matrix should be of
low rank. However the matrix probably not sparse in this case.

To improve the methods so that they can be applied on more application, I think there are
at least 2 things we can do. First, we can introduce grouping into the algorithm. Sometimes

7. Future works

5

the features are naturally grouped, like in the V AR model the features are group by dates.
p(cid:80)
i ||2 , maybe we can use (cid:80)
If our question is which group should be chosen in the regression model, then treating each
feature individually should not be a good approach. To achieve this, instead of adding
penalty ||B ||2,1 =
||bT
||B T
I ||F , where B T
I
i=1
I
formed by the row vector in a group with indices in I .
Besides, we may also try to improve the estimate (6.1) or use another variance estimate
when the assumption the variance matrix E = σI is not likely to be true.

is a sub-matrix of B

References

[1] G. C. Reinsel and R. P. Velu, “Multivariate Reduced-Rank Regression: Theory and Applications”,
Lecture Notes in Statistics 136, Springer, New York, 1998.
[2] F. Bunea, Y. She and M. Wegkamp, “Joint variable and rank selection for parsimonious estimation of
high dimensional matrices”, 2011.
[3] F. Bunea, Y. She and M. Wegkamp, “Optimal selection of reduced rank estimators of high-dimensional
matrices”, Annals of Statistics, 39, 1282-1309, 2011.
[4] http://www.gsb.stanford.edu/jacksonlibrary/


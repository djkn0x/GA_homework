Active Classi ﬁcation based on Value of Classi ﬁer

Tianshi Gao
Department of Electrical Engineering
Stanford University
Stanford, CA 94305
tianshig@stanford.edu

Daphne Koller
Department of Computer Science
Stanford University
Stanford, CA 94305
koller@cs.stanford.edu

Abstract

Modern classiﬁcation tasks usually involve many class labe ls and can be informed
by a broad range of features. Many of these tasks are tackled by constructing a
set of classiﬁers, which are then applied at test time and the n pieced together in a
ﬁxed procedure determined in advance or at training time. We present an active
classiﬁcation process at the test time, where each classiﬁe
r in a large ensemble
is viewed as a potential observation that might inform our classiﬁcation process.
Observations are then selected dynamically based on previous observations, using
a value-theoretic computation that balances an estimate of the expected classiﬁca-
tion gain from each observation as well as its computational cost. The expected
classiﬁcation gain is computed using a probabilistic model
that uses the outcome
from previous observations. This active classiﬁcation pro cess is applied at test
time for each individual test instance, resulting in an efﬁc ient instance-speciﬁc de-
cision path. We demonstrate the bene ﬁt of the active scheme o n various real-world
datasets, and show that it can achieve comparable or even higher classiﬁcation ac-
curacy at a fraction of the computational costs of traditional methods.

1 Introduction

As the scope of machine learning applications has increased, the complexity of the classiﬁcation
tasks that are commonly tackled has grown dramatically. On one dimension, many classiﬁcation
problems involve hundreds or even thousands of possible classes [8]. On another dimension, re-
searchers have spent considerable effort developing new feature sets for particular applications, or
new types of kernels. For example, in an image labeling task, we have the option of using GIST
feature [26], SIFT feature [23], spatial HOG feature [33], Object Bank [21] and more. The bene ﬁts
of combining information from different types of features can be very signiﬁcant [12, 33].

To solve a complex classiﬁcation problem, many researchers have resorted to ensemble methods, in
which multiple classiﬁers are combined to achieve an accura te classiﬁcation decision. For example,
the Viola-Jones classiﬁer [32] uses a cascade of classiﬁers
, each of which focuses on different spatial
and appearance patterns. Boosting [10] constructs a committee of weak classiﬁers, each of which
focuses on different input distributions. Multiclass classiﬁcation problems are very often reduced
to a set of simpler (often binary) decisions, including one-vs-one [11], one-vs-all, error-correcting
output codes [9, 1], or tree-based approaches [27, 13, 3]. Intuitively, different classiﬁers provide
different “expertise ” in making certain distinctions that
can inform the classiﬁcation task. However,
as we discuss in Section 2, most of these methods use a ﬁxed pro cedure determined at training time
to apply the classiﬁers without adapting to each individual
test instance.

In this paper, we take an active and adaptive approach to combine multiple classiﬁers/features at
test time, based on the idea of value of information [16, 17, 24, 22]. At training time, we construct
a rich family of classiﬁers, which may vary in the features th at they use or the set of distinctions
that they make (i.e., the subset of classes that they try to distinguish). Each of these classiﬁers is
trained on all of the relevant training data. At test time, we dynamically select an instance-speciﬁc

1

subset of classiﬁers. We view each our pre-trained classiﬁe
r as a possible observation we can make
about an instance; each one adds a potential value towards our ability to classify the instance, but
also has a cost. Starting from an empty set of observations, at each stage, we use a myopic value-of-
information computation to select the next classiﬁer to app ly to the instance in a way that attempts to
increase the accuracy of our classiﬁcation state (e.g., dec rease the uncertainty about the class label)
at a low computational cost. This process stops when one of the suitable criteria is met (e.g., if
we are sufﬁciently con ﬁdent about the prediction). We provi
de an efﬁcient probabilistic method for
estimating the uncertainty of the class variable and about the expected gain from each classiﬁer. We
show that this approach provides a natural trajectory, in which simple, cheap classiﬁers are applied
initially, and used to provide guidance on which of our more expensive classiﬁers is likely to be
more informative. In particular, we show that we can get comparable (or even better) performance
to a method that uses a large range of expensive classiﬁers, a t a fraction of the computational cost.

2 Related Work

Our classiﬁcation model is based on multiple classiﬁers, so
it resembles ensemble methods like
boosting [10], random forests [4] and output-coding based multiclass classiﬁcation [9, 1, 29, 14].
However, these methods use a static decision process, where all classiﬁers have to be evaluated
before any decision can be made. Moreover, they often consider a homogeneous set of classiﬁers,
but we consider a variety of heterogeneous classiﬁers with d ifferent features and function forms.

Some existing methods can make classiﬁcation decisions bas ed on partial observations. One exam-
ple is a cascade of classiﬁers [32, 28], where an instance goe s through a chain of classiﬁers and the
decision can be made at any point if the classiﬁer response pa sses some threshold. Another type of
method focuses on designing the stopping criteria. Schwing et al. [30] proposed a stopping criterion
for random forests such that decisions can be made based on a subset of the trees. However, these
methods have a ﬁxed evaluation sequence for any instance, so there is no adaptive selection of which
classiﬁers to use based on what we have already observed.

Instance-speciﬁc decision paths based on previous observa tions can be found in decision tree style
models, e.g., DAGSVM [27] and tree-based methods [15, 13, 3]. Instead of making hard decisions
based on individual observations like these methods, we use a probabilistic model to fuse informa-
tion from multiple observations and only make decisions when it is sufﬁciently con ﬁdent.

When observations are associated with different features, our method also performs feature selec-
tion. Instead of selecting a ﬁxed set of features in the learn ing stage [34], we actively select instance-
speciﬁc features in the test stage. Furthermore, our method also considers computational properties
of the observations. Our selection criterion trades off between the statistical gain and the compu-
tational cost of the classiﬁer, resulting in a computationa lly efﬁcient cheap-to-expensive evaluation
process. Similar ideas are hard-coded by Vedaldi et al. [31] without adaptive decisions about when to
switch to which classiﬁer with what cost. Angelova et al. [2] performed feature selection to achieve
certain accuracy under some computational budget, but the selection is at training time without adap-
tation to individual test instances. Chai et al. [5] considered test-time feature value acquisition with
a strong assumption that observations are conditionally independent given the class variable.

Finally, our work is inspired by decision-making under uncertainty based on value of informa-
tion [16, 17, 24, 22]. For classiﬁcation, Krause and Guestri n [19] used it to compute a conditional
plan for asking the expert, trying to optimize classiﬁcatio n accuracy while requiring as little expert
interaction as possible. In machine learning, Cohn et al. [7] used active learning to select training
instances to reduce the labeling cost and speedup the learning, while our work focuses on inference.

3 Model

We denote the instance and label pair as (X, Y ). Furthermore, we assume that we have been pro-
vided a set of trained classiﬁers H, where hi ∈ H : X → R can be any real-valued classiﬁers
(functions) from existing methods. For example, for multiclass classiﬁcation, hi can be one-vs-all
classiﬁers, one-vs-one classiﬁers and weak learners from t
he boosting algorithms. Note that hi ’s
do not have to be homogeneous meaning that they can have different function forms, e.g., linear
or nonlinear, and more importantly they can be trained on different types of features with various
computational costs. Given an instance x, our goal is to infer Y by sequentially selecting one hi to
evaluate at a time, based on what has already been observed, until we are sufﬁciently con ﬁdent about

2

Y or some other stopping criterion is met, e.g., the computational constraint. The key in this process
is how valuable we think a classiﬁer hi is, so we introduce the value of a classiﬁer as follows.
Value of Classiﬁer. Let O be the set of classiﬁers that have already been evaluated (em pty at the
beginning). Denote the random variable Mi = hi (X ) as the response/margin of the i-th classiﬁer
in H and denote the random vector for the observed classiﬁers as MO = [Mo1 , Mo2 , . . . , Mo|O| ]T ,
where ∀oi ∈ O. Given the actual observed values mO of MO , we have a posterior P (Y |mO )
over Y . For now, suppose we are given a reward R : P → R which takes in a distribution P and
returns a real value indicating how preferable P is. Furthermore, we use C (hi |O) to denote the
computational cost of evaluating classiﬁer hi conditioned on the set of evaluated classiﬁers O. This
is because if hi shares the same feature with some oi ∈ O, we do not need to compute the feature
again. With some chosen reward R and a computational model C (hi |O), we de ﬁne the value of an
unobserved classiﬁer as follows.

1
τ

(1)

C (hi |O)

De ﬁnition 1 The value of classiﬁer V (hi |mO ) for a classiﬁer hi given the observed classiﬁer re-
sponses mO is the combination of the expected reward of the state informed by hi and the compu-
tational cost of hi . Formally,
V (hi |mO ) ∆= Z P (mi |mO )R(P (Y |mi , mO ))dmi −
1
=Emi∼P (Mi |mO ) (cid:2)R(P (Y |mi , mO ))(cid:3) −
τ
The value of classiﬁer has two parts corresponding to the sta tistical and computational properties
VR (hi |mO ) ∆= E (cid:2)R(P (Y |mi , mO ))(cid:3) is the expected
of the classiﬁer respectively. The ﬁrst part
reward of P (Y |mi , mO ), where the expectation is with respect to the posterior of Mi given mO .
The second part VC (hi |mO ) ∆= − 1
τ C (hi |O) is a computational penalty incurred by evaluating the
classiﬁer hi . The constant τ controls the tradeoff between the reward and the cost.
Given the de ﬁnition of the value of classiﬁer, at each step of
our sequential evaluations, our goal is
to pick hi with the highest value:
h∗ = argmax
V (hi |mO ) = argmax
hi∈H\O
hi∈H\O

VR (hi |mO ) + VC (hi |mO )

C (hi |O)

(2)

We introduce the building blocks of the value of classiﬁer, i .e., the reward, the cost and the proba-
bilistic model in the following, and then explain how to compute it.
Reward De ﬁnition. We propose two ways to de ﬁne the reward R : P → R.
Residual Entropy. From the information-theoretical point of view, we want to reduce the uncertainty
of the class variable Y by observing classiﬁer responses. Therefore, a natural way to de ﬁne the
reward is to consider the negative residual entropy, that is the lower the entropy the higher the
reward. Formally, given some posterior distribution P (Y |mO ) , we de ﬁne
R(P (Y |mO )) = −H (Y |mO ) = Xy
The value of classiﬁer under this reward de ﬁnition is closel
y related to information gain. Speciﬁcally,
VR (hi |mO ) =Emi∼P (Mi |mO ) (cid:2) − H (Y |mi , mO )(cid:3) + H (Y |mO ) − H (Y |mO )
=I (Y ; Mi |mO ) − H (Y |mO )
Since H (Y |mO ) is a constant w.r.t. hi , we have
h∗ = argmax
VR (hi |mO ) + VC (hi |mO ) = argmax
hi∈H/O
hi∈H/O

I (Y ; Mi |mO ) + VC (hi |mO )

P (y |mO ) log P (y |mO )

(5)

(3)

(4)

Therefore, at each step, we want to pick the classiﬁer with th e highest mutual information with the
class variable Y given the observed classiﬁer responses mO with a computational constraint.
Classiﬁcation Loss . From the classiﬁcation loss point of view, we want to minimi ze the expected
loss when choosing classiﬁers to evaluate. Therefore, give n a loss function ∆(y , y ′ ) specifying the

3

penalty of classifying an instance of class y to y ′ , we can de ﬁne the reward as the negative of the
minimum expected loss:

(6)

P (y |mO )∆(y , y ′ ) = − min
y ′

y ′ Xy
Ey∼P (Y |mO ) (cid:2)∆(y , y ′ )(cid:3)
R(P (Y |mO )) = − min
To gain some intuition about this de ﬁnition, consider a 0-1 l oss function, i.e., ∆(y , y ′ ) = 1{y 6= y ′},
then R(P (Y |mO )) = −1 + maxy ′ P (y ′ |mO ). To maximize R, we want the peak of P (Y |mO ) to
be as high as possible. In our experiment, these two reward de ﬁnitions give similar results.
Classiﬁcation Cost. The cost of evaluating a classiﬁer h on an instance x can be broken down into
two parts. The ﬁrst part is the cost of computing the feature φ : X → Rn on which h is built, and
the second is the cost of computing the function value of h given the input φ(x). If h shares the
same feature as some evaluated classiﬁers in O, then C (h|O) only consists of the cost of evaluating
the function h, otherwise it will also include the cost of computing the feature input φ. Note that
computing φ is usually much more expensive than evaluating the function value of h.
Probabilistic Model. Given a test instance x, we construct an instance-speciﬁc joint distribution
over Y and the selected observations MO . Our probabilistic model is a mixture model, where each
component corresponds to a class Y = y , and we use a uniform prior P (Y ). Starting from an empty
O, we model P (Mi , Y ) as a mixture of Gaussian distributions. At each step, given the selected MO ,
we model the new joint distribution P (Mi , MO , Y ) = P (Mi |MO , Y )P (MO , Y ) by modeling the
new P (Mi |MO , Y = y ) as a linear Gaussian, i.e., P (Mi |MO , Y = y ) = N (θT
y ). As we
MO , σ2
y
show in Section 5, this choice of probabilistic model works well empirically. We discuss how to
learn the distribution and do inference in the next section.

4 Learning and Inference

Learning P (Mi |mO , y ). Given the subset of the training set {(x(j) , y (j) = y )}Ny
j=1 corresponding
to the instances from class y, we denote m(j)
i = hi (x(j) ), then our goal is to learn P (Mi |mO , y )
from {(m(j) , y (j) = y )}Ny
j=1 . If O = ∅, then P (Mi |mO , y ) reduces to the marginal distribution
Ny Pj m(j)
y ), and based on maximum likelihood estimation, we have µy = 1
,
P (Mi |y ) = N (µy , σ2
i
Ny Pj (m(j)
y = 1
and σ2
If O 6= ∅, we assume that P (Mi |mO , y ) is a linear Gaus-
i − µy )2 .
mO . Note that we also append a constant 1 to mO as the bias term. Since
sian, i.e., µy = θT
y
y by maximizing the local likelihood with a Gaus-
we know mO at test time, we estimate θy and σ2
(j)
k2
kmO −m
O
β

sian prior on θy . Speciﬁcally, for each training instance j from class y , let wj = e−
where β is a bandwidth parameter, then the regularized local log likelihood is

,

L(θy , σy ; mO ) = −λ k θy k2
2 +

Ny
Xj=1

wj log N (m(j)
i

; θT
y

m

(j)
O , σ2
y )

(7)

where we overload the notation N (x; µy , σ2
y ) to mean the value of a Gaussian PDF with mean µy and
variance σ2
y evaluated at x. Note that maximizing (7) is equivalent to locally weighted regression [6]
with ℓ2 regularization. Maximizing (7) results in:
Ny
Xj=1

W ¯MO + λI)−1 ¯MT
O

ˆθy = argmin
θy

λ k θy k2
2 +

wj k m(j)
i − θT
y

m

(j)
2= ( ¯MT
O k2
O

W ¯Mi

(8)

(j)T
where ¯MO is a matrix whose j -th row is m
O , W is a diagonal matrix whose diagonal entries are
wj ’s , ¯Mi is an column vector whose j -th element is m(j)
, and I is an identity matrix. It is worth
i
noting that ( ¯MT
W ¯MO + λI)−1W in (8) does not depend on i, so it can be computed once and
O
shared for different classiﬁers hi ’s. Finally, the estimated σ2
y is

ˆσy

2 =

Ny
Xj=1

1
PNy
j=1 wj

wj k m(j)
i − ˆθy

T

m

(j)
O k2

(9)

4

(10)

Computing V (fi |mO ). Given the learned distribution, we can easily compute the two CPDs
in (1), i.e., P (Mi |mO ) and P (Y |mi , mO ). P (Mi |mO ) can be obtained as P (Mi |mO ) =
Py P (Mi |mO , y )P (y |mO ), where P (Y |mO ) is the posterior over Y given some observation
mO which is tracked over iterations. Speciﬁcally, P (Y |mi , mO ) ∝ P (mi , mO |Y )P (Y ) =
P (mi |mO , Y )P (mO |Y )P (Y ), where all terms are available by caching previous computations.
Finally, to compute V (fi |mO ), the computational part VC (fi |mO ) is just a lookup in a cost table,
and the expected reward part VR (fi |mO ) can be rewritten as:
VR (hi |mO ) = Xy
P (y |mO )Emi∼P (Mi |mO ,y)(cid:2)R(P (Y |mi , mO ))(cid:3)
Therefore, each component Emi∼P (Mi |mO ,y) (cid:2)R(P (Y |mi , mO ))(cid:3) is the expectation of a function
of a scalar Gaussian variable. We use Gaussian quadrature [18] 1 to approximate each component
expectation, and then do the weighted average to get VR (hi |mO ).
Dynamic Inference. Given the building blocks introduced before, one can execute the classiﬁcation
process in |H| steps, where at each step, the values of all the remaining classiﬁers are computed.
However, this will incur a large scheduling cost. This is due to the fact that usually |H| is large. For
example, in multiclass classiﬁcation, if we include all one -vs-one classiﬁers into H, |H| is quadratic
in the number of classes. Since we are maintaining a belief over Y as observations are accumulated,
we can use it to make the inference process more adaptive resulting in small scheduling cost.
Early Stopping. Based on the posterior P (Y |mO ), we can make dynamic and adaptive decision
about whether to continue observing new classiﬁers or stop t he process. We propose two stop-
ping criteria. We stop the inference process whenever either of them is met, and use the pos-
terior over Y at that point to make classiﬁcation decision. The ﬁrst crite
rion is based on the
information-theoretic point of view. Given the current posterior estimation P (Y |mi , mO ) and
the previous posterior estimation P (Y |mO ), the relative entropy (KL-divergence) between them
is D(cid:16)P (Y |mO ) k P (Y |mi , mO )(cid:17). We stop the inference procedure when this divergence is below
some threshold t. The second criterion is based on the classiﬁcation point of view. We consider the
gap between the probability of the current best class and that of the runner-up. Speciﬁcally, we de ﬁne
the margin given a posterior P (Y |mO ) as δm (P (Y |mO )) = P (y∗ |mO ) − maxy 6=y∗ P (Y |mO ),
where y∗ = argmaxy P (y |mO ). If δm (P (Y |mO )) ≥ t′ , then the inference stops.
Dynamic Pruning of Class Space. In many cases, a class is mainly confused with a small number of
other classes (the confusion matrix is often close to sparse). This implies that after observing a few
classiﬁers, the posterior P (Y |mO ) is very likely to be dominated by a few modes leaving the rest
with very small probability. For those classes y with very small P (y |mO ), their contributions to the
value of classiﬁer (10) are negligible. Therefore, when com puting (10), we ignore the components
whose P (y |mO ) is below some small threshold (equivalent to setting the contribution from this
component to 0). Furthermore, when P (y |mO ) falls below some very small threshold for a class y ,
we will not estimate the likelihood related to y , i.e., P (Mi |mO , y ), but use a small constant.
Dynamic Classiﬁer Space. To avoid computing the values of all the remaining classiﬁer s, we can
dynamically restrict the search space of classiﬁers to thos e having high expected mutual informa-
tion with Y with respect to the current posterior P (Y |mO ). Speciﬁcally, during the training, for
each classiﬁer hi we can compute the mutual information I (Mi ; By ) between its response Mi and
a class y , where By is a binary variable indicating whether an instance is from class y or not.
Given our current posterior P (Y |mO ), we tried two ways to rank the unobserved classiﬁers. First,
we simply select the top L classiﬁers with the highest
I (Mi ; B ˆy ), where ˆy is the most probable
class based on current posterior. Since we can sort classiﬁe rs in the training stage, this step is con-
stant time. Another way is that for each classiﬁer, we can com pute a weighted mutual information
score, i.e., Py P (y |mO )I (Mi ; By ), and we restrict the classiﬁer space to those with the top L
scores. Note that computing the scores is very efﬁcient, sin ce it is just an inner product between two
vectors, where I (Y ; By )’s have been computed and cached before testing. Our experiments showed
that these two scores have similar performances, and we used the ﬁrst method to report the results.
Analysis of Time Complexity. At each iteration t, the scheduling overhead includes selecting
the top L candidate observations, and for each candidate i, learning P (Mi |mO , y ) and computing

1We found that 3 or 5 points provide an accurate approximation.

5

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

 
0

results on satimage dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree

 

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

5
10
number of evaluated classifiers

15

 
0

5

results on pendigits dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
20
10
35
30
25
15
number of evaluated classifiers

40

 

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

45

0.2

 
0

5

10

results on vowel dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
25
15
40
35
30
20
number of evaluated classifiers

45

50

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

55

 
0.1
100

results on letter dataset

 

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
101
102
number of evaluated classifiers

Figure 1: (Best viewed magniﬁed and in colors) Performance c omparisons on UCI datasets. From
the left to right are the results on satimage, pendigits, vowel and letter (in log-scale) datasets. Note
that the error bars for pendigits and letter datasets are very small (around 0.5% on average).

V (fi |mO ). First, selecting the top L candidate observations is a constant time, since we can sort
the observations based on I (Mi ; By ) before the test process. Second, estimating P (Mi |mO , y )
requires computing (8) and (9) for different y ’s. Given our dynamic pruning of class space, suppose
there are only Nt,Y promising classes to consider instead of the total number of classes K . Since
( ¯MT
W ¯MO + λI)−1W in (8) does not depend on i, we compute it for each promising class, which
O
y + t2Ny + t3 ) ﬂoating point operations, and share it for different
takes O(tN 2
i’s. After computing
this shared component, for each pair of i and a promising class, computing (8) and (9) both take
t,Y ). Putting everything together, the overall cost at
O(tNy ). Finally, computing (10) takes O(N 2
t,Y ). The key to have a low cost is to
iteration t is O(Nt,Y (tN 2
y + t2Ny + t3 ) + LNt,Y tNy + LN 2
effectively prune the class space (small Nt,Y ) and reach a decision quickly (small t).

5 Experimental Results

We performed experiments on a collection of four UCI datasets [25] and on a scene recognition
dataset [20]. All tasks are multiclass classiﬁcation probl ems. The ﬁrst set of experiments focuses on
a single feature type and aims to show that (i) our probabilistic model is able to combine multiple
binary classiﬁers to achieve comparable or higher classiﬁc
ation accuracy than traditional methods;
(ii) our active evaluation strategy successfully selects a signiﬁcantly fewer number of classiﬁers. The
second set of experiments considers multiple features, with varying computational complexities.
This experiment shows the real power of our active scheme. Speciﬁcally, it dynamically selects an
instance-speciﬁc subset of features, resulting in higher c lassiﬁcation accuracy of using all features
but with a signiﬁcant reduction in the computational cost.
Basic Setup. Given a feature φ, our set of classiﬁers Hφ consists of all one-vs-one classiﬁers, all
one-vs-all classiﬁers, and all node classiﬁers from a tree-
based method [13], where a node classiﬁer
can be trained to distinguish two arbitrary clusters of classes. Therefore, for a K -class problem, the
|Hφ | = (K−1)K
number of classiﬁers given a single feature is
+ K + Nφ,tree , where Nφ,tree is the
2
i=1 , our pool of classiﬁers is
number of nodes in the tree model. If there are multiple features {φi }F
i=1Hφi . The form of all classiﬁers is linear SVM for the ﬁrst set of ex
periments and nonlinear
H = ∪F
SVM with various kernels for the second set of experiments. During training, in addition to learning
the classiﬁers, we also need to compute the response m(j)
i of each classiﬁer hi ∈ H for each training
instance x(j) . In order to make the training distribution of the classiﬁer
responses better match the
test distribution, when evaluating classiﬁer hi on x(j) , we do not want hi to be trained on x(j) . To
achieve this, we use a procedure similar to cross validation. Speciﬁcally, we split the training set
into 10 folds, and for each fold, instances from this fold are tested using the classiﬁers trained on the
other 9 folds. After this procedure, each training instance x(j) will be evaluated by all hi ’s. Note
that the classiﬁers used in the test stage are trained on the e ntire training set. Although for different
, m(k)
training instances x(j) and x(k) from different folds and a test instance x, m(j)
and mi are
i
i
obtained using different hi ’s, our experimental results con ﬁrmed that their empirical distributions
are close enough to achieve good performance.
Standard Multiclass Problems from UCI Repository. The ﬁrst set of experiments are done on
four standard multiclass problems from the UCI machine learning repository [25]: vowel (speech
recognition, 11 classes), letter (optical character recognition, 26 classes), satimage (pixel-based clas-
siﬁcation/segmentation on satellite images, 6 classes) and pendigits (hand written digits recognition,

6

10 classes). We used the same training/test split as speciﬁed i n the UCI repository. For each dataset,
there is only one type of feature, so it will be computed at the ﬁrst step no matter which classiﬁer
is selected. After that, all classiﬁers have the same comple xity, so the results will be independent
of the τ parameter in the de ﬁnition of value of classiﬁer (1). For the
baselines, we have one-vs-one
with max win, one-vs-all, DAGSVM [27] and a tree-based method [13]. These methods vary both
in terms of what set of classiﬁers they use and how those class iﬁers are evaluated and combined.
To evaluate the effectiveness of our classiﬁer selection sc heme, we introduce another baseline that
selects classiﬁers randomly, for which we repeated the expe riments for 10 times and the average and
one standard deviation are reported. We compare different methods in terms of both the classiﬁca-
tion accuracy and the number of evaluated classiﬁers. For ou r algorithm and the random selection
baseline, we show the accuracy over iterations as well. Since in our framework the number of it-
erations (classiﬁers) needed varies over instances due to e arly stopping, the maximum number of
iterations shown is de ﬁned as the mean plus one standard deri vation of the number of classiﬁer
evaluations of all test instances. In addition, for the tree-based method, the number of evaluated
classiﬁers is the mean over all test instances.

Figure 1 shows a set of results. As can be seen, our method can achieve comparable or higher
accuracy than traditional methods. In fact, we achieved the best accuracy on three datasets and
the gains over the runner-up methods are 0.2%, 5.2%, 8.2% for satimage, vowel, and letter datasets
respectively. We think the statistical gain might come from two facts: (i) we are performing instance-
speciﬁc “feature selection ” to only consider those most inf
ormative classiﬁers; (ii) another layer of
probabilistic model is used to combine the classiﬁers inste ad of the uniform voting of classiﬁers used
by many traditional methods. In terms of the number of evaluated classiﬁers, our active scheme is
very effective: the mean number of classiﬁer evaluations fo r 6-class, 10-class, 11-class and 26-class
problems are 4.50, 3.22, 6.15 and 7.72. Although the tree-based method can also use a few number
of classiﬁers, sometimes it suffers from a signiﬁcant drop i
n accuracy like on the vowel and letter
datasets. Furthermore, compared to the random selection scheme, our method can effectively select
more informative classiﬁers resulting in faster convergen ce to a certain classiﬁcation accuracy.

The performance gain of our method is not free. To maintain a belief over the class variable Y
and to dynamically select classiﬁers with high value, we hav e introduced additional computational
costs, i.e., estimating conditional distributions and computing the value of classiﬁers. For example,
this additional cost is around 10ms for satimage, however, evaluating a linear classiﬁer onl y takes
less than 1ms due to very low feature dimension, so the actual running time of the active scheme
is higher than one-vs-one. Therefore, our method will have a real computational advantage only
if the cost of evaluating the classiﬁers is higher than the co st of our probabilistic inference. We
demonstrate such bene ﬁt of our method in the context of multi ple high dimensional features below.
Scene Recognition. We test our active classiﬁcation on a benchmark scene recogn ition dataset
Scene15 [20]. It has 15 scene classes and 4485 images in total. Following the protocol used in
[20, 21], 100 images per class are randomly sampled for training and the remaining 2985 for test.

model

all features
best feature OB [21]
fastest feature GIST [26]
ours τ = 25
ours τ = 100
ours τ = 600

accuracy

feature cost
(# of features)
86.40% 52.645s (184)
83.38% 6.20s
72.70% 0.399s
86.26% 1.718s (5.62)
86.77% 6.573s (4.71)
88.11% 19.821s (4.46)

classiﬁer
cost
0.426s
0.024s
0.0002s
0.010s
0.014s
0.031s

scheduling
cost
0
0
0
0.141s
0.116s
0.094s

total
running time
53.071s
6.224s
0.3992s
1.869s (28.4x)
6.703s (7.9x)
19.946s (2.7x)

Table 1: Detailed performance comparisons on Scene15 dataset with various feature types. For our
methods, we show the speedup factors with respective to using all the features in a static way.
We consider various types of features, since as shown in [33], the classiﬁcation accuracy can be
signiﬁcantly improved by combining multiple features but a t a high computational cost. Our feature
set includes 7 features from [33], including GIST, spatial HOG, dense SIFT, Local Binary Pattern,
self-similarity, texton histogram, geometry speciﬁc hist ograms (please refer to [33] for details), and
another recently proposed high-level image feature Object Bank [21]. The basic idea of Object
Bank is to use the responses of various object detectors as the feature. The current release of the
code from the authors selected 177 object detectors, each of which outputs a feature vector φi with

7

dimension 252. These individual vectors are concatenated together to form the ﬁnal feature vector
Instead of treating Φ as an undecomposable single feature
Φ = [φ1 ; φ2 ; . . . ; φ177 ] ∈ R44,604 .
vector, we can think of it as a collection of 177 different features {φi}177
i=1 . Therefore, our feature
pool consists of 184 features in total. Their computational costs vary from 0.035 to 13.796 seconds,
with the accuracy from 54% to 83%. One traditional way to combine these features is through
multiple kernel learning. Speciﬁcally, we take the average of individual kernels constructed based
on individual features, and train a one-vs-all SVM using the joint average kernel. Surprisingly, this
simple average kernel performs comparably with learning the weights to combine them [12].

For our active classiﬁcation, we will not compute all featur es at the beginning of the evaluation
process, but will only compute a component φi when a classiﬁer h based on it is selected. We
will cache all evaluated φi ’s, so different classiﬁers sharing the same φi will not induce repeated
computation of the common φi . We decompose the computational costs per instance into three
parts: (1) the feature cost, which is the time spent on computing the features; (2) the classiﬁer cost,
which is the time spent on evaluating the function value of the classiﬁers; (3) the scheduling cost,
which is the time spent on selecting the classiﬁers using our method. To demonstrate the trade-off
between the accuracy and computational cost in the de ﬁnitio n of value of classiﬁer, we run multiple
experiments with various τ ’s.

 

0.85

0.8

0.75

0.7

0.9

results on scene15 dataset

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

The results are shown in Table 1. We also report
comparisons to the best individual features in terms
of either accuracy or speed (the reported accuracy
is the best of one-vs-one and one-vs-all). As can
be seen, combining all features using the traditional
method indeed improves the accuracy signiﬁcantly
over those individual features, but at an expensive
computational cost. However, using active classiﬁ-
cation, to achieve similar accuracy as the baseline
of all features, we can get 28.4x speedup (τ = 25).
Note that at this con ﬁguration, our method is faster
than the state-of-the-art individual feature [21], and
is also 2.8% better in accuracy. Furthermore, if we
put more emphasis on the accuracy, we can get the
best accuracy 88.11% when τ = 600.
To further test the effectiveness of our active selec-
tion scheme, we compare with another baseline that
sequentially adds one feature at a time from a ﬁltered
pool of features. Speciﬁcally, we ﬁrst rank the individual f
eatures based on their classiﬁcation accu-
racy, and only consider the top 80 features (using 80 features achieves essentially the same accuracy
as using 184 features). Given this selected pool, we arrange the features in order of increasing com-
putational complexity, and then train a classiﬁer based on t he top N features for all values of N from
1 to 80. As shown in Figure 2, our active scheme is one order of magnitude faster than the baseline
given the same level of accuracy.

sequentially adding features
active classification
GIST
LBP
spatial HOG
Object Bank
dense SIFT
25
15
35
30
20
running time (seconds)

Figure 2: Classiﬁcation accuracy versus run-
ning time for the baseline, active classiﬁca-
tion, and various individual features.

40

45

50

5

10

0.65

 
0

6 Conclusion and Future Work

In this paper, we presented an active classiﬁcation process based on the value of classiﬁer. We ap-
plied this active scheme in the context of multiclass classiﬁcation, and achieved comparable and
even higher classiﬁcation accuracy with signiﬁcant comput
ational savings compared to traditional
static methods. One interesting future direction is to estimate the value of features instead of individ-
ual classiﬁers. This is particularly important when comput ing the feature is much more expensive
than evaluating the function value of classiﬁers, which is o ften the case. Once a feature has been
computed, a set of classiﬁers that are built on it will be chea p to evaluate. Therefore, predicting the
value of the feature (equivalent to the joint value of multiple classiﬁers sharing the same feature) can
potentially lead to more computationally efﬁcient classiﬁ
cation process.
Acknowledgment. This work was supported by the NSF under grant No. RI-0917151, the Ofﬁce of
Naval Research MURI grant N00014-10-10933, and the Boeing company. We thank Pawan Kumar
and the reviewers for helpful feedbacks.

8

References
[1] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing multiclass to binary: a unifying approach for
margin classi ﬁers.
J. Mach. Learn. Res., 1:113–141, 2001.
[2] A. Angelova, L. Matthies, D. Helmick, and P. Perona. Fast terrain classi ﬁcation using variable-length
representation for autonomous navigation. CVPR, 2007.
[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multiclass task. In NIPS, 2010.
[4] L. Breiman. Random forests. In Machine Learning, pages 5–32, 2001.
[5] X. Chai, L. Deng, and Q. Yang. Test-cost sensitive naive bayes classi ﬁcation. In ICDM, 2004.
[6] W. S. Cleveland and S. J. Devlin. Locally weighted regression: An approach to regression analysis by
local ﬁtting.
Journal of the American Statistical Association, 83:596–610, 1988.
[7] D.A. Cohn, Zoubin Ghahramani, and M.I. Jordan. Active learning with statistical models. CoRR,
cs.AI/9603104, 1996.
[8] J. Deng, A.C. Berg, K. Li, and L. Fei-Fei. What does classifying more than 10,000 image categories tell
us? In ECCV10, pages V: 71–84, 2010.
[9] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. J.
of A. I. Res., 2:263–286, 1995.
[10] Y. Freud. Boosting a weak learning algorithm by majority. In Computational Learning Theory, 1995.
[11] Jerome H. Friedman. Another approach to polychotomous classi ﬁcation. Technical report, Department
of Statistics, Stanford University, 1996.
[12] P.V. Gehler and S. Nowozin. On feature combination for multiclass object classi ﬁcation. In ICCV, 2009.
[13] G. Grifﬁn and P. Perona. Learning and using taxonomies f or fast visual categorization. In CVPR, 2008.
[14] V. Guruswami and A. Sahai. Multiclass learning, boosting, and error-correcting codes. In Proc. of the
Twelfth Annual Conf. on Computational Learning Theory, 1999.
[15] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of statistical learning: data mining, inference,
and prediction. 2009.
[16] R. A. Howard. Information value theory. IEEE Trans. on Systems Science and Cybernetics, 1966.
[17] R. A. Howard. Decision analysis: Practice and promise. Management Science, 1988.
[18] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press,
2009.
[19] A. Krause and C. Guestrin. Optimal value of information in graphical models. Journal of Arti ﬁcial
Intelligence Research (JAIR), 35:557–591, 2009.
[20] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing
natural scene categories. In CVPR, 2006.
[21] L.-J. Li, H. Su, E.P. Xing, and L. Fei-Fei. Object bank: A high-level image representation for scene
classi ﬁcation and semantic feature sparsi ﬁcation. In
NIPS, 2010.
[22] D. V. Lindley. On a Measure of the Information Provided by an Experiment. The Annals of Mathematical
Statistics, 27(4):986–1005, 1956.
[23] D.G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.
[24] V.S. Mookerjee and M.V. Mannino. Sequential decision models for expert system optimization. IEEE
Trans. on Knowledge & Data Engineering, (5):675.
[25] D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. Uci repository of machine learning databases, 1998.
[26] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the
spatial envelope. IJCV, 2001.
[27] J.C. Platt, N. Cristianini, and J. Shawe-taylor. Large margin dags for multiclass classi ﬁcation. In NIPS,
2000.
[28] M.J. Saberian and N. Vasconcelos. Boosting classi ﬁer c ascades. In NIPS, 2010.
[29] Robert E. Schapire. Using output codes to boost multiclass learing problems. In ICML, 1997.
[30] G. A. Schwing, C. Zach, Zheng Y., and M. Pollefeys. Adaptive random forest - how many “experts” to
ask before making a decision? In CVPR, 2011.
[31] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV,
2009.
[32] P. Viola and M. Jones. Robust Real-time Object Detection. IJCV, 2002.
[33] J.X. Xiao, J. Hays, K.A. Ehinger, A. Oliva, and A.B. Torralba. Sun database: Large-scale scene recogni-
tion from abbey to zoo. In CVPR, 2010.
[34] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML,
pages 412–420, 1997.

9


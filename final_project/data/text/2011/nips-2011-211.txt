Kernel Bayes’ Rule

Kenji Fukumizu
The Institute of Statistical
Mathematics, Tokyo
fukumizu@ism.ac.jp

Le Song
College of Computing
Georgia Institute of Technology
lsong@cc.gatech.edu

Arthur Gretton
Gatsby Unit, UCL
MPI for Intelligent Systems
arthur.gretton@gmail.com

Abstract

A nonparametric kernel-based method for realizing Bayes’ rule is proposed, based
on kernel representations of probabilities in reproducing kernel Hilbert spaces.
The prior and conditional probabilities are expressed as empirical kernel mean
and covariance operators, respectively, and the kernel mean of the posterior dis-
tribution is computed in the form of a weighted sample. The kernel Bayes’ rule
can be applied to a wide variety of Bayesian inference problems: we demonstrate
Bayesian computation without likelihood, and ﬁltering wit h a nonparametric state-
space model. A consistency rate for the posterior estimate is established.

1

Introduction

Kernel methods have long provided powerful tools for generalizing linear statistical approaches to
nonlinear settings, through an embedding of the sample to a high dimensional feature space, namely
a reproducing kernel Hilbert space (RKHS) [16]. The inner product between feature mappings need
never be computed explicitly, but is given by a positive deﬁn ite kernel function, which permits efﬁ-
cient computation without the need to deal explicitly with the feature representation. More recently,
the mean of the RKHS feature map has been used to represent probability distributions, rather than
mapping single points: we will refer to these representations of probability distributions as ker-
nel means. With an appropriate choice of kernel, the feature mapping becomes rich enough that
its expectation uniquely identi ﬁes the distribution: the a ssociated RKHSs are termed characteristic
[6, 7, 22]. Kernel means in characteristic RKHSs have been applied successfully in a number of
statistical tasks, including the two sample problem [9], independence tests [10], and conditional in-
dependence tests [8]. An advantage of the kernel approach is that these tests apply immediately to
any domain on which kernels may be deﬁned.

,

We propose a general nonparametric framework for Bayesian inference, expressed entirely in terms
of kernel means. The goal of Bayesian inference is to ﬁnd the p osterior of x given observation y ;
qY (y) = Z p(y |x)π(x)dµX (x),
p(y |x)π(x)
q(x|y) =
qY (y)
where π(x) and p(y |x) are respectively the density function of the prior, and the conditional density
or likelihood of y given x. In our framework, the posterior, prior, and likelihood are all expressed
as kernel means: the update from prior to posterior is called the Kernel Bayes’ Rule (KBR). To
implement KBR, the kernel means are learned nonparametrically from training data: the prior and
likelihood means are expressed in terms of samples from the prior and joint probabilities, and the
posterior as a kernel mean of a weighted sample. The resulting updates are straightforward matrix
operations. This leads to the main advantage of the KBR approach: in the absence of a speci ﬁc para-
metric model or an analytic form for the prior and likelihood densities, we can still perform Bayesian
inference by making sufﬁcient observations on the system. A lternatively, we may have a paramet-
ric model, but it might be complex and require time-consuming sampling techniques for inference.
By contrast, KBR is simple to implement, and is amenable to well-established approximation tech-
niques which yield an overall computational cost linear in the training sample size [5]. We further

(1)

1

establish the rate of consistency of the estimated posterior kernel mean to the true posterior, as a
function of training sample size.

The proposed kernel realization of Bayes’ rule is an extension of the approach used in [20] for
state-space models. This earlier work applies a heuristic, however, in which the kernel mean of the
previous hidden state and the observation are assumed to combine additively to update the hidden
state estimate. More recently, a method for belief propagation using kernel means was proposed
[18, 19]: unlike the present work, this directly estimates conditional densities assuming the prior
to be uniform. An alternative to kernel means would be to use nonparametric density estimates.
Classical approaches include ﬁnite distribution estimate s on a partitioned domain or kernel density
estimation, which perform poorly on high dimensional data. Alternatively, direct estimates of the
density ratio may be used in estimating the conditional p.d.f. [24]. By contrast with density estima-
tion approaches, KBR makes it easy to compute posterior expectations (as an RKHS inner product)
and to perform conditioning and marginalization, without requiring numerical integration.

2 Kernel expression of Bayes’ rule

2.1 Positive deﬁnite kernel and probabilities

We begin with a review of some basic concepts and tools concerning statistics on RKHS [1, 3, 6, 7].
Given a set Ω, a (R-valued) positive deﬁnite kernel k on Ω is a symmetric kernel k : Ω×Ω → R such
that Pn
i,j=1 ci cj k(xi , xj ) ≥ 0 for arbitrary points x1 , . . . , xn in Ω and real numbers c1 , . . . , cn . It is
known [1] that a positive deﬁnite kernel on Ω uniquely deﬁnes a Hilbert space H (RKHS) consisting
of functions on Ω, where hf , k(·, x)i = f (x) for any x ∈ Ω and f ∈ H (reproducing property).
Let (X , BX , µX ) and (Y , BY , µY ) be measure spaces, and (X, Y ) be a random variable on X ×
Y with probability P . Throughout this paper, it is assumed that positive deﬁnite kernels on the
measurable spaces are measurable and bounded, where boundedness is deﬁned as supx∈Ω k(x, x) <
∞. Let kX be a positive deﬁnite kernel on a measurable space (X , BX ), with RKHS HX . The kernel
mean mX of X on HX is deﬁned by the mean of the HX -valued random variable kX (·, X ), namely
mX = Z kX (·, x)dPX (x).
(2)
For notational simplicity, the dependence on kX in mX is not shown. Since the kernel mean depends
only on the distribution of X (and the kernel), it may also be written mPX ; we will use whichever
of these equivalent notations is clearest in each context. From the reproducing property, we have
(3)
hf , mX i = E [f (X )]
(∀f ∈ HX ).
Let kX and kY be positive deﬁnite kernels on X and Y with respective RKHS HX and HY . The
(uncentered) covariance operator CY X : HX → HY is deﬁned by the relation
( = hg ⊗ f , m(Y X ) iHY ⊗HX )
(∀f ∈ HX , g ∈ HY ).
hg , CY X f iHY = E [f (X )g(Y )]
It should be noted that CY X is identi ﬁed with the mean m(Y X ) in the tensor product space HY ⊗HX ,
which is given by the product kernel kY kX [1]. The identi ﬁcation is standard: the tensor product is
isomorphic to the space of linear maps by the correspondence ψ ⊗ φ ↔ [h 7→ ψhφ, hi]. We also
deﬁne CXX : HX → HX by hf2 , CXX f1 i = E [f2 (X )f1 (X )] for any f1 , f2 ∈ HX .
We next introduce the notion of a characteristic RKHS, which is essential when using kernels to ma-
nipulate probability measures. A bounded measurable positive deﬁnite kernel k is called character-
istic if EX∼P [k(·, X )] = EX ′∼Q [k(·, X ′ )] implies P = Q: probabilities are uniquely determined
by their kernel means [7, 22]. With this property, problems of statistical inference can be cast in
terms of inference on the kernel means. A widely used characteristic kernel on Rm is the Gaussian
kernel, exp(−kx − yk2 /(2σ2 )).
Empirical estimates of the kernel mean and covariance operator are straightforward to obtain. Given
an i.i.d. sample (X1 , Y1 ), . . . , (Xn , Yn ) with law P , the empirical kernel mean and covariance op-
erator are respectively
nXi=1
nXi=1
1
1
bm(n)
bC (n)
X =
kY (·, Yi ) ⊗ kX (·, Xi ),
Y X =
kX (·, Xi ),
n
n
Y X is written in the tensor product form. These are known to be √n-consistent in norm.
where bC (n)

2

2.2 Kernel Bayes’ rule

We now derive the kernel mean implementation of Bayes’ rule. Let Π be a prior distribution on
X with p.d.f. π(x).
In the following, Q and QY denote the probabilities with p.d.f. q(x, y) =
p(y |x)π(x) and qY (y) in Eq. (1), respectively. Our goal is to obtain an estimator of the kernel
mean of posterior mQX |y = R kX (·, x)q(x|y)dµX (x). The following theorem is fundamental in
manipulating conditional probabilities with positive deﬁ nite kernels.
Theorem 1 ([6]). If E [g(Y )|X = ·] ∈ HX holds for g ∈ HY , then
CXX E [g(Y )|X = ·] = CX Y g .
If CXX is injective, the above relation can be expressed as
−1CX Y g .
E [g(Y )|X = ·] = CXX
Using Eq. (4), we can obtain an expression for the kernel mean of QY .
Theorem 2 ([20]). Assume CXX is injective, and let mΠ and mQY be the kernel means of Π in HX
and QY in HY , respectively. If mΠ ∈ R(CXX ) and E [g(Y )|X = ·] ∈ HX for any g ∈ HY , then
−1mΠ .
(5)
mQY = CY X CXX

(4)

As discussed in [20], the operator CY X C −1
XX implements forward ﬁltering of the prior π with the
conditional density p(y |x), as in Eq. (1). Note, however, that the assumptions E [g(Y )|X = ·] ∈
HX and injectivity of CXX may not hold in general; we can easily provide counterexamples. In
the following, we nonetheless derive a population expression of Bayes’ rule under these strong
assumptions, use it as a prototype for an empirical estimator expressed in terms of Gram matrices,
and prove its consistency subject to appropriate smoothness conditions on the distributions.

In deriving kernel realization of Bayes’ rule, we will also use Theorem 2 to obtain a kernel mean
representation of the joint probability Q:
mQ = C(Y X )X C −1
(6)
XX mΠ ∈ HY ⊗ HX .
In the above equation, C(Y X )X is the covariance operator from HX to HY ⊗ HX with
p.d.f. ˜p((y , x), x′ ) = p(x, y)δx (x′ ), where δx (x′ ) is the point measure at x.
In many applications of Bayesian inference, the probability conditioned on a particular value should
be computed. By plugging the point measure at x into Π in Eq. (5), we have a population expression
−1kX (·, x),
(7)
E [kY (·, Y )|X = x] = CY X CXX
which was used by [20, 18, 19] as the kernel mean of the conditional probability p(y |x). Let (Z, W )
be a random variable on X × Y with law Q. Replacing P by Q and x by y in Eq. (7), we obtain
E [kX (·, Z )|W = y ] = CZW C −1
(8)
W W kY (·, y).
This is exactly the kernel mean of the posterior which we want to obtain. The next step is to derive
the covariance operators in Eq. (8). Recalling that the mean mQ = m(ZW ) ∈ HX ⊗ HY can be
identi ﬁed with the covariance operator CZW : HY → HX , and m(W W ) ∈ HY ⊗ HY with CW W ,
we use Eq. (6) to obtain the operators in Eq. (8), and thus the kernel mean expression of Bayes’ rule.

The above argument can be rigorously implemented for empirical estimates of the kernel means and
covariances. Let (X1 , Y1 ), . . ., (Xn , Yn ) be an i.i.d. sample with law P , and assume a consistent
estimator for mΠ given by
ℓXj=1
bm(ℓ)
γj kX (·, Uj ),
Π =
where U1 , . . . , Uℓ is the sample that deﬁnes the estimator (which need not be gen erated by Π), and
γj are the weights. Negative values are allowed for γj . The empirical estimators for CZW and
CW W are identi ﬁed with bm(ZW ) and bm(W W ) , respectively. From Eq. (6), they are given by
XX + εn I (cid:1)−1
XX + εn I (cid:1)−1
(Y Y )X (cid:0) bC (n)
(Y X )X (cid:0) bC (n)
bmQ = bm(ZW ) = bC (n)
bm(W W ) = bC (n)
bm(ℓ)
bm(ℓ)
Π ,
Π ,
where I is the identity and εn is the coefﬁcient of Tikhonov regularization for operator i nversion.
The next two propositions express these estimators using Gram matrices. The proofs are simple
matrix manipulation and shown in Supplementary material. In the following, GX and GY denote
the Gram matrices (kX (Xi , Xj )) and (kY (Yi , Yj )), respectively.

3

Input: (i) {(Xi , Yi )}n
i=1 : sample to express P . (ii) {(Uj , γj )}ℓ
j=1 : weighted sample to express the kernel
mean of the prior bmΠ . (iii) εn , δn : regularization constants.
Computation:
1. Compute Gram matrices GX = (kX (Xi , Xj )), GY = (kY (Yi , Yj )), and a vector bmΠ =
(Pℓ
i=1 ∈ Rn .
j=1 γj kX (Xi , Uj ))n
2. Compute bµ = n(GX + nεn In )−1 bmΠ .
3. Compute RX |Y = ΛGY ((ΛGY )2 + δn In )−1Λ, where Λ = Diag(bµ).
Output: n × n matrix RX |Y .
Given conditioning value y , the kernel mean of the posterior q(x|y) is estimated by the weighted
sample {(Xi , wi )}n
i=1 with w = RX |Y kY (y), where kY (y) = (kY (Yi , y))n
i=1 .

Figure 1: Kernel Bayes’ Rule Algorithm

(9)

Proposition 3. The Gram matrix expressions of bCZW and bCW W are given by
bCZW = Pn
i=1 bµikX (·, Xi ) ⊗ kY (·, Yi ) and bCW W = Pn
i=1 bµikY (·, Yi ) ⊗ kY (·, Yi ),
respectively, where the common coefﬁcient bµ ∈ Rn is
bmΠ,i = bmΠ (Xi ) = Pℓ
bµ = n(GX + nεn In )−1 bmΠ ,
j=1γj kX (Xi , Uj ).
Prop. 3 implies that
the probabilities Q and QY are estimated by the weighted samples
i=1 , respectively, with common weights. Since the weights bµi
i=1 and {(Yi , bµi )}n
{((Xi , Yi ), bµi )}n
may be negative, we use another type of Tikhonov regularization in computing Eq. (8),
W W + δn I (cid:1)−1 bCW W kY (·, y).
bmQX |y := bCZW (cid:0) bC 2
(10)
Proposition 4. For any y ∈ Y , the Gram matrix expression of bmQX |y is given by
bmQX |y = kT
RX |Y := ΛGY ((ΛGY )2 + δn In )−1Λ,
(11)
X RX |Y
kY (y),
where Λ = Diag(bµ) is a diagonal matrix with elements bµi given by Eq. (9), kX =
n .
n , and kY = (kY (·, Y1 ), . . . , kY (·, Yn ))T ∈ HY
(kX (·, X1 ), . . . , kX (·, Xn ))T ∈ HX
We call Eqs.(10) or (11) the kernel Bayes’ rule (KBR): i.e., the expression of Bayes’ rule entirely
in terms of kernel means. The algorithm to implement KBR is summarized in Fig. 1. If our aim
is to estimate E [f (Z )|W = y ], that is, the expectation of a function f ∈ HX with respect to the
posterior, then based on Eq. (3) an estimator is given by
hf , bmQX |y iHX = f T
(12)
X RX |Y
kY (y),
where fX = (f (X1 ), . . . , f (Xn ))T ∈ Rn . In using a weighted sample to represent the posterior,
KBR has some similarity to Monte Carlo methods such as importance sampling and sequential
Monte Carlo ([4]). The KBR method, however, does not generate samples from the posterior, but
updates the weights of a sample via matrix operations. We will provide experimental comparisons
between KBR and sampling methods in Sec. 4.1.

2.3 Consistency of KBR estimator

We now demonstrate the consistency of the KBR estimator in Eq. (12). We show only the best rate
that can be derived under the assumptions, and leave more detailed discussions and proofs to the
Supplementary material. We assume that the sample size ℓ = ℓn for the prior goes to inﬁnity as the
sample size n for the likelihood goes to inﬁnity, and that bm(ℓn )
is nα -consistent. In the theoretical
Π
results, we assume all Hilbert spaces are separable. In the following, R(A) denotes the range of A.
Theorem 5. Let f ∈ HX , (Z, W ) be a random vector on X × Y such that its law is Q with
p.d.f. p(y |x)π(x), and bm(ℓn )
be an estimator of mΠ such that k bm(ℓn )
Π − mΠ kHX = Op (n−α ) as
Π
n → ∞ for some 0 < α ≤ 1/2. Assume that π/pX ∈ R(C 1/2
XX ), where pX is the p.d.f. of PX , and
W W ). For εn = n− 2
3 α and δn = n− 8
E [f (Z )|W = ·] ∈ R(C 2
27 α , we have for any y ∈ Y
kY (y) − E [f (Z )|W = y ] = Op (n− 8
f T
27 α ),
X RX |Y
(n → ∞),
kY (y) is the estimator of E [f (Z )|W = y ] given by Eq. (12).

where f T
X RX |Y

4

The condition π/pX ∈ R(C 1/2
XX ) requires the prior to be smooth. If ℓn = n, and if bm(n)
Π is a direct
empirical kernel mean with an i.i.d. sample of size n from Π, typically α = 1/2 and the theorem
implies n4/27 -consistency. While this might seem to be a slow rate, in practice the convergence may
be much faster than the above theoretical guarantee.

3 Bayesian inference with Kernel Bayes’ Rule

In Bayesian inference, tasks of interest include ﬁnding pro perties of the posterior (MAP value,
moments), and computing the expectation of a function under the posterior. We now demonstrate
the use of the kernel mean obtained via KBR in solving these problems.

First, we have already seen from Theorem 5 that we may obtain a consistent estimator under the pos-
terior for the expectation of some f ∈ HX . This covers a wide class of functions when characteristic
kernels are used (see also experiments in Sec. 4.1).
Next, regarding a point estimate of x, [20] proposes to use the preimage bx = arg minx kkX (·, x) −
, which represents the posterior mean most effectively by one point. We use
kT
kY (y)k2
X RX |Y
HX
this approach in the present paper where point estimates are considered. In the case of the Gaussian
kernel, a ﬁxed point method can be used to sequentially optim ize x [13].
In KBR the prior and likelihood are expressed in terms of samples. Thus unlike many methods for
Bayesian inference, exact knowledge on their densities is not needed, once samples are obtained.
The following are typical situations where the KBR approach is advantageous:
• The relation among variables is difﬁcult to realize with a si mple parametric model, however we
can obtain samples of the variables (e.g. nonparametric state-space model in Sec. 3).
• The p.d.f of the prior and/or likelihood is hard to obtain explicitly, but sampling is possible: (a) In
population genetics, branching processes are used for the likelihood to model the split of species,
for which the explicit density is hard to obtain. Approximate Bayesian Computation (ABC)
is a popular sampling method in these situations [25, 12, 17]. (b) In nonparametric Bayesian
inference (e.g. [14]), the prior is typically given in the form of a process without a density.
The KBR approach can give alternative ways of Bayesian computation for these problems. We
will show some experimental comparisons between KBR approach and ABC in Sec. 4.2.
• If a standard sampling method such as MCMC or sequential MC is applicable, the computation
given y may be time consuming, and real-time applications may not be feasible. Using KBR, the
expectation of the posterior given y is obtained simply by the inner product as in Eq. (12), once
X RX |Y has been computed.
f T
The KBR approach nonetheless has a weakness common to other nonparametric methods: if a new
data point appears far from the training sample, the reliability of the output will be low. Thus, we
need sufﬁcient diversity in training sample to reliably est
imate the posterior.

In KBR computation, Gram matrix inversion is necessary, which would cost O(n3 ) for sample size n
if attempted directly. Substantial cost reductions can be achieved by low rank matrix approximations
such as the incomplete Cholesky decomposition [5], which approximates a Gram matrix in the form
of ΓΓT with n × r matrix Γ. Computing Γ costs O(nr2 ), and with the Woodbury identity, the KBR
can be approximately computed with cost O(nr2 ).
Kernel choice or model selection is key to the effectiveness of KBR, as in other kernel methods.
KBR involves three model parameters: the kernel (or its parameters), and the regularization parame-
ters εn and δn . The strategy for parameter selection depends on how the posterior is to be used in the
inference problem. If it is applied in a supervised setting, we can use standard cross-validation (CV).
A more general approach requires constructing a related supervised problem. Suppose the prior is
given by the marginal PX of P . The posterior density q(x|y) averaged with PY is then equal to the
marginal density pX . We are then able to compare the discrepancy of the kernel mean of PX and
the average of the estimators bQX |y=Yi over Yi . This leads to application of K -fold CV approach.
a=1 , let bm[−a]
be the kernel mean
Namely, for a partition of {1, . . . , n} into K disjoint subsets {Ta }K
QX |y
of posterior estimated with data {(Xi , Yi )}i /∈Ta , and the prior mean bm[−a]
X with data {Xi }i /∈Ta . We
a=1(cid:13)(cid:13) 1
X (cid:13)(cid:13)2
use PK
|Ta | Pj∈Ta
|Ta | Pj∈Ta bm[−a]
for CV, where bm[a]
QX |y=Yj − bm[a]
X = 1
kX (·, Xj ).
HX
5

Application to nonparametric state-space model. Consider the state-space model,
p(X, Y ) = π(X1 )QT
t=1 p(Yt |Xt )QT −1
t=1 q(Xt+1 |Xt ),
where Yt is observable and Xt is a hidden state. We do not assume the conditional probabili-
ties p(Yt |Xt ) and q(Xt+1 |Xt ) to be known explicitly, nor do we estimate them with simple para-
metric models. Rather, we assume a sample (X1 , Y1 ), . . . , (XT +1 , YT +1 ) is given for both the
observable and hidden variables in the training phase. This problem has already been consid-
ered in [20], but we give a more principled approach based on KBR. The conditional probabil-
ity for the transition q(xt+1 |xt ) and observation process p(y |x) are represented by the covariance
T PT
operators as computed with the training sample; bCX,X+1 = 1
i=1 kX (·, Xi ) ⊗ kX (·, Xi+1 ),
T PT
bCX Y = 1
i=1 kX (·, Xi ) ⊗ kY (·, Yi ), and bCY Y and bCXX are deﬁned similarly. Note that though
the data are not i.i.d., consistency is achieved by the mixing property of the Markov model.
For simplicity, we focus on the ﬁltering problem, but smooth ing and prediction can be done similarly.
In ﬁltering, we wish to estimate the current hidden state xt , given observations ˜y1 , . . . , ˜yt . The
sequential estimate of p(xt | ˜y1 , . . . , ˜yt ) can be derived using KBR (we give only a sketch below; see
Supplementary material for the detailed derivation). Suppose we already have an estimator of the
kernel mean of p(xt | ˜y1 , . . . , ˜yt ) in the form
bmxt | ˜y1 ,..., ˜yt = PT
i=1α(t)
i kX (·, Xi ),
i = α(t)
where α(t)
i ( ˜y1 , . . . , ˜yt ) are the coefﬁcients at time
t. By applying Theorem 2 twice, the
kernel mean of p(yt+1 | ˜y1 , . . . , ˜yt ) is estimated by bmyt+1 | ˜y1 ,..., ˜yt = PT
i=1 bµ(t+1)
kY (·, Yi ), where
i
bµ(t+1) = (GX + T εT IT )−1GX,X+1 (GX + T εT IT )−1GX α(t) .
(13)
(cid:0)GX+1X (cid:1)ij = kX (Xi+1 , Xj ). With the notation
Here GX+1X is the “transfer ” matrix deﬁned by
Λ(t+1) = Diag(bµ(t+1)
, . . . , bµ(t+1)
), kernel Bayes’ rule yields
1
T
α(t+1) = Λ(t+1)GY (cid:0)(Λ(t+1)GY )2 + δT IT (cid:1)−1
Λ(t+1)kY ( ˜yt+1 ).
(14)
Eqs. (13) and (14) describe the update rule of α(t) ( ˜y1 , . . . , ˜yt ). By contrast with [20], where the
estimates of the previous hidden state and observation are assumed to combine additively, the above
derivation is based only on applying KBR. In sequential ﬁlte ring, a substantial reduction of compu-
tational cost can be achieved by low rank approximations for the matrices of a training phase: given
rank r , the computation costs only O(T r2 ) for each step in ﬁltering.
Bayesian computation without likelihood. When the likelihood and/or prior is not obtained in
an analytic form but sampling is possible, the ABC approach [25, 12, 17] is popular for Bayesian
computation. The ABC rejection method generates a sample from q(X |Y = y) as follows: (1) gen-
erate Xt from the prior Π, (2) generate Yt from p(y |Xt ), (3) if D(y , Yt ) < ρ, accept Xt ; otherwise
reject, (4) go to (1). In Step (3), D is a distance on X , and ρ is the tolerance to acceptance.
In the exactly the same situation as the above, the KBR approach gives the following method: (i)
generate X1 , . . . , Xn from the prior Π, (ii) generate a sample Yt from p(y |Xt ) (t = 1, . . . , n), (iii)
compute Gram matrices GX and GY with (X1 , Y1 ), . . . , (Xn , Yn ), and RX |Y
kY (y).
The distribution of a sample given by ABC approaches the true posterior if ρ → 0, while the
empirical posterior estimate of KBR converges to the true one as n → ∞. The computational
efﬁciency of ABC, however, can be arbitrarily low for a small ρ, since Xt is then rarely accepted
in Step (3). Finally, ABC generates a sample, which allows any statistic of the posterior to be
approximated. In the case of KBR, certain statistics of the posterior (such as conﬁdence intervals)
can be harder to obtain, since consistency is guaranteed only for expectations of RKHS functions.
In Sec. 4.2, we provide experimental comparisons addressing the trade-off between computational
time and accuracy for ABC and KBR.

4 Experiments

4.1 Nonparametric inference of posterior

First we compare KBR and the standard kernel density estimation (KDE). Let {(Xi , Yi )}n
i=1 be
an i.i.d. sample from P on Rd × Rr . With p.d.f. K (x) on Rd and H (y) on Rr , the conditional

6

j=1 KhX (x − Xj )HhY (y − Yj )/ Pn
p.d.f. p(y |x) is estimated by bp(y |x) = Pn
j=1 KhX (x − Xj ),
X K (x/hX ) and HhY (x) = h−r
where KhX (x) = h−d
Y H (y/hY ). Given an i.i.d. sample {Uj }ℓ
j=1
from the prior Π, the posterior q(x|y) is represented by the weighted sample (Ui , wi ) with wi =
bp(y |Ui )/ Pℓ
j=1 bp(y |Uj ) as importance weight (IW).
We compare the estimates of R xq(x|y)dx obtained by KBR and KDE + IW, using Gaussian kernels
for both the methods. Note that with Gaussian kernel, the function f (x) = x does not belong to
HX , and the consistency of the KBR method is not rigorously guaranteed (c.f. Theorem 5). Gaussian
kernels, however, are known to be able to approximate any continuous function on a compact subset
with arbitrary accuracy [23]. We can thus expect that the posterior mean can be estimated effectively.

 

60

50

KBR vs KDE+IW (E[X|Y=y])

KBR (CV)
KBR (Med dist)
KDE+IW (MS CV)
KDE+IW (best)

In the experiments, the dimensionality was given by
r = d ranging form 2 to 64. The distribution P of
(X, Y ) was N ((0, 1d )T , V ) with V randomly generated
for each run. The prior Π was PX = N (0, VXX /2),
where VXX is the X -component of V . The sample sizes
were n = ℓ = 200. The bandwidth parameter hX , hY
in KDE were set hX = hY and chosen by two ways,
the least square cross-validation [15] and the best mean
performance, over the set {2 ∗ i | i = 1, . . . , 10}. For
the KBR, we used use two methods to choose the devi-
ation parameter in Gaussian kernel: the median over the
pairwise distances in the data [10] and the 10-fold CV
described in Sec. 3. Fig. 2 shows the MSE of the esti-
mates over 1000 random points y ∼ N (0, VY Y ). While the accuracy of the both methods decrease
for larger dimensionality, the KBR signi ﬁcantly outperfor ms the KDE+IW.

32
Dimension
Figure 2: KBR v.s. KDE+IW.

)
s
n
u
r
 
0
5
(
 
E
S
M
 
.
e
v
A

8 12 16

2 4

20

30

40

24

48

64

10

0

 

4.2 Bayesian computation without likelihood

CPU time vs Error (6 dim.)

5.1×102

 

KBR
ABC

200

10−1

1.0×104

2.5×103

s
r
o
r
r
E
 
e
r
a
u
q
S
 
n
a
e
M
 
.
v
A

We compare KBR and ABC in terms of the estima-
tion accuracy and computational time. To compute the
estimation accuracy rigorously, Gaussian distributions
are used for the true prior and likelihood. The sam-
ples are taken from the same model as in Sec. 4.1, and
R xq(x|y)dx is evaluated at 10 different points of y . We
performed 10 runs with different covariance.
For ABC, we used only the rejection method; while
there are more advanced sampling schemes [12, 17], im-
plementation is not straightforward. Various parameters
for the acceptance are used, and the accuracy and com-
putational time are shown in Fig.3 together with total
sizes of generated samples. For the KBR method, the sample sizes n of the likelihood and prior are
varied. The regularization parameters are given by εn = 0.01/n and δn = 2εn . In KBR, Gaussian
kernels are used and the incomplete Cholesky decomposition is employed. The results indicate that
KBR achieves more accurate results than ABC in the same computational time.

Figure 3: Estimation accuracy and com-
putational time with KBR and ABC.

102
CPU time (sec)

6.4×104

7.9×105

1000

2000

101

103

104

100

400

600

800

 

4.3 Filtering problems

The KBR ﬁlter proposed in Sec. 3 is applied. Alternative stra tegies for state-space models with
complex dynamics involve the extended Kalman ﬁlter (EKF) an d unscented Kalman ﬁlter (UKF,
[11]). There are some works on nonparametric state-space model or HMM which use nonparametric
estimation of conditional p.d.f. such as KDE or partitions [27, 26] and, more recently, kernel method
[20, 21]. In the following, the KBR method is compared with linear and nonlinear Kalman ﬁlters.
KBR has the regularization parameters εT , δT , and kernel parameters for kX and kY (e.g., the de-
viation parameter for Gaussian kernel). The validation approach is applied for selecting them by
dividing the training sample into two. To reduce the search space, we set δT = 2εT and use the
Gaussian kernel deviation βσX and βσY , where σX and σY are the median of pairwise distances
among the training samples ([10]), leaving only two parameters β and εT to be tuned.

7

0.16

0.14

0.12

0.1

0.08

0.06

0.04

s
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

 

KBR
EKF
UKF

0.09

0.08

0.07

0.06

s
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

 

KBF
EKF
UKF

0.02

 

0.05

 

200

200

400
600
400
600
Training data size
Training sample size
Data (b)
Data (a)
Figure 4: Comparisons with the KBR Filter and EKF. (Average MSEs and SEs over 30 runs.)

1000

800

800

Kalman (9 dim.) Kalman (Quat.)
KBR (Tr)
KBR (Gauss)
σ2 = 10−4
0.557 ± 0.023
1.980 ± 0.083
0.146 ± 0.003
0.210 ± 0.015
σ2 = 10−3
1.935 ± 0.064
0.541 ± 0.022
0.210 ± 0.008
0.222 ± 0.009
Table 1: Average MSEs and SEs of camera angle estimates (10 runs).

We ﬁrst use two synthetic data sets with KBR, EKF, and UKF, ass uming that EKF and UKF know
the exact dynamics. The dynamics has a hidden state Xt = (ut , vt )T ∈ R2 , and is given by
θt+1 = θt + η (mod 2π),
(ut+1 , vt+1 ) = (1 + b sin(M θt+1 ))(cos θt+1 , sin θt+1 ) + Zt ,
where Zt ∼ N (0, σ2
h I2 ) is independent noise. Note that the dynamics of (ut , vt ) is nonlinear even
for b = 0. The observation Yt follows Yt = Xt + Wt , where Wt ∼ N (0, σ2
o I ). The two dynamics
are deﬁned as follows: (a) (noisy rotation) η = 0.3, b = 0, σh = σo = 0.2, (b) (noisy oscillatory
rotation) η = 0.4, b = 0.4, M = 8, σh = σo = 0.2. The results are shown in Fig. 4. In all the cases,
EKF and UKF show unrecognizably small difference. The dynamics in (a) has weak nonlinearity,
and KBR shows slightly worse MSE than EKF and UKF. For dataset (b) of strong nonlinearity, KBR
outperforms for T ≥ 200 the nonlinear Kalman ﬁlters, which know the true dynamics.
Next, we applied the KBR ﬁlter to the camera rotation problem used in [20]1 , where the angle of a
camera is the hidden variable and the movie frames of a room taken by the camera are observed. We
are given 3600 frames of 20 × 20 RGB pixels (Yt ∈ [0, 1]1200 ), where the ﬁrst 1800 frames are used
for training, and the second half are used for test. For the details on the data, see [20]. We make
the data noisy by adding Gaussian noise N (0, σ2 ) to Yt . Our experiments cover two settings. In the
ﬁrst, we assume we do not know the hidden state Xt is included in SO(3), but is a general 3 × 3
matrix. In this case, we use the Kalman ﬁlter by estimating th e relations under a linear assumption,
and the KBR ﬁlter with Gaussian kernels for Xt and Yt . In the second setting, we exploit the fact
Xt ∈ SO(3): for the Kalman ﬁlter, Xt is represented by a quanternion, and for the KBR ﬁlter
the kernel k(A, B ) = Tr[AB T ] is used for Xt . Table 1 shows the Frobenius norms between the
estimated matrix and the true one. The KBR ﬁlter signi ﬁcantl
y outperforms the Kalman ﬁlter, since
KBR has the advantage in extracting the complex nonlinear dependence of the observation on the
hidden state.

5 Conclusion

We have proposed a general, novel framework for implementing Bayesian inference, where the prior,
likelihood, and posterior are expressed as kernel means in reproducing kernel Hilbert spaces. The
model is expressed in terms of a set of training samples, and inference consists of a small number
of straightforward matrix operations. Our approach is well suited to cases where simple paramet-
ric models or an analytic forms of density are not available, but samples are easily obtained. We
have addressed two applications: Bayesian inference without likelihood, and sequential ﬁltering
with nonparametric state-space model. Future studies could include more comparisons with sam-
pling approaches like advanced Monte Carlo, and applications to various inference problems such
as nonparametric Bayesian models and Bayesian reinforcement learning.
Acknowledgements. KF was supported in part by JSPS KAKENHI (B) 22300098.

1Due to some difference in noise model, the results here are not directly comparable with those of [20].

8

References

In Proc.

[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68(3):337–404, 1950.
[2] C.R. Baker. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273–289, 1973.
[3] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Kluwer Academic Publisher, 2004.
[4] A. Doucet, N. De Freitas, and N.J. Gordon. Sequential Monte Carlo Methods in Practice. Springer, 2001.
[5] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank ke rnel representations. JMLR, 2:243–
264, 2001.
[6] K. Fukumizu, F.R. Bach, and M.I. Jordan. Dimensionality reduction for supervised learning with repro-
ducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.
[7] K. Fukumizu, F.R. Bach, and M.I. Jordan. Kernel dimension reduction in regression. Anna. Stat.,
37(4):1871–1905, 2009.
[8] K. Fukumizu, A. Gretton, X. Sun, and B. Sch ¨olkopf. Kernel measures of conditional dependence. In
Advances in NIPS 20, pages 489–496. MIT Press, 2008.
[9] A. Gretton, K.M. Borgwardt, M. Rasch, B. Sch ¨olkopf, and A. Smola. A kernel method for the two-
sample-problem. In Advances in NIPS 19, pages 513–520. MIT Press, 2007.
[10] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch ¨olkopf, and A. Smola. A kernel statistical test of
independence. In Advances in NIPS 20, pages 585–592. MIT Press, 2008.
[11] S.J. Julier and J.K. Uhlmann. A new extension of the Kalman ﬁlter to n onlinear systems.
AeroSense: The 11th Intern. Symp. Aerospace/Defence Sensing, Simulation and Controls, 1997.
[12] P. Marjoram, Jo. Molitor, V. Plagnol, and S. Tavare. Markov chain monte carlo without likelihoods. PNAS,
100(26):15324–15328, 2003.
[13] S. Mika, B. Sch ¨olkopf, A. Smola, K.-R. M ¨uller, M. Scholz, and G. R ¨atsch. Kernel pca and de-noising in
feature spaces. In Advances in NIPS 11, pages 536–542. MIT Press, 1999.
[14] P. M ¨uller and F.A. Quintana. Nonparametric bayesian data analysis. Statistical Science, 19(1):95–110,
2004.
[15] M. Rudemo. Empirical choice of histograms and kernel density estimators. Scandinavian J. Statistics,
9(2):pp. 65–78, 1982.
[16] B. Sch ¨olkopf and A.J. Smola. Learning with Kernels. MIT Press, 2002.
[17] S. A. Sisson, Y. Fan, and M. M. Tanaka. Sequential monte carlo without likelihoods. PNAS, 104(6):1760–
1765, 2007.
[18] L. Song, A. Gretton., and C. Guestrin. Nonparametric tree graphical models via kernel embeddings. In
AISTATS 2010, pages 765–772, 2010.
[19] L. Song, A. Gretton, D. Bickson, Y. Low, and C. Guestrin. Kernel belief propagation. In AISTATS 2011.
[20] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions
with applications to dynamical systems. Proc ICML2009, pages 961–968. 2009.
[21] L. Song and S. M. Siddiqi and G. Gordon and A. Smola. Hilbert Space Embeddings of Hidden Markov
Models. Proc. ICML2010, 991–998. 2010.
[22] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch ¨olkopf, and G. R.G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. JMLR, 11:1517–1561, 2010.
[23] I. Steinwart. On the Inﬂuence of the Kernel on the Consistency of S upport Vector Machines. JMLR,
2:67–93, 2001.
[24] M. Sugiyama, I. Takeuchi, T. Suzuki, T. Kanamori, H. Hachiya, and D. Okanohara. Conditional density
estimation via least-squares density ratio estimation. In AISTATS 2010, pages 781–788, 2010.
[25] S. Tavar ´e, D.J. Balding, R.C. Grifﬁthis, and P. Donnelly. Inferring coalescen ce times from dna sequece
data. Genetics, 145:505–518, 1997.
[26] S. Thrun, J. Langford, and D. Fox. Monte carlo hidden markov models: Learning non-parametric models
of partially observable stochastic processes. In ICML 1999, pages 415–424, 1999.
[27] V. Monbet , P. Ailliot, and P.F. Marteau. l1 -convergence of smoothing densities in non-parametric state
space models. Statistical Inference for Stochastic Processes, 11:311–325, 2008.

9


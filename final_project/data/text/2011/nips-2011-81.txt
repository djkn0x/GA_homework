Complexity	  of	  Inference	  in	  Latent	  Dirichlet	  Alloca6on	  
David	  Sontag,	  Daniel	  Roy	  
(NYU,	  Cambridge)	  

W66	  
Topic	  models	  are	  powerful	  tools	  for	  exploring	  large	  data	  sets	  and	  for	  making	  
inferences	  about	  the	  content	  of	  documents	  

Documents	  

Topics	  
sports	  .0105	  
religion	  .0500	  
baseball	  .0100	  
hindu	  .0092	  
soccer	  .0055	  
judiasm	  .0080	  
basketball	  .0050	  
ethics	  .0075	  
football	  .0045	  
buddhism	  .0016	  
…	  
…	  
βt = ￿ p(w | z = t) ￿
Almost	  all	  uses	  of	  topic	  models	  (e.g.,	  for	  unsupervised	  learning,	  informa6on	  
retrieval,	  classiﬁca6on)	  require	  probabilis)c	  inference:	  

poli6cs	  .0100	  
president	  .0095	  
obama	  .0090	  
washington	  .0085	  
religion	  .0060	  
…	  

New	  document	  

What	  is	  this	  document	  about?	  

Words	  w1,	  …,	  wN	  

weather	  
ﬁnance	  
sports	  

.50	  
.49	  
.01	  

Distribu6on	  of	  topics	  

θ

1. Maximize p(z1:N | w1:N ). ←− Discrete. Classiﬁcation.
2. Maximize p(θ | w1:N ). ←− Dimensionality reduction, IR
3. Sample from p(θ | w1:N ). ←− Useful for learning in EM.
4. Sample from p(z1:N | w1:N ). ←− Useful for learning in EM.
1. Maximize p(z1:N | w1:N ). ←− Discrete. Classiﬁcation.
2. Maximize p(θ | w1:N ). ←− Dimensionality reduction, IR
3. Sample from p(θ | w1:N ). ←− Useful for learning in EM.
4. Sample from p(z1:N | w1:N ). ←− Useful for learning in EM.

1. Maximize p(z1:N | w1:N ). ←− Discrete. Classiﬁcation.
2. Maximize p(θ | w1:N ). ←− Dimensionality reduction, IR
3. Sample from p(θ | w1:N ). ←− Useful for learning in EM.
4. Sample from p(z1:N | w1:N ). ←− Useful for learning in EM.

Inference goals

Inference goals

Inference goals

Complexity	  of	  Inference	  in	  Latent	  Dirichlet	  Alloca6on	  
David	  Sontag,	  Daniel	  Roy	  
(NYU,	  Cambridge)	  

W66	  
We	  study	  the	  complexity	  of	  probabilis6c	  inference	  in	  Latent	  Dirichlet	  Alloca6on	  
The problem
The problem
The problem
Input: new document with words w1:N
The problem
Input: new document, w1:N .
Input: new document, w1:N .
Input: new document, w1:N .
Input: topic-word distributions βt , t = 1, 2, . . . , T .
topic-word distributions βt , t = 1, 2, . . . , T and Dirichlet hyper-parameters α1:T
Input: topic-word distributions βt , t = 1, 2, . . . , T .
Input: new document, w1:N .
Input: topic-word distributions βt , t = 1, 2, . . . , T .
Input: topic-word distributions βt , t = 1, 2, . . . , T .
Generative model
Generative model
Genera6ve	  model	  
Generative model
Generative model
(cid:7555) 
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  Choose	  a	  distribu6on	  over	  the	  T	  topics	  	  
θ ∼ Dirichlet(α1:T )
(1)
θ ∼ Dirichlet(α1:T )
(1)
(cid:7556)  For	  each	  word	  i,	  
θ ∼ Dirichlet(α1:T )
(1)
(1 ≤ i ≤ N )
zi | θ ∼ θ
(2)
θ ∼ Dirichlet(α1:T )
(1)
(1 ≤ i ≤ N )
zi | θ ∼ θ
(2)
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  Choose	  a	  topic	  for	  i’th	  word	  
(1 ≤ i ≤ N )
zi | θ ∼ θ
(2)
wi | zi ∼ βzi
(3)
(1 ≤ i ≤ N )
zi | θ ∼ θ
(2)
wi | zi ∼ βzi
(3)
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  Sample	  a	  word	  
wi | zi ∼ βzi
(3)
wi | zi ∼ βzi
(3)
We will assume that the α1:T are ﬁxed.
We will assume that the α1:T are ﬁxed.
We will assume that the α1:T are ﬁxed.
We will assume that the α1:T are ﬁxed.
Inference goals
Popular	  inference	  problems	  
1. Maximize p(z1:N | w1:N ). ←− Discrete. Classiﬁcation.
2. Maximize p(θ | w1:N ). ←− Dimensionality reduction, IR
3. Sample from p(θ | w1:N ). ←− Useful for learning in EM.
4. Sample from p(z1:N | w1:N ). ←− Useful for learning in EM.

Complexity	  of	  Inference	  in	  Latent	  Dirichlet	  Alloca6on	  
David	  Sontag,	  Daniel	  Roy	  
(NYU,	  Cambridge)	  

W66	  

Maximize p(z1:N | w1:N )
#	  topics	  in	  MAP	  assignment	   Complexity	  
Small	  
Easy	  

Main	  Results	  
For any α

Most	  common	  
seang	  	  

Large	  

NP-­‐hard	  

Intui)on	  
First	  choose	  topic	  sizes,	  
then	  match	  words	  to	  topics	  
Reduc6on	  from	  set	  packing	  

Maximize p(θ | w1:N )
Dirichlet	  hyper-­‐parameters	   Complexity	  
αt ≥ 1
Easy	  
αt < 1
NP-­‐hard	  

Most	  common	  
seang	  	  

Intui)on	  
Maximizing	  concave	  func6on	  
Reduc6on	  from	  set	  cover	  

Sample from p(θ | w1:N )
Dirichlet	  hyper-­‐parameters	   Complexity	  
αt ≥ 1
Easy	  
αt ≈ 0
NP-­‐hard	  

Intui)on	  
Log-­‐concave	  distribu6on	  
Reduc6on	  from	  set	  cover	  


Manifold Pr ´ecis: An Annealing Technique for Diverse
Sampling of Manifolds

Nitesh Shroff †, Pavan Turaga ‡, Rama Chellappa †
†Department of Electrical and Computer Engineering, University of Maryland, College Park
‡School of Arts, Media, Engineering and ECEE, Arizona State University
{nshroff,rama}@umiacs.umd.edu, pturaga@asu.edu

Abstract
In this paper, we consider the Pr ´ecis problem of sampling K representative yet
diverse data points from a large dataset. This problem arises frequently in ap-
plications such as video and document summarization, exploratory data analysis,
and pre- ﬁltering. We formulate a general theory which encom passes not just tra-
ditional techniques devised for vector spaces, but also non-Euclidean manifolds,
thereby enabling these techniques to shapes, human activities, textures and many
other image and video based datasets. We propose intrinsic manifold measures for
measuring the quality of a selection of points with respect to their representative
power, and their diversity. We then propose efﬁcient algori thms to optimize the
cost function using a novel annealing-based iterative alternation algorithm. The
proposed formulation is applicable to manifolds of known geometry as well as
to manifolds whose geometry needs to be estimated from samples. Experimental
results show the strength and generality of the proposed approach.

1 Introduction
The problem of sampling K representative data points from a large dataset arises frequently in var-
ious applications. Consider analyzing large datasets of shapes, objects, documents or large video
sequences, etc. Analysts spend a large amount of time sifting through the acquired data to famil-
iarize themselves with the content, before using them for their application speciﬁc tasks. This has
necessitated the problem of optimal selection of a few representative exemplars from the dataset as
an important step in exploratory data analysis. Other applications include Internet-based video sum-
marization, where providing a quick overview of a video is important for improving the browsing
experience. Similarly, in medical image analysis, picking a subset of K anatomical shapes from
a large population helps in identifying the variations within and across shape classes, providing an
invaluable tool for analysts.

Depending upon the application, several subset selection criteria have been proposed in the liter-
ature. However, there seems to be a consensus in selecting exemplars that are representative of
the dataset while minimizing the redundancy between the exemplars. Liu et al.[1] proposed that
the summary of a document should satisfy the ‘coverage’ and ‘orthogonality’ criteria. Shroff et
al.[2] extended this idea to selecting exemplars from videos that maximize ‘coverage’ and ‘diver-
sity’. Simon et al.[3] formulated scene summarization as one of picking interesting and important
scenes with minimal redundancy. Similarly, in statistics, stratiﬁed sampling techniques sample the
population by dividing the dataset into mutually exclusive and exhaustive ‘strata’ (sub-groups) fol-
lowed by a random selection of representatives from each strata [4]. The splitting of population into
stratas ensures that a diverse selection is obtained. The need to select diverse subsets has also been
emphasized in information retrieval applications [5, 6].

Column Subset Selection (CSS) [7, 8, 9] has been one of the popular techniques to address this prob-
lem. The goal of CSS is to select the K most “well-conditioned ” columns from the matrix of data
points. One of the key assumptions behind this and other techniques is that the objects or their repre-
sentations, lie in the Euclidean space. Unfortunately, this assumption is not valid in many cases. In

1

applications like computer vision, images and videos are represented by features/models like shapes
[10], bags-of-words, linear dynamical systems (LDS) [11], etc. Many of these features/models have
been shown to lie in non-Euclidean spaces, implying that the underlying distance metric of the space
is not the usual `2/`p norm. Since these feature/model spaces have a non-trivial manifold structure,
the distance metrics are highly non-linear functions. Examples of features/models - manifold pairs
include: shapes - complex spherical manifold [10], linear subspaces - Grassmann manifold, co-
variance matrices - tensor space, histograms - simplex in Rn , etc. Even the familiar bag-of-words
representation, used commonly in document analysis, is more naturally considered as a statistical
manifold than as a vector space [12]. The geometric properties of the non-Euclidean manifolds allow
one to develop accurate inference and classiﬁcation algori thms [13, 14]. In this paper, we focus on
the problem of selecting a subset of K exemplars from a dataset of N points when the dataset has an
underlying manifold structure to it. We formulate the notion of representational error and diversity
measure of exemplars while utilizing the non-Euclidean structure of the data points followed by the
proposal of an efﬁcient annealing-based optimization algo rithm.
Related Work: The problem of subset selection has been studied by the communities of numerical
linear algebra and theoretical computer science. Most work in the former community is related
to the Rank Revealing QR factorization (RRQR) [7, 15, 16]. Given a data matrix Y , the goal of
RRQR factorization is to ﬁnd a permutation matrix Π such that the QR factorization of Y Π reveals
the numerical rank of the matrix. The resultant matrix Y Π has as its ﬁrst K columns the most
“well-conditioned ” columns of the matrix Y . On the other hand, the latter community has focused
on Column Subset Selection (CSS). The goal of CSS is to pick K columns forming a matrix C ∈
Rm×K such that the residual || Y − PC Y ||ζ is minimized over all possible choices for the matrix
C . Here PC = CC † denotes the projection onto the K -dimensional space spanned by the columns
of C and ζ can represent the spectral or Frobenius norm. C † indicates the pseudo inverse of matrix
C . Along these lines, different randomized algorithms have been proposed [17, 18, 9, 8]. Various
approaches include a two-stage approach [9], subspace sampling methods [8], etc.

Clustering techniques [19] have also been applied for subset selection [20, 21]. In order to select K
exemplars, data points are clustered into ` clusters with (` ≤ K ) followed by the selection of one
or multiple exemplars from each cluster to obtain the best representation or low-rank approximation
of each cluster. Afﬁnity Propagation [21], is a clustering a lgorithm that takes similarity measures as
input and recursively passes message between nodes until a set of exemplars emerges. As we discuss
in this paper, the problems with these approaches are that (a) the objective functions optimized by the
clustering functions do not incorporate the diversity of the exemplars, hence can be biased towards
denser clusters, and also by outliers, and (b) seeking low-rank approximation of the data matrix
or clusters individually is not always an appropriate subset selection criterion. Furthermore, these
techniques are largely tuned towards addressing the problem in an Euclidean setting and cannot be
applied for datasets in non-Euclidean spaces.

Recently, advances have been made in utilizing non-Euclidean structure for statistical inferences
and pattern recognition [13, 14, 22, 23]. These works have addressed inferences, clustering, dimen-
sionality reduction, etc.
in non-Euclidean spaces. To the best of our knowledge, the problem of
subset selection for analytic manifolds remains largely unaddressed. While one could try to solve
the problem by obtaining an embedding of a given manifold into a larger ambient Euclidean space, it
is desirable to have a solution that is more intrinsic in nature. This is because the chosen embedding
is often arbitrary, and introduces peculiarities that result from such extrinsic approaches. Further
manifolds such as the Grassmannian or the manifold of in ﬁnit e dimensional diffeomorphisms do
not admit a natural embedding into a vector space.
Contributions: 1) We present the ﬁrst formal treatment of subset selection f or the general case of
manifolds, 2) We propose a novel annealing-based alternation algorithm to efﬁciently solve the opti-
mization problem, 3) We present an extension of the algorithm for data manifolds, and demonstrate
the favorable properties of the algorithm on real data.
2 Subset Selection on Analytic Manifolds

In this section, we formalize the subset selection problem on manifolds and propose an efﬁcient
algorithm. First, we brie ﬂy touch upon the necessary basic c oncepts.
Geometric Computations on Manifolds: Let M be an m-dimensional manifold and, for a point
p ∈ M, consider a differentiable curve γ : (−, ) → M such that γ (0) = p. The velocity ˙γ (0)

2

denotes the velocity of γ at p. This vector is an example of a tangent vector to M at p. The set of
all such tangent vectors is called the tangent space to M at p. If M is a Riemannian manifold then
the exponential map expp : Tp (M) → M is de ﬁned by expp (v) = αv (1) where αv is a speciﬁc
geodesic. The inverse exponential map (logarithmic map) logp : M → Tp takes a point on the
manifold and returns a point on the tangent space – which is an Euclidean space.

Representational error on manifolds: Let us assume that we are given a set of points X =
{x1 , x2 , . . . xn } which belong to a manifold M. The goal is to select a few exemplars E =
{e1 , . . . eK } from the set X , such that the exemplars provide a good representation of the given
data points, and are minimally redundant. For the special case of vector spaces, two common ap-
proaches for measuring representational error is in terms of linear spans, and nearest-exemplar error.
The linear span error is given by: minz kX − Ez k2
F , where X is the matrix form of the data, and E
kxk − eik2 ,
is a matrix of chosen exemplars. The nearest-exemplar error is given by: Pi Pxk∈Φi
where ei is the ith exemplar and Φi corresponds to its Voronoi region.
Of these two measures, the notion of linear span, while appropriate for matrix approximation, is
not particularly meaningful for general dataset approximation problems since the ‘span’ of a dataset
item does not carry much perceptually meaningful information. For example, the linear span of a
vector x ∈ Rn is the set of points αx, α ∈ R. However, if x were an image, the linear span of x
would be the set of images obtained by varying the global contrast level. All elements of this set
however are perceptually equivalent, and one does not obtain any representational advantage from
considering the span of x. Further, points sampled from the linear span of few images, would not be
meaningful images. This situation is further complicated for manifold-valued data such as shapes,
where the notion of linear span does not exist. One could attempt to de ﬁne the notion of linear spans
on the manifold as the set of points lying on the geodesic shot from some ﬁxed pole toward the given
dataset item. But, points sampled from this linear span might not be very meaningful e.g., samples
from the linear span of a few shapes would give physically meaningless shapes.

Hence, it is but natural to consider the representational error of a set X with respect to a set of
exemplars E as follows:

d2
g (xj , ei )

Jrep (E ) = Xi Xxj ∈Φi
Here, dg is the geodesic distance on the manifold and Φi is the Voronoi region of the ith exemplar.
This boils down to the familiar K -means or K -medoids cost function for Euclidean spaces. In order
to avoid combinatorial optimization involved in solving this problem, we use efﬁcient approxima-
tions i.e., we ﬁrst ﬁnd the mean followed by the selection of
ei as data point that is closest to the
mean. The algorithm for optimizing Jrep is given in algorithm 1. Similar to K -means clustering,
a cluster label is assigned to each xj followed by the computation of the mean µi for each cluster.
This is further followed by selecting representative exemplar ei as the data point closest to µi .

(1)

Diversity measures on manifolds: The next question we consider is to de ﬁne the notion of diver-
sity of a selection of points on a manifold. We ﬁrst begin by ex amining equivalent constructions for
Rn . One of the ways to measure diversity is simply to use the sample variance of the points. This is
similar to the construction used recently in [2]. For the case of manifolds, the sample variance can be
K PK
replaced by the sample Karcher variance, given by the function: ρ(E ) = 1
g (µ, ei ), where
i=1 d2
µ is the Karcher mean [24], and the function value is the Karcher variance. However, this construc-
tion leads to highly inefﬁcient optimization routines, ess entially boiling down to a combinatorial
search over all possible K -sized subsets of X .
An alternate formulation for vector spaces that results in highly efﬁcient optimization routines is via
Rank-Revealing QR (RRQR) factorizations. For vector spaces, given a set of vectors X = {xi },
written in matrix form X, RRQR [7] aims to ﬁnd Q, R and a permutation matrix Π ∈ Rn×n such
that XΠ = QR reveals the numerical rank of the matrix X. This permutation XΠ = (XK Xn−K )
gives XK , the K most linearly independent columns of X. This factorization is achieved by seeking
Π which maximizes Λ(XK ) = Qi σi (XK ), the product of the singular values of the matrix XK .
For the case of manifolds, we adopt an approximate approach in order to measure diversity in terms
of the ‘well-conditioned’ nature of the set of exemplars projected on the tangent space at the mean.
In particular, for the dataset {xi } ⊆ M, with intrinsic mean µ, and a given selection of exemplars

3

Algorithm 1: Algorithm to minimize Jrep
Input: X ∈ M, k , index vector ω , Γ
Output: Permutation Matrix Π
Initialize Π ← In×n
for γ ← 1 to Γ do
Initialize Π(γ ) ← In×n
ei ← xωi for i = {1,2,. . . ,k}
for i ← 1 to k do
Φi ← {xp : arg minj dg (xp , ej ) = i }
µi ← mean of Φi
ˆj ← arg minj dg (xj , µi )
Update: Π(γ ) ← Π(γ ) Πi↔ˆj

end
Update: Π ← Π Π(γ ) , ω ← ωΠ(γ )
if Π(γ ) = In×n then
break

end

Algorithm 2: Algorithm for Diversity Maximization
Input: Matrix V ∈ Rd×n , k , Tolerance tol
Output: Permutation Matrix Π
Initialize Π ← In×n
repeat
Compute QR decomposition of V to obtain
R11 , R12 and R22 s.t., V = Q (cid:18) R11 R12
R22 (cid:19)
0
βij ← q(R−1
i R−1
11 R12 )2
ij + ||R22 αj ||2
11 ||2
2 ||αT
2
βm ← maxij βij
(ˆi, ˆj ) ← arg maxij βij
Update: Π ← Π Πi↔(j+k)
V ← V Πi↔(j+k)
until βm < tol ;

end

{ej }, we measure the diversity of exemplars as follows: matrix TE = [logµ (ej )] is obtained by
projecting the exemplars {ej } on the tangent space at mean µ. Here, log() is the inverse exponential
map on the manifold and gives tangent vectors at µ that point towards ej .
Diversity can then be quantiﬁed as Jdiv (E ) = Λ(TE ), where, Λ(TE ) represents the product of the
singular values of the matrix TE . For vector spaces, this measure is related to the sample variance
of the chosen exemplars. For manifolds, this measure is related to the sample Karcher variance. If
we denote TX = [logµ (xi )], the matrix of tangent vectors corresponding to all data-points, and if Π
is the permutation matrix that orders the columns such that the ﬁrst K columns of TX correspond
to the most diverse selection, then
Jdiv (E ) = Λ(TE ) = det(R11 ), where, TX Π = QR = Q (cid:18) R11 R12
R22 (cid:19)
0
Here, R11 ∈ RK×K is the upper triangular matrix of R ∈ Rn×n , R12 ∈ RK×(n−K ) and R22 ∈
R(n−K )×(n−K ) . The advantage of viewing the required quantity as the determinant of a sub-matrix
on the right hand-side of the above equation is
that one can obtain efﬁcient techniques for op-
timizing this cost function. The algorithm for
optimizing Jdiv is adopted from [7] and de-
scribed in algorithm 2. Input to the algorithm
is a matrix V created by the tangent-space pro-
jection of X and output is the K most “well-
conditioned ” columns of V . This is achieved
by ﬁrst decomposing V into QR and comput-
ing βij , which indicates the bene ﬁt of swapping
ith and j th columns [7]. The algorithm then se-
lects pair (ˆi, ˆj ) corresponding to the maximum
bene ﬁt swap βm and if βm > tol , this swap is
accepted. This is repeated until either βm < tol
or maximum number of iterations is completed.

Algorithm 3: Annealing-based Alternation Algo-
rithm for Subset Selection on Manifolds
Input: Data points X = {x1 , x2 , . . . , xn} ∈ M,
Number of exemplars k , Tolerance step δ
Output: E = {e1 , . . . ek } ⊆ X
Initial setup:
Compute intrinsic mean µ of X
Compute tangent vectors vi ← logµ (xi )
V ← [v1 , v2 , . . . , vn ]
ω ← [1, 2, . . . , n] be the 1 × n index vector of X
Tol ← 1
Initialize: Π ← Randomly permute columns of In×n
Update: V ← V Π, ω ← ωΠ.
while Π 6= In×n do
Diversity: Π ← Div(V , k, tol) as in algorithm 2.
Update: V ← V Π, ω ← ωΠ.
Representative Error: Π ← Rep(X, k, ω ,1) as
in algorithm 1
Update: V ← V Π, ω ← ωΠ.
tol ← tol + δ

(2)

end
ei ← xωi for i = {1,2,. . . ,k}

Representation and Diversity Trade-offs
for Subset Selection: From (1) and (2),
it can be seen that we seek a solution that
represents a trade-off between two con ﬂicting
criteria. As an example,
in ﬁgure 1(a) we
show two cases, where Jrep and Jdiv are
individually optimized. We can see that the
solutions look quite different in each case. One way to write the global cost function is as a
weighted combination of the two. However, such a formulation does not lend itself to efﬁcient
optimization routines (c.f. [2]). Further, the choice of weights is often left unjustiﬁed. Instead,
we propose an annealing-based alternating technique of optimizing the con ﬂicting criteria Jrep

4

Symbol
Γ
In×n
Φi
Πi↔j
Π(γ )
V
Hij
αj
Hαj , αT
j H

Represents
Maximum number of iterations
Identity matrix
Voronoi region of ith exemplar
Permutation matrix that swaps columns i and j
Π in the γ th iteration
Matrix obtained by tangent-space projection of X
(i, j) element of matrix H
j th column of the identity matrix
j th column and row of matrix H respectively

Computational Step
M Exponential Map (assume)
M Inverse exponential Map (assume)
Intrinsic mean of X
Projection of X to tangent-space
Geodesic distances in alg. 1
K intrinsic means
Alg. 2
Gm,p Exponential Map
Gm,p Inverse exponential map

Complexity
O(ν )
O(χ)
O((nχ + ν )Γ)
O(nχ)
O(nK χ)
O((nχ + K ν )Γ)
O(mnK log n)
O(p3 )
O(p3 )

Table 1: Notations used in Algorithm 1 - 3

Table 2: Complexity of various computational steps.

and Jdiv . Optimization algorithms for Jrep and Jdiv individually are given in algorithms 1 and
2 respectively. We ﬁrst optimize Jdiv to obtain an initial set of exemplars, and use this set as
an initialization for optimizing Jrep . The output of this stage is used as the current solution to
further optimize Jdiv . However, with each iteration, we increase the tolerance parameter tol in
algorithm 2. This has the effect of accepting only those permutations that increase the diversity
by a higher factor as iterations progress. This is done to ensure that the algorithm is guided
towards convergence. If the tol value is not increased at each iteration, then optimizing Jdiv will
continue to provide a new solution at each iteration that modiﬁes the cost function only marginally.
This is illustrated in ﬁgure 1(c), where we show how the cost f unctions Jrep and Jdiv exhibit an
oscillatory behavior if annealing is not used. As seen in ﬁgu re 1(b) , the convergence of Jdiv
and Jrep is obtained very quickly on using the proposed annealing alternation technique. The
complete annealing based alternation algorithm is described in algorithm 3. A technical detail to
be noted here is that for algorithm 2, input matrix V ∈ Rd×n should have d ≥ k . For cases where
d < k , algorithm 2 can be replaced by its extension proposed in [9]. Table 1 shows the notations
introduced in algorithms 1 - 3. Πi↔j is obtained by permuting i and j columns of the identity matrix.

3 Complexity, Special cases and Limitations

In this section, we discuss how the proposed method relates to the special case of M = Rn , and
to sub-manifolds of Rn speciﬁed by a large number of samples. For the case of Rn , the cost func-
tions Jrep and Jdiv boil down to familiar notions of clustering and low-rank matrix approximation
respectively. In this case, algorithm 3 reduces to alternation between clustering and matrix approx-
imation with the annealing ensuring that the algorithm converges. This results in a new algorithm
for subset-selection in vector spaces.

For the case of manifolds implicitly speciﬁed using samples , one can approach the problem in one
of two ways. The ﬁrst would be to obtain an embedding of the spa ce into a Euclidean space and
apply the special case of the algorithm for M = Rn . The embedding here needs to preserve the
geodesic distances between all pairs of points. Multi-dimensional scaling can be used for this pur-
pose. However, recent methods have also focused on estimating logarithmic maps numerically from
sampled data points [25]. This would make the algorithm directly applicable for such cases, without
the need for a separate embedding. Thus the proposed formalism can accommodate manifolds with
known and unknown geometries.

However, the formalism is limited to manifolds of ﬁnite dime nsion. The case of in ﬁnite dimen-
sional manifolds, such as diffeomorphisms [26], space of closed curves [27], etc. pose problems
in formulating the diversity cost function. While Jdiv could have been framed purely in terms of
pairwise geodesics, making it extensible to in ﬁnite dimens ional manifolds, it would have made the
optimization a signiﬁcant bottleneck, as already discusse d in section 2.
Computational Complexity: The computational complexity of computing exponential map and
its inverse is speciﬁc to each manifold. Let n be the number of data points and K be the number
of exemplars to be selected. Table 2 enumerates the complexity of different computational step of
the algorithm. The last two rows show the complexity of an efﬁ cient algorithm proposed by [28] to
compute the exponential map and its inverse for the case of Grassmann manifold Gm,p .
4 Experiments

Baselines: We compare the proposed algorithm with two baselines. The ﬁr st baseline is a
clustering-based solution to subset selection, where we cluster the dataset into K clusters, and pick
as exemplar the data point that is closest to the cluster centroid. Since clustering optimizes only the

5

3

2.5

2

1.5

1

 

800

600

400

 

Jdiv
Jrep

800

600

400

 

Jdiv
Jrep

 
−1.5
−1

0

4

5

0.5

0

−0.5

−1

200

  Data
  Jrep
  Jdiv
  Proposed
5
8
7
6
5
8
7
6
Iteration Number
Iteration Number
1
2
3
(a) Subset Selection
(c) Without Annealing
(b) Convergence With Annealing
Figure 1: Subset selection for a simple dataset consisting of unbalanced classes in R4 . (a) Data projected on
R2 for visualization using PCA. While trying to minimize the representational error, Jrep picks two exemplars
from the dominant class. Jdiv picks diverse exemplars but from the boundaries. The proposed approach strikes
a balance between the two and picks one ‘representative’ exemplar from each class. Convergence Analysis of
algorithm 3: (b) with annealing and (c) without annealing.

10 11 12

10 11 12

200

0
 
0

3

4

3

4

1

2

1

2

0
 
0

9

9

representation cost-function, we do not expect it to have the diversity of the proposed algorithm.
This corresponds to the special case of optimizing only Jrep . The second baseline is to apply a
tangent-space approximation to the entire data-set at the mean of the dataset, and then apply a
subset-selection algorithm such as RRQR. This corresponds to optimizing only Jdiv where the
input matrix is the matrix of tangent vectors. Since minimization of Jrep is not explicitly enforced,
we do not expect the exemplars to be the best representatives, even though the set is diverse.

A Simple Dataset: To gain some intuition, we ﬁrst perform experiments on a simp le synthetic
dataset. For easy visualization and understanding, we generated a dataset with 3 unbalanced classes
in Euclidean space R4 . Individual cost functions, Jrep and Jdiv were ﬁrst optimized to pick three
exemplars using algorithms 1 and 2 respectively. Selected exemplars have been shown in ﬁgure 1(a),
where the four dimensional dataset has been projected into two dimensions for visualization using
Principal Component Analysis (PCA). Despite unbalanced class sizes, when optimized individually,
Jdiv seeks to select exemplars from diverse classes but tends to pick them from the class boundaries.
While unbalanced class sizes cause Jrep to pick 2 exemplars from the dominant cluster. Algorithm
3 iteratively optimizes for both these cost functions and picks an exemplar from every class. These
exemplars, are closer to the centroid of the individual classes.

Figure 1(b) shows the convergence of the algorithm for this simple dataset and compares it
with the case when no annealing is applied ( ﬁgure 1(c)).
Jrep and Jdiv plots are shown as the
iterations of algorithm 3 progresses. When annealing is applied, the tolerance value (tol) is
increased by 0.05 in each iteration. It can be noted that in this case the algorithm converges to
a steady state in 7 iterations (tol = 1.35). If no annealing is applied, the algorithm does not converge.

Shape sampling/summarization: We conducted a real shape summarization experiment on the
MPEG dataset [29]. This dataset has 70 shape classes with 20 shapes per class. For our experi-
ments, we created a smaller dataset of 10 shape classes with 10 shapes per class. Figure 2(a) shows
the shapes used in our experiments. We use an afﬁne-invarian t representation of shapes based on
landmarks. Shape boundaries are uniformly sampled to obtain m landmark points. These points are
concatenated in a matrix to obtain the landmark matrix L ∈ Rm×2 . Left singular vectors (Um×2 ),
obtained by the singular value decomposition of matrix L = U ΣV T , give the afﬁne-invariant rep-
resentation of shapes [30]. This afﬁne shape-space U of m landmark points is a 2-dimensional
subspace of Rm . These p-dimensional subspaces in Rm constitute the Grassmann manifold Gm,p .
Details of the algorithms for the computation of exponential and inverse exponential map on Gm,p
can be found in [28] and has also been included in the supplemental material.

In the experiment, the cardinality of the subset was set to 10. As the number of shape classes is also
10, one would ideally seek one exemplar from each class. Algorithms 1 and 2 were ﬁrst individually
optimized to select the optimal subset. Algorithm 1 was applied intrinsically on the manifold with
multiple initializations. Figure 2(b) shows the output with the least cost among these initializations.
For algorithm 2, data points were projected on the tangent space at the mean using the inverse
exponential map and the selected subset is shown in ﬁgure 2(c ). Individual optimization of Jrep
results in 1 exemplar each from 6 classes, 2 each from 2 classes (‘apple’ and ‘ ﬂower’) and misses
2 classes (‘bell’ and ‘chopper’). While, individual optimization of Jdiv alone picks 1 each from 8
classes, 2 from the class ‘car’ and none from the class ‘bell’. It can be observed that exemplars
chosen by Jdiv for classes ‘glass’, ‘heart’,‘ ﬂower’ and ‘apple’ tend to be unusual members of the

6

(d)
(c)
(b)
(a)
Figure 2: (a) 10 classes from MPEG dataset with 10 shapes per class. Comparison of 10 exemplars selected by
(b)Jrep , (c) Jdiv and (d) Proposed Approach. Jrep picks 2 exemplars each from 2 classes (‘apple’ and ‘ ﬂower’)
and misses ‘bell’ and ‘chopper’ classes. Jdiv picks 1 from 8 different classes, 2 exemplars from class ‘car’ and
none from class ‘bell’. It can be observed that exemplars chosen by Jdiv for classes ‘glass’, ‘heart’, ‘ ﬂower’
and ‘apple’ tend to be unusual members of the class. It also picks up the ﬂipped car. While the proposed
approach picks one representative exemplars from each class as desired.

class. It also picks up the ﬂipped car. Optimizing for both Jdiv and Jrep using algorithm 3 picks one
‘representative’ exemplar from each class as shown in ﬁgure 2(d).

These exemplars picked by the three algorithms can be further used to label data points. Table 3
shows the confusion table thus obtained. For each data point, we ﬁnd the nearest exemplar, and
label the data point with the ground-truth label of this exemplar. For example, consider the row
labeled as ‘bell’. All the data points of the class ‘bell’ were labeled as ‘pocket’ by Jrep while Jdiv
labeled 7 data points from this class as ‘chopper’ and 3 as ‘pocket’. This confusion is largely due
to both Jrep and Jdiv having missed out picking exemplars from this class. The proposed approach
correctly labels all data points as it picks exemplars from every class.

Glass
Heart
Apple
Bell
Baby
Chopper
Flower
Car
Pocket
Teddy

Glass
(10,10,10)

Heart

Apple

Bell

Baby

Chopper

Flower

Car

Pocket

Teddy

(10,10,10)
(0,1,0)

(8,7,10)

(2,0,0)

(8,0,0)

(0,0,10)

(10,10,10)

(2,0,0)
(0,7,0)

(0,10,10)

(0,2,0)
(10,3,0)

(10,10,10)

(10,10,10)

(10,10,10)

(10,10,10)

Table 3: Confusion Table. Entries correspond to the tuple (Jrep , Jdiv , P roposed). Row labels correspond to
the ground truth labels of the shape and the column labels correspond to the label of the nearest exemplar. Only
non-zero entries have been shown in the table.

KTH human action dataset: The next experiment was conducted on the KTH human action
dataset [31]. This dataset consists of videos with 6 actions conducted by 25 persons in 4 different
scenarios. For our experiment, we created a smaller dataset of 30 videos with the ﬁrst 5 human
subjects conducting 6 actions in the s4 (indoor) scenario. Figure 3(a) shows sample frames from
each video. This dataset mainly consists of videos captured under constrained settings. This makes it
difﬁcult to identify the ‘usual’ or ‘unusual’ members of a cl ass. To better understand the performance
of the three algorithms, we synthetically added occlusion to the last video of each class. These
occluded videos serve as the ‘unusual’ members.

Histogram of Oriented Optical Flow (HOOF) [32] was extracted from each frame to obtain a nor-
malized time-series for the videos. A Linear Dynamical System (LDS) is then estimated from
this time-series using the approach in [11]. This model is described by the state transition equa-
tion: x(t + 1) = Ax(t) + w(t) and the observation equation z (t) = C x(t) + v(t), where

7

(d) Proposed
(c) Jdiv
(b) Jrep
(a) Dataset
Figure 3: (a) Sample frames from KTH action dataset [31]. From top to bottom action classes are { box, run,
walk, hand-clap, hand-wave and jog }. 5 exemplars selected by: (b)Jrep , (c) Jdiv and (d) Proposed. Exemplars
picked by Jrep correspond to { box, run, run, hand-clap, hand-wave } actions. While Jdiv selects { box, walk,
hand-clap, hand-wave and jog }. Proposed approach picks { box, run, walk, hand-clap and hand-wave }.

x ∈ Rd is the hidden state vector, z ∈ Rp is the observation vector, w(t) ∼ N (0, Θ) and
v(t) ∼ N (0, Ξ) are the noise components. Here, A is the state-transition matrix and C is the
observation matrix. The expected observation sequence of model (A, C ) lies in the column space
of the in ﬁnite extended ‘observability’ matrix which is com monly approximated by a ﬁnite matrix
m = [C T , (CA)T , (CA2 )T , . . . , (CAm−1 )T ]. The column space of this matrix OT
m ∈ Rmp×d is
OT
a d-dimensional subspace and hence lies on the Grassmann manifold.
In this experiment, we consider the scenario when the number of classes in a dataset is unknown.
We asked the algorithm to pick 5 exemplars when the actual number of classes in the dataset is 6.
Figure 3(b) shows one frame from each of the videos selected when Jrep was optimized alone. It
picks 1 exemplar each from 3 classes (‘box’,‘hand-clap’ and ‘hand-wave’), 2 from the class ‘run’
while misses out on ‘walk’ and ‘jog’. On the other hand, Jdiv (when optimized alone) picks 1 each
from 5 different classes and misses the class ‘run’. It can be seen that Jdiv picks 2 exemplars that
are ‘unusual’ members (occluded videos) of their respective class. The proposed approach picks
1 representative exemplar from 5 classes and none from the class ‘jog’. The proposed approach
achieves both a diverse selection of exemplars, and also avoids picking outlying exemplars.
Effect of Parameters and Initialization:
In our experiments, the effect of tolerance steps (δ ) for
smaller values (< 0.1) has very minimal effect. After a few attempts, we ﬁxed this v alue to 0.05 for
all our experiments. In the ﬁrst iteration, we start with tol = 1. With this value, algorithm 2 accepts
any swap that increases Jdiv . This makes output of algorithm 2 after ﬁrst iteration almos t insensitive
to initialization. While, in the later iterations, swaps are accepted only if they increase the value of
Jdiv signiﬁcantly and hence input to algorithm 2 becomes more imp ortant with the increase in tol .

5 Conclusion and Discussion

In this paper, we addressed the problem of selecting K exemplars from a dataset when the dataset has
an underlying manifold structure to it. We utilized the geometric structure of the manifold to formu-
late the notion of picking exemplars which minimize the representational error while maximizing the
diversity of exemplars. An iterative alternation optimization technique based on annealing has been
proposed. We discussed its convergence and complexity and showed its extension to data manifolds
and Euclidean spaces. We showed summarization experiments with real shape and human actions
dataset. Future work includes formulating subset selection for in ﬁnite dimensional manifolds and
efﬁcient approximations for this case. Also, several speci al cases of the proposed approach point to
new directions of research such as the cases of vector spaces and data manifolds.
Acknowledgement: This research was funded (in part) by a grant N 00014 − 09 − 1 − 0044 from
the Ofﬁce of Naval Research. The ﬁrst author would like to tha
nk Dikpal Reddy and Sima Taheri
for helpful discussions and their valuable comments.

References

[1] K. Liu, E. Terzi, and T. Grandison, “ManyAspects: a syste m for highlighting diverse concepts in docu-
ments,” in Proceedings of VLDB Endowment, 2008.

8

IEEE

Linear Algebra and Its Applications, vol. 88, pp. 67–82,

[2] N. Shroff, P. Turaga, and R. Chellappa, “Video Pr ´ecis: H ighlighting diverse aspects of videos,”
Transactions on Multimedia, vol. 12, no. 8, pp. 853 –868, Dec. 2010.
[3] I. Simon, N. Snavely, and S. Seitz, “Scene summarization for online image collections,” in ICCV, 2007.
[4] W. Cochran, Sampling techniques. Wiley, 1977.
[5] Y. Yue and T. Joachims, “Predicting diverse subsets usin g structural svms,” in ICML, 2008.
[6] J. Carbonell and J. Goldstein, “The use of mmr, diversity -based reranking for reordering documents and
reproducing summaries,” in SIGIR, 1998.
[7] M. Gu and S. Eisenstat, “Efﬁcient algorithms for computi
ng a strong rank-revealing QR factorization,”
SIAM Journal on Scienti ﬁc Computing , vol. 17, no. 4, pp. 848–869, 1996.
[8] P. Drineas, M. Mahoney, and S. Muthukrishnan, “Relative -error CUR matrix decompositions,” SIAM
Journal on Matrix Analysis and Applications, vol. 30, pp. 844–881, 2008.
[9] C. Boutsidis, M. Mahoney, and P. Drineas, “An improved ap proximation algorithm for the column subset
selection problem,” in SODA, 2009.
[10] D. Kendall, “Shape manifolds, Procrustean metrics and complex projective spaces,” Bulletin of London
Mathematical society, vol. 16, pp. 81–121, 1984.
[11] S. Soatto, G. Doretto, and Y. N. Wu, “Dynamic textures,”
ICCV, 2001.
[12] J. D. Lafferty and G. Lebanon, “Diffusion kernels on sta tistical manifolds,” Journal of Machine Learning
Research, vol. 6, pp. 129–163, 2005.
[13] P. T. Fletcher, C. Lu, S. M. Pizer, and S. C. Joshi, “Princ ipal geodesic analysis for the study of nonlinear
statistics of shape,”
IEEE Transactions on Medical Imaging, vol. 23, no. 8, pp. 995–1005, August 2004.
[14] A. Srivastava, S. H. Joshi, W. Mio, and X. Liu, “Statisti cal shape analysis: Clustering, learning, and
testing,”
IEEE Transactions on pattern analysis and machine intelligence, vol. 27, no. 4, 2005.
[15] G. Golub, “Numerical methods for solving linear least s quares problems,” Numerische Mathematik, vol. 7,
no. 3, pp. 206–216, 1965.
[16] T. Chan, “Rank revealing QR factorizations,”
1987.
[17] A. Frieze, R. Kannan, and S. Vempala, “Fast Monte-Carlo algorithms for ﬁnding low-rank approxima-
tions,” Journal of the ACM (JACM), vol. 51, no. 6, pp. 1025–1041, 2004.
[18] A. Deshpande and L. Rademacher, “Efﬁcient volume sampl
ing for row/column subset selection,” in Foun-
dations of Computer Science (FOCS), 2010.
[19] G. Gan, C. Ma, and J. Wu, Data clustering: theory, algorithms, and applications. Society for Industrial
and Applied Mathematics, 2007.
[20] I. Dhillon and D. Modha, “Concept decompositions for la rge sparse text data using clustering,” Machine
learning, vol. 42, no. 1, pp. 143–175, 2001.
[21] B. J. Frey and D. Dueck, “Clustering by passing messages between data points,” Science, vol. 315, pp.
972–976, Feb. 2007.
[22] R. Subbarao and P. Meer, “Nonlinear mean shift for clust ering over analytic manifolds,” in CVPR, 2006.
[23] A. Goh and R. Vidal, “Clustering and dimensionality red uction on riemannian manifolds,” in CVPR, 2008.
[24] H. Karcher, “Riemannian center of mass and molli ﬁer smo othing,” Communications on Pure and Applied
Mathematics, vol. 30, no. 5, pp. 509–541, 1977.
[25] T. Lin and H. Zha, “Riemannian manifold learning,”
Intelligence, vol. 30, pp. 796–809, 2008.
[26] A. Trouv ´e, “Diffeomorphisms groups and pattern match ing in image analysis,”
Computer Vision, vol. 28, pp. 213–221, July 1998.
[27] W. Mio, A. Srivastava, and S. Joshi, “On shape of plane el astic curves,”
Vision, vol. 73, no. 3, pp. 307–324, 2007.
[28] K. Gallivan, A. Srivastava, X. Liu, and P. Van Dooren, “E fﬁcient algorithms for inferences on grassmann
manifolds,” in IEEE Workshop on Statistical Signal Processing, 2003.
[29] L. Latecki, R. Lakamper, and T. Eckhardt, “Shape descri ptors for non-rigid shapes with a single closed
contour,” in CVPR, 2000.
[30] E. Begelfor and M. Werman, “Afﬁne invariance revisited ,” in CVPR, 2006.
[31] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing huma n actions: a local SVM approach,” in ICPR,
2004.
[32] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal, “H istograms of oriented optical ﬂow and binet-
cauchy kernels on nonlinear dynamical systems for the recognition of human actions,” in CVPR, 2009.

IEEE Transactions on Pattern Analysis and Machine

International Journal of

International Journal of Computer

9


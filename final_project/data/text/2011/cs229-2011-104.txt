PREDICTING PREFERENCES
Analyzing Reading Behavior and News Preferences
Soravis Srinawakoon Potcharapol Suteparuk
Advised by Richard Socher

1 INTRODUCTION

News reading has gradually become a signiﬁcant activity in our daily life as the world is saturated with
new information. We consume so much information every day that it becomes extremely diﬃcult to ﬁlter
and extract only the interesting and relevant news to us. To improve readers’ experience, we need to be able
to accurately predict the new stories that are most probable for them to read. This reduces to predicting
preferences which is a common problem of ﬁnding out which items are most relevant to each user and
essentially rank or feed them to tailored users. In our study, we in particular look at the stories users click
and read previously in a news-reading mobile application Pulse in order to predict which stories users are
most likely to read in the next few days.
Similar challenge has been recently tackled by researchers in data mining and natural language processing
because its implications can be applied to other popular areas such as search engine recommendation system.
This paper considers applying diﬀerent machine learning algorithms in the realm of supervised learning and
unsupervised learning in attempt to predict the news stories that each user are most likely to read from a set
of all available stories, which are much too large for the average users to parse and ﬁnd the most relevant ones.
In particular, using users’ click and read history, we explore text categorization algorithms by comparing the
accuracy of several supervised learning methods such as a simple Naive Bayes, L2-norm regularized logistic
regression, and L1-norm regularized logistic regression with Lasso algorithm. We then move on to investigate
common unsupervised learning technique such as k-mean clustering of users and a simple implementation of
collaborative ﬁltering.

2 DATASET AND TEXT REPRESENTATION

Given three set of input data from Pulse: 26,903 available stories at a given period, all stories read from a
sample of 1,068 Pulse users, and all click-through stories from the sample sample of 1,068 Pulse users during
the given period, we extract and represent all the available stories as feature vectors X = {x(i)} where x(i)
corresponds to each story’s input features. We represent this input feature using a simple form of TF-IDF
term weight (term frequency times inverse document frequency) where x(i) ∈ Rn , n = |W | is a total number
of distinct terms appeared in all document. As a starting baseline, we represent each document using only
its title and feed title so |W | is total number of distinct words appeared in all title and feed title. This gives
(cid:40)
term j in document i a TF-IDF weight of

x(i)
j =

if A(i, j) = 0
otherwise,

0
(1 + ln A(i, j )) ln(

|S |
A(j ) )

where A(i, j ) is the number of occurences of term j in a document i, A(j ) is the number of avalaible stories
that contain the term j , and |S | is total number of avalaible stories.
Later, we will explore a subset of our available data and represent each document by its title and its
content. We will make a slight adjustment to our TF-IDF term weight by adding a cosine normalization to
our feature vectors which has shown a signiﬁcant improvement in prediction for large document size as it
reduces the impact of document length [1].
Thus, the ﬁnal normalized weight is:
(cid:113)(cid:80)n
non − normalized x(i)
j
j (cid:48) × x(i)
j (cid:48) x(i)
j (cid:48)
where we sum over all the terms j in the denominator but we denote it with j (cid:48) to avoid confusion.

x(i)
j =

1

3 METHODOLOGY AND CLASSIFIER

In this part, we present all of the method and classiﬁer used. The result of each will be present in part
four where we evaluate each classiﬁers performance by measuring its RMSE, precision, recall and F-measure.

3.1 Features Selection

Our data set and in general the text categorization data are very sparse with unclear relationship between
input features and class labels. Therefore, we try to select features that are most relevant using typical text
preprocessing methods such as stop words removal, singletons removal, and stemming using Stanford JAVA
NLP library [2]. Unless otherwise stated, we will treat our data set as the combined data set of users read
stories and click-through stories.

3.2 Naive Bayes

Naive Bayes classiﬁcation assumes that each feature in our data set is conditionally independent to each
n ), which has a given label y (i) ∈ {0, 1}, we can
(cid:89)
other. That is, for each feature X (i) = (x(i)
1 , x(i)
2 , . . . , x(i)
assume the following
j = xj |y (i) = r).
P (X (i) = X |y (i) = r) =
P (x(i)
i
(cid:89)
Apply this assumption to make our hypothesis as followed,
P (X |y = r)P (y = r)
P (X )
i

P (X = xi |y = r)P (y = r).

hθ (X ) = argmaxr

= argmaxr

In our model, we use the above formula to classify whether a speciﬁc user of the application is likely to
read the given story (X ), whose title includes words x1 , x2 , x3 , . . . etc. Based on the training data, we can
calculate the conditional probability that each of these words occurs in the stories that the user read (y = 1)
and did not read (y = 0). The product of these conditional probabilities times the prior probability gives
each classiﬁcation its likelihood for X . Our hypothesis outputs the class that has the higher corresponding
likelihood.

3.3 Logistic Regression

The ﬁrst baseline of our prediction is by using logistic regression to classify our data. This model is the
special case of Generalized Linear Models (GLMs) for Bernoulli variable. The idea is to parameterize our
hypothesis with the sigmoid function:
P (y |X, θ) = (hθ (X ))y (1 − hθ (X ))y ; hθ (X ) = (1 + e−θT X )−1 .

In our case, X represents the stories with title words as their features. hθ (X ) is our hypothesis and the
probability of X being labeled 1 (i.e. read), and hence the equation above. We then can use the training
l(θ) = − n(cid:88)
data set to train our parameters θ by minimizing the negated log-likelihood
i=1
which we will use later in part 3.4 and 3.5

ln(1 + e−θT x(i) y(i)

)

3.4 L2-norm Regularized Logistic Regression

Taking our two data sets into account, we minimize a cost function that incorporates logistic regression for
users read stories, logistic regression for users clickthrough stories, and a penalty L2-norm of the diﬀerences
between θr and θc found by training logistic regression with read stories and clickthrough stories respectively.

2

Formally, we want to ﬁt our parameters θ to classify whether the users read the stories and whether the users
perform a clickthrough, and thus we want the two θ ’s to be a close approximation of each other. Thus we
L2-regularize our dataset and θ can be found, using gradient descent, by minimizing
l(θ) = l(θr ) + l(θc ) + β ||θr − θc ||
where β controls the relative weighting between trying to minimize the ﬁrst two negated log-likelihood and
of ensuring that the parameters found are as close to each other as possible. We chose β to be Mallows C p as
it has been demonstrated to be a good regularized parameter that addresses the issues of overﬁtting which
is our main concern when developing our classiﬁer. [3]

3.5 L1-norm Regularized Logistic Regression with Lasso Algorithm
Next, we L1-regularize our data set by using the well-known lasso algorithm which has shown to be
(cid:88)
powerful in solving text categorization problem [4]. We thus ﬁnd θlasso by minimizing
|θj |
j

l(θlasso ) = l(θ) + β

where β = C p again controls the degree of regularization.

3.6 K-Mean Clustering

Now, we switch to unsupervised learning by using k-mean clustering. Speciﬁcally we group users and
make predictions based on users being in the same cluster using cosine similarity. Now, the chosen k value
is critical as the number of groups can signiﬁcantly aﬀect the accuracy result. We repeatedly tried various
value of k until we settled with k = 23. This value can also be seen as a rough representation of number of
topics or category in our news.

3.7 Collaborative Filtering

Widely used technique in suggesting news based on other users who have read similar articles. This sim-
ple method yet eﬀective have been implemented extensively for example on Netﬂix for movie suggestion and
Amazon for product recommendation. It indirectly takes into account a correlation between reading similar
articles between users. Though simple, this approach capitalizes on a topic model by a nave assumption that
if two users read something in common in the past, they should read something similar in the future.

4 RESULTS AND CONCLUSIONS

To compare our testing result, a simple accuracy percentage is useless because our data is very sparse
so that a simple classiﬁer that predicts zero all the time would achieve 99% accuracy. Instead, we measure
our result by comparing its RMSE (root-mean-square-error), precision, recall and f-measure to get a better
predictive indication [see table 1].

RMSE Precision Recall F-Measure
Algorithm
0.4136
0.3974
0.4312
0.78
Naive Bayes
0.85
0.3892
0.3418
0.3640
Logistic Regression
0.4280
0.4375
0.4189
0.80
L1-Norm Logistic Regression
0.5448
0.5196
0.5725
0.72
L2-Norm Logistic Regression
0.2785
0.3041
0.2569
0.94
K-Mean Clustering
0.96
0.1201
0.4880
0.1928
Collaborative Filtering
Table 1: RMSE, precision, recall, and F-measure for diﬀerent learning algorithm

Surprisingly Naive Bayes performs better than our simple logistic regression. This is possibly because the
way we incorporate the feed title into our title when we train and test our classiﬁer as this has shown a slight

3

improvement in our algorithms (since feed title does give a good indication of particular topic as some feed
are really speciﬁc to one news category). Our logistic regression could not take into account the feed title as
well because it merely tries to separate data in the feature space so it does not consider the distinction of
each feed as well as Naive Bayes. L1-Norm LR performs a little bit better but again the model suﬀers from
almost the exact same reason. L2-Norm however outperforms every algorithm and by far produces the best
result. This makes sense because here we take into account the distinction between users read stories and
click-through stories and it turns out that click-through stories provide a better indication of what users are
likely to read in the future.
Unsupervised learning noticeably produces much worse result than supervised learning algorithms. Con-
ceivably, k-mean clustering does not produce a good result because there is not really a direct relationship
between TF-IDF and the group that it belongs to because the feature space we consider does not really
model the topic model even if the cosine similarity implies that they belong to the same group. Similarly,
our collaborative ﬁltering uses TF-IDF to group similar users together but the TF-IDF does not provide a
good insight about each topic model thus two users reading diﬀerent topics that contain similar words will
be grouped together. Also, because our algorithm only consider a pair of users each time, it is much harder
to come up with a good match with only 1,000 users with more than 26,000 available stories. The number
of users does not scale well with number of stories.
Note that it turns out when we probe through 30 stories (it already takes too long to crawl the website
and extract the word so we can only use a small subset for testing) only produce a slightly better result than
using stories title and feed as our news representation so we don’t consider this approach here since 30 is too
little to conclude anything.

F-measure compared between diﬀerent learning algorithms

L2-Norm Regularized Logistic Regression

K-Mean Clustering

4

F-measure plot against the number of training day using L2-Norm regularized logistic regression [left]
and K-Mean clustering [right]. Each black line represents each user for a sample of 50 users, while the red
line shows the average F-measure. Some irregular patterns arise because some users read only at the certain
period of a month so increasing the number of training days only changes f-measure slightly. For example,
the graph on the left shows that some users read a lot of stories in the beginning (a sharp rise in f-measure
which allows us to accurately predict his/her top stories later on. These irrigular patterns also show that
our classiﬁer’s predictive ability also relies on the structure of the stories each user read because if the users
read randomly, clearly the classiﬁer will perform poorly whereas users who read a lot of repetitive stories will
allow our classiﬁer to perform well. We present here the contrast between two algorithms for comparison and
clearly L2-Norm regularized logistic regression performs much better than k-mean clustering. Again, this
is perhaps due to the underlying structure of our feature space that does not correlate well with clustering
algorithm.

5 FUTURE WORKS

Further application of predicting news preferences can be far-reaching and signiﬁcant as this essentially
reduces to a problem of ﬁnding preferences and recommending particular group of similar items based on
past input. Based on our observation thus far, the model we use did not achieve a considerably practical
result perhaps because it does not directly link words (features) to its actual topic so the next reasonable
step to take is to consider a model that takes topic and word clustering into account. Latent Dirichlet Al-
location which gives a topic model that cluster words with similar things can be explored. Additionally, we
did not have enough time to incorporate automatic relevance determination [5] and named-entity recognition
which classiﬁes words with similar spelling but diﬀerent semantic diﬀerently based on the context. These
pre-processing on the feature space should relate our feature space closer to its actual topic model. Based
on our result, it can be seen that some of the output articles contain similar words but these words can have
diﬀerent meaning so this system can improve our classiﬁer as the best existing NER system (MUC-7) has
shown 92% recall and 93% precision with errors lie mostly in the entries that lack obvious English rules or
condition of context [6]. We should also look into how to incorporate our click-through data into our classiﬁer
model so that it generates a more meaningful data rather than something similar to read stories as it has
been shown to produce good result with text classiﬁcation [7].

6 REFERENCES

1. Salton, G. and Buckley, C. (1988), “Term-Weighting Approaches in Automatic Text Retrieval, Infor-
mation Processing and Management, 24, 513-519.

2. Stanford corenlp tools version 1.2.0. (2011, Sep 14). Retrieved from
http://nlp.stanford.edu/software/corenlp.shtml.

3. Genkin, A., Lewis, D. and Madigan, D. (2004) Sparse Logistic Regression for Text Categorization.

4. Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
Series B (Methodological) 58(1), 267288 (1996).

5. Liitiainen, E. (2006). Automatic relevance determination. In Finland: Retrieved from
http://www.cis.hut.ﬁ/Opinnot/T-61.6040/presentations s06/presentation elia.pdf.

6. Black W., Rinaldi F., and Mowatt D., “Facile: Description of the NE System Used for MUC-7, De-
partment of Language Engineering UMIS : Retrieved from
http://www-nlpir.nist.gov/related pro jects/muc/proceedings/muc 7 proceedings/facile muc7.pdf.

7. Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 133142
(2002).

5


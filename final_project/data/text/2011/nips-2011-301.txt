EigenNet: A Bayesian hybrid of generative and
conditional models for sparse learning

Yuan Qi
Computer Science and Statistics Depts.
Purdue University
West Lafayette, IN 47907, USA

Feng Yan
Computer Science Dept.
Purdue University
West Lafayette, IN 47907, USA

Abstract

For many real-world applications, we often need to select correlated variables—
such as genetic variations and imaging features associated with Alzheimer’s
disease—in a high dimensional space. The correlation between variables presents
a challenge to classical variable selection methods. To address this challenge,
the elastic net has been developed and successfully applied to many applications.
Despite its great success, the elastic net does not exploit the correlation informa-
tion embedded in the data to select correlated variables. To overcome this limi-
tation, we present a novel hybrid model, EigenNet, that uses the eigenstructures
of data to guide variable selection. Speciﬁcally, it integrates a sparse conditional
classiﬁcation model with a generative model capturing variable correlations in a
principled Bayesian framework. We develop an efﬁcient active-set algorithm to
estimate the model via evidence maximization. Experimental results on synthetic
data and imaging genetics data demonstrate the superior predictive performance
of the EigenNet over the lasso, the elastic net, and the automatic relevance deter-
mination.

1

Introduction

In this paper we consider the problem of selecting correlated variables in a high dimensional space.
Among many variable selection methods, the lasso and the elastic net are two popular choices (Tib-
shirani, 1994; Zou and Hastie, 2005). The lasso uses a l1 regularizer on model parameters. This
regularizer shrinks the parameters towards zero, removing irreverent variables and yielding a sparse
model (Tibshirani, 1994). However, the l1 penalty may lead to over-sparisiﬁcation: given many
correlated variables, the lasso often only select a few of them. This not only degenerates its pre-
diction accuracy but also affects the interpretability of the estimated model. For example, based on
high-throughput biological data such as gene expression and RNA-seq data, it is highly desirable
to select multiple correlated genes associated with a phenotype since it may reveal underlying bi-
ological pathways. Due to its over-sparsiﬁcation, the lasso may not be suitable for this task. To
address this issue, the elastic net has been developed to encourage a grouping effect, where strongly
correlated variables tend to be in or out of the model together (Zou and Hastie, 2005). However,
the grouping effect is just the result of its composite l1 and l2 regularizer; the elastic net does not
explicitly incorporate correlation information among variables in its model.
In this paper, we propose a new sparse Bayesian hybrid model to utilize the eigen-information
extracted from data for the selection of correlated variables. Speciﬁcally, it integrates a sparse con-
ditional classiﬁcation model with a generative model in a principle Bayesian framework (Lasserre
et al., 2006): the conditional model achieves sparsity via automatic relevance determination (ARD)
(MacKay, 1991), an empirical Bayesian approach for model sparisiﬁcation; and the generative
model is a latent variable model in which the observations are the eigenvectors of the unlabeled
data, capturing correlations between variables. By integrating these two models together, the hybrid

1

model enables identiﬁcation of groups of correlated variables guided by the eigenstructures. At the
same time, the model passes the information from its conditional part to its generative part, selecting
informative eigenvectors for the classiﬁcation task. Furthermore, using the Bayesian hybrid model,
we can automate the estimation of model hyperparameters.
From the regularization perspective, the new hybrid model naturally generalizes the elastic net using
a composite regularizer adaptive to the data eigenstructures. It contains a sparsity regularizer and
a directional regularizer that encourages selecting variables associated with eigenvectors chosen by
the model. When the variables are independent of each other, the eigenvectors are parallel to the
axes and this composite regularizer reduces to the combination of the ARD and a l2 regularizer
(similar to the composite regularizer of the elastic net). But when some of the input variables are
strongly correlated, the regularizer will encourage the classiﬁer aligned with eigenvectors selected
by the model. On one hand, our model is like the elastic net to retain ‘all the big ﬁsh’. On the other
hand, our model is different from the elastic net by the guidance from the eigen-information. Hence
the name EigenNet.
Experiments on synthetic data are presented in Section 5. Our results demonstrate that the EigenNet
signiﬁcantly outperforms the lasso, and the elastic net in terms of prediction accuracy. We applied
this new approach to two tasks in imaging genetics:
i) predicting cognitive function of healthy
subjects and AD patients based on brain imaging markers, and ii) classifying the healthy and AD
subjects based on single-nucleotide polymorphism (SNP) data. Compared to the lasso, the elastic
net and the ARD, our approach achieves improved prediction accuracy.

σ(yiwTxi )

2 Background: lasso and elastic net
We denote n independent and identically distributed samples as D = {(x1 , y1 ), . . . , (xn , yn )},
where xi is a p dimensional input features (i.e., explanatory variables) and yi is a scalar label (i.e.,
response). Also, we denote [x1 , . . . , xn ] by X and (y1 , . . . , yn ) by y. Although our presentation
focuses on the binary classiﬁcation problem (yi ∈ {−1, 1}), our approach can be readily applied
to other problems such as regression and survival analysis by choosing appropriate likelihood func-
tions.
n(cid:89)
For classiﬁcation, we use a probit model as the data likelihood:
p(y|X, w) =
i=1
where σ(z ) is the Gaussian cumulative distribution function and w denotes the classiﬁer.
To identify relevant variables for high dimensional problems, the lasso (Tibshirani, 1994) uses a l1
penalty, effectively shrinking w and b towards zero and pruning irrelevant variables. In a probabilis-
p(w) = (cid:89)
tic framework this penalty corresponds to a Laplace prior distribution:
λ exp(−λ|wj |)
j
where λ is a hyperparameter that controls the sparsity of the estimated model. The larger the hyper-
parameter λ, the sparser the model.
As described in Section 1, the lasso may over-penalize relevant variables and hurt its predictive
combined regularizer corresponds to the following prior distribution, p(w) ∝ (cid:81)
performance, especially when there are strongly correlated variables. To address this issue, the
elastic net (Zou and Hastie, 2005) combines l1 and l2 regularizers to avoid the over-penalization. The
j exp(−λ1 |wj | −
j ), where λ1 and λ2 are hyperparameters. While it is well known that the elastic net tends to
λ2w2
select strongly correlated variables together, it does not uses correlation information embedded in
the unlabeled data. The selection of correlated variables is merely the result of a less aggressive
regularizer for sparisty.
Besides the elastic net, there are many variants (and extensions) to the lasso, such as the bridge
(Frank and Friedman, 1993) and smoothly clipped absolute deviation (Fan and Li, 2001). These
variants modify the l1 penalty to improve variable selection, but do not explicitly use the correlation
information embedded in data.

(1)

(2)

2

(a) Independent variables

(b) Correlated variables

Figure 1: Toy examples. (a) When the variables x1 and x2 are independent of each other, both the
lasso and the EigenNet select only x1 . (b) When the variables x1 and x2 are correlated, the lasso
selects only one variable. By contrast, guided by the major eigenvector of the data, the EigenNet
selects both variables.

3 EigenNet: eigenstructure-guided variable selection

In this section, we propose to use the covariance structure in data to guide the sparse estimation of
model parameters. First, let us consider the following toy examples.

3.1 Toy examples

Figure 1(a) shows samples from two classes. Clearly the variables x1 and x2 are not correlated.
The lasso or the elastic net can successfully select the relevant variable x1 to classify the data. For
the samples in Figure 1(b), the variables x1 and x2 are strongly correlated. Despite the strong
correlation, the lasso would select only x1 and ignore x2 . The elastic net may select both x1 and x2
if the regularization weight λ1 is small and λ2 is big, so that the elastic net behaves like l2 regularized
classiﬁer. The elastic net, however, does not explore the fact that x1 and x2 are correlated.
Since the eigenstructure of the data covariance matrix captures correlation information between
variables, we propose to not only regularize the classiﬁer to be sparse, but also encourage it to be
aligned with certain eigenvector(s) that are helpful for the classiﬁcation task. Note that although
classical Fisher linear discriminant also uses the data covariance matrix to learn the classiﬁer, it
generally does not provide a sparse solution, thus not suitable for the task of selecting correlated
variables and removing irrelevant ones.
For the data in Figure 1(a), since the two eigenvectors are parallel to the horizontal and vertical axes,
the EigenNet essentially reduces to the elastic net and selects x1 . For the data in Figure 1(b), the
principle eigenvector can guide the EigenNet to select both x1 and x2 . The minor eigenvector is,
however, not useful for the classiﬁcation task (in general, we need to select which eigenvectors are
relevant to classiﬁcation). We use a Bayesian framework to materialize the above ideas as described
in the following section.

3.2 Bayesian hybrid of conditional and generative models

The EigenNet is a hybrid of conditional and generative models. The conditional component allows
us to learn the classiﬁer via ”discriminative” training; the generative component captures the cor-
relations between variables; and these two models are glued together via a joint prior distribution,
so that the correlation information is used to guide the estimation of the classiﬁer and the classiﬁ-
cation task is used to choose or scale relevant eigenvectors. Our approach is based on the general
Bayesian framework proposed by Lasserre et al. (2006)), which allows one to combine conditional
and generative models in an elegant principled way.
(cid:81)
i σ(yiwTxi ). For the classiﬁer w, we use a Gaussian prior: p(w) = (cid:81)p
Speciﬁcally, for the conditional model we have the same likelihood as (1), p(y|X, w) =
j=1 N (wj |0, β
−1
). We
j
will describe later how to efﬁciently learn the precision parameter βj from the data to obtain a sparse
classiﬁer.

3

01230.511.522.5  Lasso, EigenNet01230.511.522.5  LassoEigenNet(3)

N (vj |sj ˜w, (λv ηj )−1 I)

To encourage the classiﬁer aligned with certain
eigenvectors, we introduce ˜w—a latent vector
(tightly) linked to the classiﬁer w—in the genera-
p(V|s, ˜w) ∝ m(cid:89)
tive model:
j=1
where vj and ηj are the j -th eigenvector and
eigenvalue of the data covariance matrix, λv is a
hyperparameter, s = [s1 , . . . , sm ] are scaling fac-
tors for the parameter ˜w. To combat overﬁtting,
we assign a Gamma prior Gam(λv |c0 , d0 ) over
λv . Note that this generative model encourages ˜w
to align with the major eigenvectors with bigger
eigenvalues. However, eigenvectors are noisy and
not all of them relevant to the classiﬁcation task—
we need to select relevant eigenvectors (i.e.
the relevant sub-eigenspace) and remove irrelevant
ones.
p(s) ∝ m(cid:89)
To enable the selection of the relevant eigenvectors, we assign a Laplace prior on sj :
λs exp(−λs |sj |)
j=1

Figure 2: The graphical model of the EigenNet.

(4)

where λs is a hyperparameter.
Finally, to link the conditional and generative models together, we use a prior for ˜w conditional on
w:

p( ˜w|w) ∝ N ( ˜w|w, rI)
(5)
Note that the variance parameter r controls how similar w and ˜w are in our joint model. For
simplicity, we set r = 0 here so that p( ˜w|w) = δ( ˜w − w) where δ(a) = 1 if a = 1 and δ(a) = 0
otherwise. The graphical model representation of the EigenNet is given in Figure 2.

3.3 Model estimation

In this section we present how to estimate the model based on an empirical Bayesian approach.
Speciﬁcally, we will use expectation propagation (EP) (Minka, 2001) to estimate the posterior of
the classiﬁer w (and ˜w) and optimize the marginal likelihood of the joint model over the scaling
variables s and the precision parameters β .
(cid:17) (cid:89)
(cid:16) (cid:89)
First, given the hyperparameter λv and the latent variable s, the posterior distribution of w is
p(w|y, X, ) ∝ N (w|0, diag(β)−1 )
N (vj |sj w, (λv ηj )−1 I)
∝ N (w|mp , Vp ) (cid:89)
σ(yiwTxi )
i
j
(cid:80)
(cid:80)
σ(yiwTxi )
i
j I))−1 and mp = λv
where Vp = (diag(β + λv
j ηj sj vj . Then we initialize the EP up-
j ηj s2
dates by p(w) = N (w|mp , Vp ) and then iteratively approximate each likelihood factor σ(yiwTxi )
by a factor with the Gaussian form: N (ti |xT
−1
). In other words, EP maps the nonlinear non-
i w, h
−1
i
Gaussian factor to the Gaussian factor with the virtual observation ti and the noise variance h
.
i
After the convergence of EP, we obtain both the mean mw and the covariance Vw .
L(λv ) = Eqw [(cid:88)
Given the approximate posterior q(w), we maximize the variational lower bound over λv :
ln N (vj |sj w, (λv ηj )−1 I) + ln Gam(λv |c0 , d0 )]
j
ln λv − F
2 λv + (c0 − 1) ln λv − d0λv + contant
= pm
2

(7)

(6)

(8)

4

i=1,...,nyixiw˜wvjsjj=1,...,mβλvλsAlgorithm 1 The empirical Bayesian estimation algorithm
1. Initialize the model to contain a small fraction of features and initialize the
parameters: s = 0, λv = 1, t = 0 h = ∞.
2. Run EP to obtain the initial mean and the covariance mw and Vw .
3. Loop until convergence or reaching the maximum number of iterations
4. Loop over the j -th active set
a. Update β via (12) and (13).
b. If u2
j < rj , remove the features in the j -th active set from the model
c. Update the posterior mean mw and the covariance Vw based on EP.
d. Optimize the precision parameter λv via (9).
e. Optimize the scaling factors s via (11).
j vj ηj sj )Tmw + (cid:80)
j ηj − 2((cid:80)
where F = (cid:80)
i + (Vw )i,i ). As a result, we have
j ((mw )2
j ηj s2
λv = c0 − 1 + pm/2
.
d0 + F /2
L(s) = (cid:88)
(cid:0)Eqw [ln N (vj |sj w, (λv ηj )−1 I)] − λs |sj |(cid:1) + contant.
Similarly, we maximize the variational lower bound over s:
j
Consequently we have for each j ,
if |vT
j mw | <

, sj = Sign(vT
j mw )

; otherwise, sj = 0.

j mw | − λs /(ηj λv )
|vT
(mw )2
i + (Vw )i,i

λs
ηj λv

(9)

(10)

(11)

Uj = tTdiag(h)xj + λv

To estimate β , we develop an active-set method to iteratively maximize the model marginal like-
lihood over elements of β . In particular, we use a strategy similar to Tipping and Faul (2003)’s
approach: given the approximation factors N (t|XTw, diag(h)−1 ), the distribution over eigenvec-
tors N (vj |sj w, (λv ηj )−1 I), and the prior distribution N (w|0, diag(β)−1 ), we can compute and
decompose the log marginal likelihood L(β) = log p(y|X, s, λv ) into two parts: L(βj ) and L(β\j )
where j and \j index the elements of β in the active set and the rest elements, respectively. Note
that because the effective prior over w becomes N (w|mp , Vp ) as in (7) — instead of the zero mean
prior N (w|0, diag(β)−1 )— we cannot apply the algorithm proposed by Tipping and Faul (2003).
Instead, we decompose L(β) into L(βj ) and L(β\j ) as follows.
m(cid:88)
m(cid:88)
First let us deﬁne
k=1
k=1

k − bTmw , Rj = (xj )Tdiag(h)xj + λv
ηk sk v j
rj = βj Rj
uj = βj Uj
(cid:80)m
βj − Rj
βj − Rj
k , xj is the j -th column of the data matrix X, v j
where b = (xj )Tdiag(h)Xa + λv ea
k=1 ηk s2
j
k
is the j -th element of the vector vk , Xa are the columns of X associated with currently selected
j are the a-th elements of the j -th row of the identity matrix.
features (indexed by a), and ea
2 (ln βj − ln(βj + uj ) + r2
Then we have L(β) = L(β\j ) + 1
). where L(β\j ) does not depend
j
βj +uj
on βj . Therefore, we can directly optimize over βj without updating β\j .
j ≥ rj ,
Setting the gradient of L(β) over βj , we easily obtain the following optimality condition: if u2

k − bTVw b
ηk s2

(12)

r2
j
;
βj =
j − rj
u2
j < rj , βj = ∞. In the latter case we remove the j -th feature if it is currently in the model.
if u2

(13)

5

(a) Lasso

(b) Elastic net

(c) EigenNet

(d) True

Figure 3: Visualization of the lasso, the elastic net, the EigenNet and the true classiﬁer weights. We
used 80 training samples with 40 features. The test error rates of the lasso, the elastic net, and the
EigenNet on 2000 test samples are 0.297, 0.245, and 0.137, respectively.

The above active-set updates are very efﬁcient, because during each iteration we only deal with
a reduced model deﬁned on the currently selected features. This approach signiﬁcantly reduces
the computational cost of EP from O(np2 ) to O(nl2 ) where l is the biggest model size during the
active-set iterations. The empirical Bayesian estimation algorithm of EigenNet is summarized in
Algorithm 1.
4 Related work

The EigenNet is related to the classical eigenface approaches (Turk and Pentland, 1991; Sirovich
and Kirby, 1987). The eigenface approach learns a model in the subspace spanned by the major
eigenvectors of the data covariance matrix. The EigenNet also uses the eigensubspace to guide
the model estimation. However, unlike the eigenface approach, the EigenNet adaptively selects
eigenvectors and learns a sparse classiﬁer.
There are Bayesian versions of the lasso and the elastic net. Bayesian lasso (Park et al., 2008) puts
a hyper-prior on the regularization coefﬁcient and use a Gibbs sampler to jointly sample both re-
gression weights and the regularization coefﬁcient. Using a similar treatment to Bayesian lasso,
Bayesian elastic net (Li and Lin, 2010) samples the two regularization coefﬁcients simultaneously,
potentially avoiding the “double shrinkage” problem described in the original elastic net paper (Zou
and Hastie, 2005). As the EigenNet, these methods are grounded in a Bayesian framework, shar-
ing the beneﬁts of obtaining posterior distributions for handling estimation uncertainty. However,
Bayesian lasso and Bayesian elastic net are presented to handle regression problems (though cer-
tainly they can be generalized for classiﬁcation problems) and do not use the eigen-information
embedded in data. The EigenNet, by contrast, selects the eigen-subspace and uses it to guide classi-
ﬁcation.
Group lasso (Jacob et al., 2009) enforces sparsity on the groups of predictors—an entire group of
correlated predictors may be retained or pruned off. However, applying the idea of group lasso
to the EigenNet faces several difﬁculties: First, this approach won’t give (approximately) sparse
classiﬁers unless we truncate eigenvectors. If we use truncation, we need to decide what threshold
we should use to truncate each eigenvector—again it’s a difﬁcult task. Second, it will be hard to
tune all regularization coefﬁcients associated with all major eigenvectors–cross validation would
not sufﬁce. By contrast, our classiﬁer is sparse because of the ARD effect. More importantly, the
latent variables sj in our model are automatically estimated from data, deciding how important each
eigenvector is for the classiﬁcation task in a principled Bayesian framework.

5 Experimental results

We evaluated the new sparse Bayesian model, the EigenNet, on both synthetic and real data and
compared it with three representative variable selection methods, the lasso, the elastic net, and an
ARD approach (Qi et al., 2004). For the lasso and the elastic net, we used the Glmnet software
package that uses cyclical coordinate descent in a pathwise fashion1 . Like the EigenNet, the ARD
approach also uses EP to approximate the model marginal likelihood. For the lasso and the elastic
net, we used cross-validation to tune the hyperparameters; for the EigenNet, we estimated λv from
data and tuned λs by cross-validation.
1 http://www-stat.stanford.edu/ tibs/glmnet-matlab/

6

010203040010203040010203040010203040(c) independent features
(a) independent features
(d) correlated features
(b) correlated features
Figure 4: Predictive performance on synthetic datasets.
(a) and (b): classiﬁcation; (c) and (d):
regression The results were averaged over 10 runs. For the data with independent features, the
EigenNet outperforms the alternative methods when the number of training samples is small; for
data with correlated features, the EigenNet outperforms the alternative methods consistently.

5.1 Visualization of estimated classiﬁers

First, we tested these methods on synthetic data that contain correlated features. We sampled 40
dimensional data points, each of which contains two groups of correlated variables. The correlation
coefﬁcient between variables in each group is 0.81 and there are 4 variables in each group. We set
the values of the classiﬁer weights in one group as 5 and in the other group as -5. We also generated
the bias term randomly from a standard Gaussian distribution. We set the number of training points
to 80. Figure 3 shows the estimated classiﬁers and the true classiﬁer we used to produce the data
labels. Unlike the lasso and the elastic net, the EigenNet clearly identiﬁes two groups of correlated
variables, very close to the ground truth. As a result, on 2000 test points, the EigenNet achieves the
lowest prediction error rate, 0.137, while the test error rates of the lasso and the elastic net are 0.297
and 0.245, respectively.

5.2 Experiments on synthetic data

Now we systematically compared these methods for classiﬁcation and regression on synthetic
datasets containing correlated features and containing independent features (Although this presenta-
tion so far has been focused on classiﬁcation, we can easily implement the EigenNet for regression;
since we can compute the marginal likelihood exactly, the EP approximation is not needed for regres-
sion.) To generate data with correlated variables we used a similar procedure as in the visualization
example: we sampled 40 dimensional data points, each of which contains two groups of correlated
variables. The correlation coefﬁcient between variables in each group is 0.81 and there are 4 vari-
ables in each group. However, unlike for the previous example where the classiﬁer weights are the
same for the correlated variables, now we set the weights within the same group to have the same
sign, but with different random values. We varied the number of training points, ranging from 10 to
80, and tested all these methods. For the datasets with independent features, we followed the same
procedure except that the features are independently sampled. We ran the experiments 10 times.
Figure 4 shows the results averaged over 10 runs. We did not report the standard errors since they
are very small.
For the datasets with independent features, the EigenNet outperforms the alternative methods when
the number of training examples is small (probably because in this case the eigenspace has a smaller
dimension than than that of the classiﬁer, effectively controlling the model ﬂexibility); with more
training examples, it is not unsurprising to see all these methods perform quite similarly. For the
data with correlated features, although the results of the elastic net appear to overlaps with those
of the lasso in the ﬁgure, the elastic net often outperforms the lasso with a small margin; also, the
EigenNet consistently outperforms the lasso and the elastic net signﬁcantly. The improved predictive
performance of the EigenNet reﬂects the beneﬁt of using the valuable correlation information to help
the model estimation.

5.3 Application to imaging genetics

Imaging genetics is an emerging research area where imaging markers and genetic variations (e.g.,
SNPs) are used to study neurodegenerative diseases, in particular, Alzheimer’s disease (AD). We

7

2040608000.10.20.30.4# of training examplestest error rate  LassoElastic netEigenNet204060800.10.150.20.250.30.35# of training examplestest error rate  LassoElastic netEigenNet20406080051015# of training examplesRMSE  LassoElastic netEigenNet20406080010203040# of training examplesRMSE  LassoElastic netEigenNet(b) Classiﬁcation of healthy & AD subjects
(a) Regression of ADAS-Cog score
Figure 5: Imaging genetics applications: (a) prediction of the ADAS-Cog score based on 14 imaging
features and (b) AD classiﬁcation based on 2000 SNPs. The error bars represent the standard errors.

applied the EigenNet to two critical problems in imaging genetics and compared its performance
with that of alternative sparse learning methods.
First, we considered a regression problem where the predictors are imaging features, which were
generated by Holland et al. (2009) for ADNI and include volume measured in 14 brain regions of
interest (ROI)—including the whole brain, ventricles, hippocampus, etc. We used these imaging
features to predict the ADAS-Cog score, which is widely used to assess cognitive function of AD
patients. It is hypothesized that the brain ROI volumes are associated with the ADAS-Cog score.
But this association has not been rigorously studied by statistical learning methods. After removing
missing entries, we obtained the data of 726 subjects, including healthy people, people with mild
cognitive impairment (MCI), and AD patients. Then we applied the lasso, the elastic net, and the
EigenNet to this prediction task. We randomly selected 508 training samples and 218 test samples
for 50 times. The results are shown in Figure 5.(a).
Second, we used SNP data to classify a subject into the healthy group or AD patients. We chose the
top 2000 SNPs that are associated with AD based on a simple statistical test. There are 374 subjects
in total (roughly the same size for each class). We compared the EigenNet with the lasso and the
elastic net as well as the the ARD approach—since it corresponds to EigenNet’s conditional com-
ponent. We randomly split the dataset into 262 training and 112 test samples 10 times. The results
are summarized in Figure 5.(b). As shown in the Figure, for both the regression and classiﬁcation
problems, the EigenNet outperforms the alternative methods signiﬁcantly.

6 Conclusions

In this paper, we have presented a novel sparse Bayesian hybrid model to select correlated vari-
ables for regression and classiﬁcation. It integrates the sparse conditional ARD model with a latent
variable model for eigenvectors.
For this hybrid model, we could explore other latent variable models, such as sparse projection
methods (Guan and Dy, 2009; Archambeau and Bach, 2009); these models can better deal with
noise in the unlabeled data and improve the selection of interdependent features (i.e., predictors).
Furthermore, if we have certain prior knowledge about the interdependence between features, such
as linkage disequilibrium between SNPs, we could easily incorporate them into our model. Thus,
our model provides an elegant framework for integrating complex data generation processes and
domain knowledge in sparse learning.

7 Acknowledgments

The authors thank the anonymous reviewers and T. S. Jaakkola for constructive suggestions. This
work was supported by NSF IIS-0916443, NSF CAREER award IIS-1054903, and the Center for
Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement
CCF-0939370.

8

LassoElastic netEigenNet6.577.588.5Root−Mean−Square ErrorLassoElastic netARDEigenNet0.30.350.4Classification Error RateReferences

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267–288, 1994.
Hui Zou and Trevor Hastie. Regularization and variable selection via the Elastic Net. Journal of the
Royal Statistical Society B, 67:301–320, 2005.
Julia A. Lasserre, Christopher M. Bishop, and Thomas P. Minka. Principled hybrids of genera-
tive and discriminative models. In Proc. of IEEE Conference on Computer Vision and Pattern
Recognition, pages 87–94, 2006.
David J.C. MacKay. Bayesian interpolation. Neural Computation, 4:415–447, 1991.
Ildiko E. Frank and Jerome H. Friedman. A statistical view of some chemometrics regression tools.
Technometrics, 35(2):109–135, 1993.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001.
Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of
the 17th Conference in Uncertainty in Artiﬁcial Intelligence, pages 362–369, 2001.
Michael E. Tipping and Anita C. Faul. Fast marginal likelihood maximisation for sparse Bayesian
models. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statis-
tics, 2003.
Matthew Turk and Alex Pentland. Eigenfaces for recognition. J. Cognitive Neuroscience, 3:71–86,
1991.
L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces. J.
Opt. Soc. Am. A, 4(3):519–524, 1987.
Park, Trevor, Casella, and George. The Bayesian Lasso. Journal of the American Statistical Associ-
ation, 103(482):681–686, 2008.
Qing Li and Nan Lin. The Bayesian Elastic Net. Bayesian Analysis, 5(1):151–170, 2010.
Laurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert. Group lasso with overlap and graph
lasso. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.
Yuan Qi, Thomas P. Minka, Rosalind W. Picard, and Zoubin Ghahraman. Predictive automatic
relevance determination by expectation propagation. In Proceedings of Twenty-ﬁrst International
Conference on Machine Learning, pages 671–678, 2004.
Dominic Holland, James B Brewer, Donald J Hagler, Christine Fenema-Notestine, and Anders M
Dale. Subregional neuroanatomical change as a biomarker for alzheimer’s disease. Proceedings
of the National Academy of Sciences, 106(49):20954–20959, 2009.
Yue Guan and Jennifer Dy. Sparse probabilistic principal component analysis. JMLR W&CP:
AISTATS, 5, 2009.
C ´edric Archambeau and Francis Bach. Sparse probabilistic projections.
Information Processing Systems 21. 2009.

In Advances in Neural

9


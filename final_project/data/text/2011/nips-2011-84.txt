On the Universality of Online Mirror Descent

Nathan Srebro
TTIC
nati@ttic.edu

Karthik Sridharan
TTIC
karthik@ttic.edu

Ambuj Tewari
University of Texas at Austin
ambuj@cs.utexas.edu

We show that for a general class of convex online learning problems, Mirror Descent can always
achieve a (nearly) optimal regret guarantee.

Abstract

1

Introduction

Mirror Descent is a ﬁrst-order optimization procedure which generalizes the classic Gradient Descent procedure to
non-Euclidean geometries by relying on a “distance generating function” speciﬁc to the geometry (the squared ￿2 -
norm in the case of standard Gradient Descent) [14, 4]. Mirror Descent is also applicable, and has been analyzed,
in a stochastic optimization setting [9] and in an online setting, where it can ensure bounded online regret [20]. In
fact, many classical online learning algorithms can be viewed as instantiations or variants of Online Mirror Descent,
generally either with the Euclidean geometry (e.g. the Perceptron algorithm [5] and Online Gradient Descent [27]), or
in the simplex (￿1 geometry), using an entropic distance generating function (Winnow [13] and Multiplicative Weights
/ Online Exponentiated Gradient algorithm [11]). More recently, the Online Mirror Descent framework has been
applied, with appropriate distance generating functions derived for a variety of new learning problems like multi-task
learning and other matrix learning problems [10], online PCA [26] etc.
In this paper, we show that Online Mirror Descent is, in a sense, universal. That is, for any convex online learning
problem, of a general form (speciﬁed in Section 2), if the problem is online learnable, then it is online learnable, with
a nearly optimal regret rate, using Online Mirror Descent, with an appropriate distance generating function. Since
Mirror descent is a ﬁrst order method and often has simple and computationally efﬁcient update rules, this makes the
result especially attractive. Viewing online learning as a sequentially repeated game, this means that Online Mirror
Descent is a near optimal strategy, guaranteeing an outcome very close to the value of the game.
In order to show such universality, we ﬁrst generalize and reﬁne the standard Mirror Descent analysis to situations
where the constraint set is not the dual of the data domain, obtaining a general upper bound on the regret of Online
Mirror Descent in terms of the existence of an appropriate uniformly convex distance generating function (Section 3).
We then extend the notion of a martingale type of a Banach space to be sensitive to both the constraint set and the data
domain, and building on results of [24], we relate the value of the online learning repeated game to this generalized
notion of martingale type (Section 4). Finally, again building on and generalizing the work of [16], we show how
having appropriate martingale type guarantees the existence of a good uniformly convex function (Section 5), that in
turn establishes the desired nearly-optimal guarantee on Online Mirror Descent (Section 6). We mainly build on the
analysis of [24], who related the value of the online game to the notion of martingale type of a Banach space and
uniform convexity when the constraint set and data domain are dual to each other. The main technical advance here
is a non-trivial generalization of their analysis (as well as the Mirror Descent analysis) to the more general situation
where the constraint set and data domain are chosen independently of each other. In Section 7 several examples are
provided that demostrate the use of our analysis.
Mirror Descent was initially introduced as a ﬁrst order deterministic optimization procedure, with an ￿p constraint and
a matching ￿q Lipschitz assumption (1 ≤ p ≤ 2, 1/q + 1/p = 1), was shown to be optimal in terms of the number of
exact gradient evaluations [15]. Shalev-Shwartz and Singer later observed that the online version of Mirror Descent,
again with an ￿p bound and matching ￿q Lipschitz assumption (1 ≤ p ≤ 2, 1/q + 1/p = 1), is also optimal in terms

1

of the worst-case (adversarial) online regret. In fact, in such scenarios stochastic Mirror Descent is also optimal in
terms of the number of samples used. We emphasize that although in most, if not all, settings known to us these three
notions of optimality coincide, here we focus only on the worst-case online regret.
Sridharan and Tewri [24] generalized the optimality of online Mirror Descent (w.r.t. regret) to scenarios where learner
is constrained to a unit ball of an arbitrary Banach space (not necessarily and ￿p space) and the objective functions
have sub-gradients that lie in the dual ball of the space—for reasons that will become clear shortly, we refer to this as
the data domain. However, often we encounter problems where the constraint set and data domain are not dual balls,
but rather are arbitrary convex subsets. In this paper, we explore this more general, “non-dual”, variant, and show that
also in such scenarios online Mirror Descent is (nearly) optimal in terms of the (asymptotic) worst-case online regret.

2 Online Convex Learning Problem

An online convex learning problem can be viewed as a multi-round repeated game where on round t, the learner ﬁrst
picks a vector (predictor) wt from some ﬁxed set W , which is a closed convex subset of a vector space B . Next,
the adversary picks a convex cost function ft : W ￿→ R from a class of convex functions F . At the end of the
round, the learner pays instantaneous cost ft (wt ). We refer to the strategy used by the learner to pick the ft ’s as an
online learning algorithm. More formally, an online learning algorithm A for the problem is speciﬁed by the mapping
A : ￿n∈N F n−1 ￿→ W . The regret of the algorithm A for a given sequence of cost functions f1 , . . . , fn is given by
n￿t=1
n￿t=1
1
1
Rn (A, f1 , . . . , fn ) =
ft (A(f1:t−1 )) − inf
ft (w) .
n
n
w∈W
The goal of the learner (or the online learning algorithm), is to minimize the regret for any n.
In this paper, we consider cost function classes F speciﬁed by a convex subset X⊂B ￿ of the dual space B￿ . We
consider various types of classes, where for all of them, subgradients1 of the functions in F lie inside X (we use the
notation ￿x, w￿ to mean applying linear functional x ∈B ￿ on w ∈B ) :
FLip (X ) = {f : f is convex ∀w ∈W , ∇f (w) ∈X } ,
Flin (X ) = {w ￿→ ￿x, w￿ : x ∈X } ,
Fsup (X ) = {w ￿→ |￿x, w￿ − y | : x ∈X , y ∈ [−b, b]}
The value of the game is then the best possible worst-case regret guarantee an algorithm can enjoy. Formally :
Rn (A, f1:n )
Vn (F , X , W ) = inf
sup
A
f1:n∈F (X )
It is well known that the value of a game for all the above sets F is the same. More generally:
Proposition 1. If for a convex function class F , we have that ∀f ∈F , w ∈W , ∇f (w) ∈X then,
Vn (F , X , W ) ≤V n (Flin , X , W )
Furthermore,
Vn (FLip , X , W ) = Vn (Fsup , X , W ) = Vn (Flin , X , W )
That is, the value for any class F for which subgradients are in X , is upper bounded by the value of the class of
linear functionals in W , see e.g. [1]. In particular, this includes the class FLip which is the class of all functions with
subgradients in X , and since Flin (X ) ⊂F Lip (X ) we get the ﬁrst equality. The second equality is shown in [18].
The class Fsup (X ) corresponds to linear prediction with an absolute-difference loss, and thus its value is the best
possible guarantee for online supervised learning with this loss. We can deﬁne more generally a class F￿ =
{￿(￿x, w￿, y) : x ∈X , y ∈ [−b, b]} for any 1-Lipschitz loss ￿, and this class would also be of the desired type, with its
value upper bounded by Vn (Flin , X , W ). In fact, this setting includes supervised learning fairly generally, including
problems such as multitask learning and matrix completion, where in all cases X speciﬁes the data domain2 . The
equality in the above proposition can also be extended to other commonly occurring convex loss function classes like
the hinge loss class with some extra constant factors.
1Throughout we commit to a slight abuse of notation, with ∇f (w) indicating some sub-gradient of f at w and ∇f (w) ∈X
meaning that at least one of the sub-gradients is in X .
2Any convex supervised learning problem can be viewed as linear classiﬁcation with some convex constraint W on predictors.

(1)

2

Owing to Proposition 1, we can focus our attention on the class Flin (as other two behave similarly), and use shorthand
(2)
Vn (W , X ) := Vn (Flin , X , W )
Henceforth the term value without any qualiﬁcation refers to value of the linear game. Further, for any p ∈ [1, 2] let,
Vp := inf ￿V ￿￿￿ ∀n ∈ N, Vn (W , X ) ≤ V n−(1− 1
p )￿
(3)
Most prior work on online learning and optimization considers the case when W is the unit ball of some Banach space,
and X is the unit ball of the dual space, i.e. W and X are related to each other through duality. In this work, however,
we analyze the general problem where X∈B ￿ is not necessarily the dual ball of W . It will be convenient for us
to relate the notions of a convex set and a corresponding norm. The Minkowski functional of a subset K of a vector
space V is deﬁned as ￿v￿K := inf {α> 0 : v ∈ αK}. If K is convex and centrally symmetric (i.e. K = −K), then
is a semi-norm. Throughout this paper, we will require that W and X are convex and centrally symmetric.
￿·￿K
Further, if the set K is bounded then ￿·￿K
is a norm. Although not strictly required for our results, for simplicity we
will assume W and X are are such that ￿·￿W
and ￿·￿X
(the Minkowski functionals of the sets W and X ) are norms.
Even though we do this for simplicity, we remark that all the results go through for semi-norms. We use X ￿ and W ￿
to represent the dual of balls X and W respectively, i.e. the unit balls of the dual norms ￿·￿∗
and ￿·￿∗
.
X
W
3 Mirror Descent and Uniform Convexity

A key tool in the analysis mirror descent is the notion of strong convexity, or more generally uniform convexity:
Deﬁnition 1. Ψ: B→ R is q -uniformly convex w.r.t. ￿·￿ if for any w, w￿ ∈B :
￿w − w￿ ￿q
∀α∈[0,1] Ψ( αw + (1 − α)w￿ ) ≤ αΨ(w) + (1 − α)Ψ(w￿ ) − α(1−α)
q
We emphasize that in the deﬁnition above, the norm ￿.￿ and the subset W need not be related, and we only require
uniform convexity inside W . This allows us to relate a norm with a non-matching “ball”. To this end deﬁne,
￿￿￿￿￿
Dp := inf ￿￿ sup
p−1 -uniformly convex w.r.t. ￿·￿X ∗ , Ψ(0) = 0￿
Ψ(w)￿ p−1
p
p
Ψ: W ￿→ R+ is
w∈W
Given a function Ψ, the Mirror Descent algorithm, AMD is given by
∆Ψ (w|wt ) + η￿∇ft (wt ), w − wt ￿
wt+1 = argmin
w∈W
∆Ψ ￿w￿￿w￿t+1 ￿
or equivalently w￿t+1 = ∇Ψ∗ (∇Ψ(wt ) − η∇ft (wt )) , wt+1 = argmin
(5)
w∈W
where ∆Ψ (w|w￿ ) :=Ψ( w)−Ψ(w￿ )− ￿∇Ψ(w￿ ), w − w￿ ￿ is the Bregman divergence and Ψ∗ is the convex conjugate
2 ￿w￿2
of Ψ. As an example notice that when Ψ(w) = 1
2 then we get the gradient descent algorithm and when W is
the d dimensional simplex and Ψ(w) = ￿d
i=1 wi log(1/wi ) then we get the multiplicative weights update algorithm.
Lemma 2. Let Ψ: B ￿→ R be non-negative and q -uniformly convex w.r.t. norm ￿·￿X ∗ . For the Mirror Descent algo-
Ψ(w) and η = ￿ supw∈W Ψ(w)
￿1/p
rithm with this Ψ, using w1 = argmin
we can guarantee that for any f1 , . . . , fn
nB
w∈W
n ￿n
t=1 ￿∇ft ￿p
(where p = q
s.t. 1
q−1 ),
X ≤ 1
R(AMD , f1 , . . . , fn ) ≤ 2 ￿ supw∈W Ψ(w)
￿ 1
q
.
n
n ￿n
t=1 ￿∇ft ￿p
Note that in our case we have ∇f ∈X , i.e. ￿∇f ￿X ≤ 1, and so certainly 1
X ≤ 1. Similarly to the
value of the game, for any p ∈ [1, 2], we deﬁne:
MDp := inf ￿D : ∃Ψ,η s.t. ∀n ∈ N,
p )￿
Rn (AMD , f1:n ) ≤ Dn−(1− 1
sup
f1:n∈F (X )
where the Mirror Descent algorithm in the above deﬁnition is run with the corresponding Ψ and η . The constant MDp
is a characterization of the best guarantee the Mirror Descent algorithm can provide. Lemma 2 therefore implies:

(4)

(6)

3

Corollary 3. Vp ≤ MDp ≤ 2Dp .
Proof. The ﬁrst inequality is by the deﬁnition of Vp and MDp . Second inequality follows from previous lemma.

The Mirror Descent bound suggests that as long as we can ﬁnd an appropriate function Ψ that is uniformly convex
w.r.t. ￿·￿∗
we can get a diminishing regret guarantee. This suggests constructing the following function:
X
˜Ψq :=

argmin
ψ :ψ is q -uniformly convex
w.r.t. ￿·￿X ∗ on W and ψ≥0
If no q -uniformly convex function exists then ˜Ψq = ∞ is assumed by default. The above function is in a sense the
best choice for the Mirror Descent bound in (2). The question then is: when can we ﬁnd such appropriate functions
and what is the best rate we can guarantee using Mirror Descent?

sup
w∈W

Ψ(w) .

(7)

4 Martingale Type and Value

x0 +

sup
n

In [24], it was shown that the concept of the Martingale type (also sometimes called the Haar type) of a Banach
space and optimal rates for online convex optimization problem, where X and W are duals of each other, are closely
related. In this section we extend the classic notion of Martingale type of a Banach space (see for instance [16]) to
one that accounts for the pair (W ￿ , X ). Before we proceed with the deﬁnitions we would like to introduce a few
necessary notations. First, throughout we shall use ￿ ∈ {±1}N to represent inﬁnite sequence of signs drawn uniformly
at random (i.e. each ￿i has equal probability of being +1 or −1). Also throughout (xn )n∈N represents a sequence of
mappings where each xn : {±1}n−1 ￿→ B￿ . We shall commit to the abuse of notation and use xn (￿) to represent
xn (￿) = xn (￿1 , . . . ,￿ n−1 ) (i.e. although we used entire ￿ as argument, xn only depends on ﬁrst n − 1 signs). We are
now ready to give the extended deﬁnition of Martingale type (or M-type) of a pair (W ￿ , X ).
Deﬁnition 2. A pair (W ￿ , X ) of subsets of a vector space B￿ is said to be of M-type p if there exists a constant C ≥ 1
such that for all sequence of mappings (xn )n≥1 where each xn : {±1}n−1 ￿→ B￿ and any x0 ∈B ￿ :
X ]
W ￿ ￿ ≤ C p ￿x0 ￿p
￿ixi (￿)￿￿￿￿￿
E ￿￿￿￿￿￿
p
n￿i=1
X + ￿n≥1
E [￿xn (￿)￿p
The concept is called Martingale type because (￿nxn (￿))n∈N is a martingale difference sequence and it can be shown
that rate of convergence of martingales in Banach spaces is governed by the rate of convergence of martingales of
the form Zn = x0 + ￿n
i=1 ￿ixi (￿) (which are incidentally called Walsh-Paley martingales). We point the reader to
[16, 17] for more details. Further, for any p ∈ [1, 2] we also deﬁne,
X 
C ￿￿￿￿￿￿
Cp := inf 
W ￿ ￿ ≤ C p ￿x0 ￿p
￿ixi (￿)￿￿￿￿￿
E ￿￿￿￿￿￿
p
n￿i=1
E ￿xn (￿)￿p
∀x0 ∈B ￿ , ∀(xn )n∈N , sup
X
n
Cp is useful in determining if the pair (W ￿ , X ) has Martingale type p.
The results of [24, 18] showing that a Martingale type implies low regret, actually apply also for “non-matching” W
and X and, in our notation, imply that Vp ≤ 2Cp . Speciﬁcally we have the following theorem from [24, 18] :
Theorem 4. [24, 18] For any W∈B and any X∈B ￿ and any n ≥ 1,
￿ixi (￿)￿￿￿￿￿W ￿ ￿
E ￿￿￿￿￿￿
￿ixi (￿)￿￿￿￿￿W ￿ ￿ ≤V n (W , X ) ≤ 2 sup
E ￿￿￿￿￿￿
n￿i=1
n￿i=1
1
1
sup
n
n
x
x
where the supremum above is over sequence of mappings (xn )n≥1 where each xn : {±1}n−1 ￿→ X .
Our main interest here will is in establishing that low regret implies Martingale type. To do so, we start with the above
theorem to relate value of the online convex optimization game to rate of convergence of martingales in the Banach
space. We then extend the result of Pisier in [16] to the “non-matching” setting combining it with the above theorem
to ﬁnally get :

+ ￿n≥1

(8)

x0 +

4

Lemma 5. If for some r ∈ (1, 2] there exists a constant D > 0 such that for any n,
Vn (W , X ) ≤ Dn−(1− 1
r )
then for all p < r , we can conclude that any x0 ∈B ￿ and any B￿ sequence of mappings (xn )n≥1 where each
xn : {±1}n−1 ￿→ B￿ will satisfy :
X ]
(r − p)2 ￿p ￿x0 ￿p
￿ixi (￿)￿￿￿￿￿
E ￿￿￿￿￿￿
W ￿ ￿ ≤ ￿ 1104 D
p
n￿i=1
X + ￿i≥1
E [￿xi (￿)￿p
That is, the pair (W , X ) is of martingale type p.
The following corollary is an easy consequence of the above lemma.
Corollary 6. For any p ∈ [1, 2] and any p￿ < p : Cp￿ ≤ 1104 Vp
(p−p￿ )2
5 Uniform Convexity and Martingale Type

sup
n

x0 +

x0 +

sup
n

The classical notion of Martingale type plays a central role in the study of geometry of Banach spaces. In [16], it
was shown that a Banach space has Martingale type p (the classical notion) if and only if uniformly convex functions
with certain properties exist on that space (w.r.t. the norm of that Banach space). In this section, we extend this result
and show how the Martingale type of a pair (W ￿ , X ) are related to existence of certain uniformly convex functions.
Speciﬁcally, the following theorem shows that the notion of Martingale type of pair (W ￿ , X ) is equivalent to the
existence of a non-negative function that is uniformly convex w.r.t. the norm ￿·￿X ￿ .
Lemma 7. If, for some p ∈ (1, 2], there exists a constant C > 0, such that for all sequences of mappings (xn )n≥1
where each xn : {±1}n−1 ￿→ B￿ and any x0 ∈B ￿ :
X ]
W ￿ ￿ ≤ C p ￿x0 ￿p
E ￿￿￿￿￿￿
￿ixi (￿)￿￿￿￿￿
p
n￿i=1
X + ￿n≥1
E [￿xn (￿)￿p
(i.e. (W ￿ , X ) has Martingale type p), then there exists a convex function Ψ: B ￿→ R+ with Ψ(0) = 0, that is
X ￿ ≤ Ψ(w) ≤ C q
q ￿w￿q
q ￿w￿q
q -uniformly convex w.r.t. norm ￿·￿X ￿ s.t. ∀w ∈B , 1
.
W
The following corollary follows directly from the above lemma.
Corollary 8. For any p ∈ [1, 2], Dp ≤ Cp .
The proof of Lemma 7 goes further and gives a speciﬁc uniformly convex function Ψ satisfying the desired requirement
(i.e. establishing Dp ≤ Cp ) under the assumptions of the previous lemma:
Ψ∗q (x) := sup 
X ￿
￿ixi (￿)￿￿￿￿￿
E ￿￿￿￿￿￿
W ￿ ￿ − ￿i≥1
p
n￿i=1
E ￿￿xi (￿)￿p
1
C p sup
n
where the supremum above is over sequences (xn )n∈N and p = q
q−1 .
6 Optimality of Mirror Descent

, Ψq := (Ψ∗q )∗ .

x +

(9)

In the Section 3, we saw that if we can ﬁnd an appropriate uniformly convex function to use in the mirror descent
algorithm, we can guarantee diminishing regret. However the pending question there was when can we ﬁnd such a
function and what is the rate we can gaurantee. In Section 4 we introduced the extended notion of Martingale type of
a pair (W ￿ , X ) and how it related to the value of the game. Then, in Section 5, we saw how the concept of M-type
related to existence of certain uniformly convex functions. We can now combine these results to show that the mirror
descent algorithm is a universal online learning algorithm for convex learning problems. Speciﬁcally we show that
whenever a problem is online learnable, the mirror descent algorithm can guarantee near optimal rates:

5

Theorem 9. If for some constant V > 0 and some q ∈ [2, ∞), Vn (W , X ) ≤ V n− 1
q for all n, then for any n > eq−1 ,
there exists regularizer function Ψ and step-size η , such that the regret of the mirror descent algorithm using Ψ against
any f1 , . . . , fn chosen by the adversary is bounded as:
Rn (AMD , f1:n ) ≤ 6002 V log2 (n) n− 1
q
Proof. Combining Mirror descent guarantee in Lemma 2, Lemma 7 and the lower bound in Lemma 5 with p =
q
log(n) we get the above statement.
q−1 − 1
The above Theorem tells us that, with appropriate Ψ and learning rate η , mirror descent will obtain regret at most a
factor of 6002 log(n) from the best possible worst-case upper bound. We would like to point out that the constant V
in the value of the game appears linearly and there is no other problem or space related hidden constants in the bound.
The following ﬁgure summarizes the relationship between the various constants. The arrow mark from Cp￿ to Cp
indicates that for any n, all the quantities are within log2 n factor of each other.

(10)

p￿ < p, Cp￿ ≤ Vp
≤ MDp ≤ Dp
≤ Cp
Deﬁnition of Vp
Construction of Ψ, Lemma 10
Lemma 5
Lemma 2
(extending Pisier’s result [16])
(Generalized MD guarantee)
(extending Pisier’s result [16])
Figure 1: Relationship between the various constants

We now provide some general guidelines that will help us in picking out appropriate function Ψ for mirror descent.
First we note that though the function Ψq in the construction (9) need not be such that (qΨq (w))1/q is a norm, with
a simple modiﬁcation as noted in [17] we can make it a norm. This basically tells us that the pair (W , X ) is online
learnable, if and only if we can sandwich a q -uniformly convex norm in-between X ￿ and a scaled version of W (for
some q < ∞). Also note that by deﬁnition of uniform convexity, if any function Ψ is q -uniformly convex w.r.t. some
, then Ψ(·)
norm ￿·￿ and we have that ￿·￿ ≥ c ￿·￿X
is q -uniformly convex w.r.t. norm ￿·￿X
. These two observations
cq
together suggest that given pair (W , X ) what we need to do is ﬁnd a norm ￿·￿ in between ￿·￿￿
and C ￿·￿W
(C < ∞,
X
smaller the C better the bound ) such that ￿·￿q is q -uniformly convex w.r.t ￿·￿.
7 Examples

We demonstrate our results on several online learning problems, speciﬁed by W and X .
￿p non-dual pairs
It is usual in the literature to consider the case when W is the unit ball of the ￿p norm in some ﬁnite
dimension d while X is taken to be the unit ball of the dual norm ￿q where p, q are H ¨older conjugate exponents. Using
the machinery developed in this paper, it becomes effortless to consider the non-dual case when W is the unit ball Bp1
of some ￿p1 norm while X is the unit ball Bp2 for arbitrary p1 , p2 in [1, ∞]. We shall use q1 and q2 to represent Holder
conjugates of p1 and p2 . Before we proceed we ﬁrst note that for any r ∈ (1, 2], ψr (w) := 1
r is 2-uniformly
2(r−1) ￿w￿2
w.r.t. norm ￿·￿r (see for instance [25]). On the other hand by Clarkson’s inequality, we have that for r ∈ (2, ∞),
ψr (w) := 2r
r is r-uniformly convex w.r.t. ￿·￿r . Putting it together we see that for any r ∈ (1, ∞), the function
r ￿w￿r
ψr deﬁned above, is Q-uniformly convex w.r.t ￿·￿r for Q = max{r, 2}. The basic technique idea is to be to select ψr
based on the guidelines in the end of the previous section. Finally we show that using ˜ψr := dQ max{ 1
q2 − 1
r ,0}ψr in
Mirror descent Lemma 2 yields the bound that for any f1 , . . . , fn ∈F :
1√2(r−1) }dmax{ 1
r − 1
r ,0}+max{ 1
q2 − 1
2 max{2,
p1
Rn (AMD , f1:n ) ≤
n1/ max{r,2}
The following table summarizes the scenarios where a value of r = 2, i.e. a rate of D2 /√n, is possible, and lists the
corresponding values of D2 (up to numeric constant of at most 16):

,0}

6

p2−1 Range
p1 Range
q2 = p2
D2
1 ≤ p1 ≤ 2
1
q2 > 2
√p2 − 1
p1 ≤ q2 ≤ 2
1 ≤ p1 ≤ 2
d1/q2−1/p1 √p2 − 1
1 ≤ q2 < p1
1 ≤ p1 ≤ 2
d(1/2−1/p1 )
q2 > 2
p1 > 2
d(1/q2−1/p1 )
1 ≤ q2 ≤ 2
p1 > 2
￿log(d)
q2 = ∞
1 ≤ p1 ≤ 2
Note that the ﬁrst two rows are dimension free, and so apply also in inﬁnite-dimensional settings, whereas in the other
scenarios, D2 is ﬁnite only when the dimension is ﬁnite. An interesting phenomena occurs when d is ∞, p1 > 2 and
q2 ≥ p1 . In this case D2 = ∞ and so one cant expect a rate of O( 1√n ). However we have Dp2 < 16 and so can still
get a rate of n− 1
q2 .
Ball et al [3] tightly calculate the constants of strong convexity of squared ￿p norms, establishing the tightness of D2
when p1 = p2 . By extending their constructions it is also possible to show tightness (up to a factor of 16) for all other
values in the table. Also, Agarwal et al [2] recently showed lower bounds on the sample complexity of stochastic
optimization when p1 = ∞ and p2 is arbitrary—their lower bounds match the last two rows in the table.
Non-dual Schatten norm pairs in ﬁnite dimensions Exactly the same analysis as above can be carried out for
Schatten p-norms, i.e. when W = BS (p1 ) , X = BS (p2 ) are the unit balls of Schatten p-norm (the p-norm of the
singular values) for matrix of dimensions d1 × d2 . We get the same results as in the table above (as upper bounds
on D2 ), with d = min{d1 , d2 }. These results again follow using similar arguments as ￿p case and tight constants for
strong convexity parameters of the Schatten norm from [3].

Non-dual group norm pairs in ﬁnite dimensions
In applications such as multitask learning, groups norms such as
￿w￿q ,1 are often used on matrices w ∈ Rk×d where (q , 1) norm means taking the ￿1 -norm of the ￿q -norms of the
columns of w. Popular choices include q = 2, ∞. Here, it may be quite unnatural to use the dual norm (p, ∞) to deﬁne
the space X where the data lives. For instance, we might want to consider W = B(q ,1) and X = B(∞,∞) = B∞ . In
q ￿log(d)) using Ψ(w) = 1
such a case we can calculate that D2 (W , X ) =Θ( k1− 1
q+r−2 ￿w￿2
q ,r where r = log d
log d−1 .
Max Norm Max-norm has been proposed as a convex matrix regularizer for application such as matrix comple-
tion [21]. In the online version of the matrix completion problem at each time step one element of the matrix is
revealed, corresponding to X being the set of all matrices with a single element being 1 and the rest 0. Since
we need X to be convex we can take the absolute convex hull of this set and use X to be the unit element-wise
￿1 ball.
Its dual is ￿W ￿X ￿ = maxi,j |Wi,j |. On the other hand given a matrix W , its max-norm is given by
￿W ￿max = minU,V :W =U V ￿ (maxi ￿Ui ￿2 ) ￿maxj ￿Vj ￿2 ￿. The set W is the unit ball under the max norm. As
noted in [22] the max-norm ball is equivalent, up to a factor two, to the convex hull of all rank one sign ma-
trices. Let us now make a more general observation. Consider any set W = abscvx({w1 , . . . , wK }), the ab-
solute convex hull of K points w1 , . . . , wK ∈B .
In this case, the Minkowski norm for this W is given by
i=1 αiwi ￿K
i=1 |αi |. In this case, for any q ∈ (1, 2], if we deﬁne the norm ￿w￿W ,q =
￿w￿W := inf α1 ,...,αK :w=￿K
i=1 αiwi ￿￿K
i=1 |αi |q ￿1/q
2(q−1) ￿w￿2
, then the function Ψ(w) =
W ,q is 2-uniformly convex w.r.t.
1
inf α1 ,...,αK :w=￿K
log K−1 , then supw∈W ￿Ψ(w) = O(√log K ) and so
￿·￿W ,q (similar to ￿1 − ￿q case). Further if we use q = log K
D2 = √log K . For the max norm case the norm is equivalent to the norm got by the taking the absolute convex hull
of the set of all rank one sign matrices. Cardinality of this set is of course 2N +M . Hence using the above proposition
and noting that X ￿ is the unit ball of | · |∞ we see that Ψ is obviously 2-uniformly convex w.r.t. ￿·￿X ￿ and so we get
a regret bound O ￿￿ M +N
n ￿. This matches the stochastic (PAC) learning guarantee [22], and is the ﬁrst guarantee we
are aware of for the max norm matrix completion problem in the online setting.
8 Conclusion and Discussion

In this paper we showed that for a general class of convex online learning problems, there always exists a distance
generating function Ψ such that Mirror Descent using this function achieves a near-optimal regret guarantee. This

7

shows that a fairly simple ﬁrst-order method, in which each iteration requires a gradient computation and a prox-
map computation, is sufﬁcient for online learning in a very general sense. Of course, the main challenge is deriving
distance generating functions appropriate for speciﬁc problems—although we give two mathematical expressions for
such functions, in equations (7) and (9), neither is particularly tractable in general. In the end of Section 6 we do give
some general guidelines for choosing the right distance generating function. However obtaining a more explicit and
simple procedure at least for reasonable Banach spaces is a very interesting question.
Furthermore, for the Mirror Descent procedure to be efﬁcient, the prox-map of the distance generating function must be
efﬁciently computable, which means that even though a Mirror Descent procedure is always theoretically possible, we
might in practice choose to use a non-optimal distance generating function, or even a non-MD procedure. Furthermore,
we might also ﬁnd other properties of w desirable, such as sparsity, which would bias us toward alternative methods
[12, 7]. Nevertheless, in most instances that we are aware of, Mirror Descent, or slight variations of it, is truly an
optimal procedure and this is formalized and rigorously establish here.
In terms of the generality of the problems we handle, we required that the constraint set W be convex, but this seems
unavoidable if we wish to obtain efﬁcient algorithms (at least in general). Furthermore, we know that in terms of
worst-case behavior, both in the stochastic and in the online setting, for convex cost functions, the value is unchained
when the convex hull of a non-convex constraint set [18]. The requirement that the data domain X be convex is
perhaps more restrictive, since even with non-convex data domain, the objective is still convex. Such non-convex X
are certainly relevant in many applications, e.g. when the data is sparse, or when x ∈X is an indicator, as in matrix
completion problems and total variation regularization. In the total variation regularization problem, W is the set of
all functions on the interval [0, 1] with total variation bounded by 1 which is in fact a Banach space. However set X
we consider here is not the entire dual ball and in fact is neither convex nor symmetric. It only consists of evaluations
of the functions in W at points on interval [0, 1] and one can consider a supervised learning problem where the goal
is to use the set of all functions with bounded variations to predict targets which take on values in [−1, 1] . Although
the total-variation problem is not learnable, the matrix completion problem certainly is of much interest. In the matrix
completion case, taking the convex hull of X does not seem to change the value, but we are unaware of neither a
guarantee that the value of the game is unchanged when a non-convex X is replaced by its convex hull, nor of an
example where the value does change—it would certainly be useful to understand this issue. We view the requirement
that W and X be symmetric around the origin as less restrictive and mostly a matter of convenience.
We also focused on a speciﬁc form of the cost class F , which beyond the almost unavoidable assumption of convexity,
is taken to be constrained through the cost sub-gradients. This is general enough for considering supervised learning
with an arbitrary convex loss in a worst-case setting, as the sub-gradients in this case exactly correspond to the data
points, and so restricting F through its sub gradients corresponds to restricting the data domain. Following Proposition
1, any optimality result for FLip also applies to Fsup , and this statement can also be easily extended to any other
reasonable loss function, including the hinge-loss, smooth loss functions such as the logistic loss, and even strongly-
convex loss functions such as the squared loss (in this context, note that a strongly convex scalar function for supervised
learning does not translate to a strongly convex optimization problem in the worst case). Going beyond a worst-case
formulation of supervised learning, one might consider online repeated games with other constraints on F , such as
strong convexity, or even constraints on {ft } as a sequence, such as requiring low average error or conditions on the
covariance of the data—these are beyond the scope of the current paper.
Even for the statistical learning setting, online methods along with online to batch conversion are often preferred due
to their efﬁciency especially in high dimensional problems. In fact for ￿p spaces in the dual case, using lower bounds
on the sample complexity for statistical learning of these problems, one can show that for large dimensional problems,
mirror descent is an optimal procedure even for the statistical learning problem. We would like to consider the question
of whether Mirror Descent is optimal for stochastic convex optimization (convex statistical learning) setting [9, 19, 23]
in general. Establishing such universality would have signiﬁcant implications, as it would indicate that any learnable
(convex) problem, is learnable using a one-pass ﬁrst-order online method (i.e. Stochastic Approximation approach).

References
[1] J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online
convex games. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2008.
[2] Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Information-theoretic lower
bounds on the oracle complexity of convex optimization.

8

[3] Keith Ball, Eric A. Carlen, and Elliott H. Lieb. Sharp uniform convexity and smoothness inequalities for trace
norms. Invent. Math., 115:463–482, 1994.
[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization.
Operations Research Letters, 31(3):167–175, 2003.
[5] H. D. Block. The perceptron: A model for brain functioning. Reviews of Modern Physics, 34:123–135, 1962.
Reprinted in ”Neurocomputing” by Anderson and Rosenfeld.
[6] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Sparse and low-rank matrix decompositions. In IFAC
Symposium on System Identiﬁcation, 2009.
[7] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the ￿1 -ball for learning in high
dimensions. In Proceedings of the 25th International Conference on Machine Learning, 2008.
[8] Ali Jalali, Pradeep Ravikumar, Sujay Sanghavi, and Chao Ruan. A Dirty Model for Multi-task Learning. In
NIPS, December 2010.
[9] A. Juditsky, G. Lan, A. Nemirovski, and A. Shapiro. Stochastic approximation approach to stochastic program-
ming. SIAM J. Optim, 19(4):1574–1609, 2009.
[10] Sham M. Kakade, Shai Shalev-shwartz, and Ambuj Tewari. On the duality of strong convexity and strong
smoothness: Learning applications and matrix regularization, 2010.
[11] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information
and Computation, 132(1):1–64, January 1997.
[12] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. In Advances in Neural Informa-
tion Processing Systems 21, pages 905–912, 2009.
[13] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine
Learning, 2:285–318, 1988.
[14] A. Nemirovski and D. Yudin. On cesaro’s convergence of the gradient descent method for ﬁnding saddle points
of convex-concave functions. Doklady Akademii Nauk SSSR, 239(4), 1978.
[15] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. Nauka Publishers,
Moscow, 1978.
[16] G. Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics, 20(3–4):326–350,
1975.
[17] G. Pisier. Martingales in banach spaces (in connection with type and cotype). Winter School/IHP Graduate
Course, 2011.
[18] A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial parameters, and
learnability. NIPS, 2010.
[19] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT, 2009.
[20] S. Shalev-Shwartz and Y. Singer. Convex repeated games and fenchel duality. Advances in Neural Information
Processing Systems, 19:1265, 2007.
[21] Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakola. Maximum-margin matrix factorization. In Advances
in Neural Information Processing Systems 17, pages 1329–1336. MIT Press, 2005.
[22] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th Annual Con-
ference on Learning Theory, pages 545–560. Springer-Verlag, 2005.
[23] Nathan Srebro and Ambuj Tewari. Stochastic optimization for machine learning. In ICML 2010, tutorial, 2010.
[24] K. Sridharan and A. Tewari. Convex games in Banach spaces. In Proceedings of the 23nd Annual Conference
on Learning Theory, 2010.
[25] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University of
Jerusalem, 2007.
[26] Manfred K. Warmuth and Dima Kuzmin. Randomized online pca algorithms with regret bounds that are loga-
rithmic in the dimension, 2007.
[27] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.
[28] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society, Series B, 67:301–320, 2005.

9


Yelp++ : 10 Times More Information per View

Sean Choi, Ernest Ryu, Yuekai Sun

December 16, 2011

Abstract

In this pro ject we investigate two machine learning
methods, one supervised and one unsupervised, that
will allow the information content of Yelp data to
be eﬃciently conveyed to the users. The ﬁrst is ma-
trix completion via the novel ”max-norm” constraint
which out results show to be more powerful than the
traditional nuclear norm minimization. The second
is text summary via sparse PCA which can provide
a concise summary of the available immense text re-
views. We implement and run these algorithms on
actual Yelp data and provide results.

In this pro ject, we take a diﬀerent approach and use
what’s called the “max-norm.”

2.1.1 Data
Our data matrix X ∈ Rn×m is the restaurant star
ratings matrix, i.e. Xij is the i-th user’s raring of
the j -th restaurant.
It is incomplete so we only
know a subset of the entries with indices (i, j ) ∈
Ω ⊂ {1 · · · n} × {1 · · · m}. We normalize the rat-
ing by subtracting 3 to every entry so that Xij ∈
{−2, −1, 0, 1, 2}. This makes X uniformly bounded
in the sense that |Xij | ≤ 2 for all i, j .

1 Introduction

Modern consumers are inundated with information
and choices. Countless services now provide a daunt-
ing amount of information to a casual consumer. This
plethora of information brings the need for a recom-
mender system provide individually customized lists
of relevant products and a concise summary of what
the product is.
In this pro ject we investigate two methods to improve
Yelp’s service: a better recommendation system via
matrix completion with the “max-norm” and a text
summary system via sparse-PCA.

2 Theory

2.1 Collaborative (cid:12)ltering with uniformly
bounded data

Collaborative ﬁltering (CF) has been popularized in
the past few years by the Netﬂix challenge. The
mathematical statement of the problem is as follows:
can one reconstruct a matrix when only a subset of
the entires have been observed?
One popular method is nuclear norm minimization
supported by the theory of compressed sensing. [4]

2.1.2 Matrix completion using max-norm
regularization
)
(
The max-norm of X is deﬁned as
∥ui∥2 max
∥X ∥max = inf
max
i=1:::m
i=1:::m
X=U V T

∥vi∥2

(1)

and ui , vi denote the i-th row of U and V respectively.
We consider the max-norm constrained version of
∑
matrix completion.
(Xij − (U V T )ij )2
min
U ∈Rn(cid:2)k ;V ∈Rm(cid:2)k
(i;j )∈Ω
sub ject to: ∥U V T ∥max ≤ τ

where k is the rank of the prediction matrix which
can be interpreted as the number of latent variables.
Our heuristic to solve this non-convex constrained op-
timization problem is alternating minimization over
U and V . Fixing V , we obtain the following n embar-
rassingly parallel convex optimization subproblems.
i· − Vj ui∥2
∥X T
min
ui∈Rn
2
sub ject to: ∥ui∥2 ≤
τ
max1≤j≤m ∥vj ∥2
which is a k × k QP and in particular is the well-
studied “trust region subproblem” [7] which can be

1

solved very eﬃciently for small k by explicitly com-
puting the eigenvalue decomposition.1 A similar re-
sult holds for ﬁxing U and we obtain the following
matrix completion algorithm. We again emphasize

Algorithm 1 Max-norm Matrix Completion
while not converged do
for i = 1 · · · n do
∥Xi· − uiV T ∥2
minui
sub ject to: ∥ui∥2 ≤ τ / max1≤j≤m ∥vj ∥2
end for
for j = 1 · · · n do
∥X·j − U vj ∥2
minvj
sub ject to: ∥vj ∥2 ≤ τ / max1≤i≤n ∥ui∥2
end for
end while

that the for loop operation of algorithm 1 is embar-
rassingly parallel.

2.1.3 Bias correction

Nothing in algorithm 1 constrains the resulting pre-
diction matrix Y = U V Y to have the same mean as
the data matrix X .
In fact the discrepancy of the
means of Y and X is not negligible by correcting this
“bias” by letting
Y = Y − (µY − µX )
∑
ij

∑
(i;j )∈Ω

1
nm

1|Ω|

where

µY =

Yij

µX =

Xij

provides an improved RMSE empirically for us and
others. [11]

2.1.4 Interpretation of max-norm regulariza-
tion

There are two interpretation that justiﬁes the use of
max-norm regularization. The ﬁrst is that the max-
norm regularization implies that the entries of Y are
uniformly bounded by τ . By the Cauchy-Schwartz
inequality,
| ≤ ∥ ˜ui∥2∥˜vj ∥2 ≤ τ
∥Y ∥max ≤ τ ⇒ |Yij | = | ˜ui ˜vT
j
For the application in consideration, valid predictions
are integers between −2 and 2 so a sensible choice of
1The QP matrix is symmetric so for small k the eigenvalue
decomposition does not pose a signi(cid:12)cant numerical challenge.
However, the complexity grows at a rate of O(k3 ).

2

τ is 2. In particular the choice τ = 2 will ensure that
all our predictions will lie in the interval [−2, 2].

The second iterpretation is the maximum margin
classiﬁer. [10] Consider the max-norm variant of the
matrix completion problem
∥Y ∥max
min
Y
sub ject to: Xij Yij ≥ 1, (i, j ) ∈ Ω

Y can be decomposed into Y = U V T hence every
entry of Y can be expressed Yij = uT
i vj where ui
and vj are the rows of U and V respectively. We
can interpret ui as a feature vector for user i and
vj as a classiﬁer that classiﬁes users into users that
like and dislike movie j . The features are the aﬃnity
of user i to the k-th latent variable. The constraint
Xij Yij ≥ 1 ensures the classiﬁer correctly classiﬁes
users who have rated movie j .
On the other hand the well-known SVM optimization
problem is

∥w∥2
1
min
2
w;b
sub ject to: yi (wT xi + b) ≥ 1, i = 1, · · · , m
Since ∥X ∥max is deﬁned as (1), each subproblem is
essentially seeking for a maximum margin classiﬁer

∥vj ∥2
1
min
2
w
i vj + 0) ≥ 1, (i, j ) ∈ Ω
sub ject to: Xij (uT

This interpretation suggests that the max-norm reg-
ularization is a generalization of the approach to ﬁnd
the maximum margin classiﬁer for a dataset that only
contains a boolean aﬃnity of the users for each movie.

2.2 Using SPCA to summarize large text
corpora

Yelp has a large database of text data that would
make sense to a human reader. However many pop-
ular restaurants have more than 500 revies and it
is unreasonable to expect users to make a judgement
about the restaurant after digesting this large amount
of data. In a sense, Yelp is presenting these reviews to
users very ineﬃciently. Our goal is to implement an
algorithm that would provide a concise summary that
would represent the key features of the restaurant.

2.2.1 Data

Our data matrix X consists of tf-idf scores. Term
frequency inverse document frequency (tf-idf ) score,
commonly used in natural language processing, is de-
ﬁned as

tf-idf(t, d) =

#terms ∈ d
#words ∈ d

log

|D|
|{d : t ∈ d}|

where t is a speciﬁc term and d is a particular docum-
net. Each column of X corresponds to a document
and each row to a term.

2.2.2 Sparse Principal Component Analysis
(SPCA)

Principal conponent analysis (PCA) is a widely used
tool and is applicable to this application of text sum-
mary. However, the principal components gener-
ated by PCA are hard to interpret as they are not
sparse. El Ghaoui et al. [5] suggested that sparse
PCA (SPCA) can be eﬀectively used to extract con-
cise pricipal components which can be presented into
a concise list of words.
The mathematical statement of SPCA is [8]
∥X − U V T ∥2
F ro + λu∥U ∥1 + λv ∥V ∥1
1
min
2
U;V
where ∥ · ∥F ro is the standard Frobenius norm and
∥ · ∥1 is the element-wise ℓ1 norm, i.e.
n∑
k∑
j=1
i=1

∥U ∥1 =

|Uij |

λu , λv are parameters that control the sparsity. This
approach is consistent with the standard method of
imposing an ℓ1 penalization term for sparsity.
This optimization problem, however, is non-convex
and non-diﬀerentiable. The standard heuristic that
will ﬁnd a local optimum is block coordinate descent.
To be precise, the i-th principal component and its
corresponding loadings are found sequentially with
the i-th residual matrix Xi by iteratively minimizing
over u and v . The sub-problem becomes
∥X − uvT ∥2
F ro + λu∥U ∥1
(
)
and with the use of sub-gradients one can ﬁnd the
analytical solution

min
u

1
2

{
where St is the soft-thresholding function
sgn(x)(|x| − η)
if |x| ≥ η
otherwise
0

St (x, η) =

This yields the following SPCA algorithm where the
application of St is embarrassingly parallel.

Algorithm 2 SPCA for Text Summary
X1 = X
for i = 1 · · · k do
while not converged do
2 , λu/∥vi∥2
ui = St (Xivi/∥vi∥2
2 )
i ui/∥ui∥2
2 , λv /∥ui∥2
′
vi = St (X
2 )
end while
Xi+1 = Xi − uivT
i
end for

2.2.3 Varying the sparsity parameter λu , λv

Although this aspect was concealed in the previous
algorithm discussion for the sake of conciseness, it is
important to impose a diﬀerent spasity parameter for
each principal components.
When the sparsity parameters λu , λv are ﬁxed
throughout the iterations, eventually the residual ma-
trix Xi+1 = Xi − uivT
i becomes small2 and the re-
sulting PC and loadings become identically 0.
If
λu , λv are chosen to be small enough to avoid this
phenomenon then the initial PC’s will be too dense.
A solution to this problem is to decrease the value
of λu , λv throughout the iterations. In particular, we
let λu = ηuλu and λv = ηv λv when the resulting PC
became identically zero. Empirically η = 1/3 worked
well.

3 Methods

3.1 Data Processing

To test our algorithms we have collected 6.5 million
restaurant reviews for about 400000 restaurants and
about 1 million users, ranging across the entire con-
tinental USA. We speciﬁcally collected four features
of each reviews: restaurant name, user name, user
ratings and user text reviews. The raw data was ﬁrst
inserted into a database and auto increment primary
key of the MySQL database [1] was used to generate

u = St

Xivi ,

1∥v∥2
2

1∥v∥2
2

λu

2An example of a quantitative measure of this could be the
Frobenius norm.

3

restaurant ids and user ids . The ﬁnal data was com-
pressed into restaurant id, user id, user rating and
user ratings. and exported into a XML format.
To convert the XML format into a format that resem-
bles a sparse matrix representation we utilized the
Amazon EC2 cluster and Hadoop.
Extracting the restaurant ids, user ids and user rat-
ings was straightforward. As for processing the text,
we decided to use stemming and to remove stop words
as it is arguably practice in text data mining [9] and
as it empirically gave better results. For stemming
we utilized the Apache Lucene [2] library and for stop
word removal the Porter Stemmer [3] algorithm. Af-
ter this pre-processing, the text data was formed into
a tf-idf score vector as mentioned before.

3.2 Sparse Linear Algebra

Because of the data size, traditional dense linear al-
gebra becomes infeasible and therefore we utilized
Matlab’s sparse matrix functionality for our algo-
rithms. However, there were some diﬃculties, espe-
cially in the SPCA algorithm, where we would en-
counter X − uvT where X is sparse and u, v are vec-
tors. When this expression is evaluated the resulting
matrix loses its sparsity entirely. Our solution was
to utilize Stanford iCME’s shared memory machine
wich oﬀers 128GB of RAM.
For future work, one could consider a sparse linear
algebra implementation that can retain the above
expression (which is very sparse) without explicitly
evaluating the expression. We did not take this path
due to time constraints.

4 Results

4.1 Max-norm Matrix Completion

The rank parameter k was set to k = 32. As men-
tioned before there is a signiﬁcant cost in increasing
the value of k but emprically there was diminishing
returns in increasing k beyond 32 and in particular
k = 64 did not yield a signiﬁcant improvement.
The standard assessment of a matrix completion algo-
rithm is the root-mean-square error (RMSE) of cross-
validation. To this end, we held out 25% of the data
for cross validation.
The max-norm (MN) and max-norm with bias cor-
rection (MNBC) algorithms are benchmarked against
the average rating (AR), nuclear norm minimization
(Nuc.N), and a result from a candidate in the Netﬂix

competition, [11] which of course is done on an en-
tirely diﬀerent dataset.
We can see that the max-norm matrix completion

AR Nuc.N MN MNBC Netﬂix
RMSE 1.26
1.11
1.08
1.07
0.91

Table 1: Comparison of matrix completion results

outperforms the other two standard algorithms. We
have included the Netﬂix result as merely a reference
to suggest that the performance of MN is not out-
rageously sub-optimal. The RMSE value of 0.91 is
not comparable to our result as the Netﬂix dataset
has quite diﬀerent statistical properties compared to
out Yelp dataset and the diﬀerence in performance
should not be understood as a defeat.

4.2 SPCA text summary

Many of the sparse principal components from the
SPCA algorithm had a clear interpretation. Restau-
rants with loadings of such PC’s usually belonged to
the categories consistent with the PC’s interpreta-
tion.
Table 2 shows some example principal components

3rd PC 5th PC 11th PC 12th PC 13th PC
falafel
dimsum
naan
italian
taco
pepper
burrito
indian
dumplg
gyro
pita
chinese
buﬀet
crust maxica
italian
salsa
masala
noodle
sandwi

Table 2: Example principal components with their
most signiﬁcant entries.

from the SPCA algorithm with the 4 most signiﬁcant
(largest value) entries. The meaning of these PC’s are
unambiguous. Table 3 shows some example loadings

Rest. Name
El Gran Amigo
Tagueria
La Mediterra-
nee
Beijing
Restaurant
Tommaso
Ristorante
Italiano

pc3
.74

pc5
-.01

pc11
-.04

pc12 pc13
-.04
.17

-.05

-.17

.25

.06

.65

-.06

-.14

.45

.14

-.12

-.04

.16

-0.06

.95

.08

Table 3: Example loadings of the 5 presented princi-
pal components.

4

[5] L. El Ghaoui, G.-C. Li, V.-A. Duong, V. Pham,
A. Srivastava, and K. Bhaduri. Sparse machine
learning methods for understanding large text
corpora. In Proc. Conference on Intel ligent Data
Understanding, October 2011. Accepted for pub-
lication, July 2011.

[6] Yehuda Koren. Factorization meets the neigh-
borhood: a multifaceted collaborative ﬁltering
model. In Proceedings of the 14th ACM SIGKDD
international conference on Know ledge discovery
and data mining, KDD ’08, pages 426–434, New
York, NY, USA, 2008. ACM.

[7] Jorge Nocedal and Stephen J. Wright. Numerical
Optimization. Springer, August 2000.

[8] H Shen and J Huang. Sparse principal compo-
nent analysis via regularized low rank matrix ap-
proximation. Journal of Multivariate Analysis,
99(6):1015–1034, 2008.

[9] Jeﬀrey L. Solka. Text data mining: Theory and
methods. Statistics Surveys, 2:94–112, 2008.

[10] Nathan Srebro. Learning with matrix factoriza-
tions. PhD thesis, MIT, Cambridge, MA, USA,
2004. AAI0807530.

[11] Yunhong Zhou, Dennis Wilkinson, Robert
Schreiber, and Rong Pan. Large-scale parallel
collaborative ﬁltering for the netﬂix prize. Al-
gorithmic Aspects in Information and Manage-
ment, 5034:337?348, 2008.

from the SPCA algorithm. The loadings provide a
clear indication on what type of restaurant each one
is.3

5 Conclusion

We investigated 2 learning algorithms that are ap-
plicable to recommender systems and we tested the
feasibility on a speciﬁc real-world dataset. Although
this report did not detail on the complexity analy-
sis of the optimization algorithms and sparse numer-
ical linear algebra, the algorithms are quite eﬃcient
and scalable as they reduce to embarrassingly paral-
lel subproblems.
Matrix completion using max-norm regularization
showed promise as it was able to outperform the
standard nuclear norm minimization algorithm. Ul-
timately,
it is unlikely that the max-norm matrix
completion algorithm alone will fare well against its
competitors as the best matrix completion algorithms
utilize “blending,” or mixing of diﬀerent such algo-
rithms. [6] However, the max-norm matrix comple-
tion algorithm could contribute to this blending as a
powerful ingredient.
Text summary via SPCA also showed considerable
promise as it was successful in detecting the most im-
portant features of restaurants. However, it did lack
the ability to extract ﬁner features such as whether
the service of the restaurant is good or whether the
restaurant is crowded. In that regard, there is room
for future work with text summary via SPCA. In
a practical standpoint, automatic text summary of
Yelp reviews are not very powerful if it cannot extract
any information beyond the genre of the restaurant.

References

[1] http://dev.mysql.com/doc/refman/5.0/en/
example-auto-increment.html.

[2] http://lucene.apache.org/java/docs/index.html.

[3] http://snowball.tartarus.org/algorithms/english/
stemmer.html.

[4] Emmanuel J. Cand´es and Benjamin Recht. Ex-
act matrix completion via convex optimization.
Found. Comput. Math., 9:717–772, December
2009.

3The restaurant names (which are tremendously descriptive
for the above 4 examples) were of course not used in the SPCA
algorithm.

5


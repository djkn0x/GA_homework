Transfer from Multiple MDPs

Alessandro Lazaric
INRIA Lille - Nord Europe, Team SequeL, France
alessandro.lazaric@inria.fr

Marcello Restelli
Department of Electronics and Informatics, Politecnico di Milano, Italy
restelli@elet.polimi.it

Abstract

Transfer reinforcement learning (RL) methods leverage on the experience col-
lected on a set of source tasks to speed-up RL algorithms. A simple and effective
approach is to transfer samples from source tasks and include them in the train-
ing set used to solve a target task. In this paper, we investigate the theoretical
properties of this transfer method and we introduce novel algorithms adapting the
transfer process on the basis of the similarity between source and target tasks.
Finally, we report illustrative experimental results in a continuous chain problem.

1 Introduction
The objective of transfer in reinforcement learning (RL) [10] is to speed-up RL algorithms by reusing
knowledge (e.g., samples, value function, features, parameters) obtained from a set of source tasks.
The underlying assumption of transfer methods is that the source tasks (or a suitable combination
of these) are somehow similar to the target task, so that the transferred knowledge can be useful in
learning its solution. A wide range of scenarios and methods for transfer in RL have been studied
in the last decade (see [12, 6] for a thorough survey). In this paper, we focus on the simple transfer
approach where trajectory samples are transferred from source MDPs to increase the size of the
training set used to solve the target MDP. This approach is particularly suited in problems (e.g.,
robotics, applications involving human interaction) where it is not possible to interact with the envi-
ronment long enough to collect samples to solve the task at hand. If samples are available from other
sources (e.g., simulators in case of robotic applications), the solution of the target task can bene ﬁt
from a larger training set that includes also some source samples. This approach has been already
investigated in the case of transfer between tasks with different state-action spaces in [11], where the
source samples are used to build a model of the target task whenever the number of target samples is
not large enough. A more sophisticated sample-transfer method is proposed in [5]. The authors in-
troduce an algorithm which estimates the similarity between source and target tasks and selectively
transfers from the source tasks which are more likely to provide samples similar to those generated
by the target MDP. Although the empirical results are encouraging, the proposed method is based
on heuristic measures and no theoretical analysis of its performance is provided. On the other hand,
in supervised learning a number of theoretical works investigated the effectiveness of transfer in
reducing the sample complexity of the learning process. In domain adaptation, a solution learned on
a source task is transferred to a target task and its performance depends on how similar the two tasks
are. In [1] and [8] different distance measures are proposed and are shown to be connected to the
performance of the transferred solution. The case of transfer of samples from multiple source tasks
is studied in [2]. The most interesting ﬁnding is that the tra nsfer performance bene ﬁts from using a
larger training set at the cost of an additional error due to the average distance between source and
target tasks. This implies the existence of a transfer tradeoff between transferring as many samples
as possible and limiting the transfer to sources which are similar to the target task. As a result, the
transfer of samples is expected to outperform single-task learning whenever negative transfer (i.e.,
transfer from source tasks far from the target task) is limited w.r.t. to the advantage of increasing

1

the size of the training set. This also opens the question whether it is possible to design methods
able to automatically detect the similarity between tasks and adapt the transfer process accordingly.
In this paper, we investigate the transfer of samples in RL from a more theoretical perspective w.r.t.
previous works. The main contributions of this paper can be summarized as follows:

• Algorithmic contribution. We introduce three sample-transfer algorithms based on ﬁtt ed
Q-iteration [3]. The ﬁrst algorithm ( AST in Sec. 3) simply transfers all the source samples.
We also design two adaptive methods (BAT and BTT in Sec. 4 and 5) whose objective is
to solve the transfer tradeoff by identifying the best combination of source tasks.
• Theoretical contribution. We formalize the setting of transfer of samples and we derive a
ﬁnite-sample analysis of AST which highlights the importan ce of the average MDP ob-
tained by the combination of the source tasks. We also report the analysis for BAT which
shows both the advantage of identifying the best combination of source tasks and the addi-
tional cost in terms of auxiliary samples needed to compute the similarity between tasks.
• Empirical contribution. We report results (in Sec. 6) on a simple chain problem which
con ﬁrm the main theoretical ﬁndings and support the idea tha
t sample transfer can signiﬁ-
cantly speed-up the learning process and that adaptive methods are able to solve the transfer
tradeoff and avoid negative transfer effects.

The proofs and additional experiments are available in [7].

2 Preliminaries
In this section we introduce the notation and the transfer problem considered in the rest of the paper.
We de ﬁne a discounted Markov decision process (MDP) as a tupl e M = hX , A, R, P , γ i where
the state space X is a bounded closed subset of the Euclidean space, A is a ﬁnite ( |A| < ∞)
action space, the reward function R : X × A → R is uniformly bounded by Rmax , the transition
kernel P is such that for all x ∈ X and a ∈ A, P (·|x, a) is a distribution over X , and γ ∈
(0, 1) is a discount factor. We denote by S (X × A) the set of probability measures over X × A
and by B (X × A; Vmax = Rmax
1−γ ) the space of bounded measurable functions with domain X × A
and bounded in [−Vmax , Vmax ]. We de ﬁne the optimal action-value function Q∗ as the unique
ﬁxed-point of the optimal Bellman operator T : B (X × A; Vmax ) → B (X × A; Vmax ) de ﬁned as
(T Q)(x, a) = R(x, a) + γ RX maxa′∈A Q(y , a′ )P (dy |x, a).
For any measure µ ∈ S (X × A) obtained from the combination of a distribution ρ ∈ S (X ) and
a uniform distribution over the discrete set A, and a measurable function f : X × A → R, we
|A| Pa∈A RX f (x, a)2ρ(dx). The supremum norm of f is
µ = 1
de ﬁne the L2 (µ)-norm of f as ||f ||2
||f ||∞ = supx∈X |f (x)|. Finally, we de ﬁne the standard L2 -norm for a vector α ∈ Rd
de ﬁned as
i . We denote by φ(·, ·) = (cid:0)ϕ1 (·, ·), . . . , ϕd (·, ·)(cid:1)⊤ a feature vector with features
as ||α||2 = Pd
i=1 α2
: X × A → [−C, C ], and by F = {fα(·, ·) = φ(·, ·)⊤α} the linear space of action-value
ϕi
functions spanned by the basis functions in φ. Given a set of state-action pairs {(Xl , Al )}L
l=1 , let
Φ = [φ(X1 , A1 )⊤ ; . . . ; φ(XL , AL )⊤ ] be the corresponding feature matrix. We de ﬁne the orthogona l
projection operator Π : B (X × A; Vmax ) → F as ΠQ = arg minf ∈F ||Q − f ||µ . Finally, by T (Q)
we denote the truncation of a function Q in the range [−Vmax , Vmax ].
We consider the transfer problem in which M tasks {Mm}M
m=1 are available and the objective is
to learn the solution for the target task M1 transferring samples from the source tasks {Mm}M
m=2 .
We de ﬁne an assumption on how the training sets are generated .
De ﬁnition 1.
(Random Tasks Design) An input set {(Xl , Al )}L
l=1 is built with samples drawn from
an arbitrary sampling distribution µ ∈ S (X × A), i.e. (Xl , Al ) ∼ µ. For each task m, one
transition and reward sample is generated in each of the state-action pairs in the input set, i.e.
Y m
l ∼ P (·|Xl , Al ), and Rm
l = R(Xl , Al ). Finally, we de ﬁne the random sequence {Ml}L
l=1 where
the indexes Ml are drawn i.i.d. from a multinomial distribution with parameters (λ1 , . . . , λM ). The
training set available to the learner is {(Xl , Al , Yl , Rl )}L
l=1 where Yl = Yl,Ml and Rl = Rl,Ml .
This is an assumption on how the samples are generated but in practice, a single realization of
samples and task indexes Ml is available. We consider the case in which λ1 ≪ λm (m = 2, . . . , M ).
This condition implies that (on average) the number of target samples is much less than the source

2

Input: Linear space F = span{ϕi , 1 ≤ i ≤ d}, initial function eQ0 ∈ F
for k = 1, 2, . . . do
Build the training set {(Xl , Al , Yl , Rl )}L
l=1 [according to random tasks design]
Build the feature matrix Φ = [φ(X1 , A1 )⊤ ; . . . ; φ(XL , AL )⊤ ]
Compute the vector p ∈ RL with pl = Rl + γ maxa′∈A eQk−1 (Yl , a′ )
Compute the projection ˆαk = (Φ⊤Φ)−1Φ⊤ p and the function bQk = f ˆαk
Return the truncated function eQk = T ( bQk )
end for
Figure 1: A pseudo-code for All-Sample Transfer (AST) Fitted Q-iteration.

samples and it is usually not enough to learn an accurate solution for the target task. We will also
consider the pure transfer case in which λ1 = 0 (i.e., no target sample is available). Finally, we
notice that Def. 1 implies the existence of a generative model for all the MDPs, since the state-
action pairs are generated according to an arbitrary sampling distribution µ.

3 All-Sample Transfer Algorithm
We ﬁrst consider the case when the source samples are generat ed according to Def. 1 and the de-
signer has no access to the source tasks. We study the algorithm called All-Sample Transfer (AST)
(Fig. 1) which simply runs FQI with a linear space F on the whole training set {(Xl , Al , Yl , Rl )}L
l=1 .
At each iteration k , given the result of the previous iteration eQk−1 = T ( bQk−1), the algorithm returns
LXl=1 (cid:18)f (Xl , Al ) − (Rl + γ max
a′∈A eQk−1 (Yl , a′ ))(cid:19)2
1
bQk = arg min
(1)
.
L
f ∈F
In the case of linear spaces, the minimization problem is solved in closed form as in Fig. 1. In the
following we report a ﬁnite-sample analysis of the performa nce of AST. Similar to [9], we ﬁrst study
the prediction error in each iteration and we then propagate it through iterations.

3.1 Single Iteration Finite-Sample Analysis
We de ﬁne the average MDP Mλ as the average of the M MDPs at hand. We de ﬁne its reward
function Rλ and its transition kernel P λ as the weighted average of reward functions and transition
kernels of the basic MDPs with weights determined by the proportions λ of the multinomial distribu-
tion in the de ﬁnition of the random tasks design (i.e., Rλ = PM
m=1 λmRm , P λ = PM
m=1 λmPm ).
We also denote by T λ its optimal Bellman operator. In the random tasks design, the average MDP
plays a crucial role since the implicit target function of the minimization of the empirical loss in
Eq. 1 is indeed T λ eQk−1 . At each iteration k , we prove the following performance bound for AST.
Theorem 1. Let M be the number of tasks {Mm}M
m=1 , with M1 the target task. Let the training
set {(Xl , Al , Yl , Rl )}L
l=1 be generated as in Def. 1, with a proportion vector λ = (λ1 , . . . , λM ). Let
= ΠT1 eQk−1 = arg inf f ∈F ||f − T1 eQk−1 ||µ , then for any 0 < δ ≤ 1, bQk (Eq. 1) satisﬁes
fαk
∗
∗ − T1 eQk−1 ||µ + 5qEλ ( eQk−1 )
||T ( bQk ) − T1 eQk−1 ||µ ≤ 4||fαk
+ 32Vmaxs 2
∗ ||)r 2
(cid:19).
log (cid:18) 27(12Le2)2(d+1)
9
+ 24(Vmax + C ||αk
log
L
δ
L
δ
with probability 1 − δ (w.r.t. samples), where ||ϕi ||∞ ≤ C and Eλ ( eQk−1 ) = k(T1 − T λ ) eQk−1 k2
µ .
Remark 1 (Analysis of the bound). We ﬁrst notice that the previous bound reduces (up to constan ts)
to the standard bound for FQI when M = 1 [7]. The bound is composed by three main terms: (i)
approximation error, (ii) estimation error, and (iii) transfer error. The approximation error ||fαk
∗ −
T1 eQk−1 ||µ is the smallest error of functions in F in approximating the target function T1 eQk−1 and it
does not depend on the transfer algorithm. The estimation error (third and fourth terms in the bound)
is due to the ﬁnite random samples used to learn bQk and it depends on the dimensionality d of the
function space and it decreases with the total number of samples L with the fast rate of linear spaces
3

(O(d/L) instead of O(pd/L)). Finally, the transfer error Eλ accounts for the difference between
source and target tasks. In fact, samples from source tasks different from the target might bias bQk
towards a wrong solution, thus resulting in a poor approximation of the target function T1 eQk−1 . It
is interesting to notice that the transfer error depends on the difference between the target task and
the average MDP Mλ obtained by taking a linear combination of the source tasks weighted by the
parameters λ. This means that even when each of the source tasks is very different from the target,
if there exists a suitable combination which is similar to the target task, then the transfer process is
still likely to be effective. Furthermore, Eλ considers the difference in the result of the application
of the two Bellman operators to a given function eQk−1 . As a result, when the two operators T1 and
T λ have the same reward functions, even if the transition distributions are different (e.g., the total
variation ||P1 (·|x, a) − P λ (·|x, a)||TV is large), their corresponding averages of eQk−1 might still be
similar (i.e., R maxa′ eQ(y , a′ )P1 (dy |x, a) similar to R maxa′ eQ(y , a′ )P λ (dy |x, a)).
Remark 2 (Comparison to single-task learning). Let bQk
s be the solution obtained by solving one
iteration of FQI with only samples from the source task, the performance bounds of bQk and bQk
s can
be written as (up to constants and logarithmic factors)
+ Vmaxr d
∗ ||)r 1
+ pEλ ,
∗ − T1 eQk−1 ||µ + (Vmax + C ||αk
kT ( bQk ) − T1 eQk−1 kµ ≤ ||fαk
L
L
+ Vmaxr d
∗ ||)r 1
∗ − T1 eQk−1 ||µ + (Vmax + C ||αk
s ) − T1 eQk−1 kµ ≤ ||fαk
kT ( bQk
,
N1
N1
with N1 = λ1L (on average). Both bounds have the same approximation error. The main difference
s uses only N1 samples and, as a result, has a much bigger estimation error than bQk , which
is that bQk
takes advantage of all the L samples transferred from the source tasks. At the same time, bQk suffers
from an additional transfer error. Thus, we can conclude that AST is expected to perform better
than single-task learning whenever the advantage of using more samples is greater than the bias
due to samples coming from tasks different from the target task. This introduces a transfer tradeoff
between including many source samples, so as to reduce the estimation error, and ﬁnding source
tasks whose combination leads to a small transfer error. In Sec. 4 we de ﬁne an adaptive transfer
algorithm which selects proportions λ so as to keep the transfer error Eλ as small as possible. Finally,
in Sec. 5 we consider a different setting where the number of samples in each source is limited.

3.2 Propagation Finite-Sample Analysis
We now study how the previous error is propagated through iterations. Let ν be the evaluation norm
1
(i.e., in general different from the sampling distribution µ). We ﬁrst report two assumptions.
Assumption 1. [9] Given µ, ν , p ≥ 1, and an arbitrary sequence of policies {πp}p≥1 , we assume
π1 · · · P 1
that the future-state distribution µP 1
is absolutely continuous w.r.t. ν . We assume that
πp
πp )/ν ||∞ satisﬁes Cµ,ν = (1 − γ 2)2 Pp pγ p−1c(p) < ∞.
π1 · · · P 1
c(p) = supπ1 ···πp ||d(µP 1
Assumption 2. Let G ∈ Rd×d be the Gram matrix with [G]ij = R ϕi (x, a)ϕj (x, a)µ(dx, a). We
assume that its smallest eigenvalue ω is strictly positive (i.e., ω > 0).
Theorem 2. Let Assumptions 1 and 2 hold and the setting be as in Thm. 1. After K iterations, AST
returns an action-value function eQK , whose corresponding greedy policy πK satisﬁes
(1 − γ )3/2 pCµ,ν "4 sup
2γ
||Q∗ − QπK ||ν ≤
inf
f ∈F ||f − T1g ||µ + 5 sup
α k(T1 − T λ )T (fα)kµ
g∈F
+ 32Vmaxs 2
γK #.
)r 2
(cid:19) +
log (cid:18) 27K (12Le2)2(d+1)
2VmaxpCµ,ν
9K
Vmax√ω
log
+ 56(Vmax +
δ
L
L
δ
Remark (Analysis of the bound). The bound reported in the previous theorem displays few dif-
to the single-iteration bound (see [7] for further discussion). The transfer error
ferences w.r.t.
supα k(T1 − T λ )T (fα)kµ characterizes the difference between the target and average Bellman op-
erators through the space F . As a result, even MDPs with signiﬁcantly different rewards and tran-
sitions might have a small transfer error because of the functions in F . This introduces a tradeoff
1We refer to [9] for a thorough explanation of the concentrability terms.

4

Input: Space F = span{ϕi , 1 ≤ i ≤ d}, initial function eQ0 ∈ F , number of samples L
Build the auxiliary set {(Xs , As , Rs,1 , . . . , Rs,M }S
s=1 and {Y t
t=1 for each s
s,1 ,. . ., Y t
s,M }T
for k = 1, 2, . . . do
Compute bλk = arg minλ∈Λ bEλ ( eQk−1 )
Run one iteration of AST (Fig. 1) using L samples generated according to bλk
end for
Figure 2: A pseudo-code for the Best Average Transfer (BAT) algorithm.

in the design of F between a “large ” enough space containing functions able to
approximate T1Q
(i.e., small approximation error) and a small function space where the Q-functions induced by T1
and T λ can be closer (i.e., small transfer error). This term also displays interesting similarities with
the notion of discrepancy introduced in [8] in domain adaptation.

4 Best Average Transfer Algorithm
As discussed in the previous section, the transfer error Eλ plays a crucial role in the comparison with
single-task learning. In particular, Eλ is related to the proportions λ inducing the average Bellman
operator T λ which de ﬁnes the target function approximated at each itera tion. We now consider
the case where the designer has direct access to the source tasks (i.e., it is possible to choose how
many samples to draw from each source) and can de ﬁne an arbitr ary proportion λ. In particular, we
propose a method that adapts λ at each iteration so as to minimize the transfer error Eλ .
We consider the case in which L is ﬁxed as a parameter of the algorithm and λ1 = 0 (i.e.,
no target samples are used in the learning training set). At each iteration k , we need to esti-
mate the quantity Eλ ( eQk−1 ). We assume that for each task additional samples available. Let
s=1 be an auxiliary training set where (Xs , As ) ∼ µ and Rs,m =
{(Xs , As , Rs,1 , . . . , Rs,M )}S
Rm (Xs , As ). In each state-action pair, we generate T next states for each task, that is Y t
s,m ∼
Pm (·|Xs , As ) with t = 1, . . . , T . Thus, for any function Q we de ﬁne the estimated transfer error as
s,m , a′ )(cid:17)#2
SXs=1 "Rs,1 −
TXt=1 (cid:16) max
MXm=2
MXm=2
1
γ
bEλ (Q) =
s,1 , a′ )−
Q(Y t
Q(Y t
(2)
λm max
λmRs,m +
.
S
T
a′
a′
At each iteration, the algorithm Best Average Transfer (BAT) (Fig. 2) ﬁrst computes bλk =
arg minλ∈Λ bEλ ( eQk−1 ), where Λ is the (M -2)-dimensional simplex, and then runs an itera-
tion of AST with samples generated according to the proportions bλk . We denote by λk
∗ =
arg minλ∈Λ Eλ ( eQk−1 ) the best combination at iteration k .
Theorem 3. Let eQk−1 be the function returned at the previous iteration and bQk
BAT the function
returned by the BAT algorithm (Fig. 2). Then for any 0 < δ ≤ 1, bQk
BAT satisﬁes
∗ − T1 eQk−1 ||µ + 5qEλk
BAT) − T1 eQk−1 ||µ ≤ 4||fαk
||T ( bQk
( eQk−1 )
∗
!1/4
+ 5p2Vmax  (M − 2) log 8S/δ
+ 20Vmaxr log 8SM /δ
S
T
+ 32Vmaxs 2
∗ ||)r 2
log (cid:18) 54(12Le2)2(d+1)
18
+ 24(Vmax + C ||αk
log
L
δ
L
δ
with probability 1 − δ .
Remark 1 (Comparison with AST and single-task learning). The bound shows that BAT outper-
forms AST whenever the advantage in achieving the smallest possible transfer error Eλk
is larger
∗
than the additional estimation error due to the auxiliary training set. When compared to single-task
learning, BAT has a better performance whenever the best combination of source tasks has a small
transfer error and the additional auxiliary estimation error is smaller than the estimation error in
single-task learning. In particular, this means that O((M /S )1/4 ) + O((1/T )1/2 ) should be smaller
than O((d/N )1/2 ) (with N the number of target samples). The number of calls to the generative

(cid:19).

5

Table 1: Parameters for the ﬁrst set of tasks

Table 2: Parameters for the second set of tasks

tasks

M1

M2
M3
M4
M5

p

0.9

0.9
0.9
0.9
0.9

l

1

2
1
1
1

η

Reward

0.1 +1 in [−11, −9] ∪ [9, 11]

0.1 −5 in [−11, −9] ∪ [9, 11]
0.1 +5 in [−11, −9] ∪ [9, 11]
+1 in [−6, −4] ∪ [4, 6]
0.1
−1 in [−6, −4] ∪ [4, 6]
0.1

tasks

M1

M6
M7
M8
M9

p

0.9

0.7
0.1
0.9
0.7

l

1

1
1
1
1

η

Reward

0.1 +1 in [−11, −9] ∪ [9, 11]

0.1 +1 in [−11, −9] ∪ [9, 11]
0.1 +1 in [−11, −9] ∪ [9, 11]
0.1 −5 in [−11, −9] ∪ [9, 11]
0.5 +5 in [−11, −9] ∪ [9, 11]

model for BAT is S T . In order to have a fair comparison with single-task learning we set S = N 2/3
and T = N 1/3 , then we obtain the condition M ≤ d2N −4/3 that constrains the number of tasks to
be smaller than the dimensionality of F . We remark that the dependency of the auxiliary estimation
error on M is due to the fact that the λ vectors (over which the transfer error is optimized) belong
to the simplex Λ of dimensionality M -2. Hence, the previous condition suggests that, in general,
adaptive transfer methods may signiﬁcantly improve the tra nsfer performance (i.e., in this case a
smaller transfer error) at the cost of additional sources of errors which depend on the dimensionality
of the search space used to adapt the transfer process (in this case Λ).

5 Best Transfer Trade-off Algorithm
The previous algorithm is proved to successfully estimate the combination of source tasks which
better approximates the Bellman operator of the target task. Nonetheless, BAT relies on the implicit
assumption that L samples can always be generated from any source task 2 and it cannot be applied
to the case where the number of source samples is limited. Here we consider the more challenging
case where the designer has still access to the source tasks but only a limited number of samples
is available in each of them. In this case, an adaptive transfer algorithm should solve a tradeoff
between selecting as many samples as possible, so as to reduce the estimation error, and choosing
the proportion of source samples properly, so as to control the transfer error. The solution of this
tradeoff may return non-trivial results, where source tasks similar to the target task but with few
samples are removed in favor of a pool of tasks whose average roughly approximate the target task
but can provide a larger number of samples.

Here we introduce the Best Tradeoff Transfer (BTT) algorithm. Similar to BAT, it relies on an
auxiliary training set to solve the tradeoff. We denote by Nm the maximum number of samples
available for source task m. Let β ∈ [0, 1]M be a weight vector, where βm is the fraction of samples
from task m used in the transfer process. We denote by Eβ ( bEβ ) the transfer error (the estimated
transfer error) with proportions λ where λm = (βmNm)/ Pm′ (βm′ Nm′ ). At each iteration k , BTT
returns the vector β which optimizes the tradeoff between estimation and transfer errors, that is
β∈[0,1]M (cid:16) bEβ ( eQk−1 ) + τ s
m=1 βmNm (cid:17),
d
ˆβ k = arg min
(3)
PM
where τ is a parameter. While the ﬁrst term accounts for the transfer error induced by β , the second
term is the estimation error due to the total amount of samples used by the algorithm.
Unlike AST and BAT, BTT is a heuristic algorithm motivated by the bound in Thm. 1 and we do not
provide any theoretical guarantee for it. The main technical difﬁculty is that the setting considered
here does not match the random task design assumption (see Def. 1) since the number of source
samples is constrained by Nm . As a result, given a proportion λ, we cannot assume samples to be
drawn at random according to a multinomial of parameters λ. Without this assumption, it is an open
question whether a similar bound as AST and BAT could be derived.
6 Experiments
In this section, we report preliminary experimental results of the transfer algorithms. The main ob-
jective is to illustrate the functioning of the algorithms and compare their results with the theoretical
ﬁndings. We consider a continuous extension of the chain wal k problem proposed in [4]. The state
is described by a continuous variable x and two actions are available: one that moves toward left and
the other toward right. With probability p each action makes a step of length l , affected by a noise η ,

2 If λm = 1 for task m, then the algorithm would generate all the L training samples from task m.

6

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

-0.1

p
e
t
s
 
r
e
p
 
d
r
a
w
e
r
 
e
g
a
r
e
v
A

-0.2

 0

 0.5

 0.4

 0.3

y
t
i
l
i
b
a
b
o
r
P

 0.2

 0.1

 0

-0.1

without transfer
BAT with 1000 samples
BAT with 5000 samples
BAT with 10000 samples
AST with 10000 samples

λ
2λ
3λ
4λ
5
 10  11  12  13

 9

 2000

 4000
 6000
 6
 8
 7
Number of target samples
Number of iterations
Figure 3: Transfer from M2 , M3 , M4 , M5 . Left: Comparison between single-task learning, AST
with L = 10000, BAT with L = 1000, 5000, 10000. Right: Source task probabilities estimated by
BAT algorithm as a function of FQI iterations.

 10000

 8000

 5

 1

 2

 3

 4

in the intended direction, while with probability 1 − p it moves in the opposite direction. In the target
task M1 , the state –transition model is de ﬁned by the following para meters: p = 0.9, l = 1, and η is
uniform in the interval [−0.1, 0.1]. The reward function provides +1 when the system state reaches
the regions [−11, −9] and [9, 11] and 0 elsewhere. Furthermore, to evaluate the performance of the
transfer algorithms previously described, we considered eight source tasks {M2 , . . . , M9} whose
state –transition model parameters and reward functions ar e reported in Tab. 1 and 2. To approximate
the Q-functions, we use a linear combination of 20 radial basis functions. In particular, for each ac-
tion, we consider 9 Gaussians with means uniformly spread in the interval [−20, 20] and variance
equal to 16, plus a constant feature. The number of iterations for the FQI algorithm has been empiri-
cally ﬁxed to 13. Samples are collected starting from the state x0 = 0 with actions chosen uniformly
at random. All the results are averaged over 100 runs and we report standard deviation error bars.
We ﬁrst consider the pure transfer problem where no target samples are actually used in the learning
training set (i.e., λ1 = 0). The objective is to study the impact of the transfer error due to the use
of source samples and the effectiveness of BAT in ﬁnding a sui table combination of source tasks.
The left plot in Fig. 3 compares the performances of FQI with and without the transfer of samples
from the ﬁrst four tasks listed in Tab. 1. In case of single-ta sk learning, the number of target samples
refers to the samples used at learning time, while for BAT it represents the size S of the auxiliary
training set used to estimate the transfer error. Thus, while in single-task learning the performance
increases with the target samples, in BAT they just make estimation of Eλ more accurate. The
number of source samples added to the auxiliary set for each target sample was empirically ﬁxed
to one (T = 1). We ﬁrst run AST with L = 10000 and λ2 = λ3 = λ4 = λ5 = 0.25 (which
on average corresponds to 2500 samples from each source). As it can be noticed by looking at the
models in Tab. 1, this combination is very different from the target model and AST does not learn
any good policy. On the other hand, even with a small set of auxiliary target samples, BAT is able to
learn good policies. Such result is due to the existence of linear combinations of source tasks which
closely approximate the target task M1 at each iteration of FQI. An example of the proportion
coefﬁcients computed at each iteration of BAT is shown in the
right plot in Fig. 3. At the ﬁrst
iteration, FQI produces an approximation of the reward function. Given the ﬁrst four source tasks,
BAT ﬁnds a combination ( λ ≃ (0.2, 0.4, 0.2, 0.2)) that produces the same reward function as R1 .
However, after a few FQI iterations, such combination is no more able to accurately approximate
functions T1 eQ. In fact, the state –transition model of task M2 is different from all the other ones
(the step length is doubled). As a result, the coefﬁcient λ2 drops to zero, while a new combination
among the other source tasks is found. Note that BAT signiﬁca ntly improves single-task learning, in
particular when very few target samples are available.
In the general case, the target task cannot be obtained as any combination of the source tasks, as it
happens by considering the second set of source tasks (M6 , M7 , M8 , M9 ). The impact of such
situation on the learning performance of BAT is shown in the left plot in Fig. 4. Note that, when
a few target samples are available, the transfer of samples from a combination of the source tasks
using the BAT algorithm is still bene ﬁcial. On the other hand , the performance attainable by BAT is
bounded by the transfer error corresponding to the best source task combination (which in this case
is large). As a result, single-task FQI quickly achieves a better performance.

7

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

p
e
t
s
 
r
e
p
 
d
r
a
w
e
r
 
e
g
a
r
e
v
A

 0

 0

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

p
e
t
s
 
r
e
p
 
d
r
a
w
e
r
 
e
g
a
r
e
v
A

 0

 0

without transfer
BAT with 1000 source samples
BAT with 5000 source samples
BAT with 10000 source samples

without transfer
BAT with 1000 source samples + target samples
BAT with 10000 source samples + target samples
BTT with max 5000 samples for each source
BTT with max 10000 samples for each source

 8000

 2000

 4000
 6000
 2000
 3000
Number of target samples
Number of target samples
Figure 4: Transfer from M6 , M7 , M8 , M9 . Left: Comparison between single-task learning and
BAT with L = 1000, 5000, 10000. Right: Comparison between single-task learning, BAT with
L = 1000, 10000 in addition to the target samples, and BTT (τ = 0.75) with 5000 and 10000
samples for each source task. To improve readability, the plot is truncated at 5000 target samples.

 10000

 1000

 4000

 5000

Results presented so far for the BAT transfer algorithm assume that FQI is trained only with the
samples obtained through combinations of source tasks. Since a number of target samples is already
available in the auxiliary training set, a trivial improvement is to include them in the training set
together with the source samples (selected according to the proportions computed by BAT). As
shown in the plot in the right side of Fig. 4 this leads to a signiﬁcant improvement. From the behavior
of BAT it is clear that with a small set of target samples, it is better to transfer as many samples as
possible from source tasks, while as the number of target samples increases, it is preferable to reduce
the number of samples obtained from a combination of source tasks that actually does not match the
target task. In fact, for L = 10000, BAT has a much better performance at the beginning but it is
then outperformed by single-task learning. On the other hand, for L = 1000 the initial advantage
is small but the performance remains close to single-task FQI for large number of target samples.
This experiment highlights the tradeoff between the need of samples to reduce the estimation error
and the resulting transfer error when the target task cannot be expressed as a combination of source
tasks (see Sec. 5). BTT algorithm provides a principled way to address such tradeoff, and, as shown
by the right plot in Fig. 4, it exploits the advantage of transferring source samples when a few target
samples are available, and it reduces the weight of the source tasks (so as to avoid large transfer
errors) when samples from the target task are enough. It is interesting to notice that increasing the
number of samples available for each source task from 5000 to 10000 improves the performance
in the ﬁrst part of the graph, while keeping unchanged the ﬁna
l performance. This is due to the
capability of the BTT algorithm to avoid the transfer of source samples when there is no need for
them, thus avoiding negative transfer effects.
7 Conclusions
In this paper, we formalized and studied the sample-transfer problem. We ﬁrst derived a ﬁnite-
sample analysis of the performance of a simple transfer algorithm which includes all the source
samples into the training set used to solve a given target task. At the best of our knowledge, this
is the ﬁrst theoretical result for a transfer algorithm in RL showing the potential bene ﬁt of transfer
over single-task learning. When the designer has direct access to the source tasks, we introduced
an adaptive algorithm which selects the proportion of source tasks so as to minimize the bias due
to the use of source samples. Finally, we considered a more challenging setting where the number
of samples available in each source task is limited and a tradeoff between the amount of transferred
samples and the similarity between source and target tasks must be solved. For this setting, we
proposed a principled adaptive algorithm. Finally, we report a detailed experimental analysis on a
simple problem which con ﬁrms and supports the theoretical ﬁ
ndings.
Acknowledgments This work was supported by French National Research Agency through the
projects EXPLO-RA n◦ ANR-08-COSI-004, by Ministry of Higher Education and Research, Nord-
Pas de Calais Regional Council and FEDER through the “contra t de projets ´etat region 2007 –2013 ”,
and by PASCAL2 European Network of Excellence. The research leading to these results has
also received funding from the European Community’s Seventh Framework Programme (FP7/2007-
2013) under grant agreement n 231495.

8

References

[1] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer
Vaughan. A theory of learning from different domains. Machine Learning, 79:151 –175, 2010.
[2] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources.
Journal of Machine Learning Research, 9:1757 –1774, 2008.
[3] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement
learning. Journal of Machine Learning Research, 6:503 –556, 2005.
[4] M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning
Research, 4:1107 –1149, 2003.
[5] A. Lazaric, M. Restelli, and A. Bonarini. Transfer of samples in batch reinforcement learning.
In Proceedings of the Twenty-Fifth Annual International Conference on Machine Learning
(ICML’08), pages 544 –551, 2008.
[6] Alessandro Lazaric. Knowledge Transfer in Reinforcement Learning. PhD thesis, Poltecnico
di Milano, 2008.
[7] Alessandro Lazaric and Marcello Restelli. Transfer from Multiple MDPs. Technical Report
00618037, INRIA, 2011.
[8] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learn-
ing bounds and algorithms.
In Proceedings of the 22nd Conference on Learning Theory
(COLT’09), 2009.
[9] R. Munos and Cs. Szepesv ´ari. Finite time bounds for ﬁtte d value iteration. Journal of Machine
Learning Research, 9:815 –857, 2008.
[10] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT
Press, Cambridge, MA, 1998.
[11] Matthew E. Taylor, Nicholas K. Jong, and Peter Stone. Transferring instances for model-based
reinforcement learning.
In Proceedings of the European Conference on Machine Learning
(ECML’08), pages 488 –505, 2008.
[12] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A
survey. Journal of Machine Learning Research, 10(1):1633 –1685, 2009.

9


Supervised Link Prediction by Geographic and Social Attributes

Wonhong Lee
Department of Computer Science
Stanford University

S. Jun Yu
Institute for Computational & Mathematical Engineering
Stanford University

wonhong@stanford.edu

sjyu@stanford.edu

Abstract

In this paper, we employ learning algorithms
to develop an eﬃcient link prediction model
based on geographic and social attributes.

1. Introduction

Link prediction in complex network is an active area of
research in network analysis. This task is complicated
by the fact that shape dynamics of the network is con-
stantly changing, and it is diﬃcult to deﬁne which in-
herent factors drive this change. In this paper, we will
complement an existing algorithm by considering so-
cial, geographic, and demographic features to enhance
the performance of link predictions.

1.1. Related Work

Backstrom and Leskovec [1] devised a link prediction
model based on features involving personal attributes
and network topology. This algorithm, however, may
be infeasible in many cases due to limitations in gath-
ering personal information. With a heightened aware-
ness towards privacy issues, it has become more diﬃ-
cult to collect these data.

The model introduced by Liben-Nowell and Keinberg
[3] predicts possible connections between nodes in a
social network by using graph theoretic features. Al-
though this algorithm is eﬀective in making predictions
for existing nodes, we do not have the same assurance
for newly formed nodes since they do not hold any
network topological information.

The main idea in the prediction model proposed by

Keywords: link prediction, supervised learning.
This pro ject was carried out in collaboration with Jeongjin
Ku from the Department of Computer Science.

Scellato, Noulas, and Mascolo [7] is to incorporate ge-
ographic features such as physical distance and check-
in data. This is a promising approach which demon-
strates how features other than network topology and
social attributes can be relevant in link prediction. We
believe that there are new ways to render such geo-
graphic information to improve prediction results.

1.2. Problem Formulation

We want to develop a robust algorithm that can
make accurate predictions for both existing and newly
formed nodes. Some graph theoretic features, such as
the Adamic-Adar score, play a crucial role in link pre-
diction, and they often yield outstanding results for
existing nodes. We will certainly incorporate these
features in building a new prediction model.

As mentioned above, however, it is diﬃcult to make
predictions for newly formed nodes by solely analyzing
the network topological properties. We run into simi-
lar obstacles when predicting possible links between a
pair of nodes with distance greater than 3. It is easy
to see how notions of topology deﬁned for immediate
neighbors might be insuﬃcient for a meaningful pre-
diction. To address such limitations, we complement
topological features by employing new features of so-
cial, geographic, and demographic ﬂavor.

1.3. Heuristic Overview
We consider three types of data sets: (1) S2 , topologi-
cal features are meaningful; (2) S3 , topological features
are unmeaningful; and (3) S∞ , newly formed nodes.
Then we deﬁne features which are classiﬁed into three
cateogires: (1) τ , topological features; (2) γ , geometric
features; and (3) σ , social features. The basic idea is
to train the prediction model on τ , τ ∪ γ , and τ ∪ γ ∪ σ
separately for each data set, and see how performance
is improved with the addition of each feature class.

Supervised Link Prediction by Geographic and Social Attributes

2. Data Rendering

We consider two types of data sets: (1) social net-
work data obtained from Gowalla, an online location-
based social network owned by Facebook; and (2) de-
mographic data based on the 2000 United States Cen-
sus which consists of demographics for each ZIP code
area.

2.1. Social Network Data

For the ﬁrst data set, we have friendship snapshots
taken at July of 2010 and October of 2010, and public
check-in history on February of 2009 and October of
2010 for users worldwide.

Not only is the size of the ﬁrst data set massive, but
the demographic information from the second data set
is restricted to the United States.

We therefore extract information on the set of users
with at least one check-in point in the United States.
The following table shows the size of reduced data set.

Time of snapshot
July of 2010
October of 2010

Number of nodes
26,989
36,231

Number of edges
115,495
178,791

As for the check-in history, there are 3,742,003 loca-
tions for the two time frames combined.

For simplicity,
in
let t1 and t2 denote the time,
chronological order, at which friendship snapshots
were taken. We will also refer to users as nodes. Each
node is uniquely assigned to a nonnegative integer.

Since the adjacency matrix for the reduced data set is
extremely sparse, we only consider the set A of nodes
incident to an edge that is present at t2 but not at t1 .
In other words, A consists of active nodes. We further
deﬁne A1 as the subset of A with nodes present only at
t1 , and set A2 = A \ A1 . Hence, A2 is the set of newly
formed nodes. Note also that A1 and A2 partition A.
Let u, v , and w be nodes in A. The degree of u is
denoted deg u. We write w ∼ {u, v} when w is adja-

cent to both u and v . The distance between u and v
is written as d(u, v).

The set of all check-in locations for both time frames
will be denoted by Λ. Hence, we do not distinguish be-
tween check-in points visited in diﬀerent time frames.
We also deﬁne λ(u) to be the set of all check-in loca-
tions of u ∈ A. Note that Λ is the disjoint union of
λ(u) for all u ∈ A.
Furthermore, we select subsets of A × A from which
we plan to build the training examples. First, let
S2 = {(u, v) ∈ A1 × A1 : d(u, v) = 2 with u > v}.

Many graph theoretic features are meaningful for this
data set. Similarly, we deﬁne
S3 = {(u, v) ∈ A1 × A1 : d(u, v) = 3 with u > v}.

Observe that topological features deﬁned for immedi-
ate neighbors are not useful in this case. Finally, the
data set for newly formed nodes is given by
S∞ = {(u, v) ∈ A1 × A2 : u ∼ v}.

By testing the prediction model on these classes of
training examples, we hope to demonstrate social, ge-
ographic, and demographic features signiﬁcantly im-
prove the accuracy of link predictions.

2.2. Demographic Data

The second data set consists of 19 ﬁelds representing
various demographical attributes. The following table
lists some of these features relevant to link prediction.

ZIP code
Population
Population density
Geographic area
Race
Age
Education
Household income
Per capita income
House value
Housing density

Area code
Total population
Population per unit area
Urban, suburban, farm, non-farm
White, Black, Asian, Indian, Hawaiian, other
Age groups
Education level of population over 18
Median household income
Median income per person
Average value of homes
Number of houses per unit area

For consistency, the check-in locations in the ﬁrst data
set are converted into an area code by using the Geo-
postal Service provided by Nuestar.

In order to take full advantage of the second data set,
we must ﬁrst deal with the missing values. Let D de-
note the entire demographic data set, where each el-
ement is a row vector ri for the ith area code. We
also write r (cid:48)
i to denote the truncation of ri , where the
entry with the missing values are simply deleted. We
can also form a column vectors cj for the j th ﬁeld, and
deﬁne its trucation c(cid:48)
j in a similar fashion. Note that
the size of r (cid:48)
i may vary for each i, and the same is true

Supervised Link Prediction by Geographic and Social Attributes
The Adamic-Adar score τa of u, v ∈ A is given by
(cid:88)
j . Furthermore, we let µi and µ(cid:48)
of c(cid:48)
i denote the sam-
ple of the entries of ri and r (cid:48)
i , respectively. As for the
1
sample covariance matrix, let Σ correspond to rows
and columns of the observed entries, while Σ(cid:48) corre-
log (deg w)
w∼{u,v}
sponds to the rows of missing entries and the columns
of the observed entries. We employ the expectation-
maximiation algorithm to estimate the missing values.

τa (u, v) =

.

Algorithm 1. (Expectation-Maximiation)
Initialization
(a) Set zij = µj and (cid:96)(cid:48) (Θ) = 0, and choose  > 0.
(b) Set i = 1, (cid:96)0 (Θ) = (cid:96)(Θ), and (cid:96)(Θ) = 0.
E-step
i + Σ(cid:48)Σ−1 (ri − µi ).
(c) Set µ(cid:48)
i = µ(cid:48)
(d) Set (cid:96)(Θ) = (cid:96)(Θ) − 1
2 {(ri − µi )T Σ−1 (ri − µi ) − log |Σ|}.
(e) Set i = i + 1, and if i < n, then go to (b).
M-step
(f ) Set Θ = arg maxΘ (cid:96)(Θ).
(g) If |(cid:96)(Θ) − (cid:96)0 (Θ)| ≥ , then go to (b), otherwise, break.

Note that, to get faster convergnece, we initialize the
missing values by the sample mean of the observed
entries in the corresponding column vectors.

3. Filter Feature Selection

We deﬁne and classify various features, and then run
a feature selection algorithm to eliminate the insignif-
icant ones.

3.1. Topological Features

The topological features are by far the most important
features as they retain information on the graph the-
oretical properties of the network. The most natural
topological feature is the number of common nodes,
(cid:88)
denoted τn . That is, given u ∈ A and v ∈ A,
1(w ∼ {u, v}).
w /∈{u,v}

τn (u, v) =

Observe that this feature does not take into account
the fact that users corresponding to nodes with higher
degree are more likely to be friends with a larger group
of users.
The cosine similarity τc of u, v ∈ A is deﬁned as

τc (u, v) =

τn (u, v)
deg u · deg v

.

By incorporating this feature into the prediction
model, we lend less signiﬁcance to a pair of nodes with
higher degree since users corresponding to these two
nodes are more likely to have many common friends.

We employ this feature to downgrade the eﬀect of com-
mon nodes with higher degree since users correspond-
ing to these nodes are more likely to be friends of a
large group of users.
We also deﬁne the preferential attachment τp of u ∈ A
and v ∈ A as

τp (u, v) = deg u · deg v .

This feature captures more active users corresponding
to nodes with higher degree.

3.2. Geograhpic Features

We now deﬁne a set of geographic features based on
the check-in history. Each check-in point is a physical
location which can be written in the geographic coor-
dinate system, that is, for x ∈ λ(u) for some u ∈ A,

γp (x) = (θ , φ),

where θ and φ are the latitude and longditude of x(u),
respectively.
The mode γm of u ∈ A is given by

γm (u) = arg max
x∈λ(u)

P(x(u)),

that is, the check-in location of u that occurs most
frequently.
Similarly, the sample mean γs of u ∈ A is deﬁned in
(cid:80)
the usual way as
(cid:80)
x∈λ(u) x
x∈Λ 1(x ∈ λ(u))
that is, the arithmetic mean of the check-in locations
of u.

γs (u) =

,

We would also like to deﬁne a feature that captures
the intuition of communities within a network. To do
this, we repeatedly apply k-means clustering to form
a binary decision tree. We proceed as follows.

Algorithm 2. (Modified k-Means Clustering)
(a) Set k = 2 and choose σ2 > 0.
(b) For u ∈ A, set (cid:96)1 = {λ(u)} and (cid:96)2 = (cid:96)(u) = Ø.
(c) For λ ∈ (cid:96)1 , run k-means clustering to get λ1 and λ2 .
(d) If var(λ1 ) ≥ σ2 , then λ1 ∈ (cid:96)2 ; otherwise, λ1 ∈ (cid:96)(u).
(e) If var(λ2 ) ≥ σ2 , then λ2 ∈ (cid:96)2 ; otherwise, λ2 ∈ (cid:96)(u).

Supervised Link Prediction by Geographic and Social Attributes
Finally, the urban population density σu of u ∈ A is
(f ) Set (cid:96)1 = (cid:96)1 \{λ}, and if (cid:96)1 (cid:54)= Ø, then go to (c).
deﬁned as
(g) Set (cid:96)1 = (cid:96)2 and (cid:96)2 = Ø, and go to (c).
The elements of (cid:96)(u) for u ∈ A in Algorithm 2 are
precisely the leaves of the binary decision tree for u.
We now let µ(u) denote the mean of the cluster in (cid:96)(u)
with the largest number of check-in points.

U (u)
N (u)
where U (u) is the population living in urban areas
within the area code for u.

σu (u) =

,

3.4. Mutual Information

We now compute the Kullback-Leibler divergence for
each feature, and eliminate features with low scores.

Feature MI Score
0.0435
τn
0.0205
τc
τa
0.0850
0.1670
τp

Feature MI Score
0.0988
γp
0.1456
γm
0.1042
γs
γc
0.1523

Feature MI Score
0.0737
σh
0.0435
σw
0.0620
σp
σu
0.0298

Among the 27 social features, many of which are not
listed in the table above, we eliminated the ones with
scores below 0.01. For instance, σf , the population
density of farmers had the lowest score of 9.7 × 10−4 .
Now among the selected features, we let τ , γ , and σ
denote the set of topological, geographical, and social
features, respectively. For S2 , by training on sets
2 = X (cid:48)
2 = X2 ∪ γ , X (cid:48)(cid:48)
X2 = S2 ∪ τ , X (cid:48)
2 ∪ σ,
we hope to observe how the addition of γ and σ en-
hance prediction outcomes. As for, S3 , we consider
3 = X (cid:48)
3 = X3 ∪ γ , X (cid:48)(cid:48)
X3 = S3 ∪ {τp}, X (cid:48)
3 ∪ σ.
Note that we only include τp from τ as this is the only
topological feature relevant to link predition for S3 .
Similarly, we deﬁne the training sets
X∞ = S∞ ∪ {τp}, X (cid:48)
∞ = X (cid:48)
∞ = X∞ ∪ γ , X (cid:48)(cid:48)
∞ ∪ σ
for S∞ . We expect γ and σ to play an even bigger role
in this case.

4. Supervised Learning Algorithms

We carry out three diﬀerent learning algorithms on
each training set deﬁned in Section 3.4.

4.1. Ridge Logistic Regression
We ﬁrst implement the ridge regression [2], where we
m(cid:88)
want to ﬁnd the maximum likelihood estimator ˆθ for
(1 − h(x(i) ))1−y(i)
i=1

log h(x(i) )y(i)

+ λθT θ ,

(cid:96)(θ) =

where h for parameter θ is given by

h(x; θ) =

1
1 + e−θT x
Note that we have added the L2 -norm of θ as a penalty
term to the log-likelihood function (cid:96).

.

The ﬁgure above shows the result of applying this algo-
rithm to a user living in California. Now the clustering
distance between u ∈ A and v ∈ A is deﬁned as
γc (u, v) = (cid:107)µ(u) − µ(v)(cid:107)2 ,
that is, the Euclidean distance between the mean of
the largest clusters in (cid:96)(u) and (cid:96)(v).

3.3. Social Features

Although 27 social features are considered in the pre-
diction model, we only discuss a few of the important
ones as the rest are deﬁned similarly. We write N (u) to
denote the total population of the area code for u ∈ A.
We deﬁne the housing density σh of u ∈ A as
H (u)
A(u)

σh (u) =

,

where H (u) is the number of houses in the area code
for u.
The density of white population σw for u ∈ A is
W (u)
N (u)

σw (u) =

,

where W (u) is the white population of the area code
for u.
We write the per capita income σp of u ∈ A as
I (u)
N (u)

σp (u) =

,

where I (u) is the net income of residents in the area
code for u.

Supervised Link Prediction by Geographic and Social Attributes

4.2. Naive Bayes Classiﬁer
n(cid:89)
Given a new example with feature x, we have that
j=1

P(x|y) =

P(xj |y)

by the naive Bayes assumption. We use the parameters
1(cid:112)2πσ2
φx|y=0 = P(xi = x|y = 0) =
1(cid:112)2πτ 2
i
i

φx|y=1 = P(xi = x|y = 1) =

e−(x−µi )2 /2σ2
i ,

e−(x−νi )2 /2τ 2
i ,

along with

φy=1 = P(y = 1).

to estimate the posterior probability of a new example
with features x as
P(y = k |x) =

P(x|y = k)P(y = k)
P(x)

,

where k ∈ {0, 1}.

4.3. Soft Margin Support Vector Machine
We apply the ν -soft margin classiﬁer [5] with parame-
ter 0 ≤ ν ≤ 1, where we minimize
m(cid:88)
i=1

wT w − ν ρ +

1
m

1
2

ξi

sub ject to the constraints
yi (wT xi + b) ≥ ρ − ξi , ξi ≥ 0, and ρ ≥ 0
for 1 ≤ i ≤ n. The parameter 0 ≤ ν ≤ 1 is a lower
bound on the fraction of support vectors. Note that
we have an additional variable ρ, where upon setting
ξi = 0 for 1 ≤ i ≤ n, the separating hyperplane is
given by 2ρ/wT w.

4.4. Prediction Results by Cross Validation

For each of the learning algorithms, we carry out the
k-fold cross validation for l models as follows.

Algorithm 3. (Cross Validation)
(a) Randomly partion S into S1 , ..., Sk ; set i = 1, j = 1.
(b) Train Mi on ∪p(cid:54)=j Sp to ﬁnd hypothesis hij .
(c) Test hij on Sj to ﬁnd ij ; set j = j + 1.
(d) If j < k , go to (b); else if, go to (e).
(e) Set i = 1
k Σ1≤j≤k ij , i = i + 1, j = 1.
(f ) If i < l, go to (b); else if, set i = arg min1≤i≤l i .
(g) Retrain model Mi on S to ﬁnd hypothesis h.

Here, ij is the training error. We used k = 10 with
l = 3 corresponding to naive Bayes, logistic regression,
and soft margin classiﬁers.

The following table shows the F1 scores when the clas-
siﬁers are trained by the set of features built on S2 .
X (cid:48)(cid:48)
X (cid:48)
X2
Classiﬁer
2
2
81.3
80.3
77.1
Naive Bayes
82.4
80.1
76.7
Logistic Regression
76.3
80.0
81.6
Support Vector Machine

Since all three classiﬁers perform similarly, the k-fold
cross validation is not very meaningful in this case.
Although τ alone return high F1 scores, it is evident
that γ and σ signiﬁcantly enhance the accuracy of link
predictions.
As for the set of features built on S3 , the k-fold cross
validation selects logistic regression as it returns the
higest F1 scores for each training set.
X (cid:48)
X3
3
65.3
48.1
70.1
59.2
40.3
62.1

Classiﬁer
Naive Bayes
Logistic Regression
Support Vector Machine

X (cid:48)(cid:48)
3
80.1
87.3
79.2

Observe that all three classiﬁers experience a steep
learning curve despite the low F1 scores for X3 .
In
this case, γ and σ play a crucial role in improving the
performance of all three classiﬁers.

We now look at the F1 scores corresponding to the set
of features built on S∞ .
Classiﬁer
Naive Bayes
Logistic Regression
Support Vector Machine

∞ X (cid:48)(cid:48)
X∞ X (cid:48)
∞
79.1
80.3
74.1
78.2
80.1
76.7
65.3
70.1
77.2

As with the previous cases, we see an overall improve-
ment with the addition of γ and σ into the feature
set. The naive Bayes classiﬁer has a slightly higher F1
score for X (cid:48)(cid:48)
∞ than the other two.

References

1. L. Backstrom, J. Lescovec, Supervised Random Walks: Pre-
dicting and Recommending Links in Social Networks, In
Proc. ACM WSDM. (2011)

2. S. le Cessie, J. van Houwelingen, Ridge Estimators in Logistic
Regression, Appl. Statist. 41 No. 1. (1992)

3. D. Liben-Nowell, J. Kleinberg, The Link Prediction Problem
for Social Networks, In International Conference on Informa-
tion and Knowledge Management. (2003)

4. D. Liben-Nowell, J. Novak, R. Kumar, P. Raghavan, A.
Tomkins, Geographic Routing in Social Networks, PNAS
102(33). (2005)

5. P. Chen, C. Lin, B. Sch¨olkopf, A Tutorial on ν -Support Vec-
tor Machines, Applied Stochastic Models in Business and In-
dustry 21. (2005)

6. A. Narayanan, V. Shmatikov, De-anonymizing Social Net-
works, In Proc. of IEEE Symposium on Security and Privacy.
(2009)

7. S. Scellato, A. Noulas, C. Mascolo, Exploiting Place Features
in Link Prediction on Location-based Social Networks, In
Proceedings of 17th ACM International Conference on Knowl-
edge Discovery and Data Mining. (2011)


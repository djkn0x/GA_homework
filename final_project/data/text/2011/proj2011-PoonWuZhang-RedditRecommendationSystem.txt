Reddit Recommendation System  
Daniel Poon, Yu Wu, David (Qifan) Zhang 
CS229, Stanford University 
December 11th, 2011 

 

1. Introduction 

Reddit  is one of  the most popular  online  social  news websites with millions of  registered 
users.  A  user  can  submit  content  (ident ified  by  “links”),  and  each  of  these  can  be  up-
voted or down-voted by others. Reddit does not yet have a recommendat ion system so the 
goal  of  this  project  is  to  develop  one  using  machine  learning  concepts.   Our  model  can 
also  be  used  to  rank  mult iple  recommendat ions  since  each  recommendation  has  a  real-
value score. 

 
2. Data and Statistics 

The  init ial  data  set,  obtained  from  the  Reddit  team,  had  23,091,688  votes,  43,976  users 
and 3,436,063  links (within 11,675 sub-reddits). This data-set represents 17% of all votes 
in 2008.  

We  reduced  the  amount  of  data  used  to  improve  efficiency.  For  instance,  the  number  of 
links was  too  large so we eliminated all  links which had  less  than 500 votes. The reduced 
data  set has 2,294,532 votes,  24,504 users  and 3,212  links  (within 29  sub-reddits). Some 
addit ional statist ics fo llow:  

Statistic 
links with more than 750 votes 
links with more than 1,000 votes 
average value of votes  
(sum of all vote values / number of votes)  
number of down -votes 
number of up-votes 
number of votes per link  
number of votes per user 
number of votes per sub-reddit 
number of users per sub-reddit  

Value 
987 
295 
0.8037 

225,165 
2,069,366 
714 
94 
79,122 
845 

 

3. Model Evaluation: RMSE 

In  order  to  evaluate  our  predict ions,  we  use  the  widely  acknowledged  root  mean  square 
error (RMSE) defined as: 

       √

 
        

∑              
             

 

At test phase, we evaluate on 10% of the data which had been held out during training. 

 

4. Baseline: Naive Predictor 

Since  about  90%  of  votes  are  up-votes  (value  +1)  our  first  attempt  is  to  try  a  naive 
predictor  that  guesses +1  for  all  votes. By doing  so, we obtain  an RMSE of  0.6265. Note 
that this  is a decent result  because the statist ics show that average value of votes is 0.8037, 
which  is close to 1. 

 

5. k-Nearest-Neighbors (kNN)  

In  the  kNN  model,  each  user  is  represented  as  a  vector  of  votes  ( -1,  0,  or  +1)  and  each 
vote  is  for  a  particular  link.  We  find  the  nearest  neighbor  users  based  on  Euclidean 
distance. The number of nearest neighbors, k, was adjusted manually.  

In the first  variant of the kNN model, we predict an unknown user vote as fo llows:  if the 
link has been seen by at  least one neighbor, we predict the average o f his neighbor’s 
votes for the link; otherwise the  link  is unseen and we predict the user’s own vote average.  
Unfortunately, the best performance was seen at k=1 (RMSE = 0.52397) which suggests 
that, in general, user average vote on unseen  links  is a better predictor than neighbor 
average on seen  links. 

The  model  above  was  improved  by  predict ing  a  weighted  average  of  neighbor  votes 
(wneighbor  = 0.3) and user average (wuser = 0.7)  for seen  links  to achieve RMSE  = 0.521522 
at k = 50.  

One variant that did not lead to improvement was using  link average rat ing for all unseen 
links. The best performance with this approach yields RMSE = 0.572846 at k = 10. 
 
5.1 Incorporating Sub-reddit  Data 

Since  the  data  provided  has  sub-reddit   informat ion,  where  each  link  belongs  to  one  sub-
reddit, it was important to incorporate this natural clustering  in our model.  

We  were  able  to  further  improve  the  kNN  model  by  predict ing  each  unseen  link  as  a 
weighted  average  of  average  sub-reddit   votes  (wsub-reddit  =  0.3)  and  average  user  vote 
(wuser  =  0.7).  This  gives  us  RMSE  =  0.518808  at k  =  50.  The  improvement  we  obtained  
confirms that the clustering  inherent in sub-reddit  data is useful for the recommender.  

 

6.  Singular Value Decomposition (SVD) 

SVD  is  one  o f  most  widely  used  models  for  collaborative  filtering  problems.  Data  in 
recommendat ion  systems  usually  comes  with  very  high  dimensionality.  The  algorithm 
assumes  that  the  data  actually  lies  in  a  lower  dimensional  subspace.  Therefore,  by 
compressing  and  reducing  dimensionality,  we  try  to  discover  hidden  correlations  and 
patterns  in  the  data  while  reducing  no isy  and  redundant  features.  In  essence,  the 
algorithm  tries  to  decompose  the  original  user - link  matrix  A  into  the  product  of  three 
matrices: 

                          

From  this, we  can  construct  diagonal  matrix  S  so  that                                 . Now 
we have: 

                           
Here, A'  is the best rank-k approximat ion for the original matrix A, and we use the entries 
of A' as predict ions.  

Since users only provide ratings for a small set of  links, we need to fill the missing values 
in  A. We  have  tested  two  approaches:  filling  with  the  average  vote  for  a  particular  link,  
and  filling  with  average  user  vote.  Empirically,  we  have  determined  that  the  first 
approach  generates  a  better  result  so  that  is  what  we  used  in  the  final  model.  For  the 
decomposit ion  algorithm,  we  used  a  randomized  algorithm  implemented  in  the  redsvd 
library.  This  algorithm  very  effic ient ly  so lves  the  decomposit ion  with  high  accuracy.  
SVD by  itself obtains an RMSE of 0.55948, with r = 100 and k = 30. 

 

7.  Bayesian  Probabilistic  Matrix  Factorization  using  Markov  Chain  Monte  Carlo 
(BPMF-MCMC) 

This model  tries  to  tackle  the matrix  factorizat ion pro blem  through  a Bayesian  approach.  
The prior distributions over user and  link vectors are assumed to be Gaussian:  

 
   
                ∏              
   

 

 
   
                ∏              
   

 

In  addit ion,  the  model  also  places  Gaussian-Wishart  priors  on  the  user  and  link  hyper  
parameters                 and                : 
                                                 
                                                  
Here W  is the Wishart distribution with     degrees of freedom, and    is a D×D matrix: 
 
 
 
 

                  

               

      
     

The  model  is  then  trained  through  Gibbs  sampling.  BPMF-MCMC  by  it self  achieves  an 
RMSE of 0.5018, which  is a very decent performance.  

 

8.  Stochastic Gradient Descent (SGD) 

We  have  also  tested  a  simpler  model  based  on  stochast ic  gradient  descent.  Let     be  the 
link  vector,     be  the  user  vector,  and      be  the  rating  from  user  u  for  link  l. The  model 
tries to minimize the predict ion error: 

   
     

                                
∑           
 
    

     .  Then  we  can 
During  the  training  process,  we  first  calculate  the  error                 
update the parameters  in the fo llowing way:  

                           
                          

SGD  is  able  to  attain  an RMSE of  0.596085. Although SGD does not perform  as we ll  as 
BPMF-MCMC,  we  determined  that  the  performance  was  st ill  decent  enough  that  SGD 
should be  included  in our final combined model.  

 

9.  K-means 

Another model we  tried  invo lved  using  k-means. As with  kNN,  each  user  is  represented 
as a vector of votes, one vote for each  link. We used k -means  to cluster similar users and 
make  predict ions  for  a  test  user-link  pair  by  taking  the  average  of  votes  made  by  other 
cluster members for that particular  link. In the case where no cluster member has seen the 
link,  we  predict  user  average.  This  algorithm  gives  us  RMSE  of  0.536  for  both  10  and 
100  clusters.  However,  upon  closer  examinat ion,  it  turns  out  that  the  majority  o f  users 
(>90%)  were  assigned  to  one  cluster  –  clustering  had  very  little  effect .  Therefore,  we 
decided to exclude k-means from the final model.  

 

10.  Linear Combination of Models 

At  this  point, we  had  four models  (kNN with  sub-reddit  data,  SVD,  BPMF-MCMC,  and 
SGD)  that  we  wanted  to  use  in  a  linear  combination  for  the  final  model.   These  will  be 
indexed  in  order,  1  through  4.  Given  a  test  user-link  pair,  a  model  will  make  the 
predict ion  of                ,  where  u  is  the  user  and  l  is  the  link.  We  also  added  the  +1 
and -1 constant terms to the linear combinat ion. Equivalent ly,  

                   
                    

Our linear combination  is:  

 
                     ∑                    
   

   

           

We  implemented  an  automat ic  parameter  tuner  to  efficient ly  adjust  the  weights  for  each 
model.  Each  weight  was  init ialized  to  ⅙.  At  each  iterat ion,  the  algorithm  randomly 
chooses  a weight  and  assigns  it  a  random  real  value  in  the  range  [0,  1].  If  the  combined 
model  performs  better  than  the  current  best  RMSE,  the  assignment   is  kept;  if 
performance  is  the  same  as  the  best,  the  assignment   is  kept  with  50%  probability; 
otherwise, the assignment  is rejected. 

Several  variat ions  of  this  tuning  process  were  attempted.  We  tried  to  normalize  the 
weights  so  that  they  sum  to  1  after  a  weight  assignment  at  every  iteration  but  the 
algorithm  converged much  slower  than without  normalizat ion. We  also  tried  the uniform 
distribut ion  U(0,  1)  and  Gaussian  distribut ion  N(wx,  0.1|wx|)  when  generating  a  rea l 
number  at  each  iteration.  The  uniform  distribut ion was  much  faster  at  zoning  in  on  a  set 
of near-optimal weights, while  the normal distribution was much more useful  to fine -tune 
weights.  Note  that  for  the  normal  distribut ion,  since  variance  approaches  zero  as  wx 
approaches  zero,  the  weights  can  get  stuck  near  zero.  To  address  this,  we  ensure  that 
variance  >  0.001.  Our  final  parameter  tuner  runs  about  50  iterations  with  uniform 
distribut ion,  fo llowed  by  about  500  iterations  with  the  normal  distribution.  The  result ing 
weights  were:  w1  =  0.0677075,  w2  =  0.0985695,  w3  =  0.934,  w4  =  0.0413389,  w5  = 
0.0604689,  and  w6  =  0.165892.  This  gives  us  a  combined  model  RMSE  of  0.491433, 
which  is  better  than  any  model  individually,  and  the  best  performance  overall.   The 
fo llowing graph summarizes the performances of all models:  

 

 

11. Conclusion and Future Work 

Using data provided by  the Reddit  team,  we were able  to build a recommendat ion system 
which achieves an RMSE of 0.491433 using a  linear combinat ion of kNN  with sub-reddit 
data,  SVD,  BPMF-MCMC,  and  SGD.  However,  the  recommender  can  certainly  be 
improved further. 

The  provided  data  has  limited  features.  There  are  many  powerful  models  that  could  be 
used  if we were given more detailed data. For example,  if we had  t ime stamps associated 
with each vote, we would be able to test a few popular temporal models. Intuit ively, more 
recent votes  should be valued  higher  than older ones.   Furthermore,  if we were given  the 
actual URLs and  tit les of  the  links rather than encrypted  ids, we may use NLP  techniques 
to  add more  features  to  the  data.  Another  promising  group of models  that we  should  test 
are ones based on weighted  ratings. Although  the original data does not have  any weight 
or  confidence  informat ion, we  can  try  to  infer  the weights based on  the popularity of  the 
links  and  the  act iveness  o f  the  users.   These  addit ional  features  all  have  the  potential  to 
decrease our RMSE score and significant ly  improve the recommendat ion system.  


WHAT MAKES FOR A HIT POP SONG? WHAT MAKES FOR A POP
SONG?

NICHOLAS BORG AND GEORGE HOKKANEN

Abstract. The possibility of a hit song prediction algorithm is both academically inter-
esting and industry motivated. While several companies currently attest to their ability to
make such predictions, publicly available research suggests that current methods are unlikely
to produce accurate predictions. Support Vector Machines were trained on song features
and YouTube view counts to very limited success. We discuss the possibility that musical
features alone cannot account for popularity. Given the lack of substantial ﬁndings in pop-
ularity position, we attempted a more feasible pro ject. Current research into automated
genre detection given features extracted from music has shown more promising. Using a
combination of K-Means clustering and Support Vector Machines, as well as a Random
Forest, we produced two automated classiﬁers that performs ﬁve times better than chance
for ten genres.

1. Data

Our main source of data was the Million
Song Dataset Subset distributed by Labrosa.
The subset provides pre-extracted features
for 10000 songs including song and artist
names, song duration, timbral data (MFCC-
like features) and spectral data for the entire
song, number of sections and average section
length, and a number of other features.
In order to measure the popularity of a
song, for each song we collected the number of
view counts registered on the video returned
as the ﬁrst link in a YouTube search using
the YouTube API, where the query consisted
of the song and artist names. We checked by
hand the accuracy of this scraping method
and concluded that the two errors in thirty
randomly drawn songs were unproblematic
given that the errors also coordinate well with
very low view counts (i.e. something unpop-
ular enough to not return a copy of the song
on youtube ends up returning an unrelated
video with a low view count).
For Genre Classiﬁcation, we used the Mil-
lion Song Dataset Genre subset. The dataset
includes features extracted from 59,600 songs

divided into ten genres: classic pop and rock,
folk, dance and electronica, jazz and blues,
soul and reggae, punk, metal, classical, pop,
and hip-hop. Features for each song include
loudness, tempo, time signature, key, mode,
duration, as well as average timbral data and
average timbral variance.

2. Preliminary Analysis: Popularity
Prediction

We ﬁrst ran basic correlation coeﬃcients
between diﬀerent parts of the metadata and
also with our extracted youtube view counts.
The results were largely insigniﬁcant and in-
cluded weak correlations such as one of .2
between the tempo and loudness metadata
features. Correlations between the youtube
view counts and the echonest metadata fea-
tures loudness, tempo, hotttness, and dance-
ability were completely negligible (less than
.05 in magnitude). The fact that these are
not at all correlated is interesting in its own
right because it points to no single metadata
feature being at all a good predictor of views
on youtube.

1

2

NICHOLAS BORG AND GEORGE HOKKANEN

3. Popularity Prediction –
Methodology and Results

3.1. Linear Classiﬁcation using Support
Vector Machines. First we note that the
feature vectors given by the million song
dataset are not of uniform length as the spec-
tral and mfcc coeﬃcients are provided for in-
tervals of the song and thus the number of
those features depends on the length of the
song. To account for all of the data and
compress it to uniform length for each song,
we took averages of the coeﬃcients for each
half, fourth, or sixth of the song and con-
catenated the result for feature vectors of
length 24, 48, or 72 for each set of coeﬃcients,
and then normalized the resulting lists of fea-
ture vectors. We trained Support Vector Ma-
chines with each extracted feature thinking
that some number of segments would outper-
form others (perhaps as they become closer to
the average of the actual number of segments
in the songs). We altered the cost, bias, and
kernel, but the precision never gained more
than one percent on our bias. That is, 53% of
the songs had youtube view counts over 10K
and 19% were over 100K. The Support Vector
Machines regardless of feature choice and pa-
rameters never achieved more than 53% and
81.5% precision. The recall was always less
than 53.5% and 82%.
Finally, we tried a Support Vector Ma-
chine on spectral averages and metadata fea-
tures including tempo, time signature, en-
ergy, loudness, duration, the number of sec-
tions, and ﬁnally the Echonest’s popularity
measure, ’hotttnesss’. Trained on popular-
ity measure of 100K views, these features
resulted in accuracy no better than before.
However, when trained on the popularity
measure of 10K views, the precision rose from
under 53% to over 55%. Using the same fea-
tures without ’hotttnesss’ results in the same
performance as above (no better than the
bias of the dataset). From this we conclude
that the Echonest’s ’hotttnesss’ measure (for
which they have not released an explanation

as to how it is quantiﬁed) gives a very small
amount of predictive power (2-3% above the
bias) on whether or not a song will have more
than 10K views on Youtube.

3.2. String Kernel. Given that the mfcc
and spectral data is temporal, we wanted
to use the ordering therein to describe the
sound. Motivating a string kernel Support
Vector Machine approach, we create ’string’
features for our songs as follows.
For each i, we take the spectral bucket
i (corresponding to a frequency-aggregate
magnitude) for each spectra vector within a
range of each song (usually about 45 sec-
onds in the middle). This gives a list of val-
ues which correspond to the magnitudes over
time of this slice of the sound spectrum for
every song. Next, using a subset of this data
(we used two hundred of the ten thousand
songs) we compute a list of intervals (we used
26 of them, corresponding to the characters
a through z) that uniformly distribute the
data. Then using these intervals, we compute
a string for each sequence of data obtained in
the ﬁrst step by replacing each value with a
symbol or letter that represents the interval.
When we tried a string kernel Support Vec-
tor Machine on this data, it was unable to
complete even the ﬁrst iteration towards con-
vergence. Notably, string kernels are not
guaranteed to be general Mercer kernels, and
our string data was so long and varied that
we suppose edit distance may be a very poor
metric. Furthermore, edit distance is not
aware of operations such as translation, which
can help tell that two pieces of music are sim-
ilar (consider putting a silent delay at the be-
ginning of a song, then the method mentioned
above will not recognize the songs as anything
similar). For this reason, we conclude that
this method of using a string kernel is a dead
end and may only be helped by more gen-
eral pattern matching metrics instead of the
overly simplistic edit distance. Algorithms
such as these, however, are their own area of
research and fall under the categorization of

WHAT MAKES FOR A HIT POP SONG? WHAT MAKES FOR A POP SONG?

3

structural segmentation and similarity met-
rics (most often used for identifying song cov-
ers). We ﬁnd the possibility of research on
estimating popularity by structural segmen-
tation to be interesting.

4. Popularity Prediction Discussion

Similarly to Pachet, we have concluded
that from the audio features extracted there
does not seem to be embedded the informa-
tion relevant in making the song popular.
This could be a result either of feature selec-
tion, or of popularity being driven by social
forces,
i.e.
”the inherent unpredictability
of cultural markets” (Pachet). The eﬀect
of cumulative advantage may help explain
this phenomenon and is demonstrated in the
curve (below) showing the declining percent-
age of view counts (e.g. there is very little
diﬀerence between how many songs have a
view count of 400K and 1M). Furthermore
once an artist is popular, they may later
produce works which are musically diﬀerent
from those that made them popular, yet the
new tracks will become popular simply by
virtue of being created by the popular artist.

5. Genre Classification

In light of our modest results at predict-
ing popularity, we began to investigate an-
other problem involving only musical
fea-
tures. Using the million song genre subset,

we tried several classiﬁcation algorithms in-
cluding Support Vector Machines with ten-
fold cross validation, k-nearest neighbors, and
random forests. These were run on the entire
data set, on a uniformly distributed subset,
and also on a four-genre subset consisting of
classical, metal, soul and reggae, and pop.
The results reported are after running pa-
rameter selection on the Support Vector Ma-
chine as well as on k for the k-nearest neigh-
bor implementation. We have read of imple-
mentations of KNN using KL-divergence as
a distance metric (Mandel), but the million
song genre subset did not have information
suﬃcient to compute the covariance matrix
of the timbral averages. Mirex hosts a num-
ber of papers on genre classiﬁcation, however,
an overview of the current literature suggests
that Random Forests are rarely used for genre
classiﬁcation tasks.
In addition, Support Vector Machines were
trained on individual pairs of genres, with n
songs from each genre.

6. Genre Classification Results

Results for various methods on diﬀerent
subsets are reported in the table below. Be-
cause the dataset was not uniformly pop-
ulated, we also used the uniform dataset.
The original distribution of songs in a given
genre is shown below in the histogram. Aver-
age results over all four clusters are reported
for cross-validation on the entries labeled K-
Means + Support Vector Machine. In addi-
tion, classiﬁcation results for pairs of genre
are reported in the second table below.
Results for four genres are comparable to
results seen elsewhere, but recent work on
music classiﬁcation suggests that it is possi-
ble to gain more accuracy on up to ten genres.
[4].
Notably, in all three classiﬁcation scenar-
ios, Random Forests perform approximately
as well as K-means in combination with Sup-
port Vector Machines. It is interesting to note
that, upon adding more trees to our random

4

NICHOLAS BORG AND GEORGE HOKKANEN

forest, the results seem to suggest that our
Support Vector Machines after K-Means are
both apprach the same accuracy. This can
be seen in the following graph, representing

the out of bag error of our Random Forest as
more trees are added. This result is from the
using the entire genre dataset.

Method
Dataset
SVM
All 59600 Songs
K-Means + SVM
All 59600 Songs
All 59600 Songs
Random Forest
All 59600 Songs 10-Nearest Neighbors
Uniform Genre
SVM
K-Means + SVM
Uniform Genre
Random Forest
Uniform Genre
10-Nearest Neighbors
Uniform Genre
4 Genre
SVM
K-Means + SVM
4 Genre
Random Forest
4 Genre
4 Genre
10-Nearest Neighbors

Result
49.09
54.15
56.80
43.84
47.60
52.93
51.18
19.72
76.00
83.35
83.19
74.58

WHAT MAKES FOR A HIT POP SONG? WHAT MAKES FOR A POP SONG?

5

Classical
Classical
Classic Rock
Electronica
Folk
Hip-hop
Jazz/Blues
Metal
Punk
Soul

Classic Rock Electronica Folk Hip-hop Jazz/Blues Metal Pop Punk
Soul
91.86 91.30 91.59
93.47
83.99
92.04
83.05
87.07
83.05
72.23 77.00 77.48
87.75
76.43
80.35
73.50
74.56
0
0
0
81.03
77.16
79.25
89.25
79.79 84.45 79.54
78.64 86.68 82.27
92.17
76.75
87.61
0
0
0
79.60 80.70 70.44
87.07
83.18
0
0
0
0
0
0
0
0
0
91.62
83.02 87.74 82.74
91.01 81.09 94.36
0
0
0
0
0
0
82.07 77.60
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
84.40
7. Moving Forward
http://www.csie.ntu.edu.tw/ cjlin/liblinear/
2001.
[2] Mandel, M., & Ellis, D. (2005b).
Song-level features and Support Vector Ma-
chines for music classiﬁcation. Extended Ab-
stract. MIREX 2005 genre classiﬁcation con-
test
(www.music-ir.org/evaluation/mirex-
results).
[3] Pachet, F. & Roy, P. (2007). Explor-
ing billions of audio features. In International
Workshop on Content-Based Multimedia In-
dexing (CBMI), pp. 227235.
[4] Pachet, F., Roy, P.: Hit song science is
not yet a science. In: 9th International Con-
ference on Music Information Retrieval (IS-
MIR 2008) (2008)
[5] Thierry Bertin-Mahieux, Daniel P.W.
Ellis, Brian Whitman, and Paul Lamere. The
Million Song Dataset. In Proceedings of the
12th International Society for Music Infor-
mation Retrieval Conference (ISMIR 2011),
2011.
[6] Mirex 2009 Audio Competition ("http:
//www.music-ir.org/mirex/wiki/2009:
Audio_Genre_Classification_%28Mixed_
Set%29_Results")

First we make a note on feature selection.
In Exploring Billions of Audio Features [5],
Pachet notes a distinction between the low-
and high-level features used for many mu-
sic information retrieval and/or classiﬁcation
tasks. He describes that many of the fea-
tures so commonly used, in our case spec-
tral coeﬃcients and mfccs, are not adequate
for many tasks and gives a general frame-
work for exploring the enormous possibili-
ties in exploring the feature space for musical
signals. Relating this to our failure to pro-
vide results with respect to estimating pop-
ularity, it would seem that the task is not
necessarily entirely hopeless and that our in-
conclusive ﬁndings (as well as Pachet’s) do
not demonstrate the inability to predict pop-
ularity at least somewhat from musical fea-
tures (though the many studies of cultural
dynamics would seem to indicate that there
are other parameters at work, though possi-
bly in conjunction with the musical qualities
as well).

8. References

[1]C.-C. Chang, C.-J. Lin.
blinear:
a
library
for
large
available
classiﬁcation Software
ear

Li-
lin-
at


Learning with the Weighted Trace-norm under
Arbitrary Sampling Distributions

Rina Foygel
Department of Statistics
University of Chicago
rina@uchicago.edu

Ruslan Salakhutdinov
Department of Statistics
University of Toronto
rsalakhu@ustat.toronto.edu

Ohad Shamir
Microsoft Research New England
ohadsh@microsoft.com

Nathan Srebro
Toyota Technological Institute at Chicago
nati@ttic.edu

Abstract

We provide rigorous guarantees on learning with the weighted trace-norm under
arbitrary sampling distributions. We show that the standard weighted-trace norm
might fail when the sampling distribution is not a product distribution (i.e. when
row and column indexes are not selected independently), present a corrected vari-
ant for which we establish strong learning guarantees, and demonstrate that it
works better in practice. We provide guarantees when weighting by either the true
or empirical sampling distribution, and suggest that even if the true distribution is
known (or is uniform), weighting by the empirical distribution may be beneﬁcial.

1

Introduction

One of the most common approaches to collaborative ﬁltering and matrix completion is trace-norm
regularization [1, 2, 3, 4, 5]. In this approach we attempt to complete an unknown matrix, based on
a small subset of revealed entries, by ﬁnding a matrix with small trace-norm, which matches those
entries as best as possible.
This approach has repeatedly shown good performance in practice, and is theoretically well under-
stood for the case where revealed entries are sampled uniformly [6, 7, 8, 9, 10, 11]. Under such
uniform sampling, Θ(n log(n)) entries are sufﬁcient for good completion of an n × n matrix—
i.e. a nearly constant number of entries per row. However, for arbitrary sampling distributions, the
worst-case sample complexity lies between a lower bound of Ω(n4/3 ) [12] and an upper bound of
O(n3/2 ) [13], i.e. requiring between n1/3 and n1/2 observations per row, and indicating it is not
appropriate for matrix completion in this setting.
Motivated by these issues, Salakhutdinov and Srebro [12] proposed to use a weighted variant of the
trace-norm, which takes the distribution of the entries into account, and showed experimentally that
this variant indeed leads to superior performance. However, although this recent paper established
that the weighted trace-norm corrects a speciﬁc situation where the standard trace-norm fails, no
general learning guarantees are provided, and it is not clear if indeed the weighted trace-norm al-
ways leads to the desired behavior. The only theoretical analysis of the weighted trace-norm that we
are aware of is a recent report by Negahban and Wainwright [10] that provides reconstruction guar-
antees for a low-rank matrix with i.i.d. noise, but only when the sampling distribution is a product
distribution, i.e. the rows index and column index of observed entries are selected independently. A
product distribution assumption does not seem realistic in many cases—e.g. for the Netﬂix data, it
would indicate that all users have the same (conditional) distribution over which movies they rate.

1

In this paper we rigorously study learning with a weighted trace-norm under an arbitrary sampling
distribution, and show that this situation is indeed more complicated, requiring a correction to the
weighting. We show that this correction is necessary, and present empirical results on the Netﬂix
and MovieLens dataset indicating that it is also helpful in practice. We also rigorously consider
weighting according to either the true sampling distribution (as in [10]) or the empirical frequencies,
as is actually done in practice, and present evidence that weighting by the empirical frequencies
might be advantageous. Our setting is also more general than that of [10]—we consider an arbitrary
loss and do not rely on i.i.d. noise, instead presenting results in an agnostic learning framework.
Setup and Notation. We consider an arbitrary unknown n × m target matrix Y , where a subset
of entries {Yit ,jt }s
t=1 indexed by S = {(i1 , j1 ), . . . , (is , js )} is revealed to us. Without loss of
generality, we assume n ≥ m. Throughout most of the paper, we assume S is drawn i.i.d. according
(cid:104)
(cid:105)
to some sampling distribution p(i, j ) (with replacement). Based on this subset on entries, we would
like to ﬁll in the missing entries and obtain a prediction matrix ˆXS ∈ Rn×m , with low expected loss
Lp ( ˆXS ) = Eij∼p
(cid:96)(( ˆXS )ij , Yij )
, where (cid:96)(x, y) is some loss function. Note that we measure the
loss with respect to the same distribution p(i, j ) from which the training set is drawn (this is also the
case in [12, 10, 13]).
The trace-norm of a matrix X ∈ Rn×m , written (cid:107)X (cid:107)tr , is deﬁned as the sum of its singular values.
(cid:13)(cid:13)(cid:13)diag (pr )1/2 · X · diag (pc )1/2(cid:13)(cid:13)(cid:13)tr
Given some distribution p(i, j ) on [n] × [m], the weighted trace-norm of X is given by [12]
(cid:107)X (cid:107)tr(pr ,pc ) =
,
where pr ∈ Rn and pc ∈ Rm denote vectors of the row- and column-marginals respectively. Note
that the weighted trace-norm only depends on these marginals (but not their joint distribution) and
that if pr and pc are uniform, then (cid:107)X (cid:107)tr(pr ,pc ) = 1√
nm (cid:107)X (cid:107)tr . The weighted trace-norm does not
generally scale with n and m, and in particular, if X has rank r and entries bounded in [−1, 1], then
(cid:107)X (cid:107)tr(pr ,pc ) ≤ √
r(cid:9),
Wr [p] = (cid:8)X ∈ Rn×m : (cid:107)X (cid:107)tr(pr ,pc ) ≤ √
r regardless of which p (i, j ) is used. This motivates us to deﬁne the class
although we emphasize that our results do not directly depend on the rank, and Wr [p] certainly
(cid:80)s
includes full-rank matrices. We analyze here estimators of the form ˆXS = arg min{ ˆLS (X ) : X ∈
Wr [p]} where ˆLS (X ) = 1
t=1 (cid:96)(Xit ,jt , Yit ,jt ) is the empirical error on the observed entries.
s
Although we focus mostly on the standard inductive setting, where the samples are drawn i.i.d. and
the guarantee is on generalization for future samples drawn by the same distribution, our results can
also be stated in a transductive model, where a training set and a test set are created by splitting a
ﬁxed subset of entries uniformly at random (as in [13]). The transductive setting is discussed, and
transductive variants of our Theorems are given, in Section 4.2 and in the Supplementary Materials.

2 Learning with the Standard Weighting

In this Section, we consider learning using the weighted trace-norm as suggested by Salakhutdinov
and Srebro [12], i.e. when the weighting is according to the sampling distribution p(i, j ). Following
the approach of [6] and [11], we base our results on bounding the Rademacher complexity of Wr [p],
as a class of functions mapping index pairs to entry values. However, we modify the analysis for the
weighted trace-norm with non-uniform sampling.
For a class of matrices X and a sample S = {(i1 , j1 ), . . . , (is , js )} of indexes in [n] × [m], the
(cid:35)
(cid:34)
s(cid:88)
empirical Rademacher complexity of the class (with respect to S ) is given by
ˆRS (X ) = Eσ∼{±1}s
1
σtXit jt
sup
X ∈X
s
t=1
where σ is a vector of signs drawn uniformly at random. Intuitively, ˆRS (X ) measures the extent to
which the class X can “overﬁt” data, by ﬁnding a matrix X which correlates as strongly as possible
to a sample from a matrix of random noise. For a loss (cid:96)(x, y) that is Lipschitz in x, the Rademacher
complexity can be used to uniformly bound the deviations |Lp (X )− ˆLS (X )| for all X ∈ X , yielding
a learning guarantee on the empirical risk minimizer [14].

,

2

2.1 Guarantees for Special Sampling Distributions

We begin by providing guarantees for an arbitrary, possibly unbounded, Lipschitz loss (cid:96)(x, y), but
only under sampling distributions which are either product distributions (i.e. p(i, j ) = pr (i)pc (j ))
or have uniform marginals (i.e. pr and pc are uniform, but perhaps the rows and columns are not
independent). In Section 2.3 below, we will see why this severe restriction on p is needed.
(cid:110) ˆLS (X ) : X ∈ Wr [p]
(cid:111)
Theorem 1. For an l-Lipschitz loss (cid:96), ﬁx any matrix Y , sample size s, and distribution
Let ˆXS =
p, such that p is either a product distribution or has uniform marginals.
. Then, in expectation over the training sample S drawn i.i.d.
arg min
(cid:32)
(cid:33)
(cid:114)
from the distribution p,
Lp ( ˆXS ) ≤ inf
l ·
X ∈Wr [p]

rn log(n)
s

Lp (X ) + O

.

(1)

ES

Here and elsewhere we state learning guarantees in expectation for simplicity. Since the guaran-
tees are obtained by bounding the Rademacher complexity, one can also immediately obtain high-
probability guarantees, with logarithmic dependence on the conﬁdence parameter, via standard tech-
(cid:105)
(cid:104) ˆRS (Wr [p])
niques (e.g. [14]).
Proof. We will show how to bound the expected Rademacher complexity ES
, from
which the desired results follows using standard arguments (Theorem 8 of [14]1 ). Following [11] by
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) s(cid:88)
 ,
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) s(cid:88)
 =
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)sp
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)sp
including the weights, using the duality between spectral norm (cid:107)·(cid:107)sp and trace-norm, we compute:
(cid:104) ˆRS (Wr [p])
(cid:105)
(cid:112)pr (it ) pc (jt )
√
√
eit ,jt
r
r
ES,σ
ES,σ
σt
Qt
=
s
s
t=1
t=1
√
∈ Rn×m . Since the Qt ’s are i.i.d. zero-
(cid:104)(cid:107)(cid:80)s
(cid:105)
eit ,jt
where ei,j = ei eT
j and Qt = σt
= O(cid:0)ρ(cid:112)log(n) + R log(n)(cid:1), where R and ρ are deﬁned to satisfy
pr (it )pc (jt )
mean matrices, Theorem 6.1 of [15], combined with Remarks 6.4 and 6.5 there, establishes
(cid:3)(cid:13)(cid:13)sp
(cid:3)(cid:13)(cid:13)sp , (cid:13)(cid:13)(cid:80) E (cid:2)QtQT
(cid:107)Qt(cid:107)sp ≤ R (almost surely) and ρ2 = max (cid:8) (cid:13)(cid:13)(cid:80) E (cid:2)QT
(cid:9). Calculating
t=1 Qt(cid:107)sp
that ES,σ
these bounds (see Supplementary Material), we get R ≤ (cid:113)
t Qt
t
(cid:118)(cid:117)(cid:117)(cid:116)s max
(cid:26)
(cid:114)
mini,j {npr (i)·mpc (j )} , and
nm
(cid:111) ≤
(cid:88)
(cid:88)
ρ ≤
p (i, j )
p (i, j )
max
pr (i) pc (j )
pr (i) pc (j )
i
i
j
(cid:16)(cid:113) rn log(n)
(cid:104) ˆR(Wr [p])
(cid:105) ≤ O
(cid:17)
If p has uniform row- and column-marginals, then for all i, j , npr (i) = mpc (j ) = 1. This yields
ES
, as desired. (Here we assume s > n log(n), since otherwise we
√
s
r), which holds trivially for any matrix in Wr [p].)
need only establish that excess error is O(l
If p does not have uniform marginals, but instead is a product distribution, then the quantity R
(cid:27)
(cid:26)
deﬁned above is potentially unbounded, so we cannot apply the same simple argument. However,
(cid:111)(cid:17)
(cid:16)
Xij I(cid:110)
we can consider the “p-truncated” class of matrices
√
: X ∈ Wr [p]
Z =
p (i, j ) ≥ log(n)
(cid:104) ˆRS (Z )
(cid:105) ≤
.
Z (X ) =
nm
s
ij
(cid:16)(cid:113) rn log(n)
(cid:17)
. Applying Theorem 8 of [14], this bounds (cid:0)Lp (Z ( ˆXS )) − ˆLS (Z ( ˆXS ))(cid:1) (in ex-
By a similar calculation of the expected spectral norms, we can now bound ES
O
s
(cid:54)= ( ˆXS )ij only on the extremely low-probability entries, we can
pectation). Since Z ( ˆXS )ij
1Theorem 8 of [14] gives a learning guarantee holding with high probability, but their proof of this theorem
(in particular, the last series of displayed equations) contains a guarantee in expectation, which we use here.

sn
mini,j {npr (i) · mpc (j )} .

, max
j

3

also bound (cid:0)Lp ( ˆXS ) − Lp (Z ( ˆXS ))(cid:1) and (cid:0) ˆLS (Z ( ˆXS )) − ˆLS ( ˆXS )(cid:1). Combining these steps,
we can bound (cid:0)Lp ( ˆXS ) − ˆLS ( ˆXS )(cid:1). We similarly bound ˆLS (X ∗ ) − Lp (X ∗ ), where X ∗ =
arg minX ∈Wr [p] Lp (X ). Since ˆLS ( ˆXS ) ≤ ˆLS (X ∗ ), this yields the desired bound on excess er-
ror. The details are given in the Supplementary Materials.

Examining the proof of Theorem 1, we see that we can generalize the result by including distri-
butions p with row- and column-marginals that are lower-bounded. More precisely, if p satisﬁes
pr (i) ≥ 1
Cn , pc (j ) ≥ 1
Cm for all i, j , then the bound (1) holds, up to a factor of C . Note that this
result does not require an upper bound on the row- and column-marginals, only a lower bound, i.e. it
only requires that no marginals are too low. This is important to note since the examples where the
unweighted trace-norm fails under a non-uniform distribution are situations where some marginals
are very high (but none are too low) [12]. This suggests that the low-probability marginals could
perhaps be “smoothed” to satisfy a lower bound, without removing the advantages of the weighted
trace-norm. We will exploit this in Section 3 to give a guarantee that holds more generally for
arbitrary p, when smoothing is applied.

2.2 Guarantees for bounded loss

In Theorem 1, we showed a strong bound on excess error, but only for a restricted class of distri-
butions p. We now show that if the loss function (cid:96) is bounded, then we can give a non-trivial, but
weaker, learning guarantee that holds uniformly over all distributions p. Since we are in any case
discussing Lipschitz loss functions, requiring that the loss function be bounded essentially amounts
to requiring that the entries of the matrices involved be bounded. That is, we can view this as a
guarantee on learning matrices with bounded entries. In Section 2.3 below, we will show that this
(cid:110) ˆLS (X ) : X ∈ Wr [p]
(cid:111)
boundedness assumption is unavoidable if we want to give a guarantee that holds for arbitrary p.
Theorem 2. For an l-Lipschitz loss (cid:96) bounded by b, ﬁx any matrix Y , sample size s, and any
for r ≥ 1. Then, in expectation over
distribution p. Let ˆXS = arg min
(cid:32)
(cid:33)
(cid:114)
the training sample S drawn i.i.d. from the distribution p,
Lp ( ˆXS ) ≤ inf
(l + b) · 3
rn log(n)
Lp (X ) + O
X ∈Wr [p]
s
(cid:113) rn log(n)
(cid:104) ˆR((cid:96) ◦ Wr [p])
(cid:105) ≤ O
(cid:16)
(cid:17)
The proof is provided in the Supplementary Materials, and is again based on analyzing the expected
(l + b) · 3
Rademacher complexity, ES
.
s
2.3 Problems with the standard weighting

(2)

.

In the previous Sections, we showed that for distributions p that are either product distributions or
have uniform marginals, we can prove a square-root bound on excess error, as shown in (1). For
arbitrary p, the only learning guarantee we obtain is a cube-root bound given in (2), for the special
case of bounded loss. We would like to know whether the square-root bound might hold uniformly
over all distributions p, and if not, whether the cube-root bound is the strongest result that we can
give for the bounded-loss setting, and whether any bound will hold uniformly over all p in the
unbounded-loss setting.
The examples below demonstrate that we cannot improve the results of Theorems 1 and 2 (up to log
factors), by constructing degenerate examples using non-product distributions p with non-uniform
marginals. Speciﬁcally, in Example 1, we show that in the special case of bounded loss, the cube-
(cid:1). In Example 2, we show that with unbounded (Lipschitz) loss, we cannot bound
error is Ω (cid:0) 3(cid:112) n
root bound in (2) is the best possible bound (up to the log factor) that will hold for all p, by giving
a construction for arbitrary n = m and arbitrary s ≤ nm, such that with 1-bounded loss, excess
s
excess error better than a constant bound, by giving a construction for arbitrary n = m and arbitrary
s ≤ nm in the unbounded-loss regime, where excess error is Ω(1). For both examples we ﬁx
r = 1. We note that both examples can be modiﬁed to ﬁt the transductive setting, demonstrating
that smoothing is necessary in the transductive setting as well.

4

, (p (i, j )) =

 ,
 1
Example 1. Let (cid:96)(x, y) = min{1, |x − y |} ≤ 1, let a = (2s/n)2/3 < n, and let matrix Y and
(cid:32)
(cid:33)
block-wise constant distribution p be given by
2s · 1a× n
0a× n
0a× n
A
2
2
1− an
2
· 1(n−a)× n
0(n−a)× n
0(n−a)× n
4s
0(n−a)× n
(n−a) n
2
2
2
2
2
where A ∈ {±1}a× n
2 is any sign matrix. Clearly, (cid:107)Y (cid:107)tr(pr ,pc ) ≤ 1, and so inf X ∈Wr [p] Lp (X ) = 0.
3(cid:112) n
Now suppose we draw a sample S of size s from the matrix Y , according to the distribution p. We
ij = Yij I {ij ∈ S }, and note that (cid:13)(cid:13)Y S (cid:13)(cid:13)tr(pr ,pc ) ≤ 1. Since ˆLS (Y S ) = 0, it
will show an ERM ˆY such that in expectation over S , Lp ( ˆY ) ≥ 1
s .
8
4 , we see that E (cid:2)Lp (Y S )(cid:3) ≥ 1
3(cid:112) n
Consider Y S where Y S
2s , where N is the number of ±1’s in Y which are not
is clearly an ERM. We also have Lp (Y S ) = N
observed in the sample. Since E [N ] ≥ an
4 ≥ 1
2s · an
s .
8
Example 2. Let (cid:96)(x, y) = |x − y |. Let Y = 0n×n ; trivially, Y ∈ Wr [p]. Let p (1, 1) = 1
s , and
p (i, 1) = p (1, j ) = 0 for all i, j > 1, yielding pr (1) = pc (1) = 1
s . (The other entries of p may be
deﬁned arbitrarily.) We will show an ERM ˆY such that, in expectation over S , Lp ( ˆY ) ≥ 0.25. Let
A be the matrix with X11 = s and zeros elsewhere, and note that (cid:107)A(cid:107)tr(pr ,pc ) = 1. With probability
≥ 0.25, entry (1, 1) will not appear in S , in which case ˆY = A is an ERM, with Lp ( ˆY ) = 1.
The following table summarizes the learning guarantees that can be established for the (standard)
weighted trace-norm. As we saw, these guarantees are tight up to log-factors.
(cid:113) rn log(n)
(cid:113) rn log(n)
1-Lipschitz, unbounded loss
1-Lipschitz, 1-bounded loss
(cid:113) rn log(n)
(cid:113) rn log(n)
(cid:113) rn log(n)
s
s
s
s
1
3
s
3 Smoothing the weighted trace norm

pr , pc = uniform

p = product

Y =

p arbitrary

Considering Theorem 1 and the degenerate examples in Section 2.3, it seems that in order to be
able to generalize for non-product distributions, we need to enforce some sort of uniformity on
the weights. The Rademacher complexity computations in the proof of Theorem 1 show that the
problem lies not with large entries in the vectors pr and pc (i.e. if pr and/or pc are “spiky”), but with
the small entries in these vectors. This suggests the possibility of “smoothing” any overly low row-
or column-marginals, in order to improve learning guarantees.
In Section 3.1, we present such a smoothing, and provide guarantees for learning with a smoothed
weighted trace-norm. The result suggests that there is no strong negative consequence to smoothing,
but there might be a large advantage, if confronted with situations as in Examples 1 and 2. In Section
3.2 we check the smoothing correction to the weighted trace-norm on real data, and observe that
indeed it can also be beneﬁcial in practice.

3.1 Learning guarantee for arbitrary distributions
Fix a distribution p and a constant α ∈ (0, 1), and let ˜p denote the smoothed marginals:
˜pr (i) = α · pr (i) + (1 − α) · 1
˜pc (j ) = α · pc (j ) + (1 − α) · 1
(3)
n ,
m .
In the theoretical results below, we use α = 1
2 , but up to a constant factor, the same results hold for
any ﬁxed choice of α ∈ (0, 1).
(cid:110) ˆLS (X ) : X ∈ Wr [ ˜p]
(cid:111)
Theorem 3. For an l-Lipschitz loss (cid:96), ﬁx any matrix Y , sample size s, and any distribution p. Let
(cid:32)
(cid:33)
ˆXS = arg min
. Then, in expectation over the training sample S drawn
(cid:114)
i.i.d. from the distribution p,
Lp ( ˆXS ) ≤ inf
l ·
X ∈Wr [ ˜p]

rn log(n)
s

Lp (X ) + O

(4)

.

5

(cid:18)(cid:113) rn log(n)
(cid:19)
(cid:104) ˆRS (Wr [ ˜p])
(cid:105) ≤ O
Proof. We bound ES∼p
, and then apply Theorem 8 of [14].
s
√
(cid:105)
(cid:104)(cid:13)(cid:13)(cid:80)s
The proof of this Rademacher bound is essentially identical to the proof in Theorem 1, with the
(cid:13)(cid:13)sp
(cid:80)
(cid:80)
1√
√
. Then (cid:107)Qt(cid:107)sp ≤ maxij
≤ 2
.
eit ,jt
modiﬁed deﬁnition of Qt = σt
= R,
nm
(cid:105) ≤ 4sn. Setting ρ
(cid:104)(cid:13)(cid:13)(cid:80)s
˜pr (i) ˜pc (j )
˜pr (i) ˜pc (j )
(cid:13)(cid:13)sp
˜pr (i) ˜pc (j ) ≤ s · maxi
≤ 4sm. Similarly,
= s · maxi
and E
p(i,j )
p(i,j )
t=1 QtQT
2 pr (i)· 1
√
t
1
j
j
2m
.
E
4sn and applying [15], we obtain the result.
t=1 QT
t Qt
=
Moving from Theorem 1 to Theorem 3, we are competing with a different class of matrices:
inf X ∈Wr [p] Lp (X ) (cid:32) inf X ∈Wr [ ˜p] Lp (X ).
In most applications we can think of, this change is
be a lower bound on the rank, we would need to assume (cid:13)(cid:13)diag (pr )1/2 X diag (pc )1/2 (cid:13)(cid:13)2
not signiﬁcant. For example, we consider the low-rank matrix reconstruction problem, where the
(cid:13)(cid:13)2
2 ≤ m and (cid:13)(cid:13)(X ∗ )(j )(cid:13)(cid:13)2
we also assume that (cid:13)(cid:13)(X ∗ )(i)
trace-norm bound is used as a surrogate for rank. In order for the (squared) weighted trace-norm to
F ≤ 1 [11]. If
2 ≤ n for all rows i and columns j — i.e. the
row and column magnitudes are not “spiky” — then X ∗ ∈ Wr [ ˜p]. Note that this condition is much
weaker than placing a spikiness condition on X ∗ itself, e.g. requiring |X ∗ |∞ ≤ 1.

3.2 Results on Netﬂix and MovieLens Datasets

We evaluated different models on two publicly-available collaborative ﬁltering datasets: Netﬂix [16]
and MovieLens [17]. The Netﬂix dataset consists of 100,480,507 ratings from 480,189 users on
17,770 movies. Netﬂix also provides a qualiﬁcation set containing 1,408,395 ratings, but due to the
sampling scheme, ratings from users with few ratings are overrepresented relative to the training
set. To avoid dealing with different training and test distributions, we also created our own valida-
tion and test sets, each containing 100,000 ratings set aside from the training set. The MovieLens
dataset contains 10,000,054 ratings from 71,567 users on 10,681 movies. We again set aside test
and validation sets of 100,000 ratings. Ratings were normalized to be zero-mean.
When dealing with large datasets the most practical way to ﬁt trace-norm regularized models is
via stochastic gradient descent [18, 3, 12]. For computational reasons, however, we consider
rank-truncated trace-norm minimization, by optimizing within the restricted class {X : X ∈
Wr [p] , rank (X ) ≤ k} for k = 30 and k = 100, and for various values of smoothing parameters α
(as in (3)). For each value of α and k , the regularization parameter was chosen by cross-validation.
The following table shows root mean squared error (RMSE) for the experiments. For both k=30 and
k=100 the weighted trace-norm with smoothing (α = 0.9) signiﬁcantly outperforms the weighted
trace-norm without smoothing (α = 1), even on the differently-sampled Netﬂix qualiﬁcation set.
The proposed weighted trace-norm with smoothing outperforms max-norm regularization [19], and
performs comparably to “geometric” smoothing [12]. On the Netﬁx qualiﬁcation set, using k=30,
max-norm regularization and geometric smoothing achieve RMSE 0.9138 [19] and 0.9091 [12],
compared to 0.9096 achieved by the weighted trace-norm with smoothing. We note that geometric
smoothing was proposed by [12] as a heuristic without any theoretical or conceptual justiﬁcation.
MovieLens
Netﬂix
Qual
Test
k
100
0.7852
0.9107
100
0.7831
0.9096
100
0.7836
0.9173
100
0.7864
0.9198
0.9249
0.7997
100

Qual
0.9078
0.9068
0.9161
0.9207
0.9236

Test
0.7404
0.7391
0.7419
0.7528
0.7659

Test
0.7604
0.7589
0.7601
0.7712
0.7887

Test
0.7821
0.7798
0.7815
0.7871
0.7987

k
30
30
30
30
30

α
1
0.9
0.5
0.3
0

k
30
30
30
30
30

k
100
100
100
100
100

4 The empirically-weighted trace norm

In practice, the sampling distribution p is not known exactly — it can only be estimated via the
locations of the entries which are observed in the sample. Deﬁning the empirical marginals
#{t : jt = j }
#{t : it = i}
s
s

, ˆpc (j ) =

ˆpr (i) =

,

6

we would like to give a learning guarantee when ˆXS is estimated via regularization on the ˆp-
weighted trace-norm, rather than the p-weighted trace-norm.
In Section 4.1, we give bounds on excess error when learning with smoothed empirical marginals,
which show that there is no theoretical disadvantage as compared to learning with the smoothed true
marginals. In fact, we provide evidence that suggests there might even be an advantage to using
the empirical marginals. To this end, in Section 4.2, we introduce the transductive learning setting,
and give a result based on the empirical marginals which implies a sample complexity bound that
is better by a factor of log1/2 (n). In Section 4.3, we show that in low-rank matrix reconstruction
simulations, using empirical marginals indeed yields better reconstructions.

4.1 Guarantee for the standard (inductive) setting
(cid:1) ,
(cid:0) ˆpc (j ) + 1
(cid:1) , ˇpc (j ) = 1
(cid:0) ˆpr (i) + 1
We ﬁrst show that when learning with the smoothed empirical marginals, deﬁned as
ˇpr (i) = 1
2
n
2
m
(cid:110) ˆLS (X ) : X ∈ Wr [ ˇp]
(cid:111)
we can obtain the same guarantee as for learning with the smoothed (true) marginals, given by ˜p.
Theorem 4. For an l-Lipschitz loss (cid:96), ﬁx any matrix Y , sample size s, and any distribution p. Let
(cid:114)
(cid:19)
(cid:18)
ˆXS = arg min
. Then, in expectation over the training sample S drawn
i.i.d. from the distribution p,
Lp ( ˆXS ) ≤ inf
Lp (X ) + O
X ∈Wr [ ˜p]

r max{n, m} log(n + m)
s

l ·

(5)

.

Note that although we regularize using the (smoothed) empirically-weighted trace-norm, we still
compare ourselves to the best possible matrix in the class deﬁned by the (smoothed) true marginals.
The proof of this Theorem (in the Supplementary Material) uses Theorem 3 and involves showing
that when s = Ω(n log(n)), which is required for all Theorems so far to be meaningful, the true and
empirical marginals are the same up to a constant factor. For this to be the case, such a sample size is
even necessary. In fact, the log(n) factor in our analysis (e.g. in the proof of Theorem 1) arises from
the bound on the expected spectral norm of a matrix, which, for a diagonal matrix, is just a bound
on the deviation of empirical frequencies. Might it be possible, then, to avoid this logarithmic factor
by using the empirical marginals? Although we could not establish such a result in the inductive
setting, we now turn to the transductive setting, where we could indeed obtain a better guarantee.

4.2 Guarantee for the transductive setting
In the transductive model, we ﬁx a set S ⊂ [n] × [m] of size 2s, and then randomly split S into a
training set S and a test set T of equal size s. The goal is to obtain a good estimator for the entries
in T based on the values of the entries in S , as well as the locations (indexes) of all elements on S .
We will use the smoothed empirical marginals of S , for the weighted trace-norm.
We now show that, for bounded loss, there may be a beneﬁt to weighting with the smoothed empir-
ical marginals—the sample size requirement can be lowered to s = O(rn log1/2 (n)).
Theorem 5. For an l-Lipschitz loss (cid:96) bounded by b, ﬁx any matrix Y and sample size s. Let
S ⊂ [n] × [m] be a ﬁxed subset of size 2s, split uniformly at random into training and test
(cid:110) ˆLS (X ) : X ∈ Wr [p]
(cid:111)
(cid:115)
sets S and T , each of size s. Let p denote the smoothed empirical marginals of S . Let ˆXS =
(cid:33)
(cid:32)
. Then in expectation over the splitting of S into S and T ,
arg min
rn log1/2 (n)
b√
l ·
ˆLT ( ˆXS ) ≤ inf
ˆLT (X ) + O
X ∈Wr [p]
s
s

(6)

+

.

This result (proved in the Supplementary Materials) is stated in the transductive setting, with a
somewhat different sampling procedure and evaluation criteria, but we believe the main difference is
in the use of the empirical weights. Although it is usually straightforward to convert a transductive
guarantee to an inductive one, the situation here is more complicated, since the hypothesis class
depends on the weighting, and hence on the sample S . Nevertheless, we believe such a conversion
might be possible, establishing a similar guarantee for learning with the (smoothed) empirically

7

for the true (non-empirical) weighting with a sample of size s = O(cid:0)n(r log1/2 (n) + log(n))(cid:1).
weighted trace-norm also in the inductive setting. Furthermore, since the empirical marginals are
close to the true marginals when s = Θ(n log(n)), it might be possible to obtain a learning guarantee
Theorem 5 can be viewed as a transductive analog to Theorem 3 (with weights based on the com-
bined sample S ). In the Supplementary Materials we give transductive analogs to Theorems 1 and 2.
As mentioned in Section 2.3, our lower bound examples can also be stated in the transductive setting,
and thus all our guarantees and lower bounds can also be obtained in this setting.

4.3 Simulations with empirical weights

In order to numerically investigate the possible advantage of empirical weighting, we performed
simulations on low-rank matrix reconstruction under uniform sampling with the unweighted, and the
smoothed empirically weighted, trace-norms. We choose to work with uniform sampling in order to
emphasize the beneﬁt of empirical weights, even in situations where one might not consider to use
any weights at all. In all the experiments, we attempt to reconstruct a possibly noisy, random rank-2
(n, n, 0, . . . , 0), ensuring (cid:107)M (cid:107)F = n. We measure error
“signal” matrix M with singular values 1√
2
using the squared loss2 . Simulations were performed using MATLAB, with code adapted from the
SO FT IM PUT E code developed by [20]. We performed two types of simulations:
ˆXS = arg min (cid:8) (cid:107)X (cid:107) : ˆLS (X ) = 0(cid:9), where (cid:107)X (cid:107) = (cid:107)X (cid:107)tr or = (cid:107)X (cid:107)tr( ˆpr , ˆpc ) , as appropriate.
Sample complexity comparison in the noiseless setting: We deﬁne Y = M , and compute
In Figure 1(a), we plot the average number of samples per row needed to get average squared error
(over 100 repetitions) of at most 0.1, with both uniform weighting and empirical weighting.
noise N has i.i.d. standard normal entries. We compute ˆXS = arg min (cid:8) (cid:107)X (cid:107) : ˆLS (X ) ≤ ν 2(cid:9).
Excess error comparison in the noiseless and noisy settings: We deﬁne Y = M + νN , where
In Figure 1(b), we plot the resulting average squared error (over 100 repetitions) over a range of
sample sizes s and noise levels ν , with both uniform weighting and empirical weighting. A larger
plot including standard error bars is shown in the Supplementary Materials.
The results from both experiments show a signiﬁcant beneﬁt to using the empirical marginals.

Figure 1: (a) Left: Sample size needed to obtain avg. error 0.1, with respect to n. (b) Right: Excess
error level over a range of sample sizes, for ﬁxed n = 200. (Axes are on a logarithmic scale.)

5 Discussion

In this paper, we prove learning guarantees for the weighted trace-norm by analyzing expected
Rademacher complexities. We show that weighting with smoothed marginals eliminates degenerate
scenarios that can arise in the case of a non-product sampling distribution, and demonstrate in exper-
iments on the Netﬂix and MovieLens datasets that this correction can be useful in applied settings.
We also give results for empirically-weighted trace-norm regularization, and see indications that
using the empirical distribution may be better than using the true distribution, even if it is available.

2Although Lipschitz in a bounded domain, it is probably possible to improve all our results (removing the
square root) for the special case of the squared-loss, possibly with an i.i.d. noise assumption, as in [10].

8

100141200282400910111213Matrix dimension ns/n = Avg. # samples per row  True pEmpirical p500100020000.10.20.40.8Sample size sAvg. squared error  ν=0.4, true pν=0.4, empirical pν=0.2, true pν=0.2, empirical pν=0.0, true pν=0.0, empirical pReferences
[1] M. Fazel. Matrix rank minimization with applications. PhD Thesis, Stanford University, 2002.
[2] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. Advances in
Neural Information Processing Systems, 17, 2004.
[3] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in Neural Infor-
mation Processing Systems, 20, 2007.
[4] F. Bach. Consistency of trace-norm minimization. Journal of Machine Learning Research,
9:1019–1048, 2008.
[5] E. Cand `es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE
Trans. Inform. Theory, 56(5):2053–2080, 2009.
[6] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. 18th Annual Conference on
Learning Theory (COLT), pages 545–560, 2005.
[7] B. Recht. A simpler approach to matrix completion. arXiv:0910.0651, 2009.
[8] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of
Machine Learning Research, 11:2057–2078, 2010.
[9] V. Koltchinskii, A. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal rates for
noisy low rank matrix completion. arXiv:1011.6256, 2010.
[10] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion:
Optimal bounds with noise. arXiv:1009.2118, 2010.
[11] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction.
24th Annual Conference on Learning Theory (COLT), 2011.
[12] R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning
with the Weighted Trace Norm. Advances in Neural Information Processing Systems, 23, 2010.
[13] O. Shamir and S. Shalev-Shwartz. Collaborative ﬁltering with the trace norm: Learning,
bounding, and transducing. 24th Annual Conference on Learning Theory (COLT), 2011.
[14] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and struc-
tural results. Journal of Machine Learning Research, 3:463–482, 2002.
[15] J.A. Tropp. User-friendly tail bounds for sums of random matrices. arXiv:1004.4389, 2010.
In Proceedings of KDD Cup and Workshop,
[16] J. Bennett and S. Lanning. The netﬂix prize.
volume 2007, page 35. Citeseer, 2007.
[17] MovieLens Dataset. Available at http://www.grouplens.org/node/73. 2006.
[18] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering
model. ACM Int. Conference on Knowledge Discovery and Data Mining (KDD’08), pages
426–434, 2008.
[19] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical Large-Scale Optimization
for Max-Norm Regularization. Advances in Neural Information Processing Systems, 23, 2010.
[20] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning
large incomplete matrices. Journal of Machine Learning Research, 11:2287–2322, 2010.

9


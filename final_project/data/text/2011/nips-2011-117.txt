Online Submodular Set Cover,
Ranking, and Repeated Active Learning

Andrew Guillory
Department of Computer Science
University of Washington
guillory@cs.washington.edu

Jeff Bilmes
Department of Electrical Engineering
University of Washington
bilmes@ee.washington.edu

Abstract

We propose an online prediction version of submodular set cover with connections
to ranking and repeated active learning. In each round, the learning algorithm
chooses a sequence of items. The algorithm then receives a monotone submodu-
lar function and suffers loss equal to the cover time of the function: the number of
items needed, when items are selected in order of the chosen sequence, to achieve
a coverage constraint. We develop an online learning algorithm whose loss con-
verges to approximately that of the best sequence in hindsight. Our proposed
algorithm is readily extended to a setting where multiple functions are revealed at
each round and to bandit and contextual bandit settings.

1 Problem

In an online ranking problem, at each round we choose an ordered list of items and then incur some
loss. Problems with this structure include search result ranking, ranking news articles, and ranking
advertisements. In search result ranking, each round corresponds to a search query and the items
correspond to search results. We consider online ranking problems in which the loss incurred at
each round is the number of items in the list needed to achieve some goal. For example, in search
result ranking a reasonable loss is the number of results the user needs to view before they ﬁnd the
complete information they need. We are speciﬁcally interested in problems where the list of items is
a sequence of questions to ask or tests to perform in order to learn. In this case the ranking problem
becomes a repeated active learning problem. For example, consider a medical diagnosis problem
where at each round we choose a sequence of medical tests to perform on a patient with an unknown
illness. The loss is the number of tests we need to perform in order to make a conﬁdent diagnosis.
We propose an approach to these problems using a new online version of submodular set cover.
A set function F (S ) deﬁned over a ground set V is called submodular if it satisﬁes the following
diminishing returns property: for every A ⊆ B ⊆ V \ {v}, F (A + v) − F (A) ≥ F (B + v) − F (B ).
Many natural objectives measuring information, inﬂuence, and coverage turn out to be submodular
[1, 2, 3]. A set function is called monotone if for every A ⊆ B , F (A) ≤ F (B ) and normalized if
F (∅) = 0. Submodular set cover is the problem of selecting an S ⊆ V minimizing |S | under the
constraint that F (S ) ≥ 1 where F is submodular, monotone, and normalized (note we can always
rescale F ). This problem is NP-hard, but a greedy algorithm gives a solution with cost less than
1 + ln 1/ that of the optimal solution where  is the smallest non-zero gain of F [4].
We propose the following online prediction version of submodular set cover, which we simply call
online submodular set cover. At each time step t = 1 . . . T we choose a sequence of elements
n ) where each v t
i is chosen from a ground set V of size n (we use a superscript
S t = (v t
2 , . . . v t
1 , v t
(cid:96)(F t , S t ) (cid:44) min(cid:0){n} ∪ {i : F t (S t
(cid:1)
for rounds of the online problem and a subscript for other indices). After choosing S t , an adversary
reveals a submodular, monotone, normalized function F t , and we suffer loss (cid:96)(F t , S t ) where
i ) ≥ 1}i
1

(1)

(cid:44) (cid:83)
(cid:96) can be equivalently written (cid:96)(F t , S t ) (cid:44) (cid:80)n
j } is deﬁned to be the set containing the ﬁrst i elements of S t (let S t
(cid:44) ∅). Note
j≤i {v t
and S t
0
i
i ) < 1) where I is the indicator function.
i=0 I (F t (S t
Intuitively, (cid:96)(F t , S t ) corresponds to a bounded version of cover time: it is the number of items up to
n needed to achieve F t (S ) ≥ 1 when we select items in the order speciﬁed by S t . Thus, if coverage
oblivious adversary). The goal of our learning algorithm is to minimize the total loss (cid:80)
is not achieved, we suffer a loss of n. We assume that F t (V ) ≥ 1 (therefore coverage is achieved if
S t does not contain duplicates) and that the sequence of functions (F t )t is chosen in advance (by an
t (cid:96)(F t , S t ).
To make the problem clear, we present it ﬁrst in its simplest, full information version. However, we
will later consider more complex variations including (1) a version where we only produce a list of
length k ≤ n instead of n, (2) a multiple objective version where a set of objectives F t
2 , . . . F t
1 , F t
m
is revealed each round, (3) a bandit (partial information) version where we do not get full access to
F t and instead only observe F t (S t
n ), and (4) a contextual bandit version where
1 ), F t (S t
2 ), . . . F t (S t
there is some context associated with each round.
We argue that online submodular set cover, as we have deﬁned it, is an interesting and useful model
for ranking and repeated active learning problems. In a search result ranking problem, after present-
ing search results to a user we can obtain implicit feedback from this user (e.g., clicks, time spent
viewing each result) to determine which results were actually relevant. We can then construct an
objective F t (S ) such that F t (S ) ≥ 1 iff S covers or summarizes the relevant results. Alternatively,
we can avoid explicitly constructing an objective by considering the bandit version of the problem
where we only observe the values F t (S t
i ). For example, if the user clicked on k total results then
i ) (cid:44) ci /k where ci ≤ k is the number of results in the subset Si which were clicked.
we can let F (S t
Note that the user may click an arbitrary set of results in an arbitrary order, and the user’s decision
whether or not to click a result may depend on previously viewed and clicked results. All that we
assume is that there is some unknown submodular function explaining the click counts. If the user
clicks on a small number of very early results, then coverage is achieved quickly and the ordering is
desirable. This coverage objective makes sense if we assume that the set of results the user clicked
are of roughly equal importance and together summarize the results of interest to the user.
In the medical diagnosis application, we can deﬁne F t (S ) to be proportional to the number of
candidate diseases which are eliminated after performing the set of tests S on patient t. If we assume
that a particular test result always eliminates a ﬁxed set of candidate diseases, then this function is
submodular. Speciﬁcally, this objective is the reduction in the size of the version space [5, 6]. Other
active learning problems can also be phrased in terms of satisfying a submodular coverage constraint
including problems that allow for noise [7]. Note that, as in the search result ranking problem, F t is
not initially known but can be inferred after we have chosen S t and suffered loss (cid:96)(F t , S t ).

2 Background and Related Work

Recently, Azar and Gamzu [8] extended the O(ln 1/) greedy approximation algorithm for sub-
modular set cover to the more general problem of minimizing the average cover time of a set of
objectives. Here  is the smallest non-zero gain of all the objectives. Azar and Gamzu [8] call this
the ground set V to minimize (cid:80)m
problem ranking with submodular valuations. More formally, we have a known set of functions
F1 , F2 , . . . , Fm each with an associated weight wi . The goal is then to choose a permutation S of
i=1 wi (cid:96)(Fi , S ). The ofﬂine approximation algorithm for ranking
with submodular valuations will be a crucial tool in our analysis of online submodular set cover.
In particular, this ofﬂine algorithm can viewed as constructing the best single permutation S for a
sequence of objectives F 1 , F 2 . . . F T in hindsight (i.e., after all the objectives are known). Recently
the ranking with submodular valuations problem was extended to metric costs [9].
Online learning is a well-studied problem [10]. In one standard setting, the online learning algorithm
has a collection of actions A, and at each time step t the algorithm picks an action S t ∈ A. The
learning algorithm then receives a loss function (cid:96)t , and the algorithm incurs the loss value for the
action it chose (cid:96)t (S t ). We assume (cid:96)t (S t ) ∈ [0, 1] but make no other assumptions about the form
in hindsight: R = (cid:80)T
t=1 (cid:96)t (S t ) − minS∈A (cid:80)T
of loss. The performance of an online learning algorithm is often measured in terms of regret, the
guarantee E[R] ≤ (cid:112)T ln |A| for adversarial sequences of loss functions [11]. Note that because
difference between the loss incurred by the algorithm and the loss of the best single ﬁxed action
t=1 (cid:96)t (S ). There are randomized algorithms which
2

E[R] = o(T ) the per round regret approaches zero. In the bandit version of this problem the learning
algorithm only observes (cid:96)t (S t ) [12].
Our problem ﬁts in this standard setting with A chosen to be the set of all ground set permutations
(v1 , v2 , . . . vn ) and (cid:96)t (S t ) (cid:44) (cid:96)(F t , S t )/n. However, in this case A is very large so standard online
learning algorithms which keep weight vectors of size |A| cannot be directly applied. Furthermore,
our problem generalizes an NP-hard ofﬂine problem which has no polynomial time approximation
scheme, so it is not likely that we will be able to derive any efﬁcient algorithm with o(T ln |A|)
t=1 (cid:96)t (S t ) − α minS∈A (cid:80)T
times the best ﬁxed prediction. Rα = (cid:80)T
regret. We therefore instead consider α-regret, the loss incurred by the algorithm as compared to α
t=1 (cid:96)t (S ). α-regret is a standard
notion of regret for online versions of NP-hard problems. If we can show Rα grows sub linearly
with T then we have shown loss converges to that of an ofﬂine approximation with ratio α.
Streeter and Golovin [13] give online algorithms for the closely related problems of submodular
to maximize (cid:80)
function maximization and min-sum submodular set cover. In online submodular function maxi-
mization, the learning algorithm selects a set S t with |S t | ≤ k before F t is revealed, and the goal is
t F t (S t ). This problem differs from ours in that our problem is a loss minimization
problem as opposed to an objective maximization problem. Online min-sum submodular set cover
ˆ(cid:96)(F t , S t ) (cid:44) n(cid:88)
is similar to online submodular set cover except the loss is not cover time but rather
max(1 − F t (S t
i ), 0).
i=0
repeated active learning problems minimizing (cid:80)
Min-sum submodular set cover penalizes 1 − F t (S t
i ) where submodular set cover uses I (F t (S t
i ) <
the number of questions asked. Minimizing (cid:80)
1). We claim that for certain applications the hard threshold makes more sense. For example, in
t (cid:96)(F t , S t ) naturally corresponds to minimizing
ˆ(cid:96)(F t , S t ) does not have this interpretation as it
t
charges less for questions asked when F t is closer to 1. One might hope that minimizing (cid:96) could
be reduced to or shown equivalent to minimizing ˆ(cid:96). This is not likely to be the case, as the ap-
mizes (cid:80)
proximation algorithm of Streeter and Golovin [13] does not carry over to online submodular set
cover. Their online algorithm is based on approximating an ofﬂine algorithm which greedily maxi-
t min(F t (S ), 1). Azar and Gamzu [8] show that this ofﬂine algorithm, which they call the
cumulative greedy algorithm, does not achieve a good approximation ratio for average cover time.
Radlinski et al. [14] consider a special case of online submodular function maximization applied
to search result ranking. In their problem the objective function is assumed to be a binary valued
submodular function with 1 indicating the user clicked on at least one document. The goal is then
to maximize the number of queries which receive at least one click. For binary valued functions
ˆ(cid:96) and (cid:96) are the same, so in this setting minimizing the number of documents a user must view
before clicking on a result is a min-sum submodular set cover problem. Our results generalize this
problem to minimizing the number of documents a user must view before some possibly non-binary
submodular objective is met. With non-binary objectives we can incorporate richer implicit feedback
such as multiple clicks and time spent viewing results. Slivkins et al. [15] generalize the results of
Radlinski et al. [14] to a metric space bandit setting.
Our work differs from the online set cover problem of Alon et al. [16]; this problem is a single
set cover problem in which the items that need to be covered are revealed one at a time. Kakade
et al. [17] analyze general online optimization problems with linear loss. If we assume that the
functions F t are all taken from a known ﬁnite set of functions F then we have linear loss over a |F |
dimensional space. However, this approach gives poor dependence on |F |.

(2)

3 Ofﬂine Analysis

In this work we present an algorithm for online submodular set cover which extends the ofﬂine
algorithm of Azar and Gamzu [8] for the ranking with submodular valuations problem. Algorithm 1
shows this ofﬂine algorithm, called the adaptive residual updates algorithm. Here we use T to denote
the number of objective functions and superscript t to index the set of objectives. This notation is
chosen to make the connection to the proceeding online algorithm clear: our online algorithm will
approximately implement Algorithm 1 in an online setting, and in this case the set of objectives in

3

Algorithm 1 Ofﬂine Adaptive Residual
Input: Objectives F 1 , F 2 , . . . F T
Output: Sequence S1 ⊂ S2 ⊂ . . . Sn
(cid:80)
S0 ← ∅
for i ← 1 . . . n do
v ← argmax
t δ(F t , Si−1 , v)
v∈V
Si ← Si−1 + v
end for

Figure 1: Histograms used in ofﬂine analysis

δ(F t , S, v) (cid:44)

, 1)

the ofﬂine algorithm will be the sequence of objectives in the online problem. The algorithm is a
(cid:40)
greedy algorithm similar to the standard algorithm for submodular set cover. The crucial difference
is that instead of a normal gain term of F t (S + v) − F t (S ) it uses a relative gain term
min( F t (S+v)−F t (S )
if F (S ) < 1
1−F t (S )
otherwise
0
The intuition is that (1) a small gain for F t matters more if F t is close to being covered (F t (S ) close
to 1) and (2) gains for F t with F t (S ) ≥ 1 do not matter as these functions are already covered. The
Theorem 1 ([8]). The loss (cid:80)
main result of Azar and Gamzu [8] is that Algorithm 1 is approximately optimal.
t (cid:96)(F t , S ) of the sequence produced by Algorithm 1 is within
4(ln(1/) + 2) of that of any other sequence.
We note Azar and Gamzu [8] allow for weights for each F t . We omit weights for simplicity. Also,
Azar and Gamzu [8] do not allow the sequence S to contain duplicates while we do–selecting a
ground set element twice has no beneﬁt but allowing them will be convenient for the online analy-
sis. The proof of Theorem 1 involves representing solutions to the submodular ranking problem as
histograms. Each histogram is deﬁned such that the area of the histogram is equal to the loss of the
corresponding solution. The approximate optimality of Algorithm 1 is shown by proving that the
histogram for the solution it ﬁnds is approximately contained within the histogram for the optimal
solution. In order to convert Algorithm 1 into an online algorithm, we will need a stronger version of
(cid:80)
Theorem 1. Speciﬁcally, we will need to show that when there is some additive error in the greedy
selection rule Algorithm 1 is still approximately optimal.
For the optimal solution S ∗ = argminS∈V n
t (cid:96)(F t , S ) (V n is the set of all length n sequences of
ground set elements), deﬁne a histogram h∗ with T columns, one for each function F t . Let the tth
column have with width 1 and height equal to (cid:96)(F t , S ∗ ). Assume that the columns are ordered by
increasing cover time so that the histogram is monotone non-decreasing. Note that the area of this
histogram is exactly the loss of S ∗ .
For a sequence of sets ∅ = S0 ⊆ S1 ⊆ . . . Sn (e.g., those found by Algorithm 1) deﬁne a corre-
(cid:40)
sponding sequence of truncated objectives
min( F t (S∪Si−1 )−F t (Si−1 )
i (S ) (cid:44)
1−F t (Si−1 )
ˆF t
1

if F t (Si−1 ) < 1
otherwise

, 1)

ˆF t
i (S ) is essentially F t except with (1) Si−1 given “for free”, and (2) values rescaled to range
i is submodular and that if F t (S ) ≥ 1 then ˆF t
i (S ) ≥ 1. In this
between 0 and 1. We note that ˆF t
i ({v}) − ˆF t
i (∅) = δ(F t , Si−1 , v). In
sense ˆF t
i is an easier objective than F t . Also, for any v , ˆF t
i at ∅ is the normalized gain of F t at Si−1 . This property will be crucial.
other words, the gain of ˆF t
We next deﬁne truncated versions of h∗ : ˆh1 , ˆh2 , . . . ˆhn which correspond to the loss of S ∗ for the
i . For each j ∈ 1 . . . n, let ˆhi have T columns of height j
easier covering problems involving ˆF t
j ) − ˆF t
i (S ∗
i (S ∗
with the tth such column of width ˆF t
j−1 ) (some of these columns may have 0 width).
Assume again the columns are ordered by height. Figure 1 shows h∗ and ˆhi .
n ) ≥ 1 for every t (clearly some choice of S ∗
We assume without loss of generality that F t (S ∗
contains no duplicates, so under our assumption that F t (V ) ≥ 1 we also have F t (S ∗
n ) ≥ 1). Note

4

(cid:80)
that the total width of ˆhi is then the number of functions remaining to be covered after Si−1 is given
for free (i.e., the number of F t with F t (Si−1 ) < 1). It is not hard to see that the total area of ˆhi is
i , S ∗ ) where ˆl is the loss function for min-sum submodular set cover (2). From this we know
ˆ(cid:96)( ˆF t
t
ˆhi has area less than h∗ . In fact, Azar and Gamzu [8] show the following.
Lemma 1 ([8]). ˆhi is completely contained within h∗ when ˆhi and h∗ are aligned along their lower
right boundaries.
Qi = (cid:80)
(cid:80)n
We need one ﬁnal lemma before proving the main result of this section. For a sequence S deﬁne
t δ(F t , Si−1 , vi ) to be the total normalized gain of the ith selected element and let ∆i =
j=i Qj be the sum of the normalized gains from i to n. Deﬁne Πi = |{t : F t (Si−1 ) < 1}| to be
the number of functions which are still uncovered before vi is selected (i.e., the loss incurred at step
i). [8] show the following result relating ∆i to Πi .
Lemma 2 ([8]). For any i, ∆i ≤ (ln 1/ + 2)Πi
We now state and prove the main result of this section, that Algorithm 1 is approximately optimal
even when the ith greedy selection is preformed with some additive error Ri . This theorem shows
that in order to achieve low average cover time it sufﬁces to approximately implement Algorithm 1.
Aside from being useful for converting Algorithm 1 into an online algorithm, this theorem may be
useful for applications in which the ground set V is very large. In these situations it may be possible
to approximate Algorithm 1 (e.g., through sampling). Streeter and Golovin [13] prove similar results
for submodular function maximization and min-sum submodular set cover. Our result is similar, but
the proof is non trivial. The loss function (cid:96) is highly non linear with respect to changes in F t (S t
i ),
so it is conceivable that small additive errors in the greedy selection could have a large effect. The
analysis of Im and Nagarajan [9] involves a version of Algorithm 1 which is robust to a sort of
multplicative error in each stage of the greedy selection.
(cid:88)
(cid:88)
Theorem 2. Let S = (v1 , v2 , . . . vn ) be any sequence for which
δ(F t , Si−1 , vi ) + Ri ≥ max
Then (cid:80)
t (cid:96)(F t , S t ) ≤ 4(ln 1/ + 2) (cid:80)
t (cid:96)(F t , S ∗ ) + n (cid:80)
δ(F t , Si−1 , v)
v∈V
t
t
i Ri
ith column have width (Qi + Ri )/(2γ ) and height max(Πi − (cid:80)
Proof. Let h be a histogram with a column for each Πi with Πi (cid:54)= 0. Let γ = (ln 1/ + 2). Let the
j Rj , 0)/(2(Qi + Ri )). Note that
Πi (cid:54)= 0 iff Qi + Ri (cid:54)= 0 as if there are functions not yet covered then there is some set element with
max(Πi − (cid:80)
(cid:88)
(cid:88)
(cid:88)
non zero gain (and vice versa). The area of h is
j Rj , 0)
2(Qi + Ri )
i:Πi (cid:54)=0
t
j
Assume h and every ˆhi are aligned along their lower right boundaries. We show that if the ith
Consider the ith column in h. Assume this column has non zero area so Πi ≥ (cid:80)
column of h has non-zero area then it is contained within ˆhi . Then, it follows from Lemma 1 that h
is contained within h∗ , completing the proof.
is at most (∆i + (cid:80)
ˆhi it sufﬁces to show that after selecting the ﬁrst k = (cid:98)(Πi − (cid:80)
j Rj . This column
j≥i Rj )/(2γ ) away from the right hand boundary. To show that this column is in
j≥i Rj )/(2γ ) . The most that (cid:80)
still have (cid:80)
k )) ≥ (∆i + (cid:80)
j Rj )/(2(Qi + Ri ))(cid:99) items in S ∗ we
t (1 − ˆF t
i (S ∗
ˆF t
i can increase through
i (∅) ≤ k(Qi + Ri ) ≤ Πi /2 − (cid:88)
k ) − (cid:88)
(cid:88)
t
the addition of one item is Qi + Ri . Therefore, using the submodularity of ˆF t
i ,
i (S ∗
ˆF t
ˆF t
Therefore (cid:80)
k )) ≥ Πi/2 + (cid:80)
j Rj /2 since (cid:80)
Rj /2
t
t
j
(cid:88)
(cid:88)
(cid:88)
t (1 − ˆF t
t (1 − ˆF t
i (∅)) = Πi . Using Lemma 2
i (S ∗
Rj /2 ≥ (∆i +
Rj /2 ≥ ∆i/(2γ ) +
Rj )/(2γ )
Πi /2 +
j≥i
j
j

(cid:96)(F t , S ) − n
4γ

≥ 1
4γ

1
2γ

(Qi + Ri )

Rj

5

Algorithm 2 Online Adaptive Residual
Input: Integer T
Initialize n online learning algorithms
E1 , E2 , . . . En with A = V
for t = 1 → T do
∀i ∈ 1 . . . n predict v t
i with Ei
S t ← (v t
1 , . . . v t
n )
Receive F t , pay loss l(F t , S t )
For Ei , (cid:96)t (v) ← (1 − δ(F t , S t
i−1 , v))
end for

4 Online Analysis

Figure 2: Ei selects the ith element in S t .

We now show how to convert Algorithm 1 into an online algorithm. We use the same idea used by
Streeter and Golovin [13] and Radlinski et al. [14] for online submodular function maximization: we
run n copies of some low regret online learning algorithm, E1 , E2 , . . . En , each with action space
A = V . We use the ith copy Ei to select the ith item in each predicted sequence S t . In other
words, the predictions of Ei will be v1
i . Figure 2 illustrates this. Our algorithm assigns
i , v2
i , . . . vT
loss values to each Ei so that, assuming Ei has low regret, Ei approximately implements the ith
greedy selection in Algorithm 1. Algorithm 2 shows this approach. Note that under our assumption
that F 1 , F 2 , . . . F T is chosen by an oblivious adversary, the loss values for the ith copy of the online
algorithm are oblivious to the predictions of that run of the algorithm. Therefore we can use any
algorithm for learning against an oblivious adversary.
E[R] ≤ √
√
Theorem 3. Assume we use as a subroutine an online prediction algorithm with expected regret
T ln n. Algorithm 2 has expected α-regret E[Rα ] ≤ n2
T ln n for α = 4(ln(1/) + 2)

Ri

T ln n

max
v∈V

Proof. Deﬁne a meta-action ˜vi for the sequence of actions chosen by Ei , ˜vi = (v1
i ). We
i , v2
i , . . . vT
can extend the domain of F t to allow for meta-actions F t (S ∪ {ˆvi}) = F t (S ∪ {v t
i }). Let ˜S be
the sequence of meta actions ˜S = ( ˜v1 , ˜v2 , . . . ˜vn ). Let Ri be the regret of Ei . Note that from the
(cid:88)
δ(F t , ˜Si−1 , v) − (cid:88)
deﬁnition of regret and our choice of loss values we have that
δ(F t , ˜Si−1 , ˜vi ) = Ri
t
t
Therefore, ˜S approximates the greedy solution in the sense required by Theorem 2. Theorem 2 did
(cid:88)
(cid:88)
(cid:88)
(cid:88)
not require that S be constructed V . From Theorem 2 we then have
(cid:96)(F t , ˜S ) ≤ α
(cid:96)(F t , S ∗ ) + n
(cid:96)(F t , S t ) =
The expected α-regret is then E[n (cid:80)
t
t
t
i
√
i Ri ] ≤ n2
We describe several variations and extensions of this analysis, some of which mirror those for related
work [13, 14, 15].
Avoiding Duplicate Items Since each run of the online prediction algorithm is independent, Algo-
rithm 2 may select the same ground set element multiple times. This drawback is easy to ﬁx. We
can simply select any arbitrary vi /∈ Si−1 if Ei selects a vi ∈ Si−i . This modiﬁcation does not affect
the regret guarantee as selecting a vi ∈ Si−1 will always result in a gain of zero (loss of 1).
Truncated Loss In some applications we only care about the ﬁrst k items in the sequence S t . For
(cid:96)k (F t , S t ) (cid:44) min(cid:0){k} ∪ {|S t
i ) ≥ 1}(cid:1)
these applications it makes sense to consider a truncated version of l(F t , S t ) with parameter k
i | : F t (S t
t (cid:96)k (F t , S t ) ≤ 4(ln 1/ + 2) (cid:80)
t (cid:96)(F t , S ∗ ) + k (cid:80)k
(cid:80)
This is cover time computed up to the k th element in S t . The analysis for Theorem 2 also shows
i=1 Ri . The corresponding regret bound is then
6

T ln n. Note here we are bounding truncated loss (cid:80)
(cid:80)
√
t (cid:96)k (F t , S t ) in terms of untruncated loss
k2
t (cid:96)(F t , S ∗ ). In this sense this bound is weaker. However, we replace n2 with k2 which may be
much smaller. Algorithm 2 achieves this bound simultaneously for all k .
and incur loss (cid:80)m
Multiple Objectives per Round Consider a variation of online submodular set cover in which in-
stead of receiving a single objective F t each round we receive a batch of objectives F t
2 , . . . F t
1 , F t
(1/m) (cid:80)m
m
i , S t ). In other words, each rounds corresponds to a ranking with sub-
i=1 (cid:96)(F t
It is easy to extend Algorithm 2 to this setting by using 1 −
total regret where L∗ = (cid:80)T
(cid:80)m
√
modular valuations problem.
mL∗ ln n + k2m ln n)
i−1 , v) for the loss of action v in Ei . We then get O(k2
i=1 δ(F t
i , S t
i , S ∗ ) (Section 2.6 of [10]).
i=1 (cid:96)(F t
t=1
Bandit Setting Consider a setting where instead of receiving full access to F t we only observe
the sequence of objective function values F t (S t
n ) (or in the case of multiple
1 ), F t (S t
2 ), . . . F t (S t
objectives per round, F t
j ) for every i and j ). We can extend Algorithm 2 to this setting using a
i (S t
nonstochastic multiarmed bandits algorithm [12]. We note duplicate removal becomes more subtle
in the bandit setting: should we feedback a gain of zero when a duplicate is selected or the gain of
the non-duplicate replacement? We propose either is valid if replacements are chosen obliviously.
Bandit Setting with Expert Advice We can further generalize the bandit setting to the contextual
bandit setting [18] (e.g., the bandit setting with expert advice [12]). Say that we have access at time
step t to predictions from a set of m experts. Let ˜vj be the meta action corresponding to the sequence
with respect to ˜V (cid:88)
(cid:88)
of predictions from the j th expert and ˜V be the set of all ˜vj . Assume that Ei guarantees low regret
i ) + Ri ≥ max
δ(F t , S t
i−1 , v t
δ(F t , S t
i−1 , ˜v)
3. Additionally assume that F t ( ˜V ) ≥ 1 for every t. In this case we can show (cid:80)
˜v∈ ˜V
t
t
(cid:80)
t (cid:96)m (F t , S ∗ ) + k (cid:80)k
where we have extended the domain of each F t to include meta actions as in the proof of Theorem
t (cid:96)k (F t , S t ) ≤
√
√
i=1 Ri . The Exp4 algorithm [12] has Ri = O(
nT ln m) giving
minS ∗∈ ˜V m
total regret O(k2
nT ln m). Experts may use context in forming recommendations. For example,
in a search ranking problem the context could be the query.

(3)

5 Experimental Results

5.1 Synthetic Example

We present a synthetic example for which the online cumulative greedy algorithm [13] fails, based
on the example in Azar and Gamzu [8] for the ofﬂine setting. Consider an online ad placement
problem where the ground set V is a set of available ad placement actions (e.g., a v ∈ V could
correspond to placing an ad on a particular web page for a particular length of time). On round
t, we receive an ad from an advertiser, and our goal is to acquire λ clicks for the ad using as few
advertising actions as possible. Deﬁne F t (S t
i , λ)/λ where ct
i ) to be min(ct
i is number of clicks
acquired from the ad placement actions S t
i .
Say that we have n advertising actions of two types: 2 broad actions and n − 2 narrow actions. Say
that the ads we receive are also of two types. Common type ads occur with probability (n − 1)/n
and receive 1 and λ − 1 clicks respectively from the two broad actions and 0 clicks from narrow
actions. Uncommon type ads occur with probability 1/n and receive λ clicks from one randomly
chosen narrow action and 0 clicks from all other actions. Assume λ ≥ n2 . Intuitively broad actions
could correspond to ad placements on sites for which many ads are relevant. The optimal strategy
giving an average cover time O(1) is to ﬁrst select the two broad actions covering all common ads
then select the narrow actions in any order. However, the ofﬂine cumulative greedy algorithm will
pick all narrow actions before picking the broad action with gain 1 giving average cover time O(n).
The left of Figure 3 shows average cover time for our proposed algorithm and the cumulative greedy
algorithm of [13] on the same sequences of random objectives. For this example we use n = 25
and the bandit version of the problem with the Exp3 algorithm [12]. We also plot the average cover
times for ofﬂine solutions as baselines. As seen in the ﬁgure, the cumulative algorithms converge to
higher average cover times than the adaptive residual algorithms. Interestingly, the online cumulative
algorithm does better than the ofﬂine cumulative algorithm: it seems added randomization helps.

7

Figure 3: Average cover time

5.2 Repeated Active Learning for Movie Recommendation

Consider a movie recommendation website which asks users a sequence of questions before they are
given recommendations. We deﬁne an online submodular set cover problem for choosing sequences
of questions in order to quickly eliminate a large number of movies from consideration. This is
similar conceptually to the diagnosis problem discussed in the introduction. Deﬁne the ground set
V to be a set of questions (for example “Do you want to watch something released in the past
10 years?” or “Do you want to watch something from the Drama genre?”). Deﬁne F t (S ) to be
proportional to the number of movies eliminated from consideration after asking the tth user S .
Speciﬁcally, let H be the set of all movies in our database and V t (S ) be the subset of movies
consistent with the tth user’s responses to S . Deﬁne F t (S ) (cid:44) min(|H \ V t (S )|/c, 1) where c is a
constant. F t (S ) ≥ iff after asking the set of questions S we have eliminated at least c movies.
We set H to be a set of 11634 movies available on Netﬂix’s Watch Instantly service and use 803
questions based on those we used for an ofﬂine problem [7]. To simulate user responses to questions,
on round t we randomly select a movie from H and assume the tth user answers questions consis-
tently with this movie. We set c = |H | − 500 so the goal is to eliminate about 95% of all movies.
We evaluate in the full information setting: this makes sense if we assume we receive as feedback
the movie the user actually selected. As our online prediction subroutine we tried Normal-Hedge
[19], a second order multiplicative weights method [20], and a version of multiplicative weights for
small gains using the doubling trick (Section 2.6 of [10]). We also tried a heuristic modiﬁcation of
Normal-Hedge which ﬁxes ct = 1 for a ﬁxed, more aggressive learning rate than theoretically justi-
ﬁed. The right of Figure 3 shows average cover time for 100 runs of T = 10000 iterations. Note the
different scale in the bottom row–these methods performed signiﬁcantly worse than Normal-Hedge.
The online cumulative greedy algorithm converges to a average cover time close to but slightly
worse than that of the adaptive greedy method. The differences are more dramatic for prediction
subroutines that converge slowly. The modiﬁed Normal-Hedge has no theoretical justiﬁcation, so it
may not generalize to other problems. For the modiﬁed Normal-Hedge the ﬁnal average cover times
are 7.72 adaptive and 8.22 cumulative. The ofﬂine values are 6.78 and 7.15.

6 Open Problems

It is not yet clear what practical value our proposed approach will have for web search result ranking.
A drawback to our approach is that we pick a ﬁxed order in which to ask questions. For some
problems it makes more sense to consider adaptive strategies [5, 6].

Acknowledgments

This material is based upon work supported in part by the National Science Foundation under grant
IIS-0535100, by an Intel research award, a Microsoft research award, and a Google research award.

8

References
[1] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In HLT,
2011.
[2] D. Kempe, J. Kleinberg, and ´E. Tardos. Maximizing the spread of inﬂuence through a social
network. In KDD, 2003.

[3] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes:
Theory, efﬁcient algorithms and empirical studies. JMLR, 2008.

[4] L.A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.
Combinatorica, 2(4), 1982.

[5] D. Golovin and A. Krause. Adaptive submodularity: A new approach to active learning and
stochastic optimization. In COLT, 2010.

[6] Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. In ICML, 2010.

[7] Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise.
In ICML, 2011.

[8] Yossi Azar and Iftah Gamzu. Ranking with Submodular Valuations. In SODA, 2011.

[9] S. Im and V. Nagarajan. Minimum Latency Submodular Cover in Metrics. ArXiv e-prints,
October 2011.

[10] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,
2006.

[11] Y. Freund and R. Schapire. A desicion-theoretic generalization of on-line learning and an
application to boosting. In Computational learning theory, pages 23–37, 1995.

[12] P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal on Computing, 32(1):48–77, 2003.

[13] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In
NIPS, 2008.

[14] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed
bandits. In ICML, 2008.

[15] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning optimally diverse rankings over large
document collections. In ICML, 2010.

[16] N. Alon, B. Awerbuch, and Y. Azar. The online set cover problem. In STOC, 2003.

[17] Sham M. Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation
algorithms. In STOC, 2007.

[18] J. Langford and T. Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In
NIPS, 2007.

[19] K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In NIPS, 2009.

[20] N. Cesa-Bianchi, Y. Mansour, and G. Stoltz.
with expert advice. Machine Learning, 2007.

Improved second-order bounds for prediction

9


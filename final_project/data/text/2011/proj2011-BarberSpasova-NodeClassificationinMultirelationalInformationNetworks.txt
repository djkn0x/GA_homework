Node Classiﬁcation in Multirelational Information Networks

Rick Barber, Mirela Spasova

December 15, 2011

1 Problem Background

Our goal will be to predict the types of entities in a large multirelational information network. In
particular, if we suppose we are given a graph G = (V , E ), a hierarchy of types T , and an observed
labeling function ˆl which for each node gives a path beginning at the root of T , called the type(s)
of the node. Our goal will be to learn l the true labeling function of the nodes

2 Data

Wikipedia contains a large amount of unstructured information. Infoboxes, however, are a mech-
anism used by the site to package many prominent features of a given Wikipedia entity into a
structured summary–see below.

1

Rick Barber, Mirela Spasova

Pro ject Milestone

CS 229

The DBpedia dataset is a post-processed version of Wikipedia’s infoboxes, and it will be the data
set in our study. DBpedia contains more than 3.6 million entities among which there are almost
8.5 millions directed relations.

DBpedia includes a type hierarchy of 237 types and gives a set of type labels for most nodes in
the network. However, a cursory look at the dataset will reveal that the set of types if often far
underspeciﬁed (Obama is labeled as a Thing, Person but not a Politician or President for instance)
and DBpedia’s own website claims that they have over a million entities which aren’t classiﬁed
beyond ”Thing” which is the root level of the ontology.

3 Method

3.1 Data generation and model

Some of the parsing machinery was written in joint work with Klemen Simonic and Rick Barber
advised by Jure Leskovec in August of this year.

DBpedia’s data is available in raw form in three ﬁles: the ontology ﬁle, the properties ﬁle, and
the types ﬁle.

The ontology ﬁle is an XML ﬁle describing the hierarchy of DBpedia types. The root node is

2

Rick Barber, Mirela Spasova

Pro ject Milestone

CS 229

Thing since all entities in the dataset are assumed to be Things. We parse the XML, and store
the data as a tree structure on which a number of queries can be eﬃciently performed.

The properties ﬁle is a list of relations represented as triples (s, p, o) where s is an infobox entity
that we call the sub ject, p is the name of a relation we call a predicate, and o is either a literal
value or another entity which we call the ob ject of the relation.

An example is given below

<http://dbpedia.org/resource/Aristotle> <http://dbpedia.org/ontology/influenced>
<http://dbpedia.org/resource/Ptolemy> .

We represent this data as an actual graph. For example, with the above line we will ensure that
there is a node corresponding to Aristotle and Ptolemy in our graph and we will add an edge with
label ”inﬂuenced” between these two nodes.

The types ﬁle is a list of triples (e, n, t) where e is the name of an entity and t is a type of the
entity and n is a constant we ignore.

An example is given below

<http://dbpedia.org/resource/Autism>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://dbpedia.org/ontology/Disease> .
<http://dbpedia.org/resource/Autism>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://www.w3.org/2002/07/owl#Thing> .

This says that the DBpedia labeling function assigns labels Disease and Thing to the entity
Autism. Of course, we can’t be certain that the true, onobserved labeling function labels Autism
in exactly the same way, but we will try to use the signal of the observed labeling function to
learn the true labeling function, which we will detail later. We parse this ﬁle and store the type
information in dictionaries to query later.

3.2 Classiﬁers and training/test data

For our ﬁrst task, we have trained binary one vs all classiﬁers for various entity types and for
various subsets of the data.

We use logistic regression throughout (the L 2 regularized version for liblinear, to be exact) in
order to easily intepret the results of our prediction probabilistically, which will be important in
the ﬁnal classiﬁer.

3

Rick Barber, Mirela Spasova

Pro ject Milestone

CS 229

For instance, DBpedia has a Comedian type which corresponds to entities in the dataset who are
comedians; this will be our running example.

3.2.1 Protocol 1: uniform sampling of negative examples

Here we randomly sample m + p comedians and m + p non comedians from anywhere in the type
hierarchy and retain the labels of m for training purposes while hiding the labels for p for testing
purposes.

We used this protocol as a proof of concept to get us started and it served to show that we could
diﬀerentiate types on DBpedia.

3.2.2 Protocol 2: sampling from neighbor types

Here we sample positive examples as before, but for negative examples, we from types which are
sibling types of Comedian in the hierarchy. We follow this procedure because our ﬁnal, holistic
classiﬁer will need to be able to diﬀerentiate between neighbor types.

3.2.3 The holistic classiﬁer

We trained a classiﬁer for each of the 237 types following protocol 2 above.

Our procedure for making a prediction on a new input x is as follows:
1 p r e d i c t ( x , t y p e t r e e )
2
l a s t P r e d i c t = T h i n g
3
c a n d i d a t e s = T h i n g . c h i l d r e n
4
t y p e . a d d ( l a s t P r e d i c t )
5
w h i l e ( l a s t P r e d i c t n o t n u l l a n d n o t l e a f )
6
l a s t P r e d i c t = \ a r g \ m a x _ { c \ i n c a n d i d a t e s } g ( \ t h e t a _ c ^ T x )
7
i f g ( \ t h e t a _ { l a s t P r e d i c t } ^ T x ) < . 5
8
l a s t P r e d i c t = n u l l
9
e l s e
10
t y p e . a d d ( l a s t P r e d i c t )
11
c a n d i d a t e s = l a s t P r e d i c t . c h i l d r e n
12
13

r e t u r n t y p e

What this says is that we let all of the classiﬁers in the subtree of the last predicted type vote on
whether they think x is of their type or not, and we take the most conﬁdent yes vote to make a
subprediction and proceed all over again with its children as candidates.

4

Rick Barber, Mirela Spasova

Pro ject Milestone

CS 229

The returned type variable will indeed be a path to the root in the type tree, which is to say it
follows our deﬁnition of a labeling function.

3.3 Features

Our feature set consists of features of three distinct types. We have a set of features called the
node local features which correspond to network features like in degree, out degree, total degree,
and clustering coeﬃcient. We have another set of features called the edge type features which
are a vector of counts of the types of edges leaving and entering the node in question. Referring
to our earlier data triplet, Aristotle will have at least one edge of type ”inﬂuenced” leaving his
node while Ptolemy will have at least one edge of type ”inﬂuenced” entering his node. Finally,
we will use the count of types of adjacent entities in the graph as a feature. For instance, from
the triplet above we know Aristotle has at least one neighbor of type Mathematician, Person, and
Thing namely Ptolemy.

4 Results

The results of attempting to fully label 1000 randomly drawn DBpedia entities is below.

Depth 1
Depth 2
Depth 3
Depth 4

Precision Recall Accuracy
0.83
1.0
0.83
0.69
0.85
0.77
0.47
0.53
0.33
0.71
1.0
0.71

We seperate our results according to the depth in the type tree at which the subprediction was
made.

At a given sublevel suppose the true label is l. If our subprediction at this level is l then we record
a true positive for class l. If our subprediction at this level is s (cid:54)= l then we record a false negative
for class l and a false positive for s. If our classiﬁer prematurely stopped making predictions, we
count the subsequent true types as false negatives, and if we make too speciﬁc a prediction these
are counted as false positives for the labels actually predicted.

But since our DBpedia data is known to be incompletely speciﬁed in terms of types our results
should be read as having better precision than is indicated because we can’t actually count the
DBpedia data as being the full truth. Indeed, as we noted before, the Barack Obama entity is
speciﬁed to have type Thing and Person in the DBpedia network, but our classiﬁer predicts Obama
as being a Thing, Person, Politician, President. By our evaluation standards this prediction would
reduce the depth 3 and 4 precision even though we see that our prediction is actually more credible

5

Rick Barber, Mirela Spasova

Pro ject Milestone

CS 229

than the ”correct” prediction according to the data.

We output these spurious predictions to a ﬁle, and many but not all of them were of this nature.

5 Summary

Even though our training data was not of perfect quality, we were able to perform reasonably
well for a 237 class classiﬁcation problem. Given that DBpedia’s types were originally annotated
fully by crowd sourcing, our results could represent signiﬁcant time savings over a purely manual
eﬀort. Furthermore, our results were obtained with a static dataset of a predetermined quality.
We belive that if instead of tasking DBpedia’s contributors with randomly assigning types to
the entities in their network, they answered yes or no classiﬁcation questions corresponding to a
learning algorithm’s predictions, the training data and classiﬁer quality would iteratively improve
to nearly converging with what should be the true labels of the data.

6


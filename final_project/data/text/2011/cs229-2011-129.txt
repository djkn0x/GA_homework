Unsupervised Morphological Segmentation
with Recursive Neural Network

Minh-Thang Luong { CS224N/CS229 - Final Pro ject Report

1. Introduction

parse tree for a word could be derived from the RNN.

Recent works have been successful in applying Recur-
sive Neural Network (RNN) architectures to predict
hierarchical tree structures of scene images and natu-
ral language sentences (Socher et al., 2010; 2011). In
this pro ject, we focus on the natural language modality
and explore how RNNs could address the morpholog-
ical segmentation problem.

Motivated by (Socher et al., 2010; 2011)’s work in syn-
tactic parsing of natural language sentences, where the
input is a sequence of words, our goal is to learn similar
hierarchical parse trees but for words instead, treat-
ing each character as a unit. By recursively grouping
characters together, we aim to achieve unsupervised
learning of not only the shallow morphological seg-
mentation, i.e. breaking words into morphemes, but
also the deep structure of word formations.

Unlike them, we explore learning the segmentation
task in an unsupervised manner. Two novel types of
information, lexical and structural, are proposed to in-
corporate into the RNN that helps boost performance.

The report is organized as follow. Section 2 formu-
lates the RNN architecture, while details of the back-
propagation process are given Section 4. Unsupervised
learning is described in Section 3, followed by our dis-
cussion in Section 5 about incorporating lexical and
structural information into the RNN. Experimental
setup and results are given in Section 6. We suggest
future work in Section 7, and conclude in Section 8.

2. Recursive Neural Network (RNN)

2.1. Representation

Let V be an ordered set of all characters in a language,
which is parameterized by the matrix W c of size d ×
|V |. Speciﬁcally, the ith character is in d-dimensional
space, represented by the ith column of W c .

2.2. Structure Prediction Formulation

Suppose the RNN parameters have been learned (de-
tails in Section 3), we discuss how the most probable

Figure 1. Recursive Neural Network architecture:
(a) { The recursive process of constructing (n − 1) new
nodes [p(1) ; : : : ; p(n(cid:0)1) ] from the original character nodes
[c(1) ; : : : ; c(n) ] of a word, e.g. \cats". (b) { The merg-
ing process of combining children nodes [c(lk ) ; c(rk ) ] into a
parent node p(k) with a local decision score sk .

x = c1 : : : cn ,
Let x be a word of length n,
i.e.
and a parse tree y corresponds to an ordered set of
(n-1) local decisions in the form of merging triplets,
[c(lk )c(rk ) → p(k) ], where lk and rk are indices of the
left and right children respectively. The parent node
p(k) is also considered as c(n+k) . At the end of the
merging process, the RNN tree will have a total of
2n − 1 nodes, with [c(1) ; : : : ; c(n) ] being the original
)
(
nodes (Figure 1). The score for each local decision is
denoted as sk and computed as:

sk = g

p(k) ; (cid:18)s

p(k) = f (z (k) )
z (k) = W [c(lk ) ; c(rk ) ; 1]

(1)

(2)

(3)

where (cid:18)s and W are part of the RNN parameters. W
is of size d × (2d + 1), in which d is the dimension
of character vectors. We use tanh as our activation
function f . For ﬂexibility, we abstractly denote the
score function as g which takes in parameters (cid:18)s .

The score s(x, y) of a word x and a parse tree y is

1

n−1∑
simply the score sum of all the local decisions:
k=1

s(x; y) =

sk

(4)

Lastly, given T (x) be the set of candidate parsed trees
for a word x, its RNN score is maxy∈T (x) s(x; y).

Note: Follow (Socher et al., 2010; 2011), we use
⊤
g (x; (cid:18)s ) = ((cid:18)s )
x.

3. Unsupervised Learning

3.1. Training Examples

Our input is a set of m distinct words X =
{x(1) ; : : : ; x(m)} in a language, which we will treat as
positive training examples.

To employ unsupervised learning, negative examples
are artiﬁcially constructed by corrupting the input
words. Given an input word x(i) , one way to cor-
rupt it is to randomly select a position and replace
the character at that position with a newly random
one. Other options could be to change multiple char-
acters at a time, or scramble the characters of x(i) , or
both. We opt to use the former option of corrupting
one character at a time to create a controlled setting
that could teach the RNN gradually. In fact, initial
experiments show that the system learns poorly when
multiple mutations take place at the same time.

3.2. Subgradient Methods

From Section 2, our parameters
include (cid:18) =
{W c ; (cid:18)s ; W }. Follow (Collobert & Weston, 2008), the
cost function is designed so that the RNN will be op-
timized towards giving higher scores for correct word
forms while penalizing corrupted ones. Speciﬁcally,
our ranking-type cost attempts to boost the score of
each positive example x(i) up to a margin (cid:11) (set to 0.1
)
(
m∑
experimentally) towards its negative example x(i) :
0; (cid:11) − s(x(i) ) + s(x(i) )
i=1

J ((cid:18)) =

max

(5)

Due to hinge loss, the ob jective function J is not dif-
(
)
ferentiable. Hence, subgradient method (Ratliﬀ et al.,
m∑
2007) is employed instead of gradient-ascent ones:
x(i)
@(cid:18)
i=1

@ s(x(i) )
@(cid:18)

@J ((cid:18))
@(cid:18)

− @ s

(6)

=

+

For a word x, to compute @
s(x), we ﬁrst decode x to
@(cid:18)
∗
. We have, from Eq. (4), @
ﬁnd the best tree y
s(x) =
@(cid:18)

∑
n−1
@
sk , where the gradients for each local decision
@(cid:18)
k=1
are addressed in Section 4.

∗

Note: we approximate the best parse tree y
by per-
forming a greedy search as in (Socher et al., 2010),
which iteratively ﬁnds a pair of adjacent nodes with
the highest score and combine them to yield a new
set of adjacent nodes for the next iteration. Context-
aware greedy search was experimented as well; how-
ever, without any further information about the word
structure or the morpheme distribution, the search
continues cluelessly, resulting in parameters that just
do not correlate with the segmentation task ob jective.
We further justify these in the later sections.

4. Back-propagation Through Structure

To compute the gradients for each local decision score
sk , backpropagation through structure (BTS) (Goller
& K¨uchler, 1996) is employed.

4.1. \Error-term" Derivation
{
′ (
) ◦ ((W (1) )
As an intermediate step, we deﬁne the “error terms”
(cid:14) (h)
k = @
@z (h) sk , related by a recursive formula1 :
′ (
) ◦ ((W (2) )
⊤
(cid:14) (p(h))
z (h)
if left split
f
)
k
⊤
(cid:14) (p(h))
z (h)
if right split
f
)
k
(
)
(7)
for h ≺ k , where W = [W (1)W (2)b]. At the base case:
◦ (cid:18)s
′

(cid:14) (h)
k =

z (k)

(cid:14) (k)
k = f

(8)

4.2. BTS Gradient Formulae

The gradients of
the model parameters, (cid:18) =
{W c ; (cid:18)s ; W } with respect to sk could be derived as:
⊤
@ sk
= (cid:14) (k)
k [c(lk ) ; c(rk ) ; 1]
@W
@ sk
@ (cid:18)s

= p(k)

(10)

(9)

For W c , let wc be the column vector representing for
a character c. Let {p(i1 ) ; : : : ; p(ih ) } be all the nodes
under the subtree rooted at p(k) (inclusive), which are
arranged in the order added by the RNN, Using the
chain rule technique for ordered derivatives suggested
h∑
h∑
in (Werbos, 1990), we have:
∗
∗
@
@
@
@
@wc z (ij ) :(cid:14) (ij )
@wc z (ij ) :
@wc sk =
k
@ z (ij )
j=1
j=1
1h ≼ k indicates that the node p(h) is part of the tree
rooted at p(k) . p(h) is a child of the node p(p(h)) .

sk =

Len Type Token
9418
400
2
3
2232
8123
6829
4167
4
5550
4425
5
4316
3809
6
7
2927
3168
2179
2060
8
1399
1343
9
10
805
829

es
in
er
re
ed
ng
at
ti
te

\Morpheme" subsequences
ation
43
tion
128
ing
231
222
ion
54
atio
31
tions
ating
24
ness
51
ati
191
iness
22
ting
48
ers
168
ities
16
ling
45
ate
158
150
ess
45
ions
15
ement
house
14
ator
44
tio
145
alize
13
ally
38
ies
141
134
ent
36
ates
12
ously

31
14
12
8
7
7
7
7
6

5.2. Minimum-Height Parse Trees

Table 1. Training Data Statistics & Examples: (left) { the type and token counts of \morpheme" subsequences of
lengths 2 to 10 characters; (right) the top frequent \morpheme" subsequences of lengths 2 to 5 with their token counts.

∗
indicates “simple” derivatives that ignore the
Here @
fact that z (ij ) depends on z (i1 ) ; : : : ; z (ij(cid:0)1 ) :
(
)
c = c(lij ) ̸= c(rij )
⊤
(W (1) )
c = c(rij ) ̸= c(lij )
⊤
(W (2) )
⊤
c = c(lij ) = c(rij )
(W (1) )
5. Morpheme Prior and Word
Structure Information

⊤
+ (W (2) )

@

∗
z (ij )
@wc =

(11)

0

Otherwise

5.1. Character Subsequence Distribution

The RNN could potentially learn the distributional
representation of characters through W c , and their
compositional patterns by means of the parameters
W . However, it is not clear how the cost function
could drive the model towards optimizing the mor-
phological segmentation task. Especially in the con-
text of unsupervised learning, without labeled data,
the model needs some form of prior information about
what could possibly be a morpheme unit and what
may not, to help it bootstrap the learning process.
Hence, we incorporate the distribution of morpheme
subsequences into the model ob jective.

Speciﬁcally, we ﬁrst collect counts of all character sub-
sequences of each word across the entire dataset. Af-
ter that, ML estimates conditioned on the subsequence
length, in terms of characters, are derived, which gives
us the prior probabilities lex(m) of each “morpheme”
(
)
subsequence m. Such lexical information is incorpo-
rated into model by modifying Eq. (1) to become:

sk = g

p(k) ; (cid:18)s

+ (cid:12) lex(p(k) )

where (cid:12) is the lexical weight, which we set to 10 ex-
perimentally. Table 1 gives the training set statistics
on the type and token counts of “morpheme” subse-
quences with some examples of the top frequent ones.

Linguistically, words are constructed by building up
from minimal meaning-bearing units, which are mor-
phemes. However, the RNN has no clue about the
underlying structure of a word, which we could think
of as a hidden layer of morpheme labels. As a re-
sult,
it could end up in a wrong left-skewed parse
tree as in Figure 2(a). To alleviate this problem, we
enforce a minimum-height constraint on each node.
Speciﬁcally, recall that a binary tree of n nodes will
have a minimum height of ([log2 (n − 1)] + 1). Hence,
for each subtree rooted at a parent node p, span-
ning over n leaf nodes and possessing a height of
h, the structural score of that parent node will be
struct(p) = ([log2 (n − 1)] + 1 − h).
We incorporate such structural scores into the RNN
model similar to the addition of lexical information in
the previous section. Note, however, that only nodes
in the parse trees of the positive training examples
will include structural scores. As these scores are non-
positive, such structural constraints are expected to
prevent the RNN from considering unbalanced trees
(
)
for “good” words. Eq. (1) for the positive training
examples is extended to:

sk = g

p(k) ; (cid:18)s

+ (cid:12) lex(p(k) ) + (cid:13) struct(p(k) )

where (cid:13) is the structural weight and set to 0:1 exper-
imentally. While ad-hoc in nature, this method, to
some extent, has helped alleviated the problem. Fig-
ure 2(b) shows an example in which our model could
ﬁnd a correct tree with a more balanced structure.

6. Experiments and Results

6.1. Data and Evaluation Metrics

We test our model on the English portion of the mor-
phological segmentation data are publicly available at

Figure 2. Parse trees of the word "disregarded", shown are: (a) a wrong left-skewed parse tree of height 8 and (b)
a correct parsed tree of height 5.

the Morpho Chal lenge 2010 - Semi-supervised and Un-
supervised Analysis website. There are a total of 1686
words with gold-standard segmentation, and those do
not contain punctuation, e.g., quotes or hyphens, are
retained, which leaves us with a dataset of 1295 words.
We train and test on the same set, but no segmenta-
tion information is used during training.

Without label information, we need to approximately
evaluate how good or bad a parse tree is, with respect
to the true segmentation of a word, e.g., “dis regard
ed”. To do that, a simple metric is designed to look
at indices of the word span that each node in a tree
covers, and compare them with the true segmentation
spans. In our example, the true segmentation results
in spans [1-3], [4-9], and [10-11], while those of the tree
in Figure 2(b) are [1-1], [2-2], [3-3], [2-3], [1-3], etc.

If a span of the correct segmentation appears in the
tree spans, a segment score of one is given. If all correct
spans of a word are in the tree spans, a word score of
one is awarded. Normalizing the segment and word
scores by the total number of correct spans and words
respectively gives the segmentation and word accuracy.

6.2. Results

Our ﬁnal segmentation results are presented in Table 2
in terms of word and segment accuracy. We start with
a baseline where a greedy search strategy is used with-
out any other information, which gives 23.38% and
37.76% in word and segment accuracy respectively.

At this point, one might wonder, by means of our met-
rics, how a random system would score. To make it
fair, we give some explicit ﬁgures here: there are 262
words without any segmentation out of 1295 words,

and 387 segments with a single character out of 2733
correct segmentations. These will give a random sys-
tem a score of 262=1295 = 20:23% in word accuracy
and (262 + 387)=2733 = 23:75% in segment accuracy.

Greedy
Greedy+lex
Greedy+lex+struct
Context+lex+struct

Segment
Word
37.76%
23.38%
48.65%
27.08%
53.25%
30.48%
37.19% 56.18%

Table 2. Experimental results: shown are the word and
segment accuracy of (a) the base system using greedy
search, (b) adding lexical information, (c) constraining on
the word structure, and (d) greedy, context-aware search
with lexical and structural information.

information and con-
Gradually adding the lexical
straining on the word structure consistently improve
the system performance, with a gain of 7.10% and
15.49% in absolute word and segment accuracy respec-
tively compared to the baseline. Context-aware greedy
search further boosts the results by another 6:71% in
absolute word accuracy and 2:93% in absolute segment
accuracy, achieving the best performance of 37:19%
and 56:18% in word and segment accuracy.

6.3. Discussion

Compared to the performance of state-of-the-art un-
supervised methods, in which the best result for En-
glish has (Precision, Recall, F-Measure) of (80.77%,
53.76%, 64.55%)2 , our performance is considered mod-
est. However, it is worth noting that we have not made

2 http://research.ics.tkk.fi/events/
morphochallenge2010/comp1- results.shtml

(a) Height = 8disregarded(b) Height=5disregarded  use of the full word list of 878K words with frequency
information that these unsupervised methods utilize.

At the same time, we only model each character with
a 100-dimensional vector, while in principle, it could
be scaled up to a much larger value since our vocab-
ulary size is small (only 26 characters). Due to time
constraint, the vector dimension is kept small so that
we could experiment with diﬀerent training parameter
conﬁgurations. In reality, the set of parameter com-
binations to choose from is enormous, which makes it
very tricky to get the RNN to work. As such, it is en-
couraging to start seeing the model produces sensible
analyses though with errors. Sample segmentations of
several long words in the dataset are shown in Table 3.

Gold
anthrop olog ical
collect iv iz ation
co religion ist s
rational iz ation
re conciliat ion s
respons ibil iti es
transmogriﬁc ation

RNN
anthrop olog ical
collecti viz ation
core ligion ist s
ration al iz ation
re conc iliat ion s
respons ibilit ies
trans mogriﬁ cation

Table 3. Sample segmentations of several long words in the
dataset: shown are the true and automatic ones.

7. A Final Thought - Future Work

While adding lexical and structural information does
help limiting the search space during parsing, it does
not, however, directly involve in the optimization pro-
cess. As a result, the RNN sometimes could scale
up the parameter values, diminishing the lexical and
structural inﬂuence. Hence, we suggest for future work
an approach that tackles the problem of ﬁnding the
underlying structure of words.

In (Creutz & Lagus, 2002), a codebook of all
∑
morphemes mi
in the data is maintained, and
an EM-like algorithm was used to minimized a
cost function as the data likelihood:
cost(data) =
− ln p(mi ). Given segmented data,
morph tokens mi
p(mi ) could be reestimated using ML estimates.

We mimic by maintaining list of morphemes, which
will be iteratively updated, and introduce a logistic
layer for the RNN to give each node a probabilistic
score of being a morpheme unit or not. Given parame-
ters of the RNN, morpheme labels could be inferred for
a tree by labeling nodes with the classiﬁcation scores
sorted in descending order; each subtree will be ig-
nored once its root node has been labeled. For each
morpheme-labeled node, a term -ln p(m), where m is

the character sequence that node covers, will be added
to its score formula in Eq. (1), which essentially incor-
porates the data likelihood into the RNN cost function.

8. Conclusion

In this pro ject, we have achieved a better understand-
ing of how RNN could be applied and “interact” with
the morphological segmentation task, especially in un-
supervised context. While the performance is mod-
est, we have demonstrated the eﬀectiveness of using
morpheme subsequence distribution and tree height
constraint. By making the RNN cost function bet-
ter correlated with the task ob jective, these additional
information has yielded non-trivial improvements for
the task. Capturing the underlying structure of words
in a more principle manner is a worthy goal that we
plan to pursuit in future work.

Acknowledgement: we thank Richard Socher for
providing the code base and feedbacks for the pro ject.

References

Collobert, R. and Weston, J. A uniﬁed architecture for
natural language processing: Deep neural networks
with multitask learning. In ICML, 2008.

Creutz, Mathias and Lagus, Krista. Unsupervised dis-
covery of morphemes. In Workshop on Morphologi-
cal and Phonological Learning of ACL, 2002.

Learning task-
Goller, C. and K¨uchler, A.
dependent distributed representations by backprop-
agation through structure. IEEE Transactions on
Neural Networks, 1:347–352, 1996.

Ratliﬀ, Nathan D., Bagnell, J. Andrew, and Zinke-
vich, Martin A. (online) subgradient methods for
structured prediction, 2007.

Socher, Richard, Manning, Christopher, and Ng, An-
drew. Learning Continuous Phrase Representations
and Syntactic Parsing with Recursive Neural Net-
works. In NIPS*2010 Workshop on Deep Learning
and Unsupervised Feature Learning, 2010.

Socher, Richard, Lin, Cliﬀ C., Ng, Andrew Y., and
Manning, Christopher D. Parsing Natural Scenes
and Natural Language with Recursive Neural Net-
works. In ICML, 2011.

Werbos, P. Back propagation through time: What it
does and how to do it. In Proceedings of the IEEE,
volume 78, pp. 1550–1560, 1990.


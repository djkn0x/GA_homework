Efﬁcient inference in matrix-variate Gaussian models
with iid observation noise

Oliver Stegle1
Max Planck Institutes
T ¨ubingen, Germany
stegle@tuebingen.mpg.de

Christoph Lippert1
Max Planck Institutes
T ¨ubingen, Germany
clippert@tuebingen.mpg.de

Joris Mooij
Institute for Computing and Information Sciences
Radboud University
Nijmegen, The Netherlands
j.mooij@cs.ru.nl

Neil Lawrence
Department of Computer Science
University of Shefﬁeld
Shefﬁeld, UK
N.Lawrence@sheffield.ac.uk

Karsten Borgwardt
Max Planck Institutes & Eberhard Karls Universit ¨at
T ¨ubingen, Germany
karsten.borgwardt@tuebingen.mpg.de

Abstract

Inference in matrix-variate Gaussian models has major applications for multi-
output prediction and joint learning of row and column covariances from matrix-
variate data. Here, we discuss an approach for efﬁcient inference in such models
that explicitly account for iid observation noise. Computational tractability can be
retained by exploiting the Kronecker product between row and column covariance
matrices. Using this framework, we show how to generalize the Graphical Lasso
in order to learn a sparse inverse covariance between features while accounting for
a low-rank confounding covariance between samples. We show practical utility on
applications to biology, where we model covariances with more than 100,000 di-
mensions. We ﬁnd greater accuracy in recovering biological network structures
and are able to better reconstruct the confounders.

1

Introduction

Matrix-variate normal (MVN) models have important applications in various ﬁelds. These models
have been used as regularizer for multi-output prediction, jointly modeling the similarity between
tasks and samples [1]. In related work in Gaussian processes (GPs), generalizations of MVN distri-
butions have been used for inference of vector-valued functions [2, 3]. These models with Kronecker
factored covariance have applications in geostatistics [4], statistical testing on matrix-variate data [5]
and statistical genetics [6].
In prior work, different covariance functions for rows and columns have been combined in a ﬂexible
manner. For example, Dutilleul and Zhang et al. [7, 1] have performed estimation of free-form
covariances with different norm penalties. In other applications for prediction [2] and dimension
reduction [8], combinations of free-form covariances with squared exponential covariances have
been used.

1These authors contributed equally to this work.

1

In the absence of iid observation noise, an efﬁcient inference scheme also known as the “ﬂip-ﬂop
algorithm” can be derived. In this iterative approach, estimation of the respective covariances is
decoupled by rotating the data with respect to one of the covariances to optimize parameters of the
other [7, 1]. While this simplifying assumption of noise-free matrix-variate data has been used with
some success, there are clear motivations for including iid noise in the model. For example, Bonilla
et al. [2] have shown that in multi-task regression a noise free GP with Kronecker structure leads to
a cancelation of information sharing between the various prediction tasks. This effect, also known
from the geostatistics literature [4], eliminates any beneﬁt from multivariate prediction compared
to na¨ıve approaches. Alternatively, when including observation noise in the model, computational
tractability has been limited to smaller datasets. The covariance matrix no longer directly factor-
izes into a Kronecker product, thus rendering simple approaches such as the “ﬂip-ﬂop algorithm”
inappropriate.
Here, we address these shortcomings and propose a general framework for efﬁcient inference in
matrix-variate normal models that include iid observation noise. Although in this model the co-
variance matrix no longer factorizes into a Kronecker product, we show how efﬁcient parameter
inference can still be done. To this end, we provide derivations of both the log-likelihood and gra-
dients with respect to hyperparameters that can be computed in the same asymptotic runtime as
iterations of the “ﬂip-ﬂop algorithm” on a noise-free model. This allows for parameter learning of
covariance matrices of size 105 × 105 , or even bigger, which would not be possible if done na¨ıvely.
First, we show how for any combination of covariances, evaluation of model likelihood and gradients
with respect to individual covariance parameters is tractable. Second, we apply this framework
to structure learning in Gaussian graphical models, while accounting for a confounding non-iid
sample structure. This generalization of the Graphical Lasso [9, 10] (GLASSO) allows to jointly
learn and account for a sparse inverse covariance matrix between features and a structured (non-
diagonal) sample covariance. The low rank component of the sample covariance is used to account
for confounding effects, as is done in other models for genomics [11, 12].
We illustrate this generalization called “Kronecker GLASSO” on synthetic datasets and heteroge-
neous protein signaling and gene expression data, where the aim is to recover the hidden network
structures. We show that our approach is able to recover the confounding structure, when it is known,
and reveals sparse biological networks that are in better agreement with known components of the
latent network structure.

2 Efﬁcient inference in Kronecker Gaussian processes
Assume we are given a data matrix Y ∈ RN ×D with N rows and D columns, where N is the
number of samples with D features each. As an example, think of N as a number of micro-array
experiments, where in each experiment the expression levels of the same D genes are measured;
here, yrc would be the expression level of gene c in experiment r . Alternatively, Y could represent
multi-variate targets in a multi-task prediction setting, with rows corresponding to tasks and columns
to features. This setting occurs in geostatistics, where the entries yrc correspond to ecological mea-
surements taken on a regular grid.
First we introduce some notation. For any L × M matrix A, we deﬁne vec(A) to be the vector
obtained by concatenating the columns of A; further, let A ⊗ B denote the Kronecker product (or
 a11B a12B . . .
 ;
 a11
 .
tensor product) between matrices A and B:
a1M B
a2M B
a21
a21B a22B . . .
...
...
. . .
. . .
. . .
aLM
aLM B
aL1B aL2B . . .
For modeling Y as a matrix-variate normal distribution with iid observation noise, we ﬁrst introduce
N × D additional latent variables Z, which can be thought of as the noise-free observations. The
p(Y | Z, σ2 ) = N (cid:0)vec(Y) (cid:12)(cid:12) vec(Z), σ2 IN ·D
(cid:1) .
data Y is then given by Z plus iid Gaussian observation noise:
2

A ⊗ B =

vec(A) =

(1)

,

If the covariance between rows and columns of the noise-free observations Z factorizes, we may
assume a zero-mean matrix-variate normal model for Z:
exp{− 1
2 Tr[C−1ZTR−1Z]}
p(Z | C, R) =
(2π)N D/2 |R|N/2 |C|D/2
which can be equivalently formulated as a multivariate normal distribution:
= N (vec(Z) | 0N ·D , C(ΘC ) ⊗ R(ΘR )) .
(2)
Here, the matrix C is a D × D column covariance matrix and R is an N × N row covariance matrix
p(Y | C, R, σ2 ) = N (cid:0)vec(Y) (cid:12)(cid:12) 0N ·D , C(ΘC ) ⊗ R(ΘR ) + σ2 IN ·D
(cid:1) .
that may depend on hyperparameters ΘC and ΘR respectively. Marginalizing over the noise-free
observations Z results in the Kronecker Gaussian process model of the observed data Y
(3)
For notational convenience we will drop the dependency on hyperparameters ΘC , ΘR and σ2 .
Note that for σ2 = 0, the likelihood model in Equation (3) reduces to the matrix-variate normal
distribution in Equation (2).

2.1 Efﬁcient parameter estimation
For efﬁcient optimization of the log likelihood, L = ln p(Y | C, R, σ2 ), with respect to the hyper-
parameters, we exploit an identity that allows us to write a matrix product with a Kronecker product
matrix in terms of ordinary matrix products:
(C ⊗ R)vec(Y) = vec(RTYC).
(4)
We also exploit the compatibility of a Kronecker product plus a constant diagonal term with the
eigenvalue decomposition:
(C ⊗ R + σ2 I) = (UC ⊗ UR )(SC ⊗ SR + σ2 I)(UT
C ⊗ UT
R ),
where C = UCSCUT
C is the eigenvalue decomposition of C, and similarly for R.
ln (cid:12)(cid:12)SC ⊗ SR + σ2 I(cid:12)(cid:12)
Likelihood evaluation Using these identities, the log of the likelihood in Equation (3) follows as
L = − N · D
ln(2π) − 1
2
2
− 1
RYUC )T (SC ⊗ SR + σ2 I)−1vec(UT
vec(UT
RYUC ).
2
This term can be interpreted as a multivariate normal distribution with diagonal covariance matrix
(SC ⊗ SR + σ2 I) on rotated data vec(UT
RYUC )T , similar to an approach that is used to speed up
mixed models in genetics [13].
Gradient evaluation Derivatives of the log marginal likelihood with respect to a particular co-
(cid:16)
(SC ⊗ SR + σ2 I)−1(cid:17)T
(cid:16)
(cid:17)
variance parameter θR ∈ ΘR can be expressed as
(cid:17)
(cid:16)
L = − 1
d
diag
dθR
2
d
1
RUR ˜YSC
vec( ˜Y)Tvec
UT
+
R
dθR
2
where vec( ˜Y) = (SC ⊗ SR + σ2 I)−1vec(UT
RYUC ). Analogous expressions follow for partial
derivatives with respect to θC ∈ ΘC and the noise level σ2 . Full details of all derivations, including
derivatives wrt. σ2 , can be found in the supplementary material.

SC ⊗ (UT
R

diag

d
dθR

RUR )

,

(5)

(6)

(7)

Runtime and memory complexity A na¨ıve implementation for optimizing the likelihood (3) with
respect to the hyperparameters would have runtime complexity O(N 3D3 ) and memory complexity
O(N 2D2 ). Using the likelihood and derivative as expressed in Equations (6) and (7), each eval-
uation with new kernel parameters involves solving the symmetric eigenvalue problems of both R
and C, together having a runtime complexity of O(N 3 + D3 ). Explicit evaluation of any matrix
Kronecker products is not necessary, resulting in a low memory complexity of O(N 2 + D2 ).

3

3 Graphical Lasso in the presence of confounders

Estimation of sparse inverse covariance matrices is widely used to identify undirected network struc-
tures from observational data. However, non-iid observations due to hidden confounding variables
may hinder accurate recovery of the true network structure. If not accounted for, confounders may
lead to a large number of false positive edges. This is of particular relevance in biological appli-
cations, where observational data are often heterogeneous, combining measurements from different
labs, data obtained under various perturbations or from a range of measurement platforms.
As an application of the framework described in Section 2, we here propose an approach to learn-
ing sparse inverse covariance matrices between features, while accounting for covariation between
samples due to confounders. First, we brieﬂy review the “orthogonal” approaches that account for
the corresponding types of sample and feature covariance we set out to model.

3.1 Explaining feature dependencies using the Graphical Lasso

A common approach to model relationships between variables in a graphical model is the GLASSO.
It has been used in the context of biological studies to recover the hidden network structure of
gene-gene interrelationships [14], for instance. The GLASSO assumes a multivariate Gaussian dis-
tribution on features with a sparse precision (inverse covariance) matrix. The sparsity is induced by
an L1 penalty on the entries of C−1 , the inverse of the feature covariance matrix.
Under the simplifying assumption of iid samples, the posterior distribution of Y under this model is
N(cid:89)
proportional to
N (Yr,: | 0D , C) .
p(Y , C−1 ) = p(C−1 )
p(C−1 ) ∝ exp (cid:0) − λ (cid:13)(cid:13)C−1(cid:13)(cid:13)1
(cid:1) [C−1 (cid:31) 0],
r=1
Here, the prior on the precision matrix C−1 is
(9)
with (cid:107)A(cid:107)1 deﬁned as the sum over all absolute values of the matrix entries. Note that this prior is
only nonzero for positive-deﬁnite C−1 .

(8)

3.2 Modeling confounders using the Gaussian process latent variable model

Confounders are unobserved variables that can lead to spurious associations between observed vari-
ables and to covariation between samples. A possible approach to identify such confounders is
dimensionality reduction. Here we brieﬂy review two dimensionality reduction methods, dual prob-
abilistic PCA and its generalization, the Gaussian process latent variable model (GPLVM) [15].
In the context of applications, these methods have previously been applied to identify regulatory
processes [16], and to recover confounding factors with broad effects on many features [11, 12].
In dual probabilistic PCA [15], the observed data Y is explained as a linear combination of K latent
variables (“factors”), plus independent observation noise. The model is as follows:

Y = XW + E,
where X ∈ RN ×K contains the values of K latent variables (“factors”), W ∈ RK×D contains inde-
pendent standard-normally distributed weights that specify the mapping between latent and observed
variables. Finally, E ∈ RN ×D contains iid Gaussian noise with Erc ∼ N (0, σ2 ). Marginalizing
D(cid:89)
(cid:12)(cid:12) 0N , XXT + σ2 IN
N (cid:0)Y:,c
(cid:1) .
over the weights W yields the data likelihood:
p(Y | X) =
(10)
c=1
eral Gram matrix R, with Rrs = κ(cid:0)(xr1 , . . . , xrK ), (xs1 , . . . , xsK )(cid:1) for some covariance function
Learning the latent factors X and the observation noise variance σ2 can be done by maximum
likelihood. The more general GPLVM [15] is obtained by replacing XXT in (10) with a more gen-
κ : RK × RK → R.

4

3.3 Combining the two models

We propose to combine these two different explanations of the data into one coherent model. Instead
of treating either the samples or the features as being (conditionally) independent, we aim to learn a
joint covariance for the observed data matrix Y . This model, called Kronecker GLASSO, is a special
p(Y | R, C−1 ) = N (cid:0)vec(Y) (cid:12)(cid:12) 0N ·D , C ⊗ R + σ2 IN ·D
(cid:1) .
instance of the Kronecker Gaussian process model introduced in Section 2, as the data likelihood
can be written as:
(11)
Here, we build on the model components introduced in Section 3.2 and Section 3.1. We use the
sparse L1 penalty (9) for the feature inverse covariance C−1 and use a linear kernel for the covari-
ance on rows R = XXT + ρ2 IN . Learning the model parameters proceeds via MAP inference,
optimizing the log likelihood implied by Equation (11) with respect to X and C−1 , and the hyper-
parameters σ2 , ρ2 . By combining the GLASSO and GPLVM in this way, we can recover network
structure in the presence of confounders.
An equivalent generative model can be obtained in a similar way as in dual probabilistic PCA.
The main difference is that now, the rows of the weight matrix W are sampled from a N (0D , C)
distribution instead of a N (0D , ID ) distribution. This generative model for Y given latent variables
X ∈ RN ×K and feature covariance C ∈ RD×D is of the form Y = XW + ρV + E, where
W ∈ RK×D , V ∈ RN ×D and E ∈ RN ×D are jointly independent with distributions vec(W) ∼
N (0KD , C ⊗ IK ), vec(V) ∼ N (0N D , C ⊗ IN ) and vec(E) ∼ N (0N D , σ2 IN D ).

3.4

Inference in the joint model

As already mentioned in Section 2, parameter inference in the Kronecker GLASSO model implied
by Equation (11), when done na¨ıvely, is intractable for all but very low dimensional data matrices Y .
Even using the tricks discussed in Section 2, free-form sparse inverse covariance updates for C−1
are intractable under the L1 penalty when depending on gradient updates.
Similar as in Section 2, the ﬁrst step towards efﬁcient inference is to introduce N × D additional
p(Y |Z, σ2 ) = N (cid:0)vec(Y) (cid:12)(cid:12) vec(Z), σ2 IN ·D
(cid:1)
latent variables Z, which can be thought of as the noise-free observations:
(12)
p(Z|R, C) = N (vec(Z) | 0N ·D , C ⊗ R) .
(13)
We consider the latent variables Z as additional model parameters. We now optimize the distribution
p(Y , C−1 | Z, R, σ2 ) = p(Y | Z, σ2 )p(Z | R, C)p(C−1 ) with respect to the unknown parameters
Z, C−1 , σ2 , and R (which depends on X and kernel parameters ΘR ) by iterating through the
following steps:

1. Optimize for σ2 , R after integrating out Z, for ﬁxed C:
p(Y | C, R(ΘR , X), σ2 ) =
N (cid:0)vec(Y) (cid:12)(cid:12) 0N ·D , C ⊗ R(ΘR , X) + σ2 IN ·D
argmax
σ2 ,ΘR ,X
argmax
σ2 ,ΘR ,X
2. Calculate the expectation of Z for ﬁxed R, C, and σ2 :
vec( ˆZ) = (C ⊗ R)(C ⊗ R + σ2 IN ·D )−1vec(Y)
(cid:12)(cid:12)(cid:12) 0, ˆC ⊗ R
N (cid:16)
(cid:17)
3. Optimize ˆC−1 for ﬁxed R and ˆZ:
p( ˆC−1 | ˆZ, R) = argmax
argmax
ˆC−1
ˆC−1
and set C = ˆC.

vec( ˆZ)

(cid:1)

(14)

p( ˆC−1 )

As a stopping criterion we consider the relative reduction of the negative log-marginal likelihood
(Equation (11)) plus the regularizer on C−1 . The choice to optimize ˆC−1 for ﬁxed ˆZ is motivated
by computational considerations, as this subproblem then reduces to conventional GLASSO; a full
EM approach with latent variables Z does not seem feasible. Step 1 can be done using the efﬁcient
likelihood evaluations and gradients described in Section 2. We will now discuss step 3 in more
detail.

5

(a) Precision-recall curve

(b) Ground truth

(c) GLASSO (d) Kron GLASSO (e) Ideal GLASSO

Figure 1: Network reconstruction on the simulated example. (a) Precision-recall curve, when varying the
sparsity penalty λ. Compared are the standard GLASSO, our algorithm with Kronecker structure (Kronecker
GLASSO) and as a reference an idealized setting, applying standard GLASSO to a similar dataset without
confounding inﬂuences (Ideal GLASSO). The model that accounts for confounders approaches the performance
of an idealized model, while standard GLASSO ﬁnds a large fraction of false positive edges. (b) Ground truth
network. (c-e) Recovered networks for GLASSO, Kronecker GLASSO and Ideal GLASSO at 40% recall (star
in (a)). False positive predicted edges are colored in red. Because of the effect of confounders, standard
GLASSO predicted an excess of edges to 4 of the nodes.

ln

vec( ˆZ)

−1 vec( ˆZ).

vec( ˆZ)T ( ˆC ⊗ R)

Optimizing for ˆC−1 The third step, optimizing with respect to ˆC−1 , can be done efﬁciently, using
(cid:12)(cid:12)(cid:12) − 1
(cid:12)(cid:12)(cid:12) ˆC ⊗ R
(cid:12)(cid:12)(cid:12) 0N ·D , ˆC ⊗ R
(cid:17)
ln N (cid:16)
similar ideas as in Section 2. First consider:
= − N · D
ln(2π) − 1
2
2
2
Now, using the Kronecker identity (4) and
ln |A ⊗ B| = rank(B) ln |A| + rank(A) ln |B| ,
(cid:12)(cid:12)(cid:12) 0, ˆC ⊗ R
(cid:17)
ln N (cid:16)
we can rewrite the log likelihood as:
(cid:12)(cid:12)(cid:12) ˆC−1 (cid:12)(cid:12)(cid:12) − 1
p( ˆC−1 )
vec( ˆZ)
= − N ·D
2 D ln |R| + 1
ln(2π) − 1
2 Tr( ˆZTR−1 ˆZ ˆC−1 ).
2 N ln
2
(cid:18)
−1(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13) ˆC
−1 (cid:12)(cid:12)(cid:12) − λ
(cid:12)(cid:12)(cid:12) ˆC
Thus we obtain a standard GLASSO problem with covariance matrix ˆZTR−1 ˆZ:
−1 | ˆZ, R) = argmax
− 1
−1 ) +
−1 ˆZ ˆC
1
argmax
2
2
ˆC−1
ˆC−1(cid:31)0
The inverse sample covariance R−1 in Equation (15) rotates the data covariance, similar as in the
established ﬂip-ﬂop algorithm for inference in matrix-variate normal distributions [7, 1].

(cid:19)

.

p( ˆC

Tr( ˆZTR

N ln

(15)

4 Experiments

In this Section, we describe three experiments with the generalized GLASSO.

4.1 Simulation study

First, we considered an artiﬁcial dataset to illustrate the effect of confounding factors on the solution
quality of sparse inverse covariance estimation. We created synthetic data, with N = 100 samples
and D = 50 features according to the generative model described in Section 3.3. We generated
the sparse inverse column covariance C−1 choosing edges at random with a sparsity level of 1%.
Non-zero entries of the inverse covariance were drawn from a Gaussian with mean 1 and variance
2. The row covariance matrix R was created from K = 3 random factors xk , each drawn from
unit variance iid Gaussian variables. The weighting between the confounders and the iid component
ρ2 was set such that the factors explained equal variance, which corresponds to moderate extent
of confounding inﬂuences. Finally, we added independent Gaussian observation noise, choosing a
signal-to-noise ratio of 10%.

6

(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)(cid:31)012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849(a) Precision-recall curve

(b) Ground truth

(c) GLASSO

(d) Kron GLASSO

Figure 2: Network reconstruction of a protein signaling network from Sachs et al. (a) Precision-recall curve,
when varying the sparsity penalty λ. Compared are the standard GLASSO, and our algorithm with Kronecker
structure (Kronecker GLASSO). Standard GLASSO, not accounting for confounders, found more false positive
edges for a wide range of recall rates. (b) Ground truth network. (c-d) Recovered networks for GLASSO and
Kronecker GLASSO at 40% recall (star in (a)). False positive edge predictions are colored in red.

Next, we applied different methods to reconstruct the true simulated network. We considered stan-
dard GLASSO and our Kronecker model that accounts for the confounding inﬂuence (Kronecker
GLASSO). For reference, we also considered an idealized setting, applying GLASSO to a similar
dataset without the confounding effects (Ideal GLASSO), obtained by setting X = 0N ·K in the
generative model. To determine an appropriate latent dimensionality of Kronecker GLASSO, we
used the BIC criterion on multiple restarts with K = 1 to K = 5 latent factors. For all models
we varied the sparsity parameter of the graphical lasso, setting λ = 5x , with x linearly interpolated
between −8 and 3. The solution set of lasso-based algorithms is typically unstable and depends on
slight variation of the data. To improve the stability of all methods, we employed stability selec-
tion [17], applying each algorithm for all regularization parameters 100 times to randomly drawn
subsets containing 90% of the data. We then considered edges that were found in at least 50% of all
100 restarts.
Figure 1a shows the precision-recall curve for each algorithm. Kronecker GLASSO performed
considerably better than standard GLASSO, approaching the performance of the ideal model with-
out confounders. Figures 1b-d show the reconstructed networks at 40% recall. While Kronecker
GLASSO reconstructed the same network as the ideal model, standard GLASSO found an excess of
false positive edges.

4.2 Network reconstruction of protein-signaling networks

Important practical applications of the GLASSO include the reconstruction of gene and protein
networks. Here, we revisit the extensively studied protein signaling data from Sachs et al. [18]. The
dataset provides observational data of the activations of 11 proteins under various external stimuli.
We combined measurements from the ﬁrst 3 experiments, yielding a heterogeneous mix of 2,666
samples that are not expected to be an iid sample set. To make the inference more difﬁcult, we
selected a random fraction of 10% of the samples, yielding a ﬁnal data matrix of size 266 times 11.
We used the directed ground truth network and moralized the graph structure to obtain an undirected
ground truth network. Parameter choice and stability selection were done as in the simulation study.
Figure 2 shows the results. Analogous to the simulation setting, the Kronecker GLASSO model
found true network links with greater accuracy than standard graphical lasso. This results suggest
that our model is suitable to account for confounding variation as it occurs in real settings.

4.3 Large-scale application to yeast gene expression data

Next, we considered an application to large-scale gene expression proﬁling data from yeast. We
revisited the dataset from Smith et al. [19], consisting of 109 genetically diverse yeast strains, each of
which has been expression proﬁled in two environmental conditions (glucose and ethanol). Because

7

prafpmekplcgPIP2PIP3p44/42pakts473PKAPKCP38pjnkprafpmekplcgPIP2PIP3p44/42pakts473PKAPKCP38pjnkprafpmekplcgPIP2PIP3p44/42pakts473PKAPKCP38pjnk(a) Confounder reconstruction

(b) GLASSO consistency (68%)

(c) Kron. GLASSO consistency
(74%)

Figure 3: (a) Correlation coefﬁcient between learned confounding factor and true environmental condition for
different subsets of all features (genes). Compared are the standard GPLVM model with a linear covariance
and our proposed model that accounts for low rank confounders and sparse gene-gene relationships (Kronecker
GLASSO). Kronecker GLASSO is able to better recover the hidden confounder by accounting for the covari-
ance structure between genes. (b,c) Consistency of edges on the largest network with 1,000 nodes learnt on the
joint dataset, comparing the results when combining both conditions with those for a single condition (glucose).

the confounder in this dataset is known explicitly, we tested the ability of Kronecker GLASSO to
recover it from observational data. Because of missing complete ground truth information, we could
not evaluate the network reconstruction quality directly. An appropriate regularization parameter
was selected by means of cross validation, evaluating the marginal likelihood on a test set (analogous
to the procedure described in [10]). To simplify the comparison to the known confounding factor,
we chose a ﬁxed number of confounders that we set to K = 1.

Recovery of the known confounder Figure 3a shows the r2 correlation coefﬁcient between the
inferred factor and the true environmental condition for increasing number of features (genes) that
were used for learning. In particular for small numbers of genes, accounting for the network struc-
ture between genes improved the ability to recover the true confounding effect.

Consistency of obtained networks Next, we tested the consistency when applying GLASSO and
Kronecker GLASSO to data that combines both conditions, glucose and ethanol, comparing to the
recovered network from a single condition alone (glucose). The respective networks are shown in
Figures 3b and 3c. The Kronecker GLASSO model identiﬁes more consistent edges, which shows
the susceptibility of standard GLASSO to the confounder, here the environmental inﬂuence.

5 Conclusions and Discussion

We have shown an efﬁcient scheme for parameter learning in matrix-variate normal distributions
with iid observation noise. By exploiting some linear algebra tricks, we have shown how hyper-
parameter optimization for the row and column covariances can be carried out without evaluating
the prohibitive full covariance, thereby greatly reducing computational and memory complexity. To
the best of our knowledge, these measures have not previously been proposed, despite their general
applicability.
As an application of our framework, we have proposed a method that accounts for confounding in-
ﬂuences while estimating a sparse inverse covariance structure. Our approach extends the Graphical
Lasso, generalizing the rigid assumption of iid samples to more general sample covariances. For
this purpose, we employ a Kronecker product covariance structure and learn a low-rank covariance
between samples, thereby accounting for potential confounding inﬂuences. We provided synthetic
and real world examples where our method is of practical use, reducing the number of false positive
edges learned.

Acknowledgments This research was supported by the FP7 PASCAL II Network of Excellence.
OS received funding from the Volkswagen Foundation. JM was supported by NWO, the Netherlands
Organization for Scientiﬁc Research (VENI grant 639.031.036).

8

101102103Number of features (genes)0.40.50.60.70.80.91.0r^2 correlation with true confounderGPLVMKronecker GLassoReferences
[1] Y. Zhang and J. Schneider. Learning multiple tasks with a sparse matrix-normal penalty. In
Advances in Neural Information Processing Systems, 2010.
[2] E. Bonilla, K.M. Chai, and C. Williams. Multi-task gaussian process prediction. Advances in
Neural Information Processing Systems, 20:153–160, 2008.
[3] M.A. Alvarez and N.D. Lawrence. Computationally efﬁcient convolved multiple output gaus-
sian processes. Journal of Machine Learning Research, 12:1425–1466, 2011.
[4] H. Wackernagel. Multivariate geostatistics: an introduction with applications. Springer Ver-
lag, 2003.
[5] G.I. Allen and R. Tibshirani. Inference with transposable data: Modeling the effects of row
and column correlations. Arxiv preprint arXiv:1004.0209, 2010.
[6] M. Lynch and B. Walsh. Genetics and Analysis of Quantitative Traits. Sinauer Associates Inc.,
U.S., 1998.
[7] P. Dutilleul. The MLE algorithm for the matrix normal distribution. Journal of Statistical
Computation and Simulation, 64(2):105–123, 1999.
[8] K. Zhang, B. Sch ¨olkopf, and D. Janzing. Invariant gaussian process latent variable models and
application in causal discovery. In Uncertainty in Artiﬁcial Intelligence, 2010.
[9] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum
likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning
Research, 9:485–516, 2008.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graph-
ical lasso. Biostatistics, 9(3):432, 2008.
[11] J.T. Leek and J.D. Storey. Capturing heterogeneity in gene expression studies by surrogate
variable analysis. PLoS Genetics, 3(9):e161, 2007.
[12] O. Stegle, L. Parts, R. Durbin, and J. Winn. A bayesian framework to account for complex
non-genetic factors in gene expression levels greatly increases power in eqtl studies. PLoS
Computational Biology, 6(5):e1000770, 2010.
[13] C. Lippert, J. Listgarten, Y. Liu, C.M. Kadie, R.I. Davidson, and D. Heckerman. FaST linear
mixed models for genome-wide association studies. Nature Methods, 8:833–835, 2011.
[14] P. Men ´endez, Y.A.I. Kourmpetis, C.J.F. Ter Braak, and F.A. van Eeuwijk. Gene regulatory
networks from multifactorial perturbations using graphical lasso: Application to the dream4
challenge. PLoS One, 5(12):e14147, 2010.
[15] N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process
latent variable models. Journal of Machine Learning Research, 6:1783–1816, 2005.
[16] K.Y. Yeung and W.L. Ruzzo. Principal component analysis for clustering gene expression data.
Bioinformatics, 17(9):763, 2001.
[17] N. Meinshausen and P. B ¨uhlmann. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417–473, 2010.
[18] K. Sachs, O. Perez, D. Pe’er, D.A. Lauffenburger, and G.P. Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523, 2005.
[19] E.N. Smith and L. Kruglyak. Gene–environment interaction in yeast gene expression. PLoS
Biology, 6(4):e83, 2008.

9


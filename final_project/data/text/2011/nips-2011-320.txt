On Causal Discovery with
Cyclic Additive Noise Models

Joris M. Mooij
Radboud University
Nijmegen, The Netherlands
j.mooij@cs.ru.nl

Dominik Janzing
Max Planck Institute for Intelligent Systems
T ¨ubingen, Germany
dominik.janzing@tuebingen.mpg.de

Tom Heskes
Radboud University
Nijmegen, The Netherlands
t.heskes@cs.ru.nl

Bernhard Sch ¨olkopf
Max Planck Institute for Intelligent Systems
T ¨ubingen, Germany
bs@tuebingen.mpg.de

Abstract

We study a particular class of cyclic causal models, where each variable is a (possi-
bly nonlinear) function of its parents and additive noise. We prove that the causal
graph of such models is generically identiﬁable in the bivariate, Gaussian-noise
case. We also propose a method to learn such models from observational data. In
the acyclic case, the method reduces to ordinary regression, but in the more chal-
lenging cyclic case, an additional term arises in the loss function, which makes
it a special case of nonlinear independent component analysis. We illustrate the
proposed method on synthetic data.

1

Introduction

Causal discovery refers to a special class of statistical and machine learning methods that infer
causal relationships between variables from data and prior knowledge [1, 2, 3]. Whereas in machine
learning, one traditionally concentrates on the task of predicting the values of variables given obser-
vations of other variables (for example in regression or classiﬁcation tasks), causal discovery focuses
on predicting the results of interventions on the system: if one forces one (or more) of the variables
into a particular state, how will the probability distribution of the other variables be affected? In this
sense, causal discovery concentrates more on inferring the underlying mechanism that generated the
data than on modeling the data itself.
An important assumption often made in causal discovery is that the causal mechanism is acyclic, i.e.,
that no feedback loops are present in the system. For example, if A causes B, and B causes C, then
the possibility that C also causes A is usually excluded from the outset. This acyclicity assumption
is useful because it simpliﬁes the theoretical analysis and often is also a reasonable assumption to
make. Nevertheless, causal cycles are known to occur frequently in biological systems such as gene
regulatory networks and protein interaction networks. One would expect that taking such feedback
loops into account during data analysis should therefore signiﬁcantly improve the quality of the
inferred causal structure.
Essentially two strategies for dealing with cycles in causal models can be distinguished. The ﬁrst
one is to perform repeated measurements in time, and to infer a causal model for the dynamics of
the underlying system. The fact that causes always precede their effects provides additional prior
knowledge that simpliﬁes causal discovery, which is exploited in methods based on Granger causal-
ity [4]. Additionally, under certain assumptions, “unrolling” the model in time effectively removes
the cycles, which is used in methods such as vector auto-regressive models, which are popular in

1

econometrics, or more generally, Dynamic Bayesian Networks [5] and ordinary differential equa-
tion models. However, all these methods need time series data where the temporal resolution of the
measurements is high relative to the characteristic time scale of the feedback loops in order to rule
out instantaneous cyclic relationships. Therefore, a signiﬁcant practical drawback of this strategy
is that obtaining time series data with sufﬁciently high temporal resolution is often costly—or even
impossible—using current technology.
The second strategy is based on the assumption that the system is in equilibrium, and that the data
have been gathered from an equilibrium distribution (in the ergodic case, the data can also consist of
snapshots of the dynamical system, taken at different points in time). The equilibrium distribution
is then used to draw conclusions about the underlying dynamic system, and to predict the results
of interventions. This is the approach taken in the current paper. We assume the equilibrium to be
described by ﬁxed point equations, where each variable is a function of some other variables, plus
noise. This noise models unobserved causes and is assumed to be different for each independent
realization of the system, but constant during equilibration. In the simplest case (assuming causal
sufﬁciency), the noise terms are jointly independent. Together, these assumptions deﬁne an interest-
ing model class that forms a direct generalization of Structural Equation Models (SEMs) [2] to the
nonlinear (and cyclic) case.
An important novel aspect of our work is that we consider continuous-valued variables and nonlin-
ear causal mechanisms. Although the linear case has been studied in considerable detail already
[6, 7, 8], as far as we know, nobody has yet investigated the (more realistic) case of nonlinear causal
mechanisms. The basic assumption made in [7] is the so-called Global Directed Markov Condition,
which relates (conditional) independences between the variables with the structure of the causal
graph. In the cyclic case, however, it is not obvious what the relationship is with the class of nonlin-
ear causal models that we consider here. Therefore, direct generalization of the algorithm proposed
in [7] to the nonlinear case seems difﬁcult. Furthermore, conditional independences only allow
identiﬁcation of the graph up to Markov equivalence classes. For instance, in the bivariate case,
one cannot distinguish between X → Y , Y → X and X (cid:28) Y using conditional independences
alone. Researchers have also studied cyclic causal models with discrete variables [9, 10]. However,
if the measured variables are intrinsically continuous-valued, it is desirable to avoid discretization
as a preprocessing step, as this throws away information that is useful for causal discovery.

2 Cyclic additive noise models

Let V be a ﬁnite index set. Let (Xi )i∈V be random variables modeling measurable properties of the
system of interest and let (Ei )i∈V be other random variables modeling unobservable noise sources.
We assume that all random variables take values in the real numbers. We also assume that the noise
(cid:89)
variables (Ei )i∈V have densities and are jointly independent:
i∈V
For each i, let pa(i) ⊆ V \ {i} be a set deﬁning the parents of i and fi : R|pa(i)| → R be a con-
tinuously differentiable function. Under certain assumptions (see below), the following equations
specify a unique probability distribution on the observable variables (Xi )i∈V :
i ∈ V .
Xi = fi (Xpa(i) ) + Ei ,
Using vector notation, we can write the ﬁxed point equations (2) in a more compact manner as

p(eV ) =

pEi (ei ).

(1)

(2)

(3)
X = f (X ) + E .
The probability distribution p(X ) induced by these equations is interpreted as the equilibrium dis-
tribution of an underlying dynamic system. Each function fi represents a causal mechanism which
determines Xi as a function of its parents Xpa(i) , which model its direct causes. The noise variables
can be interpreted as other, unobserved causes for their corresponding variables. By assuming inde-
pendence of the noise variables, we are assuming causal sufﬁciency, or in other words, absence of
confounders (hidden common causes).
We call a model speciﬁed by (1) and (2) an additive noise model. With any additive noise model
we can associate a directed graph with vertices V and directed edges i → j if i ∈ pa(j ), i.e., from

2

causes to their direct effects.1 If this graph is acyclic, we call the model an acyclic additive noise
model. If the graph contains (directed) cycles, we call the model a cyclic additive noise model.2
Interpretation in the cyclic case
Note that the presence of cycles increases the complexity of the model, because the equations (2)
become recursive. The interpretation of these equations also becomes less straightforward in the
cyclic case. In general, for a ﬁxed noise value E = e, the ﬁxed point equations x = f (x) + e can
have any number of ﬁxed points between 0 and ∞. For simplicity, however, we will assume that for
each noise value e there exists a unique ﬁxed point x = F (e). Later, in Section 3.1, we will give a
sufﬁcient condition for this to be the case. Under this assumption, the joint probability distribution
p(E ) induces a unique joint probability distribution p(X ).
This interpretation also shows a way to sample from the joint distribution: First, one samples a
joint value of the noise e. Then, one iterates the ﬁxed point equations (2) to ﬁnd the corresponding
ﬁxed point x = F (e). This yields one sample x. Different independent samples are obtained by
repeating this process. Thus, the equations can be interpreted as the equilibrium distribution of a
dynamic system in the presence of noise which is constant during equilibration, but differs across
measurements (data points). If in reality the noise does change over time, but on a slow time scale
relative to the time scale of the equilibration, then this model can be considered as the ﬁrst-order
approximation.
The induced density
Although the mapping F : e (cid:55)→ x that maps noise values to their corresponding ﬁxed points under
(3) is nontrivial in most cases, a crucial observation is that its inverse G = F −1 = I − f has
a very simple form (here, I is the identity mapping). Under the change of variables e (cid:55)→ x, the
(cid:0)x − f (x)(cid:1) |I − ∇f (x)| = |I − ∇f (x)| (cid:89)
(cid:0)xi − fi (xpa(i) )(cid:1)
transformation rule of the densities reads:
i∈V
where ∇f (x) is the Jacobian of f evaluated at x and |·| denotes the absolute value of the determinant
of a matrix.
Note that although sampling from the distribution pX is elaborate (as it typically involves many
iterations of the ﬁxed point equations), the corresponding density can be easily expressed analyti-
cally in terms of the noise distributions and partial derivatives of the causal mechanisms. Later we
will see that the fact that the model has a simple structure in the “backwards” direction allows us to
efﬁciently learn it from data, which may be surprising considering the fact that the model is complex
in the “forward” direction.
Causal interpretation
An additive noise model can be used for ordinary prediction tasks (i.e., predict some of the variables
conditioned on observations of some other variables), but can also be used to predict the results
of interventions: if we force some of the variables to certain values, what will happen with the
others? Such an intervention can be modeled by replacing the equations for the intervened variables
by simple equations Xi = Ci , with Ci the value set by the intervention. This procedure results
in another additive noise model. If the altered ﬁxed point equations induce a unique probability
distribution on X , then this is the predicted distribution on X under the intervention. In this sense,
additive noise models are given a causal interpretation. Hereafter, we will therefore refer to the
graph associated with the additive noise model as the causal graph.

pX (x) = pE

pEi

(4)

3

Identiﬁability

An interesting and important question for causal discovery is under which conditions the causal
graph is identiﬁable given only the joint distribution p(X ). Lacerda et al. [8] have shown that under
1 If some causal mechanism fj does not depend on one of its parents i ∈ pa(j ), i.e., if ∂ fj
everywhere, then we discard the edge i → j .
∂Xi
2Cyclic additive noise models are also known as “non-recursive” (nonlinear) structural equation models,
whereas the acyclic versions are known as “recursive” (nonlinear) SEMs. This terminology is common usage
but confusing, as it is precisely in the cyclic case that one needs a recursive procedure to calculate the solutions
of equations (2), and not the other way around.

(Xpa(j ) ) = 0

3

the additional assumption of linearity (i.e., all functions fi are linear), the causal graph is completely
identiﬁable if at most one of the noise sources has a Gaussian distribution. The proof is based on
Independent Component Analysis. Our aim here is to deal with the more difﬁcult nonlinear case. In
this work, we focus our attention on the bivariate case. Our main result, Theorem 1, can be seen as
an extension of the identiﬁability result for acyclic nonlinear additive noise models derived in [11],
although we make the additional simplifying assumption that the noise variables are Gaussian. We
believe that similar identiﬁability results can be derived in the multivariate case (|V | > 2) and for
non-Gaussian noise distributions. However, proving such results seems to be signiﬁcantly harder as
the calculations become very cumbersome, and we leave this as an open problem for future work.

3.1 The bivariate case

Before we state our identiﬁability result, we ﬁrst give a sufﬁcient condition for existence of a unique
equilibrium distribution for the bivariate case.
Lemma 1 Consider the ﬁxed point equations x = fX (y) + cX , y = fY (x) + cY parameterized
by constants (cX , cY ). If supx,y |f (cid:48)
Y (x)| = r < 1, then for any (cX , cY ), the ﬁxed point
X (y)f (cid:48)
equations converge to a unique ﬁxed point that does not depend on the initial conditions.
Proof. Consider the mapping deﬁned by applying the ﬁxed point equations twice. Its Jacobian
is diagonal and the absolute values of the entries are bounded from above by r < 1 under the
assumption above. According to Banach’s ﬁxed point theorem, it is a contraction (e.g., with respect
to the Euclidean norm on R2 ) and therefore has a ﬁxed point that is unique. Independent of the initial
conditions, under repeated application of this mapping, one converges to this ﬁxed point. Lemma 1
in the supplement then shows that the same conclusion must hold for the mapping that applies the
(cid:3)
ﬁxed point equations only once.

This lemma provides a sufﬁcient condition for an additive noise model to be well-deﬁned in the
bivariate case. Also, the result of any intervention will be well-deﬁned under this condition.
Now suppose we are given the joint distribution pX,Y of two real-valued random variables X, Y
which is induced by an additive noise model. The question is whether we can identify the causal
graph corresponding with the true model out of the four possibilities (X Y , X → Y , Y → X ,
X (cid:28) Y ). Hoyer et al. [11] have shown that if one excludes the cyclic case X (cid:28) Y , then in the
generic case, the causal structure is identiﬁable. Our aim is to prove a stronger identiﬁability result
where the cyclic case is not excluded a priori. As a ﬁrst step in this direction, we consider here the
case of Gaussian noise.
Theorem 1 Let pX,Y be induced by two additive Gaussian noise models, M and ˜M:
(M)
X ), EY ∼ N (0, α−1
X = fX (Y ) + EX , Y = fY (X ) + EY , EX ⊥⊥ EY , EX ∼ N (0, α−1
Y )
(cid:12)(cid:12)(cid:12) < 1, then the two
(cid:12)(cid:12)(cid:12) ˜f (cid:48)
( ˜M)
X = ˜fX (Y ) + ˜EX , Y = ˜fY (X ) + ˜EY , ˜EX ⊥⊥ ˜EY , ˜EX ∼ N (0, ˜α−1
X ), ˜EY ∼ N (0, ˜α−1
Y )
Assuming that supx,y |f (cid:48)
Y (x)| < 1 and similarly supx,y
X (y) ˜f (cid:48)
X (y)f (cid:48)
Y (x)
corresponding causal graphs coincide: GM = G ˜M , i.e.:
fX is constant ⇐⇒ ˜fX is constant,
fY is constant ⇐⇒ ˜fY is constant,
and
or the models are of the following very special form:
• either: fX , ˜fX , fY , ˜fY are all afﬁne,
• or: one model (say ˜M) is acyclic, the other is cyclic, and the following equations hold:
fY (x) = C x + D with C (cid:54)= 0, fX (y) =
˜fX (y) − αY
˜αX
αY
CD, ˜fY (x) = ˜D (5)
αX
αX
αX
and ˜fX satisﬁes the following differential equation:3
− 1
˜fX − αY C y + αY CD)( ˜αX
αX
= αY (y − D) − ˜αY (y − ˜D) + C

X − αY C ) + ˜αX
˜f (cid:48)
˜f (cid:48)(cid:48)
˜αX
X
αX − ( ˜αX
X − αY C )C
˜f (cid:48)

C y +

( ˜αX

˜fX

˜f (cid:48)
X

.

(6)

3Or similar equations with the roles of X and Y reversed.

4

We will only sketch the proof here, and refer to the supplementary material for the details. What
the theorem shows is that, apart from a small class of exceptions, bivariate additive Gaussian-noise
models induce densities that allow a perfect reconstruction of the causal graph. In a certain sense,
the situation can be seen as similar to the well-known “faithfulness assumption” [3]:
the latter
assumption is often made in order to exclude the highly special cases of causal models which would
spoil identiﬁability of the Markov equivalence class. The usual reasoning is that these cases are so
rare that they can be ignored in practice. A similar reasoning can be made in our case.
Although our main identiﬁability result, Theorem 1, may seem rather restricted as it only considers
two variables, it may be possible to use this two-variable identiﬁability result as a key building block
for deriving more general identiﬁability results for many variables, similar as how [12] generalized
the (acyclic) identiﬁability result of [11] from two to many variables.

∂ 2πX,Y
∂x∂ y

(8)

(9)

= ˜αX

˜f (cid:48)
X (y) + ˜αY

Y (x) −
˜f (cid:48)

∂ 2πX,Y
∂x∂ y

A similar equation holds for the other model:

3.2 Proof sketch
(cid:0)x − fX (y)(cid:1) + πEY
(cid:0)y − fY (x)(cid:1) + log |1 − f (cid:48)
Writing π··· (· · · ) := log p··· (· · · ) for logarithms of densities, we reexpress (4) for the bivariate case:
Y (x)|
X (y)f (cid:48)
(7)
πX,Y (x, y) = πEX
Partial differentiation with respect to x and y yields the following equation, which will be the equa-
(cid:0)y − fY (x)(cid:1)f (cid:48)
(cid:0)x − fX (y)(cid:1)f (cid:48)
tion on which we base our identiﬁability proof:
(cid:0)1 − f (cid:48)
Y (x)(cid:1)2
X (y)f (cid:48)(cid:48)
f (cid:48)(cid:48)
X (y) − π (cid:48)(cid:48)
= −π (cid:48)(cid:48)
Y (x) −
Y (x)
X (y)f (cid:48)
EX
EY
We will now specialize to Gaussian noise and give a sketch of how to prove identiﬁability of the
causal graph. We assume EX ∼ N (0, α−1
X ) and EY ∼ N (0, α−1
X , αY = σ−2
Y ) where αX = σ−2
Y
are the precisions (inverse variances) of the Gaussian noise variables. Equation (8) simpliﬁes to:
(cid:0)1 − f (cid:48)
Y (x)(cid:1)2
X (y)f (cid:48)(cid:48)
f (cid:48)(cid:48)
Y (x) −
X (y) + αY f (cid:48)
= αX f (cid:48)
Y (x)
X (y)f (cid:48)
(cid:0)1 − ˜f (cid:48)
Y (x)(cid:1)2
X (y) ˜f (cid:48)(cid:48)
˜f (cid:48)(cid:48)
∂ 2πX,Y
Y (x)
X (y) ˜f (cid:48)
∂x∂ y
The general idea of the identiﬁability proof is as follows. We consider two cases: (i) model ˜M has
X (cid:54)= 0, ˜f (cid:48)
Y = 0; (ii) model ˜M has one “arrow”, say, ˜f (cid:48)
zero “arrows”, i.e., ˜f (cid:48)
X = 0 and ˜f (cid:48)
Y = 0.
By equating the r.h.s.’s of (9) and (10), we show in both cases that generically (i.e., except for very
special choices of the model parameters), model M must equal model ˜M. This then implies that
the causal graphs of M and ˜M must be the same in the generic case.
0 = (cid:0)αX f (cid:48)
Y (x)(cid:1)(cid:0)1 − f (cid:48)
Y (x)(cid:1)2 − f (cid:48)(cid:48)
For example, in the ﬁrst case, because ˜f (cid:48)
X = ˜f (cid:48)
Y = 0, we obtain the following equation:
X (y)f (cid:48)(cid:48)
X (y) + αY f (cid:48)
X (y)f (cid:48)
(11)
Y (x)
This is a nonlinear partial differential equation in φ(x) := f (cid:48)
Y (x) and ψ(y) := f (cid:48)
X (y). Inspired
by the identiﬁability proof in [13], we adopt the solution method from [14, Supplement S.4.3] that
gives a general method for solving functional-differential equations of the form
Φ1 (x)Ψ1 (y) + Φ2 (x)Ψ2 (y) + · · · + Φk (x)Ψk (y) = 0
where the functionals Φi (x) and Ψi (y) depend only on x and y , respectively:
Φi (x) = Φi (x, φ, φ(cid:48) ),
Ψi (y) = Ψi (y , ψ , ψ (cid:48) ).
The idea behind the solution method is to repeatedly divide by one of the functionals and differenti-
(cid:18) ∂
respect to x, we obtain:(cid:18) ∂
(cid:19)
(cid:19)
ate with respect to the corresponding variable. For example, dividing by Φ1 and differentiating with
∂x
∂x

Ψ2 (y) + · · · +

Φ2 (x)
Φ1 (x)

Φk (x)
Φ1 (x)

Ψk (y) = 0

(10)

(12)

5

which is again of the form (12), but with one fewer term. This process is repeated until an equation
of the form (12) remains with only 2 terms. That equation is easily solved, as its general solution
can be written as
C1Φ1 (x) + C2Φ2 (x) = 0, C2Ψ1 (y) − C1Ψ2 (y) = 0
for arbitrary constants C1 , C2 ∈ R, and there are also two degenerate solutions Φ1 = Φ2 = 0
(and Ψ1 , Ψ2 arbitrary) and Ψ1 = Ψ2 = 0 (and Φ1 , Φ2 arbitrary). These equations, which are
now ordinary differential equations, can be solved by standard methods. The solutions are then
substituted into the original equation (12) in order to remove redundant constants of integration.
Applying this method to the case at hand, one obtains equations for f (cid:48)
X and f (cid:48)
Y . Solving these
equations, one ﬁnds that either M = ˜M, or that f (cid:48)
X = ˜f (cid:48)
Y = ˜f (cid:48)
X = f (cid:48)
Y = 0. In the second case
(where ˜M has one arrow) the equations show that either M = ˜M, or the model parameters should
satisfy equations (5) and (6).

p( ˆf )

argmax
ˆf

pEi

(13)

i − ˆfi
x(n)

4 Learning additive noise models from observational data
In this section, we propose a method to learn an additive noise model from a ﬁnite data set D :=
{x(n) }N
n=1 . We will only describe the bivariate case in detail, although the method can be extended
to more than two variables in a straightforward way.
We ﬁrst consider how we can learn the causal mechanisms {fi }i∈V for a ﬁxed causal structure. This
can be done efﬁciently by a MAP estimate with respect to (the parameters of) the causal mechanisms.
(cid:32)(cid:12)(cid:12)(cid:12)I − ∇ ˆf (cid:0)x(n) (cid:1)(cid:12)(cid:12)(cid:12) (cid:89)
(cid:1)(cid:17)(cid:33)
(cid:16)
N(cid:89)
(cid:0)x(n)
Using (4), the MAP problem can be written as:
pa(i)
i∈V
n=1
where p( ˆf ) speciﬁes the prior distribution of the causal mechanisms. Note the presence of the
determinant; in the acyclic case, this term becomes 1, and the method reduces to standard regression.
In the cyclic case, however, the determinant is necessary in order to penalize dependencies between
the estimated noise variables. One can consider this as a special case of nonlinear independent
component analysis, as the MAP estimate (13) can also be interpreted as the minimizer of the mutual
information between the noise variables. If the estimated functions lead to noise estimates ˆEi =
Xi − ˆfi (Xpa(i) ) which are mutually independent according to some independence test, then we
accept the model.
One can try all possible causal graph structures and test which ones ﬁt the data. The models that
lead to independent estimated noise values are possible causal explanations of the data. If multiple
models with different causal graphs lead to independent estimated noise values, we prefer models
with fewer arrows in the graph.4 If the number of data points is large enough, Theorem 1 suggests
that for two variables with Gaussian noise, in the generic case, a unique causal structure will be
identiﬁed in this way. For more than two variables, and for other noise distributions, the method can
still be applied, but we do not know whether (in general and asymptotically) there will be a unique
causal structure that explains the data.
We now work out the bivariate Gaussian case in more detail. The prior for the functions ˆf can be
chosen arbitrarily, for example using some parametric approach. Here, we will use a nonparametric
approach using Gaussian processes. The negative log-likelihood L := − ln p(D | ˆfX , ˆfY ) can be
written in terms of the observational data D := {(x(n) , y (n) )}N
(cid:12)(cid:12)(cid:12)1 − ˆf (cid:48)
(cid:12)(cid:12)(cid:12) .
(cid:0)x(i) − ˆfX (y (i) )(cid:1) − N(cid:88)
(cid:0)y (i) − ˆfY (x(i) )(cid:1) − N(cid:88)
L = − N(cid:88)
n=1 as:
Y (x(i) ) ˆf (cid:48)
X (y (i) )
log
πEX
πEY
ors for the causal mechanisms fX and fY , i.e., taking ˆx := fX (y) ∼ N (cid:0)0, KX (y)(cid:1) and
i=1
i=1
i=1
Assuming Gaussian noise EX ∼ N (0, σ2
X ), EY ∼ N (0, σ2
Y ) and using Gaussian Process pri-
4Note that if a certain model leads to independent noise terms, then adding more arrows will still allow
independent noise terms, by setting some functions to 0—see also Figure 1 below.

6

ˆy := fY (x) ∼ N (cid:0)0, KY (x)(cid:1) where KX is the Gram matrix with entries KX ;ij = kX (y (i) , y (j ) )
for some covariance function kX : R2 → R, and similarly for KY , we obtain:
(cid:18) 1
L = N log σX + N log σY +
log |KX | +
log |KY |
1
1
min
2
2
ˆx, ˆy
(cid:33)
(cid:107)x − ˆx(cid:107)2 +
(cid:107)y − ˆy(cid:107)2 +
ˆyT K −1
1
1
1
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)1 −
(cid:19) (cid:18) ∂ kX
(cid:18) ∂ kY
− N(cid:88)
+ min
Y ˆy
2σ2
2σ2
2
2
ˆx, ˆy
Y
X
(y (i) , y)K −1
(x(i) , x)K −1
Y ˆy
X ˆx
∂x
∂ y
i=1
(cid:18)
(cid:19)
where we used the expected derivatives of the Gaussian Processes for approximating the
determinant-term. In our experiments, we used Gaussian covariance kernels
− (y − y (cid:48) )2
kX (y , y (cid:48) ) = λ2
+ ρδy ,y (cid:48) ,
X exp
2κ2
X
and likewise for kY . Note that we added a small constant (ρ = 10−4 ) to the diagonal to allow for
small, independent measurement errors or rounding errors (which occur because the Gram matrices
are very ill-conditioned). The optimization problem can be solved numerically, e.g., using standard
methods such as conjugate gradient or L-BFGS. We optimize simultaneously with respect to the
noise values ˆx, ˆy and the hyperparameters log σX , log κX , log λX , log σY , log κY , log λY .

ˆxT K −1
X ˆx +

log

,

5 Experiments

We illustrate the method on several synthetic data sets in Figure 1. Each row shows a data set with
N = 500 data points. Because of space constraints, we only show the learned cyclic additive noise
models, omitting the acyclic ones. In each case, we calculated the p-value for independence of the
two noise variables using the HSIC (Hilbert-Schmidt Independence Criterion) test [15]; for p-values
substantially above 0 (say larger than 1%), we do not reject the null hypothesis of independence and
hence accept the model as possible causal explanation of the data. This happens in four out of six
cases, except for the cases displayed in rows 1b and 3b, which are rejected.
Rows 1a and 1b concern the same data generated from a nonlinear and acyclic model. We found two
different local minima, one of which is accepted (the one more closely resembling the true model),
and one is rejected. Even though we learned a causal model with cyclic structure, in the accepted
solution, one of the learned causal mechanisms becomes (almost) constant. Rows 3a and 3b show
again two different solutions for the same data, now generated from a nonlinear cyclic model. Note
that the solution in row 3b could be preferred over that in row 3a based upon its likelihood, but
is actually rejected because its estimated noises are highly dependent. Row 4 shows data from a
linear, cyclic model, where the ratio of the noise sources equals the ratio of the slopes of the causal
mechanisms. This makes this linear model part of the special class of unidentiﬁable additive noise
models. In this case, the MAP estimates for the causal mechanisms are quite different from the true
ones.

6 Discussion and Conclusion

We have studied a particular class of cyclic causal models given by nonlinear SEMs with additive
noise. We have discussed how these models can be interpreted to describe the equilibrium distribu-
tion of a dynamic system with noise that is constant in time. We have looked in detail at the bivariate
Gaussian-noise case and shown generic identiﬁability of the causal graph. We have also proposed a
method to learn such models from observational data and illustrated it on synthetic data.
Even though we have shown that in this “laboratory setting”, the method can be made to work on
purely observational data when enough data is available, it includes several assumptions that make it
challenging to apply in real-world scenarios. Also, from our experiments, it appears that the method
often ﬁnds other solutions (local minima of the log likelihood) which differ from the expected true
data generating model but which have dependent estimated noises.
Thus there is ample opportunity for future work: For example, improving the robustness of the
learning method, and generalizing the results to many variables and non-Gaussian noise.

7

1a:

1b:

2:

3a:

3b:

4:

Figure 1: From left to right: observed data pairs (x, y), true (blue) and estimated (red) functions fY and fX ,
respectively, estimated noise values (eX , eY ) and reconstructed data (x, y) based on the estimated noise. Rows
1a and 1b show two different solutions (minima of the log likelihood) for the same data, as do rows 3a and 3b.
The true models used to generate the data, the p-values for independence of the estimated residuals, and the
negative log-likelihoods are, from top to bottom:

#

1a
1b
2
3a
3b
4

Identiﬁable?

+
+
+
+
+
−

Linear?
−
−
−
−
−
+

Cyclic?
−
−
−
+
+
+

fY (x)

fX (y)

0.9 tanh(2x)
0.9 tanh(2x)
0
0.9 cos(x)
0.9 cos(x)
−0.4x

0
0
0.9 tanh(2x)
0.9 tanh(y)
0.9 tanh(y)
0.8y

σX

1
1
0.5
1
1
0.5

σY

0.5
0.5
1
1
1
1

pEX ⊥⊥ EY
0.76
7 × 10−3
0.74
0.78
3 × 10−58
0.61

L
−2.56 × 103
−2.51 × 103
−2.57 × 103
−2.24 × 103
−2.26 × 103
−2.73 × 103

Acknowledgments

We thank Stefan Maubach and Wieb Bosma for their help with the computer algebra. DJ was supported by
DFG, the German Research Foundation (SPP 1395). TH and JM were supported by NWO, the Netherlands
Organization for Scientiﬁc Research (VICI grant 639.023.604 and VENI grant 639.031.036, respectively).

8

XYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataXYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataXYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataXYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataXYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataXYDataXYX −> fY(X)YXY −> fX(Y)EXEYEstimated noiseXYReconstructed dataReferences
[1] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.
[2] K. A. Bollen. Structural Equations with Latent Variables. John Wiley & Sons, 1989.
[3] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Springer-Verlag,
1993. (2nd ed. MIT Press 2000).
[4] C.W.J. Granger. Investigating causal relations by econometric models and cross-spectral meth-
ods. Econometrica, 37:424438, 1969.
[5] N. Friedman, K. Murphy, and S. Russell. Learning the structure of dynamic probabilistic net-
works. In Proceedings of the Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-98), pages 139–147, 1998.
In Proceedings of
[6] P. Spirtes. Directed cyclic graphical representations of feedback models.
the 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI-95), page 491499, 1995.
[7] T. Richardson. A discovery algorithm for directed cyclic graphs. In Proceedings of the Twelfth
Conference on Uncertainty in Artiﬁcial Intelligence (UAI-1996), 1996.
[8] G. Lacerda, P. Spirtes, J. Ramsey, and P. O. Hoyer. Discovering cyclic causal models by
In Proceedings of the 24th Conference on Uncertainty in
independent components analysis.
Artiﬁcial Intelligence (UAI-2008), 2008.
[9] M. Schmidt and K. Murphy. Modeling discrete interventional data using directed cyclic graph-
ical models. In Proceedings of the 25th Annual Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI-09), 2009.
[10] S. Itani, M. Ohannessian, K. Sachs, G. P. Nolan, and M. A. Dahleh. Structure learning in
In JMLR Workshop and Conference Proceedings, volume 6, page
causal cyclic networks.
165176, 2010.
[11] P.O. Hoyer, D.Janzing, J.M.Mooij, J.Peters, and B.Sch ¨olkopf. Nonlinear causal discovery
with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,
Advances in Neural Information Processing Systems 21 (NIPS*2008), pages 689–696, 2009.
[12] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch ¨olkopf.
Identiﬁability of
In Proceedings of the 27th Annual Conference on
causal graphs using functional models.
Uncertainty in Artiﬁcial Intelligence (UAI-11), 2011.
[13] K. Zhang and A. Hyv ¨arinen. On the identiﬁability of the post-nonlinear causal model. In Pro-
ceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI-09), Montreal,
Canada, 2009.
[14] A.D. Polyanin and V.F. Zaitsev. Handbook of Nonlinear Partial Differential Equations. Chap-
man & Hall / CRC, 2004.
[15] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch ¨olkopf. Kernel methods for
measuring independence. Journal of Machine Learning Research, 6:2075–2129, 2005.

9


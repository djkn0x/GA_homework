Shaping Level Sets with Submodular Functions

Francis Bach
INRIA - Sierra Project-team
Laboratoire d’Informatique de l’Ecole Normale Sup ´erieur e, Paris, France
francis.bach@ens.fr

Abstract
We consider a class of sparsity-inducing regularization terms based on submodular func-
tions. While previous work has focused on non-decreasing functions, we explore sym-
metric submodular functions and their Lov ´asz extensions. We show that the Lov ´asz
extension may be seen as the convex envelope of a function that depends on level sets
(i.e., the set of indices whose corresponding components of the underlying predictor are
greater than a given constant): this leads to a class of convex structured regularization
terms that impose prior knowledge on the level sets, and not only on the supports of the
underlying predictors. We provide uniﬁed optimization alg orithms, such as proximal
operators, and theoretical guarantees (allowed level sets and recovery conditions). By
selecting speciﬁc submodular functions, we give a new inter pretation to known norms,
such as the total variation; we also de ﬁne new norms, in parti cular ones that are based
on order statistics with application to clustering and outlier detection, and on noisy cuts
in graphs with application to change point detection in the presence of outliers.

1 Introduction
The concept of parsimony is central in many scientiﬁc domain s. In the context of statistics, signal
processing or machine learning, it may take several forms. Classically, in a variable or feature
selection problem, a sparse solution with many zeros is sought so that the model is either more
interpretable, cheaper to use, or simply matches available prior knowledge (see, e.g., [1, 2, 3] and
references therein). In this paper, we instead consider sparsity-inducing regularization terms that
will lead to solutions with many equal values. A classical example is the total variation in one or
two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various
image labelling problems [6, 5], or change point detection tasks [7, 8, 9]. Another example is the
“Oscar ” penalty which induces automatic grouping of the fea
tures [10]. In this paper, we follow
the approach of [3], who designed sparsity-inducing norms based on non-decreasing submodular
functions, as a convex approximation to imposing a speciﬁc p rior on the supports of the predictors.
Here, we show that a similar parallel holds for some other class of submodular functions, namely
non-negative set-functions which are equal to zero for the full and empty set. Our main instance of
such functions are symmetric submodular functions.
We make the following contributions:
− We provide in Section 3 explicit links between priors on level sets and certain submodular
functions: we show that the Lov ´asz extensions (see, e.g., [ 11] and a short review in Section 2)
associated to these submodular functions are the convex envelopes (i.e., tightest convex lower
bounds) of speciﬁc functions that depend on all level sets of
the underlying vector.
− In Section 4, we reinterpret existing norms such as the total variation and design new norms,
based on noisy cuts or order statistics. We propose applications to clustering and outlier de-
tection, as well as to change point detection in the presence of outliers.
− We provide uniﬁed algorithms in Section 5, such as proximal o perators, which are based on a
sequence of submodular function minimizations (SFMs), when such SFMs are efﬁcient, or by
adapting the generic slower approach of [3] otherwise.
− We derive uniﬁed theoretical guarantees for level set recov ery in Section 6, showing that even
in the absence of correlation between predictors, level set recovery is not always guaranteed,
a situation which is to be contrasted with traditional support recovery situations [1, 3].

1

Notation. For w ∈ Rp and q ∈ [1, ∞], we denote by kwkq the ℓq -norm of w. Given a subset A
of V = {1, . . . , p}, 1A ∈ {0, 1}p is the indicator vector of the subset A. Moreover, given a vector
w and a matrix Q, wA and QAA denote the corresponding subvector and submatrix of w and Q.
Finally, for w ∈ Rp and A ⊂ V , w(A) = Pk∈A wk = w⊤ 1A (this de ﬁnes a modular set-function).
In this paper, for a certain vector w ∈ Rp , we call level sets the sets of indices which are larger (or
smaller) or equal to a certain constant α, which we denote {w > α} (or {w 6 α}), while we call
constant sets the sets of indices which are equal to a constant α, which we denote {w = α}.
2 Review of Submodular Analysis

In this section, we review relevant results from submodular analysis. For more details, see, e.g., [12],
and, for a review with proofs derived from classical convex analysis, see, e.g., [11].
De ﬁnition.
Throughout this paper, we consider a submodular function F de ﬁned on the power set
2V of V = {1, . . . , p}, i.e., such that ∀A, B ⊂ V , F (A) + F (B ) > F (A ∪ B ) + F (A ∩ B ). Unless
otherwise stated, we consider functions which are non-negative (i.e., such that F (A) > 0 for all A ⊂
V ), and that satisfy F (∅) = F (V ) = 0. Usual examples are symmetric submodular functions, i.e.,
such that ∀A ⊂ V , F (V \A) = F (A), which are known to always have non-negative values. We give
several examples in Section 4; for illustrating the concepts introduced in this section and Section 3,
we will consider the cut in an undirected chain graph, i.e., F (A) = Pp−1
j=1 |(1A )j − (1A )j+1 |.
Lov ´asz extension. Given any set-function F such that F (V ) = F (∅) = 0, one can de ﬁne its
Lov ´asz extension f : Rp → R, as f (w) = RR F ({w > α})dα (see, e.g., [11] for this particular
formulation). The Lov ´asz extension is convex if and only if F is submodular. Moreover, f is
piecewise-linear and for all A ⊂ V , f (1A) = F (A), that is, it is indeed an extension from 2V
(which can be identiﬁed to {0, 1}p through indicator vectors) to Rp . Finally, it is always positively
homogeneous. For the chain graph, we obtain the usual total variation f (w) = Pp−1
j=1 |wj − wj+1 |.
Base polyhedron. We denote by B (F ) = {s ∈ Rp , ∀A ⊂ V , s(A) 6 F (A), s(V ) = F (V )}
the base polyhedron [12], where we use the notation s(A) = Pk∈A sk . One important result in
submodular analysis is that if F is a submodular function, then we have a representation of f as a
maximum of linear functions [12, 11], i.e., for all w ∈ Rp , f (w) = maxs∈B (F ) w⊤ s. Moreover,
instead of solving a linear program with 2p − 1 contraints, a solution s may be obtained by the
following “greedy algorithm ”: order the components of w in decreasing order wj1 > · · · > wjp ,
and then take for all k ∈ {1, . . . , p}, sjk = F ({j1 , . . . , jk }) − F ({j1 , . . . , jk−1 }).
Tight and inseparable sets. The polyhedra U = {w ∈ Rp , f (w) 6 1} and B (F ) are polar to each
other (see, e.g., [13] for de ﬁnitions and properties of pola r sets). Therefore, the facial structure of U
may be obtained from the one of B (F ). Given s ∈ B (F ), a set A ⊂ V is said tight if s(A) = F (A).
It is known that the set of tight sets is a distributive lattice, i.e., if A and B are tight, then so are A ∪B
and A ∩ B [12, 11]. The faces of B (F ) are thus intersections of hyperplanes {s(A) = F (A)} for
A belonging to certain distributive lattices (see Prop. 3). A set A is said separable if there exists a
non-trivial partition of A = B ∪ C such that F (A) = F (B ) + F (C ). A set is said inseparable if it
is not separable. For the cut in an undirected graph, inseparable sets are exactly connected sets.

3 Properties of the Lov ´asz Extension

In this section, we derive properties of the Lov ´asz extensi on for submodular functions, which go
beyond convexity and homogeneity. Throughout this section, we assume that F is a non-negative
submodular set-function that is equal to zero at ∅ and V . This immediately implies that f is invariant
by addition of any constant vector (that is, f (w + α1V ) = f (w) for all w ∈ Rp and α ∈ R), and
that f (1V ) = F (V ) = 0. Thus, contrary to the non-decreasing case [3], our regularizers are not
norms. However, they are norms on the hyperplane {w⊤ 1V = 0} as soon as for A 6= ∅ and A 6= V ,
F (A) > 0 (which we assume for the rest of this paper).
We now show that the Lov ´asz extension is the convex envelope of a certain combinatorial function
which does depend on all levets sets {w > α} of w ∈ Rp (see proof in [14]):
Proposition 1 (Convex envelope) The Lov ´asz extension f (w) is the convex envelope of the function
w 7→ maxα∈R F ({w > α}) on the set [0, 1]p + R1V = {w ∈ Rp , maxk∈V wk − mink∈V wk 6 1}.

2

1w =w
2

w > w >w1
2
3

(0,0,1)/F({3})
w > w >w
2
1
3

(1,0,1)/F({1,3})

1w > w >w3
2

(1,0,0)/F({1})

(0,1,1)/F({2,3})

w > w >w
3
1
2

(1,0,1)/2

(0,1,0)/F({2})
w =w1
3

(1,0,0)

(0,0,1)

(0,1,1)

(0,1,0)/2

2w =w
3

w > w >w
2
3
1

2w > w >w1
3
(1,1,0)
(1,1,0)/F({1,2})
Figure 1: Top: Polyhedral level set of f (projected on the set w⊤ 1V = 0), for 2 different submodular
symmetric functions of three variables, with different inseparable sets leading to different sets of
extreme points; changing values of F may make some of the extreme points disappear. The various
extreme points cut the space into polygons where the ordering of the component is ﬁxed. Left:
F (A) = 1|A|∈{1,2} , leading to f (w) = maxk wk − mink wk (all possible extreme points); note
that the polygon need not be symmetric in general. Right: one-dimensional total variation on three
nodes, i.e., F (A) = |11∈A − 12∈A | + |12∈A − 13∈A |, leading to f (w) = |w1 − w2 | + |w2 − w3 |, for
which the extreme points corresponding to the separable set {1, 3} and its complement disappear.

Note the difference with the result of [3]: we consider here a different set on which we compute the
convex envelope ([0, 1]p + R1V instead of [−1, 1]p ), and not a function of the support of w, but of
all its level sets.1 Moreover, the Lov ´asz extension is a convex relaxation of a f unction of level sets
(of the form {w > α}) and not of constant sets (of the form {w = α}). It would have been perhaps
more intuitive to consider for example RR F ({w = α})dα, since it does not depend on the ordering
of the values that w may take; however, to the best of our knowledge, the latter function does not
lead to a convex function amenable to polynomial-time algorithms. This de ﬁnition through level
sets will generate some potentially undesired behavior (such as the well-known staircase effect for
the one-dimensional total variation), as we show in Section 6.
The next proposition describes the set of extreme points of the “unit ball” U = {w, f (w) 6 1},
giving a ﬁrst illustration of sparsity-inducing effects (s ee example in Figure 1, in particular for the
one-dimensional total variation).
Proposition 2 (Extreme points) The extreme points of the set U ∩ {w⊤ 1V = 0} are the projections
of the vectors 1A/F (A) on the plane {w⊤ 1V = 0}, for A such that A is inseparable for F and V \A
is inseparable for B 7→ F (A ∪ B ) − F (A).
Partially ordered sets and distributive lattices. A subset D of 2V is a (distributive) lattice if it
is invariant by intersection and union. We assume in this paper that all lattices contain the empty
set ∅ and the full set V , and we endow the lattice with the inclusion order. Such lattices may be
represented as a partially ordered set (poset) Π(D) = {A1 , . . . , Am} (with order relationship <),
where the sets Aj , j = 1, . . . , m, form a partition of V (we always assume a topological ordering
of the sets, i.e., Ai < Aj ⇒ i > j ). As illustrated in Figure 2, we go from D to Π(D), by
considering all maximal chains in D and the differences between consecutive sets. We go from
Π(D) to D, by constructing all ideals of Π(D), i.e., sets J such that if an element of Π(D) is lower
than an element of J , then it has to be in J (see [12] for more details, and an example in Figure 2).
Distributive lattices and posets are thus in one-to-one correspondence. Throughout this section, we
go back and forth between these two representations. The distributive lattice D will correspond to
all authorized level sets {w > α} for w in a single face of U , while the elements of the poset Π(D)
are the constant sets (over which w is constant), with the order between the subsets giving partial
constraints between the values of the corresponding constants.
Faces of U . The faces of U are characterized by lattices D, with their corresponding posets Π(D) =
{A1 , . . . , Am}. We denote by U ◦
D (and by UD its closure) the set of w ∈ Rp such that (a) f (w) 6 1,
(b) w is piecewise constant with respect to Π(D), with value vi on Ai , and (c) for all pairs (i, j ),
1Note that the support {w = 0} is a constant set which is the intersection of two level sets.

3

{2}

{1,2}

{1,2,5,6}

{1,2,3,4,5,6}

{2}

{5,6}

{5,6}
{3,4}
{1}
{2,5,6}
{2,3,4,5,6}
Figure 2: Left: distributive lattice with 7 elements in 2{1,2,3,4,5,6} , represented with the Hasse dia-
gram corresponding to the inclusion order (for a partial order, a Hasse diagram connects A to B if
A is smaller than B and there is no C such that A is smaller than C and C is smaller than B ). Right:
corresponding poset, with 4 elements that form a partition of {1, 2, 3, 4, 5, 6}, represented with the
Hasse diagram corresponding to the order < (a node points to its immediate smaller node according
to <). Note that this corresponds to an “allowed ” lattice (see Pr op. 3) for the one-dimensional total
variation.

Ai < Aj ⇒ vi > vj . For certain lattices D, these will be exactly the relative interiors of all faces
of U (see proof in [14]):
Proposition 3 (Faces of U ) The (non-empty) relative interiors of all faces of U are exactly of the
form U ◦
D , where D is a lattice such that:
(i) the restriction of F to D is modular, i.e., for all A, B ∈ D, F (A)+F (B ) = F (A∪B )+F (A∩B ),
(ii) for all j ∈ {1, . . . , m}, the set Aj is inseparable for the function Cj 7→ F (Bj−1 ∪ Cj ) −
F (Bj−1 ), where Bj−1 is the union of all ancestors of Aj in Π(D),
(iii) among all lattices corresponding to the same unordered partition, D is a maximal element of
the set of lattices satisfying (i) and (ii).

Among the three conditions, the second one is the easiest to interpret, as it reduces to having constant
sets which are inseparable for certain submodular functions, and for cuts in an undirected graph,
these will exactly be connected sets. Note also that extreme points from Prop. 2 are recovered with
D = {∅, A, V }.
Since we are able to characterize all faces of U (of all dimensions) with non-empty relative interior,
we have a partition of the space and any w ∈ Rp which is not proportional to 1V , will be, up to
the strictly positive constant f (w), in exactly one of these relative interiors of faces; we refer to this
lattice as the lattice associated to w. Note that from the face w belongs to, we have strong constraints
on the constant sets, but we may not be able to determine all level sets of w, because only partial
constraints are given by the order on Π(D). For example, in Figure 2 for the one-dimensional total
variation, w2 may be larger or smaller than w5 = w6 (and even potentially equal, but with zero
probability, see Section 6).

4 Examples of Submodular Functions

In this section, we provide examples of submodular functions and of their Lov ´asz extensions. Some
are well-known (such as cut functions and total variations), some are new in the context of supervised
learning (regular functions), while some have interesting effects in terms of clustering or outlier
detection (cardinality-based functions).
Symmetrization. From any submodular function G, one may de ﬁne F (A) = G(A) + G(V \A) −
G(∅) − G(V ), which is symmetric. Potentially interesting examples which are beyond the scope of
this paper are mutual information, or functions of eigenvalues of submatrices [3].
Cut functions. Given a set of nonnegative weights d : V × V → R+ , de ﬁne the cut F (A) =
Pk∈A,j∈V \A d(k , j ). The Lov ´asz extension is equal to f (w) = Pk,j∈V d(k , j )(wk − wj )+ (which
shows submodularity because f is convex), and is often referred to as the total variation. If the
weight function d is symmetric, then the submodular function is also symmetric. In this case, it can
be shown that inseparable sets for functions A 7→ F (A ∪ B ) − F (B ) are exactly connected sets.
Hence, by Props. 3 and 6, constant sets are connected sets, which is the usual justiﬁcation behind
the total variation. Note however that some con ﬁgurations o f connected sets are not allowed due
to the other conditions in Prop. 3 (see examples in Section 6). In Figure 5 (right plot), we give an
example of the usual chain graph, leading to the one-dimensional total variation [4, 5]. Note that
these functions can be extended to cuts in hypergraphs, which may have interesting applications in
computer vision [6]. Moreover, directed cuts may be interesting to favor increasing or decreasing
jumps along the edges of the graph.

4

s
t
h
g
i
e
w

5

0

−5

s
t
h
g
i
e
w

5

0

−5

s
t
h
g
i
e
w

5

0

−5

 
 
 

TV
TV
TV
robust TV
robust TV
robust TV
robust TV − 2
robust TV − 2

0.4
0.4
0.4

0.3
0.3
0.3

0.2
0.2
0.2

0.1
0.1
0.1

r
r
r
o
o
o
r
r
r
r
r
r
e
e
e
 
 
 
n
n
n
o
o
o
i
i
i
t
t
t
a
a
a
m
m
m
i
i
i
t
t
t
s
s
s
e
e
e

2
2
2
5
20
15
10
5
20
15
10
5
20
15
10
log(σ2)
log(σ2)
log(σ2)
Figure 3: Three left plots: Estimation of noisy piecewise constant 1D signal with outliers (indices
5 and 15 in the chain of 20 nodes). Left: original signal. Middle: best estimation with total variation
(level sets are not correctly estimated). Right: best estimation with the robust total variation based on
noisy cut functions (level sets are correctly estimated, with less bias and with detection of outliers).
Right plot: clustering estimation error vs. noise level, in a sequence of 100 variables, with a single
jump, where noise of variance one is added, with 5% of outliers (averaged over 20 replications).

0
0
0

0
0
0
 
 
 
−2
−2
−2

4
4
4

6
6
6

Regular functions and robust total variation. By partial minimization, we obtain so-called
regular functions [6, 5]. One application is “noisy cut functions”: for a given weight function
d : W × W → R+ , where each node in W is uniquely associated in a node in V , we consider
the submodular function obtained as the minimum cut adapted to A in the augmented graph (see
an example in the right plot of Figure 5): F (A) = minB⊂W Pk∈B , j∈W \B d(k , j ) + λ|A∆B |.
This allows for robust versions of cuts, where some gaps may be tolerated; indeed, compared to
having directly a small cut for A, B needs to have a small cut and be close to A, thus allowing
some elements to be removed or added to A in order to lower the cut. See examples in Figure 3,
illustrating the behavior of the type of graph displayed in the bottom-right plot of Figure 5, where
the performance of the robust total variation is signiﬁcant ly more stable in presence of outliers.
Cardinality-based functions. For F (A) = h(|A|) where h is such that h(0) = h(p) = 0 and h
concave, we obtain a submodular function, and a Lov ´asz exte nsion that depends on the order statis-
tics of w, i.e., if wj1 > · · · > wjp , then f (w) = Pp−1
k=1 h(k)(wjk − wjk+1 ). While these examples do
not provide signiﬁcantly different behaviors for the non-d ecreasing submodular functions explored
by [3] (i.e., in terms of support), they lead to interesting behaviors here in terms of level sets, i.e.,
they will make the components w cluster together in speciﬁc ways. Indeed, as shown in Sectio n 6,
allowed constant sets A are such that A is inseparable for the function C 7→ h(|B ∪ C |) − h(|B |)
(where B ⊂ V is the set of components with higher values than the ones in A), which imposes that
the concave function h is not linear on [|B |, |B |+ |A|]. We consider the following examples:
1. F (A) = |A| · |V \A|, leading to f (w) = Pp
i,j=1 |wi − wj |. This function can thus be also
seen as the cut in the fully connected graph. All patterns of level sets are allowed as the
function h is strongly concave (see left plot of Figure 4). This function has been extended
in [15] by considering situations where each wj is a vector, instead of a scalar, and replacing
the absolute value |wi − wj | by any norm kwi − wj k, leading to convex formulations for
clustering.
2. F (A) = 1 if A 6= ∅ and A 6= V , and 0 otherwise, leading to f (w) = maxi,j |wi − wj |. Two
large level sets at the top and bottom, all the rest of the variables are in-between and separated
(Figure 4, second plot from the left).
3. F (A) = max{|A|, |V \A|}. This function is piecewise afﬁne, with only one kink, thus o nly
one level set of cardinalty greater than one (in the middle) is possible, which is observed in
Figure 4 (third plot from the left). This may have applications to multivariate outlier detection
by considering extensions similar to [15].

5 Optimization Algorithms

In this section, we present optimization methods for minimizing convex objective functions regular-
ized by the Lov ´asz extension of a submodular function. Thes e lead to convex optimization problems,
which we tackle using proximal methods (see, e.g., [16, 17] and references therein). We ﬁrst start
by mentioning that subgradients may easily be derived (but subgradient descent is here rather inef-
ﬁcient as shown in Figure 5). Moreover, note that with the squ are loss, the regularization paths are
piecewise afﬁne, as a direct consequence of regularizing by a polyhedral function.

5

10

5

0

−5

s
t
h
g
i
e
w

10

5

0

−5

s
t
h
g
i
e
w

10

5

0

−5

s
t
h
g
i
e
w

10

5

0

−5

s
t
h
g
i
e
w

−10

0

−10

0

1

2

3

−10

0

−10

0

0.03

0.01

0.02
λ
λ
λ
λ
Figure 4: Left: Piecewise linear regularization paths of proximal problems (Eq. (1)) for different
functions of cardinality. From left to right: quadratic function (all level sets allowed), second ex-
ample in Section 4 (two large level sets at the top and bottom), piecewise linear with two pieces (a
single large level set in the middle). Right: Same plot for the one-dimensional total variation. Note
that in all these particular cases the regularization paths for orthogonal designs are agglomerative
(see Section 5), while for general designs, they would still be piecewise afﬁne but not agglomerative.

0.2

0.4

1

2

3

Subgradient. From f (w) = maxs∈B (F ) s⊤w and the greedy algorithm2 presented in Section 2,
one can easily get in polynomial time one subgradient as one of the maximizers s. This allows to
use subgradient descent, with slow convergence compared to proximal methods (see Figure 5).
Proximal problems through sequences of submodular function minimizations (SFMs). Given
regularized problems of the form minw∈Rp L(w) + λf (w), where L is differentiable with Lipschitz-
continuous gradient, proximal methods have been shown to be particularly efﬁcient ﬁrst-order
methods (see, e.g., [16]).
In this paper, we use the method “I STA” and its accelerated variant
“FISTA” [16]. To apply these methods, it sufﬁces to be able to
solve efﬁciently:
1
2 kw − z k2
min
2 + λf (w),
w∈Rp
which we refer to as the proximal problem. It is known that solving the proximal problem is related
to submodular function minimization (SFM). More precisely, the minimum of A 7→ λF (A) − z (A)
may be obtained by selecting negative components of the solution of a single proximal problem [12,
11]. Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p
submodular function minimizations of the form A 7→ λF (A) − z (A), by a decomposition algorithm
adapted from [18], and described in [11].
Thus, computing the proximal operator has polynomial complexity since SFM has polynomial com-
plexity. However, it may be too slow for practical purposes, as the best generic algorithm has
complexity O(p6 ) [19]3 . Nevertheless, this strategy is efﬁcient for families of su bmodular functions
for which dedicated fast algorithms exist:

(1)

– Cuts: Minimizing the cut or the partially minimized cut, plus a modular function, may be
done with a min-cut/max- ﬂow algorithm [see, e.g., 6, 5]. For proximal methods, we need in
fact to solve an instance of a parametric max- ﬂow problem, which may be done using other
efﬁcient dedicated algorithms [21, 5] than the decompositi on algorithm derived from [18].
– Functions of cardinality: minimizing functions of the form A 7→ λF (A) − z (A) can be done
in closed form by sorting the elements of z .

Proximal problems through minimum-norm-point algorithm.
In the generic case (i.e., beyond
cuts and cardinality-based functions), we can follow [12, 3]: since f (w) is expressed as a mini-
mum of linear functions, the problem reduces to the projection on the polytope B (F ), for which we
happen to be able to easily maximize linear functions (using the greedy algorithm described in Sec-
tion 2). This can be tackled efﬁciently by the minimum-norm- point algorithm [12], which iterates
between orthogonal projections on afﬁne subspaces and the g reedy algorithm for the submodular
function4. We compare all optimization methods on synthetic examples in Figure 5.

2The greedy algorithm to ﬁnd extreme points of the base polyhe dron should not be confused with the greedy
algorithm (e.g., forward selection) that is common in supervised learning/statistics.
3Note that even in the case of symmetric submodular functions, where more efﬁcient algorithms in O(p3 )
for submodular function minimization (SFM) exist [20], the minimization of functions of the form λF (A) −
z (A) is provably as hard as general SFM [20].
4 Interestingly, when used for submodular function minimization (SFM), the minimum-norm-point algo-
rithm has no complexity bound but is empirically faster than algorithms with such bounds [12].

6

100

10−5

10−10

10−15

)
f
(
n
i
m
−
)
w
(
f

 

fista−generic
ista−generic
subgradient
fista−card
ista−card
subgradient−sqrt

W

 
0

2

8

10

V
4
6
time (seconds)
Figure 5: Left: Matlab running times of different optimization methods on 20 replications of a least-
squares regression problem with p = 1000 for a cardinality-based submodular function (best seen
in color). Proximal methods with the generic algorithm (using the minimum-norm-point algorithm)
are faster than subgradient descent (with two schedules for the learning rate, 1/t or 1/√t). Using the
dedicated algorithm (which is not available in all situations) is signiﬁcantly faster. Right: Examples
of graphs (top: chain graph, bottom: hidden chain graph, with sets W and V and examples of a set
A in light red, and B in blue, see text for details).

Proximal path as agglomerative clustering. When λ varies from zero to +∞, then the unique
optimal solution of Eq. (1) goes from z to a constant. We now provide conditions under which
the regularization path of the proximal problem may be obtained by agglomerative clustering (see
examples in Figure 4):
Proposition 4 (Agglomerative clustering) Assume that for all sets A, B such that B ∩ A = ∅ and
A is inseparable for D 7→ F (B ∪ D) − F (B ), we have:
∀C ⊂ A, |C |
|A| [F (B ∪ A) − F (B )] 6 F (B ∪ C ) − F (B ).
Then the regularization path for Eq. (1) is agglomerative, that is, if two variables are in the same
constant for a certain µ ∈ R+ , so are they for all larger λ > µ.
As shown in [14], the assumptions required for by Prop. 4 are satisﬁed by (a) all submodular set-
functions that only depend on the cardinality, and (b) by the one-dimensional total variation —we
thus recover and extend known results from [7, 22, 15].
Adding an ℓ1 -norm. Following [4], we may add the ℓ1 -norm kwk1 for additional sparsity of w (on
top of shaping its level sets). The following proposition extends the result for the one-dimensional
total variation [4, 23] to all submodular functions and their Lov ´asz extensions:

(2)

Proposition 5 (Proximal problem for ℓ1 -penalized problems) The unique minimizer of 1
2 kw −
2 + f (w) + λkwk1 may be obtained by soft-thresholding the minimizers of 1
2 kw − z k2
z k2
2 + f (w).
That is, the proximal operator for f + λk · k1 is equal to the composition of the proximal operator
for f and the one for λk · k1 .

6 Sparsity-inducing Properties
Going from the penalization of supports to the penalization of level sets introduces some complexity
and for simplicity in this section, we only consider the analysis in the context of orthogonal design
matrices, which is often referred to as the denoising problem, and in the context of level set esti-
mation already leads to interesting results. That is, we study the unique global minimum ˆw of the
proximal problem in Eq. (1) and make some assumption regarding z (typically z = w∗ + noise), and
provide guarantees related to the recovery of the level sets of w∗ . We ﬁrst start by characterizing the
allowed level sets, showing that the partial constraints de ﬁned in Section 3 on faces of {f (w) 6 1}
do not create by chance further groupings of variables (see proof in [14]).
Proposition 6 (Stable constant sets) Assume z ∈ Rp has an absolutely continuous density with
respect to the Lebesgue measure. Then, with probability one, the unique minimizer ˆw of Eq. (1) has
constant sets that de ﬁne a partition corresponding to a latt ice D de ﬁned in Prop. 3.
We now show that under certain conditions the recovered constant sets are the correct ones:

7

Theorem 1 (Level set recovery) Assume that z = w∗ + σε, where ε ∈ Rp is a standard Gaus-
sian random vector, and w∗ is consistent with the lattice D and its associated poset Π(D) =
(A1 , . . . , Am ), with values v∗
j on Aj , for j ∈ {1, . . . , m}. Denote Bj = A1 ∪ · · · ∪ Aj for
j ∈ {1, . . . , m}. Assume that there exists some constants ηj > 0 and ν > 0 such that:
|Aj | [F (Bj−1 ∪Aj )− F (Bj−1 )] > ηj min(cid:8) |Cj |
|Aj | , 1− |Cj |
∀Cj ⊂ Aj , F (Bj−1 ∪Cj )− F (Bj−1 )− |Cj |
|Aj | (cid:9), (3)
(4)
∀i, j ∈ {1, . . . , m}, Ai < Aj ⇒ v∗
i − v∗
j > ν,
F (Bj )−F (Bj−1 )
(5)
(cid:12)(cid:12) 6 ν /4.
∀j ∈ {1, . . . , m}, λ(cid:12)(cid:12)
|Aj |
Then the unique minimizer ˆw of Eq. (1) is associated to the same lattice D than w∗ , with probability
λ2 η2
j=1 exp (cid:0) − ν 2 |Aj |
greater than 1 − Pm
32σ2 (cid:1) − 2 Pm
2σ2 |Aj |2 (cid:1).
j=1 |Aj | exp (cid:0) −
j
We now discuss the three main assumptions of Theorem 1 as well as the probability estimate:
– Eq. (3) is the equivalent of the support recovery condition for the Lasso [1] or its exten-
sions [3]. The main difference is that for support recovery, this assumption is always met
for orthogonal designs, while here it is not always met. Interestingly, the validity of level set
recovery implies the agglomerativity of proximal paths (Eq. (2) in Prop. 4).
Note that if Eq. (3) is satisﬁed only with ηj > 0 (it is then exactly Eq. (2) in Prop. 4), then,
even with in ﬁnitesimal noise, one can show that in some cases , the wrong level sets may be
obtained with non vanishing probability, while if ηj is strictly negative, one can show that
in some cases, we never get the correct level sets. Eq. (3) is thus essentially sufﬁc ient and
necessary.
– Eq. (4) corresponds to having distinct values of w∗ far enough from each other.
– Eq. (5) is a constraint on λ which controls the bias of the estimator: if it is too large, then there
may be a merging of two clusters.
– In the probability estimate, the second term is small if all σ2 |Aj |−1 are small enough (i.e.,
given the noise, there is enough data to correctly estimate the values of the constant sets) and
the third term is small if λ is large enough, to avoid that clusters split.

One-dimensional total variation.
In this situation, we always get ηj = 0, but in some cases, it
cannot be improved (i.e., the best possible ηj is equal to zero), and as shown in [14], this occurs
as soon as there is a “staircase ”, i.e., a piecewise constant
vector, with a sequence of at least two
consecutive increases, or two consecutive decreases, showing that in the presence of such staircases,
one cannot have consistent support recovery, which is a well-known issue in signal processing (typ-
ically, more steps are created). If there is no staircase effect, we have ηj = 1 and Eq. (5) becomes
8 minj |Aj |. If we take λ equal to the limiting value in Eq. (5), then we obtain a probability
λ 6 ν
less than 1 − 4p exp(− ν 2 minj |Aj |2
128σ2 maxj |Aj |2 ). Note that we could also derive general results when an ad-
ditional ℓ1 -penalty is used, thus extending results from [24]. Finally, similar (more) negative results
may be obtained for the two-dimensional total variation [25, 14].
Clustering with F (A) = |A| · |V \A|.
In this case, we have ηj = |Aj |/2, and Eq. (5) becomes
4p , leading to the probability of correct support estimation greater than 1 − 4p exp (cid:0) − ν 2
128pσ2 (cid:1).
λ 6 ν
This indicates that the noise variance σ2 should be small compared to 1/p, which is not satisfactory
and would be corrected with the weighting schemes proposed in [15].

7 Conclusion

We have presented a family of sparsity-inducing norms dedicated to incorporating prior knowledge
or structural constraints on the level sets of linear predictors. We have provided a set of common al-
gorithms and theoretical results, as well as simulations on synthetic examples illustrating the behav-
ior of these norms. Several avenues are worth investigating: ﬁrst, we could follow current practice in
sparse methods, e.g., by considering related adapted concave penalties to enhance sparsity-inducing
capabilities, or by extending some of the concepts for norms of matrices, with potential applications
in matrix factorization [26] or multi-task learning [27].

Acknowledgements. This paper was partially supported by the Agence Nationale de la Recherche
(MGA Project), the European Research Council (SIERRA Project) and Digiteo (BIOVIZ project).

8

References
[1] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning
Research, 7:2541 –2563, 2006.
[2] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-
dimensional analysis of M-estimators with decomposable regularizers. In Adv. NIPS, 2009.
[3] F. Bach. Structured sparsity-inducing norms through submodular functions. In Adv. NIPS,
2010.
[4] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the
fused Lasso. J. Roy. Stat. Soc. B, 67(1):91 –108, 2005.
[5] A. Chambolle and J. Darbon. On total variation minimization and surface evolution using
parametric maximum ﬂows.
International Journal of Computer Vision, 84(3):288 –307, 2009.
[6] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts.
IEEE Trans. PAMI, 23(11):1222 –1239, 2001.
[7] Z. Harchaoui and C. L ´evy-Leduc. Catching change-point s with Lasso. Adv. NIPS, 20, 2008.
[8] J.-P. Vert and K. Bleakley. Fast detection of multiple change-points shared by many signals
using group LARS. Adv. NIPS, 23, 2010.
[9] M. Kolar, L. Song, and E. Xing. Sparsistent learning of varying-coefﬁcient models with struc-
tural changes. Adv. NIPS, 22, 2009.
[10] H. D. Bondell and B. J. Reich. Simultaneous regression shrinkage, variable selection, and
supervised clustering of predictors with oscar. Biometrics, 64(1):115 –123, 2008.
[11] F. Bach. Convex analysis and optimization with submodular functions: a tutorial. Technical
Report 00527714, HAL, 2010.
[12] S. Fujishige. Submodular Functions and Optimization. Elsevier, 2005.
[13] R. T. Rockafellar. Convex Analysis. Princeton University Press, 1997.
[14] F. Bach. Shaping level sets with submodular functions. Technical Report 00542949-v2, HAL,
2011.
[15] T. Hocking, A. Joulin, F. Bach, and J.-P. Vert. Clusterpath: an algorithm for clustering using
convex fusion penalties. In Proc. ICML, 2011.
[16] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183 –202, 2009.
[17] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penal-
ties. Technical Report 00613125, HAL, 2011.
[18] H. Groenevelt. Two algorithms for maximizing a separable concave function over a polyma-
troid feasible region. European Journal of Operational Research, 54(2):227 –236, 1991.
[19] J. B. Orlin. A faster strongly polynomial time algorithm for submodular function minimization.
Mathematical Programming, 118(2):237 –251, 2009.
[20] M. Queyranne. Minimizing symmetric submodular functions. Mathematical Programming,
82(1):3 –12, 1998.
[21] G. Gallo, M. D. Grigoriadis, and R. E. Tarjan. A fast parametric maximum ﬂow algorithm and
applications. SIAM Journal on Computing, 18(1):30 –55, 1989.
[22] H. Hoe ﬂing. A path algorithm for the fused Lasso signal a pproximator. Technical Report
0910.0526v1, arXiv, 2009.
[23] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse
coding. Journal of Machine Learning Research, 11:19 –60, 2010.
[24] A. Rinaldo. Properties and re ﬁnements of the fused Lass o. Ann. Stat., 37(5):2922 –2952, 2009.
[25] V. Duval, J.-F. Aujol, and Y. Gousseau. The TVL1 model: A geometric point of view. Multi-
scale Modeling and Simulation, 8(1):154 –189, 2009.
[26] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Adv.
NIPS 17, 2005.
[27] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-
ing, 73(3):243 –272, 2008.

9


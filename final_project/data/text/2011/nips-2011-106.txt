Learning	  to	  Learn	  with	  
Compound	  HD	  Models	  

Russ	  Salakhutdinov	  
Department	  of	  Sta<s<cs,	  
University	  of	  Toronto	  

Joshua	  Tenenbaum	  
Brain	  and	  Cogni<ve	  Sciences	  
MIT	  

Antonio	  Torralba	  
CSAIL,	  MIT	  

One shot learning of simple visual concepts
Hierarchical-­‐Deep	  Models	  
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
One-­‐Shot	  Learning	  

Super-­‐category	  

HD	  Models:	  Integrate	  
Abstract
hierarchical	  Bayesian	  models	  
People can learn visual concepts from just one example, but
it remains a mystery how this is accomplished. Many authors
with	  deep	  networks.	  
have proposed that transferred knowledge from more familiar
concepts is a route to one shot learning, but what is the form
of this abstract knowledge? One hypothesis is that the shar-
ing of parts is core to one shot learning, and we evaluate this
idea in the domain of handwritten characters, using a massive
new dataset. These simple visual concepts have a rich inter-
Hierarchical	  Bayes:	  
nal part structure, yet they are particularly tractable for com-
putational models. We introduce a generative model of how
Figure 1: Test yourself on one shot learning. From the example
	   • 	  Learn	  hierarchies	  of	  categories	  for	  sharing	  
characters are composed from strokes, where knowledge from
boxed in red, can you ﬁnd the others in the array? On the left is a
previous characters helps to infer the latent strokes in novel
Segway and on the right is the ﬁrst character of the Bengali alphabet.
characters. The stroke model outperforms a competing state-
abstract	  knowledge.	  
of-the-art character model on a challenging one shot learning
AnswerfortheBengalicharacter:Row2,Column3;Row4,Column2.
• 	  Explicitly	  share	  parameters	  that	  are	  relevant	  
task, and it provides a good ﬁt to human perceptual data.
Shared	  higher-­‐level	  features	  
Keywords: category learning;
transfer learning; Bayesian
modeling; neural networks
to	  learning	  new	  concept.	  
A hallmark of human cognition is learning from just a few
examples. For instance, a person only needs to see one Seg-
Deep	  Networks:	  
way to acquire the concept and be able to discriminate future
	   • 	  Learn	  hierarchies	  of	  features.	  
Segways from other vehicles like scooters and unicycles (Fig.
1 left). Similarly, children can acquire a new word from one
• 	  Unsupervised	  feature	  learning	  –	  no	  need	  
encounter (Carey & Bartlett, 1978). How is one shot learning
possible?
to	  rely	  on	  human-­‐craJed	  input	  features.	  
New concepts are almost never learned in a vacuum. Past
• 	  Distributed	  representa=ons.	  	  
Figure 2: Examples from a new 1600 character database.
experience with other concepts in a domain can support the
useful representational basis for many different vehicle and
rapid learning of novel concepts, by showing the learner what
artifact concepts – a representation that is likely learned in
matters for generalization. Many authors have suggested this
the course of learning the concepts that they support. Several
as a route to one shot learning: transfer of abstract knowledge
papers from the recent machine learning and computer vision
from old to new concepts, often called transfer learning, rep-
literature argue for such an approach: joint learning of many
resentation learning, or learning to learn. But what is the
concepts and a high-level part vocabulary that underlies those
nature of the learned abstract knowledge that lets humans ac-
concepts (e.g., Torralba, Murphy, & Freeman, 2007; Fei-Fei,
quire new object concepts so quickly?
Fergus, & Perona, 2006). Another recently popular machine
The most straightforward proposals invoke attentional
learning approach is based on deep learning (Salakhutdinov
learning (Smith, Jones, Landau, Gershkoff-Stowe, & Samuel-
& Hinton, 2009): unsupervised learning of hierarchies of dis-
son, 2002) or overhypotheses (Kemp, Perfors, & Tenenbaum,
tributed feature representations in neural-network-style prob-
2007; Dewar & Xu, in press), like the shape bias in word
abilistic generative models. These models do not specify ex-
learning. Prior experience with concepts that are clearly orga-
plicit parts and structural relations, but they can still construct
nized along one dimension (e.g., shape, as opposed to color or
meaningful representations of what makes two objects deeply
material) draws a learner’s attention to that same dimension
similar that go substantially beyond low-level image features.
(Smith et al., 2002) – or increases the prior probability of new
concepts concentrating on that same dimension (Kemp et al.,
These approaches from machine learning may be com-
pelling ways to understand how humans learn so quickly,
2007). But this approach is limited since it requires that the
but there is little experimental evidence that directly supports
relevant dimensions of similarity be deﬁned in advance.
For many real-world concepts, the relevant dimensions of
them. Models that construct parts or features from sensory
data (pixels) while learning object concepts have been tested
similarity may be constructed in the course of learning to
in elegant behavioral experiments with very simple stimuli
learn. For instance, when we ﬁrst see a Segway, we may
parse it into a structure of familiar parts arranged in a novel
and a very small number of concepts (Austerweil & Grifﬁths,
2009; Schyns, Goldstone, & Thibaut, 1998). But there have
conﬁguration:
it has two wheels, connected by a platform,
been few systematic comparisons of multiple state-of-the-art
supporting a motor and a central post at the top of which are
two handlebars. These parts and their relations comprise a
computational approaches to representation learning with hu-

Shared	  low-­‐level	  features	  

	  
	  

	  
	  
	  
	  

Hierarchical	  Genera<ve	  Model	  

Tree	  hierarchy	  of	  	  
classes	  is	  learned	  

“animal”	  

“vehicle”	  

	  
	  (Nested	  Chinese	  Restaurant	  Process)	  
prior:	  a	  nonparametric	  prior	  over	  tree	  
structures	  

	  
	  (Hierarchical	  Dirichlet	  Process)	  prior:	  
a	  nonparametric	  prior	  allowing	  categories	  to	  
share	  higher-­‐level	  features,	  or	  parts.	  

	  horse	  

	  cow	  

	  car	  

	  van	  

	  truck	  

Deep	  Boltzmann	  Machine	  

Enforce	  (approximate)	  global	  consistency	  	  
through	  many	  local	  constraints.	  	  

Images,	  HandwriRen	  characters,	  
Mo<on	  capture	  datasets.	  

Learning	  to	  Learn	  from	  Few	  Examples	  
Training	  Examples	  (by	  row)	  
Condi<onal	  Samples	  

Learning	  from	  
3	  examples	  

Genera<ng	  
Novel	  
Characters	  

Learned	  Super-­‐Classes	  (by	  row)	   Sampled	  Novel	  Characters	  


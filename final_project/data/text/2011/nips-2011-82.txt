Sparse Estimation with Structured Dictionaries

David P. Wipf ∗
Visual Computing Group
Microsoft Research Asia
davidwipf@gmail.com

Abstract

In the vast majority of recent work on sparse estimation algorithms, performance
has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian
or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in
reality, these types of dictionaries represent only a subset of the dictionaries that
are actually used in practice (largely restricted to idealized compressive sensing
applications). In contrast, herein sparse estimation is considered in the context
of structured dictionaries possibly exhibiting high coherence between arbitrary
groups of columns and/or rows. Sparse penalized regression models are analyzed
with the purpose of ﬁnding, to the extent possible, regimes o f dictionary invari-
ant performance. In particular, a Type II Bayesian estimator with a dictionary-
dependent sparsity penalty is shown to have a number of desirable invariance
properties leading to provable advantages over more conventional penalties such
as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees
no longer hold. This can translate into improved performance in applications such
as model selection with correlated features, source localization, and compressive
sensing with constrained measurement directions.

1

Introduction

We begin with the generative model

(1)
Y = ΦX0 + E ,
where Φ ∈ Rn×m is a dictionary of basis vectors or features, X0 ∈ Rm×t is a matrix of unknown
coefﬁcients we would like to estimate, Y ∈ Rn×t is an observed signal matrix, and E is a noise
matrix with iid elements distributed as N (0, λ). The objective is to estimate the unknown genera-
tive X0 under the assumption that it is row-sparse, meaning that many rows of X0 have zero norm.
The problem is compounded considerably by the additional assumption that m > n, meaning the
dictionary Φ is overcomplete. When t = 1, this then reduces to the canonical sparse estimation of
a coefﬁcient vector with mostly zero-valued entries or mini mal ℓ0 norm [7]. In contrast, estimation
of X0 with t > 1 represents the more general simultaneous sparse approximation problem [6, 15]
relevant to numerous applications such as compressive sensing and multi-task learning [9, 16], man-
ifold learning [13], array processing [10], and functional brain imaging [1]. We will consider both
scenarios herein but will primarily adopt the more general notation of the t > 1 case.
One possibility for estimating X0 involves solving

mXi=1
X kY − ΦX k2
F + λd(X ),
min
where the indicator function I [kxk > 0] equals one if kxk > 0 and equals zero otherwise (kxk
is an arbitrary vector norm). d(X ) penalizes the number of rows in X that are not equal to zero;

I [kxi·k > 0] ,

(2)

λ > 0,

d(X ) ,

∗Draft version for NIPS 2011 pre-proceedings.

1

for nonzero rows there is no additional penalty for large magnitudes. Moreover, it reduces to the ℓ0
norm when t = 1, i.e., d(x) = kxk0 , or a count of the nonzero elements in the vector x. Note that
to facilitate later analysis, we deﬁne x·i as the i-th column of matrix X while xi· represents the i-th
row. For theoretical inquiries or low-noise environments, it is often convenient to consider the limit
as λ → 0, in which case (2) reduces to
min
d(X ),
X

s.t. ΦX0 = ΦX.

(3)

Unfortunately, solving either (2) or (3) involves a combinatorial search and is therefore not tractable
in practice. Instead, a family of more convenient sparse penalized regression cost functions are re-
viewed in Section 2. In particular, we discuss conventional Type I sparsity penalties, such as the ℓ1
norm and the ℓ1,2 mixed norm, and a Type II empirical Bayesian alternative characterized by dictio-
nary dependency. When the dictionary Φ is incoherent, meaning the columns are roughly orthogonal
to one another, then certain Type I selections are well-known to produce good approximations of X0
via efﬁcient implementations. However, as discussed in Sec tion 3, more structured dictionary types
can pose difﬁculties. In Section 4 we analyze the underlying cost functions of Type I and Type II,
and demonstrate that the later maintains several properties that suggest it will be robust to highly
structured dictionaries. Brief empirical comparisons are presented in Section 5.

2 Estimation via Sparse Penalized Regression

(4)

(5)

Directly solving either (2) or (3) is intractable, so a variety of approximate methods have been
proposed. Many of these can be viewed simply as regression with a sparsity penalty convenient for
optimization purposes. The general regression problem we consider here involves solving
X kY − ΦX k2
min
F + λg(X ),
where g is some penalty function of the row norms. Type I methods use a separable penalty of the
form
g (I ) (X ) = Xi
h (kxi·k2 ) ,
where h is a non-decreasing, typically concave function.1 Common examples include h(z ) =
z p , p ∈ (0, 1] [11] and h(z ) = log(z + α), α ≥ 0 [4]. The parameters p and α are often heuristically
selected on an application-speci ﬁc basis. In contrast, Typ e II methods, with origins as empirical
Bayesian estimators, implicitly utilize a more complicated penalty function that can only be ex-
pressed in a variational form [18]. Herein we will consider the selection
Tr (cid:2)X T Γ−1X (cid:3) + t log (cid:12)(cid:12)αI + ΦΓΦT (cid:12)(cid:12) , α ≥ 0,
where Γ is a diagonal matrix of non-negative variational parameters [14, 18]. While less transparent
than Type I, it has been shown that (6) is a concave non-decreasing function of each row norm of X ,
hence it promotes row sparsity as well. Moreover, the dictionary-dependency of this penalty appears
to be the source of some desirable invariance properties as discussed in Section 4. Analogous to (3),
for analytical purposes all of these methods can be reduced as λ → 0 to solving
s.t. ΦX0 = ΦX.
min
g(X )
X

g (I I ) (X ) , min
Γ(cid:23)0

(7)

(6)

3 Structured Dictionaries

It is now well-established that when the dictionary Φ is constructed with appropriate randomness,
e.g., iid Gaussian entries, then for certain choices of g , in particular the convex selection g(X ) =
Pi kxi· k2 (which represents a generalization of the ℓ1 vector norm to row-sparse matrices), we
can expect to recover X0 exactly in the noiseless case or to close approximation otherwise. This
assumes that d(X0 ) is sufﬁciently small relative to some function of the dictio nary coherence or a
related measure. However, with highly structured dictionaries these types of performance guarantees
completely break down.

1Other row norms, such as the ℓ∞ , have been considered as well but are less prevalent.

2

At the most basic level, one attempt to standardize structured dictionaries is by utilizing some form
of column normalization as a pre-processing step. Most commonly, each column is scaled such that
it has unit ℓ2 norm. This helps ensure that no one column is implicitly favored over another during
the estimation process. However, suppose our observation matrix is generated via Y = ΦX0 , where
Φ = eΦD + σabT , eΦ is some well-behaved, incoherent dictionary, D is a diagonal matrix, and σabT
represents a rank one adjustment. If we apply column normalization to remove the effect of D , the
resulting scale factors will be dominated by the rank one term when σ is large. But if we do not
column normalize, then D can completely bias the estimation results.
In general, if our given dictionary is effectively W eΦD , with W an arbitrary invertible matrix that
scales and correlates rows, and D diagonal, the combined effect can be severely disruptive. As
an example from neuroimaging, the MEG/EEG source localization problem involves estimating
sparse neural current sources within the brain using sensors placed near the surface of the scalp.
The effective dictionary or forward model is characterized by highly correlated rows (because the
sensors are physically constrained to be near one another) and columns with drastically different
scales (since deep brain sources produce much weaker signals at the surface than super ﬁcial ones).
More problematic is the situation where Φ = eΦS , since an unrestricted matrix S can introduce
arbitrary coherence structure between individual or groups of columns in Φ, meaning the structure
of Φ is now arbitrary regardless of how well-behaved the original eΦ.
4 Analysis
We will now analyze the properties of both Type I and Type II cost functions when coherent or highly
structured dictionaries are present. Ideally, we would like to arrive at algorithms that are invariant,
to the extent possible, to dictionary transformations that would otherwise disrupt the estimation
efﬁcacy. For simplicity, we will primarily consider the noi seless case, although we surmise that
much of the underlying intuition carries over into the noiseless domain. This strategy mirrors the
progression in the literature of previous sparse estimation theory related to the ℓ1 norm [3, 7, 8]. All
proofs have been deferred to the Appendix, with some details omitted for brevity.

4.1
Invariance to W and D
We will ﬁrst consider the case where the observation matrix i s produced via Y = ΦX0 = W eΦDX0 .
Later in Sections 4.2 and 4.3 we will then address the more challenging situation where Φ = eΦS .
Lemma 1. Let W denote an arbitrary full-rank n × n matrix and D an arbitrary full-rank m × m
diagonal matrix. Then with α → 0, the Type II optimization problem
s.t. W eΦDX0 = W eΦDX
g (I I ) (X )
(8)
min
X
is invariant to W and D in the sense that if X ∗ is a global (or local) minimum to (8), then D−1X ∗
is a global (or local) minimum when we optimize g (I I ) (X ) subject to the constraint eΦX0 = eΦX .
Therefore, while switching between Φ = W eΦD and Φ = eΦ may inﬂuence the initialization and pos-
sibly the update rules of a particular Type II algorithm, it does not fundamentally alter the underlying
cost function. In contrast, Type I methods do not satisfy this invariance. Invariance is preserved with
a W factor in isolation. Likewise, inclusion of a D factor alone with column normalization leads to
invariance. However, inclusion of both W and D together can be highly disruptive.
Note that for improving Type I performance, it is not sufﬁcie nt to apply some row decorrelating and
normalizing ˆW −1 to Φ and then column normalize with some ˆD−1 . This is because the application
of ˆD−1 will disrupt the effects of ˆW −1 . But one possibility to compensate for dictionary structure is
to jointly learn a ˆW −1 and ˆD−1 that produces a Φ satisfying: (i) ΦΦT = C I (meaning rows have a
constant ℓ2 norm of C and are uncorrelated, (ii) kφ·i k2 = 1 for all i. Up to irrelevant scale factors, a
unique such transformation will always exist. In Section 5 we empirically demonstrate that this can
be a highly effective strategy for improving the performance of Type I methods. However, as a ﬁnal
point, we should mention that the invariance Type II exhibits towards W and D (or any corrected
form of Type I) will no longer strictly hold once noise is added.

3

4.2
Invariance to S : The t > 1 Case (Simultaneous Sparse Approximation)
We now turn to the potentially more problematic scenario with Φ = eΦS . We will assume that S
is arbitrary with the only restriction being that the resulting Φ satis ﬁes spark [Φ] = n + 1, where
matrix spark quanti ﬁes the smallest number of linearly depe ndent columns [7]. Consequently, the
spark condition is equivalent to saying that each n × n sub-matrix of Φ is full rank. This relatively
weak assumption is adopted for simplicity; in many cases it can be relaxed.

Lemma 2. Let Φ be an arbitrary dictionary with spark [Φ] = n + 1 and X0 a coefﬁcient matrix with
d(X0 ) < n. Then there exists a constant ρ > 0 such that the optimization problem (7), with g(X ) =
g (I I ) (X ) and α → 0, has no local minima and a unique, global solution at X0 if (x0 )T
i· (x0 )j · ≤ ρ
for all i 6= j (i.e., the nonzero rows of X0 are below some correlation threshold). Also, if we enforce
exactly zero row-wise correlations, meaning ρ = 0, then a minimizing solution X ∗ will satisfy
i·k2 = k(x0 )i· k2 for all i (i.e., a matching row-sparsity support), even for d(X0 ) ≥ n. This
kx∗
solution will be unique whenever ΦX0X T
0 Φ = ΦΓΦT has a unique solution for some non-negative,
diagonal Γ.2

Corollary 1. There will always exist dictionaries Φ and coefﬁcients X0 , consistent with the con-
ditions from Lemma 2, such that the optimization problem (7) with any possible g(X ) of the form
g (I ) (X ) = Pi h (kxi· k2 ) will have minimizing solutions not equal to X0 (with or without column
normalization).
In general, Lemma 2 suggests that for estimation purposes uncorrelated rows in X0 can potentially
compensate for troublesome dictionary structure, and together with Corollary 1 it also describes a
potential advantage of Type II over Type I. Of course this result only stipulates sufﬁcient conditions
for recovery that are certainly not necessary, i.e., effective sparse recovery is possible even with
correlated rows (more on this below). We also emphasize that the ﬁnal property of Lemma 2 implies
that the row norms of X0 (and therefore the row-sparsity support) can still be recovered even up
to the extreme case of d(X0 ) = m > n. While this may seem surprising at ﬁrst, especially since
even brute force minimization of (3) can not achieve a similar feat, it is important to keep in mind
that (3) is blind to the correlation structure of X0 . Although Type II does not explicitly require any
such structure, it is able to outperform (3) by implicitly leveraging this structure when the situation
happens to be favorable. While space prevents a full treatment, in the context of MEG/EEG source
estimation, we have successfully localized 500 nonzero sources (rows) using a 100×1000 dictionary.
However, what about the situation where strong correlations do exist between the nonzero rows of
X0 ? A couple things are worth mentioning in this regard. First, Lemma 2 can be strengthened
considerably via the expanded optimization problem: minX,B g (I I ) (X ) s.t. ΦX0 = ΦX B , which
achieves a result similar to Lemma 2 but with a weaker correlation condition (although the row-
norm recovery property is lost). Secondly, in the case of perfect correlation between rows (the
hardest case), the problem reduces to an equivalent one with t = 1, i.e., it exactly reduces to the
canonical sparse recovery problem. We address this situation next.

4.3

Invariance to S : The t = 1 Case (Standard Sparse Approximation)

This section considers the t = 1 case, meaning Y = y and X0 = x0 are now vectors. For
convenience, we deﬁne X (S , P ) as the set of all coefﬁcient vectors in Rm with support (or nonzero
coefﬁcient locations) speci ﬁed by the index set
S ⊂ {1, . . . , m} and sign pattern given by P ∈
{−1, +1}|S | (here the | · | operator denotes the cardinality of a set).
Lemma 3. Let Φ be an arbitrary dictionary with spark [Φ] = n + 1. Then for any X (S , P ) with
|S | < n, there exists a non-empty subset ¯X ⊂ X (S , P ) (with nonzero Lebesgue measure), such
that if x0 ∈ ¯X , the Type II minimization problem
g (I I ) (x)
(9)
s.t. Φx0 = Φx, α → 0
min
x
will have a unique minimum and it will be located at x0 .
2See Appendix for more details about this condition. In most situations, it will hold if m < n(n + 1)/2,
and likely for many instances with m even greater than this.

4

This Lemma can be obtained with a slight modi ﬁcation of resul
ts in [18]. In other words, no mat-
ter how poorly structured a particular dictionary is with regard to a given sparsity proﬁle, there
will always be sparse coefﬁcients we are guaranteed to recov er (provided we utilize a convergent
algorithm). In contrast, an equivalent claim can not be made for Type I:
Lemma 4. Given an arbitrary Type I penalty g (I ) (x) = Pi h(|xi |), with h a ﬁxed, non-decreasing
function, there will always exist a dictionary Φ (with or without normalized columns) and set
X (S , P ) such that for any x0 ∈ X (S , P ), the problem
g (I ) (x)
(10)
s.t. Φx0 = Φx
min
x

will not have a unique minimum located at x0 .

This can happen because the global minimum does not equal x0 and/or because of the presence of
local minima. Of course this does not necessarily imply that a particular Type I algorithm will fail.
For example, even with multiple minima, an appropriate optimization strategy could conceivably
still locate an optimum that coincides with x0 . While it is difﬁcult to analyze all possible algorithms,
we can address one inﬂuential variety based on iterative rew eighted ℓ1 minimization [4, 18]. Here
the idea is that if h is concave and differentiable, then a convergent means of minimizing (10) is to
utilize a ﬁrst-order Taylor series approximation of g (I ) (x) at some point ˆx. This leads to an iterative
procedure where at each step we must ﬁrst compute h′
, dh(z )/dz |z=| ˆxi | and then minimize
i
Pi h′
i |xi | subject to Φx0 = Φx to update ˆx. This method produces a sparse estimate at each
iteration and is guaranteed to converge to a local minima (or stationary point) of (10). However, this
solution may be suboptimal in the following sense:
Corollary 2. Given an arbitrary g (I ) (x) as in Lemma 4, there will always exist a Φ and X (S , P ),
such that for any x0 ∈ X (S , P ), iterative reweighted ℓ1 minimization will not converge to x0 when
initialized at the minimum ℓ1 norm solution.

Note that this failure does not result from a convergence pathology. Rather, the presence of minima
different from x0 explicitly disrupts the algorithm.
In general, with highly structured dictionaries deviating from the ideal, the global minimum of con-
vex penalties often does not correspond with x0 as theoretical equivalence results break down. This
in turn suggests the use of concave penalty functions to seek possible improvement. However, as
illustrated by the following result, even the simplest of sparse recovery problems, that of estimating
some x0 with only one nonzero element using a dictionary with a 1D null-space, Type I can be
characterized by problematic local minima with (strictly) concave penalties. For this purpose we
deﬁne φ∗ as an arbitrary column of Φ and ¯Φ∗ as all columns of Φ excluding φ∗ .

Lemma 5. Let h denote a concave, non-decreasing function with h′
, limz→0 dh(z )/dz and
max
, limz→∞ dh(z )/dz . Also, let Φ be a dictionary with unit ℓ2 norm columns and spark [Φ] =
h′
min
m = n + 1 (i.e., a 1D null-space), and let x0 satisfy kx0 k0 = 1 with associated φ∗ . Then the Type
I problem (10) can have multiple local minima if

h′
max
h′
min

> k ¯Φ−1
∗ φ∗ k1 .

(11)

This result has a very clear interpretation related to how dictionary coherence can potentially disrupt
even the most rudimentary of estimation tasks. The righthand side of (11) is bounded from below by
1, which is approached whenever one or more columns in some ¯Φ∗ are similar to φ∗ (i.e., coherent).
Thus, even the slightest amount of curvature (or strict concavity) in h can lead to the inequality being
satis ﬁed when highly coherent columns are present. While obv iously with h(z ) = z this will not be
an issue (consistent with the well-known convexity of the ℓ1 problem), for many popular non-convex
penalties, this gradient ratio may be large relative to the righthand side, indicating that local minima

5

are always possible. For example, with the h(z ) = log(z + α) selection from [4] h′
min → 0 for all α
while h′
max → 1/α. We note that Type II has provably no local minima in this regime (this follows
as a special case of Lemma 3). Of course the point here is not that Type I algorithms are incapable
of solving simple problems with kx0 k0 = 1 (and any iterative reweighted ℓ1 scheme will succeed
on the ﬁrst step anyway). Rather, Lemma 5 merely demonstrate s how highly structured dictionaries
can begin to have negative effects on Type I, potentially more so than with Type II, even on trivial
tasks. The next section will empirically explore this conjecture.

5 Empirical Results

We now present two simulation examples illustrating the potential beneﬁts of Type II with highly
structured dictionaries. In the ﬁrst experiment, the dicti onary represents an MEG leadﬁeld, which at
a high level can be viewed as a mapping from the electromagnetic (EM) activity within m brain vox-
els to n sensors placed near the scalp surface. Computed using Maxwell’s equations and a spherical
shell head model [12], the resulting Φ is characterized by highly correlated rows, because the small
scalp surface requires that sensors be placed close together, and vastly different column norms, since
the EM ﬁeld strength drops off rapidly for deep brain sources . These effects are well represented by a
dictionary such as Φ = W eΦD as discussed previously. Figure 1 (Left) displays trial-averaged results
comparing Type I algorithms with Type II using such an MEG leadﬁeld dictionary. Data generation
proceeded as follows: We produce Φ by choosing 50 random sensor locations and 100 random vox-
els within the brain volume. We then create a coefﬁcient matr ix X0 with t = 5 columns and d(X0 )
an experiment-dependent parameter. Nonzero rows of X0 are drawn iid from a unit Gaussian distri-
bution. The observation matrix is then computed as Y = ΦX0 . We run each algorithm and attempt
to estimate X0 , calculating the probability of success averaged over 200 trials as d(X0 ) is varied
from 10 to 50. We compared Type II, implemented via a simple iterative reweighted ℓ2 approach,
with two different Type I schemes. The ﬁrst is a homotopy cont
inuation method using the Type I
penalty g (I ) (X ) = Pi log(kxi· k2
2 + α), where α is gradually reduced to zero during the estimation
process [5]. We have often found this to be the near optimal Type I approach on a variety empirical
tests. Secondly, we used the standard mixed-norm penalty g (I ) (X ) = kX k1,2 = Pi kxi· k2 , which
leads to a convex minimization problem that generalizes basis pursuit (or the lasso), to the t > 1
domain [6, 10].
While Type II displays invariance to W - and D-like transformations, Type I methods do not. Con-
sequently, we examined two dictionary-standardization methods for Type I. First, we utilized basic
ℓ2 column normalization, without which Type I will have difﬁcu lty with the vastly different column
scalings of Φ. Secondly, we developed an algorithm to learn a transformed dictionary ˆU Φ ˆΠ, with
ˆU arbitrary, ˆΠ diagonal, such that the combined dictionary has uncorrelated, unit ℓ2 norm rows, and
unit ℓ2 norm columns (as discussed in Section 4.1). Figure 1(left) contains results from all of these
variants, where it is clear that some compensation for the dictionary structure is essential for good
recovery performance. We also note that Type II still outperforms Type I in all cases, suggesting
that even after transformation of the latter, there is still residual structure in the MEG leadﬁeld being
exploited by Type II. This is a very reasonable assumption given that Φ will typically have strong
column-wise correlations as well, which are more effectively modeled by right multiplication by
some S . As a ﬁnal point, the Type II success probability does not go t o zero even when d(X0 ) = 50,
implying that in some cases it is able to ﬁnd a number of nonzer os equal to the number of rows in Φ.
This is possible because even with only t = 5 columns, the nonzero rows of X0 display somewhat
limited sample correlation, and so exact support recovery is still possible. With t > 5 these sample
correlations can be reduced further, allowing consistent support recovery when d(X0 ) > n (not
shown).
To further test the ability of Type II to handle structure imposed by some eΦS , we performed a
second experiment with explicitly controlled correlations among groups of columns. For each trial
we generated a 50 × 100 Gaussian iid dictionary eΦ. Correlations were then introduced using a
block-diagonal S with 4 × 4 blocks created with iid entries drawn from a uniform distribution
(between 0 and 1). The resulting Φ = eΦS was then scaled to have unit ℓ2 norm columns. We
then generated a random x0 vector (t = 1 case) using iid Gaussian nonzero entries with d(x0 )
varied from 10 to 25 (with t = 1, we cannot expect to recover as many nonzeros as when t = 5).
Signal vectors are computed as y = Φx0 or, for purposes of direct comparison with a canonical
iid dictionary, y = eΦx0 . We evaluated Type II and the Type I iterative reweighted ℓ1 minimization
6

method from [4], which is guaranteed to do as well or better than standard ℓ1 norm minimization.
Trial-averaged results using both Φ and eΦ are shown in Figure 1(right), where it is clear that while
Type II performance is essentially unchanged, Type I performance degrades substantially.
1
 
1
 
0.9

s
s
e
c
c
u
s
 
f
o
 
y
t
i
l
i
b
a
b
o
r
p

0.8

0.6

0.4

0.2

Type II
Type I, homotopy, norm
Type I, homotopy, invariant
Type I, basis pursuit, norm
Type I, basis pursuit, invariant

0
 
10

15

20

25

30
row−sparsity

35

40

45

50

s
s
e
c
c
u
s
 
f
o
 
y
t
i
l
i
b
a
b
o
r
p

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1
 
10

Type II, iid Φ
Type II, coherent Φ
Type I, iid Φ
Type I, coherent Φ

15

row−sparsity

20

25

Figure 1: Left: Probability of success recovering coefﬁcient matrices wi
th varying degrees of row-
sparsity using an MEG leadﬁeld as the dictionary. Two Type I m ethods were compared, a homotopy
continuation method from [5] and a version of basis pursuit extended to the simultaneous sparse
approximation problem by minmizing the ℓ1,2 mixed norm [6, 10]. Type I methods were compared
using standard ℓ2 column normalization and a learned invariance transformation. Right: Probability
of success recovering sparse vectors using a Gaussian iid dictionary eΦ and a coherent dictionary Φ
with clustered columns. The Type I method was the interactive reweighted ℓ1 algorithm from [4].
6 Conclusion

When we are free to choose the basis vectors of an overcomplete signal dictionary, the sparse estima-
tion problem is supported by strong analytical and empirical foundations. However, there are many
applications where physical restrictions or other factors impose rigid constraints on the dictionary
structure such that the assumptions of theoretical recovery guarantees are violated. Examples in-
clude model selection problems with correlated features, source localization, and compressive sens-
ing with constrained measurement directions. This can have signi ﬁcant consequences depending
on how the estimated coefﬁcients will ultimately be utilize d. For example, in the source localiza-
tion problem, correlated dictionary columns may correspond with drastically different regions (e.g.,
brain areas), so recovering the exact sparsity proﬁle can be important. Ideally we would like our
recovery algorithms to display invariance, to the extent possible, to the actual structure of the dic-
tionary. With typical Type I sparsity penalties this can be a difﬁcult undertaking; however, with the
natural dictionary dependence of the Type II penalty, to some extent it appears this structure can be
accounted for, leading to more consistent performance across dictionary types.

Appendix

Here we provide brief proofs of several results from the paper. Some details have been omitted for
space considerations.
Proof of Lemma 1: First we address invariance with respect to W . Obviously the equality constraint
is unaltered by a full rank W , so it only remains to check that the dictionary-dependent penalty
g (I I ) is invariant. However, since by standard determinant relationships log |W eΦDΓD eΦT W T | =
log |W || eΦDΓD eΦT ||W T | = log | eΦDΓD eΦT |+C , where C is an irrelevant constant for optimization
purposes, this point is established. With respect to D , we re-parameterize the problem by deﬁning
eX , DX and eΓ , DΓD . It is then readily apparent that the penalty (6) satis ﬁes
Tr h eX T eΓ−1 eX i + log | eΦeΓ eΦT |.
Tr (cid:2)X T Γ−1X (cid:3) + log | eΦDΓD eΦT | = min
g (I I ) (X ) ≡ min
(12)
Γ(cid:23)0
eΓ(cid:23)0
So we are effectively solving: min eX g (I I ) ( eX ) s.t. eΦDX0 = eΦ eX .
Proof of Lemma 2 and Corollary 1: Minimizing the Type II cost function can be accomplished
equivalently by minimizing

(cid:4)

7

L(Γ) , Tr hΦt−1X0X T
0 ΦT (cid:0)ΦΓΦT (cid:1)−1 i + log |ΦΓΦT |,
(13)
over the non-negative diagonal matrix Γ (this follows from a duality principle in Type II models
[18]). L(Γ) includes an observed covariance Φt−1X0X T
0 ΦT and a parameterized model covariance
0 ] [17]. Moreover, if ΦΓ∗ΦT is sufﬁ-
ΦΓΦT , and is globally minimized with Γ∗ = t−1diag[X0X T
0 are not too large, then
0 ΦT , meaning the off-diagonal elements of X0X T
ciently close to t−1ΦX0X T
it can be shown by differentiating along the direction between any arbitrary point Γ′ and Γ∗ that no
local minima exist, leading to the ﬁrst part of Lemma 2.
Regarding the second part, we now allow d(X0 ) to be arbitrary but require that X0X T
0 be diagonal
(zero correlations). Using similar arguments as above, it is easily shown that any minimizing solu-
tion Γ∗ must satisfy ΦΓ∗ΦT = Φt−1X0X T
0 ΦT . This equality can be viewed as n(n + 1)/2 linear
equations (equal to the number of unique elements in an n × n covariance matrix) and m unknowns,
namely, the diagonal elements of Γ∗ . Therefore, if n(n + 1)/2 > m this system of equations will
typically be overdetermined (e.g., if suitable randomness is present to avoid adversarial conditions)
with a unique solution. Moreover, because of the requirement that Γ be non-negative, it is likely that
a unique solution will exist in many cases where m is even greater than n(n + 1)/2 [2].
Finally, we address Corollary 1. First, consider the case where t = 1, so X0 = x0 . To satisfy the
now degenerate correlation condition, we must have d(x0 ) = 1. Even in this simple regime it can be
demonstrated that a unique minimum at x0 is possible iff h(z ) = z based on Lemma 5 (below) and a
complementary result in [17]. So the only Type I possibility is h(z ) = z . A simple counterexample
with t = 2 serves to rule this selection out. Consider a dictionary Φ and two coefﬁcient matrices
given by
 ,
 , X(2) = 
ǫ −ǫ # , X(1) = 
Φ = " ǫ
0
1
1
1
1
1
ǫ
0 −1
1 −1
1 −1
0
0
ǫ
0
0
0
0
0
ǫ
0
0
0
It is easily veri ﬁed that ΦX(1) = ΦX(2) and that X(1) = X0 , the maximally row-sparse
solution. Computing the Type I cost for each with h(z ) = z gives g (I ) (X(1) ) = 2√2 and
g (I ) (X(2) ) = 2(1 + ǫ). Thus, if we allow ǫ to be small, g (I ) (X(2) ) < g (I ) (X(1) ), so X(1) = X0
cannot be the minimizing solution. Note that ℓ2 column normalization will not change this
conclusion since all columns of Φ have equal norm already.
(cid:4)

(14)

Proof of Lemma 4 and Corollary 2: For brevity, we will assume that h is concave and differentiable,
as is typical of most sparsity penalties used in practice (the more general case follows with some
additional effort). This of course includes h(z ) = z , which is both concave and convex, and leads
to the ℓ1 norm penalty. These results will now be demonstrated using a simple counterexample
similar to the one above. Assume we have the dictionary Φ from (14), and that S = {1, 2} and
P = {+1, +1}, which implies that any x0 ∈ X (S , P ) can be expressed as x0 = [α1 , α2 , 0, 0]T , for
some α1 , α2 > 0. We will now show that with any member from this set, there will not be a unique
minimum to the Type I cost at x0 for any possible concave, differentiable h.
First assume α1 ≥ α2 . Consider the alternative feasible solution x(2) = [(α1 − α2 ), 0, ǫα2 , ǫα2 ]T .
To check if this is a local minimum, we can evaluate the gradient of the penalty function g (I ) (x)
along the feasible region near x(2) . Given v = [1, 1, −ǫ, −ǫ]T ∈ Null(Φ), this can be accomplished
by computing ∂ g (I ) (x(2) + βv)/∂β = h′ (|α1 − α2 + β |) + h′ (|β |) + 2ǫh′ (|ǫα2 − ǫβ |). In the limit
as β → 0 (from the right or left), this expression will always be positive for ǫ < 0.5 based on the
concavity of h. Therefore, x(2) must be a minimum. By symmetry an equivalent argument can be
made when α2 ≥ α1 . (In the special case where α1 = α2 , there will actually exist two maximally
sparse solutions, the generating x0 and x(2) .) It is also straightforward to verify analytically that
iterative reweighted ℓ1 minimization will fail on this example when initialized at the minimum ℓ1
norm solution. It will always become trapped at x(2) after the ﬁrst iteration, assuming α1 ≥ α2 , or
a symmetric local minimum otherwise.
(cid:4)

Proof of Lemma 5: This result can be shown by examining properties of various gradients along
the feasible region, not unlike some of the analysis above, and then bounding the resultant quantity.
We defer these details to a later publication.
(cid:4)

8

References

J.

IEEE Trans. Information

IEEE Trans. Signal Pro-

IEEE Signal

“Electromagnet ic brain mapping,”

[1] S. Baillet, J.C. Mosher, and R.M. Leahy,
Processing Magazine, pp. 14–30, Nov. 2001.
[2] A.M. Bruckstein, M. Elad, and M. Zibulevsky, “A non-nega tive and sparse enough solution of
an underdetermined linear system of equations is unique,”
IEEE Trans. Information Theory,
vol. 54, no. 11, pp. 4813–4820, Nov. 2008.
[3] E. Cand `es, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information,”
IEEE Trans. Information Theory, vol. 52, no.
2, pp. 489–509, Feb. 2006.
[4] E. Cand `es, M. Wakin, and S. Boyd, “Enhancing sparsity by reweighted ℓ1 minimization,”
Fourier Anal. Appl., vol. 14, no. 5, pp. 877–905, 2008.
[5] R. Chartrand and W. Yin, “Iteratively reweighted algori thms for compressive sensing,” Proc.
Int. Conf. Accoustics, Speech, and Signal Proc., 2008.
[6] S.F. Cotter, B.D. Rao, K. Engan, and K. Kreutz-Delgado,
“ Sparse solutions to linear inverse
problems with multiple measurement vectors,”
IEEE Trans. Signal Processing, vol. 53, no. 7,
pp. 2477–2488, April 2005.
[7] D.L. Donoho and M. Elad,
“Optimally sparse representati on in general (nonorthogonal) dic-
tionaries via ℓ1 minimization,” Proc. National Academy of Sciences, vol. 100, no. 5, pp. 2197–
2202, March 2003.
[8] J.J. Fuchs, “On sparse representations in arbitrary red undant bases,”
Theory, vol. 50, no. 6, pp. 1341–1344, June 2004.
[9] S. Ji, D. Dunson, and L. Carin,
“Multi-task compressive s ensing,”
cessing, vol. 57, no. 1, pp. 92–106, Jan 2009.
[10] D.M. Malioutov, M. C¸ etin, and A.S. Willsky,
“Sparse si gnal reconstruction perspective for
source localization with sensor arrays,”
IEEE Transactions on Signal Processing, vol. 53, no.
8, pp. 3010–3022, August 2005.
[11] B.D. Rao, K. Engan, S. F. Cotter, J. Palmer, and K. Kreutz-Delgado, “Subset selection in noise
based on diversity measure minimization,”
IEEE Trans. Signal Processing, vol. 51, no. 3, pp.
760–770, March 2003.
[12] J. Sarvas, “Basic methematical and electromagnetic co ncepts of the biomagnetic inverse prob-
lem,” Phys. Med. Biol., vol. 32, pp. 11–22, 1987.
[13] J.G. Silva, J.S. Marques, and J.M. Lemos,
“Selecting la ndmark points for sparse manifold
learning,” Advances in Neural Information Processing Systems 18, pp. 1241–1248, 2006.
[14] M.E. Tipping,
“Sparse Bayesian learning and the releva nce vector machine,”
Journal of
Machine Learning Research, vol. 1, pp. 211–244, 2001.
[15] J.A. Tropp,
“Algorithms for simultaneous sparse appro ximation. Part II: Convex relaxation,”
Signal Processing, vol. 86, pp. 589–602, April 2006.
[16] M.B. Wakin, M.F. Duarte, S. Sarvotham, D. Baron, and R.G. Baraniuk, “Recovery of jointly
sparse signals from a few random projections,” Advances in Neural Information Processing
Systems 18, pp. 1433–1440, 2006.
[17] D.P. Wipf, Bayesian Methods for Finding Sparse Representations, PhD Thesis, University of
California, San Diego, 2006.
[18] D.P. Wipf and S. Nagarajan, “Iterative reweighted ℓ1 and ℓ2 methods for ﬁnding sparse solu-
tions,” J. Selected Topics in Signal Processing (Special Issue on Compressive Sensing), vol. 4,
no. 2, April 2010.

9


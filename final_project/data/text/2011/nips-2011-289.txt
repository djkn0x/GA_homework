The Impact of Unlabeled Patterns in Rademacher
Complexity Theory for Kernel Classi ﬁers

Davide Anguita, Alessandro Ghio, Luca Oneto, Sandro Ridella
Department of Biophysical and Electronic Engineering
University of Genova
Via Opera Pia 11A, I-16145 Genova, Italy
{Davide.Anguita,Alessandro.Ghio} @unige.it
{Luca.Oneto,Sandro.Ridella} @unige.it

Abstract

We derive here new generalization bounds, based on Rademacher Complexity the-
ory, for model selection and error estimation of linear (kernel) classi ﬁers, which
exploit the availability of unlabeled samples.
In particular, two results are ob-
tained: the ﬁrst one shows that, using the unlabeled samples , the conﬁdence term
of the conventional bound can be reduced by a factor of three; the second one
shows that the unlabeled samples can be used to obtain much tighter bounds, by
building localized versions of the hypothesis class containing the optimal classi-
ﬁer.

1

Introduction

Understanding the factors that inﬂuence the performance of a statistical procedure is a key step for
ﬁnding a way to improve it. One of the most explored procedure s in the machine learning approach
to pattern classi ﬁcation aims at solving the well –known model selection and error estimation prob-
lem, which targets the estimation of the generalization error and the choice of the optimal predictor
from a set of possible classi ﬁers. For reaching this target,
several approaches have been proposed
[1, 2, 3, 4], which provide an upper bound on the generalization ability of the classi ﬁer, which can
be used for model selection purposes as well. Typically, all these bounds consists of three terms:
the ﬁrst one is the empirical error of the classi ﬁer (i.e. the
error performed on the training data),
the second term is a bias that takes into account the complexity of the class of functions, which the
classi ﬁer belongs to, and the third one is a conﬁdence term, w hich depends on the cardinality of the
training set. These approaches are quite interesting because they investigate the ﬁnite sample behav-
ior of a classi ﬁer, instead of the asymptotic one, even thoug h their practical applicability has been
questioned for a long time1 . One of the most recent methods for obtaining these bounds is to exploit
the Rademacher Complexity, which is a powerful statistical tool that has been deeply investigated
during the last years [5, 6, 7]. This approach has shown to be of practical use, by outperforming more
traditional methods [8, 9] for model selection in the small – sample regime [10, 5, 6], i.e. when the
dimensionality of the samples is comparable, or even larger, than the cardinality of the training set.
We show in this work how its performance can be further improved by exploiting some extra knowl-
edge on the problem. In fact, real –world classi ﬁcation prob
lems often are composed of datasets
with labeled and unlabeled data [11, 12]: for this reason an interesting challenge is ﬁnding a way to
exploit the unlabeled data for obtaining tighter bounds and, therefore, better error estimations.

In this paper, we present two methods for exploiting the unlabeled data in the Rademacher Com-
plexity theory [2]. First, we show how the unlabeled data can have a role in reducing the conﬁdence

1See, for example, the NIPS 2004 Workshop (Ab)Use of Bounds or the 2002 Neurocolt Workshop on Bounds
less than 0.5

1

term, by obtaining a new bound that takes into account both labeled and unlabeled data. Then, we
propose a method, based on [7], which exploits the unlabeled data for selecting a better hypothesis
space, which the classi ﬁer belongs to, resulting in a much sh arper and accurate bound.

2 Theoretical framework and results

We consider the following prediction problem: based on a random observation of X ∈ X ⊆ Rd
one has to estimate Y ∈ Y ⊆ {−1, 1} by choosing a suitable prediction rule f : X → [−1, 1].
The generalization error L(f ) = E{X ,Y } ℓ(f (X ), Y ) associated to the prediction rule is deﬁned
through a bounded loss function ℓ(f (X ), Y ) : [−1, 1] × Y → [0, 1]. We observe a set of labeled
nu )(cid:9).
nl )(cid:9) and a set of unlabeled ones Dnu : (cid:8)(X u
samples Dnl : (cid:8)(X l
1 ), · · · , (X u
1 , Y l
1 ), · · · , (X l
nl , Y l
The data consist of a sequence of independent, identically distributed (i.i.d.) samples with the same
distribution P (X , Y ) for Dnl and Dnu . The goal is to obtain a bound on L(f ) that takes into
account both the labeled and unlabeled data. As we do not know the distribution that have generated
the data, we do not know L(f ) but only its empirical estimation Lnl (f ) = 1/nl Pnl
i ).
i=1 ℓ(f (X l
i ), Y l
In the typical context of Structural Risk Minimization (SRM) [13] we deﬁne an inﬁnite sequence of
hypothesis spaces of increasing complexity {Fi , i = 1, 2, · · · }, then we choose a suitable function
space Fi and, consequently, a model f ∗ ∈ Fi that ﬁts the data. As we do not know the true data
distribution, we can only say that:
(1)
f ∈Fi {L(f ) − Lnl (f )}
{L(f ) − Lnl (f )}f ∈Fi ≤ sup

or, equivalently:

∀f ∈ Fi
f ∈Fi {L(f ) − Lnl (f )} ,
L(f ) ≤ Lnl (f ) + sup
In this framework, the SRM procedure brings us to the following choice of the function space and
the corresponding optimal classi ﬁer:
f ∈Fi {L(f ) − Lnl (f )}#
Fi∈{F1 ,F2 ,··· } " min
f ∗ , F ∗ :
Lnl (f )f ∈Fi + sup
min
f ∈Fi
Since the generalization bias (supf ∈Fi {L(f ) − Lnl (f )}) is a random variable, it is possible to
statistically analyze it and obtain a bound that holds with high probability [5].

arg

(3)

(2)

ℓS (fS (x), y) =

ℓH (fH (x), y) =

From this point, we will consider two types of prediction rule with the associated loss function:
1 − yfH (x)
fH (x) =sign(wT φ(x) + b),
2
fS (x) = (cid:26)min(1, wT φ(x) + b)
if wT φ(x) + b > 0
1 − yfS (x)
if wT φ(x) + b ≤ 0
max(−1, wT φ(x) + b)
2
where φ(·) : Rd → RD with D >> d, w ∈ RD and b ∈ R. The function φ(·) is introduced to
allow for a later introduction of kernels, even though, for simplicity, we will focus only on the linear
case. Note that both the hard loss ℓH (fH (x), y) and the soft loss (or ramp loss) [14] ℓS (fS (x), y)
are bounded ([0, 1]) and symmetric (ℓ(f (x), y) = 1 − ℓ(f (x), −y)). Then, we recall the deﬁnition
of Rademacher Complexity (R) for a class of functions F :
nl
nl
2
Xi=1
Xi=1
ˆRnl (F ) = Eσ sup
σi ℓ(f (xi ), yi ) = Eσ sup
nl
f ∈F
f ∈F
where σ1 , . . . , σnl are nl independent Rademacher random variables, i.e. independent random vari-
ables for which P(σi = +1) = P(σi = −1) = 1/2, and the last equality holds if we use one
ˆR is a computable realization of the expected Rademacher
of the losses deﬁned before. Note that
ˆR(F ). The most renowed result in Rademacher Complexity theory
Complexity R(F ) = E(X ,Y )
states that [2]:
L(f )f ∈F ≤ Lnl (f )f ∈F + ˆRnl (F ) + 3s log (cid:0) 2
δ (cid:1)
2nl
which holds with probability (1 − δ) and allows to solve the problem of Eq. (3).

σi f (xi )

1
nl

(7)

(4)

(5)

(6)

,

2

2.1 Exploiting unlabeled samples for reducing the conﬁdenc e term

Assuming that the amount of unlabeled data is larger than the number of labeled samples, we split
them in blocks of similar size by deﬁning the quantity m = ⌊nu /nl ⌋ + 1, so that we can consider a
composed of mnl pattern. Then, we can upper bound the expected generaliza-
ghost sample D ′mnl
tion bias in the following way 2 :
E{X ′ ,Y ′ } 
ℓi
f ∈F 
ℓ′k 
i·nl
nl
m
1
1
1
Xi=1
Xk=(i−1)·nl+1
Xi=1
f ∈F {L(f ) − Lnl (f )} = E{X ,Y } sup
E{X ,Y } sup
 −


m
nl
nl
Xk=(i−1)·nl+1 (cid:16)ℓ′k − ℓ|k|nl (cid:17)
f ∈F 
i·nl
m
1
Xi=1
≤ E{X ,Y }E{X ′ ,Y ′ }
sup


m
σ|k|nl hℓ′k − ℓ|k|nl i
f ∈F 
i·nl
m
1
Xk=(i−1)·nl+1
Xi=1
= E{X ,Y }E{X ′ ,Y ′ }Eσ
sup


nl
ℓk 
f ∈F 
i·nl
m
m
1
Xk=(i−1)·nl+1
Xi=1
Xi=1
ˆRi
σ|k|nl
 = E{X ,Y }
sup
nl (F )

m
where |k |nl = (k − 1) mod (nl ) + 1. The last quantity (that we call Expected Extended Rademacher
ˆRnu (F )) and the expected generalization bias are both deterministic quantities
Complexity E{X ,Y }
and we know only one realization of them, dependent on the sample. Then, we can use the McDi-
armid’s inequality [15] to obtain:

≤ E{X ,Y }Eσ

1
m

1
m

1
nl

2
nl

(10)

(11)

f ∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F ) + ǫ# ≤
P " sup
f ∈F {L(f ) − Lnl (f )} + aǫ# +
P " sup
f ∈F {L(f ) − Lnl (f )} ≥ E{X ,Y } sup
ˆRnu (F ) ≥ ˆRnu (F ) + (1 − a)ǫi ≤
P hE{X ,Y }
(mnl )
(1−a)2 ǫ2
e−2nl a2 ǫ2
+ e−
2
√m
with a ∈ [0, 1]. By choosing a =
2+√m , we can write:
nl (F ) + ǫ# ≤ 2e−
P " sup
m
Xi=1
ˆRi
f ∈F {L(f ) − Lnl (f )} ≥
and obtain an explicit bound which holds with probability (1 − δ):
√m s log (cid:0) 2
2 + √m
δ (cid:1)
m
Xi=1
2nl
where ˆRi
nl (F ) is the Rademacher Complexity of the class F computed on the i-th block of
unlabeled data. Note that for m = 1 the training set does not contain any unlabeled data
and the bound given by Eq.
(3) is recovered, while for large m the conﬁdence term is re-
duced by a factor of 3. At a ﬁrst sight, it would seem impossibl e to compute the term ˆRi
nl
without knowing the labels of the data, but it is easy to show that this is not the case.
In
= +1o and K−i =
i = nk ∈ {k = (i − 1) · nl + 1, . . . , i · nl } : σ|k|nl
let us deﬁne K+
fact,
2we deﬁne ℓ(f (xi ), yi ) ≡ ℓi to simplify the notation

L(f )f ∈F ≤ Lnl (f )f ∈F +

ˆRi
nl (F ) +

2mnl ǫ2
(2+√m)2

1
m

1
m

(12)

(8)

(9)

(13)

3

(a) Coventional function classes

(b) Localized function classes

Figure 1: The effect of selecting a better center for the hypothesis classes.

= 1 +

= −1o, then we have:
nk ∈ {k = (i − 1) · nl + 1, . . . , i · nl } : σ|k|nl
1
nl 
m
1
2
ℓ(fk , yk ) − Xk∈K+
ℓ(fk , yk ) − Xk∈K−i
 Xk∈K+
Xi=1
ˆRnu (F ) = 1 +
Eσ sup

m
f ∈F
i
i
ℓ(fk , yk )
f ∈F 
m
1
2
2
nl Xk∈K−i
nl Xk∈K+
Xi=1
ℓ(fk , −yk ) −
−
Eσ sup

m
i
f ∈F 
yk )
i·nl
m
2
Xi=1
Xk=(i−1)·nl+1
= 1 +
−
Eσ sup
ℓ(fk , −σ|k|nl

nl
)
f ∈F 
i·nl
m
2
Xi=1
Xk=(i−1)·nl+1
= 1 −
Eσ inf


nl
which corresponds to solving a classi ﬁcation problem using all the available data with random labels.
The expectation can be easily computed with some Monte Carlo trials.

ℓ(fk , σ|k|nl

1
m

1
m

2.2 Exploiting the unlabeled data for tightening the bound

Another way of exploiting the unlabeled data is to use them for selecting a more suitable sequence of
hypothesis spaces. For this purpose we could use some of the unlabeled samples or, even better, the
nc = nu − ⌊nu /nl ⌋ nl samples left from the procedure of the previous section. The idea is inspired
by the work of [3] and [7], which propose to inﬂate the hypothe sis classes by centering them around
a ‘good’ classi ﬁer. Usually, in fact, we have no a-priori inf ormation on what can be considered a
good choice of the class center, so a natural choice is the origin [13], as in Figure 1(a). However,
if it happens that the center is ‘close’ to the optimal classi ﬁer, the search for a suitable class will
stop very soon and the resulting Rademacher Complexity will be consequently reduced (see Figure
1(b)). We propose here a method for ﬁnding two possible ‘good ’ centers for the hypothesis classes.
Let us consider nc unlabeled samples and run a clustering algorithm on them, by setting the number
of clusters to 2, and obtaining two clusters C1 and C2 . We build two distinct labeled datasets by
assigning the labels +1 and −1 to C1 and C2 , respectively, and then vice-versa. Finally, we build
two classi ﬁers fC1 (x) and fC2 (x) = −fC1 (x) by learning the two datasets3 . The two classi ﬁers,
which have been found using only unlabeled samples, can then be used as centers for searching
a better hypothesis class.
It is worthwhile noting that any supervised learning algorithm can be
used [16], because the centers are only a hint for a better centered hypothesis space: their actual
classi ﬁcation performance is not of paramount importance. The underlying principle that inspired

3Note that we could build only one classiﬁer by assigning the most probable la bels to the nc samples,
according to the nl labeled ones but, rigorously speaking, this is not allowed by the SRM principle, because
it would lead to use the same data for both choosing the space of functions and computing the Rademacher
Complexity.

4

this procedure relies on the reasonable hypothesis that P(X ) is correlated with P(X , Y ): in fact, in
an unlucky scenario, where the two classes are heavily overlapped, the method would obviously fail.

+ e−

(mnl )(1−a)2 ǫ2
ˆRnu (F )
2E{X ,Y }

Choosing a good center for the SRM procedure can greatly reduce the second term of the bound
given by Eq. (13) [7] (the bias or complexity term). Note, however, that the conﬁdence term is not
affected, so we propose here an improved bound, which makes this term depending on ˆRi
nl (F ) as
well. We use a recent concentration result for Self Bounding Functions [17], instead of the looser
McDiarmid’s inequality. The detailed proof is omitted due to space constraints and we give here
only the sketch (it is a more general version of the proof in [18] for Rademacher Complexities):
f ∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F ) + ǫ# ≤ e−2nl a2 ǫ2
P " sup
√m
, we obtain:
with a ∈ [0, 1]. Choosing a =
√m+2qE{X ,Y }
ˆRi
1
m Pm
(F )
nl
i=1
f ∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F ) + ǫ# ≤ 2e
P " sup
so that the following explicit bound holds with probability (1 − δ):
2qE{X ,Y }
ˆRnu (F ) + √m
s log (cid:0) 2
δ (cid:1)
√m
2nl
ˆRnu (F ) = 1 and we obtain again Eq. (13). Unfortunately, the
Note that, in the worst case, E{X ,Y }
Expected Extended Rademacher Complexity cannot be computed, but we can upper bound it with
its empirical version (see, for example, [19], pages 420–42 2, for a justi ﬁcaton of this step) as in
Eq.(10) to obtain:
f ∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F ) + ǫ# ≤ e−2nl a2 ǫ2
P " sup
with a ∈ [0, 1]. Differently from Eq. (15) the previous expression cannot be put in explicit form, but
it can be simply computed numerically by writing it as:

L(f )f ∈F ≤ Lnl (f )f ∈F + ˆRnu (F ) +

2mnl ǫ2
(√m+2√E{X ,Y }
ˆRnu (F ))2

(mnl )(1−a)2 ǫ2
2( ˆRnu (F )+(1−a)ǫ)

+ e−

−

(14)

(15)

(16)

(17)

1
m

L(f )f ∈F ≤ Lnl (f )f ∈F +

m
Xi=1
u can be obtained by upper bounding with δ the last term of Eq. (17) and solving the
The value ǫb
inequality respect to a and ǫ, so that the bound holds with probability (1 − δ).
We can show the improvements obtained through these new results, by plotting the values of the
conﬁdence terms and comparing them with the conventional on e [2]. Figure 2 shows the value of
u , as a function of the number of
ǫl in Eq. (7) against ǫu , the corresponding term in Eq. (13), and ǫb
samples.

ˆRi
nl (F ) + ǫb
u

(18)

3 Performing the Structural Risk Minimization procedure

Computing the values of the bounds described in the previous sections is a straightforward process,
at least in theory. The empirical error Lnl (f ) is found by learning a classi ﬁer with the original
labeled dataset, while the (Extended) Rademacher Complexity ˆRi
nl (F ) is computed by learning the
dataset composed of both labeled and unlabeled samples with random labels.
In order apply in practice the results of the previous section and to better control the hypothesis
space, we formulate the learning phase of the classi ﬁer as th e following optimization problem, based

5

m ∈ [1,10]

m = 1, R ∈ [0,1]

1

0.9

0.8

0.7

0.6

ε

0.5

0.4

0.3

0.2

0.1

0

 

 

ε
l
ε
u

m = 1

m = 2

m = 10

40

60

80

100

120
n

140

160

180

200

1

0.9

0.8

0.7

0.6

ε

0.5

0.4

0.3

0.2

0.1

0

 

 

ε
u
εb
u

R = 1

R = 0.9

R = 0

40

60

80

100

120

n

140

160

180

200

(a) ǫl VS ǫu

(b) ǫnl VS ǫb
u with m = 1

Figure 2: Comparison of the new conﬁdence terms with the conv entional one.

ηi

(19)

min
w,b,ξ

on the Ivanov version of the Support Vector Machine (I-SVM) [13]:
n
Xi=1
kw − ˆwk2 ≤ ρ2
yi (cid:0)wT φ(xi ) + b(cid:1) ≥ 1 − ξi
ξi ≥ 0,
ηi = min (2, ξi )
where the size of the hypothesis space, centered in ˆw , is controlled by the hyperparameter ρ and
the last constraint is introduced for bounding the SVM loss function, which would be otherwise
unbounded and would prevent the application of the theory developed so far. Note that, in practice,
ˆw = + ˆwC1 and the second one with ˆw =
two sub-problems must be solved: the ﬁrst one with
− ˆwC1 , then the solution corresponding to the smaller value of the objective function is selected.
Unfortunately, solving a classi ﬁcation problem with a boun ded loss function is computationally in-
tractable, because the problem is no longer convex and even state-of-the-art solvers like, for example,
CPLEX [20] fail to found an exact solution, when the training set size exceeds few tens of samples.
Therefore, we propose here to ﬁnd an approximate solution th rough well –known algorithms like,
for example, the Peeling [6] or the Convex–Concave Constrai ned Programming (CCCP) technique
[14, 21, 22]. Furthermore, we derive a dual formulation of problem (19) that allows us exploiting
the well known Sequential Minimal Optimization (SMO) algorithm for SVM learning [23].

min
w,b,ξ

Problem (19) can be rewritten in the equivalent Tikhonov formulation:
n
1
Xi=1
2 kw − ˆwk2 + C
ηi
yi (cid:0)wT φ(xi ) + b(cid:1) ≥ 1 − ξi
ηi = min (2, ξi )
ξi ≥ 0,
which gives the same solution of the Ivanov formulation for some value of C [13]. The method
for ﬁnding the value of C , corresponding to a given value of ρ, is reported in [10], where it is also
shown that C cannot be used directly to control the hypothesis space. Then, it is possible to apply
the CCCP technique, which is synthesized in Algorithm 1, by splitting the objective function in its
convex and concave parts:

(20)

min
w,b,ξ

Jconvex (θ)
Jconcave (θ)
z
{
}|
{
}|
z
n
n
1
Xi=1
Xi=1
2 kw − ˆwk2 + C
ςi
−C
ξi
yi (cid:0)wT φ(xi ) + b(cid:1) ≥ 1 − ξi
ςi = max(0, ξi − 2)
ξi ≥ 0,

6

(21)

n
Xi=1

min
w,b,ξ

where

d (−C ςi )
dθ

βi yi (cid:0)wT φ(xi ) + b(cid:1)

where θ = [w |b] is introduced to simplify the notation. Obviously, the algorithm does not guarantee
to ﬁnd the optimal solution, but it converges to a (usually go od) solution in a ﬁnite number of steps
[14]. To apply the algorithm we must compute the derivative of the concave part of the objective
function:
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)θt ! θ =
(cid:12)(cid:12)(cid:12)(cid:12)θt (cid:19) θ =   n
(cid:18) dJconcave (θ)
Xi=1
dθ
Then, the learning problem becomes:
n
n
1
Xi=1
Xi=1
∆i yi (cid:0)wT φ(xi ) + b(cid:1)
2 kw − ˆwk2 + C
ξi +
yi (cid:0)wT φ(xi ) + b(cid:1) ≥ 1 − ξi ,
ξi ≥ 0
∆i = (cid:26) C
if yi f t (xt ) < −1
otherwise
0
Finally, it is possible to obtain the dual formulation (derivation is omitted due to lack of space):

ˆαj yi ˆyj K ( ˆxj , xi ) − 1
nC1
n
n
n
Xi=1
Xj=1
Xi=1
Xj=1
 βi

βi yi = 0
− ∆i ≤ βi ≤ C − ∆i ,

βiβj yi yj K (xi , xj ) +

(22)

(23)

(24)

(25)

min
β

1
2

n
Xi=1
where we have used the kernel trick [24] K (·, ·) = φ(·)T φ(·).
4 A case study

We consider the MNIST dataset [25], which consists of 62000 images, representing the numbers
from 0 to 9: in particular, we consider the 13074 patterns containing 0’s and 1’s, allowing us to deal
with a binary classi ﬁcation problem. We simulate the small –
sample regime by randomly sampling
a training set with low cardinality (nl < 500), while the remaining 13074 − nl images are used as
a test set or as an unlabeled dataset, by simply discarding the labels. In order to build statistically
relevant results, this procedure is repeated 30 times.
In Table 1 we compare the conventional bound with our proposal. In the ﬁrst column the number
of labeled patterns (nl ) is reported, while the second column shows the number of unlabeled ones
(nu ). The optimal classi ﬁer f ∗ is selected by varying ρ in the range [10−6 , 1], and selecting the
function corresponding to the minimum of the generalization error estimate provided by each bound.
Then, for each case, the selected f ∗ is tested on the remaining 13074 − (nl + nu ) samples and the
classi ﬁcation results are reported in column three and four , respectively. The results show that the
f ∗ selected by exploiting the unlabeled patterns behaves better than the other and, furthermore, the
estimated L(f ), reported in column ﬁve and six, shows that the bound is tight er, as expected by
theory.

The most interesting result, however, derives from the use of the new bound of Eq. (18), as reported
in Table 2, where the unlabeled data is exploited for selecting a more suitable center of the hypoth-
esis space. The results are reported analogously to Table 1. Note that, for each experiment, 30%

Algorithm 1 CCCP procedure
Initialize θ0
repeat
θ t+1 = arg minθ Jconvex (θ) + (cid:16) dJconcave (θ)
dθ
until θ t+1 = θ t

(cid:12)(cid:12)(cid:12)θt (cid:17) θ
7

Table 1: Model selection and error estimation, exploiting unlabeled data for tightening the bound.
Test error of f ∗
Estimated L(f )
Eq. (7)
Eq. (13)
Eq. (7)
Eq. (13)
157.70 ± 0.97
13.20 ± 0.86
12.40 ± 0.82
194.00 ± 0.97
116.33 ± 1.06
142.00 ± 1.06
8.93 ± 1.29
8.93 ± 1.20
84.85 ± 0.59
103.00 ± 0.59
6.02 ± 0.17
6.26 ± 0.16
70.68 ± 0.48
85.50 ± 0.48
5.88 ± 0.13
5.95 ± 0.12
60.86 ± 0.40
73.70 ± 0.40
5.30 ± 0.07
5.61 ± 0.07
5.36 ± 0.21
5.51 ± 0.22
66.10 ± 0.37
54.62 ± 0.37
4.98 ± 0.40
5.36 ± 0.40
61.30 ± 0.33
50.82 ± 0.33
4.41 ± 0.53
4.08 ± 0.51
55.10 ± 0.28
45.73 ± 0.28
43.60 ± 0.26
52.40 ± 0.26
3.40 ± 0.64
3.59 ± 0.57
39.98 ± 0.19
48.10 ± 0.19
2.67 ± 0.48
2.75 ± 0.47
35.44 ± 0.22
42.70 ± 0.22
2.05 ± 0.03
2.07 ± 0.03
2.02 ± 0.04
1.94 ± 0.04
39.20 ± 0.17
32.57 ± 0.17
1.93 ± 0.02
1.79 ± 0.02
34.90 ± 0.19
29.16 ± 0.19

nl
10
20
40
60
80
100
120
150
170
200
250
300
400

nu
20
40
80
120
160
200
240
300
340
400
500
600
800

Table 2: Model selection and error estimation, exploiting unlabeled data for selecting a more suitable
hypothesis center.

nl
7
14
28
42
56
70
84
105
119
140
175
210
280

nu
3
6
12
18
24
30
36
45
51
60
75
90
120

Test error of f ∗
Eq. (7)
Eq. (18)
8.98 ± 1.12
13.20 ± 0.86
5.10 ± 0.67
8.93 ± 1.20
6.26 ± 0.16
3.05 ± 0.23
5.95 ± 0.12
2.36 ± 0.23
5.61 ± 0.07
1.96 ± 0.14
1.63 ± 0.11
5.36 ± 0.21
1.44 ± 0.11
4.98 ± 0.40
1.27 ± 0.09
4.41 ± 0.53
1.20 ± 0.08
3.59 ± 0.57
2.75 ± 0.47
1.08 ± 0.09
2.07 ± 0.03
0.92 ± 0.05
0.81 ± 0.07
2.02 ± 0.04
1.93 ± 0.02
0.70 ± 0.06

Estimated L(f )
Eq. (7)
Eq. (18)
104.01 ± 1.62
219.15 ± 0.97
86.70 ± 0.01
159.79 ± 1.06
115.58 ± 0.59
51.35 ± 0.00
95.77 ± 0.48
38.37 ± 0.00
82.59 ± 0.40
31.39 ± 0.00
26.83 ± 0.00
74.05 ± 0.37
23.77 ± 0.00
68.56 ± 0.33
20.36 ± 0.00
61.59 ± 0.28
18.77 ± 0.00
58.50 ± 0.26
53.72 ± 0.19
16.82 ± 0.00
47.73 ± 0.22
14.52 ± 0.00
12.91 ± 0.00
43.79 ± 0.17
38.88 ± 0.19
10.86 ± 0.00

of the data (nu ) are used for selecting the hypothesis center and the remaining ones (nl ) are used
for training the classi ﬁer. The proposed method consistent
ly selects a better classi ﬁer, which reg-
isters a threefold classi ﬁcation error reduction on the tes t set, especially for training sets of smaller
cardinality. The estimation of L(f ) is largely reduced as well.
We have to consider that this very clear performance increase is also favoured by the characteristics
of the MNIST dataset, which consists of well –separated clas ses: this particular data distribution im-
plies that only few samples sufﬁce for identifying a good hyp othesis center. Many more experiments
with different datasets and varying the ratio between labeled and unlabeled samples are needed, and
are currently underway, for establishing the general validity of our proposal but, in any case, these
results appear to be very promising.

5 Conclusion

In this paper we have studied two methods which exploit unlabeled samples to tighten the
Rademacher Complexity bounds on the generalization error of linear (kernel) classi ﬁers. The ﬁrst
method improves a very well –known result, while the second o ne aims at changing the entire ap-
proach by selecting more suitable hypothesis spaces, not only acting on the bound itself. The recent
literature on the theory of bounds attempts to obtain tighter bounds through more reﬁned concentra-
tion inequalities (e.g. improving Mc Diarmid’s inequality), but we believe that the idea of reducing
the size of the hypothesis space is a more appealing ﬁeld of re search because it opens the road to
possible signi ﬁcant improvements.

References

[1] V.N. Vapnik and A.Y. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and its Applications, 16:264, 1971.

8

[2] P.L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. The Journal of Machine Learning Research, 3:463–482, 2003.
[3] P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of
Statistics, 33(4):1497–1537, 2005.
[4] O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning
Research, 2:499–526, 2002.
[5] P.L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error estimation. Machine
Learning, 48(1):85–113, 2002.
[6] D. Anguita, A. Ghio, and S. Ridella. Maximal discrepancy for support vector machines. Neu-
rocomputing, 74(9):1436–1443, 2011.
[7] D. Anguita, A. Ghio, L. Oneto, and S. Ridella. Selecting the Hypothesis Space for Improv-
ing the Generalization Ability of Support Vector Machines. In The 2011 International Joint
Conference on Neural Networks (IJCNN), San Jose, California. IEEE, 2011.
[8] S. Arlot and A. Celisse. A survey of cross-validation procedures for model selection. Statistics
Surveys, 4:40–79, 2010.
[9] B. Efron and R. Tibshirani. An introduction to the bootstrap. Chapman & Hall/CRC, 1993.
[10] D. Anguita, A. Ghio, L. Oneto, and S. Ridella. In-sample Model Selection for Support Vector
Machines. In The 2011 International Joint Conference on Neural Networks (IJCNN), San Jose,
California. IEEE, 2011.
[11] K.P. Bennett and A. Demiriz. Semi-supervised support vector machines. In Advances in neural
information processing systems 11: proceedings of the 1998 conference, page 368. The MIT
Press, 1999.
[12] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning. The MIT Press, page 528,
2010.
[13] V.N. Vapnik. The nature of statistical learning theory. Springer Verlag, 2000.
[14] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability.
In Pro-
ceedings of the 23rd international conference on Machine learning, pages 201–208. ACM,
2006.
[15] C. McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148–
188, 1989.
[16] S. Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR Upper Saddle
River, NJ, USA, 1994.
[17] S. Boucheron, G. Lugosi, and P. Massart. On concentration of self-bounding functions. Elec-
tronic Journal of Probability, 14:1884–1899, 2009.
[18] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities using the entropy method.
The Annals of Probability, 31(3):1583–1614, 2003.
[19] G. Casella and R.L. Berger. Statistical inference. 2001.
[20] I. CPLEX. 11.0 users manual. ILOG SA, 2008.
[21] J. Wang, X. Shen, and W. Pan. On efﬁcient large margin sem isupervised learning: Method and
theory. Journal of Machine Learning Research, 10:719–742, 2009.
[22] J. Wang and X. Shen. Large margin semi –supervised learn ing. Journal of Machine Learning
Research, 8:1867–1891, 2007.
[23] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector ma-
chines. Advances in Kernel MethodsSupport Vector Learning, 208:1–21, 1998.
[24] J. Shawe-Taylor and N. Cristianini. Margin distribution and soft margin. Advances in Large
Margin Classi ﬁers , pages 349–358, 2000.
[25] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of
deep architectures on problems with many factors of variation. In 24th ICML, pages 473–480,
2007.

9


Gaussian process modulated renewal processes

Vinayak Rao
Gatsby Computational Neuroscience Unit
University College London
vrao@gatsby.ucl.ac.uk

Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
ywteh@gatsby.ucl.ac.uk

Abstract

Renewal processes are generalizations of the Poisson process on the real line
whose intervals are drawn i.i.d. from some distribution. Modulated renewal pro-
cesses allow these interevent distributions to vary with time, allowing the introduc-
tion of nonstationarity. In this work, we take a nonparametric Bayesian approach,
modelling this nonstationarity with a Gaussian process. Our approach is based on
the idea of uniformization, which allows us to draw exact samples from an oth-
erwise intractable distribution. We develop a novel and efﬁcient MCMC sampler
for posterior inference. In our experiments, we test these on a number of synthetic
and real datasets.

1

Introduction

Renewal processes are stochastic point processes on the real line where intervals between succes-
sive points (times) are drawn i.i.d.
from some distribution. The simplest example of a renewal
process is the homogeneous Poisson process, whose interevent times are exponentially distributed.
A limitation of this is the memoryless property of the exponential distribution, resulting in an ‘as
bad as old after a repair’ property [1] that is not true of many real-world phenomena. For example,
immediately after ﬁring, a neuron is depleted of its resources and incapable of ﬁring again, and the
gamma distribution is used to model interspike intervals [2]. Similarly, because of the phenomenon
of elastic rebound, some time is required to recharge stresses released after an earthquake and an
inverse Gaussian distribution is used to model intervals between major earthquakes [3]. Other ex-
amples include using the Pareto distribution to better capture the burstiness and self-similarity of
network trafﬁc arrival times [4], and the Erlang distribution to model the fact that buying incidence
of frequently purchased goods is less variable than Poisson [5].
Modelling interevent times as i.i.d. draws from a general renewal density can allow larger or smaller
variances than an exponential with the same mean (overdispersion or underdispersion), but effec-
tively encodes an ‘as good as new after a repair’ property. Again, this is often only an approximation:
because of age or other time-varying factors, the interevent distribution of the point process may vary
with time. For instance, internet trafﬁc can vary with time of the day, day of the week and in re-
sponse to advertising and seasonal trends. Similarly, an external stimulus can modulate the ﬁring
rate of the neuron, economic trends can modulate ﬁnancial transactions etc. The most popular way
of modelling this nonstationarity is via an inhomogeneous Poisson process whose intensity function
determines the instantaneous event rate, and there has also been substantial work extending this to
renewal processes in various ways (see section 2.2).
In this paper, we describe a nonparametric Bayesian approach where a renewal process is modulated
by a random intensity function which is given a Gaussian process prior. Our approach extends
work by [6] on the Poisson process, using a generalization of the idea of Poisson thinning called
uniformization [7] to draw exact samples from the model. We extend recent ideas from [8] to
develop a more natural and efﬁcient block Gibbs sampler than the incremental Metropolis-Hastings
algorithm used in [6]. In our experiments we demonstrate the usefulness of our model and sampler
on a number of synthetic and real-world datasets.

1

2 Modulated renewal processes
Consider a renewal process R over an interval [0, T ] whose interevent time is distributed according
to a renewal density g . Let G = {G1 , G2 , ...} be the ordered set of event times sampled from this
renewal process, i.e.

h(τ ) =

G ∼ R(g)
(1)
For simplicity1 we place a starting event G0 at time 0, so for each i ≥ 1 we have (Gi − Gi−1 ) ∼ g .
Associated with the renewal density g is a hazard function h, where h(τ )∆, for inﬁnitesimal ∆ > 0,
is the probability of the interevent interval being in [τ , τ + ∆] conditioned on it being at least τ , i.e.
1 − (cid:82) τ
g(τ )
0 g(u)du
Let λ(t) be some time-varying intensity function. A simple way to introduce nonstationarity into
a renewal process is to modulate the hazard function by λ(t) so that it depends on both the time τ
since the last event, and on the absolute time t [9, 10]:
h(τ , t) ≡ m(h(τ ), λ(t))
(3)
where m(·, ·) is some interaction function. Examples include additive (h(τ ) + λ(t)) and multi-
plicative (h(τ )λ(t)) interactions. For concreteness, we assume multiplicative interactions in what
follows, however our results extend easily to general interaction functions.
With a modulated hazard rate, the distribution of interevent times is no longer stationary. Instead,
(cid:18)
(cid:19)
plugging a multiplicative interaction into (2) and solving for g (see the supplementary material for
(cid:90) τ
details), we get
λ(tprev + u)h(u)du
0
where tprev is the previous event time. Observe that equation (4) encompasses the inhomogeneous
Poisson process as a special case (a constant hazard function with multiplicative modulation).
2.1 Gaussian process intensity functions

g(τ |tprev ) = λ(tprev + τ )h(τ ) exp

(2)

(4)

−

In this paper we are interested in estimating both parameters of the hazard function h(τ ) as well as
the intensity function λ(t) itself. Taking a Bayesian nonparametric approach, we model λ(t) using
a Gaussian process (GP) [11] prior, which has support over a rich class of functions and offers a
ﬂexibility not afforded by parametric approaches. We call the resulting model a Gaussian process
modulated renewal process. A minor issue is that samples from a GP can take negative values; we
(cid:82) ∞
address this using a sigmoidal link function. Finally, we use a gamma family for the hazard function:
xγ−1 e−γx
x uγ−1 e−γu du where γ is the shape parameter2 . Our complete model is thus
h(τ ) =
l(·) ∼ GP (µ, K ),
λ(·) = λ∗σ(l(·)),
G ∼ R(λ(·), h(·))
(5)
where µ and K are the GP mean and covariance kernel, λ∗ is a positive scale parameter, and
σ(x) = (1 + exp(−x))−1 . We place a gamma hyperprior on λ∗ as well as hyperpriors on the
GP hyperparameters.
2.2 Related work

The idea of deﬁning a nonstationary renewal process by modulating the hazard function dates back
to Cox [9]. Early work [12] focussed on hypothesis testing for the stationarity assumption. [13,
14, 1] proposed parametric (generalized linear) models where the intensity function was a linear
combination of some known functions; these regression coefﬁcients were estimated via maximum
likelihood.
[15] considers general modulated hazard functions as well; however they assume it
has known form and are concerned with calculating statistical properties of the resulting process.

1With renewal processes there is an ambiguity about the time of the ﬁrst event, which is typically taken to
be exponentially distributed. It is straightforward to handle this case.
2We parametrize the hazard function to produce 1 event per unit time; other parametrizations may be used.

2

Finally, [10] describe a model that is a generalization of ours, but again have to resort to maximum
likelihood estimation (our ideas can easily be extended to their more general model too).
A different approach to producing inhomogeneity is by ﬁrst sampling from a homogeneous renewal
process and then rescaling time [16, 17]. The trend renewal process [18] uses such an approach, and
the authors propose an iterative kernel smoothing scheme to approximate a maximum likelihood
estimate of the intensity function. [2] uses time-rescaling to introduce inhomogeneity and, similar
to us, a Gaussian process prior for the intensity function. Unlike us, they had to discretize time and
used a variational approach to inference.
Finally, we note that our approach generalizes [6], who describe a doubly stochastic Poisson process
and an MCMC sampler which does not require time discretization. In the next sections we describe
a generalization of their model to the inhomogeneous renewal process using a twist on a classical
idea called uniformization.
3 Sampling via Uniformization

Before we consider Markov chain Monte Carlo (MCMC) inference for our model, observe that
to even na¨ıvely generate samples from the prior is difﬁcult; this requires evaluating integrals of a
continuous-time function drawn from a GP (see equation (4)). One approach is to evaluate these
integrals numerically by discretizing time [2], which can be time consuming and introduce approx-
imation errors. In section 3.2 we will show how a classical idea called uniformization allows us to
efﬁciently draw exact samples from the model, without approximations due to discretization. Then
in section 4 we will develop a novel MCMC algorithm based on uniformization.
3.1 Modulated Poisson processes

We start with thinning, a well-known result to sample from an inhomogeneous Poisson process with
intensity λ(t). Suppose that λ(t) is upper bounded by some constant Ω. Let E be a set of locations
sampled from a homogeneous Poisson process with rate Ω. We thin this set by deleting each point
e ∈ E independently with probability 1 − λ(e)
Ω . Let F be the remaining set of points. Then:
Proposition 1 ([19]). The set F is a draw from a Poisson process with intensity function λ(t) .

3.2 Modulated renewal processes

Less well-known is a generalization of this result to renewal processes [13]. Note that the thinning
result of the previous section builds on the memoryless property of the exponential distribution (or
the complete randomness [20] of the Poisson process): events in disjoint sets occur independently
of each other. For a renewal process, events are no longer independent of their neighbours. This
suggests a generalization of thinning involving a Markov chain over the set of events. This idea of
thinning a Poisson process by a subordinated Markov chain is called uniformization [7].
[21] describes a uniformization scheme to sample from a homogeneous renewal process. We extend
it to the modulated case here. We will assume that both the intensity function λ(t) and the hazard
function h(τ ) are bounded, so that there exists a constant Ω such that
Ω ≥ max
t,τ
Note that because of the sigmoidal link function, our model has λ(t) ≤ λ∗ , while the gamma hazard
h(τ ) is bounded by the shape parameter γ if γ ≥ 1. We now sample a set of times E = {E0 =
0, E1 , E2 , . . .} from a homogeneous Poisson process with rate Ω and thin this set by running a
discrete time Markov chain on the times in E . Let Y0 = 0, Y1 , Y2 , . . . be an integer-valued Markov
chain, where each Yi either equals Yi−1 or i. We interpret Yi as indicating the index of the last
unthinned event prior or equal to Ei . That is, Yi = Yi−1 means that Ei is thinned, and Yi = i means
Ei is not thinned. Note that Ei − EYi gives the time since the last unthinned event. For i > j ≥ 0,
deﬁne the transition probabilities of the Markov chain (conditioned on E ) as follows,
p(Yi = j |Yi−1 = j ) = 1 − h(Ei − Ej )λ(Ei )
h(Ei − Ej )λ(Ei )
p(Yi = i|Yi−1 = j ) =
,
Ω
Ω
After drawing a sample from Y , we deﬁne F = {Ei ∈ E s.t. Yi = i}.

h(τ )λ(t)

(6)

(7)

3

Proposition 2. For any Ω ≥ maxt,τ h(τ )λ(t), F is a sample from a modulated renewal process
with hazard h(·) and modulating intensity λ(·).
The proof of this is included in the supplementary material. The basic idea is to write down the
probability p(E , Y ) of the whole generative process and marginalize out the thinned times, showing
that the resulting interevent time is simply (4). For a different proof of a similar result, see [13].
Now recall that we have a GP prior for l(·). The uniformization procedure above only requires the
intensity function evaluated at the times in E (which is ﬁnite on a ﬁnite interval), and this is easily
obtained by sampling from a ﬁnite dimensional Gaussian N (µE , KE ), with mean and covariance
being the corresponding GP parameters µ and K evaluated at E . Our procedure to sample from a
GP-modulated renewal process now follows: sample from a homogeneous Poisson process P (Ω)
on [0, T ], instantiate the GP on this ﬁnite set of points and then thin the set by running the Markov
chain described previously. Deﬁning lE as l(t) evaluated on the set E , E ∗
i as the restriction of E to
(cid:16)λ(Fi )h(Fi−Fi−1 )
(cid:16)
(cid:17)|F |+1(cid:89)
(cid:17)
|F |(cid:89)
(cid:89)
the interval (Fi−1 , Fi ), and Fi+1 = T we can write the joint distribution:
1 − λ(e)h(e−Fi−1 )
P (F , l, E ) = Ω|E | e−ΩT N (lE |µE , KE )
Ω
Ω
e∈E ∗
i=1
i=1
Inference
i

(8)

4

We now consider posterior inference on the modulating function λ(t) (and any unknown hyperpa-
rameters) given an observed set of event times G. Our sampling algorithm is based on ideas devel-
oped in [8]. We imagine G was generated via uniformization, so that there exists an unobserved set
of thinned events ˜G. We then proceed by Markov chain Monte Carlo, setting up a Markov chain
whose state consists of the number and locations of ˜G, the values of the GP on the set G ∪ ˜G as well
as the current sampled hyperparameters. Note from equation (8) that given these values, the value of
the modulating function at any other location is independent of the observations and can be sampled
from the conditional distribution of a multivariate Gaussian.
The challenge now is to construct a transition operator that results in this Markov chain having
the desired posterior distribution as its equilibrium distribution. In their work, [6] deﬁned a transi-
tion operator by proposing insertions and deletions of thinned events as well as by perturbing their
locations. The proposals were accepted or rejected using a Metropolis-Hastings correction. The
remaining variables were updated using standard Gaussian process techniques. We show below that
instead of incrementally updating ˜G, it is actually possible to produce a new independent sample of
the entire set ˜G (conditioned on all other variables). This leads to a more natural sampler that does
not require any external tuning and that mixes more rapidly.
(cid:33)
(cid:32)
To understand our algorithm, suppose ﬁrst that the modulating function λ(t) is known for all t.
(cid:90) Gi
|G|+1(cid:89)
|G|(cid:89)
Then, from (4), the probability of the set of events G on the interval [0, T ] is3 :
λ(t)h(t − Gi−1 )dt
−
Gi−1
i=1
i=1
Now, suppose that in each consecutive interval (Gi−1 , Gi ) we independently sample a set of events
i from an inhomogeneous Poisson process with rate (Ω − λ(t)h(t − Gi−1 )), and let ˜G = ∪ ˜G∗
˜G∗
exp

(cid:32)
(cid:33) (cid:89)
i .
(cid:90) Gi
|G|+1(cid:89)
A little algebra shows that:
−
dt (Ω − λ(t)h(t − Gi−1 )
(Ω − λ(˜g)h(˜g − Gi−1 ))
(cid:32)
(cid:33)
(cid:90) Gi
|G|+1(cid:89)
|G|(cid:89)
Gi−1
˜g∈ ˜G∗
i=1
i
λ(t)h(t − Gi−1 )dt
λ(Gi )h(Gi − Gi−1 )
−
(10)
(cid:18) λ(Gi )h(Gi − Gi−1 )
(cid:18)
(cid:19) |G|+1(cid:89)
exp
(cid:89)
|G|(cid:89)
Gi−1
i=1
i=1
1 − λ(˜g)h(˜g − Gi−1 )
= Ω|G|+| ˜G| exp (−ΩT )
Ω
Ω
˜g∈ ˜G∗
i=1
i=1
i

λ(Gi )h(Gi − Gi−1 )

P ( ˜G, G|λ(t)) =

P (G|λ(t)) =

(cid:19)

(11)

exp

(9)

×

3Recall that G0 = 0. We also take G|G|+1 = T .

4

Comparing with equation (8), we have the following proposition:
Proposition 3. The sets (E , F ) and (G ∪ ˜G, G) are equivalent i.e. they have the same distribution.

In other words, given a set of event times G, the inhomogeneous Poisson process-distributed points
˜G can be taken to be the events thinned in the procedure of section 3.2. The only complication left
is that we do not know the function λ(t) everywhere. This is easily overcome by uniformization (in
fact, just by thinning, since we’re dealing with a Poisson process). Speciﬁcally, let G be the set of
observed events and ˜Gprev the previous set of thinned events. To sample the new set ˜G∗
i from the
Poisson process on [Gi−1 , Gi ] with rate (Ω − λ(t)h(t − Gi−1 )), we ﬁrst sample a set of points A
from a homogeneous Poisson process on [Gi−1 , Gi ] with rate Ω and instantiate the Gaussian process
on those points, conditioned on G ∪ ˜Gprev and lG∪ ˜Gprev
(note that all this involves is conditionally
sampling from a multivariate Gaussian4 ). Finally, we keep a ∈ A with probability 1− λ(a)h(a−Gi−1 )
.
Ω
Having resampled ˜G (and the associated set of GP values), we next must resample the value of
the GP at G. This does involve the sigmoid likelihood function, and we proceed by elliptical slice
sampling [22] 5 . Algorithm 1 lists the steps involved.

Algorithm 1 Blocked Gibbs sampler for GP-modulated renewal process on the interval [0, T ]
Input: Set of event times G, set of thinned times ˜Gprev and l instantiated at G ∪ ˜Gprev .
of the GP on G ∪ ˜Gnew .
Output: A new set of thinned times ˜Gnew and a new instantiation lG∪ ˜Gnew
1: Sample A ⊂ [0, T ] from a Poisson process with rate Ω.
(cid:16)
(cid:17)
2: Sample lA |lG∪ ˜Gprev
.
1 − λ∗ σ(l(a))h(a−Gi−1 )
3: Thin A, keeping element a ∈ A ∩ [Gi−1 , Gi ] with probability
.
Ω
4: Let ˜Gnew be the resulting set and l ˜Gnew
be the restriction of lA to this set. Discard ˜Gprev and
.
l ˜Gprev
5: Resample lG∪ ˜Gnew

using, for example, elliptical slice sampling.

The gamma prior on λ∗ is conjugate to the Poisson, resulting in a gamma posterior. We resampled
the GP hyperparameters using slice sampling [23] 5 , while parameters of the hazard function were
updated using Metropolis-Hastings moves along with equation (8).
4.1 Computational considerations

The inferential bottleneck in our model is the Gaussian process: sampling a GP on a set of points is,
in the worst case, cubic in the size of that set. In our model, each iteration sees on average |G| + 2|E |
values of the GP, where |G| is the number of observations and |E | is the average number of points
sampled from the subordinating Poisson process. Note that |E | varies from iteration to iteration
(being proportional to the scaling factor λ∗ ). Since we perform posterior inference on this quantity,
the complexity of our model can be thought to adapt to that of the problem. This is in contrast with
time-discretization approaches, where a resolution is picked beforehand, ﬁxing the complexity of the
inference problem accordingly. For instance, [2] use a resolution of 1ms to model neural spiking,
making it impossible to na¨ıvely deal with spike trains extending over more than a second. However
as they demonstrate in their work, instantiating a GP on a regular lattice allows the development
of fast approximate inference algorithms that scale linearly with the number of grid-points. In our
case, the Gaussian processes is sampled at random locations. Moreover, these locations change each
iteration, requiring the inversion of a new covariance matrix; this is the price we have to pay for an
exact sampler.
One approach is to try reduce the number of thinned events |E |. Recall that our generative approach
is to thin a sample from a subordinating, homogeneous Poisson process whose rate upper bounds
the modulated hazard rate. We can reduce the number of thinned events by subordinating to an
inhomogeneous Poisson process, one whose rate more closely resembles the instantaneous hazard
rate. Thus, instead of using a single constant λ∗ , one could use (say) a piecewise linear function
4 In particular, it does not require any sophisticated GP sampling algorithm
5Code available on Iain Murray’s website: http://homepages.inf.ed.ac.uk/imurray2/

5

λ∗ (t) The more segments we use, the more ﬂexibility we have; the price being the complexity of
resampling this function, and slower mixing because of correlations it introduces.
This however does not help if G, the number of observations itself is large. In such a situation one
has to call upon the vast literature concerning approximate inference for Gaussian processes [11].
The question then is how these approximation compare with those like [2]. We believe this is an
interesting question in its own right, and raises the possibility of approximate inference algorithms
that combine ideas from [2] with the adaptive nature of our approach.
5 Experiments

In this section we evaluate our model and sampler on a number of datasets. We used gamma dis-
tributed interevent times with shape parameter γ ≥ 1. When γ = 1, we recover the Poisson process,
and our model reduces to that of [6], while γ > 1 models ‘refractoriness’, where two events in quick
succession are less likely than under a Poisson process. When appropriate, we place a noninforma-
tive prior on the shape parameter: an exponential with rate 0.1 shifted to have a minimum value of
1. Note that for shape parameters less than 1, the renewal process becomes ‘bursty’ and the hazard
function becomes unbounded. This is an interesting scenario but beyond the scope of this paper.
An interesting issue concerns the identiﬁability of the shape parameter under our model. We ﬁnd
from our experiments that this is only a problem when the length scale of the intensity function is
comparable to the refractory period of the renewal process. The base rate of the modulated renewal
process (i.e. the rate when the intensity function is ﬁxed at 1) is set to the empirical rate of the ob-
served point process. As a result the identiﬁability of the shape parameter is a consequence of the
dispersion of the point process rather than of some sort of rate matching.
Synthetic data. Our ﬁrst set of experiments uses three synthetic datasets generated by modulating
a gamma renewal process (shape parameter γ = 3) with three different functions (see ﬁgure 1):
• λ1 (t) = 2 exp(t/5) + exp(−((t − 25)/10)2 ,
t ∈ [0, 50]: 44 events
• λ2 (t) = 5 sin(t2 ) + 6,
t ∈ [0, 5]: 12 events
• λ3 (t): a piecewise linear function ,
t ∈ [0, 100]: 153 events
Additionally, for each function, we also generated 10 test sets. We ran three settings of our model:
with the shape parameter ﬁxed to 1 (MRP Exp), with the shape parameter ﬁxed to the truth (MRP
Gam3), and with a hyperprior on the shape parameter (MRP Full). For comparison, we also ran
an approximate discrete-time sampler where the Gaussian process was instantiated on a regular grid
covering the interval of interest. In this case, all intractable integrals were approximated numerically
and we use elliptical slice sampling to run MCMC on this Gaussian vector.
Figure 1 shows the results from 5000 MCMC samples after a burn-in of 1000 samples. We quantify
these in Table 1 by calculating the l2 distance of the posterior means from the truth. We also calcu-
lated the mean predictive probabilities of the 10 test sequences. Not surprisingly, the inhomogeneous
Poisson process forms a poor approximation to the gamma renewal process; it underestimates the
intensity function required to produce a sequence of events with refractory intervals. Fixing the
shape parameter to the truth signiﬁcantly reduces the l2 error and increases the predictive probabil-
ities, but interestingly, for these datasets, the model with a prior on the shape parameter performs
comparably with the ‘oracle’ model. We have also included plots of the posterior distribution over
the gamma parameter; these are peaked around 3. Discretizing time into a 100 bins (Disc100) results
in comparable performance for the ﬁrst two datasets on the l2 error; for the third, (which spans a
longer interval and has a larger event count), we had to increase the resolution to 500 bins to improve
accuracy. Discretizing to 25 bins was never sufﬁcient. A conclusion is that with time discretization,
for a small bias, one must be conservative in choosing the time-resolution; however, evaluating a GP
on a ﬁne grid can result in slow mixing. Our sampler has the advantage of automatically picking the
‘right’ resolution. However as we discussed in the section on computation, time discretization has
its own advantages that make it a viable model [2].
Coal mine disaster data. For our next experiment, we ran our model on the coal mine disaster
dataset commonly used in the point process literature. This dataset records the dates of a series
of 191 coal mining disasters, each of which killed ten or more men [24]. Figure 2(left) shows the
posterior mean of the intensity function (surrounded by 1 standard deviation) returned by our model.
Not included is the posterior distribution over the shape parameter; this concentrated in the interval
1 to around 1.1, suggesting that the data is well modelled as an inhomogeneous Poisson process, and

6

Figure 1: Synthetic Datasets 1-3: Posterior mean intensities plotted against time (top) and gamma
shape posteriors (bottom)

l2 error
log pred. prob.
l2 error
log pred. prob.
l2 error
log pred. prob.

MRP Exp MRP Gam3 MRP Full
2.548
3.19
7.8458
-37.3712
-38.0703
-47.5469
56.2183
141.0067
58.4361
-2.945298
-3.280871
-3.704396
11.4167
13.4441
82.0289
-48.2777
-89.8787
-48.57

Disc25
4.089003
-41.646350
91.321069
-5.245478
122.335151
87.170034

Disc100
2.426973
-41.016425
57.896300
-3.848443
38.047332
-55.802997

Table 1: l2 distance from the truth and mean log-predictive probabilities of the held-out datasets for
synthetic datasets 1(top) to 3(bottom).

is in agreement with [24]. As sanity check, and to shed further light on the issue of identiﬁability, we
processed the dataset by deleting every alternate event. With such a transformation, a homogeneous
Poisson would reduce to a gamma renewal process with shape 2. Our model returns a posterior
peaked around 1.5 (in agreement with the form of the inhomogeneity). Note that the posteriors over
intensity functions are similar (except for the obvious scaling factor of about 2).
Spike timing data We next ran our model on neural spike train data recorded from grasshopper
auditory receptor cells [25]. This dataset is characterized by a relatively high ﬁring rate (∼ 150

Figure 2: Left: Posterior mean intensity for coal mine data with 1 standard deviation error bars
(plotted against time in years). Centre: Posterior mean intensity for ‘thinned’ coalmine data with 1
standard deviation error bars. Right: Gamma shape posterior for ‘thinned’ coal mine data.

7

01020304050−0.500.511.522.53Intensity  TruthMRP ExpMRP Gam3MRP FullDisc100012345−202468101202040608010001231234500.050.10.150.21234500.050.10.150.21234500.10.20.30.4185019001950−101234Intensity185019001950−10123411.5200.050.10.150.2Figure 3: Left: Posterior mean
intensity for neural data with
1 standard deviation error
bars. Superimposed is the log
stimulus (scaled and shifted).
Right:
Posterior over
the
gamma shape parameter.

Synthetic dataset 1
Mean ESS
Minimum ESS
93.45 ± 6.91
50.94 ± 5.21
19.34 ± 11.55
56.37 ± 10.30

Gibbs
MH

Time(sec)
77.85
345.44

Mean ESS
53.54 ± 8.15
47.83 ± 9.18

Coalmine dataset
Minimum ESS
24.87 ± 7.38
18.91 ± 6.45

Time(sec)
282.72
1703

Table 2: Sampler comparisons. Numbers are per 1000 samples.

Hz), making refractory effects more prominent. We plot the posterior distribution over the intensity
function given a sequence of 200 spikes in a 1.6 second interval. We also included the posterior dis-
tribution over gamma shape parameters in ﬁgure 3; this concentrates around 1.5, agreeing with the
refractory nature of neuronal ﬁring. The results above follow from using noninformative hyperpri-
ors; we have also plotted the log-transformed stimulus, an amplitude-modulated signal. In practice,
other available knowledge (viz. the shape parameter, the stimulus length-scale, the transformation
from the stimulus to the input of the neuron etc) can be used to make more accurate inferences.
Computational efﬁciency and mixing. For our ﬁnal experiment, we compare our proposed blocked
Gibbs sampler with the Metropolis-Hastings sampler of [6]. We ran both algorithms on two datasets,
synthetic dataset 1 from section 5 and the coal mine disaster dataset. All involved 20 MCMC runs
with 5000 iterations each (following a burn-in period of a 1000 iterations). For both datasets, we
evaluated the latent GP on a uniform grid of 200 points, calculating the effective sample size (ESS)
of each component of the Gaussian vectors (using R-CODA [26]). For each run, we return the
mean and the minimum ESS across all 200 components. In Table 2, we report these numbers: not
only does our sampler mix faster (resulting in larger ESSs), but also takes less computation time.
Additionally, our sampler is simpler and more natural to the problem, and does not require any
external tuning.

6 Discussion

We have described how to produce exact samples from a nonstationary renewal process whose haz-
ard function is modulated by a Gaussian process. Our scheme is based on the idea of uniformization,
and using this idea, we also develop a novel MCMC sampler. There are a number of interesting av-
enues worth following. First is the restriction that the hazard function be bounded: while this covers
a large and useful class of renewal processes, it is worth considering how our approach can be
extended to produce exact or approximate samples for renewal processes with unbounded hazard
functions. In any case, following [13], it is easy to extend our ideas to Bayesian inference for more
general point processes. Because of the latent Gaussian process, our approach will not scale well
to large problems; however there is a vast literature concerning approximate sampling for Gaussian
processes. An important question is how these approximations compare to approximations intro-
duced via time-discretization. Finally, even though we considered GP modulating functions, our
uniformization-based sampler will also be useful for Bayesian inference involving simpler priors on
modulating functions, eg. splines or Markov jump processes.

Acknowledgements

We thank the Gatsby Charitable Foundation for generous funding. We thank Ryan Adams and Iain
Murray for code and comments; and Jakob Macke and Lars Buesing for useful discussions. The
grasshopper data was collected by Ariel Rokem at Andreas Herz’s lab and provided through the
CRCNS program (http://crcns.org).

8

0500100015001.522.53Time (ms)11.5200.10.2References
[1] J. F. Lawless and K. Thiagarajah. A point-process model incorporating renewals and time trends, with
application to repairable systems. Technometrics, 38(2):131–138, 1996.
[2] John P. Cunningham, Byron M. Yu, Krishna V. Shenoy, and Maneesh Sahani. Inferring neural ﬁring rates
from spike trains using Gaussian processes. In Advances in Neural Information Processing Systems 20,
2008.
[3] T. Parsons. Earthquake recurrence on the south Hayward fault is most consistent with a time dependent,
renewal process. Geophysical Research Letters, 35, 2008.
[4] V. Paxson and S. Floyd. Wide area trafﬁc: the failure of Poisson modeling. IEEE/ACM Transactions on
Networking, 3(3):226–244, June 1995.
[5] C. Wu. Counting your customers: Compounding customer’s in-store decisions, interpurchase time and
repurchasing behavior. European Journal of Operational Research, 127(1):109–119, November 2000.
[6] Ryan P. Adams, Iain Murray, and David J. C. MacKay. Tractable nonparametric Bayesian inference in
Poisson processes with Gaussian process intensities. In Proceedings of the 26th International Conference
on Machine Learning (ICML), 2009.
[7] A. Jensen. Markoff chains as an aid in the study of Markoff processes. Skand. Aktuarietiedskr., 36:87–91,
1953.
[8] V. Rao and Y. W. Teh. Fast MCMC sampling for Markov jump processes and continuous time Bayesian
networks. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence, 2011.
[9] D.R. Cox. The statistical analysis of dependencies in point processes. In P.A. Lewis, editor, Stochastic
point processes, pages 55–56. New York: Wiley 1972, 1972.
[10] Robert E. Kass and Val ´erie Ventura. A spike-train probability model. Neural Computation, 13(8):1713–
1720, 2001.
[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[12] M. Berman. Inhomogeneous and modulated gamma processes. Biometrika, 68(1):143, 1981.
[13] Yosihiko Ogata. On Lewis’ simulation method for point processes. IEEE Transactions on Information
Theory, 27(1):23–31, 1981.
[14] Mark Berman and T. Rolf Turner. Approximating point process likelihoods with GLIM. Journal of the
Royal Statistical Society. Series C (Applied Statistics), 41(1):pp. 31–38, 1992.
[15] I. Sahin. A generalization of renewal processes. Operations Research Letters, 13(4):259–263, May 1993.
[16] Emery N. Brown, Riccardo Barbieri, Val ´erie Ventura, Robert E. Kass, and Loren M. Frank. The time-
rescaling theorem and its application to neural spike train data analysis. Neural computation, 14(2):325–
46, February 2002.
[17] I. Gerhardt and B. L. Nelson. Transforming renewal processes for simulation of nonstationary arrival
processes. INFORMS Journal on Computing, 21(4):630–640, April 2009.
[18] Bo Henry Lindqvist. Nonparametric estimation of time trend for repairable systems data. In V.V. Rykov,
N. Balakrishnan, and M.S. Nikulin, editors, Mathematical and Statistical Models and Methods in Relia-
bility, Statistics for Industry and Technology, pages 277–288. Birkhuser Boston, 2011.
[19] P. A. W. Lewis and G. S. Shedler. Simulation of nonhomogeneous Poisson processes with degree-two
exponential polynomial rate function. Operations Research, 27(5):1026–1040, September 1979.
[20] J. F. C. Kingman. Poisson processes, volume 3 of Oxford Studies in Probability. The Clarendon Press
Oxford University Press, New York, 1993. Oxford Science Publications.
[21] J George Shanthikumar. Uniformization and hybrid simulation/analytic models of renewal processes.
Oper. Res., 34:573–580, July 1986.
[22] Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling. JMLR: W&CP, 9,
2010.
[23] Iain Murray and Ryan Prescott Adams. Slice sampling covariance hyperparameters of latent Gaussian
models. In Advances in Neural Information Processing Systems 23, 2010.
[24] B. Y. R. G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66(1):191–193,
1979.
[25] Ariel Rokem, Sebastian Watzl, Tim Gollisch, Martin Stemmler, and Andreas V.M. Herz. Spike-Timing
Precision Underlies the Coding Efﬁciency of Auditory Receptor Neurons. Journal of Neurophysiology,
pages 2541–2552, 2006.
[26] Martyn Plummer, Nicky Best, Kate Cowles, and Karen Vines. CODA: Convergence diagnosis and output
analysis for MCMC. R News, 6(1):7–11, March 2006.

9


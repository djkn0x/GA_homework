Information Rates and Optimal Decoding in Large
Neural Populations

Kamiar Rahnama Rad Liam Paninski
Department of Statistics, Columbia University
{kamiar,liam}@stat.columbia.edu
http://www.stat.columbia.edu/˜liam/research/pubs/kamiar-ss-info.pdf

Abstract
Many fundamental questions in theoretical neuroscience involve optimal decod-
ing and the computation of Shannon information rates in populations of spiking
neurons. In this paper, we apply methods from the asymptotic theory of statistical
inference to obtain a clearer analytical understanding of these quantities. We ﬁnd
that for large neural populations carrying a ﬁnite total amount of information, the
full spiking population response is asymptotically as informative as a single obser-
vation from a Gaussian process whose mean and covariance can be characterized
explicitly in terms of network and single neuron properties. The Gaussian form
of this asymptotic sufﬁcient statistic allows us in certain cases to perform optimal
Bayesian decoding by simple linear transformations, and to obtain closed-form
expressions of the Shannon information carried by the network. One technical
advantage of the theory is that it may be applied easily even to non-Poisson point
process network models; for example, we ﬁnd that under some conditions, neural
populations with strong history-dependent (non-Poisson) effects carry exactly the
same information as do simpler equivalent populations of non-interacting Poisson
neurons with matched ﬁring rates. We argue that our ﬁndings help to clarify some
results from the recent literature on neural decoding and neuroprosthetic design.

Introduction
It has long been argued that many key questions in neuroscience can best be posed in information-
theoretic terms; the efﬁcient coding hypothesis discussed in [2, 3, 1], represents perhaps the best-
known example. Answering these questions quantitatively requires us to compute the Shannon
information rate of neural channels, whether numerically using experimental data or analytically
in mathematical models. In many cases it is useful to exploit connections with “ideal observer”
analysis, in which the performance of an optimal Bayesian decoder places fundamental bounds on
the performance of any biological system given access to the same neural information. However, the
non-linear, non-Gaussian, and correlated nature of neural responses has hampered the development
of this theory, particularly in the case of high-dimensional and/or time-varying stimuli.
The neural decoding literature is far too large to review systematically here; instead, we will focus
our attention on work which has attempted to develop an analytical theory to simplify these complex
decoding and information-rate problems. Two limiting regimes have received signiﬁcant analytical
attention in the neuroscience literature. In the “high-SNR” regime, n → ∞, where n is the number
of neurons encoding the signal of interest; if the information rate of each neuron is bounded away
from zero and neurons respond in a conditionally weakly-dependent manner given the stimulus, then
the total information provided by the neural population becomes inﬁnite, and the error rate of any
reasonable neural decoder tends to zero. For discrete stimuli, the Shannon information is effectively
determined in this asymptotic limit by a simpler quantity known as the Chernoff information [9];
for continuous stimuli, maximum likelihood estimation is asymptotically optimal, and the asymp-

1

totic Shannon information is controlled by the Fisher information [8, 7]. On the other hand we can
consider the “low-SNR” limit, where only a few neurons are observed and each neuron is asymptot-
ically weakly tuned to the stimulus. In this limit, the Shannon information tends to zero, and under
certain conditions the optimal Bayesian estimator (which can be strongly nonlinear in general) can
be approximated by a simpler linear estimator; see [5] and more recently [16] for details.
In this paper, we study information transmission and optimal decoding in what we would argue
is a more biologically-relevant “intermediate” regime, where n is large but the total amount of
information provided by the population remains ﬁnite, and the problem of decoding the stimulus
given the population neural activity remains nontrivial.
Likelihood in the intermediate regime: the inhomogeneous Poisson case
For clarity, we begin by analyzing the information in a simple population of neurons, represented as
inhomogenous Poisson processes that are conditionally independent given the stimulus. We will ex-
tend our analysis to more general neural populations in the next section. In response to the stimulus,
at each time step t neuron i ﬁres with probability λi (t)dt, where the rate is given by
(1)
λi (t) = f [bi (t) + #i,t (θ)] ,
where f (.) is a smooth rectifying non-linearity and  is a gain factor controlling each neuron’s
sensitivity. The baseline ﬁring rate is determined by bi (t) and is independent of the input signal.
The true stimulus at time t is deﬁned by θt , and θ abbreviates the time varying stimulus θ0:T in the
time interval [0, T dt]. The term #i,t (θ) summarizes the dependence of the neuron’s ﬁring rate on
θ; depending on the setting, this term may represent e.g. a tuning curve or a spatiotemporal ﬁlter
applied to the stimulus (see examples below).
The likelihood includes all the information about the stimulus encoded in the population’s spiking
response. Neuron i’s response at time step t is designated by by the binary variable ri (t). The log-
likelihood at the parameter value ϑ (which may be different from the true parameter θ) is given by
the standard point-process formula [21]:

n
!i=1
Lϑ (r) := log p(r|ϑ) =
This expression can be expanded around  = 0:
∂Lϑ(r)
∂ 

where

T
!t=0

ri (t) log λi (t) − λi (t)dt.

(2)

∂Lϑ (r)
∂ 

Lϑ(r) = Lϑ (r)|=0 + 

2 ∂ 2Lϑ (r)
1
|=0 + O(n3 ),
|=0 +
∂ 2
2
f !
#i,t (ϑ)"ri (t)
f #bi (t)$ − f ! (bi (t))dt%
|=0 = !i,t
f !
∂ 2Lϑ (r)
i,t (ϑ)"ri (t)#
f $! #bi (t)$ − f !! (bi (t))dt%.
|=0 = !i,t
#2
∂ 2
Let ri denote the vector representation of the ith neuron’s spike train and let1
f !
f !
(bi (T )) − f ! (bi (T ))dt’T
:= &ri (1)
(bi (1)) − f ! (bi (1))dt
ri (T )
f
f
f !
f !
f $! (bi (T )) − f !! (bi (T ))dt’T
:= &ri (1)#
f $! (bi (1)) − f !! (bi (1))dt
ri (T )#
#i,T (ϑ)’T
:= &#i,1 (ϑ) #i,2 (ϑ)
· · ·
n
n
1
!i=1
!i=1
#i (ϑ)T gi (ri ) +
Lϑ (r) = Lϑ (r)|=0 + 
2
1With a slight abuse of notation, we use T for both the total number of time steps and the transpose opera-
tion; the difference is clear from the context.

#i (ϑ)
then

2

#i (ϑ)T diag[hi (ri )]#i (ϑ) + O(n3 ).

gi (ri )

hi (ri )

· · ·

· · ·

;

2



(3)

#i (ϑ)T gi (ri ).

This second-order loglikelihood expansion is standard in likelihood theory [24]; as usual, the ﬁrst
term is constant in ϑ and can therefore be ignored, while the third (quadratic) term controls the
curvature of the loglikelihood at  = 0, and scales as n2 .
In the high-SNR regime discussed
above, where n → ∞ and  is ﬁxed, the likelihood becomes sharply peaked at θ (and therefore the
Fisher information, which may be understood as the curvature of the log-likelihood at θ, controls the
asymptotics of the estimation error in the case of continuous stimuli), and estimation of θ becomes
easy; in the low-SNR regime, we ﬁx n and consider the  → 0 limit.
Now, ﬁnally, we can more precisely deﬁne the “intermediate” SNR regime: we will focus on the
case of large populations (n → ∞), but in order to keep the total information in a ﬁnite range we
need to scale the sensitivity  as  ∼ n−1/2 . In this setting, the error term O(n3 ) = O(n− 1
2 ) = o(1)
and can therefore be neglected, and the law of large numbers (LLN) implies that
|=0 = Er |θ( 1
#i (ϑ)T diag[hi (ri )]#i (ϑ));
2 ∂ 2Lϑ (r)
n !i
∂ 2
|=0 will be independent of the observed spike train and
consequently, the quadratic term 2 ∂ 2Lϑ (r)
therefore void of information about θ. So the ﬁrst derivative term is the only part of the likelihood
∂ 2
that depends both on the neural activity and ϑ, and may therefore be considered a sufﬁcient statistic
in this asymptotic regime: all the information about the stimulus is summarized in
1
∂Lϑ(r)
√n !i
|=0 =
∂ 
We may further apply the central limit theorem (CLT) to this sum of independent random vectors to
conclude that this term converges to a Gaussian process indexed by ϑ (under mild technical condi-
tions that we will ignore here, for clarity). Thus this model enjoys the local asymptotic normality
property observed in many parametric statistical models [24]: all of the information in the data can
be summarized asymptotically by a sufﬁcient statistic with a sampling distribution that turns out to
be Gaussian.
Example: Linearly ﬁltered stimuli and state-space models
In many cases neurons are modeled in terms of simple rectiﬁed linear ﬁlters responding to the
stimulus. We can handle this case easily using the language introduced above, if we let Ki denote
the matrix implementing the transformation (Ki θ)t = #i,t (θ), the projection of the stimulus onto
the i-th neuron’s stimulus ﬁlter. Then,
fi , ri − f !i dt-) := ϑT ∆(r),
|=0 = ϑT ( 1
n
i *diag + f !i
∂Lϑ(r)
!i=1
K T
√n
∂ 
where fi stands for the vector version of f [bi (t)]. Thus all the information in the population spike
train can be summarized in the random vector ∆(r), which is a simple linear function of the observed
spike train data. This vector has an asymptotic Gaussian distribution, with mean and covariance
n
i *diag + f !i
fi , *fi dt + f !i dt
)- − f !i dt-
1
!i=1
K T
√n
Er |θ (∆(r)) =
dt’Ki)θ + O(
= ( 1
n
i diag& f !2
1
!i=1
K T
i
√n
)
n
fi
n
fi , covr |θ &ri ’diag + f !i
i diag + f !i
fi , Ki
1
!i=1
K T
J := covr |θ (∆(r)) =
n
n
i diag& f !2
1
1
dt’Ki + O(
!i=1
K T
i
√n
).
fi
n
Thus, the neural population’s non-linear and temporally dynamic response to the stimulus is as
informative in this intermediate regime as a single observation from a standard Gaussian experiment,

Kiθ
√n

+ O(

1
n



=

3

in which the parameter θ is ﬁltered linearly by J and corrupted by Gaussian noise. All of the ﬁltering
properties of the population are summarized by the matrix J . (Note that if we consider each Ki as a
random sample from some distribution of ﬁlters, then J will converge by the law of large numbers
to a matrix we can compute explicitly.)
Thus in many cases we can perform optimal Bayesian decoding of θ given the spike trains quite
easily. For example, if θ has a zero mean Gaussian prior distribution with covariance Cθ , then the
posterior mean and the maximum-a-posteriori (MAP) estimate is well-known and coincides with the
optimal linear estimate (OLE):
(4)
ˆθOLE (r) = E (θ|r) = (J + C −1
θ )−1∆(r).
We may compute the Shannon information I (θ : r) between r and θ in a similarly direct fashion.
We know that, asymptotically, the sufﬁcient statistic ∆(r) is as informative as the full population
response r

I (θ : r) = I (θ : ∆(r)).
In the case that the prior of θ is Gaussian, as above, then the information can therefore be computed
quite explicitly via standard formulas for the linear-Gaussian channel [9]:
1
(5)
I (θ : ∆(r)) =
log det(I + J Cθ ).
2
To summarize, when the encodings #i,t (θ) are linear in θ, and we are in the intermediate-SNR
regime, and the parameter θ has a Gaussian prior distribution, then the optimal Bayesian estimate is
obtained by applying a linear transformation to the sufﬁcient statistic ∆(r) which itself is linear in
the spike train, and the mutual information between the stimulus and full population response has
a particularly simple form. These results help to extend previous theoretical studies [5, 18, 20, 16]
demonstrating that in some cases linear decoding can be optimal, and also shed some light on recent
experimental studies indicating that optimal linear and nonlinear Bayesian estimators often have
similar performance in practice [13, 12].
To work through a concrete example, consider the case that the temporal sequence of parameter
values θt is generated by an autoregressive process:
ηt ∼ N (0, R),
θt+1 = Aθt + ηt
for a stable dynamics matrix A and positive-semideﬁnite covariance matrix R. Further assume that
the observation matrices Ki act instantaneously, i.e., Ki is block-diagonal with blocks Ki,t , and
therefore the responses are modeled as
ri (t) ∼ P oiss[f (bi (t) + Ki,tθt )dt].
Thus θ and the responses r together represent a state-space model. This framework has been shown
to lead to state-of-the-art performance in a wide variety of neural data analysis settings [14]. To
understand optimal inference in this class of models in the intermediate SNR regime, we may fol-
low the recipe outlined above: we see that the asymptotic sufﬁcient statistic in this model can be
represented as

t ∼ N (0, Jt ),
∆t = Jt θt + t
where the effective ﬁlter matrix J deﬁned above is block-diagonal (due to the block-diagonal
structure of the ﬁlter matrices Ki ), with blocks we have denoted Jt . Thus ∆t represents obser-
vations from a linear-Gaussian state-space model, i.e., a Kalman ﬁlter model [17]. Optimal de-
coding of θ given the observation sequence ∆1:T can therefore be accomplished via the standard
forward-backward Kalman ﬁlter-smoother [10]; see Fig. 1 for an illustration. The information rate
limT →∞ I (θ0:T : r0:T ) = limT →∞ I (θ0:T : ∆(r)0:T ) may be computed via similar recursions in
the stationary case (i.e., when Jt is constant in time). The result may be expressed most explicitly in
terms of a matrix which is the solution of a Riccati equation involving the effective Kalman model
parameters; the details are provided in the appendix.
Nonlinear examples: orientation coding, place ﬁelds, and small-time expansions
While the linear setting discussed above can handle many examples of interest, it does not seem
general enough to cover two well-studied decoding problems: inferring the orientation of a visual

4

stimulus from a population of cortical neurons [19, 4], or inferring position from a population of
hippocampal or entorhinal neurons [6]. In the former case, the stimulus is a phase variable, and
therefore does not ﬁt gracefully into the linear setting described above; in the latter case, place
ﬁelds and grid ﬁelds are not well-approximated as linear functions of position. If we apply our
general theory in these settings, the interpretation of the encoding function #i (θ) does not change
signiﬁcantly: #i (θ) could represent the tuning curve of neuron i as a function of the orientation of
the visual stimulus, or of the animal’s location in space. However, without further assumptions the
limiting sufﬁcient statistic, which is a weighted sum of these encoding functions #i (θ) (recall eq. 3)
may result in an inﬁnite-dimensional Gaussian process, which may be computationally inconvenient.
To simplify matters somewhat, we can introduce a mild assumption on the tuning functions #i (θ).
Let’s assume that these functions may be expressed in some low-dimensional basis: #i (θ) =
KiΦ(θ), for some vectors Ki , and Φ(θ) is deﬁned to map θ into an mT -dimensional space which
is usually smaller than dim(θ) = dim(θt )T . This ﬁnite-basis assumption is very natural: in the
orientation example, tuning curves are periodic in the angle θt and are therefore typically expressed
as sums of a few Fourier functions; similarly, two-dimensional ﬁnite Fourier or Zernike bases are
often used to represent grid or place ﬁelds [6]. The key point here is that we may now simply follow
the derivation of the last section with Φ(θ) in place of θ; we ﬁnd that the sufﬁcient statistic may
be represented asymptotically as an mT -dimensional Gaussian vector with mean J and covariance
J Φ(θ), with J deﬁned as in the preceding section.
We should note that this nonlinear case does remain slightly more complicated than the linear case
in one respect: while the likelihood with respect to Φ(θ) reduces to something very simple and
tractable, the prior (which is typically deﬁned as a function of θ) might be some complicated function
of the remapped variable Φ(θ). So in most interesting nonlinear cases we can no longer compute the
optimal Bayesian decoder or the Shannon information rate analytically. However, our approach does
lead to a major simpliﬁcation in numerical investigations into theoretical coding issues. For example,
to examine the coding efﬁciency of a population of neurons encoding an orientation variable in this
intermediate SNR regime we do not need to simulate the responses of the entire population (which
would involve drawing nT random variables, for some large population size n); instead, we only
need to draw a single equivalent mT -dimensional Gaussian vector ∆(r), and quantify the decoding
performance based on the approximate loglikelihood

),

Φ(ϑ)T J Φ(ϑ) + O(

1
1
Lϑ(r) = Lϑ (r)|=0 + Φ(ϑ)T ∆(r) +
√n
2
which as emphasized above has a simple quadratic form as a function of Φ(ϑ). Since m can typically
be chosen to be much smaller than n, this approach can result in signiﬁcant computational savings.
We now switch gears slightly and examine another related intermediate regime in which nonlinear
encoding plays a key role: instead of letting the sensitivity  of each neuron become small (in order to
keep the total information in the population ﬁnite), we could instead keep the sensitivity constant and
let the time period over which we are observing the population scale inversely with the population
size n. This short-time limit is sensible in some physiological and psychophysical contexts [22] and
was examined analytically in [15] to study the impact of inter-neuron dependencies on information
transmission. Our methods can also be applied to this short-time limit. We begin by writing the
loglikelihood of the observed spike count vector r in a single time-bin of length dt:
Lϑ(r) := log p(r|θ) = !i
ri log f [bi + #i (ϑ)] − f [bi + #i (ϑ)] dt.
The second term does not depend on r; therefore, all information in r about θ resides in the sufﬁcient
statistic
∆ϑ (r) := !i
Since the i-th neuron ﬁres with probability f [bi + #i (θ)] dt, the mean of ∆ϑ (r) scales with ndt, and
it is clear that dt = 1/n is a natural scaling of the time bin. With this scaling ∆ϑ (r) converges to a
Gaussian stochastic process with mean

ri log f [bi + #i (ϑ)] .

Er |θ [∆ϑ (r)] =

1
n !i

f [bi + #i (θ)] log f [bi + #i (ϑ)]

5

and covariance

riKi .

1
f [bi + #i (θ)] .log f [bi + #i (ϑ)]/.log f [bi + #i (ϑ! )]/,
n !i
covr |θ [∆ϑ (r), ∆ϑ! (r)] =
where we have used the fact that the variance of a Poisson random variable coincides with its mean.
In general, this limiting Gaussian process will be inﬁnite-dimensional. However, if we choose the ex-
ponential nonlinearity (f (.) = exp(.)) and the encoding functions #i (θ) are of the ﬁnite-dimensional
form considered above, #i (θ) = K T
i Φ(θ), then the log f [bi + #i (ϑ)] term in the deﬁnition of ∆ϑ (r)
simpliﬁes: in this case, all information about θ is captured by the sufﬁcient statistic
∆(r) = !i
If we again let dt = 1/n, then we ﬁnd that ∆(r) converges to a ﬁnite-dimensional Gaussian random
vector with mean and covariance
1
1
n !i
n !i
i Φ(θ)1 KiK T
f 0bi + K T
f 0bi + K T
i Φ(θ)1 Ki ;
Er |θ [∆(r)] =
covr |θ [∆(r)] =
i ;
again, if the ﬁlters Ki are modeled as independent draws from some ﬁxed distribution, then the
above normalized sums converge to their expectations, by the LLN. Thus, as in the intermediate-
SNR regime, we see that inference can be dramatically simpliﬁed in this short-time setting.
Likelihood in the intermediate regime: non-Poisson effects
We conclude by discussing the generalization to non-Poisson networks with interneuronal depen-
dencies and nontrivial correlation structure. We generalize the rate equation (1) to
λi (t) = fi 0bi (t) + #i,t (θ)22Ht 1 ,
where Ht stands for the spiking activity of all neurons prior to time t: Ht = {ri (t! )}t!<t,1≤i≤n .
Note that the inﬂuence of spiking history may be different for each neuron: refractory periods,
self-inhibition and coupling between neurons can be formulated by appropriately deﬁning the de-
pendence of fi (.) on Ht .
We begin, as usual, by expanding the log-likelihood. The basic point-process likelihood (eq. 2)
remains valid. Let gi (r) and hi (r) denote the vector versions of
f !
f !
f &bi (t)22Ht’ − f !i &bi (t)22Ht’dt
f $!&bi (t)22Ht’ − f !!i &bi (t)22Ht ’dt,
and ri (t)#
ri (t)
respectively, analogously to the Poisson case. Then, the ﬁrst and second terms in the expansion of
the loglikelihood may be written as
2 ∂ 2Lϑ (r)
1
1
∂Lϑ(r)
|=0 =  !i
2 !i
|=0 =
∂ 2
2
∂ 
2
as before. For independent neurons, the log-likelihood was composed of normalized sums of in-
dependent random variables that converged to a Gaussian process, by the CLT. In the history-
dependent, coupled case, gi (r) and hi (r) depend not only on the i-th neuron’s activity ri , but rather
on the whole network history. Nonetheless, under technical conditions on the network’s dependence
structure (to ensure that the ﬁring rates and correlations in the network remain bounded), we may still
exploit versions of the LLN and CLT. Thus, under conditions ensuring the validity of the LLN we
may conclude that, as before, the second-order term 2 ∂ 2Lϑ (r)
|=0 converges to its expectation under
∂ 2
2 scaling, and therefore carries no information about θ. When we discard
the intermediate  ∼ n− 1
this second-order term, along with higher-order terms that are negligible in the intermediate-SNR,
large-n limit, we are left once again with the gradient term  ∂Lϑ (r)
|=0 = 1√n 3i #i (ϑ)T gi (r),
∂ 
which under appropriate conditions (ensuring the validity of a CLT) will converge to a Gaussian
process limit whose mean and covariance we can often compute analytically.

#T
i (ϑ)diag[hi (r)]#i (ϑ),



#T
i (ϑ)gi (r)

and

6

Let’s turn to a speciﬁc example, in order to make these claims somewhat more concrete. Consider
a network with weak couplings and possibly strong self-inhibition and history dependence; more
precisely, we assume that interneuronal conditional cross-covariances are weak, given the stimulus:
cov[ri (t), rj (t + τ )|θ] = O(n−1 )
for i &= j.
See, e.g., [11, 23] for further discussion of this condition, which is satisﬁed for many spiking net-
works in which the synaptic weights scale uniformly as O(n−1 ). For simplicity, we will also
restrict our attention to linear encoding functions, though generalizations to the nonlinear case
are straightforward. Thus, as before, let Ki denote the matrix implementing the transformation
(Kiθ)t = #i,t (θ), the projection of the stimulus onto the i-th neuron’s stimulus ﬁlter. Then
|=0 = ϑT ( 1
fi , ri − f !i dt-),
n
i *diag + f !i
∂Lϑ(r)
!i=1
K T
√n

∂ 
where fi stands for the vector version of fi&bi (t)22Ht’; in other words, the t-th entry of fidt is the
probability of observing a spike in the interval [t, t + dt], given the network spiking history Ht in
the absence of input. Our sufﬁcient statistic is therefore exactly as in the Poisson setting,
n
i *diag + f !i
fi , ri − f !i dt- ,
1
(6)
!i=1
K T
√n
∆(r) :=
except for the history-dependence induced through the redeﬁnition of fi .
Computing the necessary means and covariances in this case requires more work than in the Poisson
It is helpful (though not necessary) to make the stationarity
case; see the appendix for details.
assumption bi (t) ≡ bi , which implies in this setting that E( f !
) can also be chosen to be time-
2
i
fi
invariant; in this case the limiting covariance and mean of the sufﬁcient statistic are given by
n
2
f !i
1
Kidiag&Er |θ=0#
dt$’Ki ;
!i=1
Er |θ [∆(r)] = J θ,
J := covr |θ [∆(r)] =
n
fi
where the expectations are over the spontaneous network activity in the absence of any input. In
short, once again, we have ∆(r) →D N (J θ, J ). Analytically, the only challenge here is to compute
the expectations in the deﬁnition of J . In many cases this can be done analytically (e.g., in any pop-
ulation of uncoupled renewal-process neurons), or by using mean-ﬁeld theory [23], or numerically
by simply calculating the mean ﬁring rate of the network in the undriven state θ = 0.
We examine this convergence quantitatively in Fig. 1. In this case the stimulus θt was a sample path
from a one-dimensional autoregressive (AR(1)) process. Spikes were generated according to
wj i Ij (t)
λi (t) = λo exp 
n
θt√n
!j=1
 1τi (t)>τref ,

where Ij (t) is the synaptic input from the j -th cell (generated by convolving the spike train rj with
an exponential of time constant 20 ms), wj i is the synaptic weight matrix coupling the output of
neuron j to the input of neuron i, τi (t) is the time since the last spike; therefore, 1τi (t)>τref enforces
the absolute refractory period τref , which was set to be 2 ms here. Since the encoding ﬁlters Ki act
instantaneously in this model (Ki can be represented as a delta function, weighted by n−1/2 ), the
observed spike trains can be considered observations from a state-space model, as described above.
The weights wj i were generated randomly from a uniform distribution on the interval −[5/n, 5/n],
with self-weights wii = 0, and 3j wj i = 0 to enforce detailed balance in the network. Note that,
while the interneuronal coupling is weak in this example, the autocorrelation in these spike trains is
quite strong on short time scales, due to the absolute refractory effect.
We compared two estimators of θ: the full (nonlinear) MAP estimate ˆθM AP = arg maxθ p(θ|r),
which we computed using the fast direct optimization methods described in [14], and the limiting
optimal estimator ˆθ∆ := (J + C −1
θ )−1∆(r). Note that J is diagonal; we computed the expectations
in the deﬁnition of J using the numerical approach described above in this simulation, though in

+

7

stimuli

 

spike train(s) with 2ms refractory period, 
20ms synaptic time constant and baseline rate 30Hz

sufficient statistics   Δ(r)

5

0

−5

 

5

0

−5

5

0

−5

0

1
 
=
 
n

θ
θMAP
θΔ

5
 
=
 
n

0
2
 
=
 
n

0.05

0.1
time(sec)

0.15

0.2

0

0.05

0.1
time(sec)

0.15

0.2

2.5
2
1.5
1
0.5
0

0.4

0.3

0.2

0.1

0

0.4

0.3

0.2

0.1

0

0

0.05

0.1
time(sec)

0.15

0.2

Figure 1: The left panels show the true stimulus (green), MAP estimate (red) and the limiting optimal
estimator ˆθ∆ := (J + C −1
θ )−1∆(r) (blue) for various population sizes n. The middle panels show
the spike trains used to compute these estimates. The right panels show the sufﬁcient statistics ∆(r)
used to compute ˆθ∆ . Note that the same true stimulus was used in all three simulations. As n
increases, the linear decoder converges to the MAP estimate, despite the nonlinear and correlated
nature of the network model generating the spike trains (see main text for details).

other simulations (with uncoupled renewal-model populations) we checked that the fully-analytical
approach gave the correct solution. In addition, C −1
is tridiagonal in this state-space setting; thus
the linear matrix equation in eq. (4) can be solved efﬁciently in O(T ) time using standard tridiagonal
θ
matrix solvers. We ﬁnd that, as predicted, the full nonlinear Bayesian estimator ˆθM AP approaches
the limiting optimal estimator ˆθ∆ as n becomes large; n = 20 is basically sufﬁcient in this case,
although of course the convergence will be slower for larger values of the gain factor  (or, equiva-
lently, larger ﬁlters Ki or larger values of the variance of θt ).
We conclude with a few comments about these results. First, note that the covariance matrix J
we have computed here coincides almost exactly with what we computed previously in the Poisson
case. Indeed, we can make this connection much more precise: we can always choose an equivalent
Poisson network with rates deﬁned so that the Er |θ=0 [(f !i )2/fi ] term in the non-Poisson network
matches the (f !i )2 /fi term in the Poisson network. Since J determines the information rate com-
pletely, we conclude that for any weakly-coupled network there is an equivalent Poisson network
which conveys exactly the same information in the intermediate regime. However, note that the the
sufﬁcient statistic ∆(r) is different in the Poisson and non-Poisson settings, since the f !/f term
linearly reweights the observed spikes, depending on how likely they were given the history; thus
the optimal Bayesian decoder incorporates non-Poisson effects explicitly.
A number of interesting questions remain open. For example, while we expect a LLN and CLT to
continue to hold in many cases of strong, structured interneuronal coupling, computing the asymp-
totic mean and covariance of the sufﬁcient statistic ∆(r) may be more challenging in such cases,
and new phenomena may arise.

8

References
[1] J. Atick. Could information theory provide an ecological theory of sensory processing? Net-
work: Computation in Neural Systems, pages 213–251, May 1992.
[2] F. Attneave. Some informational aspects of visual perception. Psychological Review, 1954.
[3] H. B. Barlow. Possible principles underlying the transformation of sensory messages. Sensory
Communication, pages 217–234, 1961.
[4] P. Berens, A. S. Ecker, S. Gerwinn, A. S. Tolias, and M. Bethge. Reassessing optimal neu-
ral population codes with neurometric functions. Proceedings of the National Academy of
Sciences, 108:4423–4428, 2011.
[5] W. Bialek and A. Zee. Coding and computation with neural spike trains. Journal of Statistical
Physics, 59:103–115, 1990.
[6] E. Brown, L. Frank, D. Tang, M. Quirk, and M. Wilson. A statistical paradigm for neural spike
train decoding applied to position prediction from ensemble ﬁring patterns of rat hippocampal
place cells. Journal of Neuroscience, 18:7411–7425, 1998.
[7] N. Brunel and J.-P. Nadal. Mutual information, ﬁsher information, and population coding.
Neural Comput., 10(7):1731–1757, 1998.
[8] B. Clarke and A. Barron. Information-theoretic asymptotics of Bayes methods. IEEE Trans-
actions on Information Theory, 36:453 – 471, 1990.
[9] T. Cover and J. Thomas. Elements of information theory. Wiley, New York, 1991.
[10] J. Durbin and S. Koopman. Time Series Analysis by State Space Methods. Oxford University
Press, 2001.
[11] I. Ginzburg and H. Sompolinsky. Theory of correlations in stochastic neural networks. Phys
Rev E, 50(4):3171–3191, 1994.
[12] V. Lawhern, W. Wu, N. Hastopoulos, and L. Paninski. Population decoding of motor cortical
activity using a generalized linear model with hidden states. Journal of Neuroscience Methods,
2011.
[13] J. Macke, L. Sing, B. Cunningham, J.P. snd Yu, K. Shenoy, and M. Sahani. Modelling low-
dimensional dynamics in recorded spiking populations. COSYNE, 2011.
[14] L. Paninski, Y. Ahmadian, D. Ferreira, S. Koyama, K. Rahnama Rad, M. Vidne, J. Vogelstein,
and W. Wu. A new look at state-space models for neural data. Journal of Computational
Neuroscience, 29(1):107–126, 2010.
[15] S. Panzeri, S. Schultz, A. Treves, and E. Rolls. Correlations and the encoding of information in
the nervous system. Proceedings of the Royal Society London B, 266(1423):1001–1012, 1999.
[16] J. Pillow, Y. Ahmadian, and L. Paninski. Model-based decoding, information estimation, and
change-point detection in multi-neuron spike trains. Neural Computation, 23(1):1–45, January
2011.
[17] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Compu-
tation, 11:305–345, 1999.
[18] E. Salinas and L. Abbott. Vector reconstruction from ﬁring rates. Journal of Computational
Neuroscience, 1:89–107, 1994.
[19] H. S. Seung and H. Sompolinsky. Simple models for reading neuronal population codes.
Proceedings of the National Academy of Sciences, 90:10749–10753, 1993.
[20] H. Snippe. Parameter extraction from population codes: A critical assesment. Neural Compu-
tation, 8:511–529, 1996.
[21] D. Snyder and M. Miller. Random Point Processes in Time and Space. Springer-Verlag, 1991.
[22] S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature,
381:520–522, 1996.
[23] T. Toyoizumi, K. Rahnama Rad, and L. Paninski. Mean-ﬁeld approximations for coupled
populations of generalized linear model spiking neurons with Markov refractoriness. Neural
Computation, 21:1203–1243, 2009.
[24] A. van der Vaart. Asymptotic statistics. Cambridge University Press, Cambridge, 1998.

9


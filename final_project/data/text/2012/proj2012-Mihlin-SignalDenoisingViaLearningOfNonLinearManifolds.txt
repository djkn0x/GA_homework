Signal Denoising via Learning of Non-Linear
Manifolds

Alex Mihlin

Abstract

A signal denoising method based on non-linear manifold learning
was implemented. This method is applicable for a wide range of noise
types. It improves with training and may be optimized for diﬀerent
signal types. Experiments were performed on images with Gaussian
noise (Fig. 1.1) and with a superimposed image (Fig. 1.2).

Figure 1: Denoised images. (1) Gaussian noise and (2) superimposed image.
The ﬁgures illustrate: (a) the original image, (b) denoised image, (c) noisy
test image and (d) noisy training image (another, clean, version of the
training image was used).

Introduction

The method at hand assumes that the noise preserves the local geometry of
the signal in feature space. Thus, denoising a signal amounts to inverting
the global transformation induced by the noise. This inverse transformation
is learned from a set of clean and noisy training signals. The learning
process requires the embedding of one feature space into another. Such
embedding may be done via dimensionality reduction methods.

1

Two canonical dimensionality reduction methods are principal compo-
nent analysis (PCA) [1] and multidimensional scaling (MDS) [2]. These
methods are appealing since their optimization is well understood and since
they are not prone to local minima. However, the PCA and MDS methods
are unable to embed the feature space into non-linear manifolds. Fig. 2.1
illustrates this shortcoming: if the feature manifold is non-linear, far away
points in feature space may be embedded into close locations.
A more powerful, non-linear, dimensionality reduction method, called
Locally Linear Embedding (LLE) was recently proposed [3]. This method
attempts to ﬁnd a low dimensional embedding, which preserves the local
geometry of the feature manifold. Fig. 2.2 illustrates the advantage of this
method over PCA and MDS.

Figure 2: Reduction from 3-D into 2-D feature space by (1) PCA and MDS
methods and (2) LLE method. The left ﬁgure illustrates a shortcoming of
the PCA and MDS methods: distant points are embedded into close loca-
tions. The right ﬁgure demonstrates that LLE overcomes this shortcoming.

The denoising method at hand uses locally linear embedding in order
to embed the feature manifold of the noisy test image into the feature
manifold of the noisy training image (c.f. Fig. 3). The denoised image is
then obtained by embedding this manifold into the feature manifold of the
clean training image. This procedure is described in the next section.

Method description

The method at hand is based on a recent work by R. Shi, I-F. Shen and
W. Chen [4]. As an example, Fig. 3 illustrates 3-dimensional training and
test feature spaces. A patch (solid square) corresponding to a speciﬁc data
(cid:88)
point, Ii , is deﬁned by the K nearest neighbours, {Ij }j=1,...,K , of Ii , so that
where (cid:80)
j
j Wij = 1. The denoised test point, Di , corresponding to each
noisy test point, Ii , is determined by the following three steps (correspond-

Wij Ij

Ii =

2

Figure 3: The denoising method. The denoising step (left arrow) assumes
that the noise preserves the local geometry in feature space.

ing to the three arrows in Fig. 3):

Step 1: The test patch corresponding to data point Ii is embedded into the
noisy training manifold (top arrow). This is done by locating the appro-
priate K nearest noisy training neighbours, {Tj }. To this end, the training
and test noise types should be similar. A direct implementation of the
nearest neighbours search has a large complexity of
(cid:13) [Ntrain · Nnearest neighbours ]

(1)

where Ntrain and Nnearest neighbours are the numbers of training data points
and nearest neighbours respectively.
In order to address this issue, the
training data points were arranged into a tree structure (Fig. 4). The tree
brunches corresponded to points nearest to appropriate central points (cyan
circles). These central points were found via k-means unsupervised learn-
(cid:3)
(cid:13) (cid:2)logNbins
ing. This method yielded an improved nearest neighbour search complexity
of
(Ntrain ) · Nnearest neighbors
The nearest neighbours of a speciﬁc data point were thus found by (i)
locating the closest central point and (ii) searching for nearest neighbours
only within the corresponding brunch. In order to account for points near
brunch boundaries, neighbouring brunches were appended with overlapping
regions (c.f. bottom of Fig. 4).

(2)

Step 2: Calculate the weights, Wij , for the reconstruction of point Ii from
its nearest neighbours, {Tj }. Namely, solve the following optimization prob-

3

lem:

min
Wij

Wij Tj (cid:107)

Figure 4: A tree structure for eﬃcient nearest neighbours search.
(cid:107)Ii − Ntrain(cid:88)
j=1
Ntrain(cid:88)
s.t. Wij = 0 if Ii , Tj are not nearest neighbours
j=1

Wij = 1
(cid:88)
This optimization problem is equivalent to the following system of linear
equations:
l
where Gi
j l=0 if Tj , Tl are not nearest neighbours of Ii and Gi is the local
Gram matrix,

Gi
j lWil = 1

(3)

(4)

j l = (Ii − Tj )T (Ii − Tl )
Gi
The resultant weights are normalized so that (cid:80)
j Wij = 1. If the num-
ber of nearest neighbours is larger than the feature space dimension, the
rank of Gi is smaller than the number of nearest neighbours and equation
(4) is ill deﬁned. This issue was addressed via L2 regularization, which
resulted in an addition of a small constant to the diagonal elements of Gi .
Optimization problem (3) may be made convex, by requiring that Wij ≥ 0.
This requirement forces each data point to lie within a convex hull of its
nearest neighbours.

4

Di =

Wij Cj

Step 3: Assuming that the noise preserves the local geometry, the appro-
priate denoised patch is deﬁned by the K clean training data points, {Cj },
corresponding to the K noisy training nearest neighbours, {Tj } (left arrow
(cid:88)
in Fig. 3). The denoised data point, Di , is thus given (bottom arrow in
Fig. 3) by:
j
where the weights, Wij , were calculated in step 2.
This method was tested on images with two types of noise: (i) Gaussian
noise with amplitude of 20% of the maximal feature value (Fig. 1.1) and
(ii) superimposed image (Fig. 1.2). The images were divided into patches.
Each N pixel patch represented a 3N -dimensional point, corresponding to
the red, green and blue colour values of each pixel. The patches were chosen
with some overlap, in order to insure the smoothness of the denoised image.
The images contained about 150,000, 27-dimensional (3x3 pixel patches)
data points.
To account for the incompleteness of the training set, an iterative method
was proposed. This method required at least two pairs of training signals.
First, one of the training signals was denoised using the other. This de-
noised signal was then used as a training signal to further denoise an already
denoised test signal. This procedure could be repeated several times, con-
tinuously reﬁning the denoised image. The denoised image from Fig. 1.2
was obtained using two such iterations.

Conclusion

A signal denoising method (Fig. 3) was implemented based on the Locally
Linear Embedding manifold learning method (Fig. 2). An eﬃcient nearest
neighbour search method was implemented via k-means unsupervised learn-
ing (Fig. 4). The use of a large number of nearest neighbours was enabled
via L2 regularization. An improved iterative denoising method was pro-
posed and implemented. The method was successfully tested by denoising
an image of Gaussian noise and of a superimposed image (Fig. 1).

References

[1]

[2]

[3]

[4]

I. T. Jolliﬀe, Principal Component Analysis, Springer-Verlag, New York (1986)

T. Cox, M. Cox, Multidimensional Scaling, Chapman & Hall, London (1994)

L. K. Saul, S. T. Roweis, Think Globally, Fit Locally: Unsupervised Learning of
Low Dimensional Manifolds, Journal of Machine Learning Research, 4, 119-155
(2003)

R. Shi, I-F. Shen, W. Chen, Image Denoising through Locally Linear Embed-
ding, Proceedings of the International Conference on Computer Graphics, Imag-
ing and Visualization, 147-152 (2005)

5


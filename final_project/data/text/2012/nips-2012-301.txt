Burn-in, bias, and the rationality of anchoring

Falk Lieder
Translational Neuromodeling Unit
ETH and University of Zurich
Zurich, Switzerland
falk.lieder@gmail.com

Thomas L. Grifﬁths
Psychology Department
University of California, Berkeley
Berkeley, CA
tom griffiths@berkeley.edu

Noah D. Goodman
Department of Psychology
Stanford University
Stanford, CA
ngoodman@stanford.edu

Abstract

Bayesian inference provides a unifying framework for addressing problems in ma-
chine learning, artiﬁcial intelligence, and robotics, as well as the problems facing
the human mind. Unfortunately, exact Bayesian inference is intractable in all
but the simplest models. Therefore minds and machines have to approximate
Bayesian inference. Approximate inference algorithms can achieve a wide range
of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-
accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the
mind’s inference algorithm(s). We ﬁnd that reasonably accurate decisions are pos-
sible long before the Markov chain has converged to the posterior distribution, i.e.
during the period known as “burn-in”. Therefore the strategy that is optimal sub-
ject to the mind’s bounded processing speed and opportunity costs may perform
so few iterations that the resulting samples are biased towards the initial value.
The resulting cognitive process model provides a rational basis for the anchoring-
and-adjustment heuristic. The model’s quantitative predictions are tested against
published data on anchoring in numerical estimation tasks.

1

Introduction

What are the algorithms and representations used in human cognition? This central question of cog-
nitive science has been attacked from different directions. The heuristics and biases program started
by Kahneman and Tversky [1] seeks to constrain the space of possible algorithms by the systematic
deviations of human behavior from normative standards. This approach has demonstrated a large
number of cognitive biases, i.e. systematic “errors” in human judgment and decision-making that
are thought to result from people’s use of heuristics [1]. Heuristics are problem-solving strategies
that are simple and efﬁcient but not guaranteed to ﬁnd the optimal solution. By contrast, Bayesian
models of cognition seek to identify and understand the problems solved by the human mind, and
to constrain the space of possible algorithms by considering the optimal solutions to those prob-
lems [2].
Bayesian models and heuristics characterize cognition at two different levels of analysis [3], namely
the computational and the algorithmic level respectively. Explanations formulated at the computa-
tional level do not specify psychological mechanisms, but they can be used to derive or constrain
psychological process models [3]. Though heuristics seem very different from Bayesian inference,
it can nevertheless be useful to think of a given heuristic as an algorithm for approximating the so-

1

lution to a Bayesian inference problem. Thus approximate inference algorithms including sampling
algorithms and variational Bayes can inspire models of the psychological mechanisms underlying
cognitive performance. In fact, models developed in this way (e.g. [4–7]) provide a rational basis
for established psychological process models such as the Win-Stay-Lose-Shift algorithm [6] and
exemplar models [5], as well as cognitive biases such as probability matching [6, 7]. Such rational
process models are informed by the teleological constraint that they should approximate Bayesian
inference as well as by processing constraints on working memory (e.g. [6]) or computation time
(e.g. [7]). In addition to the success of these models—which are based on sampling algorithms—
there is some direct experimental evidence that speaks to sampling as a psychological mechanism:
people appear to sample from posterior probability distributions when making predictions [8, 9],
categorizing objects [10], or perceiving ambiguous visual stimuli [11, 12]. Furthermore, sampling
algorithms can be implemented in biologically plausible networks of spiking neurons [13, 14], and
individual sampling algorithms are applicable to a wide range of problems including inference on
complex structured representations. In what follows, we will therefore use sampling algorithms as
our starting point for exploring how the human mind might perform probabilistic inference.
The problem of having to trade off accuracy for speed is ubiquitous not only in machine learning and
artiﬁcial intelligence, but also in human cognition. The mind’s bounded computational resources
necessitate tradeoffs, and these tradeoffs should be chosen such as to maximize expected utility
subject to resource bounds (bounded optimality). Therefore understanding what the optimal time-
accuracy tradeoffs are may beneﬁt cognitive modelling as well as real-time AI. A previous analysis
[7] concluded that time costs make it rational for systems with bounded processing speed to make
decisions based on very few samples from the posterior distribution. However, generating even
a single perfect sample from the posterior distribution may be very costly, for instance requiring
thousands of iterations of a Markov chain Monte Carlo (MCMC) algorithm. The values generated
during this burn-in phase are typically discarded, because they are biased towards the Markov chain’s
initial value. This paper analyses under which conditions it is boundedly optimal to tolerate biased
samples and in particular to make a decision based on the values generated in the burn-in phase of
MCMC. The results may further our understanding of bounded optimality in real-time computing
[15] which may become a framework for developing algorithmic models of human cognition as well
as AI systems. We demonstrate the utility of the bounded optimality framework by presenting an
algorithmic model of probabilistic inference that can explain a cognitive bias that might otherwise
appear irrational.

2 The Time-Bias Tradeoff of MCMC Algorithms

Exact Bayesian inference is either impossible or intractable in most real-world problems. Thus nat-
ural and artiﬁcial information processing systems have to approximate Bayesian inference. There
is a wide range of approximation algorithms that could be used. Which algorithm is best suited
for a particular problem depends on two criteria: time cost and error cost. Iterative inference al-
gorithms such as MCMC gradually reﬁne their estimate over time allowing the system to smoothly
adjust to time pressure. This section formalizes the problem of optimally approximating probabilis-
tic inference for real-time decision-making under time costs for a speciﬁc MCMC algorithm, the
Metropolis-Hastings algorithm [16].

2.1 Problem Deﬁnition: Probabilistic Inference in Real-Time

The problem of decision-making under uncertainty in real-time can be formalized as the optimiza-
tion of a utility function that incorporates decision time [15]. Here, we analyze the special case that
we will use to model a psychology experiment in Section 3. In this special case, actions are point
estimates of the latent variable of a Normal-Normal model,
p ), P (Y |X = x) = N (x, σ2
P (X ) = N (µp , σ2
(1)
l ),
where X is unknown and Y is observed. The utility of predicting state a when the true state is x
after time t is given by the function
u(a, x, t) = −(cid:107)x − a(cid:107) − c · t.
(2)
This function has two terms—error cost and time cost—whose relative importance is determined by
the parameter c.

2

In certain situations the mind may be faced with the problem of maximizing the utility function
given in Equation 2 using an iterative inference algorithm. Under this assumption the decision time
in Equation 2 can be modeled as a linear function of the number of iterations i used to compute the
decision, with

i
iterations
sec
v
The slope of this function is the system’s processing speed v measured in iterations per second. The
next section solves this bounded optimality problem assuming that the mind’s inference algorithm
is based on MCMC.

+ t0 , [v ] =

t =

.

(3)

2.2 A Rational Process Model of Probabilistic Inference

We assume that the brain’s inference process iteratively reﬁnes an approximation Q to the posterior
distribution. The ﬁrst iteration computes approximation Q1 from an initial distribution Q0 , the tth
iteration computes approximation Qt from the previous approximation Qt−1 , and so on. Under
the assumption that the agent updates its belief according to the Metropolis-Hastings algorithm
[16], the temporal evolution of its belief distribution can be simulated by repeated multiplication
(cid:110)
(cid:111)
with a transition matrix T : Qt+1 = T · Qt . Each element of T is determined by two factors:
the probability that a transition is proposed, i.e. Ppropose (xi , xj ) = N (xi ; µ = xj , σ2 ), and the
1, P (xi |y)·Ppropose (xj ,xi )
probability that this proposal is accepted, i.e. α(xi , xj ) = min
. In brief,
P (xj |y)·Ppropose (xi ,xj )
Ti,j = Ppropose (xi , xj ) · α(xi , xj ).
Concretely, the psychological process underlying probabilistic inference can be thought of as a se-
quence of adjustments of an initial guess, the anchor x0 . In each iteration a potential adjustment
is drawn from a Gaussian distribution with mean zero (δ ∼ N (0, σ2 )). The adjustment will either
be accepted, i.e. xt+1 = xt + δ , or rejected, i.e. xt+1 = xt . If a proposed adjustment makes
the estimate more probable (p(xt + δ) > p(xt )), then it will always be accepted. Otherwise the
adjustment will be made with probability α = p(xt+δ)
p(xt ) , i.e. according to the posterior probability
of the adjusted relative to the unadjusted estimate. This simple scheme implements the Metropolis-
Hastings algorithm [16] and thereby renders the sequence of estimates (x0 , x1 , x2 , · · · ) a Markov
chain converging to the posterior distribution. Beyond this instantiation of MCMC we assume that
the mind stops after the optimal number of (potential) adjustments that will be derived in the follow-
ing subsections. This algorithm can be seen as a formalization of anchoring-and-adjustment [17]
and therefore provides a rational basis for this heuristic.

2.3 Bias decays exponentially with the number of MCMC Iterations

The bias of a probabilistic belief Q approximating a normative posterior P can be measured by the
absolute value of the deviation of its expected value from the posterior mean,
Bias[Q; P ] = (cid:107)EQ [X ] − EP [X ](cid:107).
(4)
If the proposal distribution is positive, then the distribution Qi that the Metropolis-Hastings algo-
rithm samples from in iteration i converges to the posterior distribution geometrically fast under mild
regularity conditions on the posterior [18]. Here, the convergence of the sampling distribution Qi to
the posterior P was deﬁned as the convergence of the supremum of the absolute difference between
the expected values EQi [f (X )] and EP [f (X )] over all functions f with ∀x : (cid:107)f (x)(cid:107) < V (x) for
some function V that depends on the posterior. Using this theorem, one can prove the geometric
convergence of the bias of the distribution Qi to zero and the geometric convergence of the expected
utility of actions sampled from Qi to the expected utility of actions sampled from P :
and (cid:12)(cid:12)EQ(A)
(cid:2)EP (X ) [u(x, a, t)](cid:3)(cid:12)(cid:12) ≤ M · ri ,
(cid:2)EP (X ) [u(x, a, t)](cid:3) − EP (A)
∃M , r ∈ R+ : Bias[Qi ; P ] ≤ M · ri ,
where the chosen proposal and initial distributions determine the tightness of the bound M , the
initial bias, and the convergence rate r .
We simulated the decay of bias in the Normal-Normal case that we are focussing on (Equation
1). All Markov chains were initialized with the prior mean. Our results show that the mean of

(5)

3

the sampling distribution converges geometrically as well (see Figure 1). Thus the reduction in
bias tends to be largest in the ﬁrst few iterations and decreases quickly from there on, suggesting
a situation of diminishing returns for further iterations: an agent under time pressure may do well
stopping after the initial reduction in bias.

Figure 1: Bias of the mean of the approximation Qt , i.e. |E[ ˜Xt ] − E[X |Y = y ]| where Xt ∼ Qt , as
a function of the number of iterations t of our Metropolis-Hastings algorithm. The ﬁve lines show
this relationship for different posterior distributions whose means are located 1σp , · · · , 5σp away
from the prior mean (σp is the standard deviation of the prior). As the plot shows, the bias decays
geometrically with the number of iterations in all ﬁve cases.

2.4 Optimal Time-Bias Tradeoffs

This subsection combines the result reported in the previous subsection with time costs and computes
the optimal bias-time tradeoffs depending on the ratio of time cost to error cost and on how large
the initial bias is. It suggests that intelligent agents might use these results to choose the number
of MCMC iterations according to their estimate of the initial bias. Formally, we deﬁne the optimal
number of iterations i(cid:63) and resulting bias b(cid:63) as
E [u(ai , x, t0 + i/v)|Y = y ] , where ai ∼ Qi
i(cid:63) = arg max
i
b(cid:63) = Bias [Qi(cid:63) ; P ]
using the variables deﬁned above. If the upper bound in Equation 5 is tight, then the optimal number
of iterations and the resulting bias can be calculated analytically,
· (log(c) − R · log(R · log(1/r)))

(7)

(6)

(8)

i(cid:63) =
b(cid:63) ≤

1
log(r)
c
log (1/r)

.

(9)

Figure 2 shows the optimal number of iterations for various initial distances and ratios of time cost
to error cost (c), and Figure 3 shows the resulting biases. Over a large range of time cost to error cost
ratios, the optimal number of iterations is about 10 and below. Thus by the standard use of MCMC
algorithms, the samples used in the boundedly optimal decisions would have been discarded as
“burn-in”. As one might expect, the optimal number of iterations increases with the distance from
initial value to the posterior mean, but the conclusions hold regardless of those differences. As a
result, rational agents should tolerate a non-negligible amount of bias in their approximation to the
posterior.

4

Figure 2: Number of iterations i(cid:63) that maximizes the agent’s expected utility as a function of the
ratio between the cost per iteration and the cost per unit error. The ﬁve lines correspond to the same
posterior distributions as in Figure 1.

Figure 3: Bias of the approximate posterior (Equation 4) after the optimal number of iterations
shown in Figure 2. The ﬁve lines correspond to the same posterior distributions as in Figure 1.

Contrary to traditional recommendations that MCMC algorithms should be run for thousands of
iterations (e.g. [19]) our analysis suggests that the boundedly optimal number of iterations for real-
time decision-making is about two orders of magnitude lower. This implies that real-time decision-
making can and should tolerate bias in exchange for speed.
In separate analyses we found that
the tolerable bias is even higher for decision problems with non-linear error costs. In predicting a
binary random variable with 0 − 1 loss, for instance, bias has almost no effect on accuracy unless it
pushes the agent’s probability estimate across the 50% threshold (see Figure 4). In cases like this,
the optimal bias-time tradeoff tolerates even more bias. These surprising insights have important
impications for modelling mental algorithms and cognitive biases, as we will illustrate in the next
section.

5

Figure 4: Expected number of correct predictions of a binary event from 16 samples as a function
of the bias of the belief distribution from which these samples are drawn. Different lines correspond
to different degrees of predictability. As the belief becomes biased, the expected number of correct
predictions declines gracefully and stays near its maximum for a wide range of biases; especially if
the predictability is high. This illustrates that biased beliefs can support decisions with high utility.
Therefore the value of reducing bias can be rather low.

3 A rational explanation of the anchoring bias in numerical estimation

The bounded optimality of biased estimates might be able to explain why people’s estimates of
certain unknown quantities (e.g.
the duration of Mars’s orbit around the sun) are systematically
biased towards other values that come to mind easily (e.g. 365 days). This effect is known as the
anchoring bias. The anchoring bias has been explained as a consequence of people’s use of the
anchoring-and-adjustment heuristic [1]. The anchoring-and-adjustment heuristic generates an initial
estimate by recalling a related quantity from memory and adjusts it until a plausible value is reached
[17]. Epley and Gilovich have shown that people use an anchoring-and-adjustment for unknown
quantities that call to mind a related value by demonstrating that the anchoring bias decreases with
subjects’ motivation to be accurate and increases with cognitive load and time pressure [17, 20, 21].
Here we model Experiment 1B from [17]. In this study, one group of 54 subjects estimated six
numerical quantities including the duration of Mars’s orbit around the sun and the freezing point of
vodka. Each quantity was chosen such that subjects would not know its true value but the value of
a related quanity (i.e. the intended anchor, e.g. 365 days and 32◦F). Another group was asked to
provide plausible ranges ([li , ui ], 1 ≤ i ≤ 6) for the same quantities. If responses were sampled
uniformly from the plausible values or a Gaussian centered on their mean, then mean responses
should fall into the center of the plausible ranges. By contrast, the ﬁrst group’s mean responses
deviated signiﬁcantly from the center of the plausible range towards the anchor; the deviations were
measured by dividing the difference between the mean response and the range endpoint nearest the
intended anchor by the range of plausible values (“mean skew”). Thus subjects’ adjustments were
insufﬁcient, but why? Our model explains insufﬁcient adjustments as the optimal bias-time tradeoff
of an iterative inference algorithm that formalizes the anchoring-and-adjustment heuristic.

3.1 Modelling Estimation as Probabilistic Inference

When people are asked to estimate numerical quantities they appear to sample from an internal
probability distribution [22]. For quantities such as the duration of Mars’s orbit around the sun the
process computing those distributions may be reconstruction from memory. Reconstruction from
memory has been shown to combine noisy memory traces y with prior knowledge about categories
p(X ) into a more accurate posterior belief p(X |y) [23, 24]. For other quantities such as the duration
of Mars’ orbit there may be no memory trace of the quantity itself, but there will be memory traces
of related quantities. In either case the relation between the memory trace and the quantity to be
estimated is probabilistic and can be described by a likelihood function. Therefore estimation always
amounts to combining noisy evidence with prior knowledge. The six estimation tasks were thus
modelled as probabilistic inference problems.

6

The unknown posterior distributions were approximated by Gaussians with a mean equal to the
actual value and a variance such that the 95% posterior credible intervals are as wide as the plausible
ui−li
ranges reported by the subjects (σi =
Φ−1 (0.975)−Φ−1 (0.025) ). Epley and Gilovich [17] assumed that
the anchoring-and-adjustment process starts from self-generated anchors, such as the duration of
a year in the case of estimating the duration of Mars’ orbit. The initial distributions were thus
modelled by delta-distributions on the subjects’ self-generated anchors. Considered adjustments
were assumed to be sampled from N (µ = 0, σ = 10). Thus numeric estimation can be modelled
as probabilistic inference according to our rational model. The model’s optimal bias-time tradeoff
depends only on the ratio of the cost per iteration to the cost per unit error. This ratio (˜c = c/v ) was
estimated from subjects’ mean responses reported in [17] by weighted least squares. The weights
were the precisions (inverse variances) of the assumed posterior distributions. The resulting point
estimate was used to predict subjects’ mean responses.

3.2 Results

Figure 5 illustrates the model-ﬁt in terms of the adjustment scores. The predicted adjustment scores
are highly correlated with those computed from subjects’ responses (r = 0.95). Importantly, when-
ever the experiment found an insufﬁcient adjustment (skew < 0.5) this is also predicted by our
model. Conversely, in the one condition where the experiment did not ﬁnd an insufﬁcient adjust-
ment that is also what our model predicts. Thus the anchoring bias reported in [17] may result from
a boundedly optimal probabilistic inference algorithm. In another study (Experiment 2C in [17])

Figure 5: Comparison of people’s mean adjustment scores in the six estimation tasks of [17] with
those predicted by our model. The model achieves a good ﬁt with just a single parameter. The
correlation between predicted and measured adjustment scores is about 0.95.

Epley and Gilovich found that the anchoring bias was stronger when while estimating numerical
quantities subjects had to remember an eight-letter string (high cognitive load) than when they did
not(low cognitive load). We used this data set to assess the model’s validity. If the model’s parameter
˜c measures the relative cost per iteration, then its estimate should be higher when the simultaneous
performance of an additional task restricts subjects’ computational resources. This is indeed what we
found. For subjects under high cognitive load the estimated relative cost per iteration (˜cbusy = 0.31)
was higher than for subjects under low cognitive load (˜cnotbusy = 0.18). This is consistent with
the parameter’s intended interpretation, and the resulting model predictions captured the increases
of the anchoring bias under cognitive load.

4 Discussion

In this article we have performed a bounded-optimality analysis of the Metropolis-Hastings algo-
rithm. Across a wide range of time costs the boundedly-optimal solution performs so few iterations
that the resulting estimates are biased towards the initial value. This provides a rational basis for

7

the anchoring-and-adjustment heuristic [1] as a previous analysis [25] did for the representativeness
heuristic [1]. By deriving the anchoring-and-adjustment heuristic from a general approximate infer-
ence algorithm we open the door to understanding how this heuristic should apply in domains more
complex than the estimation of a single number. From previous work in psychology, it is not obvious
what an adjustment should look like in the space of objects or events, for instance. By reinterpreting
adjustment in terms of MCMC, we extend it to almost arbitrary domains of inference. Furthermore
our model illustrates that heuristics in general can be formalized by and derived as boundedly opti-
mal approximations to rational inference. This provides a new perspective on the resulting cognitive
biases: the anchoring bias is tolerated because its consequences are less costly than the time that
would be required to eliminate it. Thus the anchoring bias can be interpreted as a sign of bounded
optimality rather than irrationality. This may be equally true of other cognitive biases, because elim-
inating bias can be costly and biased beliefs do not necessarily lead to poor decisions (see Figure
4).
This article illustrates the value of bounded optimality as a framework for deriving models of cog-
nitive processes. This approach is a formal synthesis of the function-ﬁrst approach underlying
Bayesian models of cognition [2] with the limitations-ﬁrst approach that starts from cognitive and
perceptual illusions (e.g. [1]). This synthesis can be achieved by augmenting the problems facing
the mind with constraints on information processing and solving them formally using optimization.
Rather than determining optimal beliefs and actions, this approach seeks to determine boundedly
optimal processes. Conversely, given a process the same framework can be used to determine the
processing constraints under which it is boundedly optimal. This idea is so general that it can be
applied to reverse-engineering the whole spectrum of mental algorithms and processing constraints.
Understanding the mind’s probabilistic inference algorithm(s) will require many more steps than we
have been able to take so far. We have demonstrated the bounded-optimality of very few iterations
in a trivial inference problem for which an analytical solution exists. Thus, one may argue that our
conclusions could be speciﬁc to a simple one-dimensional inference problem. However, our results
follow from a very general mathematical property: the geometric convergence of the Metropolis-
Hastings algorithm. This property is also true of many complex and high-dimensional inference
problems [18]. Furthermore, Equation 8 predicts that if the inference problem becomes more difﬁ-
cult, then the bias b(cid:63) tolerated by a boundedly-optimal MCMC algorithm increases, because higher
complexity leads to slower convergence and this means r → 1. This suggests, that resource-rational
solutions to the challenging inference problems facing the human mind are likely to be biased, but
this remains to be shown. In addition, the solution determined by our bounded-optimality analysis
remains to be evaluated against alternative solutions to real-time decision-making under uncertainty.
Furthermore, while our model was consistent with existing data, our formalization of the estimation
task can be questioned, and since [17] did not report standard errors, we were unable to perform a
proper statistical assessment of our model. It therefore remains to be shown whether or not people’s
bias-time tradeoff is indeed near-optimal. This will require dedicated experiments that systemati-
cally manipulate probability structure, time pressure, and error costs.
The prevalence of Bayesian computational-level models has made a sampling based view of mental
processing attractive—after all, sampling algorithms are ﬂexible and efﬁcient solutions to difﬁcult
Bayesian inference problems. Vul et al. [7] explored the rational use of a sampling capacity, arguing
that it is often rational to decide based on only one perfect sample. However, perfect samples can
be hard to come by; here we have shown that it can be rational to decide based on one imperfect
sample. A rational sampling-based view of mental processing thus leads naturally to a biased mind.

References

[1] A. Tversky and D. Kahneman. Judgment under uncertainty: Heuristics and biases. Science,
185(4157):1124–1131, 1974.

[2] T. L. Grifﬁths, C. Kemp, and J. B. Tenenbaum. Bayesian models of cognition. In R. Sun, editor,
The Cambridge handbook of computational psychology, chapter 3, pages 59–100. Cambridge
University Press, 2008.

[3] D. Marr. Vision: A Computational Investigation into the Human Representation and Process-
ing of Visual Information. W. H. Freeman, 1983.

8

[4] A. N. Sanborn, T. L. Grifﬁths, and D. J. Navarro. Rational approximations to rational models:
Alternative algorithms for category learning. Psychological Review, 117(4):1144–1167, 2010.
[5] L. Shi, T. L. Grifﬁths, N. H. Feldman, and A. N. Sanborn. Exemplar models as a mechanism
for performing Bayesian inference. Psychonomic Bulletin & Review, 17(4):443–464, 2010.
[6] E. Bonawitz, S. Denison, A. Chen, A. Gopnik, and T. L. Grifﬁths. A simple sequential algo-
rithm for approximating bayesian inference. Proceedings of the 33rd Annual Conference of
the Cognitive Science Society, Austin, TX: Cognitive Science Society, 2011.
[7] E. Vul, N. D. Goodman, T. L. Grifﬁths, and J. B. Tenenbaum. One and done? Optimal decisions
from very few samples. Proceedings of the 31st Annual Conference of the Cognitive Science
Society, 2009.
[8] T. L. Grifﬁths and J. B. Tenenbaum. Optimal predictions in everyday cognition. Psychological
Science, 17(9):767–773, 2006.
[9] T. L. Grifﬁths and J. B. Tenenbaum. Predicting the future as bayesian inference: People com-
bine prior knowledge with observations when estimating duration and extent. Journal of Ex-
perimental Psychology: General, 140(4):725–743, 2011.
[10] N.D. Goodman, J. B. Tenenbaum, J. Feldman, and T. L. Grifﬁths. A rational analysis of rule-
based concept learning. Cognitive Science, 32(1):108–154, 2008.
[11] S. J. Gershman, E. Vul, and J. B. Tenenbaum. Multistability and perceptual inference. Neural
Computation, 24(1):1–24, 2011.
[12] R. Moreno-Bote, D. C. Knill, and A. Pouget. Bayesian sampling in visual perception. Pro-
ceedings of the National Academy of Sciences, 108(30):12491–12496, 2011.
[13] L. Buesing, J. Bill, B. Nessler, and W. Maass. Neural dynamics as sampling: a model for
stochastic computation in recurrent networks of spiking neurons. PLoS Computational Biol-
ogy, 7(11):e1002211+, 2011.
[14] D. Pecevski, L. Buesing, and W. Maass. Probabilistic inference in general graphical models
through sampling in stochastic networks of spiking neurons. PLoS Computational Biology,
7(12):e1002294+, 2011.
[15] E. Horvitz. Reasoning about beliefs and actions under computational resource constraints. In
Uncertainty in Artiﬁcial Intelligence 3 Annual Conference on Uncertainty in Artiﬁcial Intelli-
gence (UAI-87), pages 301–324, Amsterdam, NL, 1987. Elsevier Science.
[16] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97–109, 1970.
[17] N. Epley and T. Gilovich. The anchoring-and-adjustment heuristic. Psychological Science,
17(4):311–318, 2006.
[18] K. L. Mengersen and R. L. Tweedie. Rates of convergence of the Hastings and Metropolis
algorithms. The Annals of Statistics, 24(1):101–121, 1996.
[19] A. E. Raftery and S. Lewis. How many iterations in the Gibbs sampler. In Bayesian Statistics
4, volume 4, pages 763–773, 1992.
[20] N. Epley and T. Gilovich. When effortful thinking inﬂuences judgmental anchoring: differen-
tial effects of forewarning and incentives on self-generated and externally provided anchors.
Journal of Behavioral Decision Making, 18(3):199–212, 2005.
[21] N. Epley, B. Keysar, L. Van Boven, and T. Gilovich. Perspective taking as egocentric anchoring
and adjustment. Journal of Personality and Social Psychology, 87(3):327–339, 2004.
[22] E. Vul and H. Pashler. Measuring the crowd within. Psychological Science, 19(7):645–647,
2008.
[23] J. Huttenlocher, L. V. Hedges, and J. L. Vevea. Why do categories affect stimulus judgment?
Journal of experimental psychology. General, 129(2):220–241, 2000.
[24] P. Hemmer and M. Steyvers. A Bayesian account of reconstructive memory. Topics in Cogni-
tive Science, 1(1):189–202, 2009.
[25] J. B. Tenenbaum and T. L. Grifﬁths. The rational basis of representativeness. In Proceedings
of the 23rd Annual Conference of the Cognitive Science Society, pages 84–98, 2001.

9


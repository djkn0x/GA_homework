Kernel Hyperalignment

Alexander Lorbert & Peter J. Ramadge
Department of Electrical Engineering
Princeton University

Abstract

We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes
problem, or hyperalignment. Our new method, called Kernel Hyperalignment,
expands the scope of hyperalignment to include nonlinear measures of similar-
ity and enables the alignment of multiple datasets with a large number of base
features. With direct application to fMRI data analysis, kernel hyperalignment is
well-suited for multi-subject alignment of large ROIs, including the entire cortex.
We report experiments using real-world, multi-subject fMRI data.

1

Introduction

One of the goals of multi-set data analysis is forming qualitative comparisons between datasets. To
the extent that we can control and design experiments to facilitate these comparisons, we must ﬁrst
ask whether the data are aligned. In its simplest form, the primary question of interest is whether
corresponding features among the datasets measure the same quantity. If yes, we say the data are
aligned; if not, we must ﬁrst perform an alignment of the data.
The alignment problem is crucial to multi-subject fMRI data analysis, which is the motivation for
this work. An appreciable amount of effort is devoted to designing experiments that maintain the
focus of a subject. This is to ensure temporal alignment across subjects for a common stimulus.
However, with each subject exhibiting his/her own unique spatial response patterns, there is a need
for spatial alignment. Speciﬁcally, we want between subject correspondence of voxel j at TR i
(Time of Repetition). The typical approach taken is anatomical alignment [20] whereby anatomi-
cal landmarks are used to anchor spatial commonality across subjects. In linear algebra parlance,
anatomical alignment is an afﬁne transformation with 9 degrees of freedom.
Recently, Haxby et al. [9] proposed Hyperalignment, a function-based alignment procedure. Instead
of a 9-parameter transformation, a higher-order, orthogonal transformation is derived from voxel
time-series data. The underlying assumption of hyperalignment is that, for a ﬁxed stimulus, a sub-
ject’s time-series data will possess a common geometry. Accordingly, the role of alignment is to
ﬁnd isometric transformations of the per-subject trajectories traced out in voxel space so that the
transformed time-series best match each other. Using their method, the authors were able to achieve
a between-subject classiﬁcation accuracy on par with—and even greater than—within-subject accu-
racy.
Suppose that subject data are recorded in matrices X1:m ∈ Rt×n . This could be data from an
experiment involving m subjects, t TRs, and n voxels. We are interested in extending the regularized
minimize (cid:80)
hyperalignment problem
i<j (cid:107)XiRi − Xj Rj (cid:107)2
F
subject to RT
k = 1, 2, . . . , m ,
k AkRk = I
where matrices A1:m ∈ Rn×n are symmetric and positive deﬁnite. In general, the above problem
manifests itself in many application areas. For example, when Ak = I we have hyperalignment or

(1)

1

a multi-set orthogonal Procrustes problem, commonly used in shape analysis [6, 7]. When Ak =
k Xk , (1) represents a form of multi-set Canonical Correlation Analysis (CCA) [12, 13, 8].
XT
The success of hyperalignment engenders numerous questions and in this work we address two of
them. First, is hyperalignment scalable? In [9], the authors consider a subset of ventral temporal cor-
tex (VT), using hundreds of voxels. The relatively-low voxel count alleviates a huge computational
cost and storage burden. However, the current method for solving (1) is infeasible when considering
many or all voxels, and therefore limits the scope of hyperalignment to a local alignment procedure.
For example, if n = 50,000 voxels, then storing the n × n matrix for one subject requires over 18
gigabytes of memory. Moreover, computing a full SVD for a matrix this size is a tall order.
Coupled with scalability, we also ask whether we can include new features of our subjects’ data.
For example, we may want to augment the input data with the associated second-order mixtures,
i.e., n voxels become ( n
2 ) = n(n+1)/2 features. Again, for a reasonably-sized voxel count,
1 ) + ( n
running hyperalignment is infeasible.
Addressing scalability and feature extension results in the main contribution of kernel hyperalign-
ment. The inclusion of a large feature space motivates the use of kernel methods. Additionally,
numerous optimization problems that use the kernel trick possess global optimizers spanned by the
mapped examples. This is guaranteed by the Representer Theorem [14, 18]. Therefore, the two sep-
arate issues of scalability and feature extension are merged into a single problem through the use of
kernel methods. With kernel hyperalignment, the bottleneck shifts from voxel count to the number
of TRs times subjects (or the original inputs to the number of examples).
The problem we address in this paper is the alignment of multiple datasets in the same and extended
feature space. Multi-set data analysis by means of kernel methods has already been considered in
the framework of CCA [16, 1]. Our approach deviates from [1] and [15] because we focus on align-
ment and never leave feature space until training and testing. We use the kernel trick as a means
of navigating through a high-dimensional orthogonal group. Our CCA variant is more constrained,
and each dataset is assigned the same kernel, supplying us with a richer, single reproducing kernel
Hilbert space (RKHS) over a collection of m smaller and distinct ones. Allowing for subject-speciﬁc
kernels leads to the difﬁcult problem of selecting them—a signiﬁcantly harder problem than select-
ing a single kernel. In this respect, we assume a single kernel can provide the sought-after linearity
used for comparing multiple datasets.
The paper is organized as follows: in §2 we review regularized hyperalignment, or the regularized
multi-set orthogonal Procrustes problem. Next, in §3 we formulate its kernel variant, and in §4 we
discuss classiﬁcation with aligned data. We provided experimental results in §5, and we conclude in
§6. All proofs are supplied in the Supplemental Material.

2 Hyperalignment
minimize (cid:80)m
The hyperalignment problem of (1) is equivalent to [7]:
(cid:80)m
i=1 (cid:107)XiRi − Y(cid:107)2
F
and RT
k AkRk = I for k = 1, . . . , m .
subject to Y = 1
j=1Xj Rj
m
The matrix Y is the image centroid and serves as the catalyst for computing a solution: for dataset
i, ﬁx a centroid and solve for Ri . This process cycles over all datasets for a speciﬁed number of
rounds, or until approximate convergence is reached (see Algorithm 1). The dynamic centroid Y
can be a sample mean or a leave-one-out (LOO) mean. Regardless of type, the last round should use
the ﬁxed sample mean provided by the penultimate round. We can set Qk = A1/2
k Rk , using the
symmetric, positive deﬁnite square root1 , yielding the key operation
− 1
minimize (cid:107)XkA
k Qk − Y(cid:107)2
2
F
subject to QT
k Qk = I .
The above is the familiar orthogonal Procrustes problem [19] and is solved using the SVD of
− 1
k Y .
k XT
A
2
1 In practice, we would use the Cholesky factorization of Ak . However, in deriving the kernel hyperalign-
ment procedure it is necessary to familiarize the reader with this approach.

(2)

(3)

2

3 Kernel Hyperalignment

The previous section dealt with alignment based on the original data. In the context of optimization,
the alignment problem of (1) is indifferent to both data generation and data recording. There are,
however, implicit assumptions about these two processes. The data are generated according to a
common input signal, and each of the m datasets represents a speciﬁc view of this signal. In other
words, the matrices X1:m have row correspondence. The alignment problem of (1) seeks column
correspondence through a linear mapping of the original features.
In fMRI, the m views are manifested by m subjects experiencing a common, synchronous stimulus.
Each data matrix records fMRI time-series data: the rows are indexed by a TR and the columns are
indexed by a voxel. There are t TRs and n voxels per subject, i.e., Xk ∈ Rt×n . The synchrony of the
stimulus ensures row correspondence. Hyperalignment can be posed as the minimization problem
of (2) with Ak = I. Voxel (column) correspondence is then achieved via an orthogonal constraint
placed on each of the linear mappings. The orthogonal constraint present in hyperalignment follows
a subject-independent isometry assumption. We can view the time-series data of each subject as a
trajectory in Rn . For a ﬁxed stimulus this trajectory is [approximately] identical—up to a rotation-
reﬂection—across subjects.
As stated above, we are assuming equivalence of the per-view information in its original form, but
we are not assuming that this information can be related through a linear mapping. Now suppose
there is a common set of N features—derived from each n-dimensional example—that does allow
for a linear relationship between views. Alternatively, there may be derivative features of interest
that lead to better alignment via a linear mapping. For example, it is conceivable that second-order
data, i.e., pairwise mixtures of the original data, obey a linear construct and may be a preferred
feature set for alignment. In general, we wish to formulate an alignment technique for this new
feature set. Rather than limit expression of the data to the n given coordinates, we consider an
N -coordinate representation, where N may be much greater than n.
φ1 (xi
 ∈ Rt×N .
i(cid:48) ∈ Rn . We introduce the row-based mapping of Xi :
Let Xi ∈ Rt×n have i(cid:48) -th row [xi
i(cid:48) ]T with xi
· · · φN (xi
1 ) φ2 (xi
1 )
1 )
...
...
...
· · ·
φN (xi
t ) φ2 (xi
φ1 (xi
t )
t )
The N functions φ1:N : Rn → R are used to derive N features from the original data. For matrix
Xi ∈ Rt×n let Φi = Φ(Xi ). In general, for Xi ∈ Rt×n and Xj ∈ Rs×n , we deﬁne the Gram
j ∈ Rt×s . We also write Ki (cid:44) Kii = ΦiΦT
matrix Kij (cid:44) ΦiΦT
i . We assume that there is an
appropriate positive deﬁnite kernel, ˆk : Rn × Rn → R, so that we can leverage the kernel trick
[2, 10] and obtain the i(cid:48) j (cid:48) -th element of Kij via
(Kij )i(cid:48) j (cid:48) = ˆk( xi
i(cid:48) , xj
j (cid:48) ) .
minimize (cid:80)
Using the feature map Φ(·), we form the regularized Kernel Hyperalignment problem:
i<j (cid:107)Φ(Xi )Ri − Φ(Xj )Rj (cid:107)2
F
subject to RT
k AkRk = I for k = 1, . . . , m .
The latent variables are R1:m ∈ RN ×N and we are given symmetric, positive deﬁnite matrices
A1:m ∈ RN ×N . Although different than the original hyperalignment problem, obtaining a solution
to (6) is accomplished in the same way: ﬁx a centroid and ﬁnd the individual linear maps. To this
end, the key operation involves solving
− 1
(cid:107)ΦiR − Ψ(cid:107)2
i Q − Ψ(cid:107)2
arg min
arg min
F ,
2
F
(cid:80)
QT Q=I
RT AiR=I
where Φi = Φ(Xi ), i ≥ 1, is the current, individual dataset under consideration and Ψ =
j∈A Φj ˆRj is a centroid based on the current estimates of R1:m , denoted ˆR1:m . The index set
1|A|
A ⊆ {1, . . . , m} determines how the estimated centroid is calculated (sample or LOO mean).

(cid:107)ΦiA

Φ(Xi ) =

(4)

(5)

(6)

(7)

or

3

The difﬁculty of (7) lies in the size of N . Any of the well-known kernels correspond to an N so large
that direct computation is generally impractical. For example, if using second-order interactions as
the feature set, the number of unknowns in kernel hyperalignment is O(mn4 ) in contrast to O(mn2 )
unknowns for hyperalignment. Nevertheless, the minimization problem of (7) places us in familiar
territory of solving an orthogonal Procrustes problem.
Since we are now in feature space, the matrix Ai poses a problem unless we conﬁne it to a speciﬁc
−1/2
would be infeasible for large N . Additionally,
form. For example, if Ai is random, ﬁnding A
i
the constraint RT
i AiRi = I would lack any intuition. Therefore, we restrict Ai = αI + βΦT
i Φi
with α > 0 and β ≥ 0. As with regularized hyperalignment [22], when (α, β ) = (1, 0) we obtain
hyperalignment and when (α, β ) ≈ (0, 1) we obtain a form of CCA.
i , where Λi = diag{λi1 , . . . , λit} or diagj {λij } for
Let Ki have eigen-decomposition ViΛiVT
1√
short. We introduce two symmetric, positive deﬁnite matrices: Bi = Vi diagj {
}VT
i and
α+βλij
1√
α )}VT
− 1√
Ci = Vi diagj { 1
i .
(
λij
α+βλij

Lemma 3.1. For Ai = αI + βΦT
i Φi we have A

We can use Lemma 3.1 to transform (7) into
(cid:107)BiΦiQ − Ψ(cid:107)2
F

− 1
i = 1√
i CiΦi and ΦiA
α I + ΦT
2
(cid:104) 1|A|
(cid:16)
(cid:80)
j∈A Bj Φj ˆQj
where ˆQj is the current estimate of Qj . Solving for the matrix Q is still well beyond practical
computation. The following lemma is the gateway for managing this problem.
Lemma 3.2. If ˜U ∈ St(N , d) and ˜G ∈ O(d), then ˜Q = IN − ˜U(Id − ˜G) ˜UT ∈ O(N ).2

− 1
i = BiΦi .
2
(cid:105)(cid:17)

arg min
QT Q=I

,

(8)

or

arg max
QT Q=I

tr

QT ΦT
i Bi

Familiar applications of the above lemma include the identity matrix ( ˜G = Id ) and Householder
reﬂections ( ˜G = −Id ).
If ˜G is block diagonal with 2 × 2 blocks of Givens rotations, then the
columns of ˜U, taken two at a time, are the two-dimensional planes of rotation [7]. We therefore
refer to ˜U as the plane support matrix.
Lemma 3.2 can be interpreted as a lifting mechanism for identity deviations. The difference Id − ˜G
represents a O(d) deviation from identity. Applying ˜U(Id − ˜G) ˜UT = IN − ˜Q, “lifts” this differ-
ence to a O(N ) deviation from identity. Reversing directions, we can also utilize Lemma 3.2 for
compressing O(N ). From IN − ˜Q = ˜U(Id − ˜G) ˜UT , the rank of the deviation, IN − Q, is upper
bounded by d, producing a subset of O(N ).
Motivated by Lemma 3.2 we impose
Qi = IN − U(I − Gi )UT ,
(9)
where U ∈ St(N , r), Gi ∈ O(r), and 1 ≤ r ≤ N . Ideally, we want r small to beneﬁt from a
reduced dimension. As is typically the case when using kernel methods, leveraging the Representer
Theorem shifts the dimensionality of the problem from the feature cardinality to the number of
(cid:3)T
Φ0 = (cid:2)ΦT
examples, i.e., r = mt. We pool all of the data, forming the mt × N matrix
· · · ΦT
(10)
1 ΦT
,
2
m
− 1
0 assumed positive deﬁnite. As long as r ≤ N ,
0 ∈ RN ×r with K0 = Φ0ΦT
and set U = ΦT
0 K
2
− 1
− 1
− 1
− 1
the orthogonality constraint is met because (ΦT
0 = Ir .
)T (ΦT
0 K
0 K
) = K
0 K0K
2
2
2
2
0
0
Theorem 3.3 (Hyperalignment Representer Theorem). Within the set of global minimizers of (6)
− 1
− 1
there exists a solution {R(cid:63)
m } = {A
m} that admits a representation
1 , . . . , R(cid:63)
1 Q(cid:63)
m Q(cid:63)
1 , . . . , A
2
2
− 1
i ∈ O(mt) (i = 1, . . . , m).
i = IN − U(I − G(cid:63)
and G(cid:63)
i )UT , where U = ΦT
Q(cid:63)
0 K
2
0
2 St(N , d) (cid:44) {Z : Z ∈ RN ×d , ZT Z = Id } is the (N , d) Stiefel Manifold (N ≥ d), and
O(N ) (cid:44) {Z : Z ∈ RN ×N , ZT Z = IN } is the orthogonal group of N × N matrices.

4

Input: X1:m ∈ Rt×n , A1:m ∈ Rn×n
Output: R1:m ∈ Rn×n
Initialize Q1:m as identity (n × n)
1:m← XiA
−1/2
Set ˜Xi
(cid:40) {1, 2, . . . , m}
i
foreach round do
foreach subject/view i do
sample mean
A ←
{1, 2, . . . , m} \ {i} LOO mean
(cid:88)
Y ← 1
˜Xj Qj
|A|
j∈A
[ ¯U ¯Σ ¯V] ← SVD( ˜XT
i Y)
Qi ← ¯U ¯VT

end

end
foreach subject/view i do
− 1
Ri ← A
i Qi
2

end

Algorithm 1: Regularized Hyperalignment

(cid:3)T

Input: ˆk(·, ·), α, β , X1:m ∈ Rt×n
Initialize plane support Φ0 = (cid:2)ΦT
Output: R1:m , linear maps in feature space
Initialize feature maps Φ1 , . . . , Φm ∈ Rt×N
2 · · · ΦT
1 ΦT
m
Initialize G1:m ∈ Rr×r as identity (r = mt)
(cid:40) {1, 2, . . . , m}
foreach round do
foreach subject/view i do
sample mean
A ←
{1, 2, . . . , m} \ {i} LOO mean
(cid:88)
Y ← 1
˜Bj Gj
|A|
j∈A
[ ¯U ¯Σ ¯V] ← SVD( ˜BT
i Y)
Gi ← ¯U ¯VT

end

end
foreach subject/view i do
− 1
Qi ← I − ΦT
(Ir − Gi )K
0 K
2
0
− 1
Ri ← A
i Qi
2

− 1
0 Φ0
2

end
Algorithm 2: Regularized Kernel Hyperalignment

When mt is large enough so that evaluating an SVD of numerous mt × mt matrices is prohibitive,
we can ﬁrst perform PCA-like reduction. Let K0 have eigen-decomposition V0Λ0VT
0 , where the
nonnegative diagonal entries of Λ0 are sorted in decreasing order. We set Φ0(cid:48) = VT
0(cid:48) Φ0 , where
−1/2
V0(cid:48) is formed by the ﬁrst r columns of V0 , and then use U = ΦT
.
In general, rather
0(cid:48) K
0(cid:48)
than compute Q according to (7), involving N (N −1)/2 = O(N 2 ) degrees of freedom (when N is
ﬁnite), we end up with r(r−1)/2 = O(r2 ) degrees of freedom via the kernel trick.
GT ˜BT
 1
 ,
− 1
0 ∈ Rt×r . We reduce (8) in terms of Gi and obtain (Supplementary Material)
Let ˜Bi = BiKi0K
(cid:88)
2
|A|
Gi = arg max
tr
i
G∈O(r)
j∈A
(cid:105)
(cid:104) 1|A|
(cid:80)
where ˆGj is the current estimate of Gj . Equation (11) is the classical orthogonal Procrustes prob-
lem. If ¯U ¯Σ ¯VT is the SVD of GT ˜BT
j∈A ˜Bj ˆGj
, then a maximizer is given by ¯U ¯VT [7].
i
The kernel hyperalignment procedure is given in Algorithm 2. Using the approach taken in this
section also leads to an efﬁcient solution of the standard orthogonal Procrustes problem for n ≥ 2t
(Supplementary Material). In turn, this leads to an efﬁcient iterative solution for the hyperalignment
problem when n is large.

˜Bj ˆGj

(11)

4 Alignment Assessment

An alignment procedure is not subject to the typical train-and-test paradigm. The lack of spatial
correspondence demands an align-train-test approach. We assume these three sets have within-
subject (or within-view) alignment. With all other parameters ﬁxed, if the aligned test error is
smaller than the unaligned test error, there is strong evidence suggesting that alignment was the
underlying cause.
Kernel hyperalignment returns linear transformations R1:m that act on data living in feature space.
In general, we cannot directly train and test in the feature space due to its large size. We can,
however, learn from relational data. For example, we can compute distances between examples
and, subsequently, produce nearest neighbor classiﬁers. Assume (α, β ) = (1, 0), i.e., the R1:m

5

.

(14)

are orthogonal. If x1 ∈ Rn is a view-i example and x2 ∈ Rn is a view-j example, the respective
pre-aligned and post-aligned squared distances between the two examples are given by
(cid:107)Φ(xT
1 ) − Φ(xT
2 )(cid:107)2
F = ˆk(x1 , x1 ) + ˆk(x2 , x2 ) − 2ˆk(x1 , x2 )
(12)
F = ˆk(x1 , x1 ) + ˆk(x2 , x2 ) − 2Φ(xT
2 )Rj (cid:107)2
1 )Ri − Φ(xT
(cid:107)Φ(xT
(13)
j Φ(xT
1 )RiRT
2 )T .
The cross-term in (13) has not been expanded for a simple reason: it is too messy. We realized early
on that the alignment and training phase would be replete with lengthy expansions and, consequently,
sought to simplify matters with a computer science solution. Both binary and unary operations in
feature space can be accomplished with a simple class. Our Phi class stores expressions of the
(cid:80)K
(cid:80)K
bIN + (cid:80)K
following forms:
(cid:123)(cid:122)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:125)
k=1Φ(Xa(k) )T Mk
k=1Φ(Xa(k) )T MkΦ(Xa(k) )
k=1MkΦ(Xa(k) )
Type 2
Type 1
Type 3
Each class instance stores matrices M1:K , scalar b, right address vector a, and left address vector a.
The address vectors are pointers to the input data. This allows for faster manipulation and smaller
memory allocation. Addition and subtraction require a common type. If types match, then the M
matrices must be checked for compatible sizes. Multiplication is performed for types 1 with 2, 1
with 3, 2 with 1, 3 with 2, and 3 with 3. The ﬁrst of these cases, for example, produces a numeric
result via the kernel trick. We also deﬁne scalar multiplication and division for all types and matrix
multiplication for types 1 and 2. A transpose operator applies for all types and maps type 1 to 2,
2 to 1, and 3 to 3. More advanced operations, such as powers and inverses, are also possible. Our
implementation was done in Matlab.
The construction of the Phi class allows us to stay in feature space and avoid lengthy expansions. In
turn, this facilitates implementing the richer set of SVM classiﬁers. Let X¯1 , . . . , X ¯m ∈ Rs×n be our
training data with feature representation Φ¯ı = Φ(X¯ı ) ∈ Rs×N . Recall that kernel hyperalignment
¯ ; we now
seeks to align in feature space. Before alignment we might have considered K¯ı¯ = Φ¯ıΦT
¯ . If every row of X¯ı has a corresponding
consider the Gram matrix (Φ¯ıRi )(Φ¯Rj )T = Φ¯ıRiRT
j ΦT
 ,

T
 Φ¯1R1
 Φ¯1R1
×
label, we can train an SVM with
Φ¯1R1RT
1 ΦT
¯1 Φ¯1R1RT
2 ΦT
¯2
Φ¯2R2RT
2 ΦT
¯1 Φ¯2R2RT
1 ΦT
...
...
¯2
K ¯A =
=
...
Φ ¯mRm
Φ ¯mRm
mΦT
Φ ¯mRmRT
1 ΦT
Φ ¯mRmRT
¯1
¯m
¯A ∈ Rms×ms denotes the aligned kernel matrix. The unaligned kernel matrix, K ¯U ,
where K ¯A = KT
is also an m × m block matrix with ij -th block K¯ı¯ .
Using the dual formulation of an SVM, a classiﬁer can be constructed from the relational data
exhibited among the examples [4]. Similar to a k-nearest neighbor classiﬁer relying on pairwise
distances, an SVM relies on the kernel matrix. The kernel matrix is a matrix of inner products and
is therefore linear. This enables us to assess a partition-based alignment.
In fMRI, we perform two alignments—one for each hemisphere. Each alignment produces two
aligned kernel matrices, which we sum and then input into an SVM. Thus, linearity provides us the
means to handle ﬁner partitions by simply summing the aligned kernel matrices.

· · · Φ¯1R1RT
mΦT
¯m

(15)

. . .

6

Kernel

Table 1: Seven label classiﬁcation using movie-based alignment Below is the cross-validated,
between-subject classiﬁcation accuracy (within-subject in brackets) with (α, β ) = (1, 0). Four
hundred TRs per subject were used for the alignment. Chance = 1/7 ≈ 14.29%.
Ventral Temporal
Entire Cortex
133,590 voxels/hemisphere
2,997 voxels/hemisphere
Kernel Hyp.
Anatomical
Kernel Hyp.
Anatomical
36.25% [26.79%]
34.64% [26.79%]
48.57% [42.68%]
35.71% [42.68%]
35.00% [43.32%]
50.36% [42.32%]
36.07% [25.54%]
36.43% [25.54%]
36.43% [26.07%]
36.07% [26.07%]
48.57% [43.39%]
36.25% [43.39%]
35.89% [43.21%]
48.21% [43.21%]
35.00% [26.79%]
36.25% [26.79%]

Linear
Quadratic
Gaussian
Sigmoid

5 Experiments

The data used in this section consisted of fMRI time-series data from 10 subjects who viewed a
movie and also engaged in a block-design visualization experiment [17]. Each subject saw Raiders
of the Lost Ark (1981) lasting a total of 2213 TRs. In the visualization experiment, subjects were
shown images belonging to a speciﬁc class for 16 TRs followed by 10 TRs of rest. The 7 classes
were: (1) female face, (2) male face, (3) monkey, (4) house, (5) chair, (6) shoe and (7) dog. There
were 8 runs total, and each run had every image class represented once.
We assess alignment by classiﬁcation accuracy. To provide the same number of voxels per ROI for
all subjects, we ﬁrst performed anatomical alignment. We then selected a contiguous block of 400
TRs from the movie data to serve as the per-subject input of the kernel hyperalignment. Next, we
extracted labeled examples from the visualization experiment by taking an offset time average of
each 16 TR class exposure. An offset of 6 seconds factored in the hemodynamic response. This
produced 560 labeled examples: 10 subjects × 8 runs/subject × 7 examples/run.
Kernel hyperalignment allows us to (a) use nonlinear measures of similarity, and (b) consider more
voxels for the alignment. Consequently, we (a) experiment with a variety of kernels, and (b) do not
need to pre-select or screen voxels as was done in [9]—we include them all. Table 1 features results
from a 7-label classiﬁcation experiment. Recall that a linear kernel reduces to hyperalignment. We
classiﬁed using a multi-label ν -SVM [3]. We used the ﬁrst 400 TRs from each subject’s movie data,
and aligned each hemisphere separately. The kernel functions are supplied in the Supplementary
Material. As observed in [9] and repeated here, hyperalignment leads to increased between-subject
accuracy and outperforms within-subject accuracy. Thus, we are extracting more common structure
across subjects. Whereas employing Algorithm 1 for 2,997 voxels is feasible (and slow), 133,590
voxels is not feasible at all.
To complete the picture, we plot the effects of regularization. Figure 1 displays the cross-validated,
between-subject classiﬁcation accuracy for varying (α, β ) where α = 1−β . This traces out a route
from CCA (α ≈ 0) to hyperalignment (α = 1). When compared to the alignments in [9], our voxel
counts are orders of magnitude larger. For our four chosen kernels, hyperalignment (α = 1) presents
itself as the option with near-greatest accuracy.
Our results support the robustness of hyperalignment and imply that voxel selection may be a crucial
pre-processing step when dealing with the whole volume. More voxels mean more noisy voxels,
and hyperalignment does not distinguish itself from anatomical alignment when the entire cortex is
considered. We can visualize this phenomenon with Multidimensional Scaling (MDS) [21].
MDS takes as input all of the pairwise distances between subjects (the previous section discussed
distance calculations). Figure 2 depicts the optimal Euclidean representation of our 10 subjects be-
fore and after kernel hyperalignment ((α, β ) = (1, 0)) with respect to the ﬁrst 400 TRs of the movie
data. Focusing on VT, kernel hyperalignment manages to cluster 7 of the 10 subjects. However,
when we shift to the entire cortex, we see that anatomical alignment has already succeeded in a sim-
ilar clustering. Kernel hyperalignment manages to group the subjects closer together, and manifests
itself as a re-centering.

7

Figure 1: Cross-validated between-subject classiﬁcation accuracy (7 labels) as a function of the
regularization parameter, α = 1−β , for various kernels after alignment. The solid curves are for
Ventral Temporal and the dashed curves are for the entire cortex. Chance = 1/7 ≈ 14.29%.

Figure 2: Visualizing alignment with MDS Each locus pair approximates the normalized relation-
ship among the 10 subjects in 2D - before (left) and after (right) applying kernel hyperalignment.
Centroids are translated to the origin and numbers correspond to individual subjects.

6 Conclusion

We have extended hyperalignment in both scale and feature space. Kernel hyperalignment can
handle a large number of original features and incorporate nonlinear measures of similarity. We have
also shown how to use the linear maps—applied in feature space—for post-alignment classiﬁcation.
In the setting of fMRI, we have demonstrated successful alignment with a variety of kernels. Kernel
hyperalignment achieved better between-subject classiﬁcation over anatomical alignment for VT.
There was no noticeable difference when we considered the entire cortex. Nevertheless, kernel
hyperalignment proved robust and did not degrade with increasing voxel count.
We envision a fruitful path for kernel hyperalignment. Empirically, we have noticed a tradeoff
between feature cardinality and classiﬁcation accuracy, motivating the need for intelligent feature
selection within our established framework. Although we have limited our focus to fMRI data anal-
ysis, kernel hyperalignment can be applied to other research areas which rely on multi-set Procrustes
problems.

8

00.20.40.60.810.20.250.30.350.40.450.50.55 ( = 1-)BSC AccuracyLinear Kernel00.20.40.60.810.20.250.30.350.40.450.50.55 ( = 1-)BSC AccuracyQuadratic Kernel00.20.40.60.810.20.250.30.350.40.450.50.55 ( = 1-)BSC AccuracyGaussian Kernel00.20.40.60.810.20.250.30.350.40.450.50.55 ( = 1-)BSC AccuracySigmoid Kernel1234567891012345678910123456789101234567891012345678910123456789101234567891012345678910Ventral TemporalEntire CortexLinear KernelGaussian KernelReferences
[1] F.R. Bach and M.I. Jordan. Kernel independent component analysis. The Journal of Machine
Learning Research, 3:1–48, 2003.
[2] C.M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[3] C.C. Chang and C.J. Lin. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http:
//www.csie.ntu.edu.tw/˜cjlin/libsvm.
[4] P.H. Chen, C.J. Lin, and B. Sch ¨olkopf. A tutorial on ν -support vector machines. Applied
Stochastic Models in Business and Industry, 21(2):111–136, 2005.
[5] A. Edelman, T. As, A. Arias, and T. Smith. The geometry of algorithms with orthogonality
constraints. SIAM J. Matrix Anal. Appl, 1998.
[6] C. Goodall. Procrustes methods in the statistical analysis of shape. Journal of the Royal
Statistical Society. Series B (Methodological), pages 285–339, 1991.
[7] J.C. Gower and G.B. Dijksterhuis. Procrustes Problems, volume 30. Oxford University Press,
USA, 2004.
[8] D.R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview
with application to learning methods. Neural Computation, 16(12):2639–2664, 2004.
[9] J.V. Haxby, J.S. Guntupalli, A.C. Connolly, Y.O. Halchenko, B.R. Conroy, M.I. Gobbini,
M. Hanke, and P.J. Ramadge. A common, high-dimensional model of the representational
space in human ventral temporal cortex. Neuron, 72(2):404–416, 2011.
[10] T. Hofmann, B. Sch ¨olkopf, and A.J. Smola. Kernel methods in machine learning. The Annals
of Statistics, pages 1171–1220, 2008.
[11] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[12] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936.
[13] J.R. Kettenring. Canonical analysis of several sets of variables. Biometrika, 58(3):433, 1971.
[14] G.S. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation on stochastic
processes and smoothing by splines. The Annals of Mathematical Statistics, 41(2):495–502,
1970.
[15] M. Kuss and T. Graepel. The geometry of kernel canonical correlation analysis. Technical
report, Max Planck Institute, 2003.
[16] P.L. Lai and C. Fyfe. Kernel and nonlinear canonical correlation analysis. International Jour-
nal of Neural Systems, 10(5):365–378, 2000.
[17] M.R. Sabuncu, B.D. Singer, B. Conroy, R.E. Bryan, P.J. Ramadge, and J.V. Haxby. Function
based inter-subject alignment of human cortical anatomy. Cerebral Cortex, 2009.
[18] B. Sch ¨olkopf, R. Herbrich, and A. Smola. A generalized representer theorem. In Computa-
tional learning theory, pages 416–426. Springer, 2001.
[19] P.H. Schonemann. A generalized solution of the orthogonal procrustes problem. Psychome-
trika, 31(1):1–10, March 1966.
[20] J. Talairach and P. Tournoux. Co-planar stereotaxic atlas of the human brain: 3-dimensional
proportional system: an approach to cerebral imaging. Thieme, 1988.
[21] J.B. Tenenbaum, V. De Silva, and J.C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319–2323, 2000.
[22] H. Xu, A. Lorbert, P. J. Ramadge, J. S. Guntupalli, and J. V. Haxby. Regularized hyperalign-
ment of multi-set fmri data. Proceedings of the 2012 IEEE Signal Processing Workshop, Ann
Arbor Michigan, 2012.

9


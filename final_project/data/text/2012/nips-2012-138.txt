Statistical Consistency of Ranking Methods in A
Rank-Differentiable Probability Space

Yanyan Lan
Institute of Computing Technology
Chinese Academy of Sciences
lanyanyan@ict.ac.cn

Jiafeng Guo
Institute of Computing Technology
Chinese Academy of Sciences
guojiafeng@ict.ac.cn

Xueqi Cheng
Institute of Computing Technology
Chinese Academy of Sciences
cxq@ict.ac.cn

Tie-Yan Liu
Microsoft Research Asia
Tie-Yan.Liu@microsoft.com

Abstract

This paper is concerned with the statistical consistency of ranking methods. Re-
cently, it was proven that many commonly used pairwise ranking methods are
inconsistent with the weighted pairwise disagreement loss (WPDL), which can
be viewed as the true loss of ranking, even in a low-noise setting. This result
is interesting but also surprising, given that the pairwise ranking methods have
been shown very effective in practice. In this paper, we argue that the aforemen-
tioned result might not be conclusive, depending on what kind of assumptions
are used. We give a new assumption that the labels of objects to rank lie in a
rank-differentiable probability space (RDPS), and prove that the pairwise ranking
methods become consistent with WPDL under this assumption. What is especial-
ly inspiring is that RDPS is actually not stronger than but similar to the low-noise
setting. Our studies provide theoretical justiﬁcations of some empirical ﬁndings
on pairwise ranking methods that are unexplained before, which bridge the gap
between theory and applications.

1 Introduction

Ranking is a central problem in many applications, such as document retrieval, meta search, and
collaborative ﬁltering. In recent years, machine learning technologies called ‘learning to rank’ have
been successfully applied. A learning-to-rank process can be described as follows. In training, a
number of sets (queries) of objects (documents) are given and within each set the objects are labeled
by assessors, mainly based on multi-level ratings. The target of learning is to create a model that
provides a ranking over the objects that best respects the observed labels. In testing, given a new set
of objects, the trained model is applied to generate a ranked list of the objects.
Ideally, the learning process should be guided by minimizing a true loss such as the weighted pair-
wise disagreement loss (WPDL) [11], which encodes people’s knowledge on ranking evaluation.
However, the minimization can be very difﬁcult due to the nonconvexity of the true loss. Alterna-
tively, many learning-to-rank methods minimize surrogate loss functions. For example, RankSVM
[14], RankBoost [12], and RankNet [3] minimize the hinge loss, the exponential loss, and the cross-
entropy loss, respectively.
In machine learning, statistical consistency is regarded as a desired property of a learning method
[1, 21, 20], which reveals the statistical connection between a surrogate loss function and the true
loss. Statistical consistency in the context of ranking have been actively studied in recent years

1

[8, 9, 19, 11, 2, 18]. According to the studies in [11], many existing pairwise ranking methods
are, surprisingly, inconsistent with WPDL, even in a low-noise setting. However, as we know, the
pairwise ranking methods have been shown to work very well in practice, and have been regarded
as state-of-the-art even today [15, 16, 17]. For example, the experimental results in [2] show that
a weighted preorder loss in RankSVM [4] can outperform a consistent surrogate loss in terms of
NDCG (See Table 2 in [2]).
The contradiction between theory and application inspires us to revisit the statistical consistency of
pairwise ranking methods. In particular, we will study whether there exists a new assumption on the
probability space that can make statistical consistency naturally hold, and how this new assumption
compares with the low-noise setting used in [11].
To perform our study, we ﬁrst derive a sufﬁcient condition for statistical consistency of ranking
methods called rank-consistency, which is in nature very similar to edge-consistency in [11] and
order-preserving in [2]. Then we give an assumption on the probability space where ratings (labels)
of objects come from, which we call a rank-differentiable probability space (RDPS). Intuitively,
RDPS reveals the reason why an object (denoted as object A) should be ranked higher than another
object (denoted as object B). That is, the probability of any ratings consistent with the preference1 is
larger than that of its dual ratings (obtained by exchanging the labels of object A and object B while
keeping others unchanged). We then prove that with the RDPS assumption, the weighted pairwise
surrogate loss, which is a generalization of many surrogate loss functions used in existing pairwise
ranking methods (e.g., the preorder loss in RankSVM [2], the exponential loss in RankBoost [12],
and the logistic loss in RankNet [3]), is statistically consistent with WPDL.
Please note that our theoretical result contradicts the result obtained in [11], mainly due to the
different assumptions used. What is interesting, and to some extent inspiring, is that our RDPS
assumption is not stronger than the low-noise setting used in [11], and in some sense they are very
similar to each other (although they focus on different aspects of the probability space). We then
conducted detailed comparisons between them to gain more insights on what affects the consistency
of ranking.
According to our theoretical analysis, we argue that it is not yet appropriate to draw any conclusion
about the inconsistency of pairwise ranking methods, especially because it is hard to know what
the probability space really is. In this sense, we think the pairwise ranking methods are still good
choices for real ranking applications, due to their good empirical performances.
The rest of this paper is organized as follows. Sections 2 deﬁnes the consistency problem formally
and provides a sufﬁcient condition under which consistency with WPDL is achieved for ranking
methods. Section 3 gives the main theoretical results, including formal deﬁnition of RDPS and
conditions of statistical consistency of pairwise ranking methods. Further discussions on whether
RDPS is a strong assumption and why our results contradict with that in [11] are presented in Section
4. Conclusions are presented in Section 5.

2 Preliminaries of Statistical Consistency
Let x = {x1 , · · · , xm} be a set of objects to be ranked. Suppose the labels of the objects are given
as multi-level ratings r = (r1 , · · · , rm ) from space R, where ri denotes the label of xi . Without
loss of generality, we adopt K-level ratings used in [7], that is, ri ∈ {0, 1, · · · , K − 1}. If ri > rj , xi
should be ranked higher than xj . Assume that (x, r) is a random variable of space X × R according
to a probability measure P . Following existing literature, let f be a ranking function that gives a
score to each object to produce a ranked list and denote F as the space of all ranking functions.
∑
∑
In this paper, we adopt the weighted pairwise disagreement loss (WPDL) deﬁned in [11, 10] as the
true loss to evaluate f :
ij 1{(cid:11)i<(cid:11)j } ,
ij 1{(cid:11)i≤(cid:11)j } +
(1)
aG
aG
l0 (α, G) =
i>j
i<j
where α = (α1 , · · · , αm ) = (f (x1 ), · · · , f (xm )), G is a directed acyclic graph (DAG for short)
with edge i → j to represent the preference that xi should be ranked higher than xj , and aG
ij is a
non-negative penalty indexed by i → j on graph G.
1Here, consistency with the preference means that the rating of object A is larger than that of object B.

2

(2)

l0 (f ; x, r) =

Speciﬁcally, in the setting of multi-level ratings, i → j is constructed between pair (i, j ) with
∑
ij is thus just relevant to the labels of the two objects. For ease of representation2 , we
ri > rj , and aG
replace aG
ij with D(ri , rj ), and WPDL becomes the following form:
D(ri , rj )1{f (xi )−f (xj )≤0} ,
i;j :ri>rj
where 1{·} is an indicator function3 and D(ri , rj ) is a weight function s.t. (1) ∀ri ̸= rj , D(ri , rj ) >
0; (2) ∀ri , rj , D(ri , rj ) = D(rj , ri ); (3) ∀ri < rj < rk , D(ri , rj ) ≤ D(ri , rk ), D(rj , rk ) ≤ D(ri , rk ).
∑
The conditional expected true risk and the expected true risk of f are then deﬁned as:
R0 (f |x) = Er|x l0 (f ; x, r) =
l0 (f ; x, r)P (r|x), R0 (f ) = Ex [Er|x l0 (f ; x, r)].
r∈R
Due to the nonconvexity of the true loss, it is infeasible to minimize the true risk in Eq.(3). As is
∑
done in the literature of machine learning, we adopt a surrogate loss l(cid:8) to minimize in place of l0 .
The conditional expected surrogate risk and the expected surrogate risk of f are then deﬁned as:
R(cid:8) (f |x) = Er|x l(cid:8) (f ; x, r) =
l(cid:8) (f ; x, r)P (r|x), R(cid:8) (f ) = Ex [Er|x l(cid:8) (f ; x, r)].
r∈R
Statistical consistency is a desired property for a good surrogate loss, which measures whether the
expected true risk of the ranking function obtained by minimizing a surrogate loss converges to the
expected true risk of the optimal ranking in the large sample limit.
Deﬁnition 1. We say a ranking method that minimizes a surrogate loss l(cid:8) is statistically consistent
with respect to the true loss l0 , if ∀ϵ1 > 0, ∃ϵ2 > 0, such that for any ranking function f ∈ F ,
R(cid:8) (f ) ≤ inf h∈F R(cid:8) (h) + ϵ2 implies R0 (f ) ≤ inf h∈F R0 (h) + ϵ1 .
We then introduce a property of the surrogate loss, called rank-consistency, which is a sufﬁcient
condition for the statistical consistency of the surrogate loss, as indicated by Theorem 1.
Deﬁnition 2. We say a surrogate loss l(cid:8) is rank-consistent with the true loss l0 , if ∀x, for any
ranking function f ∈ F such that R0 (f |x) > inf h∈F R0 (h|x), the following inequality holds:
h∈F R(cid:8) (h|x) < inf {R(cid:8) (g |x) : g ∈ F , g(xi ) ≤ g(xj ), for (i, j ) where f (xi ) ≤ f (xj ).}.
inf
Rank-consistency can be viewed as a generalization of inﬁnite sample consistency for classiﬁcation
proposed in [20] (also referred to as ‘classiﬁcation-calibrated’ in [1]) to ranking on a set of objects.
It is also similar to edge-consistent in [11] and order-preserving in [2].
Theorem 1. If a surrogate loss l(cid:8) is rank-consistent with the true loss l0 on the function space F ,
then it is statistically consistent with the true loss l0 on F .
We omit the proof since it is a straightforward extension of the proof for Theorem 3 in [20]. The
proof is also similar to Lemma 3, 4, 5 and Theorem 6 in [11].

(3)

(4)

(5)

3 Main Results

In this section, we present our main theoretical results: with a new assumption on the probability
space, many commonly used pairwise ranking algorithms can be proved consistent with WPDL.

3.1 A Rank-Differentiable Probability Space

First, we give a new assumption named a rank-differentiable probability space (RDPS for short),
with which many pairwise ranking methods will be rank-consistent with WPDL. Hereafter, we will
also refer to data from RDPS as having a rank-differentiable property.

2Here we do not distinguish i > j and i < j , because they are just introduced to avoid minor technical
issues as stated in [11]. Furthermore, it will not inﬂuence the consistency results.
3 1A = 1,if A is true and 1A = 0,if A is false.

3

Before introducing the deﬁnition of RDPS, we give two deﬁnitions, an equivalence class of ratings
and dual ratings. Intuitively, we say two ratings are equivalent if they induce the same ranking
or preference relationships. And we say two ratings are the dual ratings with respect to a pair of
objects, if the two ratings just exchange the ratings of the two objects while keeping the ratings of
other objects unchanged. The formal deﬁnitions are given as follows.
Deﬁnition 3. A ratings r is called equivalent to ~r, denoted as r ∼ ~r, if P (r) = P (~r). Where
P (r) = {(i, j ) : ri > rj .} and P (~r) = {(i, j ) : ~ri > ~rj .} stand for the preference relationships
induced by r and ~r, respectively. Therefore, an equivalence class of the ratings r, denoted as [r], is
deﬁned as the set of ratings which are equivalent to r. That is, [r] = {~r ∈ R : ~r ∼ r.}.
Deﬁnition 4. Let R(i, j ) = {r ∈ R : ri > rj .}, r
′ is called the dual ratings of r ∈ R(i, j ) with
k = rk , ∀k ̸= i, j.
′
′
′
respect to (i, j ) if r
j = ri , r
i = rj , r
Now we give the deﬁnition of RDPS. An intuitive explanation on this deﬁnition is that there exists
a unique equivalence class of ratings that for each induced pairwise preference relationship, the
probability will be able to separate the two dual ratings with respect to that pair.
Deﬁnition 5. Let R(i, j ) = {r ∈ R : ri > rj .}, a probability space is called rank-differentiable
with (i, j ), if for any r ∈ R(i, j ), P (r|x) ≥ P (r
′ |x), and there exists at least one ratings r ∈
R(i, j ), s.t. P (r|x) > P (r
′ |x), where r
′ is the dual ratings of r.
Deﬁnition 6. A probability space is called rank-differentiable, if there exists an equivalence class
[r∗], s.t. P (r
) = {(i, j ) : the probability space is rank-differentiable with(i, j ).}, where P (r
∗
∗
) =
j .}. We will also call this probability space a RDPS or rank-differentiable with [r
{(i, j ) : r
∗
∗
∗
].
i > r
∗
Please note that [r
] in Deﬁnition 6 is unique, which can be directly proved by Deﬁnition 3.
Deﬁnition 5 implies that if a probability space is rank-differentiable with (i, j ), the optimal ranking
function will rank xi higher than xj , as shown in the following theorem. The proof is similar to
that of Theorem 4, thus we omit it here for space limitation. Hereafter, we will call this property
‘separability on pairs’.
∗ |x) = inf f ∈F R0 (f |x).
∗ ∈ F be an optimal ranking function that R0 (f
Theorem 2. ∀x ∈ X , let f
∗
∗
If the probability space is rank-differentiable with (i, j ), we have f
(xj ).
(xi ) > f

Further considering the ‘transitivity4 over pairs’ of a ranking function, Deﬁnition 6 implies that if a
∗
probability space is rank-differentiable with [r
], the optimal ranking function will induce the same
preference relationships, as shown in the following theorem.
∗ |x) = inf f ∈F R0 (f |x).
∗ ∈ F be an optimal ranking function that R0 (f
Theorem 3. ∀x ∈ X , let f
], for any (i, j ) ∈ P (r
∗
∗
∗
If the probability space is rank-differentiable with [r
), we have f
(xi ) >
(xj ), where P (r
) = {(i, j ) : r
j .}.
∗
∗
∗
∗
f
i > r
3.2 Conditions of Statistical Consistency
∑
With RDPS as the new assumption, we study the statistical consistency of pairwise ranking methods.
First, we deﬁne the weighted pairwise surrogate loss as
D(ri , rj )ϕ(f (xi ) − f (xj )),
l(cid:8) (f ; x, r) =
i;j :ri>rj
where ϕ is a convex function. The surrogate losses used in many existing pairwise ranking methods
can be regarded as special cases of this weighted pairwise surrogate loss, such as the hinge loss in
RankSVM [14], the exponential loss in RankBoost [12], the cross-entropy loss in RankNet [3] and
the preorder loss in [2]. For the weighted pairwise surrogate loss, we get its sufﬁcient condition
of statistical consistency as shown in Theorem 5. In order to prove this theorem, we ﬁrst prove
Theorem 4.
∗
Theorem 4. We assume the probability space is rank-differentiable with an equivalence class [r
].
Suppose that ϕ(·) : R → R in the weighted pairwise surrogate loss is a non-increasing function
such that ϕ(z ) < ϕ(−z ), ∀z > 0. ∀x ∈ X , let f ∈ F be a ranking function such that R(cid:8) (f |x) =
4Transitivity means that if xi is ranked higher than xj and xj is ranked higher than xk , xi must be ranked
higher than xk .

(6)

4

′ |x)]

′ |x)]

′ |x)]

′

(xi ) =

′ |x)]

inf h∈F R(cid:8) (h|x), then for any object pair (xi , xj ), r
j , we have f (xi ) ≥ f (xj ). Moreover, if
∗
∗
i > r
ϕ(·) is differentiable and ϕ
′
(0) < 0, we have f (xi ) > f (xj ).
′ as the function such that f
Proof. (1) We assume that f (xi ) < f (xj ), and deﬁne f
(xk ) = f (xk ), ∀k ̸= i, j . We can then get the following equation,
′
′
∑
∑
f (xj ), f
(xj ) = f (xi ), f
′ |x) − R(cid:8) (f |x)
R(cid:8) (f
[D(rk , rj )−D(rk , ri )][ϕ(f (xk )−f (xi ))−ϕ(f (xk )−f (xj ))][P (r|x)−P (r
∑
∑
′
k:rj <ri<rk
r;r
;
r∈R(i;j )
D(ri , rk )[ϕ(f (xj )−f (xk ))−ϕ(f (xi )−f (xk ))][P (r|x)−P (r
∑
∑
+
′
k:rj <rk<ri
r;r
;
r∈R(i;j )
D(rk , rj )[ϕ(f (xk )−f (xi ))−ϕ(f (xk )−f (xj ))][P (r|x)−P (r
∑
∑
+
′
k:rj <rk<ri
r;r
;
r∈R(i;j )
[D(ri , rk )−D(rj , rk )][ϕ(f (xj )−f (xk ))−ϕ(f (xi )−f (xk ))][P (r|x)−P (r
∑
+
′
k:rk<rj <ri
r;r
;
r∈R(i;j )
+[ϕ(f (xj )−f (xi ))−ϕ(f (xi )−f (xj ))]
D(ri , rj )[P (r|x)−P (r
′
r;r
;
r∈R(i;j )
∑
According to the conditions of RDPS, the requirements of the weight function D in Section 2 and
the assumption that ϕ is a non-increasing function such that ϕ(z ) < ϕ(−z ), ∀z > 0, we can obtain
′ |x) − R(cid:8) (f |x) ≤ [ϕ(f (xj )−f (xi ))−ϕ(f (xi )−f (xj ))]
D(ri , rj )[P (r|x)−P (r
′ |x)] < 0.
R(cid:8) (f
′
r;r
;
r∈R(i;j )
This is a contradiction with R(cid:8) (f )=inf h∈F R(cid:8) (h|x). Therefore, we have proven that f (xi )≥f (xj ).
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
(2) Now we assume that f (xi ) = f (xj ) = f0 . From the assumption R(cid:8) (f |x) = inf h∈F R(cid:8) (h|x), we
∑
∑
can get @R(cid:8) (f |x)
= 0, @R(cid:8) (f |x)
= 0. Accordingly, we can obtain the two following equations:
@ f (xi )
@ f (xj )
f0
f0
A1P (r|x) + A2P (r
′ |x) = 0,
B1P (r|x) + B2P (r
′ |x) = 0,
′
′
∑
∑
r;r
r;r
;
;
r∈R(i;j )
r∈R(i;j )
(f (xk )− f0 )] +
D(rk , ri )[−ϕ
∑
′
′
D(ri , rk )ϕ
A1 = B2 =
k:rj <rk<ri
k:rj <ri<rk
(f0 − f (xk )) + D(ri , rj )ϕ
′
∑
∑
D(ri , rk )ϕ
+
k:rk<rj <ri
(f (xk )− f0 )] +
D(rk , rj )[−ϕ
D(rk , rj )[−ϕ
∑
A2 = B1 =
k:rj <ri<rk
k:rj <rk<ri
(f0 − f (xk )) + D(ri , rj )[−ϕ
+
D(rj , rk )ϕ
∑
k:rk<rj <ri
(0) < 0, based on the requirements of RDPS and the weight function D , we can get
(A1 − B1 )P (r|x) + (A2 − B2 )P (r
′ |x)
∑
∑
′
r;r
;
r∈R(i;j )
(A1 − A2 )[P (r|x) − P (r
D(ri , rj )[P (r|x) − P (r
(0)
′
′
r;r
r;r
;
;
r∈R(i;j )
r∈R(i;j )
This is a contradiction with Eq.(7). Therefore, we actually have proven that f (xi ) > f (xj ).

(f (xk )− f0 )]

(f0 − f (xk ))

′ |x)] ≤ 2ϕ

′

′ |x)] < 0.

′

(0)].

′

(0).

′

′
If ϕ

=

′ |x)]

(7)

=

where,

′

′

5

Figure 1: Relationships among order-preserving, rank-differentiable and low-noise.

Theorem 5. Let ϕ(·) be a non-negative, non-increasing and differentiable function such that
′
(0) < 0. Then the weighted pairwise surrogate loss is consistent with WPDL under the as-
ϕ
sumption of RDPS.

∗

].

Proof. We assume that the probability space is rank-differentiable with an equivalence class [r
∗
∗
j , we are going to prove that
Then for any object pair (xi , xj ), r
i > r
h∈F R(cid:8) (h|x) < inf {R(cid:8) (f |x) : f ∈ F , f (xi ) ≤ f (xj ).}
∗
(cid:8)|x = inf
R
because from Theorem 3 this implies the rank-consistency condition in Eq.(5) holds.
Suppose Eq.(8) is not true, then we can ﬁnd a sequence of functions {fm } such that 0 = fm (xi ) ≤
fm (xj ), and limm R(cid:8) (fm |x) = R
∗
(cid:8)|x . We can further select a subsequence such that for each pair
(i, j ), fm (xi ) − fm (xj ) converges (may also converge to ±∞). This leads to a limit function f ,
with properly deﬁned f (xi ) − f (xj ), even when either f (xi ) or f (xj ) is ±∞. This implies that
R(cid:8) (f |x) = R
(cid:8)|x and 0 = f (xi ) ≤ f (xj ). However, this violates Theorem 4. Thus, Eq.(8) is true.
∗
Therefore, we have proven that the weighted pairwise surrogate loss is consistent with WPDL.

(8)

Many commonly used pairwise surrogate losses, such as the preorder loss in RankSVM [2], the
exponential loss in RankBoost [12], and the logistic loss in RankNet[3], satisfy the conditions in
Theorem 5, thus they are consistent with WPDL. In other words, we have shown that statistical
consistency of pairwise ranking methods is achieved under the assumption of RDPS.

4 Discussions

In Section 3, we have shown that statistical consistency of pairwise ranking methods is achieved
with the assumption of RDPS. Considering the contradicting conclusion drawn in [11], a natural
question is whether the RDPS is stronger than the low-noise setting used in [11]. In this section we
will make some discussions on this issue.

4.1 Relationships of RDPS with Previous Work

Here, we discuss the relationships between the rank-differentiable property and the assumptions used
in some previous works (including the order-preserving property in [19] and the low-noise setting
in [11]). According to our analysis, we ﬁnd that the rank-differentiable property is not a strong
assumption on the probability space. Actually, it is a weaker assumption than the order-preserving
property and is very similar to the low-noise setting. A sketch map of the relationships between the
three assumptions is presented in Figure 1, where the low-noise probability spaces stands for a set
of spaces satisfying the low-noise setting. Detailed discussions are given as follows.

6

1. Rank-Differentiable vs. Order-Preserving

The rank-differentiable property is deﬁned on the space of multi-level ratings while the order-
preserving property is deﬁned on the permutation space. To understand their relationship, we need
to put them onto the same space. Actually, we can restrict the space of multi-level ratings to the
permutation space by setting K = m − 1 and requiring the ratings of each two objects to be differ-
ent. After doing so, it is not difﬁcult to see that the rank-differentiable property is weaker than the
order-preserving property, as shown in the following theorem.
Theorem 6. Let K = m − 1. For each permutation y ∈ Y , where y(i) stands for the position of
1 , · · · , ry
i = m − y(i), i = 1, · · · , n. Assume
xi , deﬁne the corresponding ratings ry = (ry
m ) as ry
that P (ry ) = P (y), and P (r) = 0 if there does not exist a permutation y s.t. r = ry . If the
probability space is order-preserving with respect to m−1 pairs (j1 , j2 ), (j2 , j3 ),· · ·, (jm−1 , jm ),
, i = 1, · · · , m, but the
∗
∗
∗
it is rank-differentiable with the equivalence class [r
], where r
> r
ji
ji+1
converse is not always true.

2. Rank-Differentiable vs. Low-Noise

The rank-differentiable property is deﬁned on the space of multi-level ratings while the low-noise
setting is deﬁned on the space of DAGs. According to the correspondence between ratings and
DAGs (as stated in Section 2), we can restrict the space of DAGs to the space of multi-level ratings.
Consequently, we obtain the relationship between the rank-differentiable property and the low-noise
setting as follows:
(1) Mathematically, the inequalities in the low-noise setting can be viewed as the combinations of
the corresponding inequalities in the rank-differentiable property. They are similar to each other in
their forms and the rank-differentiable property is not stronger than the low-noise setting.
(2) Intuitively, the rank-differentiable property induces ‘separability on pairs’ and ‘transitivity over
pairs’ as described in Theorem 2 and 3, while the low-noise setting aims to explicitly express the
transitivity over pairs, but fails in achieving it.
Let us use an example to illustrate the above points. Suppose there are three objects to be ranked in
the setting of three-level ratings (K = 3). Furthermore, suppose that the ratings of every two objects
are different and all the graphs are fully connected DAGs in the setting of [11]. We order the ratings
and DAGs as:

r1 = (2, 1, 0), r2 = (1, 2, 0), r3 = (2, 0, 1), r4 = (0, 2, 1), r5 = (1, 0, 2), r6 = (0, 1, 2).
G1 = {(1 → 2),(2 → 3),(1 → 3)}, G2 = {(2 → 1),(1 → 3),(2 → 3)}, G3 = {(1 → 3),(3 → 2),(1 → 2)},
G4 = {(2 → 3),(3 → 1),(2 → 1)}, G5 = {(3 → 1),(1 → 2),(3 → 2)}, G6 = {(3 → 2),(2 → 1),(3 → 1)},
Therefore ri , Gi have one-to-one correspondence, we can set the probability as P (ri |x) =
P (Gi |x) = Pi and deﬁne aGi
kl = D(rik , ril ), i = 1, · · · , 6; k , l = 1, 2, 3.
Considering conditions in the deﬁnition of RDPS, rank-differentiable with [r1 ] requires the follow-
ing inequalities to hold and at least one inequalities in (9) and (10) to hold strictly.
P1 − P2 ≥ 0, P3 − P4 ≥ 0, P5 − P6 ≥ 0,
(9)
P4 − P6 ≥ 0, P2 − P5 ≥ 0, P1 − P3 ≥ 0,
(10)
We assume there are edges 1 → 2 and 2 → 3 in the difference graph. Then the low-noise setting in
Deﬁnition 8 of [11] requires that a13 − a31 ≥ a12 − a21 + a23 − a32 , where,
a12 − a21 = D(2, 1)(P1 − P2 ) + D(2, 0)(P3 − P4 ) + D(1, 0)(P5 − P6 ),
a23 − a32 = D(2, 1)(P4 − P6 ) + D(2, 0)(P2 − P5 ) + D(1, 0)(P1 − P3 ),
a13 − a31 = D(2, 1)(P3 − P5 ) + D(2, 0)(P1 − P6 ) + D(1, 0)(P2 − P4 ).
According to the above example,
(1) a12 − a21 and a23 − a32 are exactly the combinations of the terms in (9) and (10), respectively.
Thus, if the probability space is rank-differentiable with [r1 ], we can only get a12 − a21 > 0, a23 −
a32 > 0, but not the inequalities in the low-noise setting. This indicates that our rank-differentiable
property is not stronger than the low-noise setting.

7

(2) With the assumption that aij − aj i > 0 can guarantee the optimal ranking with which xi is
ranked higher than xj , it seems that the low-noise setting intends to make the preferences of 1 → 2
and 2 → 3 transitive to 1 → 3. However, the assumption is not always true. Instead, the rank-
differentiable property can naturally induce the ‘transitivity over pairs’ (See Theorem 2 and 3).
In this sense, the rank-differentiable property is much more powerful than the low-noise setting,
although not stronger.

4.2 Explanation on Theoretical Contradiction

On one hand, different conclusions on the consistency of pairwise ranking methods have been ob-
tained in our work and in [11]. On the other hand, we have shown that there exists an connection
between the rank-differentiable property and the low-noise setting (see Figure 1). Therefore, one
may get confused by the contradicting results and may wonder what will happen if a probability
space satisﬁes both the rank-differentiable property and the low-noise setting. In this subsection, we
will make discussions on this issue.
Please note that we adopt the multi-level ratings as the labeling strategy (as stated clearly in Section
2) in our analysis. With this setting, the graph space G in [11] will not contain all the DAGs. For
example, considering a three-graph case, the graph G2 = {(1, 2, 3) : (2 → 3), (3 → 1)} in the proof
of Theorem 11 of [11] (the main negative result on the consistency of pairwise surrogate losses)
actually does not exist. That is because if 2 → 3 and 3 → 1 exist in a graph G, we can get that
r2 > r3 , r3 > r1 according to the correspondence between graphs and ratings as stated in Section 2.
Therefore, we can immediately get r2 > r1 . Once again according to the correspondence between
graphs and ratings, we will get that 2 → 1 should be contained in graph G, which contradicts with
G2 . Thus, G2 will not exist in the setting of multi-level ratings. However, in the proof of [11], they
do not take the constraint of multi-level ratings into consideration, thus deduce contradict results.
From the above discussions, we can see that our theoretical results contradict with that in [11] mainly
because the two works consider different settings and assumptions. If a probability space satisﬁes
both the rank-differentiable property and the low-noise setting, the pairwise ranking methods will be
consistent with WPDL in the setting of multi-level ratings but inconsistent in the setting of DAGs.
One may argue that the setting of multi-level ratings is not as general as the DAG setting, however,
please note that multi-level ratings are the dominant setting in the literature of ‘learning to rank’
[13, 16, 15, 6] and have been widely used in many applications such as web search and document
retrieval [17, 5]. Therefore, we think the setting of multi-level ratings is general enough and our
result has its value to the mainstream research of learning to rank.
To sum up, based on all the discussions in this paper, we argue that it is not yet appropriate to draw
any conclusion about the inconsistency of pairwise ranking methods, especially because it is hard to
know what the probability space really is. In this sense, we think the pairwise ranking methods are
still good choices for real ranking applications, due to their good empirical performances.

5 Conclusions

In this paper, we have discussed the statistical consistency of ranking methods. Speciﬁcally, we
argue that the previous results on the inconsistency of commonly-used pairwise ranking methods
are not conclusive, depending on the assumptions about the probability space. We then propose a
new assumption, which we call a rank-differentiable probability space (RDPS), and prove that the
pairwise ranking methods are consistent with the same true loss as in previous studies under this
assumption. We show that RDPS is not a stronger assumption than the assumptions used in previous
work, indicating that our ﬁnding is similarly reliable to previous ones.

Acknowledgments

This research work was funded by the National Natural Science Foundation of China under Grant
No. 60933005, No. 61173008, No. 61003166 , No. 61203298 and 973 Program of China under
Grants No. 2012CB316303.

8

References
[1] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, 2006.
[2] D. Buffoni, C. Calauzenes, P. Gallinari, and N. Usunier. Learning scoring functions with
order-preserving losses and standardized supervision. In Proceedings of the 28th International
Conference on Machine Learning (ICML 2011), pages 825–832, 2011.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.
Learning to rank using gradient descent. In Proceedings of the 22th International Conference
on Machine Learning (ICML 2005), pages 89–96, 2005.
[4] O. Chapelle. Training a support vector machine in the primal. Neural Computation, 19:1155–
1178, 2007.
[5] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. Journal of Machine
Learning Research - Proceedings Track, 14:1–24, 2011.
[6] O. Chapelle, Y. Chang, and T.-Y. Liu. Future directions in learning to rank. Journal of Machine
Learning Research - Proceedings Track, 14:91–100, 2011.
[7] W. Chen, T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. Ranking measures and loss functions in learning
to rank. In 24th Annual Conference on Neural Information Processing Systems (NIPS 2009),
pages 315–323, 2009.
[8] S. Cl ´emenc¸ on, G. Lugosi, and N. Vayatis. Ranking and scoring using empirical risk minimiza-
tion. In Proceedings of the 18th Annual Conference on Learning Theory (COLT 2005), volume
3559, pages 1–15, 2005.
[9] D. Cossock and T. Zhang. Subset ranking using regression. In Proceedings of the 19th Annual
Conference on Learning Theory (COLT 2006), pages 605–619, 2006.
[10] O. Dekel, C. D. Manning, and Y. Singer. Log-linear models for label ranking. In 18th Annual
Conference on Neural Information Processing Systems (NIPS 2003), 2003.
[11] J. C. Duchi, L. W. Mackey, and M. I. Jordan. On the consistency of ranking algorithms. In
Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pages
327–334, 2010.
[12] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining
preferences. Journal of Machine Learning Research, 4:933–969, 2003.
[13] R. Herbrich, K. Obermayer, and T. Graepel. Large margin rank boundaries for ordinal regres-
sion. In Advances in Large Margin Classiﬁers., pages 115–132, 1999.
[14] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
2002), pages 133–142, 2002.
[15] H. Li, T.-Y. Liu, and C. Zhai. Learning to rank for information retrieval (lr4ir 2008). SIGIR
Forum, 42:76–79, 2008.
[16] T.-Y. Liu. Learning to rank for information retrieval. Foundation and Trends on Information
Retrieval, 3:225–331, 2009.
[17] T.-Y. Liu, J. Xu, T. Qin, W.-Y. Xiong, and H. Li. Letor: Benchmark dataset for research
on learning to rank for information retrieval. In SIGIR ’07 Workshop, San Francisco, 2007.
Morgan Kaufmann.
[18] P. D. Ravikumar, A. Tewari, and E. Yang. On ndcg consistency of listwise ranking methods.
Journal of Machine Learning Research - Proceedings Track, 15:618–626, 2011.
[19] F. Xia, T.-Y. Liu, J. Wang, W. S. Zhang, and H. Li. Listwise approach to learning to rank
In Proceedings of the 25th International Conference on Machine
- theory and algorithm.
Learning (ICML 2008), 2008.
[20] T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.
Journal of Machine Learning Research, 5:1225–1251, 2004.
[21] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk
minimization. Annuals of Statistics, 32:56–85, 2004.

9


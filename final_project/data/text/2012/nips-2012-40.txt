Transelliptical Component Analysis

Fang Han
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21210
fhan@jhsph.edu

Han Liu
Department of Operations Research
and Financial Engineering
Princeton University, NJ 08544
hanliu@princeton.edu

Abstract

We propose a high dimensional semiparametric scale-invariant principle compo-
nent analysis, named TCA, by utilize the natural connection between the ellipti-
cal distribution family and the principal component analysis. Elliptical distribu-
tion family includes many well-known multivariate distributions like multivari-
ate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al
TCA can obtain a near-optimal splog d/n estimation consistency rate in recover-
(2002) using the copula techniques. In this paper we extend the meta-elliptical
distribution family to a even larger family, called transelliptical. We prove that
ing the leading eigenvector of the latent generalized correlation matrix under the
transelliptical distribution family, even if the distributions are very heavy-tailed,
have inﬁnite second moments, do not have densities and possess arbitrarily con-
tinuous marginal distributions. A feature selection result with explicit rate is also
provided. TCA is further implemented in both numerical simulations and large-
scale stock data to illustrate its empirical usefulness. Both theories and experi-
ments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and
robustness at almost no cost.

1 Introduction
Given x1 , . . . , xn ∈ Rd as n i.i.d realizations of a random vector X ∈ Rd with population co-
m leading eigenvectors bu1 , . . . , bum of the Pearson sample covariance matrix are obtained as the
variance matrix Σ and correlation matrix Σ0 , the Principal Component Analysis (PCA) aims at
recovering the top m leading eigenvectors u1 , . . . , um of Σ. In practice, Σ is unknown and the top
estimators. However, because the PCA is well-known to be scale-variant, meaning that changing
ing eigenvectors θ1 , . . . , θm of Σ0 using the top m leading eigenvectors bθ1 , . . . , bθm of the Pearson
the measurement scale of variables will make the estimators different, the PCA conducted on the
sample correlation matrix is also regular in literatures [2]. It aims at recovering the top m lead-
sample correlation matrix. Because Σ0 is scale-invariant, we call the PCA aiming at recovering the
In high dimensional settings, when d scales with n, it has been discussed in [14] that bu1 and bθ1
eigenvectors of Σ0 the scale-invariant PCA.
angle between v1 and v2 by ∠(v1 , v2 ). [14] proved that ∠(u1 , bu1 ) and ∠(θ1 , bθ1 ) do not converge
are generally not consistent estimators of u1 and θ1 . For any two vectors v1 , v2 ∈ Rd , denote the
to zero. Therefore, it is commonly assumed that θ1 = (θ11 , . . . , θ1d )T is sparse, meaning that
card(supp(θ1 )) := card({θ1j : θ1j 6= 0}) = s < n. This results in a variety of sparse PCA
procedures. Here we note that supp(uj ) = supp(θj ), for j = 1, . . . , d.
The elliptical distributions are of special interest in Principal Component Analysis. The study of
elliptical distributions and their extensions have been launched in statistics recently by [4]. The
elliptical distributions can be characterized by their stochastic representations [5]. A random vector
Z = (Z1 , . . . , Zd )T is said to follow an elliptical distribution or be elliptically distributed with
parameters µ, Σ (cid:23) 0, and rank(Σ) = q , if it admits the stochastic representation: Z = µ + ξAU ,
where µ ∈ Rd , ξ ∈ R and U ∈ Rq are independent random variables, ξ ≥ 0, U is uniformly
distributed on the unit sphere in Rq , and A ∈ Rd×q is a ﬁxed matrix such that AAT = Σ. We call

1

ξ the generating variable. The density of Z does not necessarily exist. Elliptical distribution family
includes a variety of famous multivariate distributions: multivariate Gaussian, multivariate Cauchy,
Student’s t, logistic, Kotz, symmetric Pearson type-II and type-VII distributions. We refer to [3, 5]
and [4] for more details.
[4] introduce the term meta-elliptical distribution in extending the continuous elliptical distributions
whose densities exist to a wider class of distributions with densities existing. The construction of
the meta-elliptical distributions is based on the copula technique and it was initially introduced by
[25]. In particular, when the latent elliptical distribution is the multivariate Gaussian, we have the
meta-Gaussian or the nonparanormal distributions introduced by [16] and [19].
The elliptical distribution is of special interest in Principal Component Analysis (PCA). It has been
shown in a variety of literatures [27, 11, 22, 12, 24] that the PCA conducted on elliptical distributions
shares a number of good properties enjoyed by the PCA conducted on the Gaussian distribution. In
particular, [11] show that with regard to a range of hypothesis relevant to PCA, tests based on a mul-
tivariate Gaussian assumption have the identical power for all elliptical distributions even without
second moments. We will utilize this connection to construct a new model in this paper.
In this paper, a new high dimensional scale-invariant principle component analysis approach is pro-
posed, named Transelliptical Component Analysis (TCA). Firstly, to achieve both the estimation
accuracy and model ﬂexibility, we build the model of TCA on the transelliptical distributions. A
random vector X = (X1 , . . . , Xd )T is said to follow a transelliptical distribution if there exists a set
of univariate strictly monotone functions f = {fj }d
j=1 such that f (X ) := (f1 (X1 ), . . . , fd (Xd ))T
jk ] (cid:23) 0. Here
follows a continuous elliptical distribution with parameters µ = 0 and Σ0 = [Σ0
diag(Σ0 ) = 1. Transelliptical distributions do not necessarily possess densities and are strict exten-
sions to the meta-elliptical distributions deﬁned in [4]. TCA aims at recovering the top m leading
eigenvectors θ1 , . . . , θm of Σ0 .
{ bfj }d
Secondly, to estimate Σ0 robustly and efﬁciently, instead of estimating the transformation functions
j=1 of {fj }d
j=1 as [19] did, realizing that {fj }d
j=1 preserve the ranks of the data, we utilize the
continuous, Kendall’s tau correlation matrix approximates Σ0 in a parametric rate OP (plog d/n).
nonparametric rank-based correlation coefﬁcient estimator, Kendall’s tau, to estimate Σ0 . We prove
that even though the generating variable ξ is changing and marginal distributions are arbitrarily
This key observation makes Kendall’s tau a better estimator than Pearson sample correlation matrix
with regard to a much larger distribution family than the Gaussian.
tain the TCA estimator eθ∗
Thirdly, in terms of methodology and theory, we analyze the general case that X follows a
transelliptical distribution and θ1 is sparse. Here θ1 is the leading eigenvector of Σ0 . We ob-
sin ∠(θ1 , eθ∞ ) = OP (splog d/n), where eθ∞ is the estimator TCA obtains. A feature selection
1 of θ1 utilizing the Kendall’s tau correlation matrix. We prove that
the TCA can obtain a fast convergence rate in terms of parameter estimation and is of the rate
consistency result with explicit rate is also provided.
2 Background
We start with notations: Let M = [Mjk ] ∈ Rd×d and v = (v1 , ..., vd )T ∈ Rd . Let v ’s subvector
with entries indexed by I be denoted by vI , M ’s submatrix with rows indexed by I and columns
indexed by J be denoted by MI J . Let MI · and M·J be the submatrix of M with rows in I and all
columns, and the submatrix of M with columns in J and all rows. For 0 < q < ∞, we deﬁne the
dX
‘0 , ‘q and ‘∞ vector norm as
kvk0 := card(supp(v)), kvkq := (
|vi |q )1/q and kvk∞ := max
1≤i≤d
Pn
i=1
We deﬁne the matrix ‘max norm as the elementwise maximum value: kM kmax := max{|Mij |} and
j=1 |Mij |. Let Λj (M ) be the toppest j−th eigenvalue
the ‘∞ norm as kM k∞ := max1≤i≤m
of M. In special, Λmin (M ) := Λd (M ) and Λmax (M ) := Λ1 (M ) are the smallest and largest
eigenvalues of M . The vectorized matrix of M , denoted by vec(M ), is deﬁned as: vec(M ) :=
(M T·1 , . . . , M T·d )T . Let Sd−1 := {v ∈ Rd : kvk2 = 1} be the d-dimensional unit sphere. The
sign =d denotes that the two sides of the equality have the same distributions. For any two vectors
a, b ∈ Rd and any two squared matrices A, B ∈ Rd×d , denote the inner product of a and b, A and

|vi |.

2

B by

ha, bi := aT b and hA, B i := Tr(AT B ).

2.1 Elliptical and Transelliptical Distributions
This section is devoted to a brief discussion of elliptical and transelliptical distributions.
In the
sequel, to be clear, a random vector X = (X1 , . . . , Xd )T is said to be continuous if the marginal
distribution functions are all continuous.
2.1.1 Elliptical Distributions
In this section we shall ﬁrstly provide a deﬁnition of the elliptical distributions following [5].
Deﬁnition 2.1. Given µ ∈ Rd and Σ ∈ Rd×d , where rank(Σ) = q ≤ d, a random vector Z =
(Z1 , . . . , Zd )T is said to have an elliptical distribution or is elliptically distributed with parameters µ
and Σ, if and only if Z has a stochastic representation: Z =d µ + ξAU , where µ ∈ Rd , A ∈ Rd×q ,
AAT = Σ, ξ ≥ 0 is a random variable independent of U , U ∈ Sq−1 is uniformly distributed in the
unit sphere in Rq . In this setting we denote by Z ∼ ECd (µ, Σ, ξ ).
A random variable in R with continuous marginal distribution function does not necessarily possess
density. A well-known set of examples is the cantor distribution, whose support set is the cantor set.
We refer to [7] for more discussions on this phenomenon. Σ is symmetric and positive semi-deﬁnite,
but not necessarily to be positive deﬁnite.
Proposition 2.1. A random vector Z = (Z1 , . . . , Zd )T has the stochastic representation Z ∼
ECd (µ, Σ, ξ ), if and only if Z has the characteristic function exp(it0µ)φ(t0Σt), where φ is a
|Σ|−1/2 g (cid:0)(z − µ)T Σ−1 (z − µ)(cid:1) , where g : [0, ∞) → [0, ∞). We denote by Z ∼ ECd (µ, Σ, g).
properly-deﬁned characteristic function. We denote by X ∼ ECd (µ, Σ, φ).
If ξ is absolutely
continuous and Σ is non-singular, then the density of Z exists and is of the form: pZ (z ) =
A proof can be found in page 42 of [5]. When the density exists, ξ , φ and g are uniquely determined
by one of the other. The relationship among ξ , φ and g are described in Theorem 2.2 and Theorem
2.9 of [5]. The next proposition states that Σ, φ, ξ and A are not unique.
Proposition 2.2 (Theorem 2.15 of [5]). (i) If Z = µ + ξAU and Z = µ∗ + ξ ∗A∗U ∗ , where
A ∈ Rd×q and A∗ ∈ Rd×q , Z is continuous, then there exists a constant c > 0 such that
ξ ∗ = c−1/2 ξ . (ii) If Z ∼ ECd (µ, Σ, φ) and Z ∼ ECd (µ∗ , Σ∗ , φ∗ ),
µ∗ = µ, A∗A∗T = cAAT ,
Z is continuous, then there exists a constant c > 0 such that µ∗ = µ, Σ∗ = cΣ, φ∗ (·) = φ(c−1 ·).
The next proposition discusses the cases where (µ, Σ, ξ ) is identiﬁable for Z .
Proposition 2.3. If Z ∼ ECd (µ, Σ, ξ ) is continuous with rank(Σ) = q , then (1) P(ξ = 0) = 0;
(2)Σii > 0 for i ∈ {1, . . . , d}; (3)(µ, Σ, ξ ) is identiﬁable for Z under the constraint that
jk = Σjk /pΣj j Σkk to be the generalized correlation matrix of Z . Σ0
max(diag(Σ)) = 1.
We deﬁne Σ0 = [Σ0
jk ] with Σ0
is the correlation matrix of Z when Z ’s second moment exists and still reﬂects the rank dependency
even when Z has inﬁnite second moment [13].
2.1.2 Transelliptical Distributions
To extend the elliptical distribution, we ﬁrstly deﬁne two sets of symmetric matrices: R+
d = {Σ ∈
Rd×d : ΣT = Σ, diag(Σ) = 1, Σ (cid:31) 0}; Rd = {Σ ∈ Rd×d : ΣT = Σ, diag(Σ) = 1, Σ (cid:23) 0}.
Deﬁnition 2.2. A random vector X = (X1 , . . . , Xd )T with continuous marginal distribution func-
tions F1 , . . . , Fd and density existing is said to follow a meta-elliptical distribution if and only if
there exists a continuous elliptically distributed random vector Z ∼ ECd (0, Σ0 , g) with the marginal
distribution function Qg and Σ0 ∈ R+
d , such that (Q−1
g (F1 (X1 )), . . . , Q−1
g (Fd (Xd )))T =d Z.
In this paper, we generalize the meta-elliptical distribution family to a broader class, named the
transelliptical. The transelliptical distributions do not assume that densities exist for both X and Z
and are therefore strict extensions to meta-elliptical distributions.
Deﬁnition 2.3. A random vector X = (X1 , . . . , Xd )T is said to follow a transelliptical distribu-
tion if and only if there exists a set of strictly monotone functions f = {fj }d
j=1 and a latent
continuous elliptically distributed random vector Z ∼ ECd (0, Σ0 , ξ ) with Σ0 ∈ Rd , such that
(f1 (X1 ), . . . , fd (Xd ))T =d Z. We call such X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) and Σ0 the latent gener-
alized correlation matrix.

3

Proposition 2.4. If X follows a meta-elliptical distribution, in other words, X possesses den-
sity and has continuous marginal distributions F1 , . . . , Fd of X and a continuous random vec-
tor Z ∼ ECd (0, Σ0 , g) such that (Q−1
g (F1 (X1 )), . . . , Q−1
g (Fd (Xd )))T =d Z , then we have
X ∼ T Ed (Σ0 , ξ ; Q−1
g (F1 ), . . . , Q−1
g (Fd )).
To be more clear, the transelliptical distribution family is strictly larger than the meta-elliptical
distribution family in three senses: (i) the generating variable ξ of the latent elliptical distribution is
not necessarily absolute continuous in transelliptical distributions; (ii) the parameter Σ0 is strictly
enlarged from R+
d to Rd ; (iii) the marginal distributions of X do not necessarily possess densities.
The term meta-Gaussian (or the nonparanormal) is introduced by [16, 19]. The term meta-elliptical
copula is introduced in [6]. This is actually an alternative deﬁnition of the meta-elliptical distribu-
tion. The term elliptical copula is introduced in [18]. In summary,
transelliptical ⊃ meta-elliptical = meta-elliptical copula ⊃ elliptical* ⊃ elliptical copula,
transelliptical ⊃ meta-Gaussian = nonparanormal.
Here elliptical* represents the elliptical distributions which are continuous and possess densities.

2.2 Latent Correlation Matrix Estimation for Transelliptical Distributions
We ﬁrstly study the correlation and covariance matrices of elliptical distributions. Given Z ∼
ECd (µ, Σ, ξ ), we ﬁrst explore the relationship between the moments of Z and µ and Σ.
Proposition 2.5. Given Z ∼ ECd (µ, Σ, ξ ) with rank(Σ) = q and ﬁnite second moments and Σ0 the
generalized correlation matrix of Z , we have E(Z ) = µ, Var(Z ) = E(ξ2 )
q Σ, and Cor(Z ) = Σ0 .
When the random vector is elliptically distributed with second moment ﬁnite, the sample mean and
correlation matrices are element-wise consistent estimators of µ and Σ0 . However, the elliptical
distributions are generally very heavy-tailed (multivariate t or Cauchy distributions for example),
making Pearson sample correlation matrix a bad estimator. When the distribution family is extended
to the transelliptical, the Pearson sample correlation matrix is generally no longer a element-wise
consistent estimator of Σ0 . A similar “plug-in” idea as [6] works when ξ is known. In the general
case when ξ is unknown, the “plug-in” idea itself is unavailable.
3 The TCA
eigenvectors of Σ0 . Firstly, we estimate the Kendall’s tau correlation matrix bR. Secondly, we plug
In this section we propose the TCA approach. TCA is a two-stage method in estimating the leading
bR into a sparse PCA algorithm.
3.1 Rank-based Measures of Associations
The main idea of the TCA is to exploit the Kendall’s tau statistic to estimate the generalized cor-
relation matrix Σ0 efﬁciently and robustly. In detail, let X = (X1 , . . . , Xd )T be a d−dimensional
random vector with marginal distributions F1 , . . . , Fd and the joint distributions Fjk for the pair
(Xj , Xk ). The population Spearman’s rho and Kendall’s tau correlation coefﬁcients are given by
τ (Xj , Xk ) = P((Xj − eXj )(Xk − eXk ) > 0) − P((Xj − eXj )(Xk − eXk ) < 0),
ρ(Xj , Xk ) = Corr(Fj (Xj ), Fk (Xk )),
where ( eXj , eXk ) is a independent copy of (Xj , Xk ).
In particular, for Kendall’s tau, we have
jk given X ∼
the following theorem, which states an explicit relationship between τjk and Σ0
T Ed (Σ0 , ξ ; f1 , . . . , fd ), no matter what the generating variable ξ is. This is a strict extension to
[4]’s result on the meta-elliptical distribution family.
(cid:17)
(cid:16) π
Theorem 3.1. Given X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) transelliptically distributed, we have
2 τ (Xj , Xk )
Σ0
jk = sin
Remark 3.1. Although the conclusion in Theorem 3.1 of [4] is correct, the proof provided is wrong
or at least very ambiguous. Theorem 2.22 in [5] builds the result only for one sample statistic and
cannot be generalized to the statistic of multiple samples, like the Kendall’s tau or Spearman’s rho.
Therefore, we provide a new and clear version here. Detailed proofs can be found in the long version
of this paper [8].

(3.1)

.

4

Spearman’s rho depends not only on Σ but also on the generating variable ξ . When X follows mul-
jk /2). On the other hand, when X ∼
tivariate Gaussian, [17] proves that: ρ(Xj , Xk ) = 6
π arcsin(Σ0
) − 4( arcsin Σ0
T Ed (Σ0 , ξ ; f1 , . . . , fd ) with ξ =d 1, [10] proves that: ρ(Xj , Xk ) = 3( arcsin Σ0
)3 .
jk
jk
π
π
In estimating τ (Xj , Xk ),
let x1 , . . . , xn be n independent realizations of X , where xi =

X
(xi1 , . . . , xid )T . We consider the following rank-based statistic:
bτjk =
2
sign (xij − xi0 j ) (xik − xi0 k ) ,
n(n − 1)
bτjk = 1,
1≤i<i0≤n
if j = k .
tau correlation matrix bR = [ bRjk ] such that bRjk = sin (cid:0) π
(cid:1) .
2 bτjk
to approximate τ (Xj , Xk ) and measure the association between Xj and Xk . We deﬁne the Kendall’s
3.2 Methods

if j 6= k

(3.2)

The elliptical distribution is of special interest in Principal Component Analysis (PCA). It has been
shown in a variety of literatures [27, 11, 22, 12, 24] that the PCA conducted on elliptical distributions
share a number of good properties enjoyed by the PCA conducted on the Gaussian distribution. We
will utilize this connection to construct a new model in this paper.

(3.3)

3.2.1 TCA Model
Utilizing the natural relationship between elliptical distributions and the PCA, we propose the model
of Transelliptical Component Analysis (TCA). Here ideas of transelliptical distribution family and
scale-invariant PCA are exploited. We wish to estimate the leading eigenvector of the latent gener-
(cid:26) X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ),
alized correlation matrix. In particular, the following model Md (Σ0 , ξ , s; f ) with f = {fj }d
j=1 is
considered:
Md (Σ0 , ξ , s; f ) :
kθ1 k0 = s,
in estimating. By spectral decomposition, we write: Σ0 = Pd
where θ1 is the leading eigenvectors of the latent generalized correlation matrix Σ0 we are interested
d , where λ1 ≥ λ2 ≥
j=1 λd θd θT
. . . ≥ λd ≥ 0 and λ1 > 0 to make Σ0 non-degenerate. θ1 , . . . , θd ∈ Sd−1 are the corresponding
Inspired by the model Md (Σ0 , ξ , s; f ), it is natural to consider the
eigenvectors of λ1 , . . . , λd .
following optimization problem:eθ∗
vT bRv ,
1 = arg max
v∈Rd
sub ject to v ∈ Sd−1 ∩ B0 (s),
where B0 (s) := {v ∈ Rd : kvk0 ≤ s} and bR is the estimated Kendall’s tau correlation matrix. The
(3.4)
corresponding global optimum is denoted by eθ∗
1 .
Generally we can plug in the Kendall’s tau correlation matrix bR to any sparse PCA algorithm listed
3.2.2 TCA Algorithm
above. In this paper, to approximate θ1 , we consider using the Truncated Power method (TPower)
version of this paper [8]. The ﬁnal estimator is denoted by eθ∞ with keθ∞k0 = k . It will be shown
proposed by [28] and [20]. The main idea of the TPower is to utilize the power method, but truncate
the vector to a ‘0 ball with radius k in each iteration. Detailed algorithms are provided in the long
in Section 4 and Section 5 that the Kendall’s tau correlation matrix is a better statistic in estimating
the correlation matrix than the Pearson sample correlation matrix in the sense that (i) it enjoys the
Gaussian parametric rate in a much larger distribution family, including many distributions with
heavy tails; (ii) it is a more robust estimator, i.e. resistant to outliers.
following the discussions of [21, 15, 28, 29]. In detail, a matrix bΓ ∈ Rd×s deﬂates a vector v ∈ Rd
We use the iterative deﬂation method to learn the ﬁrst k instead of the ﬁrst one leading eigenvectors,
and achieves a new matrix bΓ0 : bΓ0 := (I − vvT )bΓ(I − vvT ). In this way, bΓ0 is orthogonal to v .
5

4 Theoretical Properties

In this section the theoretical properties of the TCA estimators are provided. Especially, we are
interested in the high dimensional case when d > n.
This section is devoted to the concentration result of the Kendall sample correlation matrix bR to the
4.1 Rank-based Correlation Matrix Estimation
Pearson correlation matrix Σ0 . The ‘max convergence rate of bR is provided in the next theorem.
letting bR be the Kendall tau correlation matrix, we have with probability at least 1 − d−5/2 ,
Theorem 4.1. Given x1 , . . . , xn n independent realizations of X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) and
k bR − Σ0 kmax ≤ 3πplog d/n.
(4.1)
Proof sketch. Theorem 4.1 can be proved by realizing that bτjk is an unbiased estimator of τ (Xj , Xk )
and is a U-statistic with size 2. Hoeffding’s inequality for U-statistic can then be applied to obtain
the result. Detailed proofs can be found in the long version of this paper [8].

then we have

4.2 TCA Estimators
of the TCA global optimum eθ∗
1 and TPower solver eθ∞ . We assume that the Model Md (Σ0 , ξ , s; f )
This section is devoted to the statement of our main result on the upper bound of the estimated error
eigenvector eθ∗
holds and the next theorem provides an upper bound on the angle between the estimated leading
Theorem 4.2. Let eθ∗
1 and true leading eigenvector θ1 .
1 be the global solution to Equation (3.4) and the Model Md (Σ0 , ξ , s; f ) holds.
q
For any two vectors v1 ∈ Sd−1 and v2 ∈ Sd−1 , letting
| sin ∠(v1 , v2 )| =
1 − (vT
1 v2 )2 ,
!
 
r log d
| sin ∠(eθ∗
1 , θ1 )| ≤ 6π
≥ 1 − d−5/2 .
· s
P
(4.2)
λ1 − λ2
n
Proof sketch. The key idea of the proof is to utilize the ‘max norm convergence result of bR to Σ0 .
Generally, when s and λ1 , λ2 do not scale with (n, d), the rate is OP (plog d/n), which is the
Detailed proofs can be found in the long version of this paper [8].
q log d
parametric rate [20, 26, 23] obtains. When (n, d) goes to inﬁnity, the two leading eigenvalues λ1
and λ2 will typically go to inﬁnity and will at least be away from zero. Hence, our rate shown in
· s
Corollary 4.1 (Feature Selection Consistency of the TCA). Let eθ∗
Theorem 4.2 will be usually better than the seemingly more common rate: 6πλ1
n .
λ1−λ2
1 be the global solution to
Equation (3.4) and the Model Md (Σ0 , ξ , s; f ) holds. Let
Θ := supp(θ1 ) and bΘ∗ := supp(eθ∗
1 ).
r log d
√
If we further have
|θ1j | ≥ 6
2π
· s
min
then we have, P( bΘ∗ = Θ) ≥ 1 − d−5/2 .
λ1 − λ2
j∈Θ
n
Proof sketch. The key of the proof is to construct a contradiction given Theorem 4.2 and the condi-
tion on the minimum value of |θ1 |. Detailed proofs can be found in the long version of this paper
[8].

,

6

5 Experiments

In this section we investigate the empirical performance of the TCA method. We utilize the TPower
algorithm proposed by [28] and the following three methods are considered: (1) Pearson:
the
classic high dimensional scale-invariant PCA using the Pearson sample correlation matrix of the
data; (2) Kendall: the TCA using the Kendall correlation matrix; (3) LatPearson: the classic high
dimensional scale-invariant PCA using the Pearson sample correlation matrix of the data drawn from
the latent elliptical distribution (perfect without data contamination).

5.1 Numerical Simulations

θ1j =

.

(5.1)

.

(5.2)

and θ2j =

and u2j =

In the simulation study we randomly sample n data points from a certain transelliptical distribution
T Ed (Σ0 , ξ ; f1 , . . . , fd ). Here we consider the set up of d = 100. To determine the transelliptical
ing eigenvectors are pre-speciﬁed to be sparse. In detail, let Σ = Pd
distribution, ﬁrstly, we derive Σ0 in the following way: A covariance matrix Σ is ﬁrstly synthesized
through the eigenvalue decomposition, where the ﬁrst two eigenvalues are given and the correspond-
j , where ω1 =
j=1 ωj uj uT
(cid:26) 1√
(cid:26) 1√
6, ω2 = 3, ω3 = . . . = ωd = 1, and the ﬁrst two leading eigenvectors of Σ, u1 and u2 , are sparse
with the ﬁrst s = 10 entries of u1 and the second s = 10 entries of u2 are nonzero, i.e.
11 ≤ j ≤ 20
1 ≤ j ≤ 10
u1j =
10
10
0
otherwise
0
otherwise
(cid:26) − 1√
(cid:26) − 1√
The remaining eigenvectors are chosen arbitrarily. The generalized correlation matrix Σ0 is gener-
ated from Σ, with λ1 = 4, λ2 = 2.5, λ3 , . . . , λd ≤ 1 and the top two leading eigenvectors sparse:
11 ≤ j ≤ 20
1 ≤ j ≤ 10
10
10
0
otherwise
0
otherwise
pY 2
Secondly, using Σ0 , we consider the following three generating schemes:
[Scheme 1] X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) with ξ ∼ χd and f1 (x) = . . . = fd (x) = x. Here
d ∼ χd with Y1 , . . . , Yd ∼i.i.d N (0, 1). In other words, χd is the chi-distribution
1 + . . . + Y 2
with degree of freedom d. This is equivalent to say that X ∼ N (0, Σ0 ) (Example 2.4 of [5]).
[Scheme 2] X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) with ξ =d √
1 /ξ ∗
mξ ∗
2 and f1 (x) = . . . = fd (x) = x.
1 ∼ χd , ξ ∗
2 ∼ χm , ξ ∗
2 and m ∈ N. This is equivalent to say that
Here ξ ∗
1 is independent of ξ ∗
X ∼ M td (m, 0, Σ0 ), i.e. X following a multivariate-t distribution with degree of freedom m, mean
0 and covariance matrix Σ0 (Example 2.5 of [5]). Here we consider m = 3.
[Scheme 3] X ∼ T Ed (Σ0 , ξ ; f1 , . . . , fd ) with ξ =d √
2 ∼ χm , ξ ∗
1 ∼ χd , ξ ∗
2 . Here ξ ∗
1 /ξ ∗
mξ ∗
1 is in-
2 and m = 3. Moreover, {f1 , . . . , fd} = {h1 , h2 , h3 , h4 , h5 , h1 , h2 , h3 , h4 , h5 , . . .},
dependent of ξ ∗
Φ(x) − R Φ(t)φ(t)dt
where
qR |t|φ(t)dt
qR (cid:0)Φ(y) − R Φ(t)φ(t)dt(cid:1)2
sign(x)|x|1/2
1 (x) := x, h−1
h−1
, h−1
2 (x) :=
3 (x) :=
exp(x) − R exp(t)φ(t)dt
qR (cid:0)exp(y) − R exp(t)φ(t)dt(cid:1)2
x3qR t6φ(t)dt
, h−1
5 (x) :=
This is equivalent to say that X is transelliptically distributed with the latent elliptical distribution
Z ∼ M td (3, 0, Σ0 ).
To evaluate the robustness of different methods, let r ∈ [0, 1) represent the proportion of samples
being contaminated. For each dimension, we randomly select bnrc entries and replace them with
either 5 or -5 with equal probability. The ﬁnal data matrix we obtained is X ∈ Rn×d . Here we
pick r = 0, 0.02 or 0.05. Under the Scheme 1 to Scheme 3 with different levels of contamination
(r = 0, 0.02 or 0.05), we repeatedly generate the data matrix X for 1,000 times and compute
the averaged False Positive Rates and False Negative Rates using a path of tuning parameters k
from 5 to 90. The feature selection performances of different methods are then evaluated by plotting
(FPR(k), 1 − FNR(k)). The corresponding ROC curves are presented in Figure 1 (A). More results
are shown in the long version of this paper [8]. It can be observed that Kendall is generally better
and more resistance to the outliers compared with Pearson.

h−1
4 (x) :=

,

.

φ(y)dy

φ(y)dy

7

(B)
(A)
Figure 1: (A) ROC curves under Scheme 1, Scheme 2 and Scheme 3 (top, middle, bottom) and data
contamination at different levels (r = 0, 0.02, 0.05 from left to right). x−axis is FPR and y−axis
is TPR. Here n = 100 and d = 100. (B) Successful matches of the market trend proportions only
using the stocks in Ak and Bk . The x−axis represents the tuning parameter k scaling from 1 to 200;
the y−axis represents the % of successful matches. The curve denoted by ’Kendall’ represents the
points of (k , ρAk ) and the curves denoted by ’Pearson’ represents the points of (k , ρBk ).

5.2 Equities Data
In this section we apply the TCA on the stock price data from Yahoo! Finance (finance.yahoo.
com). We collected the daily closing prices for J=452 stocks that were consistently in the S&P 500
index between January 1, 2003 through January 1, 2008. This gave us altogether T=1,257 data
points, each data point corresponds to the vector of closing prices on a trading day. Let S t = [S tt,j ]
denote by the closing price of stock j on day t.
eθK endall and eθP earson using the tuning parameter k ∈ N. Let Ak := supp(eθK endall ) and Bk :=
We wish to evaluate the ability of using the only k stocks to represent the trend of the whole stock
market. To this end, we run Kendall and Pearson on S t and obtain the leading eigenvectors
supp(eθP earson ). And then we let T W
, T Ak
and T Bk
denote by the trend of the whole stocks, Ak
S tt,j − X
:= I ( X
S tt,j − X
:= I (X
stocks and Bk stocks in tth day compared with t − 1th date, i.e:
t
t
t
S tt−1,j >), T Ak
T W
t
t
S tt,j − X
:= I ( X
j∈Ak
j∈Ak
j
j
S tt−1,j > 0),
P
j∈Bk
j∈Bk
P
here I is the indicator function. In this way, we can calculate the proportion of successful matches
) and ρBk :=
t = T Ak
t I (T W
of the market trend using the stocks in Ak and Bk as: ρAk := 1
t
T
t = T Bk
t I (T W
). We visualize the result by plotting (k , ρAk ) and (k , ρBk ) on a 2D ﬁgure. The
1
t
T
result is presented in Figure 1 (B).
P
It can be observed from Figure 1 (B) that Kendall summarizes the trend of the whole stock market
constantly better than Pearson. Moreover, the averaged difference between the two methods are
k (ρAk − ρBk ) = 1.4025 with the standard deviation 0.6743. Therefore, the difference is
1
200
signiﬁcant.

S tt−1,j > 0)

and

T Bk
t

6 Acknowledgement
This research was supported by NSF award IIS-1116730.

8

0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0.00.20.40.60.81.00.00.20.40.60.81.0PearsonKendallLatPearson0501001502007580859095kSuccessful Matches %PearsonKendallReferences
[1] TW Anderson.
Statistical inference in elliptically contoured and related distributions.
Recherche, 67:02, 1990.
[2] M.G. Borgognone, J. Bussi, and G. Hough. Principal component analysis in sensory analysis:
covariance or correlation matrix? Food quality and preference, 12(5-7):323–326, 2001.
[3] S. Cambanis, S. Huang, and G. Simons. On the theory of elliptically contoured distributions.
Journal of Multivariate Analysis, 11(3):368–385, 1981.
[4] H.B. Fang, K.T. Fang, and S. Kotz. The meta-elliptical distributions with given marginals.
Journal of Multivariate Analysis, 82(1):1–16, 2002.
[5] KT Fang, S. Kotz, and KW Ng. Symmetric multivariate and related distributions. Chap-
man&Hall, London, 1990.
[6] C. Genest, AC Favre, J. B ´eliveau, and C. Jacques. Metaelliptical copulas and their use in
frequency analysis of multivariate hydrological data. Water Resour. Res, 43(9):W09401, 2007.
[7] P.R. Halmos. Measure theory, volume 18. Springer, 1974.
[8] F. Han and H. Liu. Tca: Transelliptical principal component analysis for high dimensional
non-gaussian data. Technical Report, 2012.
[9] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, pages 13–30, 1963.
[10] H. Hult and F. Lindskog. Multivariate extremes, aggregation and dependence in elliptical
distributions. Advances in Applied probability, 34(3):587–608, 2002.
[11] DR Jensen. The structure of ellipsoidal distributions, ii. principal components. Biometrical
Journal, 28(3):363–369, 1986.
[12] DR Jensen. Conditioning and concentration of principal components. Australian Journal of
Statistics, 39(1):93–104, 1997.
[13] H. Joe. Multivariate models and dependence concepts, volume 73. Chapman & Hall/CRC,
1997.
[14] I.M. Johnstone and A.Y. Lu.
arXiv:0901.4392, 2009.
[15] M. Journ ´ee, Y. Nesterov, P. Richt ´arik, and R. Sepulchre. Generalized power method for sparse
principal component analysis. The Journal of Machine Learning Research, 11:517–553, 2010.
[16] KS Kelly and R. Krzysztofowicz. A bivariate meta-gaussian density for use in hydrology.
Stochastic Hydrology and Hydraulics, 11(1):17–31, 1997.
[17] W.H. Kruskal. Ordinal measures of association. Journal of the American Statistical Associa-
tion, pages 814–861, 1958.
[18] D. Kurowicka, J. Misiewicz, and RM Cooke. Elliptical copulae. In Proc of the International
Conference on Monte Carlo Simulation-Monte Carlo, pages 209–214, 2000.
[19] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high
dimensional undirected graphs. The Journal of Machine Learning Research, 10:2295–2328,
2009.
[20] Z. Ma. Sparse principal component analysis and iterative thresholding. Arxiv preprint
arXiv:1112.2432, 2011.
[21] L. Mackey. Deﬂation methods for sparse pca. Advances in neural information processing
systems, 21:1017–1024, 2009.
[22] G.P. McCabe. Principal variables. Technometrics, pages 137–144, 1984.
[23] D. Paul and I.M. Johnstone. Augmented sparse principal component analysis for high dimen-
sional data. Arxiv preprint arXiv:1202.1242, 2012.
[24] GQ Qian, G. Gabor, and RP Gupta. Principal components selection by the criterion of the
minimum mean difference of complexity. Journal of multivariate analysis, 49(1):55–75, 1994.
[25] A. Sklar. Fonctions de r ´epartition `a n dimensions et leurs marges. Publ. Inst. Statist. Univ.
Paris, 8(1):11, 1959.
[26] V.Q. Vu and J. Lei. Minimax rates of estimation for sparse pca in high dimensions. Arxiv
preprint arXiv:1202.0786, 2012.
[27] C.M. Waternaux. Principal components in the nonnormal case: The test of equality of q roots.
Journal of Multivariate Analysis, 14(3):323–335, 1984.
[28] X.T. Yuan and T. Zhang. Truncated power method for sparse eigenvalue problems. Arxiv
preprint arXiv:1112.2679, 2011.
[29] Y. Zhang, A. dAspremont, and L.E. Ghaoui. Sparse pca: Convex relaxations, algorithms and
applications. Handbook on Semideﬁnite, Conic and Polynomial Optimization, pages 915–940,
2012.

Sparse principal components analysis.

Arxiv preprint

9


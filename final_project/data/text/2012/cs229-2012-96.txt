CS 229 Project: Final Report
Incorporating Global Information into Local Upscaling
of Fluid Flow in Porous Media

Kasama Itthisawatpan and Perth (Pu) Charernwattanagul

December 14, 2012

1

Introduction

1.1 Background
Making an accurate forecast of hydrocarbon production
is one of the most important tasks in the upstream
petroleum industry. To accomplish such goal, petroleum
engineers use numerical simulator to model the ﬂuid
movement in the subsurface reservoir. The accurate pro-
duction forecast can be achieved only if the reservoir
model is accurate.
Reservoir simulation models that have ﬁner grid sizes
can generally provide more accurate results. However,
performing ﬁne-scale simulation is often not attainable
due to its high computational cost, and the simulation is
performed on the coarser models. The process that gen-
erates coarse-scale models from ﬁne-scale models while
trying to preserve the solution is called upscaling.
The pro ject focuses on the upscaling of the single-
phase ﬂow properties (only transmissibility), which is of-
ten performed almost exclusively in the petroleum indus-
try [2]. In particular, we tried to generate an accurate
upscaled transmissibility, which measures how much ﬂuid
can ﬂow between two simulation blocks given a pressure
diﬀerence, as illustrated in Figure 1.

around the interface of interest to compute the equiva-
lent coarse-scale transmissibility. It is often very quick
yet inaccurate.
On the other hand, global upscaling performs the full
ﬁne-scale single-phase simulation and use its results to
solve for eﬀective transmissibility. Global upscaling is
usually more accurate than local upscaling. However, it
is computationally expensive, as the full ﬁne-scale simu-
lation is required.

1.3 Pro ject Description

Advantages and disadvantages of local and global upscal-
ing form a motivation of the pro ject. It is desirable that
we use local upscaling for all geostatistical realizations.
However, due to the challenging geology, such accuracy is
not achieved, so some amount of global upscaling must be
used. Because ﬁne-scale models are generated by geosta-
tistical methods, there may be some similarities among
the realizations. We aimed to apply global upscaling to
a small percentage of the realizations. Then, we trained
machine learning models so that they can correct the lo-
cal upscaling results from other realizations.

Figure 1: Schematic diagram showing the ﬂow from
coarse block 1 (left) to coarse block 2 (right). Trans-
missibility is deﬁned as the ratio between ﬂow rate and
pressure drop T12 = q12
.
p1−p2

1.2 Local and Global Upscaling
The upscaling methods can be classiﬁed into local up-
scaling and global upscaling. The diﬀerence between lo-
cal and global upscaling is the amount of information
required in constructing the equivalent properties. For
transmissibility upscaling, local upscaling uses the areas

2 Data and Methods

2.1 Fine-Scale Model Generation

The data used in this study is collected from two stages.
First, we generate 2D ﬁne-scale properties by geostatis-
tical methods. We applied training image generator and
Sequential Gaussian Simulation in the software SGeMS
[6] to create the permeability distribution. The models
are representative of a reservoir deposited in the ﬂuvial
environment. The properties of each facies are similar to
those of the top layers of the standard Stanford V reser-
voir [5]. An example of rock permeability distribution is
shown in Figure 2.
It is generally known that this type of reservoir prop-
erties is very challenging for upscaling. Speciﬁcally, local
upscaling usually results in predicting ﬂow rate that is
too low given a speciﬁed pressure drop [8].

1

1. location (x, y) of the interface,
2. permeability map of the local domain, and
3. simpliﬁed permeability map of the extended domain
(12 coarse blocks around the interface).

Because the underestimation and overestimation of the
transmissibility values from local upscaling depend on
the distribution of low and high permeability in the do-
main instead of the actual permeability values, we dis-
(cid:40)
cretized the permeability to
1 pi ≥ ¯pDi
0 pi < ¯pDi

ˆpi =

where ¯pDi is the geometric mean of permeability in the
extended local domain. The attributes are illustrated in
Figure 4.

Figure 2: An example of permeability ﬁeld in log-scale
(log10 md) from one of the realizations used in this study.
Hot colors represent high permeability regions, where the
ﬂow rate is high.

2.2 Upscaling Implementation
For local upscaling, we applied the standard transmissi-
bility upscaling (see e.g. [2] for an explanation) with the
ﬁne-scale simulation result from MRST [4]. For global
upscaling, we used an iterative method as outlined in [3].
Fine-scale simulation and upscaling were performed on
all realizations to conﬁrm our expectation. The results
are shown as empirical CDF in Figure 3. It is seen that
global upscaling can accurately reproduce ﬁne-scale ﬂow
statistics, while local upscaling shows signiﬁcant under-
estimation of the ﬂow.

Figure 4: Processing of the attributes

All the attributes were scaled to [0, 1]. These attributes
are supplied directly to the classiﬁcation models with the
training data set from 40 realizations. The rest of the
realizations are used as the testing data set. For trans-
missibility prediction, we further simpliﬁed the attributes
e.g. taking averages of some groups of attributes to re-
duce the complexity of the model since we have smaller
training data sets.

2.4 Machine Learning Techniques
In this pro ject, we consider the results from global up-
scaling as the accurate results and tried to correct the
values from local upscaling. In other words, we want to
predict the factors βi of all coarse-block interfaces such
that

Tglobal,i = βiTlocal,i , or
log Tglobal,i = log βi + log Tlocal,i

Figure 5 shows the histogram of log βi .
The factors log βi indicate whether the local upscaling
results underestimate or overestimate the true transmis-
sibility. We applied machine learning techniques in two
stages: classiﬁcation and prediction.

2.4.1 Classiﬁcation
The goal of classiﬁcation is to identify the interfaces with
log βi (cid:29) 0, log βi ≈ 0, and log βi (cid:28) 0, which correspond

Figure 3: Empirical CDF from 100 realizations. The
coarse-scale simulation from local upscaling gives consis-
tently lower ﬂow rate than the ﬁne scale simulation.

We have 100 geostatistical realizations, from each of
which, 257 transmissibility values are obtained. In total,
we have 25700 data points for training and testing.

2.3 Feature Extraction

As discussed earlier, a ma jor diﬀerence between local up-
scaling and global upscaling is the amount of informa-
tion considered in the upscaling processes. While local
upscaling uses only the properties adjacent to the coarse-
block interface, global upscaling take the entire reservoir
properties, as well as the well locations, into account.
Therefore, the discrepancies between local upscaling re-
sults and global upscaling results should depend on the
global permeability ﬁeld that is not captured in the local
domain when local upscaling is performed. Therefore, we
considered the following properties as the attributes for
the model.

2

1
2

ξ (cid:63)
i sub ject to

θ = arg min
θl

sets to predict the correction factor log βi . The model
parameters are diﬀerent for diﬀerent groups.
Linear Regression. For each group l, we ﬁnd the pa-
l x(i)(cid:17)2
(cid:16)
ml(cid:88)
rameter θl such that
y (i) − θT
i=1
Support Vector Regression (SVR). Similar to sup-
port vector machines, SVR model can be constructed
ml(cid:88)
ml(cid:88)
from an optimization problem
(cid:107)w(cid:107)2 + C
1
min
ξi + C
2
w,b,ξ ,ξ(cid:63)
i=1
i=1
wT φ(x(i) ) + b − y (i) ≤  + ξi
y (i) − wT φ(x(i) ) − b ≤  + ξ (cid:63)
i
i ≥ 0
ξi , ξ (cid:63)
Because each model has to be constructed from the
training data in each group, the number of training data
points in the training set is smaller than the classiﬁcation
stage. To prevent against possible bias if the training
set is not suﬃcient, we also tried a simpler linear kernel
φ(x, z) = xT z in addition to the RBF kernel for the SVR
problem.
It is worthwhile to note that the approach is slightly
diﬀerent from the typical machine learning framework.
Earlier in the pro ject, we attempted to correct the lo-
cal upscaling results directly using linear regression and
support vector regression on the continuous-value log βi
and multiclass support vector machine on the discretized
log βi . However, as shown in Figure 5, the large ma jor-
ity of log βi are close to 0. However, the interfaces with
log βi very far from 0 are the most important contribu-
tors to the inaccuracy of local upscaling. Straightforward
prediction does not capture these outliers, so it turns out
to be an inaccurate correction.
It should also be noted that the number of realizations
used in the training data set is limited by practical ap-
plication. In reality, we do not wish to perform global
upscaling on too many realizations, as it is against the
initial goal of applying machine learning techniques to
reduce the need for global ﬁne-scale simulation. Around
100 to 200 realizations is a practical range for uncertainty
quantiﬁcation in the industry.

Figure 5: Histogram showing the statistics of log βi in
the training data set.

to the cases with transmissibility from local upscaling
being a signiﬁcant overestimation, a good approximate,
and a signiﬁcant overestimation, respectively. In partic-
−1 log βi < −a, (Underestimation)
ular, each interface in the training data set assigned the
category from the following rule:
−a ≤ log βi ≤ a, (Satisfactory Estimation)
0
1
log βi > a, (Overestimation)
where a is some threshold between 0.02 and 0.3.

y (i) =

Multiclass Support Vector Machines Using the set
of feature vectors, we applied SVM to classify the in-
terfaces. For three-class classiﬁcation, one-against-one
strategy is applied in selecting the most probable group.
The optimization problem for multiclass SVM is set up
m(cid:88)
as
1
ξ (t)
wT
min
ij wij + C
ij sub ject to
2
wij ,bij ,ξij
t=1
ij φ(x(t) ) + bij ≥ 1 − ξ (t)
wT
ij , y(t) = i
ij φ(x(t) ) + bij ≤ −1 + ξ (t)
wT
ij , y(t) = j
ij ≥ 0
ξ (t)
function (RBF) kernel φ(x, z) = exp (cid:0)−γ (cid:107)x − z(cid:107)2 (cid:1) gen-
From the experiment, we observe that the radial basis
erally gives better results than linear or polynomial ker-
nels, so we applied RBF kernel throughout the pro ject.
The library libSVM [1] is used for implementation.
We also studied the eﬀects of the parameters C and γ .
These parameters are adjusted so that we get a reason-
able result (C = 10, γ = 0.005).

k-means. We performed k-means clustering on the
training data set. The number of clusters ranges between
3 and 30 groups. After convergence, we study the char-
acteristics of each cluster by observing the distribution
of the correction factor βi .

2.4.2 Prediction
Once we classiﬁed the interfaces into groups, we con-
struct supervised learning models with reduced feature

3 Results
3.1 Classiﬁcation
Figure 6 plots an example of the classiﬁcation results
from support vector machines. Figure 7 shows how
varying the parameters a (threshold to assign actual cat-
egories) aﬀects the accuracy.
Although the classiﬁcation is not perfect, 60-70 per-
cent accuracy is suﬃcient for proceeding to the prediction

3

From the fact that support vector machines can per-
form much better with the RBF kernel than with the
simple linear kernel, it is seen that the correction fac-
tor can be separated only in the high dimensional space.
The results from k-means reinforces this observation, as
it fails to distinguish between the clusters in the primitive
feature space.

Figure 6: Classiﬁcation Result, Accuracy: 62.1%

3.2 Prediction

Figure 7: Accuracy of classiﬁcation with diﬀerent a

stage, as will be shown in the next section. The result
shown in Figure 7 suggests that it is important to choose
a reasonable values of the threshold a. For a > 0.1, larger
a leads to higher accuracy of classiﬁcation. This result is
aligned with the observations when the feature sets are
chosen (that the permeability distribution causes inaccu-
racy of the local upscaling results). Separation between
the very inaccurate results and the satisfactory results
is clearer when we deﬁne a stricter condition of deeming
the result to be very inaccurate (when a is large).
However, too large a, while having accurate classiﬁca-
tion result, will lead to too small set of interfaces being
classiﬁed as inaccurate, which will aﬀect the performance
of the next stage since the number of training data will
be too small. We choose a = 0.2 for the rest of the study.

Figure 8: Distribution of correction factors of each cluster
from k-means

An example of the results from k-means (with
nclusters = 10) is plotted in Figure 8. As the distribu-
tion of the correction factors from all clusters are very
similar, it can be concluded that the correction factors
cannot be diﬀerentiated by proximity of the feature vec-
tors alone. This general result applies when we range the
number of clusters from 3 to 30.

4

Figure 9: Comparing the prediction results from diﬀer-
ent groups and diﬀerent methods. Red dots show uncor-
rected Tlocal . Blue dots show the corrected T . Black lines
correspond to 1-1 prediction (perfect correction).

Figure 9 shows the results of the correction factor pre-
diction on diﬀerent groups by diﬀerent methods.
It is
shown that both linear regression and support vector
regression can slightly improve the transmissibility es-
timates from the values given by local upscaling as it is
seen that the predicted values cluster slightly closer to
the 1:1 line (which would correspond to perfect correc-
tion).
The performances of the predictions are measured as
mean absolute error and root mean square error between
the global upscaling result (treated as the correct value)
and the result after correction from diﬀerent methods in
the testing data set. The errors are shown in the follow-
ing table.

Method
No Correction
Linear Regression
SVR (Linear kernel)
SVR (RBF kernel)

Mean Abs. Err. RMS Err.
0.5118
0.3034
0.5018
0.2925
0.4890
0.2802
0.2741
0.4839

From the table, both linear regression and support
vector regression can slightly correct the transmissibil-
ity values from local upscaling. Comparison between the
methods shows that support vector machines with RBF

kernel gives the best correction, but the correction is still
small. From all the methods, the reduction of the mean
absolute error is very similar in magnitude to the reduc-
tion of the root mean square error. This suggests that
the extremes (data with large | log βi |) are not corrected.

4 Discussion and Conclusion
In this pro ject, we presented the potential uses and lim-
itations of an attempt to correct the eﬀective transmis-
sibility from local upscaling using the permeability dis-
tribution information. We have seen that in general, the
permeability distribution information can be used to pre-
dict whether the transmissbility from local upscaling will
be an underestimation or an overestimation (classiﬁca-
tion stage). For the prediction stage, linear regression
and support vector regression can slightly predict the cor-
rect values of the eﬀective transmissibility. However, the
correction is small and may not have a large eﬀect on the
ﬂow behavior. From the results, we make the following
observations:
• The simpliﬁed attributes applied in this pro ject can-
not capture the complex relationship between trans-
missibility from local and global upscaling. This is
seen during the pro ject that if the actual permeabil-
ity values are used as inputs instead of the processed
values, SVM gives perfect prediction on the train-
ing data set but inaccurate prediction on the testing
data set. This observation suggests that the model
has high variance, and the result may improve with
a more complex model.
• The extended domain considered in this pro ject is
not large enough to capture the general pattern of
the permeability distribution from diﬀerent geosta-
tistical realizations. The physical problem that gov-
erns the single-phase upscaling is an elliptic PDE,
so the property at a single point is aﬀected by all
points in the global domain. This pro ject assumes
that the extended local domain is large enough to
represent the entire reservoir. This assumption may
be invalid. Consideration of a larger domain may
enhance accuracy.
• The extreme values of transmissibility (the ones
whose results from local upscaling are highly incor-
rect) are still problematic. One possible remedy is to
increase the number of categories in the classiﬁcation
stages, so the extremes can be grouped together.
All the discussions above would implies the necessity
of a more complex model, as its degrees of freedom in-
creases, which would in turn demand for a larger training
data set. As discussed earlier, this may not be practical
as we aimed to limit the number of global upscaling runs
to reduce computational burden. However, it could be
more useful in a much larger reservoir model (with a
much larger number of coarse blocks), where many more
coarse block interfaces can provide more training data.
The success of this method on a larger model needs in-

vestigation.
Despite the imperfection of the prediction stage, this
pro ject provides an insight on the possibility of apply-
ing the machine learning techniques to aid the upscal-
ing problems. The classiﬁcation stage suggests that even
with the simpliﬁed attributes, the permeability distribu-
tion maps can be used to predict the ﬂuid ﬂow prop-
erties at least qualitatively. This ﬁnding can be useful
in other upscaling processes, especially those that only
focus on local domain such as relative permeability—a
multiphase parameter—upscaling, which is not consid-
ered in this pro ject. In fact, several attempts have been
successful in using k-means to aid local relative perme-
ability upscaling [7]. It is seen in this pro ject that SVM
can work better than k-means under the non-clustering
permeability distribution. It will be worth investigating
the use of SVM in relative permeability upscaling.

5 Acknowledgment
We would like to thank Professor Louis Durlofsky (En-
ergy Resources Engineering) for the motivation and guid-
ance for the pro ject. We also thank Matthieu Rousset
(Ph.D. student, Energy Resources Engineering) for pro-
viding the global upscaling code.

References

[1] C.-C. Chang and C.-J. Lin. LIBSVM: A library
for support vector machines. ACM Transactions
on Intel ligent Systems and Technology, 2:27:1–27:27,
2011. Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.
[2] Y. Chen. Upscaling and Subgrid Modeling of Flow and
Transport in Heterogeneous Reservoirs. PhD thesis,
Stanford University, 2005.
[3] Y. Chen, B. T. Mallison, and L. J. Durlofsky. Non-
linear two-point ﬂux approximation for modeling full-
tensor eﬀects in subsurface ﬂow simulations. Compu-
tational Geosciences, 2008.
[4] K. A. Lie, S. Krogstad, I. S. Ligaarden, J. R. Natvig,
H. M. Nilsen, , and B. Skaﬂestad. Open source matlab
implementation of consistent discretisations on com-
plex grids. Comp. Geo., 16(2):297–322, 2012.
[5] S. Mao and A. G. Journel. Generation of a refer-
ence petrophysical and seismic 3d data set. Technical
report, SCRF Report, Stanford Univ., 1999.
[6] N. Remy, J. Wu, and A. Boucher. Applied Geostatis-
tics with SGeMS: A User’s Guide. Cambridge Uni-
versity Press, 2009.
[7] S. Suzuki. Pattern-based approach to multiphase ﬂow
upscaling using distance-based clustering. In Proceed-
ing to SPE Annual Technical Conference and Exhibi-
tion, Denver, CO, USA, 2011.
[8] X. H. Wen, L. J. Durlofsky, and M. G. Edwards. Use
of border regions for improved permeability upscal-
ing. Math. Geol., 35:521–547, 2003.

5


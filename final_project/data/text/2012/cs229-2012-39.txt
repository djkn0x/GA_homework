Classifying the Sub jective: Determining Genre of Music From Lyrics

Ahmed Bou-Rabee

Keegan Go

Karanveer Mohan

December 14, 2012

Abstract

In this paper we address the area of multi-class clas-
siﬁcation of single-label music genres using lyrics.
Previous work [1] has achieved a maximum of 40%
accuracy, which we improve upon. To do so, we
implement and compare a collection of supervised
learning algorithms and an unsupervised algorithm
on a uniformly distributed random dataset, pro-
cessed from an online lyrics database to include
about 5,000 songs from 7 genres. We ﬁrst analyze
the performance of each algorithm after preprocess-
ing the data in various ways. We then discuss why
clustering algorithms such as k-means and kNN do
not perform very well in this setting. Overall we
found that the ensemble classiﬁer and multi-class
Naive Bayes provide the best accuracies.

1

Introduction

Classifying music into genres based on lyrics is an
interesting problem in the ﬁeld of Music Informa-
tion Retrieval that presents several challenges.
In
this paper we chose to use individual datasets with
hard labels, but in reality, classiﬁcations are sub jec-
tive: diﬀerent people may assign diﬀerent genres, or
even multiple genres, to the same song. Although
we chose individual datasets with hard labels, the
presence of such debate among classiﬁcations speaks
to a certain degree of ambiguity in the classiﬁcation
process. Songs in many genres may not have lyrics
as well.
Despite these diﬃculties, however, there is still
much potential for song lyric analysis. Focusing on
improving the prediction value of song lyrics alone
has allowed us to use music-calibrated state of the
art text classiﬁcation algorithms to improve upon
previously established benchmarks. Liang, Gu et
al., 2011 [1], had used the musiXmatch dataset as-
sociated with the Million Song Dataset [2], to clas-

sify songs into ten genres using bag-of-words lyrics
features, achieving 40% accuracy. In this paper, we
focus on improving lyrics-only classiﬁcation in hopes
that this eﬀort can help improve music classiﬁca-
tions by translating into even more accurate predic-
tions when other features are considered.

2 Gathering Data

Our ﬁrst attempt at this problem used song lyrics
from the recently released (2011) Million Song
Dataset [2]. The accuracy rates after running Naive
Bayes were quite low, similar to those observed by
Liang, Gu et al., 2011[1]. These authors noted that
the lyrics for the songs in the database were incom-
plete. We believed that a complete set of lyrics, un-
like what was used in other text-classiﬁcations, could
cause important changes in the performance of algo-
rithms on lyrics and hence proceeded to search for
a dataset of complete lyrics.
We then used a publicly available training set [3]
of complete music lyrics, with about 9,700 songs
and a total vocabulary of about 44,000 words. We
randomly chose 9/10th of the data for our training
set and used the rest for our test set, on which we
performed Naive Bayes, multi-class SVM, and kNN.
Our results were unexpected, with about 65% accu-
racy for L2-regularized multi-class SVM and Naive
Bayes, and about 60% accuracy for k-nearest neigh-
bors (kNN, k = 10). Upon further inspection, we
noticed that our dataset was highly skewed: 55%
of the songs were from the genre “classic pop and
rock.” In other words, if all of our test data had been
assigned to this category, we would have achieved a
prediction accuracy of above 50%.
In an attempt
to establish uniformity among the training data, we
tried to manually reconstruct our data to be uni-
form, training 300 random songs from each genre
and testing 50 diﬀerent random songs from each
genre. Unfortunately, our newly uniformed data was

1

too limited to provide any reliable results. To allow
a more accurate analysis of data, we decided to col-
lect our own data via web crawling that has built-in
checks to ensure a certain degree of uniformity. We
wrote a Python web crawler to obtain new data from
lyrics.wikia.com [4].
We took into account many factors when pooling
and processing our data. While parsing the data, we
ensured that only unique, English songs are consid-
ered. The genres we chose were the top 7 most popu-
lar music genres according to The Recording Indus-
try Association of America’s (RIAA) Consumer Pro-
ﬁle [5]. Using the crawler, we pooled 800 songs from
each of the following categories: blues, country, hip-
hop, pop, rap, R&B, and rock. We then parsed all
the lyrics to produce the vocabulary of the dataset.
Each song’s lyrics were converted into a multino-
mial event model representation that consists of a
list of indices of words in the vocabulary and a list
of counts for each index.
In addition, several pa-
pers mentioned the use of stemming in both reduc-
ing the size of the data and helping with accuracy
[6][7]. In light of this, we created both unstemmed
and stemmed versions of our data sets and compared
the results of our algorithms in each case. In order
to stem our data, we used the Porter stemming al-
gorithm [8], which coalesced words with common
stems and removed a standard list of stop words.
Our accuracy rates were similar for stemmed and
unstemmed datasets, with stemming helping only
to reduce the data size. Each document was repre-
sented by a matrix of 25,383 features corresponding
to counts of English words.

3 Results

In this section we describe the results of the algo-
rithms used and brieﬂy summarize algorithms not
covered in class. Each bar graph displays the accu-
racies of the algorithm for categories: 1.Rock 2.R&B
3.Country 4.Hip-hop 5.Blues 6.Rap 7.Pop.

3.1 Naive Bayes

We implemented the multinomial event version of
Naive Bayes in which we calculated the probability
of a given training example given each genre. Since
we had seven classes, we had to introduce a set of

Figure 1: Naive Bayes Categorical Accuracy

parameters for each class. Thus for l = 1, .., 7
φk|y=l = ￿m
i=1 ￿ni
j=1 1{x(i)
j = k ∧ y (i) = l} + 1
￿m
i=1 1{y (i) = l}ni + |V |
and
φy=l = ￿m
i=1 1{y (i) = l}
m
After having our probabilities and parameters
trained, we calculated the probability for the test
data and outputted each genre based on maximum
likelihood estimates of the training data belonging
to the genre.

3.2 Random Forests

Figure 2: Random Forests Categorical Accuracy

We ran the Random Forests algorithm with 100
trees over the entire dataset and computed the error
with an ooB error estimator. Random Forests classi-
ﬁes using multiple decision trees, each of which uses
a portion of the overall data. The algorithm works
by growing a given number of trees, each of which
is passed a randomly sampled portion of the over-
all training set and some features oﬀ which to base
the decision. Each node in the decision tree also
randomly chooses what features to use for that par-
ticular split. To obtain a classiﬁcation on test data,

2

an example is passed to each tree, which pushes the
example through it and outputs a classiﬁcation. The
ma jority classiﬁcation over all trees is the resulting
classiﬁcation of the algorithm. Among the noted
disadvantages of Random Forests is that it tends to
overﬁt when the data contains a lot of noise [9].

3.3 Multi-class SVM

Figure 3: Multi-class SVM Categorical Accuracy

We used an L2- regularized L2-loss SVM classi-
ﬁcation [10] by implementing the “one v.s. one”
method instead of “one against rest” since it is much
faster and, as observed by C.W. Hsu and C.J. Lin,
is comparable in performance[11]. As we were in a
multi-class setting, we solved 7 binary SVMs. We
determined the optimal value of the relative weight-
ing factor, C, by trying various values, ﬁnding that
the best performance was observed with C = 4.

3.4 Softmax Regression

k￿j=1

1{y (i) = j }log(p(y (i) = j ￿x(i) θ))] +

λ
2

k￿i=1

n￿j=1

θ2
ij

[

−

1
m

m￿i=1
where

[

−

p(y (i) = j |x(i) ; θ) =

x(i) (1{y (i) = j }− p(y (i) = j |x(i) ; θ))] + λθj

eθ ￿j x(i)
￿k
l=1 eθ ￿l x(i)
which gives the gradient ∇θj J (θ) =
m￿i=1
1
m
This approach has been found to work well in the
area of text classiﬁcation [12]. Adding the weight
decay term allows for the overparameterized system
to be solved without resorting to removing one of
the probability vectors. Our algorithm made use
of L-BFGS provided by [14] to compute the mini-
mum of the cost function. We found that softmax
initially performed very poorly with roughly 10% ac-
curacy on both the stemmed and unstemmed data.
We made improvements to the algorithm by nor-
malizing the word frequencies per song based on the
length of that song. Intuitively, this helps because
longer songs would tend to have a higher word count
of common words even though the higher count is
not indicative of the genre. By doing this, we found
that accuracy rose to about 20%. Following the idea
above, we modiﬁed the original data by eliminat-
ing words that appeared in too many categories and
ran the test using a vocabulary that focused on the
words that were diﬀerent in each set. When this was
done, the accuracy rose to around 40%, comparable
to the other tests.

3.5 k-Nearest Neighbors

Figure 4: Softmax Confusion Categorical Accuracy

As previous existing implementations were too
slow, we implemented our own version of softmax
regression with regularization. In our implementa-
tion, we used the modiﬁed cost function J (θ) =

Figure 5: kNN Categorical Accuracy

3

The k-nearest neighbors algorithm (kNN) works
by polling the points which surround a training ex-
ample. More speciﬁcally, the algorithm keeps the
entire training set it is given, and when a test exam-
ple is given to it, it ﬁnds the k nearest points to the
test example and outputs the most common class
among these k neighbors. We used diﬀerent metrics
for ﬁnding the nearest neighbor, including euclidean
distance and correlation. In general, we found that
using the metric of euclidean distance gave better
performance than the correlation metric. We quan-
titatively measured the accuracies choosing diﬀerent
values for k, and found that k = 250 gave us the best
results over several rounds of testing.

3.6 k-Means

We decided to use an implementation of k-means
[13] to test unsupervised learning on this problem.
We ran k-means with 7 clusters for 100,000 itera-
tions. Since these clusters didn’t have an a priori
class associated with them, we iterated over all per-
mutations of categories and picked our accuracy for
this algorithm to be from the best performing per-
mutation. This algorithm yielded an accuracy of 26
%. Another approach we tried was running k-means
with 400 clusters and replacing each data point that
k-means is associated with with that cluster, using
that as our new training matrix. We found that
this provided comparable results with Naive Bayes
and kNN. Gayathri, K., et al., 2011 [15] suggested
that this implementation would increase accuracy;
however, we did not see signiﬁcant increases. We
speculate that k-Means may cluster the songs into
subcategories which are not correlated with genre.

3.7 Ensemble

Our initial ensemble of classiﬁers used all our in-
dividual learning methods to see which genre most
algorithms classiﬁed the lyrics into. The rationale
behind this was that as the number of hypotheses
increases, the chances that all of them will misclas-
sify the lyrics decreases. We observed that running
this ensemble in some cases did worse than our best
algorithm. This became clear since kNN and k-
means were giving accuracies of around 35% and
26% whereas all the other algorithms had accuracies
of at least 40% and hence, kNN and k-means were
increasing the number of misclassiﬁcations. We then

Figure 6: Ensemble Confusion Matrix: This confusion matrix rep-
resents the output of our ensemble algorithm. A bluer color in-
dicates that more classiﬁcations were made in that square and a
whiter color indicates the opposite. Category 4 (Hip-Hop) has the
least confusion while category 7 (Pop) has the most confusion.

ran the ensemble with only our top 4 performing al-
gorithms. In case of a tie, preference was given in the
order of NB > SVM > Random Forests > Softmax.
The updated ensemble consistently performed bet-
ter than our best algorithms, reaching 53% accuracy.
The reason the accuracy only improved by 3-5% in
comparison to the best standalone algorithm, Naive
Bayes, for any test data was because most of our
algorithms were largely correctly classifying or mis-
classifying the same songs, again implying that cer-
tain songs cannot be classiﬁed well using lyrics.

4 Analysis

Figure 7: Each bar represents the overall accuracy of one of our
algorithms over all 7 categories.

Naive Bayes consistently provided the highest ac-
curacy rates with an average of about 48%, followed
by accuracy rates of SVM, Random Forests and
Softmax. We found that clustering algorithms such
as k-means and kNN did not provide as high accu-
racies, which we believe is largely due to the overlap
of the data. When we examined our dataset, we
found that most music genres share at least 70% of

4

their words, and furthermore, these words tend to
be common words in the English language. Cate-
gories which contained unique words were classiﬁed
more accurately than those which did not; the genre
of Hip-Hop shared the least amount of words with
other categories whereas Pop songs shared the most.
Overall, words that are unique to a category tend to
be limited to a few songs in that category and there-
fore do not provide much guidance in diﬀerentiating
between genres. Though this aﬀects all our catego-
rizations in general, our clustering algorithms are in
particular aﬀected because the neighbors nearest to
a particular example are often not in the same song
genre.
Our best results came from the ensemble learn-
ing algorithm when it did not include kNN. Since
the accuracy increase was only around 3-5%, this
suggests that our models had large overlap in which
songs it classiﬁed correctly and incorrectly, leading
to little gain when we combined them. For instance,
some songs can be remixed from one genre to oth-
ers, without any change in lyrics. This means that
certain songs have a certain ambiguity that cannot
be learned, while others are more indicative.

5 Conclusion

We have observed how using complete song lyrics
along with an ensemble of the best text-classiﬁcation
algorithms can help improve accuracy of music clas-
siﬁcation. While clustering algorithms have often
performed well in other text-classiﬁcation settings,
they seem unsuited for lyrics classiﬁcation. We have
also observed how lyrics alone cannot provide a com-
plete picture of a song’s genre. Adding more fea-
tures such as length, rhyme-scheme, and multi-word
context might allow for better classiﬁcation. Other
possibilities of future work include experimentation
with these algorithms using non-standard distance
metrics. Diﬀerent ways of examining the data, for
example with multi-label classiﬁcation, looking at
the top two choices of genre and seeing if either of
those match to the actual genre, may also provide
novel insight.

References

[1] Liang, Dawen, Haijie Gu, and Brendan OConnor.
”Music Genre Classication with the Million Song
Dataset.” Machine Learning Department, CMU

5

(2011).
http://www.cs.cmu.edu/~music/dawenl/
files/FINAL.pdf

[2] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian
Whitman, and Paul Lamere. The million song
dataset. In Proceedings of the 12th International
Conference on Music Information http://labrosa.
ee.columbia.edu/millionsong/.

[3] Publicly available, processed music lyrics: http/
/alliance.seas.upenn.edu/~cis520/wiki/
[4] Free unprocessed lyrics: lyrics.wikia.com.

[5] Top ten most popular genres 2009 http://76.74.24.
142/44510E63-7B5E-5F42-DA74-349B51EDCE0F.
pdf

[6] Gaustad, Tanja. ”Accurate Stemming of Dutch for
Text Classiﬁcation.” Alfa-Informatica Rijksuniver-
siteit Groningen (2001)

[7] Jensen, Lee S., and Tony R. Martinez. ”Improving
Text Classiﬁcation by Using Conceptual and Con-
textual Features.” Computer Science Department;
Brigham Young University (2000)

[8] Porter Stemming Algorithm included in the python
library: http://nltk.org/

[9] Segal, Mark R. ”Machine Learning Benchmarks and
Random Forest Regression.” Division of Biostatistics,
University of California (2003).

[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,
and C.-J. Lin. LIBLINEAR: A Library for Large Lin-
ear Classiﬁcation, Journal of Machine Learning Re-
search 9(2008), 1871-1874.

[11] Hsu, Chih-Wei, and Chih-Jen Lin. ”A Compari-
son of Methods for Multi-class Support Vector Ma-
chines.” Department of Computer Science and In-
formation Engineering; National Taiwan University
(2002)

[12] Stanford
regression
Softmax
explaining
site
http://ufldl.stanford.edu/wiki/index.php/
Softmax_Regression

implementation
[13] An
by Adam
k-means
of
Coates from:
https://sites.google.com/site/
k-meanslearning/

[14] An implementation of minFunc: http://www.di.
ens.fr/mschmidt/Software/minFunc.html

[15] Gayathri, K., and A. Marimuthu. ”An Improved
KNN Text Classiﬁcation Algorithm by Using K-
Mean Clustering.” International Journal of Comput-
ing Technology and Information Security 1.2 (2011):
73-76.


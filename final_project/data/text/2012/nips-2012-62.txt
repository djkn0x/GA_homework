Majorization for CRFs and Latent Likelihoods

Tony Jebara
Department of Computer Science
Columbia University
jebara@cs.columbia.edu

Anna Choromanska
Department of Electrical Engineering
Columbia University
aec2163@columbia.edu

Abstract

The partition function plays a key role in probabilistic modeling including condi-
tional random ﬁelds, graphical models, and maximum likelihood estimation. To
optimize partition functions, this article introduces a quadratic variational upper
bound. This inequality facilitates majorization methods: optimization of com-
plicated functions through the iterative solution of simpler sub-problems. Such
bounds remain efﬁcient to compute even when the partition function involves
a graphical model (with small tree-width) or in latent likelihood settings. For
large-scale problems, low-rank versions of the bound are provided and outper-
form LBFGS as well as ﬁrst-order methods. Several learning applications are
shown and reduce to fast and convergent update rules. Experimental results show
advantages over state-of-the-art optimization methods.

1 Introduction
The estimation of probability density functions over sets of random variables is a central problem
in learning. Estimation often requires minimizing the partition function as is the case in conditional
random ﬁelds (CRFs) and log-linear models [1, 2]. Training these models was traditionally done
via iterative scaling and bound-majorization methods [3, 4, 5, 6, 1] which achieved monotonic con-
vergence. These approaches were later surpassed by faster ﬁrst-order methods [7, 8, 9] and then
second-order methods such as LBFGS [10, 11, 12]. This article revisits majorization and repairs
its slow convergence by proposing a tighter bound on the log-partition function. The improved ma-
jorization outperforms state-of-the-art optimization tools and admits multiple versatile extensions.
Many decomposition methods for conditional random ﬁelds and structured prediction have sought
to render the learning and prediction problems more manageable [13, 14, 15]. Our decomposi-
tion, however, hinges on bounding and majorization: decomposing an optimization of complicated
functions through the iterative solution of simpler sub-problems [16, 17]. A tighter bound provides
convergent monotonic minimization while outperforming ﬁrst- and second-order methods in prac-
tice1 . The bound applies to graphical models [18], latent variable situations [17, 19, 20, 21] as well
as high-dimensional settings [10]. It also accommodates convex constraints on the parameter space.
This article is organized as follows. Section 2 presents the bound and Section 3 uses it for ma-
jorization in CRFs. Extensions to latent likelihood are shown in Section 4. The bound is extended
to graphical models in Section 5 and high dimensional problems in Section 6. Section 7 provides
experiments and Section 8 concludes. The Supplement contains proofs and additional results.
2 Partition Function Bound
)
(
Consider a log-linear density model over discrete y ∈ Ω
p(y |θ) =
1
θ
h(y) exp
Z (θ)

f (y)

⊤

1Recall that some second-order methods like Newton-Raphson are not monotonic and may even fail to
converge for convex cost functions [4] unless, of course, line searches are used.

1

which is parametrized by a vector θ ∈ Rd of dimensionality d ∈ N. Here, f : Ω 7→ Rd is
∑
any vector-valued function mapping an input y to some arbitrary vector. The prior h : Ω 7→ R+
is a ﬁxed non-negative measure. The partition function Z (θ) is a scalar that ensures that p(y |θ)
⊤
normalizes, i.e. Z (θ) =
f (y)). Assume that the number of conﬁgurations of y is
y h(y) exp(θ
|Ω| = n and is ﬁnite2 . The partition function is clearly log-convex in θ and a linear lower-bound
is given via Jensen’s inequality. This article contributes an analogous quadratic upper-bound on the
partition function. Algorithm 1 computes3 the bound’s parameters and Theorem 1 shows the precise
guarantee it provides.

Algorithm 1 ComputeBound
Input Parameters ˜θ , f (y), h(y) ∀y ∈ Ω
Init z → 0+ , µ = 0, (cid:6) = z I
For each y ∈ Ω {
⊤
α = h(y) exp( ˜θ
l = f (y) − µ
(cid:6) + = tanh( 1
2 log(α/z))
2 log(α/z)
µ + = α
z+α l
}
z + = α
Output z , µ, (cid:6)

⊤
ll

f (y))

∑
2 (θ − ˜θ)
(cid:6)(θ − ˜θ) + (θ − ˜θ)
⊤
⊤
Theorem 1 Algorithm 1 ﬁnds z , µ, (cid:6) such that z exp( 1
µ) upper-
f (y)) for any θ , ˜θ , f (y) ∈ Rd and h(y) ∈ R+ for all y ∈ Ω.
⊤
bounds Z (θ) =
y h(y) exp(θ
−θ ) ≤ cθ2 [22].
Proof 1 (Sketch, See Supplement for Formal Proof) Recall the bound log(eθ + e
−θ
⊤
⊤
⊤
⊤
1 ). Tilt the bound to handle log(h1 eθ
Obtain a multivariate variant log(eθ
f2 ).
f1 + h2 eθ
1 + e
⊤
⊤
⊤
Add an additional exponential term to get log(h1 eθ
f3 ). Iterate the last step
f1 + h2 eθ
f2 + h3 eθ
to extend to n elements in the summation.

The bound improves previous inequalities and its proof is in the Supplement. It tightens [4, 19] since
it avoids wasteful curvature tests (it uses duality theory to compare the bound and the optimized
function rather than compare their Hessians). It generalizes [22] which only holds for n = 2 and
h(y) constant; it generalizes [23] which only handles a simpliﬁed one-dimensional case. The bound
is computed using Algorithm 1 by iterating over the y variables (“for each y ∈ Ω”) according to an
arbitrary ordering via the bijective function π : Ω 7→ {1, . . . , n} which deﬁnes i = π(y). The order
in which we enumerate over Ω slightly varies the (cid:6) in the bound (but not the µ and z ) when |Ω| >
∑
2. However, we empirically investigated the inﬂuence of various orderings on bound performance
(in all the experiments presented in Section 7) and noticed no signiﬁcant effect across ordering
f (y))(f (y) − µ)(f (y) − µ)
⊤
⊤ with µ and z
y h(y) exp( ˜θ
schemes. Recall that choosing (cid:6) =
as in Algorithm 1 yields the second-order Taylor approximation (the Hessian) of the log-partition
function. Algorithm 1 replaces a sum of log-linear models with a single log-quadratic model which
makes monotonic majorization straightforward. The ﬁgure inside Algorithm 1 depicts the bound on
log Z (θ) for various choices of ˜θ . If there are no constraints on the parameters (i.e. any θ ∈ Rd
is admissible), a simple closed-form iterative update rule emerges: ˜θ ← ˜θ − (cid:6)
−1µ. Alternatively,
if θ must satisfy linear (convex) constraints it is straightforward to compute an update by solving a
quadratic (convex) program. This update rule is interleaved with the bound computation.
3 Conditional Random Fields and Log-Linear Models
The partition function arises naturally in maximum entropy estimation or minimum relative entropy
estimation (cf. Supplement) as well as in conditional extensions of the maximum entropy paradigm
where the model is conditioned on an observed input. Such models are known as conditional random
ﬁelds and have been useful for structured prediction problems [1, 24]. CRFs are given a data-set
{(x1 , y1 ), . . . , (xt , yt )} of independent identically-distributed (iid) input-output pairs where yj is
2 Here, assume n is enumerable. Later, for larger spaces use O(n) to denote the time to compute Z .
3By continuity, take tanh( 1
4 and limz!0+ tanh( 1
2 log(α/z ))/(2 log(α/z )) = 0.
2 log(1))/(2 log(1)) = 1

2

−50500.050.10.150.20.250.30.35θlog(Z) and Boundsfxj (y))

hxj (y) exp(θ

the observed sample in a (discrete) space Ωj conditioned on the observed input xj . A CRF deﬁnes
a distribution over all y ∈ Ωj (of which yj is a single element) as the log-linear model
∑
p(y |xj , θ) =
⊤
1
Zxj (θ)
⊤
fxj (y)). For the j ’th training pair, we are given a non-
where Zxj (θ) =
y∈Ωj
hxj (y) exp(θ
negative function hxj (y) ∈ R+ and a vector-valued function fxj (y) ∈ Rd deﬁned over the domain
∑
y ∈ Ωj . In this section, for simplicity, assume n = maxt
|Ωyj
|. Each partition function Zxj (θ) is
j=1
a function of θ . The parameter θ for CRFs is estimated by maximizing the regularized conditional
∥θ∥2 where λ ∈ R+ is a regularizer set
j=1 log p(yj |xj , θ) − tλ
t
log-likelihood4 or log-posterior:
t∑
2
using prior knowledge or cross-validation. Rewriting gives the objective of interest
fxj (yj ) − tλ
∥θ∥2 .
2
j=1
If prior knowledge (or constraints) restrict the solution vector to a convex hull (cid:3), the maximization
problem becomes arg maxθ∈(cid:3) J (θ).
Algorithm 2 proposes a method for maximizing the regularized conditional likelihood J (θ) or,
equivalently minimizing the partition function Z (θ). It solves the problem in Equation 1 subject
to convex constraints by interleaving the quadratic bound with a quadratic programming procedure.
Theorem 2 establishes the convergence of the algorithm and the proof is in the Supplement.

hxj (yj )
Zxj (θ)

⊤

+ θ

J (θ) =

log

(1)

⊤

j θ

(µj − fxj (yj ) + λ ˜θ)

Algorithm 2 ConstrainedMaximization
0: Input xj , yj and functions hxj , fxj for j = 1, . . . , t, regularizer λ ∈ R+ and convex hull (cid:3) ⊆ Rd
1: Initialize θ0 anywhere inside (cid:3) and set ˜θ = θ0
While not converged
∑
∑
2: For j = 1, . . . , t
Get µj , (cid:6)j from hxj , fxj , ˜θ via Algorithm 1
2 (θ − ˜θ)
((cid:6)j +λI)(θ − ˜θ) +
⊤
3: Set ˜θ = arg minθ∈(cid:3)
1
j
4: Output ˆθ = ˜θ
⌈
(
)⌉
Theorem 2 For any θ0 ∈ (cid:3), all ∥fxj (y)∥ ≤ r and all
|Ωj | ≤ n, Algorithm 2
(
)
∑
outputs a ˆθ such that J ( ˆθ) − J (θ0 ) ≥ (1 − ϵ) maxθ∈(cid:3) (J (θ) − J (θ0 )) in more than
)
(
n−1
−1
∑
∑
tanh(log(i)/2)
iterations.
1
1 + λ
log
/ log
)
2r2 (
i=1
log(i)
ϵ
n−1
n−1
i−1
tanh(log(i)/2)
The series
(i+1) log(i) is the logarithmic integral which is O
n
=
i=1
i=1
log n
log(i)
asymptotically [26]. The next sections show how to handle hidden variables in the learning problem,
exploit graphical modeling, and further accelerate the underlying algorithms.
4 Latent Conditional Likelihood
Section 3 showed how the partition function is useful for maximum conditional likelihood problems
involving CRFs. In this section, maximum conditional likelihood is extended to the setting where
some variables are latent. Latent models may provide more ﬂexibility than fully observable models
[21, 27, 28]. For instance, hidden conditional random ﬁelds were shown to outperform generative
hidden-state and discriminative fully-observable models [21].
Consider the latent setting where we are given t iid samples x1 , . . . , xt from some unknown distri-
bution ¯p(x) and t corresponding samples y1 , . . . , yt drawn from identical conditional distributions
¯p(y |x1 ), . . . , ¯p(y |xt ) respectively. Assume that the true generating distributions ¯p(x) and ¯p(y |x)
are unknown. Therefore, we aim to estimate a conditional distribution p(y |x) from some set of hy-
potheses that achieves high conditional likelihood given the data-set D = {(x1 , y1 ), . . . , (xt , yt )}.
4Alternatively, variational Bayesian approaches can be used instead of maximum likelihood via expectation
propagation (EP) or power EP [25]. These, however, assume Gaussian posterior distributions over parameters,
require approximations, are computationally expensive and are not necessarily more efﬁcient than BFGS.

3

L(Θ) =

.

(2)

L(Θ) =

∑
We will select this conditional distribution by assuming it emerges from a conditioned joint distri-
bution over x and y as well as a hidden variable m which is being marginalized as p(y |x, Θ) =
∑
m p(x,y ,m|Θ)
y;m p(x,y ,m|Θ) . Here m ∈ Ωm represents a discrete hidden variable, x ∈ Ωx is an input and
y ∈ Ωy is a discrete output variable. The parameter Θ contains all parameters that explore the
function class of such conditional distributions. The latent likelihood of the data L(Θ) = p(D|Θ)
∑
t∏
t∏
∑
subsumes Equation 1 and is the new objective of interest
m p(xj , yj , m|Θ)
p(yj |xj , Θ) =
y ,m p(xj , y , m|Θ)
j=1
j=1
)
(
A good choice of the parameters is one that achieves a large conditional likelihood value (or poste-
rior) on the data set D . Next, assume that each p(x|y , m, Θ) is an exponential family distribution
y ,mϕ(x) − a(θy ,m )
p(x|y , m, Θ) = h(x) exp
⊤
θ
where each conditional is speciﬁed by a function h : Ωx 7→ R+ and a feature mapping ϕ : Ωx 7→ Rd
which are then used to derive the normalizer a : Rd 7→ R+ . A parameter θy ,m ∈ Rd se-
πy;m∑
lects a speciﬁc distribution. Multiply each exponential family term by an unknown marginal dis-
tribution called the mixing proportions p(y , m|π) =
. This is parametrized by an un-
(
)
y;m πy;m
known parameter π = {πy ,m } ∀y , m where πy ,m ∈ [0, ∞). Finally, the collection of all pa-
rameters is Θ = {θy ,m , πy ,m } ∀y , m. Thus, we have the complete likelihood p(x, y , m|Θ) =
∑
y ,mϕ(x) − a(θy ,m )
⊤
πy;m h(x)
.
Insert this expression into Equation 2 and remove con-
exp
θ
y;m πy;m
(
)
∑
stant factors that appear in both denominator and numerator. Apply the change of variables
(
)
∑
exp(νy ,m ) = πy ,m exp(−a(θy ,m )) and rewrite the objective as a function5 of a vector θ :
t∏
t∏
(
) =
∑
∑
⊤
⊤
θ
m exp
yj ,mϕ(xj ) + νyj ,m
fj,yj ,m
θ
m exp
y ,m exp (θ⊤ fj,y ,m )
θ⊤
y ,m exp
y ,mϕ(xj ) + νy ,m
j=1
j=1
The last equality emerges by rearranging all Θ parameters as a vector θ ∈ R|Ωy ||Ωm |(d+1) equal to
· · · θ
⊤ and introducing fj, ˆy , ˆm ∈ R|Ωy ||Ωm |(d+1) deﬁned
⊤
⊤
⊤
|Ωy |,|Ωm | ν|Ωy |,|Ωm | ]
1,2 ν1,2
1,1 ν1,1 θ
[θ
⊤
⊤ is
⊤ (thus the feature vector [ϕ(xj )
1]δ [( ˆy , ˆm)=(1,1)] ··· [ϕ(xj )
1]δ [( ˆy , ˆm)=(|Ωy |,|Ωm |)]]
⊤
⊤
as [[ϕ(xj )
1]
positioned appropriately in the longer fj, ˆy , ˆm vector which is elsewhere zero). We will now ﬁnd a
variational lower bound on L(θ) ≥ Q(θ , ˜θ) which is tight when θ = ˜θ such that L( ˜θ) = Q( ˜θ , ˜θ).
∑
(
) ≥ eθ
We proceed by bounding each numerator and each denominator in the product over j = 1, . . . , t.
m ηj;m fj;yj ;m−∑
⊤ ∑
Apply Jensen’s inequality to lower bound each numerator term as
⊤
∑
m ηj;m log ηj;m
fj,yj ,m
exp
m
∑
) ≤ zj e
(
⊤
⊤
fj;yj ;m′
˜θ
˜θ
fj;yj ;m )/(
). Algorithm 1 then bounds the denominator
(cid:6)j (θ− ˜θ)+(θ− ˜θ)
2 (θ− ˜θ)
⊤
⊤
1
fj,y ,m
y ,m
The overall lower bound on the likelihood is then
j=1 (µj − ∑
∑
∑
− 1
⊤ ˜(cid:6)(θ− ˜θ)−(θ− ˜θ)
2 (θ− ˜θ)
⊤
Q(θ , ˜θ) = L( ˜θ)e
t
t
where ˜(cid:6) =
m ηj,m fj,yj ,m ). The right hand side is simply an
j=1 (cid:6)j and ˜µ =
exponentiated quadratic function in θ which is easy to maximize. This yields an iterative scheme
similar to Algorithm 2 for monotonically maximizing latent conditional likelihood.
5 Graphical Models for Large n
The bounds in the previous sections are straightforward to compute when Ω is small. However,
for graphical models, enumerating over Ω can be daunting. This section provides faster algorithms
5 It is now easy to regularize L((cid:18)) by adding − t(cid:21)
∥(cid:18)∥2 .
2

where ηj,m = (e

m′ e
⊤

θ

exp

θ

.

µj .

˜µ

4

that recover the bound efﬁciently for graphical models of bounded tree-width. A graphical model
represents the factorization of a probability density function. This article will consider the factor
graph notation of a graphical model. A factor graph is a bipartite graph G = (V , W, E ) with variable
vertices V = {1, . . . , k}, factor vertices W = {1, . . . , m} and a set of edges E between V and W .
∏
In addition, deﬁne a set of random variables Y = {y1 , . . . , yk } each associated with the elements of
V and a set of non-negative scalar functions Ψ = {ψ1 , . . . , ψm} each associated with the elements
of W . The factor graph implies that p(Y ) factorizes as p(y1 , . . . , yk ) = 1
c∈W ψc (Yc ) where
Z
Z is a normalizing partition function (the dependence on parameters is suppressed here) and Yc is
a subset of the random variables that are associated with the neighbors of node c. In other words,
Yc = {yi |i ∈ Ne(c)} where Ne(c) is the set of vertices that are neighbors of c.
Inference in
graphical models requires the evaluation and the optimization of Z . These computations can be
NP-hard in general yet are efﬁcient when G satisﬁes certain properties (low tree-width). Consider
a log-linear model (a function class) indexed by a parameter θ ∈ (cid:3) in a convex hull (cid:3) ⊆ Rd as
∏
)
(
follows
1
(
)
∑
∏
hc (Yc ) exp
Z (θ)
c∈W
⊤
where Z (θ) =
. The model is deﬁned by a set of vector-valued
c∈W hc (Yc ) exp
θ
fc (Yc )
functions fc (Yc ) ∈ Rd and scalar-valued functions hc (Yc ) ∈ R+ . Choosing a function from the
Y
function class hinges on estimating θ by optimizing Z (θ). However, Algorithm 1 may be inappli-
cable due to the large number of conﬁgurations in Y . Instead, consider a more efﬁcient surrogate
algorithm which computes the same bound parameters by efﬁciently exploiting the factorization of
the graphical model. This is possible since exponentiated quadratics are closed under multiplication
and the required bound computations distribute nicely across decomposable graphical models.

p(Y |θ) =

fc (Yc )

⊤

θ

fc (Yc )) and ˜θ ∈ Rd

Algorithm 3 JunctionTreeBound
Input Reverse-topological tree T with c = 1, . . . , m factors hc (Yc ) exp( ˜θ
⊤
For c = 1, . . . , m
If (c < m) {Yboth = Yc ∩ Ypa(c) , Ysolo = Yc \ Ypa(c)}
Else {Yboth = {}, Ysolo = Yc }
For each u ∈ Yboth
{ Initialize zc|x ← 0+ , µc|x = 0, (cid:6)c|x = zc|x I
∑
∏
For each v ∈ Ysolo
{
w = u ⊗ v ;
∑
lw = fc (w) − µc|u +
⊤
αw = hc (w)e ˜θ
b∈ch(c) µb|w ;
b∈ch(c)zb|w ;
fc (w)
αw
zc|u = αw ; }}
tanh( 1
⊤
))
2 log(
zc|u
b∈ch(c)(cid:6)b|w+
w ; µc|u = αw
lw l
αw
zc|u+αw
)
zc|u

(cid:6)c|u =

lw ;

2 log(

Output Bound as z = zm , µ = µm , (cid:6) = (cid:6)m

Begin by assuming that the graphical model in question is a junction tree and satisﬁes the running
intersection property [18]. In Algorithm 3 (the Supplement provides a proof of its correctness), take
ch(c) to be the set of children-cliques of clique c and pa(c) to be the parent of c. Note that the
algorithm enumerates over u ∈ Ypa(c) ∩ Yc and v ∈ Yc \ Ypa(c) . The algorithm stores a quadratic
bound for each conﬁguration of u (where u is the set of variables in common across both clique c
and its parent). It then forms the bound by summing over v ∈ Yc \ Ypa(c) , each conﬁguration of
each variable a clique c has that is not shared with its parent clique. The algorithm also collects
precomputed bounds from children of c. Also deﬁne w = u ⊗ v ∈ Yc as the conjunction of both
indexing variables u and v . Thus, the two inner for loops enumerate over all conﬁgurations w ∈ Yc
of each clique. Note that w is used to query the children b ∈ ch(c) of a clique c to report their bound
parameters zb|w , µb|w , (cid:6)b|w . This is done for each conﬁguration w of the clique c. Note, however,
that not every variable in clique c is present in each child b so only the variables in w that intersect Yb
are relevant in indexing the parameters zb|w , µb|w , (cid:6)b|w and the remaining variables do not change
the values of zb|w , µb|w , (cid:6)b|w .
Algorithm 3 is efﬁcient in the sense that computations involve enumerating over all conﬁgurations
of each clique in the junction tree rather than over all conﬁgurations of Y . This shows that the

5

}

∑
∑
|Yc |) rather than O(|Ω|) as in Algorithm 1. Thus, for estimating the
computation involved is O(
|Yc | for the graphical
c
computational efﬁciency of various algorithms in this article, take n =
model case rather than n = |Ω|. Algorithm 3 is a simple extension of the known recursions that
c
(cid:12)(cid:12)(cid:12)
are used to compute the partition function and its gradient vector. Thus, in addition to the (cid:6) matrix
which represents the curvature of the bound, Algorithm 3 is recovering the partition function value
z and the gradient since µ = ∂ log Z (θ)
.
∂θ
θ= ˜θ
6 Low-Rank Bounds for Large d
In many realistic situations, the dimensionality d is large and this prevents the storage and inver-
sion of the matrix (cid:6). We next present a low-rank extension that can be applied to any of the
algorithms presented so far. As an example, consider Algorithm 4 which is a low-rank incar-
nation of Algorithm 2. Each iteration of Algorithm 2 requires O(tnd2 + d3 ) time since step 2
computes several (cid:6)j ∈ Rd×d matrices and 3 performs inversion.
Instead, the new algorithm
provides a low-rank version of the bound which still majorizes the log-partition function but re-
quires only ˜O(tnd) complexity (putting it on par with LBFGS). First, note that step 3 in Algo-

ft (y) ; r =

Algorithm 4 LowRankBound
Input Parameter ˜θ , regularizer λ ∈ R+ , model ft (y) ∈ Rd and ht (y) ∈ R+ and rank k ∈ N
Initialize S = 0 ∈ Rk×k , V = or thonormal ∈ Rk×d , D = tλI ∈ diag(Rd×d )
For each t { Set z → 0+ ; µ = 0;
√
For each y{
√
(ft (y) − µ) ;
α
⊤
tanh( 1
z ))
2 log(
α = ht (y)e ˜θ
α
2 log(
z )
V(i, ·); r = r − p(i)V(i, ·);
⊤
For i = 1, . . . , k : p(i) = r
For i = 1, . . . , k : For j = 1, . . . , k : S(i, j ) = S(i, j ) + p(i)p(j );
AQ = svd(S); S ← A; V ← QV;
⊤
Q
s = [S(1, 1), . . . , S(k , k), ∥r∥2 ]
⊤
; ˜k = arg mini=1,...,k+1 s(i);
if (˜k ≤ k) { D = D + S(˜k , ˜k)1
⊤ |V(j, ·)| diag(|V(k , ·)|);
S(˜k , ˜k) = ∥r∥2 ; r = ∥r∥−1 r; V(k , ·) = r; }
{ D = D + 1
⊤ |r|diag(|r|); } }
else
} }
z+α (ft (y) − µ); z + = α;
µ + = α
∑
Output S ∈ diag(Rk×k ), V ∈ Rk×d , D ∈ diag(Rd×d )
j=1 µj − fxj (yj ). Clearly, Al-
rithm 2 can be written as ˜θ = ˜θ − (cid:6)
−1u where u = tλ ˜θ +
t
gorithm 1 can recover u by only computing µj for j = 1, . . . , t and skipping all steps involving
√
matrices. This merely requires O(tnd) work. Second, we store (cid:6) using a low-rank represen-
SV + D where V ∈ Rk×d is orthonormal, S ∈ Rk×k is positive semi-deﬁnite, and
⊤
tation V
D ∈ Rd×d is non-negative diagonal. Rather than increment the matrix by a rank one update of the
(fi − µi ), simply project ri onto each
⊤
tanh( 1
2 log(α/z))
form (cid:6)i = (cid:6)i−1 + ri r
i where ri =
2 log(α/z)
eigenvector in V and update matrix S and V via a singular value decomposition (O(k3 ) work).
After removing k such projections, the remaining residual from ri forms a new eigenvector ek+1
and its magnitude forms a new singular value. The resulting rank (k + 1) system is orthonormal
with (k + 1) singular values. We discard its smallest singular value and corresponding eigenvector
to revert back to an order k eigensystem. However, instead of merely discarding we can absorb
the smallest singular value and eigenvector into the D component by bounding the remaining outer-
product with a diagonal term. This provides a guaranteed overall upper bound in ˜O(tnd) (k is
assumed to be logarithmic with dimension d). Finally, to invert (cid:6), we apply the Woodbury formula:
−1 = D
−1 + D
−1V
⊤
−1 + VD
−1V
⊤
−1VD
−1 which only requires O(k3 ) work. A proof of
(cid:6)
(S
)
correctness for Algorithm 4 can be found in the Supplement.
7 Experiments
We ﬁrst focus on the logistic regression task and compare the performance of the bound (using
the low-rank Algorithm 2) with ﬁrst-order and second order methods such as LBFGS, conjugate
gradient (CG) and steepest descent (SD). We use 4 benchmark data-sets: the SRBCT and Tumors

6

data-sets from [29] as well as the Text and SecStr data-sets from http://olivier.chapelle.cc/ssl-
book/benchmarks.html. For all experiments in this section, the setup is as follows. Each data-set is
split into training (90%) and testing (10%) parts. All implementations are run on the same hardware
with C++ code. The termination criterion for all algorithms is a change in estimated parameter or
−6 (with a ceiling on the number of iterations of 106 ). Results are
function values smaller than 10
averaged over 10 random initializations close to 0. The regularization parameter λ, when used, was
chosen through crossvalidation. In Table 1 we report times in seconds and the number of iterations
for each algorithm (including LBFGS) to achieve the LBFGS termination solution modulo a small
−4 ). Table 1 also provides data-set sizes and regularization values. The ﬁrst 4
constant ϵ (set to 10
columns in Table 1 provide results for this experiment.

Data-set
Size

SRBCT
Tumors
n = 4
n = 26
t = 83
t = 308
d = 9236
d = 390260
λ = 101
λ = 101
iter
Algorithm time
time
6.10
LBFGS
42
3246.83
SD
7.27
43
18749.15
40.61 100 14840.66
CG
3.67
8
1639.93
Bound

iter
8
53
42
4

Text
n = 2
t = 1500
d = 23922
λ = 102
iter
time
7
15.54
153.10
69
23
57.30
6.18
3

SecStr
n = 2
t = 83679
d = 632
λ = 101
iter
time
47
881.31
1490.51
79
36
667.67
27.97
9

CoNLL
m = 9
t = 1000
d = 33615
λ = 101
iter
time
17
25661.54
93821.72
12
23
88973.93
16445.93
4

PennTree
m = 45
t = 1000
d = 14175
λ = 101
time
62848.08
156319.31
76332.39
27073.42

iter
7
12
18
2

Table 1: Time in seconds and iterations required to obtain within ϵ of the LBFGS solution (where
−4 ) for logistic regression problems (on SRBCT, Tumors, Text and SecStr data-sets where
ϵ = 10
n is the number of classes) and Markov CRF problems (on CoNLL and PennTree data-sets, where
m is the number of classes). Here, t is the total number of samples (training and testing), d is the
dimensionality of the feature vector and λ is the cross-validated regularization setting.

Structured prediction problems are explored using two popular data-sets. The ﬁrst one contains
Spanish news wire articles from the a session of the CoNLL 2002 conference. This corpus involves
a named entity recognition problem and consists of sentences where each word is annotated with
one of m = 9 possible labels. The second task is from the PennTree Bank. This corpus involves a
tagging problem and consists of sentences where each word is labeled with one of m = 45 possible
parts-of-speech. A conditional random ﬁeld is estimated with a Markov chain structure to give
word labels a sequential dependence. The features describing the words are constructed as in [30].
Two last columns of Table 1 provide results for this experiment. We used the low-rank version of
Algorithm 3. In both experiments, the bound always remained fastest as indicated in bold.

Figure 1: Classiﬁcation boundaries using the bound and EM for a toy latent likelihood problem.

We next performed experiments with maximum latent conditional likelihood problems. We denote
by m the number of hidden variables. Due to the non-concavity of this objective, we are most in-
terested in ﬁnding good local maxima. We start with a simple toy experiment from [19] comparing
the bound to the expectation-maximization (EM) algorithm in the binary classiﬁcation problem pre-
sented on the left image of Figure 1. The model incorrectly uses only 2 Gaussians per class while
the data is generated using 8 Gaussians total. On Figure 1 we show the decision boundary obtained
using the bound (with m = 2) and EM. EM performs as well as random chance guessing while the
bound classiﬁes the data very well. The average test log-likelihood obtained by EM was -1.5e+06
while the bound obtained -21.8.

7

−50510152025−25−20−15−10−505Data−set−50510152025−25−20−15−10−505Bound−50510152025−25−20−15−10−505EMWe next compared the algorithms (the bound, Newton-Raphson, BFGS, CG and SD) in maximum
latent conditional likelihood problems on ﬁve benchmark data-sets. These included four UCI data-
sets6 (ion, bupa, hepatitis and wine) and the previously used SRBCT data-set. The feature mapping
used was ϕ(x) = x ∈ Rd which corresponds to a mixture Gaussian-gated logistic regressions
(obtained by conditioning a mixture of m Gaussians per class). We used a value of λ = 0 throughout
the latent experiments. We explored setting m ∈ {1, 2, 3, 4}. Table 2 shows the testing latent log-
likelihood at convergence for m chosen through cross-validation (the Supplement contains a more
complete table). In bold, we show the algorithm that obtained the highest testing log-likelihood.
The bound is the best performer overall and ﬁnds better solutions in less time. Figure 2 depicts the
convergence on ion, hepatitis and SRBCT data sets.

Data-set
hepatitis wine SRBCT
bupa
ion
Algorithm m = 3 m = 2 m = 2 m = 3 m = 4
BFGS
-5.88
-21.78
-5.28
-1.79
-6.06
-5.61
-1.37
-5.14
-21.74
-5.56
SD
-5.76
-0.95
-4.84
-21.81
-5.57
CG
-5.54
-0.71
-5.50
-21.85
-5.95
Newton
-4.18
-19.95
-4.40
-0.48
-0.11
Bound

Table 2: Test log-likelihood at convergence for ion, bupa, hepatitis, wine and SRBCT data-sets.

Figure 2: Convergence of test latent log-likelihood on ion, hepatitis and SRBCT data-sets.

8 Discussion
A simple quadratic upper bound for the partition function of log-linear models was proposed and
makes majorization approaches competitive with state-of-the-art ﬁrst- and second-order optimiza-
tion methods. The bound is efﬁciently recoverable for graphical models and admits low-rank vari-
ants for high-dimensional data. It allows faster and monotonically convergent majorization in CRF
learning and maximum latent conditional likelihood problems (where it also ﬁnds better local max-
ima). Future work will explore intractable partition functions where likelihood evaluation is hard but
bound maximization may remain feasible. Furthermore, the majorization approach will be applied
in stochastic [31] and distributed optimization settings.
Acknowledgments
The authors thank A. Smola, M. Collins, D. Kanevsky and the referees for valuable feedback.
References
[1] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting
and labeling sequence data. In ICML, 2001.

[2] A. Globerson, T. Koo, X. Carreras, and M. Collins. Exponentiated gradient algorithms for log-linear
structured prediction. In ICML, 2007.

[3] J. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Annals of Math. Stat.,
43:1470–1480, 1972.

6Downloaded from http://archive.ics.uci.edu/ml/

8

−50510−25−20−15−10−50ionlog(Time) [sec]−log(J(θ))  BoundNewtonBFGSConjugate gradientSteepest descent−6−4−2024−11−10−9−8−7−6−5−4hepatitislog(Time) [sec]−log(J(θ))−4−2024−12−10−8−6−4−20SRBCTlog(Time) [sec]−log(J(θ))[4] D. Bohning and B. Lindsay. Monotonicity of quadratic approximation algorithms. Ann. Inst. Statist.
Math., 40:641–663, 1988.

[5] A. Berger. The improved iterative scaling algorithm: A gentle introduction. Technical report, 1997.

[6] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE PAMI, 19(4),
1997.

[7] R. Malouf. A comparison of algorithms for maximum entropy parameter estimation. In CoNLL, 2002.

[8] H. Wallach. Efﬁcient training of conditional random ﬁelds. Master’s thesis, University of Edinburgh,
2002.

[9] F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds. In NAACL, 2003.

[10] C. Zhu, R. Byrd, P Lu, and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale
bound-constrained optimization. TOMS, 23(4), 1997.

[11] S. Benson and J. More. A limited memory variable metric method for bound constrained optimization.
Technical report, Argonne National Laboratory, 2001.

[12] G. Andrew and J. Gao. Scalable training of ℓ1-regularized log-linear models. In ICML, 2007.

[13] D. Roth. Integer linear programming inference for conditional random ﬁelds. In ICML, 2005.

[14] Y. Mao and G. Lebanon. Generalized isotonic conditional random ﬁelds. Machine Learning, 77:225–248,
2009.

[15] C. Sutton and A. McCallum. Piecewise training for structured prediction. Machine Learning, 77:165–194,
2009.

[16] J. De Leeuw and W. Heiser. Convergence of correction matrix algorithms for multidimensional scaling,
chapter Geometric representations of relational data. 1977.

[17] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.
J. of the Royal Stat. Soc., B-39, 1977.

[18] M. Wainwright and M Jordan. Graphical models, exponential families and variational inference. Foun-
dations and Trends in Machine Learning, 1(1-2):1–305, 2008.

[19] T. Jebara and A. Pentland. On reversing Jensen’s inequality. In NIPS, 2000.

[20] J. Salojarvi, K Puolamaki, and S. Kaski. Expectation maximization algorithms for conditional likelihoods.
In ICML, 2005.

[21] A. Quattoni, S. Wang, L. P. Morency, M. Collins, and T. Darrell. Hidden conditional random ﬁelds. IEEE
PAMI, 29(10):1848–1852, October 2007.

[22] T. Jaakkola and M. Jordan. Bayesian parameter estimation via variational methods. Statistics and Com-
puting, 10:25–37, 2000.

[23] G. Bouchard. Efﬁcient bounds for the softmax and applications to approximate inference in hybrid mod-
els. In NIPS AIHM Workshop, 2007.

[24] B. Taskar, C. Guestrin, and D. Koller. Max margin Markov networks. In NIPS, 2004.

[25] Y. Qi, M. Szummer, and T. P. Minka. Bayesian conditional random ﬁelds. In AISTATS, 2005.

[26] T. Bromwich and T. MacRobert. An Introduction to the Theory of Inﬁnite Series. Chelsea, 1991.

[27] S. B. Wang, A. Quattoni, L.-P. Morency, and D. Demirdjian. Hidden conditional random ﬁelds for gesture
recognition. In CVPR, 2006.

[28] Y. Wang and G. Mori. Max-margin hidden conditional random ﬁelds for human action recognition. In
CVPR, pages 872–879. IEEE, 2009.

[29] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization for Machine Learning, chapter Convex
optimization with sparsity-inducing norms. MIT Press, 2011.

[30] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In ICML, 2003.

[31] SVN. Vishwanathan, N. Schraudolph, M. Schmidt, and K. Murphy. Accelerated training of conditional
random ﬁelds with stochastic gradient methods. In ICML, 2006.

[32] T. Jebara. Multitask sparsity via maximum entropy discrimination. JMLR, 12:75–110, 2011.

9

Majorization for CRFs and Latent Likelihoods
(Supplementary Material)

Tony Jebara
Department of Computer Science
Columbia University
jebara@cs.columbia.edu

Anna Choromanska
Department of Electrical Engineering
Columbia University
aec2163@columbia.edu

Abstract
This supplement presents additional details in support of the full article. These in-
clude the application of the majorization method to maximum entropy problems.
It also contains proofs of the various theorems, in particular, a guarantee that the
bound majorizes the partition function. In addition, a proof is provided guarantee-
ing convergence on (non-latent) maximum conditional likelihood problems. The
supplement also contains supporting lemmas that show the bound remains ap-
plicable in constrained optimization problems. The supplement then proves the
soundness of the junction tree implementation of the bound for graphical mod-
els with large n. It also proves the soundness of the low-rank implementation of
the bound for problems with large d. Finally, the supplement contains additional
experiments and ﬁgures to provide further empirical support for the majorization
methodology.

Supplement for Section 2
∑
Proof of Theorem 1 Rewrite the partition function as a sum over the integer index j = 1, . . . , n
under the random ordering π : Ω 7→ {1, . . . , n}. This deﬁnes j = π(y) and associates h and f with
∑
−1 (j )) and fj = f (π
−1 (j )). Next, write Z (θ) =
⊤
n
fj ) by introducing
hj = h(π
j=1 αj exp(λ
λ = θ − ˜θ and αj = hj exp( ˜θ
⊤
(
)
fj ). Deﬁne the partition function over only the ﬁrst i components
⊤
i
fj ). When i = 0, a trivial quadratic upper bound holds
as Zi (θ) =
j=1 αj exp(λ
Z0 (θ) ≤ z0 exp
⊤
⊤
1
µ0
(cid:6)0λ + λ
2 λ
with the parameters z0 → 0+ , µ0 = 0, and (cid:6)0 = z0 I. Next, add one term to the current partition
⊤
f1 ). Apply the current bound Z0 (θ) to obtain
function Z1 (θ) = Z0 (θ) + α1 exp(λ
Z1 (θ) ≤ z0 exp( 1
⊤
⊤
⊤
f1 ).
µ0 ) + α1 exp(λ
(cid:6)0λ + λ
2 λ
Consider the following change of variables
0 λ − (cid:6)
(f1 − µ0 ))
−1/2
u = (cid:6)1/2
0
2 (f1 − µ0 )
0 (f1 − µ0 ))
−1
⊤
γ = α1
exp( 1
(cid:6)
z0
and rewrite the logarithm of the bound as
0 (f1 − µ0 ) + λ
2 (f1 − µ0 )
log Z1 (θ) ≤ log z0 − 1
−1
⊤
(cid:6)
Apply Lemma 1 (cf. [32] p. 100) to the last term to get
⊤ (
log Z1 (θ) ≤ log z0 − 1
0 (f1 − µ0 ) + λ
2 (f1 − µ0 )
−1
⊤
(cid:6)
(u − v)
⊤
(u − v)
1
v
1+ γ exp(− 1
∥v∥2 )
I + Γvv
+
2
2

(
(
exp( 1
f1 + log
2
⊤ )
∥v∥2
1
f1 + log
exp
2
(u − v)

∥u∥2 ) + γ
)
)

)

.

⊤

+ γ

(

+

⊤

10

where Γ =

2 log(γ exp(− 1
∥v∥2 )))
1
tanh(
. The bound in [32] is tight when u = v. To achieve tightness
2
2 log(γ exp(− 1
∥v∥2 ))
(
)
(µ0 − f1 ) yielding
−1/2
2
when θ = ˜θ or, equivalently, λ = 0, we choose v = (cid:6)
0
Z1 (θ) ≤ z1 exp
⊤
⊤
1
µ1
(cid:6)1λ + λ
2 λ

where we have

⊤

f1

µ0 +

µ1 =

(cid:6)1 = (cid:6)0 +

(µ0 − f1 )(µ0 − f1 )

z1 = z0 + α1
z0
α1
z0 + α1
z0 + α1
tanh( 1
2 log(α1 /z0 ))
2 log(α1 /z0 )
This rule updates the bound parameters z0 , µ0 , (cid:6)0 to incorporate an extra term in the sum over i in
Z (θ). The process is iterated n times (replacing 1 with i and 0 with i − 1) to produce an overall
) ≤
(
(
)
bound on all terms.
Lemma 1 (See [32] p. 100)
⊤ (
(
(
)
)
⊤ )
For all u ∈ Rd , any v ∈ Rd and any γ ≥ 0, the bound log
∥u∥2
1
+ γ
exp
2
(u − v)
⊤
(u − v)
(u − v)
∥v∥2
v
1
1 + γ exp(− 1
∥v∥2 )
I + Γvv
exp
2
2
2 log(γ exp(−∥v∥2 /2)))
holds when the scalar term Γ = tanh( 1
2 log(γ exp(−∥v∥2 /2))

. Equality is achieved when u = v.

+ γ

log

+

+

1
2

.

′

θ , ϑ =

p(y)f (y) = 0,

RE (p∥h) s.t.

Proof of Lemma 1 The proof is provided in [32].
∑
Supplement for Section 3
Maximum entropy problem We show here that partition functions arise naturally in maximum
entropy estimation or minimum relative entropy RE (p∥h) =
∑
∑
y p(y) log p(y)
h(y) estimation. Consider
the following problem:
p(y)g(y) ≥ 0.
min
(
)
p
y
y
Here, assume that f : Ω 7→ Rd and g : Ω 7→ Rd
are arbitrary (non-constant) vector-valued functions
⊤
⊤
∑
(
)
over the sample space. The solution distribution p(y) = h(y) exp
/Z (θ , ϑ) is
f (y) + ϑ
g(y)
θ
recovered by the dual optimization
− log
arg
max
ϑ≥0,θ
y
where θ ∈ Rd and ϑ ∈ Rd
′
. These are obtained by minimizing Z (θ , ϑ) or equivalently by max-
imizing its negative logarithm. Algorithm 1 permits variational maximization of the dual via the
quadratic program
(cid:6)(β − ˜β) + β
2 (β − ˜β)
⊤
1
min
µ
ϑ≥0,θ
]. Note that any general convex hull of constraints β ∈ (cid:3) ⊆ Rd+d
⊤
⊤
⊤
where β
ϑ
= [θ
imposed without loss of generality.
Proof of Theorem 2 We begin by proving a lemma that will be useful later.
Lemma 2 If κ(cid:9) ≽ (cid:8) ≻ 0 for (cid:8), (cid:9) ∈ Rd×d , then
(cid:8)(θ − ˜θ) − (θ − ˜θ)
2 (θ − ˜θ)
L(θ) = − 1
⊤
⊤
µ
U (θ) = − 1
2 (θ − ˜θ)
(cid:9)(θ − ˜θ) − (θ − ˜θ)
⊤
⊤
µ
κ supθ∈(cid:3) U (θ) for any convex (cid:3) ⊆ Rd , ˜θ ∈ (cid:3), µ ∈ Rd and κ ∈ R+ .
satisfy supθ∈(cid:3) L(θ) ≥ 1

could be

f (y) + ϑ

h(y) exp

g(y)

⊤

⊤

⊤

θ

′

11

⊤
y

z

+

y

y

⊤
A(cid:8)

+y

µ = inf
y≥0

DL = inf
y≥0

Proof of Lemma 2 Deﬁne the primal problems of interest as PL = supθ∈(cid:3) L(θ) and PU =
supθ∈(cid:3) U (θ). The constraints θ ∈ (cid:3) can be summarized by a set of linear inequalities Aθ ≤ b
where A ∈ Rk×d and b ∈ Rk for some (possibly inﬁnite) k ∈ Z. Apply the change of variables
z = θ− ˜θ . The constraint A(z+ ˜θ) ≤ b simpliﬁes into Az ≤ ˜b where ˜b = b−A ˜θ . Since ˜θ ∈ (cid:3), it
is easy to show that ˜b ≥ 0. We obtain the equivalent primal problems PL = supAz≤ ˜b
(cid:8)z −
− 1
⊤
2 z
(cid:9)z − z
− 1
⊤
⊤
⊤
µ. The corresponding dual problems are
µ and PU = supAz≤ ˜b
z
2 z
⊤
⊤
⊤
−1A
−1µ
−1µ+y
⊤˜b+
A(cid:8)
y
(cid:8)
µ
2
2
⊤
−1µ
⊤
−1A
⊤˜b+
⊤
−1µ+y
(cid:9)
µ
A(cid:9)
A(cid:9)
DU = inf
+y
.
y≥0
2
2
Due to strong duality, PL = DL and PU = DU . Apply the inequalities (cid:8) ≼ κ(cid:9) and y
⊤ ˜b > 0 as
⊤
−1µ
⊤
⊤
−1A
⊤
−1µ
(cid:9)z − z
− κ
PL ≥ sup
⊤˜b +
⊤
⊤
(cid:9)
µ
A(cid:9)
y
A(cid:9)
y
y
+ y
Az≤ ˜b
2
2κ
κ
2κ
≥ 1
1
PU .
DU =
κ
κ
This proves that PL ≥ 1
κ PU .
We will use the above to prove Theorem 2. First, we will upper-bound (in the Loewner ordering
sense) the matrices (cid:6)j in Algorithm 2. Since ∥fxj (y)∥2 ≤ r for all y ∈ Ωj and since µj in
Algorithm 1 is a convex combination of fxj (y), the outer-product terms in the update for (cid:6)j satisfy
(fxj (y) − µ)(fxj (y) − µ)
⊤ ≼ 4r2 I.
Thus, (cid:6)j ≼ F (α1 , . . . , αn )4r2 I holds where
n∑
αi∑
tanh( 1
αi∑
2 log(
i−1
F (α1 , . . . , αn ) =
k=1 αk
)
2 log(
i−1
k=1 αk
i=2
using the deﬁnition of α1 , . . . , αn in the proof of Theorem 1. The formula for F starts at i = 2 since
z0 → 0+ . Assume permutation π is sampled uniformly at random. The expected value of F is then
∑
n∑
∑
α(cid:25)(i)
tanh( 1
))
2 log(
∑
i−1
Eπ [F (α1 , . . . , αn )] =
1
k=1 α(cid:25)(k)
.
α(cid:25)(i)
n!
)
2 log(
i−1
π
i=2
k=1 α(cid:25)(k)
We claim that the expectation is maximized when all αi = 1 or any positive constant. Also, F
is invariant under uniform scaling of its arguments. Write the expected value of F as E for short.
at the setting αi = 1, ∀i. Due to the expectation over π , we have ∂E
Next, consider ∂E
= ∂E
for any l, o. Therefore, the gradient vector is constant when all αi = 1. Since F (α1 , . . . , αn )
∂αo
∂αl
∂αl
is invariant to scaling, the gradient vector must therefore be the all zeros vector. Thus, the point
when all αi = 1 is an extremum or a saddle. Next, consider
for any l, o. At the setting
∂
∂E
∂αo
∂αl
= c(n)/(n − 1) for some non-negative constant function
= −c(n) and,
αi = 1, ∂ 2E
∂E
∂
∂α2
∂αl
∂αo
(
)
l
c(n). Thus, the αi = 1 extremum is locally concave and is a maximum. This establishes that
n−1∑
Eπ [F (α1 , . . . , αn )] ≤ Eπ [F (1, . . . , 1)] and yields the Loewner bound
(cid:6)j ≼
tanh(log(i)/2)
log(i)
∑
i=1
Apply this bound to each (cid:6)j in the lower bound on J (θ) and also note a corresponding upper bound
∑
(θ − ˜θ)
∥θ − ˜θ∥2−
J (θ) ≥ J ( ˜θ)−tω+tλ
(µj − fxj (yj ))
⊤
2
j
J (θ) ≤ J ( ˜θ)−tλ
(µj − fxj (yj ))
∥θ − ˜θ∥2−
(θ − ˜θ)
⊤
2
j

2r2

I = ωI.

))

12

which follows from Jensen’s inequality. Deﬁne the current ˜θ at time τ as θτ and denote by Lτ (θ) the
above lower bound and by Uτ (θ) the above upper bound at time τ . Clearly, Lτ (θ) ≤ J (θ) ≤ Uτ (θ)
with equality when θ = θτ . Algorithm 2 maximizes J (θ) after initializing at θ0 and performing
an update by maximizing a lower bound based on (cid:6)j . Since Lτ (θ) replaces the deﬁnition of (cid:6)j
with ωI ≽ (cid:6)j , Lτ (θ) is a looser bound than the one used by Algorithm 2. Thus, performing
θτ +1 = arg maxθ∈(cid:3) Lτ (θ) makes less progress than a step of Algorithm 1. Consider computing the
slower update at each iteration τ and returning θτ +1 = arg maxθ∈(cid:3) Lτ (θ). Setting (cid:8) = (tω + tλ)I,
(cid:9) = tλI and κ = ω+λ
λ allows us to apply Lemma 2 as follows
Uτ (θ) − Uτ (θτ ).
Lτ (θ) − Lτ (θτ ) =
1
sup
sup
θ∈(cid:3)
θ∈(cid:3)
κ
(
)
Since Lτ (θτ ) = J (θτ ) = Uτ (θτ ), J (θτ +1 ) ≥ supθ∈(cid:3) Lτ (θ) and supθ∈(cid:3) Uτ (θ) ≥ J (θ
obtain
1 − 1
κ
)τ
(
Iterate the above inequality starting at t = 0 to obtain
1 − 1
) ≥
J (θτ ) − J (θ
κ

)τ or log(1/ϵ) = τ log κ
(
(J (θ0 ) − J (θ
∗
)) .
⌈
⌉
1 − 1
⌈
⌉
A solution within a multiplicative factor of ϵ implies that ϵ =
κ−1 .
κ
log(1/ϵ)
Inserting the deﬁnition for κ shows that the number of iterations τ is at most
or
log (cid:20)
(cid:20)−1

J (θτ +1 ) − J (θ

(J (θτ ) − J (θ

∗

) ≥

∗

)) .

∗

), we

∗

log(1/ϵ)
log(1+λ/ω)

. Inserting the deﬁnition for ω gives the bound.

Y 2,0
1

Y 1,1
1

Y 1,1
2

Y 1,1
3

· · ·

Y 1,1
m1;1

Figure 3: Junction tree of depth 2.

Algorithm 5 SmallJunctionTree
Input Parameters ˜θ and h(u), f (u) ∀u ∈ Y 2,0
∏m1;1
Initialize z → 0+ , µ = 0, (cid:6) = z I
1
∑m1;1
For each conﬁguration u ∈ Y 2,0
{
∑m1;1
i=1 zi exp(− ˜θ
⊤
⊤
1
µi )) exp( ˜θ
α = h(u)(
i=1 µi − µ
l = f (u) +
i=1 (cid:6)i + tanh( 1
2 log(α/z))
(cid:6) + =
2 log(α/z)
µ + = α
z+α l
}
z + = α
Output z , µ, (cid:6)

⊤

ll

and zi , (cid:6)i , µi ∀i = 1, . . . , m1,1
∑m1;1
⊤
i=1 µi )) = h(u) exp( ˜θ

(f (u) +

∏m1;1
i=1 zi

f (u))

Supplement for Section 5
∑
∑
Proof of correctness for Algorithm 3 Consider a simple junction tree of depth 2 shown on Figure 3.
The notation Y a,b
refers to the cth tree node located at tree level a (ﬁrst level is considered as the one
c
with tree leaves) whose parent is the bth from the higher tree level (the root has no parent so b = 0).
refer to the sum over all conﬁgurations of variables in Y a1 ,b1
Let
and
\Y a2 ;b2
Y a1 ;b1
Y a1 ;b1
c1
c1
c1
c2
refers to the sum over all conﬁgurations of variables that are in Y a1 ,b1
but not in Y a2 ,b2
. Let ma,b
c1
c2
denote the number of children of the bth node located at tree level a + 1. For short-hand, use
⊤
f (Y )). The partition function can be expressed as:
ψ(Y ) = h(Y ) exp(θ

13

Y 1;1
1

Y 1;2
2

Y 1;1
2

Y 1;m2;1
2

Z (θ) =

Y 1;m2;1
1

Y 2;1
1
· · · Y 1;1
m1;1
ψ(u)
[
[
ψ(u)

· · · Y 1;2
Y 1;2
m1;2
1
Figure 4: Junction tree of depth 3.
 ∑

m1;1∏
(
m1;1∏
ψ(v)
\Y 2;0
v∈Y 1;1
i=1
1
i
(cid:6)i (θ − ˜θ) + (θ − ˜θ)
θ − ˜θ)
⊤
⊤
1
(
m1;1∏
2
i=1
i=1

∑
∑
u∈Y 2;0
1
∑
u∈Y 2;0
1
(θ − ˜θ)
⊤
zi exp
h(u) exp(θ
u∈Y 2;0
1
)
(
(
[
(
where the upper-bound is obtained by applying Theorem 1 to each of the terms
m1;1∏
m1;1∑
∑
By simply rearranging terms we get:
zi exp(− ˜θ
Z (θ) ≤
⊤
⊤
(
(
)
m1;1∑
f (u) +
θ
µi )
h(u)
u∈Y 2;0
i=1
i=1
1
(θ − ˜θ)
i=1

(θ − ˜θ)

)]

(cid:6)i (θ − ˜θ) + (θ − ˜θ)
∑
))
v∈Y 1;1
i
)]
µi

zi exp(

f (u))

exp

1
2

≤

=

µi

⊤

1
2

· · ·

Y 1;m2;1
m1;m2;1

)]

⊤

µi

\Y 2;0
1

ψ(v).

Y 3;0
1

Y 2;1
2

· · ·

Y 2;1
m2;1

⊤

.

(cid:6)i

Z (θ) =

exp
)
(
expression
by
upper-bounded
be
can
this
prove
can
One
that
2 (θ − ˆθ)
(cid:6)(θ − ˆθ) + (θ − ˆθ)
⊤
⊤
where z , (cid:6) and µ can be computed using Algo-
1
µ
z exp
rithm 5 (a simpliﬁcation of Algorithm 3). We will call this result Lemma A. The proof is similar to
ψ(u)
 .



ψ(v)
 ∑
 ∑
the proof of Theorem 1 so is not repeated here.
∑
m2;1∏
m1;i∏
Consider enlarging the tree to a depth 3 as shown on Figure 4. The partition function is now
(∑
))
(
∏m1;i
∑
u∈Y 3;0
\Y 3;0
v∈Y 2;1
\Y 2;1
w∈Y 1;i
i=1
j=1
(
)
1
1
i
j
i
By Lemma A we can upper bound each
v∈Y 2;1
\Y 3;0
w∈Y 1;i
\Y 2;1
ψ(w)
ψ(v)
)]
[
j=1
(
1
i
i
j
(cid:6)i (θ − ˆθ) + (θ − ˆθ)
2 (θ − ˆθ)
∑
m2;1∏
⊤
⊤
by the expression zi exp
. This yields
1
µi
(cid:6)i (θ − ˜θ) + (θ − ˜θ)
(θ − ˜θ)
⊤
⊤
u∈Y 3;0
i=1
1
This process can be viewed as collapsing the sub-trees S 2,1
1 , S 2,1
2 , . . ., S 2,1
m2;1 to super-nodes that
are represented by bound parameters, zi , (cid:6)i and µi , i = {1, 2, · · · , m2,1 }, where the sub-trees are

Z (θ) ≤

zi exp

term

ψ(w)

ψ(u)

1
2

µi

.

14

deﬁned as:

S 2,1
1
S 2,1
2
...
S 2,1
m2;1

= {Y 2,1
1
= {Y 2,1
2

, Y 1,1
1
, Y 1,2
1

, Y 1,1
2
, Y 1,2
2

, Y 1,1
3
, Y 1,2
3

, . . . , Y 1,1
m1;1
, . . . , Y 1,2
m1;2

}
}

, Y 1,m2;1
3

, Y 1,m2;1
2

, Y 1,m2;1
1

= {Y 2,1
}.
(
)
, . . . , Y 1,m2;1
m2;1
m1;m2;1
Notice that the obtained expression can be further upper bounded using again Lemma A (induction)
(cid:6)(θ − ˆθ) + (θ − ˆθ)
2 (θ − ˆθ)
⊤
⊤
.
yielding a bound of the form: z exp
1
µ
Finally, for a general tree, follow the same steps described above, starting from leaves and collapsing
nodes to super-nodes, each represented by bound parameters. This procedure effectively yields
Algorithm 3 for the junction tree under consideration.
Supplement for Section 6
Proof of correctness for Algorithm 4 We begin by proving a lemma that will be useful later.
2
 d∑
Lemma 3 For all x ∈ Rd and for all l ∈ Rd ,
d∑
√∑
l(i)2
x(i)
i=1
i=1
(
)2
⇐⇒ d∑
d∑
d∑
Proof of Lemma 3 By Jensen’s inequality,
∑
l(i)2∑
x(i)l(i)2
d
d
j=1 l(j )2
j=1 l(j )2
i=1
i=1
i=1

.
 d∑
i=1

√∑
x(i)l(i)2
d
j=1 l(j )2

x(i)2 l(i)2 ≥

x(i)2 l(i)2 ≥

2

d
j=1 l(j )2

.

x(i)2

≥

Now we prove the correctness of Algorithm 4. At the ith iteration, the algorithm stores (cid:6)i using
i SiVi + Di where Vi ∈ Rk×d is orthonormal, Si ∈ Rk×k positive
⊤
a low-rank representation V
semi-deﬁnite and Di ∈ Rd×d is non-negative diagonal. The diagonal terms D are initialized to tλI
∑
where λ is the regularization term. To mimic Algorithm 1 we must increment the (cid:6) matrix by a
⊤
rank one update of the form (cid:6)i = (cid:6)i−1 + ri r
i . By projecting ri onto each eigenvector in V, we
j=1 Vi−1 (j, ·)riVi−1 (j, ·)
⊤
⊤
k
i−1Vi−1 ri + g where g is the
can decompose it as ri =
+ g = V
remaining residue. Thus the update rule can be rewritten as:
⊤
⊤
⊤
⊤
⊤
i−1Vi−1 ri + g)
i−1Vi−1 ri + g)(V
i−1Si−1Vi−1 + Di−1 + (V
(cid:6)i = (cid:6)i−1 + ri r
i = V
⊤
′⊤
⊤
⊤
⊤
⊤
′
′
i−1 (Si−1 + Vi−1 ri r
+ Di−1
i−1 )Vi−1 + Di−1 + gg
i−1V
i−1S
i−1 + gg
= V
i V
= V
′
i−1 = Qi−1Vi−1 and deﬁned Qi−1 in terms of the singular value decomposition,
where we deﬁne V
⊤
⊤
⊤
′
′
i−1Qi−1 = svd(Si−1 + Vi−1 ri r
i−1 ). Note that S
i−1 is diagonal and nonnegative by
i−1S
Q
i V
construction. The current formula for (cid:6)i shows that we have a rank (k + 1) system (plus diagonal
term) which needs to be converted back to a rank k system (plus diagonal term) which we denote by
′
i . We have two options as follows.
(cid:6)
Case 1) Remove g from (cid:6)i to obtain
i−1 + Di−1 = (cid:6)i − gg
′⊤
′
′
′
i−1V
i−1S
(cid:6)
i = V
where c = ∥g∥2 and v = 1∥g∥ g.
′
Case 2) Remove the mth (smallest) eigenvalue in S
i−1 and its corresponding eigenvector:
(m, ·) = (cid:6)i − cvv
(m, ·)
⊤ − S
⊤
′⊤
′
′
′
′
′
′
i−1 + Di−1 + gg
i−1S
i−1V
i = V
(m, m) and v = V(m, ·)
′
′

= (cid:6)i − cvv

where c = S

(m, m)V

V

(cid:6)

⊤

⊤

⊤

.

15

⊤

⊤

⊤

x ≤ x
⊤

Fx ⇐⇒ c

cvv

.

⊤
x

need

x(i)2F(i)

i x ≥ x
′′
(cid:6)

⊤ where c ≥ 0 and
′
Clearly, both cases can be written as an update of the form (cid:6)
i = (cid:6)i + cvv
⊤
v = 1. We choose the case with smaller c value to minimize the change as we drop from a system
v
of order (k + 1) to order k . Discarding the smallest singular value and its corresponding eigenvector
Instead, consider absorbing this term into the diagonal component to
would violate the bound.
′′
′
)2
(
preserve the bound. Formally, we look for a diagonal matrix F such that (cid:6)
i + F which also
i = (cid:6)
(cid:6)ix for all x ∈ Rd . Thus, we want to satisfy:
i x ≥ x
⊤
⊤
≤ d∑
d∑
′′
maintains x
(cid:6)
(cid:6)ix ⇐⇒ x
x(i)v(i)
i=1
i=1
(∑
)2
where, for ease of notation, we take F(i) = F(i, i).
∑
1. Consider the case where v ≥ 0 though we will soon get rid of
⊤
′
w v where w = v
Deﬁne v
= 1
(∑
)2
∑
i=1 x(i)2F(i) ≥ c
this assumption. We need an F such that
d
d
. Equivalently, we
i=1 x(i)v(i)
(∑
)2
∑
cw2 ≥
′
′
d
d
= F(i)
i=1 x(i)2 F(i)
. Deﬁne F(i)
cw2 for all i = 1, . . . , d. So, we need
i=1 x(i)v(i)
′ ≥
′
′
∑
d
d
. Using Lemma 3 it is easy to show that we
such that
an F
i=1 x(i)2F(i)
i=1 x(i)v(i)
= cwv(i). Therefore, for all x ∈ Rd ,
′
′
′
. Thus, we obtain F(i) = cw2F(i)
may choose F
(i) = v(i)
(
)2
all v ≥ 0, and for F(i) = cv(i)
d∑
d∑
d
j=1 v(j ) we have
x(i)v(i)
sufﬁcient to set F(i) = c|v(i)| ∑
i=1
i=1
To generalize the inequality to hold for all vectors v ∈ Rd with potentially negative entries, it is
|v(j )|. To verify this, consider ﬂipping the sign of any v(i).
d
j=1
The left side of the Inequality 3 does not change. For the right side of this inequality, ﬂipping the
for F(i) = c|v(i)| ∑
sign of v(i) is equivalent to ﬂipping the sign of x(i) and not changing the sign of v(i). However, in
this case the inequality holds as shown before (it holds for any x ∈ Rd ). Thus for all x, v ∈ Rd and
|v(j )|, Inequality 3 holds.
d
j=1
Supplement for Section 7
Small scale experiments In additional small-scale experiments, we compared Algorithm 2 with
steepest descent (SD), conjugate gradient (CG), BFGS and Newton-Raphson. Small-scale problems
may be interesting in real-time learning settings, for example, when a website has to learn from a
user’s uploaded labeled data in a split second to perform real-time retrieval. We considered logistic
regression on ﬁve UCI data sets where missing values were handled via mean-imputation. A range of
regularization settings λ ∈ {100 , 102 , 104 } was explored and all algorithms were initialized from the
same ten random start-points. Table 3 shows the average number of seconds each algorithm needed
to achieve the same solution that BFGS converged to (all algorithms achieve the same solution due
to concavity). The bound is the fastest algorithm as indicated in bold.
data|λ a|100 a|102 a|104 b|100 b|102 b|104 c|100 c|102 c|104 d|100 d|102 d|104 e|100 e|102 e|104
3.28 2.63 2.01 1.49
2.45 3.14 2.00 1.60 4.09 1.03 1.90 5.62
0.89
1.90
BFGS
2.88
1.94 2.68 2.49 1.54
1.60 2.18 6.17 5.83 1.92 0.64 0.56 12.04 1.27
0.92
1.74
SD
CG
0.78
0.83
0.85 0.70 0.67 0.83 0.65 0.64 0.72 1.36
1.21
1.23 0.48 0.55 0.43
0.60 0.35 0.26 0.20
0.63
0.22 0.43 0.37 0.35 0.39 0.34 0.32 0.92
0.25
Newton 0.31
0.01
0.01
0.01 0.07 0.04 0.04 0.07 0.02 0.02 0.16
0.09
0.07 0.03 0.03 0.03
Bound

x(i)2F(i) ≥ c

(3)

Table 3:
Convergence time in seconds under various regularization levels for a) Bupa (t =
345, dim = 7), b) Wine (t = 178, dim = 14), c) Heart (t = 187, dim = 23), d) Ion
(t = 351, dim = 34), and e) Hepatitis (t = 155, dim = 20) data sets.

Inﬂuence of rank k on bound performance in large scale experiments We also examined the
inﬂuence of k on bound performance and compared it with LBFGS, SD and CG. Several choices

16

of k were explored. Table 4 shows results for the SRBCT data-set. In general, the bound performs
best but slows down for superﬂuously large values of k . Steepest descent and conjugate gradient
are slow yet obviously do not vary with k . Note that each iteration takes less time with smaller k
for the bound. However, we are reporting overall runtime which is also a function of the number of
iterations. Therefore, total runtime (a function of both) may not always decrease/increase with k .

64
32
16
8
4
2
1
k
LBFGS 1.37 1.32 1.39 1.35 1.46 1.40 1.54
8.80 8.80 8.80 8.80 8.80 8.80 8.80
SD
CG
4.39 4.39 4.39 4.39 4.39 4.39 4.39
0.56 0.56 0.67 0.96 1.34 2.11 4.57
Bound

Table 4: Convergence time in seconds as a function of k .

Additional latent-likelihood results For completeness, Figure 5 depicts two additional data-sets
to complement Figure 2. Similarly, Table 5 shows all experimental settings explored in order to
provide the summary Table 2 in the main article.

Figure 5: Convergence of test latent log-likelihood for bupa and wine data-sets.

Data-set
ion
bupa
hepatitis
Algorithm m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4
-4.96 -5.55 -5.88 -5.79 -22.07 -21.78 -21.92 -21.87 -4.42 -5.28 -4.95 -4.93
BFGS
-11.80 -9.92 -5.56 -8.59 -21.76 -21.74 -21.73 -21.83 -4.93 -5.14 -5.01 -5.20
SD
-5.47 -5.81 -5.57 -5.22 -21.81 -21.81 -21.81 -21.81 -4.84 -4.84 -4.84 -4.84
CG
Newton
-5.95 -5.95 -5.95 -5.95 -21.85 -21.85 -21.85 -21.85 -5.50 -5.50 -5.50 -4.50
-6.08 -4.84 -4.18 -5.17 -21.85 -19.95 -20.01 -19.97 -5.47 -4.40 -4.75 -4.92
Bound

SRBCT
wine
Data-set
Algorithm m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4
-0.90 -0.91 -1.79 -1.35 -5.99 -6.17 -6.09 -6.06
BFGS
-1.61 -1.60 -1.37 -1.63 -5.61 -5.62 -5.62 -5.61
SD
-0.51 -0.78 -0.95 -0.51 -5.62 -5.49 -5.36 -5.76
CG
Newton
-0.71 -0.71 -0.71 -0.71 -5.54 -5.54 -5.54 -5.54
-0.51 -0.51 -0.48 -0.51 -5.31 -5.31 -4.90 -0.11
Bound
Table 5: Test latent log-likelihood at convergence for different values of m ∈ {1, 2, 3, 4} on ion,
bupa, hepatitis, wine and SRBCT data-sets.

17

−50510−24−23−22−21−20−19bupalog(Time) [sec]−log(J(θ))−4−202468−20−15−10−50winelog(Time) [sec]−log(J(θ))  BoundNewtonBFGSConjugate gradientSteepest descent
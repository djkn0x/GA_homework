Coding efﬁciency and detectability of rate ﬂuctuations
with non-Poisson neuronal ﬁring

Shinsuke Koyama∗
Department of Statistical Modeling
The Institute of Statistical Mathematics
10-3 Midori-cho, Tachikawa, Tokyo 190-8562, Japan
skoyama@ism.ac.jp

Abstract
Statistical features of neuronal spike trains are known to be non-Poisson. Here, we
investigate the extent to which the non-Poissonian feature affects the efﬁciency of
transmitting information on ﬂuctuating ﬁring rates. For this purpose, we introduce
the Kullback-Leibler (KL) divergence as a measure of the efﬁciency of informa-
tion encoding, and assume that spike trains are generated by time-rescaled renewal
processes. We show that the KL divergence determines the lower bound of the de-
gree of rate ﬂuctuations below which the temporal variation of the ﬁring rates is
undetectable from sparse data. We also show that the KL divergence, as well as the
lower bound, depends not only on the variability of spikes in terms of the coefﬁ-
cient of variation, but also signiﬁcantly on the higher-order moments of interspike
interval (ISI) distributions. We examine three speciﬁc models that are commonly
used for describing the stochastic nature of spikes (the gamma, inverse Gaussian
(IG) and lognormal ISI distributions), and ﬁnd that the time-rescaled renewal pro-
cess with the IG distribution achieves the largest KL divergence, followed by the
lognormal and gamma distributions.

1 Introduction
Characterizing the statistical features of spike time sequences in the brain is important for under-
standing how the brain represents information about stimuli or actions in the sequences of spikes.
Although the spike trains recorded from in vivo cortical neurons are known to be highly irregu-
lar [20, 24], a recent non-stationary analysis has revealed that individual neurons signal with non-
Poisson ﬁring, the characteristics of which are strongly correlated with the function of the cortical
area [21].
This raises the question of what the neural coding advantages of non-Poisson spiking are. It could be
that the precise timing of spikes carries additional information about the stimuli or actions [6, 15]. It
is also possible that the efﬁciency of transmitting ﬂuctuating rates might be enhanced by non-Poisson
ﬁring [5, 17]. Here, we explore the latter possibility.
In the problem of estimating ﬁring rates, there is a minimum degree of rate ﬂuctuation below which
a rate estimator cannot detect the temporal variation of the ﬁring rate [23]. If, for instance, the degree
of temporal variation of the rate is on the same order as that of the noise, a constant rate might be
chosen as the most likely estimate for a given spike train. It is, therefore, interesting to see how the
minimum degree of rate ﬂuctuation depends on the non-Poissonian feature of spike trains.
In this study, we investigate the extent to which the non-Poissonian feature of spike trains affects
the encoding efﬁciency of rate ﬂuctuations. In addition, we address the question of how the de-
∗http://skoyama.blogspot.jp

1

tectability of rate ﬂuctuations depends on the encoding efﬁciency. For this purpose, we introduce
the Kullback-Leibler (KL) divergence to measure the encoding efﬁciency, and assume that spike se-
quences are generated by time-rescaled renewal processes. With the aid of analytical and numerical
studies, we suggest that the lower bound of detectable rate ﬂuctuations, below which the empirical
Bayes decoder cannot detect the rate ﬂuctuations, is uniquely determined by the KL divergence. By
examining three speciﬁc models (the time-rescaled renewal process with the gamma, inverse Gaus-
sian (IG) and lognormal interspike interval (ISI) distributions), it is shown that the KL divergence,
as well as the lower bound, depends not only on the ﬁrst- and second-order moments, but also sig-
niﬁcantly on the higher-order moments of the ISI distributions. We also ﬁnd that among the three
ISI distributions, the IG distribution achieves the highest efﬁciency of coding information on rate
ﬂuctuations.

2 Encoding rate ﬂuctuations using time-rescaled renewal processes
Deﬁnitions of time-rescaled renewal processes and KL divergence
We introduce time-rescaled renewal processes for a model of neuronal spike trains constructed in the
following way. Let fκ(y ) be a family of ISI distributions with the unit mean (i.e., ! ∞
0 yfκ(y )dy =
1), where κ controls the shape of the distribution, and λ(t) be a ﬂuctuating ﬁring rate. A sequence of
spikes {ti} := {t1 , t2 , . . . , tn} is generated in the following steps: (i) Derive ISIs {y 1 , y2 , . . . , yn}
independently from f κ (y ), and arrange the ISIs sequentially to form a spike train of the unit rate;
ith spike is given by summing the previous ISIs as s i = "i
j=1 yj . (ii) Transform {s 1 , s2 , . . . , sn}
to {t1 , t2 , . . . , tn} according to t i =Λ −1 (si ), where Λ−1 (si ) is the inverse of the function Λ(t) =
! t
0 λ(u)du. This transformation ensures that the instantaneous ﬁring rate of {t i} corresponds to λ(t),
while the shape of the ISI distribution f κ (y ), which characterizes the ﬁring irregularity, is unchanged
in time. This is in agreement with the empirical fact that the degree of irregularity in neuronal ﬁring
is generally maintained in cortical processing [21, 22], while the ﬁring rate λ(t) changes in time.
The probability density of the occurrence of spikes at {t i} is, then, given by
n#i=1
λ(ti )fκ (Λ(ti ) − Λ(ti−1 )).
pκ ({ti }|{λ(t)}) =
where t0 = 0.
We next introduce the KL divergence for measuring the encoding efﬁciency of ﬂuctuating rates. For
this purpose, we assume that λ(t) is ergodic with a stationary distribution p(λ), the mean of which
is given by µ:
"λ#λ := $ ∞
T $ T
1
(2)
λp(λ)dλ = lim
λ(t)dt = µ.
T →∞
0
0
Consider a probability density of a renewal process that has the same ISI density f κ (x) and the
constant rate µ:
n#n=1
µfκ(µ(ti − ti−1 )).
pκ ({ti}|µ) =
The KL divergence between p κ ({ti}|{λ(t)}) and pκ ({ti}|µ) is, then, deﬁned as
T $ T
t1 · · · $ T
0 $ T
∞%n=0
1
:= lim
pκ({ti }|{λ(t)})
Dκ (λ(t)||µ)
T →∞
tn−1
× log pκ ({ti}|{λ(t)})
dt1dt2 · · · dtn .
pκ ({ti}|µ)
Since it is deﬁned as the entropy of a renewal process with the ﬂuctuating rate λ(t) relative to that
with the constant rate µ, Dκ (λ(t)||µ) can be interpreted as the amount of information on the rate
ﬂuctuations encoded into spike trains. Note that a similar quantity has been introduced in [3], where
the quantity was computed only under a Poisson model.

(4)

(1)

(3)

2

Substituting Eqs. (1) and (3) into Eq. (4) and further assuming ergodicity of spike trains, the KL
divergence can be expressed as

:

Dκ (λ(t)||µ) = lim
n→∞

1
log pκ ({ti}|{λ(t)})
pκ ({ti}|µ)
tn − t0
n%i=1 & log λ(ti ) + log fκ (Λ(ti ) − Λ(ti−1 ))
1
= lim
tn − t0
n→∞
− log µ − log fκ (µ(ti − ti−1 ))’.
(5)
This expression can be used for computing the KL divergence numerically by simulating a large
number of spikes n % 1.
Three ISI distributions and their KL divergence
In order to examine the behavior of the KL divergence, we use the three speciﬁc ISI distributions
for fκ (y ) (the gamma, inverse Gaussian (IG) and lognormal distributions), which have been used to
describe the stochastic nature of ISIs [9, 10, 14]. These distributions and their coefﬁcient of variation
(CV = (V ar(X )/E (X )) are given by
fκ(y ) = κκ yκ−1 e−κy /Γ(κ), CV = 1/√κ,
gamma :
2πy 3 exp * −
+, CV = 1/√κ,
fκ(y ) = ) κ
κ(y − 1)2
IG :
2y
exp * −
+, CV = √eκ − 1,
2 )2
(log y + κ
1
fκ(y ) =
lognormal
y√2πκ
2κ
where Γ(κ) = ! ∞
0 xκ−1 e−xdx is the gamma function. Figure 1a illustrates the shape of the three
distributions with three different values of C V .
The KL divergence for the three models is analytically solvable when the rate ﬂuctuation has a long
time scale relative to the mean ISI. Here, we show the derivation for the gamma distribution. (The
derivations for the IG and lognormal distributions are essentially the same.) Inserting Eq. (6) into
Eq. (5) leads to
n%i=1 & log λ(ti ) + (κ − 1) log[Λ(ti ) − Λ(ti−1 )]
1
Dκ(λ(t)||µ) = lim
tn − t0
n→∞
− (κ − 1) log(ti − ti−1 )’ − κµ log µ,
(9)
tn−t0 ! tn
where we used
tn−t0 → µ as n → ∞. By introducing the “averaged”
λ(t)dt → µ and
1
n
t0
, we obtain log[Λ(ti ) − Λ(ti−1 )] = log ¯λi + log(ti −
ﬁring rate in the ith ISI: ¯λi := Λ(ti )−Λ(ti−1 )
ti−ti−1
ti−1 ). Assuming that the time scale of the rate ﬂuctuation is longer than the mean ISI so that ¯λi is
approximated to λ(t i ), Eq. (9) becomes
n%i=1
1
Dκ (λ(t)||µ) = κ lim
log λ(ti ) − κµ log µ
tn − t0
n→∞
= κ, lim
δ(t − ti ) log λ(t)dt − µ log µ-.
T $ T
0 %i
1
T →∞
The ﬂuctuation in the apparent spike count is given by the variance to mean ratio as represented
by the Fano factor [8]. For the renewal process in which ISIs are drawn from a given distribution
function, it is proven that the Fano factor is related to the ISI variability with F ≈ C 2
V [4]. Thus, for
a long range time scale in which a serial correlation of spikes is negligible, the spike train in Eq. (10)
can be approximated to
n%i=1
δ(t − ti ) ≈ λ(t) + (λ(t)/κξ (t),
3

(10)

(6)
(7)

(8)

(11)

where ξ (t) is a ﬂuctuating process such that "ξ (t)# = 0 and "ξ (t)ξ (t % )# = δ(t − t% ). Using this, the
ﬁrst term on the rhs of (10) can be evaluated as
T $ T
T $ T
0 (λ(t)/κ log λ(t)ξ (t)dt = "λ log λ#λ ,
1
1
(12)
lim
λ(t) log λ(t)dt + lim
T →∞
T →∞
0
where the second term on the lhs has vanished due to a property of stochastic integrals. Therefore,
the KL divergence of the gamma distribution is obtained as
Dκ (λ(t)||µ) = κ&"λ log λ#λ − µ log µ’.
(13)
In the same way, the KL divergence for the IG and lognormal distributions are, respectively, derived
as
1
2 "λ log λ#λ + κ + 1
2µ "(λ − µ)2 #λ ,

Dκ(λ(t)||µ) = µ
2

log µ −

(14)

and

ξ (t),

(17)

(15)

= −

1
log µ
Dκ (λ(t)||µ) = µ
2κ "λ(log λ)2 #λ .
κ "λ log λ#λ +
(log µ)2 −
2κ
See the supplementary material for the details of their derivations.
Results
We compute the KL divergence for the three models, in which the rate ﬂuctuates according to the
Ornstein-Uhlenbeck process. Formally, the rate process is given by λ(t) = [x(t)] + , where [·]+ is
the rectiﬁcation function:
[x]+ = , x, x > 0
(16)
0,
otherwise
and x(t) is derived from the Ornstein-Uhlenbeck process:
+ σ) 2
dx(t)
x(t) − µ
dt
τ
τ
where ξ (t) is the Gaussian white noise.
Figure 1b depicts the KL divergence as a function of σ for C V =0.6, 1 and 1.5. The analytical results
(the solid lines) are in good agreement with the numerical results (the error bars). The KL divergence
for the three models increases as σ is increased and as C V is decreased, which is rather obvious
since larger σ and smaller CV imply lower noise entropy of spike trains. One nontrivial result is
that, even if the three models share the same values of σ and C V , the KL divergence of each model
signiﬁcantly differs from that of the others: the IG distribution achieves the largest KL divergence,
followed by the lognormal and gamma distributions. The difference in the KL divergence among
the three models becomes larger as C V grows larger. Since the three models share the same ﬁring
rate λ(t) and CV , it can be concluded that the higher-order (more than second-order) moments of
ISI distributions strongly affect the KL divergence.
In order to conﬁrm this result for another rate process, we examine a sinusoidal rate process, λ(t) =
µ + σ sin t/τ , and observe the same behavior as the Ornstein-Uhlenbeck rate process (Figure 1c).
3 Decoding ﬂuctuating rates using the empirical Bayes method
In this section, we show that the KL divergence (4) determines the lower bound of the degree of rate
ﬂuctuation below which the empirical Bayes estimator cannot detect rate ﬂuctuations.
The empirical Bayes method
We consider decoding a ﬂuctuation rate λ(t) from a given spike train {t i } := {t1 . . . , tn} in an
observation interval [0, T ] by the empirical Bayes method. Let x(t) ∈ R be a latent variable that
4

CV=0.6

CV=1

CV=1.5

gamma
IG
lognormal

CV=0.6

CV=1

CV=1.5

CV=0.6

CV=1

CV=1.5

(c)
0.4

e
c
n
e
g
r
e
v
i
d
 
L
K

0.3

0.2

0.1

0

0

(a)

(b)
0.15

e
c
n
e
g
r
e
v
i
d
 
L
K

0.1

0.05

0

0

0.1

0.2
σ

0.3

0.2

0.4

σ

0.6

0.8

Figure 1: (a) The gamma (blue), IG (green) and lognormal (red) ISI distribution functions for
CV =0.6, 1 and 1.5. (b) The KL divergence as a function of σ for C V =0.6, 1 and 1.5, when the rate
ﬂuctuates according to the Ornstein-Uhlenbeck process (17) with µ = 1 and τ = 10. The blue, green
and red indicate the KL divergence for the gamma, IG and lognormal distribution, respectively. The
lines represent the theoretical values obtained by Eqs. (13), (14) and (15), and the error bars repre-
sent the average and standard deviation numerically computed according to Eq. (5) with n = 50, 000
and 10 trials. (c) The KL divergence for the sinusoidally modulated rate, λ(t) = µ + σ sin t/τ , with
µ = 1 and τ = 10.

(18)

is transformed from λ(t) via the log-link function x(t) = log λ(t). For the inference of λ(t) from
{ti}, we use a prior distribution of x(t), such that the large gradient of x(t) is controlled by
pγ ({x(t)}) ∝ exp . −
dt1,
0 / dx(t)
dt 02
2γ 2 $ T
1
where the hyperparameter γ controls the roughness of the latent process x(t): with the small γ , the
model requires a constant latent process, and vice versa. By inverting the conditional probability
distribution with the Bayes’ theorem, the posterior distribution of {x(t)} is obtained as
pκ,γ ({x(t)}|{ti}) = pκ ({ti}|{x(t)})pγ ({x(t)})
.
pκ,γ ({ti})
The hyperparameters, γ and κ, which represent the roughness of the latent process and the shape of
the ISI density function, can be determined by maximizing the marginal likelihood [16] deﬁned by
pκ,γ ({ti}) = $ pκ({ti }|{x(t)})pγ ({x(t)})D{x(t)},
(20)
where ! D{x(t)} represents the integration over all possible latent process paths. Under a set of
hyperparameters ˆγ and ˆκ that are determined by the marginal likelihood maximization, we can
determine the maximum a posteriori (MAP) estimate of the latent process ˆx(t). The method for
implementing the empirical Bayes analysis is summarized in the Appendix.
Detectability of rate ﬂuctuations
We ﬁrst examine the gamma distribution (6). For synthetic spike trains (n = 1, 000) generated
by the time-rescaled renewal process with the gamma ISI distribution, in which the rate ﬂuctuates
according to the Ornstein-Uhlenbeck process (17) with µ = 1 and τ = 10, we attempt to decode
λ(t) using the empirical Bayes decoder. Depending on the amplitude of the rate ﬂuctuation σ and
CV of fκ(y ), the empirical Bayes decoder provides qualitatively two distinct rate estimations: (I)
a ﬂuctuating rate estimation (ˆγ> 0) for large σ and small C V , or (II) a constant rate estimation
(ˆγ = 0) for small σ and large CV (Figure 2a). When σ is increased or C V is decreased, the

(19)

5

empirical Bayes estimator exhibits a phase transition corresponding to the switch of the most likely
rate estimation from (II) to (I) (Figure 2b). Note that below the critical point of this phase transition,
the empirical Bayes method provides a constant rate as the most likely estimation even if the true
rate process ﬂuctuates. The critical point, thus, gives the lower bound for the degree of detectable
rate ﬂuctuations. It is also conﬁrmed, using numerical simulations, that the phase transition occurs
not only with the gamma distribution, but also with the IG and lognormal distributions (Figure 2c,d).
For the time-rescaled renewal process with the gamma ISI distribution, we could analytically derive
the formula that the lower bound satisﬁes as:

,

(21)

φ(0)
Dκ (λ(t)||µ) =
4 maxη ! ∞
0 φ(u)e−ηu du
where φ(u) is the correlation function of λ(t).
(See supplementary material for the derivation.)
Eq. (21) is in good agreement with the simulation result for the entire parameter space (the solid line
in Figure 2a).
The expression of Eq. (21) itself does not depend on the gamma distribution. We investigated if this
formula is also applicable to the IG and lognormal distributions, and found that the theoretical lower
bounds (the solid lines in Figure 2c,d) indeed do correspond to those obtained by the numerical
simulations; this result implies that Eq. (21) is applicable to more general time-rescaled renewal
processes.
Figure 2e compares the lower bounds among the three distributions. The lower bound of the IG
distribution is the lowest, followed by the lognormal and gamma distributions, which is expected
from the result in Figure 1b, as the lower bound is identically determined by the KL divergence via
Eq. (21).
We also examined the sinusoidally modulated rate, λ(t) = µ + σ sin t/τ ; the qualitative result
remains the same (Figure 2f-h).

4 Discussion
In this study, we ﬁrst examined the extent to which spike trains derived from time-rescaled renewal
processes encode information on ﬂuctuating rates. The encoding efﬁciency is measured by the KL
divergence between two renewal processes with ﬂuctuating and constant rates. We showed that the
KL divergence signiﬁcantly differs among the gamma, IG and lognormal ISI distributions, even if
these three processes share the same rate ﬂuctuation λ(t) and C V (Figure 1b). This suggests that the
higher-order moments of ISIs play an important role in encoding information on ﬂuctuating rates.
Among the three distributions, the IG distribution achieves the largest KL divergence, followed by
the lognormal and gamma distributions. A similar result has been reported for stationary renewal
processes [12].
Since the KL divergence gives the distance between two probability distributions, Eq. (4) is natu-
rally related to the ability to discriminate between a ﬂuctuating rate and a constant rate. In fact,
the lower bound of the degree of rate ﬂuctuation, below which the empirical Bayes decoder cannot
discriminate the underlying ﬂuctuating rate from a constant rate, satisﬁes the formula (21). There
commonly exists a lower bound below which the underlying rate ﬂuctuations are undetectable, not
only in the empirical Bayes method with the above prior distribution (18), but also with other prior
distributions, and in other rate estimators such as a time-histogram. The lower bound in these meth-
ods has been derived for inhomogeneous Poisson processes as τ σ 2 /µ ∼ O(1), where τ , σ and µ
are the time scale, amplitude and mean of the rate ﬂuctuation, respectively [23]. Thus, Eq. (21),
or equivalently τD κ (λ(t)||µ) ∼ O(1) is regarded as a generalization to the non-Poisson processes.
Here, the crucial step for this generalization is incorporating the KL divergence into the formula.
Note that the formula (21) was derived analytically under the assumption of the gamma ISI dis-
tribution, and then was shown to hold for the IG and lognormal ISI distributions with numerical
simulations. The analytical tractability of the gamma family lies in the fact that it is the only scale
family that admits the mean as a sufﬁcient statistic. We conjecture, from our results with the three
speciﬁc models, that Eq. (21) is applicable to more general time-rescaled renewal processes (even
to “non-renewal” processes), which is open to future research.

6

(a)
0.3

σ

0.2

0.1

0

(c)
0.3

σ

(f)

σ

0.2

0.1

0

0.6

0.4

0.2

0

(I)

(II)

(d)
0.3

σ

0.2

0.1

0

(g)
0.6

σ

0.4

0.2

0

γ>0^

0.5

γ=0^
gamma
1.5

1
CV

IG
1.5

0.5

1
CV

gamma
1.5

0.5

1
CV

(b)

0.1

0.05

γ^

(I)

(II)

0

0
(e)
0.3

σ

0.2

0.1

0

(h)
0.6

σ

0.4

0.2

0

0.1

σ

0.2

0.3

0.5

1
CV

1.5

0.5

lognormal
1
1.5
CV

0.5

lognormal
1
1.5
CV

IG
1.5

0.5

1
CV

Figure 2: (a) Left: the phase diagram for sequences generated by the time-rescaled renewal process
with the gamma ISI distribution. The ordinate represents the amplitude of rate ﬂuctuation σ , and
abscissa represents CV of the gamma ISI distribution. The dots represent the result of numerical
simulations in which the empirical Bayes decoder provides a ﬂuctuating rate estimation (ˆγ> 0).
Each dot is plotted if ˆγ> 0 in more than 20 out of 40 identical trials. The solid line represents
the theoretical lower bound obtained by the formula (21). Right: raster plots of sample spike trains
and the estimated rates. The dotted lines and the solid lines represent the underlying rates and the
estimated rates, respectively. The parameters (C V ,σ ) of top (ˆγ> 0) and bottom (ˆγ = 0) are
(0.6, 0.3) and (1.5, 0.15), respectively. (b) The optimal hyperparameter ˆγ as a function of σ for
CV = 0.6. The solid line represents the theoretical value, and the error bars represent the average
and standard deviation of ˆγ determined by applying the empirical Bayes algorithm to 40 trials. (c,
d) The phase diagrams for the IG and lognormal ISI distributions. (e) Comparison of the lower
bounds among the three models. (f-h) The phase diagrams for the gamma, IG and lognormal ISI
distributions, when the rate process is given by λ(t) = µ + σ sin t/τ with µ = 1 and τ = 10.

A recent non-stationary analysis has revealed that individual neurons in the cortex signal with non-
Poisson ﬁring, which has empirically been characterized by measures based on the second-order
moment of ISIs, such as CV and LV [21, 22]. Our results, however, suggest that it may be important
to take into account the higher-order moments of ISIs for characterizing “irregularity” of cortical
ﬁring, in order to gain information on ﬂuctuating ﬁring rates. It has also been demonstrated that
using non-Poisson spiking models enhances the performance of neural decoding [2, 11, 19]. Our
results provide theoretical support for this as well.

Appendix: Implementation of the empirical Bayes method
Discretization
To construct a practical algorithm for performing empirical Bayes decoding, we ﬁrst divide the
time axis into a set of intervals (t i−1 , ti ] (i = 1, . . . , n). We assume that the ﬁring rate within
each interval (t i−1 , ti ] does not change drastically (which is a reasonable assumption in practice),
so that it can be approximated to a constant value λ i . Letting Ti = ti − ti−1 be the ith ISI, the
probability density of {T i}≡{ T1 , T2 , . . . , Tn}, given the rate process {λ i}≡{ λ1 ,λ 2 . . . ,λ n}
7

is obtained from Eq. (1) as p κ ({Ti}|{λi}) = 2n
i=1 λi fκ (λi Ti ). The rate process is linked with
the latent process via xi = log λi . With the same time-discretization, the prior distribution of the
latent process {xi }≡{ x1 , x2 , . . . , xn}, which corresponds to Eq. (18), is derived as p γ ({xi }) =
p(x1 ) 2n
i=2 pγ (xi |xi−1 ), where
exp * −
γ 2 (Ti + Ti−1 ) +,
(xi − xi−1 )2
1
(22)
pγ (xi |xi−1 ) =
(πγ 2 (Ti + Ti−1 )
and p(x1 ) is the probability density function of the initial latent rate variable.
p({Ti}|{λi}) and pγ ({xi }) deﬁne a discrete-time state space model. We note that this provides a
good approximation to the original continuous-time model if the timescale of the rate ﬂuctuation is
larger than the mean ISI.
EM algorithm
We assume that the ISI density function can be rewritten in the form of exponential family distribu-
tions with respect to the shape parameter κ:
(23)
pκ (Ti |φi ) := λi fκ (λiTi ) = exp[κS (Ti ,φ i ) − ϕ(κ) + c(Ti ,φ i )],
with an appropriate parameter representation φ i = φ(λi ,κ ). Here, κ is the natural parameter of
the exponential family and S (T i ,φ i ) is its sufﬁcient statistic. Suppose that the potential ϕ(κ) is a
convex function. The expectation of S (T i ,φ i ) is then given by
η = $ S (Ti ,φ i )pκ (Ti |φi )dTi = dϕ(κ)
(24)
.
dκ
Since ϕ(κ) is convex, there is one-to-one correspondence between κ and η , and thus η provides
alternative parametrization to κ [1]. The gamma (6), IG (7) and lognormal (8) distributions are
included in this family.
With the parameterization η , the EM algorithm for the state space model is derived as follows.
Suppose that we have estimations ˆη (m) and ˆγ(m) at the mth iteration. The estimations at the (m+1)th
iteration are given by
n%i=1
"S (Ti ,φ (xi ))#(m) ,
n%i=2
"(xi − xi−1 )2 #(m)
2
(26)
ˆγ 2
(m+1) =
,
Ti + Ti−1
n − 1
where " #(m) denotes the expectation with respect to the posterior probability of {x i}, given {Ti},
ˆη(m) and ˆγ(m) . The posterior probability is computed by the Laplace approximation, introduced
below. We update ˆη and ˆγ until the estimations converge. The estimation of κ is then transformed
from ˆη with Eq. (24).
Laplace approximation
We employ Laplace’s method to compute an approximate posterior distribution of {x i}. Let x =
(x1 , x2 , . . . , xn )t be the column vector of the latent process, ( ) t being the transpose of a vector.
The MAP estimate of the latent process is obtained by maximizing the log posterior distribution:
n%i=1
n%i=2
(27)
l(x) = log p(x1 ) +
log pγ (xi |xi−1 ) +
log pκ (Ti |xi ) + const.,
with respect to x. We use a diffuse prior for p(x 1 ) so that its contribution vanishes [7].
If
pγ (xi |xi−1 ) is log-concave in x i and xi−1 , and the pκ (Ti |xi ) is also log-concave in x i , comput-
ing the MAP estimate is a concave optimization problem [18], which can be solved efﬁciently by
a Newton method. Due to the Markovian Structure of the state-space model, the Hessian matrix,
J (x) ≡ ∇∇x l(x), becomes a tridiagonal matrix, which allows us to compute the Newton step in
O(n) time [13]. Let ˆx denote the MAP estimation of the posterior probability. The posterior proba-
bility is then approximated to a Gaussian whose mean vector and covariance matrix are given by ˆx
and −J ( ˆx)−1 , respectively.

ˆη(m+1) =

and

1
n

(25)

8

Acknowledgments
This work was supported by JSPS KAKENHI Grant Number 24700287.
References
[1] S. Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press, 2000.
[2] R. Barbieri, M. C. Quirk, L. M. Frank, M. A. Wilson, and E. N. Brown. Construction and analysis
of non-poisson stimulus-response models of neural spiking activity. Journal of Neuroscience Methods,
105:25–37, 2001.
[3] N. Brenner, S. P. Strong, R. Koberle, and W. Bialek. Synergy in a neural code. Neural Computation,
12:1531–1552, 2000.
[4] D. R. Cox. Renewal Theory. Chapman and Hal, 1962.
[5] J. P. Cunningham, B. M. Yu, K. V. Shenoy, and M. Sahani. Inferring neural ﬁring rates from spike trains
using Gaussian processes. In Neural Information Processing Systems, volume 20, pages 329–336, 2008.
[6] R. M. Davies, G. L. Gerstein, and S. N. Baker. Measurement of time-dependent changes in the irregularity
of neural spiking. Journal of Neurophysiology, 96:906–918, 2006.
[7] J. Durbin and S. J. Koopman. Time Series Analysis by State Space Methods. Oxford University Press,
2001.
[8] U. Fano.
Ionization yield of radiations. ii. the ﬂuctuations of the number of ions. Physical Review,
72:26–29, 1947.
[9] G. L. Gerstein and B. Mandelbrot. Random walk models for the spike activity of a single neuron. Bio-
physical Journal, 4:41–68, 1964.
[10] S. Ikeda and J. H. Manton. Capacity of a single spiking neuron channel. Neural Computation, 21:1714–
1748, 2009.
[11] A. L. Jacobs, G. Fridman, R. M. Douglas, N. M. Alam, P. E. Latham, G. T. Prusky, and S. Nirenberg.
Ruling out and ruling in neural codes. Proceedings of the National Academy of Sciences, 106:5936–5941,
2009.
[12] K. Kang and S. Amari. Discrimination with spike times and ISI distributions. Neural Computation,
20:1411–1426, 2008.
[13] S. Koyama and L. Paninski. Efﬁcient computation of the maximum a posteriori path and parameter estima-
tion in integrate-and-ﬁre and more general state-space models. Journal of Computational Neuroscience,
29:89–105, 2009.
[14] M. W. Levine. The distribution of the intervals between neural impulses in the maintained discharges of
retinal ganglion cells. Biological Cybernetics, 65:459–467, 1991.
[15] B. N. Lundstrom and A. L. Fairhall. Decoding stimulus variance from a distributional neural code of
interspike intervals. Journal of Neuroscience, 26:9030–9037, 2006.
[16] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4:415–447, 1992.
[17] T. Omi and S. Shinomoto. Optimizing time histograms for non-Poisson spike trains. Neural Computation,
23:3125–3144, 2011.
[18] L. Paninski. Log-concavity results on gaussian process methods for supervised and unsupervised learning.
In Neural Information Processing Systems, volume 17, pages 1025–1032, 2005.
[19] J. W. Pillow, L. Paninski, V. J. Uzzell, E. P. Simoncelli, and E. J. Chichilnisky. Prediction and decoding
of retinal ganglion cell responses with a probabilistic spiking model. Journal of Neuroscience, 23:11003–
11013, 2005.
[20] M. N. Shadlen and W. T. Newsome. The variable discharge of cortical neurons: Implications for connec-
tivity, computation, and information coding. Journal of Neuroscience, 18:3870–3896, 1998.
[21] S. Shinomoto, H. Kim, T. Shimokawa, N. Matsuno, S. Funahashi, K. Shima, I. Fujita, H. Tamura, T. Doi,
K. Kawano, N. Inaba, K. Fukushima, S. Kurkin, K. Kurata, M. Taira, K. Tsutsui, H. Komatsu, T. Ogawa,
K. Koida, J. Tanji, and K. Toyama. Relating neuronal ﬁring patterns to functional differentiation of
cerebral cortex. PLoS Computational Biology, 5:e1000433, 2009.
[22] S. Shinomoto, K. Shima, and J. Tanji. Differences in spiking patterns among cortical neurons. Neural
Computation, 15:2823–2842, 2003.
[23] T. Shintani and S. Shinomoto. Detection limit for rate ﬂuctuations in inhomogeneous poisson processes.
Physical Review E, 85:041139, 2012.
[24] W. R. Softky and C. Koch. The highly irregular ﬁring of cortical cells is inconsistent with temporal
integration of random EPSPs. Journal of Neuroscience, 13:334–350, 1993.

9


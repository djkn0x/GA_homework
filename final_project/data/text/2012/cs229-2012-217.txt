The Perfect IHUM Essay
Predicting IHUM Essay Grades

Andrew Moreland
Charlie Guo

1 Introduction

Introduction to the Humanities – otherwise
known as IHUM – has been a required course for
Stanford freshmen for several years. IHUM is a
name for a now-discontinued collection of classes
that covered topics ranging from archeology and
world religions to Russian literary history. In-
tended as a way to introduce new Stanford stu-
dents to a set of core concepts in the humanities
in order to assure a rounded education, IHUM
has been havily criticized for its opaque and (to
students) seemingly arbitrary grading.

Our pro ject attempts to use various features of
writing and diction in order to predict the ﬁ-
nal grades of essays written for IHUM classes.
We draw our training and testing examples from
a corpus that we have gathered from past stu-
dents’ essays and apply machine learning tech-
niques in order to classify these essays into one
of two categories: A or B.

2 Background

its weighting of certain features allowed it to be
“tricked” by doing things like writing longer es-
says. The problem was that PEG did not ana-
lyze the semantics of the essays; instead, it only
analyzed the structure.

More recently, however, other systems such as
Intelligent Essay Assesor (IEA), use more so-
phisticated techniques to predict scores. One
of these techniques is Latent Semantic Analysis
(LSA), “a statistical model of word usage that
permits comparisons of the semantic similarity
between pieces of textual information”. Along
with IEA, other programs like Criterion and
E-rater used other Natural Language Process-
ing (NLP) techniques to generate essay scores,
which proved more eﬀective than the initial LSA
approach.

Our pro ject attempts to replicate the statisti-
cal approach of PEG but also leverages a tech-
nique associated with classifying spam emails.
We implement two strategies: ﬁrst we compute
numeric data about the structure of the essays
and use an SVM to classify them, and then we
use a Multinomial Naive Bayes classiﬁer over the
word frequency counts of the essays.

Automated essay scoring has been around for
some time now, with approaches and use cases
becoming more varied with time. Some of the
original systems were developed in the 1960’s,
at the request of the College Board 1 . One
of these systems, Pro ject Essay Grader (PEG),
used simple features of the written essays in or-
der to determine a relative “score” (while these
features were labeled ahead of time, PEG did
not use machine learning to analyze the input
features). While PEG was somewhat eﬀective,

3 Data Collection

Generally machine learning pro jects do better if
they have a larger body of training data to draw
from. We ﬁgured that since nearly every student
in recent years who has passed through Stanford
has taken an IHUM course, it would be easy
to collect a large corpus. To facilitate this we
created a site (http://ihumessaypro ject.com/)

1

which allowed students to easily upload their
previous IHUM essays along with the grade that
they received. In reality, it turns out that peo-
ple are not so willing to invest the few minutes
it takes to ﬁnd their old essays ﬁles and upload
them. As such, we received a total of 124 usable
essays, which is a fair number and more than we
had hoped, but is fairly small compared to the
corpuses used for other eﬀorts. 2

4 Pre-Processing

In order to generate features for our SVM, we
pre-processed the essays in order to extract the
following data:

1. Essay length

2. Paragraph length

3. Number of quotations

4. Length of quotations

5. Average word length

6. Word frequency (using a Porter Stemmer)

7. Part of speech frequency (using the Natu-
ral Language Toolkit for Python1 )

8. Misspelled words (words very close to
words in the dictionary according to edit
distance)

9. Flesch-Kincaid Grade Level2

10. Flesch Reading Ease Scale score3

11. FOG score4

In addition to extracting these features, we
cleaned the data by removing things like stop
words5 and the headers that people included in
their essays.

5 Methodology

5.1 Algorithm 1: SVMs

Initially we attempted to use SVMs to clas-
sify the essays based on the computed statis-
tics above. We used LIBSVM3 in order to train
SVMs on 70% of our collected essays. We com-
puted training accuracy on the data with lin-
ear, polynomial, radial and sigmoid based ker-
nels with regularization. We attempted a grid
search of the various parameters of these mod-
els but were never able to achieve better than
58% training accuracy. For a binary classiﬁ-
cation problem where the training data is split
roughly 57 − 43 between the two classes this is
clearly abysmal performance, so we abandoned
this strategy and moved on hoping to ﬁnd more
success elsewhere.

5.2 Algorithm 2: Multinomial Naive
Bayes

We decided to start with a binary classiﬁcation
problem. We assigned essays to two classes. Es-
says that were below an A− became “B” essays,
and essays that were above a B+ became “A”
essays.

Drawing inspiration from the example of spam
classiﬁcation, we decided to use Multinomial
Naive Bayes trained over our essays in order
to classify them. We split the data set into
70% training and 30% testing, drawing inspi-
ration from machine learning competitions like
the Netﬂix Challenge and used 10−Fold Cross
Validation on the rest of the data in order to
maximize our training accuracy. Upon maxi-
mizing our training error, we computed our test
error against the holdout set and recorded that.

Initially we tried the most naive implementation
of Naive Bayes. We simply trained our clas-
siﬁer on the entire vocabulary found in all of
the essays and then attempted to classify using
the trained model. We found that we were able

1http://nltk.org/
2http://www.readabilityformulas.com/ﬂesch-grade-level-readability-formula.php
3https://github.com/sebbacon/pyﬂesch
4http://www.readabilityformulas.com/gunning-fog-readability-formula.php
5http://www.lextek.com/manuals/onix/stopwords1.html

2

to achieve roughly 60% accuracy with this ap-
proach, which is barely more than what would
be achieved if our classiﬁer simply guessed the
most prevelant class each time. Still, since it was
more accurate than guessing that we were en-
couraged to explore more advanced techniques.

We researched improvements to basic Naive
Bayes and found that feature selection was often
the most important factor in the success of the
algorithm. Research indicated that one of the
simplest feature selection algorithms was often
helpful – simply removing words that only ap-
peared in a single document helped improve the
overall accuracy of the classiﬁer. With this in

mind, we reran our computation and found that
our training accuracy improved to roughly 64%
and that our vocabulary was cut from roughly
8, 000 words down to only 3, 000.

This was still fairly poor performance but the
general approach seemed promising, so we de-
cided to attempt more advanced feature selec-
tion. Research indicated that a strong metric
for feature selection in text classiﬁcation prob-
lems was Mutual Information4 . Using the rain-
bow classiﬁer’s5 built in algorithm for mutual
information computation, we initially selected
the 1000 most informative words and saw our
training accuracy jump to 84%.

Figure 1: As we increased the number of documents in which a word in our vocabulary was required
to appear, training accuracy sharply decreased. Our vocabulary size played a smaller
and less consistent role. The error in the near right is roughly 95%, and the overall error
moves towards 40% on the left side.

At this point we decided to perform a more sys-
tematic optimization of parameters. We ran
a coarse grid search over the parameter space,
searching for the optimal number of documents
in which to require each word to appear, and the
optimal number of informative words to select as
features. Our grid search ran over the integers
in the intervals [1, 50] and [50, 500] respectively.

We ran a coarser grid search initially, and nar-
rowed in on this area by an informal evaluation
of the results.

Eventually, we narrowed in on a choice of the
500 most informative words that appeared in at
least 2 documents as the optimal parameters,
with which we achieved a consistent 97% train-
ing accuracy.
(Note: we previously observed

3

that 500 was a tipping point after which train-
ing accuracy decreased, so we did not evaluate
anything past it in our ﬁne grid search.)

6 Results

Using the parameters that we determined in our
grid search, we ran tests over our 30% hold out

set and found that we achieved a test accuracy
of 74%. We ran several more tests, varying the
training/test size (and generating the new sets
randomly each time) and found that the exact
test accuracy tending to vary but was consis-
tently over 65% for reasonable training sizes.
There also appears to be a trend towards con-
vergence between test and training error some-
where between 80% and 90% as the training size
increases.

Figure 2: As the size of the training set increases, we see that training error tends to remain fairly
constant and there appears to be a trend towards convergence.

7 Conclusions

Overall, we are fairly satisﬁed with the results
of this pro ject. We believe that we have demon-
strated that it is possible to use Naive Bayes
to predict a distinction between essays that is
fairly ﬁne – the diﬀerence between an A and a
B can be fairly slight.

We feel that with more training data, we could
have improved the accuracy of our classiﬁer.
We feel that our results are good though given
that the essays are from diﬀerent eight diﬀer-
ent classes with diﬀerent prompts and sub jects
and even more than eight graders. If we were
to do the pro ject again, we would attempt to
capture more information about the context of
the essays in our training set.
It would have
been nice to know the word limits of the essay
prompts and the actual classes and graders as-

sociated with the essays. If we had known this
information, then we could perhaps have applied
more techniques to the data. Above all though,
we express the same sentiment that many people
who have attempted to apply machine learning
have expressed: we wish we had more data.

8 References

Notes

1Dikli, S. (2006). An Overview of Automated Scoring
of Essays. Journal of Technology, Learning, and Assess-
ment, 5(1).

2http://urd.let.rug.nl/nerbonne/papers/Santos et al-
2012-grading.pdf

3Chih-Chung Chang and Chih-Jen Lin, LIBSVM:
a library for
support vector machines.
ACM

4

Transactions on Intelligent Systems and Technol-
ogy,
Software available at
2011.
2:27:1–27:27,
http://www.csie.ntu.edu.tw/ cjlin/libsvm

4Aggarwal, Charu C., A Survey of Text Classiﬁ-
cation Algorithms, http://www.charuaggarwal.net/text-

class.pdf

5McCallum, Andrew Kachites. “Bow: A toolkit for
statistical language modeling, text retrieval, classiﬁca-
tion and clustering.” http://www.cs.cmu.edu/ mccal-
lum/bow. 1996.

5


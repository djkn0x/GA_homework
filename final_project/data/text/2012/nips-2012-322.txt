Sparse Approximate Manifolds for
Differential Geometric MCMC

Ben Calderhead∗
CoMPLEX
University College London
London, WC1E 6BT, UK
b.calderhead@ucl.ac.uk

Mátyás A. Sustik
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712, USA
sustik@cs.utexas.edu

Abstract

One of the enduring challenges in Markov chain Monte Carlo methodology is
the development of proposal mechanisms to make moves distant from the current
point, that are accepted with high probability and at low computational cost. The
recent introduction of locally adaptive MCMC methods based on the natural un-
derlying Riemannian geometry of such models goes some way to alleviating these
problems for certain classes of models for which the metric tensor is analytically
tractable, however computational efﬁciency is not assured due to the necessity of
potentially high-dimensional matrix operations at each iteration.
In this paper we ﬁrstly investigate a sampling-based approach for approximating
the metric tensor and suggest a valid MCMC algorithm that extends the appli-
cability of Riemannian Manifold MCMC methods to statistical models that do
not admit an analytically computable metric tensor. Secondly, we show how the
approximation scheme we consider naturally motivates the use of (cid:96)1 regularisa-
tion to improve estimates and obtain a sparse approximate inverse of the metric,
which enables stable and sparse approximations of the local geometry to be made.
We demonstrate the application of this algorithm for inferring the parameters of a
realistic system of ordinary differential equations using a biologically motivated
robust Student-t error model, for which the Expected Fisher Information is ana-
lytically intractable.

1

Introduction

The use of Markov chain Monte Carlo methods can be extremely challenging in many modern day
applications. This difﬁculty arises from the more frequent use of complex and nonlinear statistical
models that induce strong correlation structures in their often high-dimensional parameter spaces.
The exact structure of the target distribution is generally not known in advance and local correlation
structure between different parameters may vary across the space, particularly as the chain moves
from the transient phase, exploring areas of negligible probability mass, to the stationary phase
exploring higher density regions [1].
Constructing a Markov chain that adapts to the target distribution while still drawing samples from
the correct stationary distribution is challenging, although much research over the last 15 years has
resulted in a variety of approaches and theoretical results. Adaptive MCMC for example, allows
for global adaptation based on the partial or full history of a chain; this breaks its Markov property,
although it has been shown that subject to some technical conditions [2,3] the resulting chain will
still converge to the desired stationary distribution. Most recently, advances in Riemannian Mani-
fold MCMC allow locally changing, position speciﬁc proposals to be made based on the underlying
∗ http://www.2020science.net/people/ben-calderhead

1

geometry of the target distribution [1]. This directly takes into account the changing sensitivities
of the model for different parameter values and enables very efﬁcient inference over a number of
popular statistical models. It is useful for inference over large numbers of strongly covarying pa-
rameters, however this methodology is still not suitable for all statistical models; in its current form
it is only applicable to models that admit an analytic expression for the metric tensor. In practice,
there are many commonly used models for which the Expected Fisher Information is not analytically
tractable, such as when a robust Student-t error model is employed to construct the likelihood.
In this paper we propose the use of a locally adaptive MCMC algorithm that approximates the local
Riemannian geometry at each point in the target space. This extends the applicability of Riemannian
Manifold MCMC to a much wider class of statistical models than at present. In particular, we do so
by estimating the covariance structure of the tangent vectors at a point on the Riemannian manifold
induced by the statistical model. Considering this geometric problem as one of inverse covariance
estimation naturally leads us to the use of an (cid:96)1 regularised maximum likelihood estimator. This
approximate inverse approach allows the required geometry to be estimated with few samples, en-
abling good proposals for the Markov chain while inducing a natural sparsity in the inverse metric
tensor that reduces the associated computational cost.
We ﬁrst give a brief characterisation of current adaptive approaches to MCMC, making a distinc-
tion between locally and globally adaptive methods, since these two approaches have very different
requirements in terms of proving convergence to the stationary distribution. We then discuss the
use of geometry in MCMC and the interpretation of such methods as being locally adaptive, before
giving the necessary background on Riemannian geometry and MCMC algorithms deﬁned on in-
duced Riemannian manifolds. We focus on the manifold MALA sampler, which is derived from a
Langevin diffusion process that takes into account local non-Euclidean geometry, and we discuss
simpliﬁcations that may be made for computational efﬁciency. Finally we present a valid MCMC
algorithm that estimates the Riemannian geometry at each iteration based on covariance estimates
of random vectors tangent to the manifold at the chain’s current point. We demonstrate the use of
(cid:96)1 regularisation to calculate sparse approximate inverses of the metric tensor and investigate the
sampling properties of the algorithm on an extremely challenging statistical model for which the
Expected Fisher Information is analytically intractable.

2 Background

We wish to sample from some arbitrary target density π(x) deﬁned on a continuous state space XD ,
which may be high-dimensional. We may deﬁne a Markov chain that converges to the correct sta-
tionary distribution in the usual manner by proposing a new position x∗ from the current position
xn via some ﬁxed proposal distribution q(x∗ |xn ); we accept the new move setting xn+1 = x∗with
q(xn |x∗ )
probability α(x∗ |xn ) = min( π(x∗ )
q(x∗ |xn ) , 1) and set xn+1 = xn otherwise. In a Bayesian con-
π(xn )
text, we will often have a posterior distribution as our target π(x) = p(θ |y), where y is the data and
θ are the parameters of a statistical model. The choice of proposal distribution is the critical factor in
determining how efﬁciently the Markov chain can explore the space and whether new moves will be
accepted with high probability and be sufﬁciently far from the current point to keep autocorrelation
of the samples to a minimum. There is a lot of ﬂexibility in the choice of proposal distribution, in
that it may depend on the current point in a deterministic manner.
We note that Adaptive MCMC approaches attempt to change their proposal mechanism throughout
the running of the algorithm, and for the purpose of proving convergence to the stationary distri-
bution it is useful to categorise them as follows; locally adaptive MCMC methods make proposals
based only on the current position of the chain, whereas globally adaptive MCMC methods use pre-
viously collected samples in the chain’s history to generate a new proposal mechanism. This is an
important distinction since globally adaptive methods lose their Markov property and convergence
to the stationary distribution must be proven in an alternative manner. It has been shown that such
chains may still be usefully employed as long as they satisfy some technical conditions, namely
diminishing adaptation and bounded convergence [2]. In practice these algorithms represent a step
towards MCMC as a “black box” method and may be very useful for sampling from target distri-
butions for which there is no derivative or higher order geometric information available, however
there are simple examples of standard Adaptive MCMC methods requiring hundreds of thousands
of iterations in higher dimensions before adapting to a suitable proposal distribution [3]. In addi-

2

tion, if there is more information about the target density available, then there seems little point in
trying to guess the geometric structure when it may be calculated directly. In this paper we focus
on locally adaptive methods that employ proposals constructed deterministically from information
at the current position of the Markov chain.

2.1 Locally Adaptive MCMC

Many geometric-based MCMC methods may be categorised as being locally adaptive. When
the derivative of the target density is available, MCMC methods such as the Metropolis-adjusted
Langevin Algorithm (MALA) [4] allow local adaptation based on the geometry at the current point,
but unlike globally adaptive MCMC, they retain their Markovian property and therefore converge to
the correct stationary distribution using a standard Metropolis-Hastings step and without the need to
satisfy further technical conditions.
In general, we can deﬁne position-speciﬁc proposal densities based on deterministic functions that
depend only on the current point. This idea has been previously employed to develop approaches for
sampling multimodal distributions whereby large initial jumps followed by deterministic optimisa-
tion functions were used to create mode-jumping proposal mechanisms [5]. In some instances, the
use of ﬁrst order geometric information may drastically speed up the convergence to a stationary dis-
tribution, however in other cases such algorithms exhibit very slow convergence, due to the gradients
not being isotropic in magnitude [6]; in practice gradients may vary greatly in different directions
and the rate of exploration of the target density may in addition be dependent on the problem-speciﬁc
choice of parameterisation [1]. Methods using the standard gradient implicitly assume that the slope
in each direction is approximately constant over a small distance, when in fact these gradients may
rapidly change over short distances. Incorporating higher order geometry often helps although at an
increased computational cost.
A number of Hessian-based MCMC methods have been proposed as a solution [7]. While such
approaches have been shown to work very well for selected problems there are a number of problems
with this use of geometry; ad hoc methods are often necessary to deal with the fact that the Hessian
might not be everywhere positive-deﬁnite, and second derivatives can be challenging and costly to
compute. We can also exploit higher order information through the use of Riemannian geometry.
Using a metric tensor instead of a Hessian matrix lends us nice properties such as invariance to
reparameterisation of our statistical model, and positive-deﬁniteness is also assured. Riemannian
geometry has been useful in a variety of other machine learning and statistical contexts [8] however
the limiting factor is usually analytic or computational tractability.

3 Differential Geometric MCMC

During the 1940s, Jeffreys and Rao demonstrated that the Expected Fisher Information has the
same properties as a metric tensor and indeed induces a natural Riemannian structure for a statistical
model [11, 10], providing a fascinating link between statistics and differential geometry. Much work
has been done since then elucidating the relationship between statistics and Riemannian geometry,
in particular examining geometric concepts such as distance, curvature and geodesics on statistical
manifolds, within a ﬁeld that has become known as Information Geometry [6]. We ﬁrst provide an
overview of Riemannian geometry and MCMC algorithms deﬁned on Riemannian manifolds. We
then describe a sampling scheme that allows the local geometry to be estimated at each iteration for
statistical models that do not admit an analytically tractable metric tensor.

3.1 Riemannian Geometry

Informally, a manifold is an n-dimensional space that is locally Euclidean; it is locally equivalent
to Rn via some smooth transformation. At each point θ ∈ Rn on a Riemannian manifold M there
exists a tangent space, which we denote as TθM . We can think of this as a linear approximation
(cid:104) ∂
(cid:105)
to the Riemannian manifold at the point θ and this is simply a standard vector space, whose origin
is the current point on the manifold and whose vectors are tangent to this point. The vector space
TθM is spanned by the differential operators
, which act on functions deﬁning paths
, . . . , ∂
∂ θ1
∂ θn
on the underlying manifold [9]. In the context of MCMC we can consider the target density as the

3

(cid:104) ∂
(cid:105)
log-likelihood of a statistical model given some data, such that at a particular point θ , the derivatives
of the log-likelihood are tangent to the manifold and these are just the score vectors at θ , ∇θ L =
. The tangent space at each point θ arises when we equip a differentiable manifold
, . . . , ∂
∂ θ1
∂ θn
with an inner product at each point, which we can use to measure distance and angles between
vectors. This inner product is deﬁned in terms of a metric tensor, Gθ , which deﬁnes a basis on each
tangent space TθM . The tangent space is therefore a linear approximation of the manifold at a given
point and it has the same dimensionality. A natural inner product for this vector space is given by
the covariance of the basis score vectors, since the covariance function satisﬁes the same properties
as a metric tensor, namely symmetry, bilinearity and positive-deﬁniteness [9]. This inner product
then turns out to be equivalent to the Expected Fisher Information, following from the fact that the
expectation of the score is zero, with the [i, j ]th component of the tensor given by
(cid:32)
(cid:33)
(cid:18) ∂L
(cid:19)
(cid:19)
(cid:18) ∂ 2L
∂ θi ,
∂ θi∂ θj

= −Ep(x|θ)

Gi,j = Cov

T ∂L
∂ θj

= Ep(x|θ)

∂L
∂ θj

∂L
∂ θi

(1)

Each tangent vector, t1 ∈ TθM , at a point on the manifold, θ ∈ M , has a length ||t1 || ∈ R+ ,
whose square is given by the inner product, such that ||t1 ||2
= (cid:104)t1 , t1 (cid:105)θ = tT
1 Gθ t1 . This squared
Gθ
distance is known as the ﬁrst fundamental form in Riemannian geometry [9], is invariant to repa-
rameterisations of the coordinates, and importantly for MCMC provides a local measure of distance
that takes into account the local 2nd order sensitivity of the statistical model. We note that when the
metric tensor is constant for all values of θ then the Riemannian manifold is equivalent to a vector
space with constant inner product; further, if the metric tensor is an identity matrix then the manifold
simply becomes a Euclidean space.

3.2 Manifold MCMC

∂
∂θj

d ˜bi (t) = |G(θ(t))|− 1
2

(G−1 (θ(t))ij |G(θ(t))| 1
2 )dt +

We consider the manifold version of the MALA sampling algorithm, which proposes moves based
on a stochastic differential equation deﬁning a Langevin diffusion [4]. It turns out we can also deﬁne
such a diffusion on a Riemannian manifold [12], and so in a similar manner we can derive a sampling
algorithm that takes the underlying geometric structure into account when making proposals. It is
based on the Laplace-Beltrami operator, which simply measures the divergence of a vector ﬁeld on
a manifold. The stochastic differential equation deﬁning the Langevin diffusion on a Riemannian
˜∇θ L(θ(t))dt + d ˜b(t), where the natural gradient [6] is the gradient of a
manifold is dθ(t) = 1
2
function transformed into the tangent space at the current point by a linear transformation using the
basis deﬁned by the metric tensor, such that ˜∇θ L(θ(t)) = G−1 (θ(t))∇θ L(θ(t)), and the Brownian
motion on the Riemannian manifold is deﬁned as
(cid:17)
(cid:16)(cid:112)G−1 (θ(t))db(t)
D(cid:88)
j=1
The ﬁrst part of the right hand side of Equation 2 represents the 1st order terms of the Laplace-
Beltrami operator and these relate to the local curvature of the manifold, reducing to zero if the
metric is everywhere constant. The second term on the right hand side provides a position speciﬁc
linear transformation of the Brownian motion b(t) based on the local metric. Employing a ﬁrst order
Euler integrator, the discrete form of the Langevin diffusion on a Riemannian manifold follows as
(cid:19)
(cid:18)
D(cid:88)
∂G(θn )
2
(G−1 (θn )∇θ L(θn ))i − 2
G−1 (θn )
G−1 (θn )
= θn
(cid:18)
(cid:19)
i +
(cid:16)
(cid:112)G−1 (θn )zn(cid:17)
D(cid:88)
(cid:0)G−1 (θn )(cid:1)
2
∂θj
ij
j=1
G−1 (θn )
(cid:112)G−1 (θn )zn(cid:17)
(cid:16)
ij T r
j=1
= µ(θn , )i +
4

∂G(θn )
∂θj

θn+1
i

2
2

(2)

(3)

i

+

+

i

i

∗ |θn ) = N (θ
∗ |µ(θn , ), 2G−1 (θn )) and ac-
which deﬁnes a proposal mechanism with density q(θ
ceptance probability min{1, p(θ
∗ |θn )} to ensure convergence to the invariant
)q(θn |θ
∗
∗
)/p(θn )q(θ
density p(θ). We note that this deterministically deﬁnes a position-speciﬁc proposal distribution at
each point on the manifold; we may categorise this as another locally adaptive MCMC method and
convergence to the invariant density follows from using the standard Metropolis-Hastings ratio.
It may be computationally expensive to calculate the 3rd order derivatives needed for working out the
rate of change of the metric tensor, and so an obvious approximation is to assume these derivatives
are zero for each step. In other words, for each step we can assume that the metric is locally constant.
Of course even if the curvature of the manifold is not constant, this simpliﬁed proposal mechanism
still deﬁnes a correct MCMC method which converges to the target measure, as we accept or reject
moves using a Metropolis-Hastings ratio. This is equivalent to a position-speciﬁc pre-conditioned
MALA proposal, where the pre-conditioning is dependent on the current parameter values
G−1 (θn )∇θ L(θn ) + (cid:112)G−1 (θn )zn
For a manifold whose metric tensor is globally constant, this reduces further to a pre-conditioned
MALA proposal, where the pre-conditioning is effectively independent of the current parameter
values. In this context, such pre-conditioning no longer needs to be chosen arbitrarily, but rather it
may be informed by the geometry of the distribution we are exploring.
We point out that any approximations of the metric tensor would be best employed in the simpliﬁed
mMALA scheme, deﬁning the covariance of the proposal distribution, or as a ﬂat approximation to
a manifold. In the case of full mMALA, or even Hamiltonian Monte Carlo deﬁned on a Riemannian
manifold [1], Christoffel symbols are also used, incorporating the derivatives of the metric tensor
as it changes across the surface of the manifold - in many cases the extra expense of computing or
estimating such higher order information is not sufﬁciently supported by the increase in sampling
efﬁciency [1] and for this reason we do not consider such methods further.
In the next section we consider the representation of the metric tensor as the covariance of the tangent
vectors at each point. We consider a method of estimating this such that convergence is guaranteed
by extending the state-space and introducing auxiliary variables that are conditioned on the current
point and we demonstrate its potential within a Riemannian geometric context.

θn+1 = θn +

2
2

(4)

4 Approximate Geometry for MCMC Proposals

We ﬁrst derive an acceptance ratio on an extended state-space that enables convergence to the sta-
tionary distribution before describing the implications for developing new differential geometric
MCMC methods. Following [13, 14] we can employ the oft-used trick of deﬁning an extended state
space X × D. We may of course choose D to be of any size, however in our particular case we
shall choose D to be Rm×s , where m is the dimension of the data and s is the number of samples;
the reasons for this shall become clear. We therefore sample from this extended state space, whose
joint distribution follows as π∗ = π(x) ˆπ(d|x). Given the current states [xn , dn ], we may propose a
new state q(x∗ |xn , dn ) and the MCMC algorithm will satisfy detailed balance and hence converge
to the stationary distribution if we accept joint proposals with Metropolis-Hastings probability ratio,
(cid:19)
(cid:18)
ˆπ(dn |xn )
q(xn |x∗ , d∗ )
π∗ (x∗ , d∗ )
(cid:18)
ˆπ(d∗ |x∗ )
q(x∗ |xn , dn )
π∗ (xn , dn )
ˆπ(dn |xn )
q(xn |x∗ , d∗ )
ˆπ(d∗ |x∗ )
π(x∗ )
(cid:18)
(cid:19)
q(x∗ |xn , dn )
ˆπ(d∗ |x∗ )
ˆπ(dn |xn )
π(xn )
q(xn |x∗ , d∗ )
π(x∗ )
q(x∗ |xn , dn )
π(xn )

α(x∗ , d∗ |xn , dn ) = min

= min

= min

(cid:19)

(5)

1,

1,

1,

This is a reversible transition on π(x, d), from which we can sample to obtain π(x) as the marginal
distribution. The key point here is that we may deﬁne our proposal distribution q(x∗ |xn , dn ) in
almost any deterministic manner we wish. In particular, choosing ˆπ(d|x) to be the same distribution

5

as the log-likelihood for our statistical model, the s samples from the extended state space D may
be thought of as pseudo-data, from which we can deterministically calculate an estimate of the
Expected Fisher Information to use as the covariance of a proposal distribution. Speciﬁcally, each
sampled pseudo-data can be used deterministically to give a sample of ∂L
dθ given the current θ , all of
which may then be used deterministically to obtain an approximation of the covariance of tangent
vectors at the current point. This approximation, unlike the Hessian, will always be positive deﬁnite,
and gives us an approximation of the metric tensor deﬁning the local geometry. Further, we may use
additional deterministic procedures, given xn and dn , to construct better proposals; we consider a
sparsity inducing approach in the next section.

5 Stability and Sparsity via (cid:96)1 Regularisation

We have two motivations for using an (cid:96)1 regularisation approach for computing the inverse of the
metric tensor; ﬁrstly, since the metric is equivalent to the covariance of tangent vectors, we may
obtain more stable estimates of the inverse metric tensor using smaller numbers of samples, and
secondly, it induces a natural sparsity in the inverse metric, which may be exploited to decrease the
computational cost associated with repeated Cholesky factorisations and matrix-vector multiplica-
tions. We adopted the graphical lasso [15, 16], in which the maximum likelihood solution results in
(cid:88)
the matrix optimisation problem,
i (cid:54)=j

{− log det(A) + tr(AG) + γ
arg min
A(cid:31)0

|Aij |}

(6)

where G is an empirical covariance matrix and γ is a regularisation parameter. This convex opti-
misation problem aims to ﬁnd A, the regularised maximum likelihood estimate for the inverse of
the covariance matrix. Importantly, the optimisation algorithm we employ is deterministic given our
tangent vectors, and therefore does not affect the validity of our MCMC algorithm; indeed we note
that we may use any deterministic sparse matrix inverse estimation approaches within this MCMC
algorithm. The use of the (cid:96)1 regularisation promotes sparsity [23]; larger values for the regularisation
parameter matrix Λ results in a solution that is more sparse, on the other hand when Λ approaches
zero, the solution converges to the inverse of G (assuming it exists). It is also worth noting that the
(cid:96)1 regularisation helps to recover a sparse structure in a high dimensional setting where the number
of samples is less than the number of parameters [17].
In order to achieve sufﬁciently fast computation we carefully implemented the graphical lasso algo-
rithm tailored to this problem. We used no penalisation for the diagonal and uniform regularisation
parameter value for the off-diagonal elements. The motivation for not penalising the diagonal is
that it has been shown in the covariance estimation setting that the true inverse is approached as the
number of samples is increased [18], and the structure is learned more accurately [19]. The simple
regularisation structure allowed code simpliﬁcation and reduction in memory use. We refactored
the graphical lasso algorithm of [15] and implemented it directly in FORTRAN which we then called
from MATLAB, making sure to minimise matrix copying due to MATLAB processing. This code is
available as a software package, G LA S SO FA S T [20].
In the current context, the use of this approach allows us to obtain sparse approximations to the
inverse metric tensor, which may then be used in an MCMC proposal. Indeed, even if we have
access to an analytic metric tensor we need not use the full inverse for our proposals; we could still
obtain an approximate sparse representation, which may be beneﬁcial computationally. The metric
tensor varies smoothly across a Riemannian manifold and, theoretically, if we are calculating the
inverse of 2 metric tensors that are close to each other, they may be numerically similar enough to
be able to use the solution of one to speed up convergence of solution for the other, although in the
simulations in this paper we found no beneﬁt in doing so, i.e. the metric tensor varied too much as
the MCMC sampler took large steps across the manifold.

6 Simulation Study

We consider a challenging class of statistical models that severely tests the sampling capability
of MCMC methods; in particular, two examples based on nonlinear differential equations using a

6

(a) Exact full inverse

(b) Approximate sparse inverse

Figure 1: In this comparison we plotted the exact and the sparse approximate inverses of a typical
metric tensor G; we note that only subsets of parameters are typically strongly correlated in the sta-
tistical models we consider here and that the sparse approximation still captures the main correlation
structure present. Here the dimension is p = 25, and the regularisation parameter γ is 0.05 · ||G||∞ .

Table 1: Summary of results for the Fitzhugh-Nagumo model with 10 runs of each parameter sam-
pling scheme and 5000 posterior samples.
Sampling
Time (s)
Method
Metropolis
MALA
mMALA Simp.

Mean ESS
(a, b, c)
139, 18.2, 23.4
119.3, 28.7, 52.3
283.4, 136.6, 173.7

14.5
24.9
35.9

Total Time/
(Min mean ESS)
0.80
0.87
0.26

Relative
Speed
×1.1
×1.0
×3.4

biologically motivated robust Student-t likelihood, which renders the metric tensor analytically in-
tractable. We examine the efﬁciency of our MCMC method with approximate metric on a well stud-
ied toy example, the Fitzhugh-Nagumo model, before examining a realistic, nonlinear and highly
challenging example describing enzymatic circadian control in the plant Arabidopsis thaliana [22].

6.1 Nonlinear Ordinary Differential Equations

Statistical modelling using systems of nonlinear ordinary differential equations plays a vital role in
unravelling the structure and behaviour of biological processes at a molecular level. The well-used
Gaussian error model however is often inappropriate, particularly in molecular biology where lim-
ited measurements may not be repeated under exactly the same conditions and are susceptible to
bias and systematic errors. The use of a Student-t distribution as a likelihood may help the robust-
ness of the model with respect to possible outliers in the data. This presents a problem for standard
manifold MCMC algorithms as it makes the metric tensor analytically intractable. We consider
ﬁrst the Fitzhugh-Nagumo model [1]. This synthetic dataset consisted of 200 time points simulated
from the model between t = [0, 20] with parameters [a, b, c] = [0.2, 0.2, 3], to which Gaussian dis-
tributed noise was added with variance σ2 = 0.25. We employed a Student-t likelihood with scaling
parameter v = 3, and compared M-H and MALA (both employing scaled isotropic covariances),
and simpliﬁed mMALA with approximate metric. The stepsize for each was automatically adjusted
during the burn-in phase to obtain the theoretically optimal acceptance rate.
Table 1 shows the results including time-normalised effective sample size (ESS) as a measure of
sampling efﬁciency excluding burn-in [1]. The approximate manifold sampler offers a modest im-
provement on the other two samplers; despite taking longer to run because of the computational
cost of estimating the metric, the samples it draws exhibit lower autocorrelation, and as such the
approximate manifold sampler offers the highest time-normalised ESS.
The toy Fitzhugh-Nagumo model is however rather simple, and despite being a popular example is
rather unlike many realistic models used nowadays in the molecular modelling community. As such
we consider another larger model that describes the enzymatic control of the circadian networks in
Arabidopsis thaliana [21]. This is an extremely challenging, highly nonlinear model. We consider

7

Table 2: Comparison of pseudodata sample size on the quality of metric tensor estimation, and
hence on sampling efﬁciency, using the circadian network example model, with 10 runs and 10,000
posterior samples.
Number of Time (s) Min Mean ESS
Samples
10
20
30
40

Total Time/
(Min mean ESS)
1.90
0.95
0.81
0.84

Relative
Speed
×1.0
×2.0
×2.35
×2.26

155.6
163.2
168.9
175.2

85.1
171.9
209.1
208.3

Table 3: Summary of results for the circadian network model with 10 runs of each parameter sam-
pling scheme and 10,000 posterior samples.
Sampling
Time (s) Min Mean ESS
Method
Metropolis
MALA
Adaptive MCMC
mMALA Simp.

Total Time/
(Min mean ESS)
6.2
27.4
2.34
0.81

Relative
Speed
×4.4
×1.0
×11.7
×33.8

37.1
101.3
110.4
168.9

6.0
3.7
46.7
209.1

inferring the 6 rate parameters that control production and decay of proteins in the nucleus and
cytoplasm (see [22] for the equations and full details of the model), again employing a Student-t
likelihood for which the Expected Fisher Information is analytically intractable. We used parameter
values from [22] to simulate observations for each of the six species at 48 time points representing
48 hours in the model. Student-t distributed noise was then added to obtain the data for inference.
We ﬁrst investigated the effect that the tangent vector sample size for covariance estimation has on
the sampling efﬁciency of simpliﬁed mMALA. The results in Table 2 show that there is a threshold
above which a more accurate estimate of the metric tensor does not result in additional sampling
advantage. The threshold for this particular example model is around 30 pseudodata samples. Table
3 shows the time normalised statistical efﬁciency for each of the sampling methods; this time we
also compare an Adaptive MCMC algorithm [2] with M-H, MALA, and simpliﬁed mMALA with
approximate geometry. Both the M-H and MALA algorithms fail to explore the target distribution
and have severe difﬁculties with the extreme scalings and nonlinear correlation structure present in
the manifold. The Adaptive MCMC method works reasonably well after taking 2000 samples to
learn the covariance structure, although its performance is still poorer than the simpliﬁed mMALA
scheme, which converges almost immediately with no adaptation time required; the approximation
mMALA makes of the local geometry allows it to adequately deal with the different scalings and
correlations that occur in different parts of the space.

7 Conclusions

The use of Riemannian geometry can be very useful for enabling efﬁcient sampling from arbitrary
probability densities. The metric tensor may be used for creating position-speciﬁc proposal mech-
anisms that allow MCMC methods to automatically adapt to the local correlation structure induced
by the sensitivities of the parameters of a statistical model. The metric tensor may conveniently be
deﬁned as the Expected Fisher Information, however this quantity is often either difﬁcult or impos-
sible to compute analytically. We have presented a sampling scheme that approximates the Expected
Fisher Information by estimating the covariance structure of the tangent vectors at each point on the
manifold. By considering this problem as one of inverse covariance estimation, this naturally led
us to consider the use of (cid:96)1 regularisation to improve the estimation procedure. This had the added
beneﬁt of inducing sparsity into the metric tensor, which may offer computational advantages when
proposing MCMC moves across the manifold. For future work it will be exciting to investigate the
potential impact of approximate, sparse metric tensors for high dimensional problems.

8

Ben Calderhead gratefully acknowledges his Research Fellowship through the 2020 Science pro-
gramme, funded by EPSRC grant number EP/I017909/1 and supported by Microsoft Research.

References

[1] M. Girolami and B. Calderhead, Riemann Manifold Langevin and Hamiltonian Monte Carlo
Methods (with discussion), Journal of the Royal Statistical Society: Series B, 73:123-214, 2011
[2] H. Haario, E. Saksman and J. Tamminen, An Adaptive Metropolis Algorithm, Bernoulli,
7(2):223-242, 2001
[3] G. Roberts and J. Rosenthal, Examples of Adaptive MCMC, Journal of Computational and
Graphical Statistics, 18(2), 2009
[4] G. Roberts and O. Stramer, Langevin diffusions and Metropolis-Hastings algorithms, Methodol.
Comput. Appl. Probab., 4, 337-358, 2003
[5] H. Tjelmeland and B. Hegstad, Mode Jumping Proposals in MCMC, Scandinavian Journal of
Statistics, 28(1), 2001
[6] S. Amari and H. Nagaoka, Methods of Information Geometry, Oxford University Press, 2000
[7] Y. Qi and T. Minka, Hessian-based Markov Chain Monte-Carlo algorithms, 1st Cape Cod Work-
shop Monte Carlo Methods, 2002
[8] A. Honkela, T. Raiko, M. Kuusela, M. Tornio and J. Karhunen, Approximate Riemannian con-
jugate gradient learning for ﬁxed-form variational Bayes, JMLR, 11:3235-3268, 2010
[9] M. K. Murray and J. W. Rice, Differential Geometry and Statistics, Chapman and Hall, 1993
[10] C. R. Rao, Information and accuracy attainable in the estimation of statistical parameters, Bull.
Calc. Math. Soc., 37:81-91, 1945
[11] H. Jeffreys, Theory of Probability, 1st ed. The Clarendon Press, Oxford, 1939
[12] J. Kent, Time reversible diffusions, Adv. Appl. Probab., 10:819-835, 1978
[13] J. Besag, P. Green, D. Higdon, and K. Mengersen, Bayesian Computation and Stochastic Sys-
tems, Statistical Science, 10(1):3-41, 1995
[14] A. Doucet, P. Jacob and A. Johansen, Discussion of Riemann Manifold Langevin and Hamilto-
nian Monte Carlo Methods, Journal of the Royal Statistical Society: Series B, 73:162, 2011
[15] J. Friedman, T. Hastie and R. Tibshirani, Sparse inverse covariance estimation with the graphi-
cal lasso, Biostatistics, 9(3):432-441, 2008
[16] O. Banerjee, L. El Ghaoui and A. d’Aspremont, Model Selection Through Sparse Maximum
Likelihood Estimation for Multivariate Gaussian or Binary Data, JMLR, 9(6), 2008
[17] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu., Model selection in Gaussian graphical
models: High-dimensional consistency of (cid:96)1 -regularized MLE, NIPS 21, 2008
[18] A. J. Rothman, P. J. Bickel, E. Levina and J. Zhu, Sparse permutation invariant covariance
estimation, Electronic Journal of Statistics, 2:494-515, 2008
[19] J. Duchi, S. Gould and D. Koller, Projected Subgradient Methods for Learning Sparse Gaus-
sians, Conference on Uncertainty in Artiﬁcial Intelligence, 2008
[20] M. A. Sustik and B. Calderhead, G LA S SO FA S T: An efﬁcient G LA S SO implementation, Techni-
cal Report, Computer Science Department, University of Texas at Austin, TR-12-29, 2012
[21] J. C. W. Locke, A. Millar and M. Turner, Modelling genetic networks with noisy and varied
experimental data: the circadian clock in Arabidopsis thaliana, J. Theor. Biol. 234:383-393, 2005
[22] B. Calderhead and M. Girolami, Statistical analysis of nonlinear dynamical systems using dif-
ferential geometric sampling methods, Journal of the Royal Society Interface Focus, 1(6), 2011
[23] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical
Society: Series B, 58:267-288, 1996

9


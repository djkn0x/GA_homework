Learning from the Wisdom of Crowds by Minimax
Entropy

Dengyong Zhou, John C. Platt, Sumit Basu, and Yi Mao
Microsoft Research
1 Microsoft Way, Redmond, WA 98052
{denzho,jplatt,sumitb,yimao}@microsoft.com

Abstract

An important way to make large training sets is to gather noisy labels from crowds
of nonexperts. We propose a minimax entropy principle to improve the quality
of these labels. Our method assumes that labels are generated by a probability
distribution over workers, items, and labels. By maximizing the entropy of this
distribution, the method naturally infers item confusability and worker expertise.
We infer the ground truth by minimizing the entropy of this distribution, which we
show minimizes the Kullback-Leibler (KL) divergence between the probability
distribution and the unknown truth. We show that a simple coordinate descent
scheme can optimize minimax entropy. Empirically, our results are substantially
better than previously published methods for the same problem.

1

Introduction

There is an increasing interest in using crowdsourcing to collect labels for machine learning [19,
6, 21, 17, 20, 10, 13, 12]. Currently, many companies provide crowdsourcing services. Amazon
Mechanical Turk (MTurk) [2] and CrowdFlower [4] are perhaps the most well-known ones. An
advantage of crowdsourcing is that we can obtain a large number of labels at the low cost of pennies
per label. However, these workers are not experts, so the labels collected from them are often fairly
noisy. A fundamental challenge in crowdsourcing is inferring ground truth from noisy labels by a
crowd of nonexperts.
When each item is labeled several times by different workers, a straightforward approach is to use
the most common label as the true label. From reported experimental results on real crowdsourcing
data [19] and our own experience, majority voting performs signiﬁcantly better on average than
individual workers. However, majority voting considers each item independently. When many items
are simultaneously labeled, it is reasonable to assume that the performance of a worker is consistent
across different items. This assumption underlies the work of Dawid and Skene [5, 18, 19, 11, 17],
where each worker is associated with a probabilistic confusion matrix that generates her labels. Each
entry of the matrix indicates the probability that items in one class are labeled as another. Given the
observed responses, the true labels for each item and the confusion matrices for each worker can be
jointly estimated by a maximum likelihood method. The optimization can be implemented by the
expectation-maximization (EM) algorithm [7].
Dawid and Skene’s method works well in practice. However, their method only contains a per-
worker probabilistic confusion model of generating labels.
In this paper, we assume a separate
probabilistic distribution for each worker-item pair. We propose a novel minimax entropy principle
to jointly estimate the distributions and the ground truth given the observed labels by workers in
Section 2. The theoretical justiﬁcation of minimum entropy is given in Section 2.1. To prevent over-
ﬁtting, we relax the minimax entropy optimization in Section 3. We describe an easy-to-implement
technique to carry out the minimax program in Section 4 and link minimax entropy to a principle of

1

item 1
worker 1
z11
worker 2
z21
...
...
worker m zm1

item 2
z12
z22
...
zm2

...
...
...
...
...

item n
z1n
z2n
...
zmn

item 1
worker 1
π11
worker 2
π21
...
...
worker m πm1

item 2
π12
π22
...
πm2

...
...
...
...
...

item n
π1n
π2n
...
πmn

Figure 1: Left: observed labels. Right: underlying distributions. Highlights on both tables indicate
that rows and columns of the distributions are constrained by sums over observations.

objective measurements in Section 5. Finally, we present superior experimental results on real-world
crowdsourcing data in Section 6.

2 Minimax Entropy Principle

We propose a model illustrated in Figure 1. Each row corresponds to a crowdsourced worker indexed
by i (from 1 to m). Each column corresponds to an item to be labeled, indexed by j (from 1 to n).
Each item has an unobserved label represented as a vector yj l , which is 1 when item j is in class l
(from 1 to c), and 0 otherwise. More generally, we can treat yj l as the probability that item j is in
class l. We observe a matrix of labels zij by workers. The label matrix can also be represented as a
tensor zijk , which is 1 when worker i labels item j as class k , and 0 otherwise. We assume that zij
are drawn from πij , which is the distribution for worker i to generate a label for item j. Again, πij
can also be represented as a tensor πijk , which is the probability that worker i labels item j as class
k . Our method will estimate yj l from the observed zij .
We specify the form of πij through the maximum entropy principle, where the constraints on the
per class per item (cid:80)
i zijk should match (cid:80)
maximum entropy combine the best ideas from previous work. Majority voting suggests that we
should be constraining the πij per row, with the empirical confusion matrix per worker (cid:80)
should be constraining the πij per column, with the empirical observation of the number of votes
should match (cid:80)
i πijk . Dawid and Skene’s method suggests that we
j yj l zijk
j yj lπijk . We thus have the following maximum entropy model for πij given yj l :
− m(cid:88)
c(cid:88)
n(cid:88)
πijk ln πijk
m(cid:88)
m(cid:88)
n(cid:88)
j=1
i=1
k=1
c(cid:88)
i=1
i=1
j=1
πijk = 1, ∀i, j, πijk ≥ 0, ∀i, j, k .
k=1

yj l zijk , ∀i, k , l,

zijk , ∀j, k ,

n(cid:88)
j=1

yj lπijk =

πijk =

max
π

(1b)

(1a)

s.t.

min
y

We propose that, to infer yj l , we should choose yj l to minimize the entropy in Equation (1). Intu-
itively, making πij “peaky” means that zij is the least random given yj l . We make this intuition
rigorous in Section 2.1. Thus, the inference for yj l can be expressed by a minimax entropy program:
− m(cid:88)
n(cid:88)
c(cid:88)
πijk ln πijk
m(cid:88)
m(cid:88)
n(cid:88)
i=1
j=1
k=1
c(cid:88)
i=1
i=1
j=1
πijk = 1, ∀i, j, πijk ≥ 0, ∀i, j, k ,
k=1

n(cid:88)
yj lπijk =
c(cid:88)
j=1
yj l = 1, ∀j, yj l ≥ 0, ∀j, l.
l=1

yj l zijk , ∀i, k , l,

zijk , ∀j, k ,

πijk =

max
π

(2b)

(2a)

s.t.

2

2.1

Justiﬁcation for Minimum Entropy

(3)

(4)

max
π

s.t.

yj lπijk = ϕikl , ∀i, k , l.

Now we justify the principle of choosing yj l by minimizing entropy. Think of yj l as a set of param-
eters to the worker-item label models πij . The goal in choosing the yj l is to select πij that are as
close as possible to the true distributions π∗
ij .
measurements φjk = (cid:80)
ijk and row measurements ϕikl = (cid:80)
To ﬁnd a principle to choose the yj l , assume that we have access to the row and column measure-
ments on the true distributions π∗
ij . That is, assume that we know the true values of the column
i π∗
j yj lπ∗
ijk , for a chosen set of yj l
values. Knowing these true row and column measurements, we can apply the maximum entropy
− m(cid:88)
n(cid:88)
c(cid:88)
principle to generate distributions πij :
πijk ln πijk
m(cid:88)
n(cid:88)
i=1
j=1
k=1
πijk = φjk , ∀j, k ,
i=1
j=1
Let DKL (· (cid:107) ·) denote the KL divergence between two distributions. We can choose yj l to minimize
a loss of πij with respect to π∗
n(cid:88)
m(cid:88)
ij given by
ij (cid:107) πij ).
DKL (π∗
(cid:96)(π∗ , π) =
j=1
i=1
The minimum loss can be attained by choosing yj l to minimize the entropy of the maximum distri-
(cid:19)
(cid:18) c(cid:88)
L = − m(cid:88)
n(cid:88)
c(cid:88)
m(cid:88)
n(cid:88)
butions πij . This can be shown by writing the Lagrangian of program (3):
πijk − 1
πijk ln πijk +
λij
n(cid:88)
m(cid:88)
m(cid:88)
c(cid:88)
c(cid:88)
c(cid:88)
n(cid:88)
i=1
j=1
i=1
j=1
k=1
k=1
(πijk − π∗
ijk ) +
j=1
j=1
i=1
i=1
l=1
k=1
k=1
where the newly introduced variables τjk and σikl are the Lagrange multipliers. For a solution to be
c(cid:88)
optimal, the Karush-Kuhn-Tucker (KKT) conditions must be satisﬁed [3]. Thus,
yj l (τjk + σikl ) = 0, ∀i, j, k ,
= − ln πijk − 1 + λij +
(cid:21)
(cid:20) c(cid:88)
l=1
yj l (τjk + σikl ) + λij − 1
, ∀i, j, k .
πijk = exp
l=1
(cid:21)
(cid:20) c(cid:88)
c(cid:88)
c(cid:88)
For being a probability measure, the variables πijk have to satisfy
yj l (τjk + σikl ) + λij − 1
exp
l=1
k=1
k=1
exp (cid:80)c
Eliminating λij by jointly considering Equations (5) and (6), we obtain a labeling model in the
s=1 exp (cid:80)c
(cid:80)c
exponential family:
l=1 yj l (τjk + σikl )
l=1 yj l (τj s + σisl )
Plugging Equation (7) into (4) and performing some algebraic manipulations, we prove
m(cid:88)
n(cid:88)
c(cid:88)
Theorem 2.1 Let πij be the maximum entropy distributions in (3). Then,
ijk − πijk ln πijk ).
(cid:96)(π∗ , π) =
(π∗
ijk ln π∗
j=1
i=1
k=1

yj l (πijk − π∗
ijk ),

which can be rearranged as

= 1, ∀i, j.

(5)

(6)

(7)

+

τjk

σikl

, ∀i, j, k .

∂L
∂πijk

πijk =

πijk =

3

The second term is the only term that depends on yj l . Therefore, we should choose yj l to minimize
the entropy of the maximum entropy distributions.
The labeling model expressed by Equation (7) has a natural interpretation. For each worker i, the
multiplier set {σikl } is a measure of her expertise, while for each item j, the multiplier set {τjk } is a
measure of its confusability. A worker correctly labels an item either because she has good expertise
or because the item is not that confusing. When the item or worker parameters are shifted by an
arbitrary constant, the probability given by Equation (7) does not change. The redundancy of the
constraints in (2a) causes the redundancy of the parameters.

3 Constraint Relaxation

s.t.

min
y

max
π ,ξ ,ζ

ζ 2
ikl
2βi

ξ 2
jk
2αj

In real crowdsourcing applications, each item is usually labeled only a few times. Moreover, a
worker usually only labels a small subset of items rather than all of them.
In such cases, it is
unreasonable to expect that the constraints in (2a) hold for the true underlying distributions πij . As
in the literature of regularized maximum entropy [14, 1, 9], we relax the optimization problem to
πijk ln πijk − n(cid:88)
c(cid:88)
− m(cid:88)
c(cid:88)
n(cid:88)
c(cid:88)
c(cid:88)
− m(cid:88)
prevent overﬁtting:
n(cid:88)
m(cid:88)
j=1
j=1
i=1
i=1
k=1
k=1
l=1
k=1
yj l (πijk − zijk ) = ζikl , ∀i, k , l,
(πijk − zijk ) = ξjk , ∀j, k ,
c(cid:88)
c(cid:88)
j=1
i=1
πijk = 1, ∀i, j, πijk ≥ 0, ∀i, j, k ,
l=1
k=1
where αj and βi are regularization parameters. It is obvious that program (8) is reduced to program
(2) when the slack variables ξjk and ζikl are set to zero. The two (cid:96)2 -norm based regularization terms
in the objective function force the slack variables to be not far away from zero. Other vector or
matrix norms, such as the (cid:96)1 -norm and the trace norm, can be applied as well [14, 1, 9]. We choose
the (cid:96)2 -norm only for the sake of simplicity in computation.
The justiﬁcation for minimum entropy in Section 2.1 can be extended to the regularized minimax
entropy formulation (8) with minor modiﬁcations. Instead of knowing the exact marginals, we need
n(cid:88)
m(cid:88)
to choose πij based on noisy marginals:
jk , ∀j, k , ϕikl =
ijk + ξ ∗
π∗
φjk =
j=1
i=1
n(cid:88)
m(cid:88)
We thus maximize the regularized entropy subject to the relaxed constraints:
j=1
i=1

yj lπijk + ζikl = ϕikl , ∀i, k , l.

yj l = 1, ∀j, yj l ≥ 0, ∀j, l,

πijk + ξjk = φjk , ∀j, k ,

ikl , ∀i, k , l.
ijk + ζ ∗
yj lπ∗

(8a)

(8b)

(9)

Lemma 3.1 To be the regularized maximum entropy distributions subject to (9), πij must be repre-
sented as in Equation (7). Moreover, we should have ξjk = αj τjk , ζikl = βiσikl .
Proof The ﬁrst part of the result can be veriﬁed as before. By using the labeling model in Equation
(cid:19)
(cid:18) c(cid:88)
c(cid:88)
− n(cid:88)
c(cid:88)
n(cid:88)
L = − m(cid:88)
c(cid:88)
c(cid:88)
− m(cid:88)
(7), the Lagrangian of the regularized maximum entropy program can be written as
ξ 2
ζ 2
jk
(cid:21)
(cid:20)
(cid:20)
ikl
ln
exp
− n(cid:88)
c(cid:88)
m(cid:88)
− m(cid:88)
c(cid:88)
n(cid:88)
c(cid:88)
2βi
2αj
s=1
j=1
j=1
i=1
i=1
l=1
k=1
l=1
k=1
ijk + (ζikl − ζ ∗
ijk + (ξjk − ξ ∗
yj lπ∗
π∗
jk )
ikl )
j=1
i=1
i=1
j=1
k=1
k=1
l=1
For ﬁxed τjk and σikl , maximizing the Lagrange dual over ξjk and ζikl provides the proof.

yj l (τj s + σisl )

σikl

τjk

+

+

(cid:21)
.

4

By Lemma 3.1 and some algebraic manipulations, we obtain
m(cid:88)
n(cid:88)
c(cid:88)
ijk − m(cid:88)
n(cid:88)
c(cid:88)
πijk ln πijk − n(cid:88)
c(cid:88)
Theorem 3.2 Let πij be the regularized maximum entropy distributions subject to (9). Then,
ijk ln π∗
π∗
− m(cid:88)
n(cid:88)
m(cid:88)
c(cid:88)
c(cid:88)
c(cid:88)
c(cid:88)
c(cid:88)
i=1
j=1
i=1
j=1
j=1
k=1
k=1
k=1
ξ ∗
ζ ∗
jk ξjk
ikl ζikl
αj
βi
i=1
j=1
i=1
l=1
k=1
k=1
l=1
k=1

(cid:96)(π∗ , π) =

ζ 2
ikl
βi

ξ 2
jk
αj

+

+

.

(10)

We cannot minimize the loss by minimizing the right side of Equation (10) since the random noise
is unknown. However, we can consider minimizing an upper bound instead. Note that
ikl )/2, ∀i, k , l.
ikl ζikl ≤ (ζ ∗ 2
jk ξjk ≤ (ξ ∗ 2
jk )/2, ∀j, k ,
ξ ∗
ζ ∗
jk + ξ 2
ikl + ζ 2
Denote by Ω(π , ξ , ζ ) the objective function of the regularized minimax entropy program (8). Sub-
stituting the inequalities in (11) into Equation (10), we have
(cid:96)(π∗ , π) ≤ Ω(π , ξ , ζ ) − Ω(π∗ , ξ ∗ , ζ ∗ ).
So minimizing the regularized maximum entropy leads to minimizing an upper bound of the loss.

(11)

(12)

.

.

+

4 Optimization Algorithm
A typical approach to constrained optimization is to covert the primal problem to its dual form. By
exp (cid:80)c
(cid:80)c
(cid:20) m(cid:89)
(cid:21)
L = − n(cid:88)
c(cid:88)
c(cid:88)
m(cid:88)
c(cid:88)
n(cid:88)
Lemma 3.1, the Lagrangian of program (8) can be written as
(cid:80)c
s=1 exp (cid:80)c
αj τ 2
βiσ2
k=1 zijk
l=1 yj l (τjk + σikl )
jk
ikl
ln
+
The dual problem minimizes L subject to the constraints ∆ = {yj l | (cid:80)c
2
2
l=1 yj l (τj s + σisl )
i=1
j=1
i=1
j=1
l=1
k=1
k=1
l=1 yj l = 1, ∀j, yj l ≥
0, ∀j, l}. It can be solved by coordinate descent with the variables being split into two groups: {yj l }
and {τjk , σikl }. It is easy to check that, when the variables in one group are ﬁxed, the optimization
problem on the variables in the other group is convex. When the yj l are restricted to be {0, 1}, that
exp (cid:80)c
m(cid:89)
is, deterministic labels, the coordinate descent procedure can be simpliﬁed. Let
(cid:80)c
k=1 zijk (τjk + σikl )
For any set of real-valued numbers {µj l | (cid:80)c
pj l =
s=1 exp (τj s + σisl )
i=1
(cid:80)c
exp (cid:80)c
(cid:21)
(cid:20) m(cid:89)
l=1 µj l = 1, ∀j, µj l > 0, ∀j, l}, we have the inequality
c(cid:88)
n(cid:88)
n(cid:88)
(cid:0)deterministic labels(cid:1)
(cid:80)c
s=1 exp (cid:80)c
l=1 yj l (τjk + σikl )
k=1 zijk
(cid:18) yj l pj l
(cid:19) (cid:0)Jensen’s inequality(cid:1)
(cid:18) yj l pj l
(cid:19)
yj l pj l
ln
=
ln
c(cid:88)
n(cid:88)
≥ n(cid:88)
c(cid:88)
l=1 yj l (τj s + σisl )
j=1
i=1
j=1
l=1
ln
µj l
µj l ln(yj l pj l ) − n(cid:88)
c(cid:88)
n(cid:88)
c(cid:88)
µj l
µj l
j=1
j=1
l=1
l=1
j=1
j=1
l=1
l=1
Plugging the last line into the Lagrangian L, we obtain an upper bound of L, called F . It can be
shown that we must have yj l = µj l at any stationary point of F . Our optimization algorithm is a
coordinate descent minimization of this F [15, 7]. We initialize yj l with majority vote in Equation
(13). In each iteration step, we ﬁrst optimize over τjk and σikl in (14a), which can be solved by any
convex optimization procedure, and next optimize over yj l using a simple closed form in (14b). The
optimization over yj l is the same as applying Bayes’ theorem where the result from the last iteration
is considered as a prior. This algorithm can be shown to produce only deterministic labels.

µj l ln µj l .

µj l ln

=

=

5

for t = 1, 2, . . .
ikl } = arg min
{τ t
jk , σ t
τ ,σ

Algorithm 1 Minimax Entropy Learning from Crowds
(cid:80)m
input: {zijk } ∈ {0, 1}m×n×c , {αj } ∈ Rn
+ , {βi } ∈ Rm
+
(cid:80)c
(cid:80)m
initialization:
, ∀j, l
i=1 zij l
y0
j l =
(cid:20)
m(cid:88)
n(cid:88)
c(cid:88)
c(cid:88)
exp(τj s + σisl ) − c(cid:88)
k=1 zijk
i=1
log
m(cid:88)
c(cid:88)
c(cid:88)
n(cid:88)
c(cid:88)
s=1
i=1
j=1
l=1
k=1
αj τ 2
βiσ2
exp (cid:80)c
jk
ikl
+
m(cid:89)
2
2
(cid:1)
s=1 exp (cid:0)τ t
(cid:80)c
i=1
j=1
l=1
k=1
k=1
k=1 zijk (τ t
jk + σ t
ikl )
, ∀j, l
j s + σ t
isl
i=1

y t−1
j l

+

j l ∝ y t−1
y t
j l
output: {y t
j l }

(13)
(cid:21)
zijk (τjk + σikl )

(14a)

(14b)

5 Measurement Objectivity Principle

(15)

The measurement objectivity principle can be roughly stated as follows: (1) a comparison of labeling
confusability between two items should be independent of which particular workers are included for
the comparison; (2) symmetrically, a comparison of labeling expertise between two workers should
be independent of which particular items are included for the comparison. The ﬁrst statement is
about the objectivity of item confusability. The second statement is about the objectivity of worker
expertise. In what follows, we mathematically deﬁne the measurement objectivity principle. For
deterministic labels, we show that the labeling model in Equation (7) can be recovered from the
measurement objectivity principle.
From Equation (7), given item j in class l, the probability that worker i labels it as class k is
(cid:80)c
exp (τjk + σikl )
πijkl =
s=1 exp (τj s + σisl )
Assume that a worker i has labeled two items j and j (cid:48) both of which are from the same class l. With
respect to the given worker i, for each item, we measure the confusability for class k by
πij (cid:48) kl
πijkl
πij (cid:48) ll
πij ll
For comparing the item confusabilities, we compute a ratio between them. To maintain the objectiv-
(cid:19)(cid:30)(cid:18) πij (cid:48) kl
(cid:19)
(cid:18) πi(cid:48) jkl
(cid:19)(cid:30)(cid:18) πi(cid:48) j (cid:48) kl
(cid:19)
(cid:18) πijkl
ity of confusability, the ratio should not depend on whichever worker is involved in the comparison.
Hence, given another worker i(cid:48) , we should have
πij (cid:48) ll
πi(cid:48) j ll
πi(cid:48) j (cid:48) ll
πij ll
It is straightforward to verify that the labeling model in Equation (15) indeed satisﬁes the objectivity
requirement given by Equation (17). We can further show that a labeling model which satisﬁes
Equation (17) has to be expressed by Equation (15). Let us rewrite Equation (17) as
πi(cid:48) j (cid:48) ll
πi(cid:48) jkl
πij (cid:48) kl
πijkl
πij (cid:48) ll
πi(cid:48) j ll
πi(cid:48) j (cid:48) kl
πij ll
Without loss of generality, choose i(cid:48) = 0 and j (cid:48) = 0 as the ﬁxed references such that
πijkl
πi0kl
π0jkl
π00ll
π00kl
π0j ll
πi0ll
πij ll
Assume that the referenced worker 0 chooses a class uniformly at random for the referenced item 0.
So we have π00ll = π00kl = 1/c. Equation (18) implies πijkl ∝ πi0klπ0jkl . Reparameterizing with

ρij (cid:48) k =

ρijk =

(17)

(16)

.

.

=

=

(18)

.

.

.

,

=

6

(a) Norfolk Terrier

(b) Norwich Terrier

(c) Irish Wolfhound

(d) Scottish Deerhound

Figure 2: Sample images of four breeds of dogs from the Stanford dogs dataset

πi0kl = exp(σikl ) and π0jkl = exp(τjk ) (note that l is dropped since it is determined by j ), we have
πijkl ∝ exp(τjk + σikl ). The labeling model in Equation (15) has been recovered.
Symmetrically, we can also start from the objectivity of worker expertise to recover the labeling
model in (15). Assume that two workers i and i(cid:48) have labeled a common item j which is from class
l. With respect to the given item j , for each worker, we measure the confusion from class l to k by
πi(cid:48) jkl
πijkl
πi(cid:48) j ll
πij ll
For comparing the worker expertises, we compute a ratio between them. To maintain the objectivity
of expertise, the ratio should not depend on whichever item is involved in the comparison. Hence,
(cid:19)
(cid:19)(cid:30)(cid:18) πi(cid:48) j (cid:48) kl
(cid:18) πij (cid:48) kl
(cid:19)(cid:30)(cid:18) πi(cid:48) jkl
(cid:18) πijkl
(cid:19)
given another item j (cid:48) in class l, we should have
πi(cid:48) j ll
πij (cid:48) ll
πi(cid:48) j (cid:48) ll
πij ll
We can see that Equation (20) is actually just a rearrangement of Equation (17).

ρi(cid:48) jk =

(19)

(20)

ρijk =

,

.

=

.

6 Experimental Validation

We compare our method with majority voting and Dawid & Skene’s method [5] using real crowd-
sourcing data. One is multiclass image labeling, and the other is web search relevance judging.

6.1

Image Labeling

We chose the images of 4 breeds of dogs from the Stanford dogs dataset [8]: Norfolk Terrier (172),
Norwich Terrier (185), Irish Wolfhound (218), and Scottish Deerhound (232) (see Figure 2). The
numbers of the images for each breed are in the parentheses. There are 807 images in total. We
submitted them to MTurk, and received the labels from 109 MTurk workers. A worker labeled an
image at most once, and each image was labeled 10 times. It is difﬁcult to evaluate a worker if
she only labeled few images. We thus only consider the workers who labeled at least 40 images,
which yields a label set that contains 7354 labels by 52 workers. Each image has at least 4 la-
bels and around 95% of the images have at least 8 labels. The average accuracy of the workers
is 70.60%. The best worker achieved an accuracy of 88.24% while only labeled 68 images. The
worker who labeled the most labeled 345 images and achieved an accuracy of 68.99%. The av-
erage worker confusion matrix between breeds is shown in Table 2. As expected, it consists of
two blocks. One block contains Norfolk Terrier and Norwich Terrier, and the other block contains
Irish Wolfhound and Scottish Deerhound. For our method, the regularization parameters are set
as αj = 100/(number of labels for item j ), βi = 100/(number of labels by worker i). The perfor-
mance of various methods on this image labeling task is summarized in Table 1. For this problem,
our method is somewhat better than Dawid and Skene’s method.

6.2 Web Search Relevance Judging

In another experiment, we asked workers to rate a set of 2665 query-URL pairs on a relevance rating
scale from 1 to 5. The larger the rating, the more relevant the URL. The true labels were derived by

7

Method
Minimax Entropy
Dawid & Skene
Majority Voting
Average Worker

Dogs Web
88.05
84.63
83.98
84.14
73.07
82.09
70.60
37.05

Norfolk
Norwich
Irish
Scottish

Norfolk Norwich
27.35
71.04
66.71
31.99
0.55
1.19
1.20
0.38

Irish
1.03
1.13
69.35
26.77

Scottish
0.58
0.18
28.91
71.65

Table 1: Accuracy of methods (%)

Table 2: Average worker confusion (%)

using consensus from 9 experts. The noisy labels were provided by 177 nonexpert workers. Each
pair was judged by around 6 workers, and each worker judged a subset of the pairs. The average
accuracy of workers is 37.05%. Seventeen workers have an accuracy of 0 and they judged at most 7
pairs. The worker who judged the most judged 1225 pairs and achieved an accuracy of 76.73%. For
our method, the regularization parameters are set as αj = 200/(number of labels for item j ), βi =
200/(number of labels by worker i). The performance of various methods on this relevance judging
task is summarized in Table 1. In this case, our method is substantially better.

7 Related Work

This paper can be regarded as a natural extension to Dawid and Skene’s work [5], discussed in Sec-
tion 1. Our approach can be reduced to Dawid and Skene’s by setting the regularization parameters
to be αj = ∞, βi = 0. The essential difference between our work and Dawid and Skene’s work is
that, in addition to worker expertise, we also take item confusability into account.
In computer vision, a minimax entropy method was proposed for estimating the probability density
of certain visual patterns such as textures [22]. The authors compute empirical marginal distributions
through various features, then construct a density model that can reproduce all empirical marginal
distributions. Among all models satisfying the constraints, the one with maximum entropy is pre-
ferred. However, one wants to select the features which are most informative: the constructed model
should approximate the underlying density by minimizing a KL divergence. The authors formulate
the combined density estimation and feature selection as a minimax entropy problem.
The measurement objectivity principle is inspired by the Rasch model [16], used to design and ana-
lyze psychological and educational measurements. In the Rasch model, given an examinee and a test
item, the probability of a correct response is modeled as a logistic function of the difference between
the examinee ability and the item difﬁculty. Rasch deﬁned “speciﬁc objectivity”: the comparison of
any two subjects can be carried out in such a way that no other parameters are involved than those
of the two subjects. The speciﬁc objectivity property of the Rasch model comes from the algebraic
separation of examinee and item parameters. If the probability of a correct response is modeled with
other forms, such as a logistic function of the ratio between the examinee ability and the item difﬁ-
culty [21], objective measurements cannot be achieved. The most fundamental difference between
the Rasch model and our work is that we must infer ground truth, rather than take them as given.

8 Conclusion

We have proposed a minimax entropy principle for estimating the true labels from the judgements
of a crowd of nonexperts. We have also shown that the labeling model derived from the minimax
entropy principle uniquely satisﬁes an objectivity principle for measuring worker expertise and item
confusability. Experimental results on real-world crowdsourcing data demonstrate that the proposed
method estimates ground truth more accurately than previously proposed methods. The presented
framework can be easily extended. For example, in the web search experiment, the multilevel rel-
evance scale is treated as multiclass. By taking the ordinal property of ratings into account, the
accuracy may be further improved. The framework could be extended to real-valued labels. A
detailed discussion on those topics is beyond the scope of this paper.

Acknowledgments

We thank Daniel Hsu, Xi Chen, Chris Burges and Chris Meek for helpful discussions, and Gabriella
Kazai for generating the web search dataset.

8

References
[1] Y. Altun and A. Smola. Unifying divergence minimization and statistical inference via convex
duality. In Proceedings of the 19th Annual Conference on Learning Theory, 2006.
[2] Amazon Mechanical Turk. https://www.mturk.com/mturk.
[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[4] CrowdFlower. http://crowdﬂower.com/.
[5] A. P. Dawid and A. M. Skene. Maximum likeihood estimation of observer error-rates using
the EM algorithm. Journal of the Royal Statistical Society, 28(1):20–28, 1979.
[6] O. Dekel and O. Shamir. Vox populi: Collecting high-quality labels from a crowd. In Proceed-
ings of the 22nd Annual Conference on Learning Theory, 2009.
[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38, 1977.
[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A large-scale hi-
erarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 248–255, 2009.
[9] M. Dudik, S. J. Phillips, and R. E. Schapire. Maximum entropy density estimation with gener-
alized regularization and an application to species distribution modeling. Journal of Machine
Learning Research, 8:1217–1260, 2007.
[10] S. Ertekin, H. Hirsh, and C. Rudin. Approximating the wisdom of the crowd. In Proceedings
of the Workshop on Computational Social Science and the Wisdom of Crowds, 2011.
[11] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on Amazon Mechanical Turk. In
Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 64–67, 2010.
[12] E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in large-
In Proceedings of the 11th International Conference on Autonomous
scale crowdsourcing.
Agents and Multiagent Systems, pages 467–474, 2012.
[13] D. R. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. In
Advances in Neural Information Processing Systems 24, pages 1953–1961, 2011.
[14] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. In
Advances in Neural Information Processing Systems 14, pages 447–454, 2001.
[15] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse,
In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368.
and other variants.
Kluwer Academic, Dordrecht, MA, 1998.
[16] G. Rasch. On general laws and the meaning of measurement in psychology. In Proceedings
of the 4th Berkeley Symposium on Mathematical Statistics and Probability, volume 4, pages
321–333, Berkeley, CA, 1961.
[17] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.
[18] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective
labelling of venus images. In Advances in neural information processing systems, pages 1085–
1092, 1995.
[19] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast—but is it good? Evalu-
ating non-expert annotations for natural language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pages 254–263, 2008.
[20] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds.
In Advances in Neural Information Processing Systems 23, pages 2424–2432, 2010.
[21] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more:
optimal integration of labels from labelers of unknown expertise. In Advances in Neural Infor-
mation Processing Systems 22, pages 2035–2043, 2009.
[22] S. C. Zhu, Y. N. Wu, and D. B. Mumford. Minimax entropy principle and its applications to
texture modeling. Neural Computation, 9:1627–1660, 1997.

9


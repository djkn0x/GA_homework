Random function priors for exchangeable arrays with
applications to graphs and relational data

James Robert Lloyd
Department of Engineering
University of Cambridge

Peter Orbanz
Department of Statistics
Columbia University

Zoubin Ghahramani
Department of Engineering
University of Cambridge

Daniel M. Roy
Department of Engineering
University of Cambridge

Abstract

A fundamental problem in the analysis of structured relational data like graphs,
networks, databases, and matrices is to extract a summary of the common struc-
ture underlying relations between individual entities. Relational data are typically
encoded in the form of arrays; invariance to the ordering of rows and columns
corresponds to exchangeable arrays. Results in probability theory due to Aldous,
Hoover and Kallenberg show that exchangeable arrays can be represented in terms
of a random measurable function which constitutes the natural model parameter in
a Bayesian model. We obtain a ﬂexible yet simple Bayesian nonparametric model
by placing a Gaussian process prior on the parameter function. Efﬁcient inference
utilises elliptical slice sampling combined with a random sparse approximation
to the Gaussian process. We demonstrate applications of the model to network
data and clarify its relation to models in the literature, several of which emerge as
special cases.

1

Introduction

Structured relational data arises in a variety of contexts, including graph-valued data [e.g. 1, 5],
micro-array data, tensor data [e.g. 27] and collaborative ﬁltering [e.g. 21]. This data is typiﬁed by
expressing relations between 2 or more objects (e.g. friendship between a pair of users in a social
network). Pairwise relations can be represented by a 2-dimensional array (a matrix); more generally,
relations between d-tuples are recorded as d-dimensional arrays (d-arrays). We consider Bayesian
models of inﬁnite 2-arrays (Xij )i,j∈N , where entries Xij take values in a space X . Each entry
Xij describes the relation between objects i and j . Finite samples—relational measurements for n
objects—are n × n-arrays. As the sample size increases, the data aggregates into a larger and larger
array. Graph-valued data, for example, corresponds to the case X = {0, 1}. In collaborative ﬁltering
problems, the set of objects is subdivided into two disjoint sets, e.g., users and items.
Latent variable models for such data explain observations by means of an underlying structure or
summary, such as a low-rank approximation to an observed array or an embedding into a Euclidean
space. This structure is formalized as a latent (unobserved) variable. Examples include matrix
factorization [e.g. 4, 21], non-linear generalisations [e.g. 12, 27, 28], block modelling [e.g. 1, 10],
latent distance modelling [e.g. 5] and many others [e.g. 14, 17, 20].
Hoff [4] ﬁrst noted that a number of parametric latent variable models for relational data are
exchangeable—an applicable assumption whenever the objects in the data have no natural order-
ing e.g., users in a social network or products in ratings data—and can be cast into the common
functional form guaranteed to exist by results in probability theory. Building on this connection,

1

U1

U2

0

0

U1

U2

0
Pr{Xij = 1}

Θ

1

1

1

Figure 1: Left: The distribution of any exchangeable random graph with vertex set N and edges
E = (Xij )i,j∈N can be characterised by a random function Θ : [0, 1]2 → [0, 1]. Given Θ, a graph
can be sampled by generating a uniform random variable Ui for each vertex i, and sampling edges as
Xij ∼ Bernoulli(Θ(Ui , Uj )). Middle: A heat map of an example function Θ. Right: A 100 × 100
symmetric adjacency matrix sampled from Θ. Only unordered index pairs Xij are sampled in the
symmetric case. Rows and columns have been ordered by increasing value of Ui , rather than i.

we consider nonparametric models for graphs and arrays. Results of Aldous [2], Hoover [6] and
Kallenberg [7] show that random arrays that satisfy an exchangeability property can be represented
in terms of a random function. These representations have been further developed in discrete analy-
sis for the special case of graphs [13]; this case is illustrated in Fig. 1. The results can be regarded as
a generalization of de Finetti’s theorem to array-valued data. Their implication for Bayesian model-
ing is that we can specify a prior for an exchangeable random array model by specifying a prior on
(measurable) functions. The prior is a distribution on the space of all functions that can arise in the
representation result, and the dimension of this space is inﬁnite. A prior must therefore be nonpara-
metric to have reasonably large support since a parametric prior concentrates on a ﬁnite-dimensional
subset. In the following, we model the representing function explicitly using a nonparametric prior.

2 Background: Exchangeable graphs and arrays

A fundamental component of every Bayesian model is a random variable Θ, the parameter of the
model, which decouples the data. De Finetti’s theorem [9] characterizes this parameter for random
sequences: Let X1 , X2 , . . . be an inﬁnite sequence of random variables, each taking values in a
common space X . A sequence is called exchangeable if its joint distribution is invariant under
arbitrary permutation of the indices, i.e., if
for all π ∈ S∞ .
(X1 , X2 , . . .) d= (Xπ(1) , Xπ(2) , . . .)
(2.1)
Here, d= denotes equality in distribution, and S∞ is the set of all permutations of N that permute
a ﬁnite number of elements. De Finetti’s theorem states that, (Xi )i∈N is exchangeable if and only
if there exists a random probability measure Θ on X such that X1 , X2 , . . . | Θ ∼iid Θ, i.e., condi-
tioned on Θ, the observations are independent and Θ-distributed. From a statistical perspective, Θ
represents common structure in the observed data—and thus a natural target of statistical inference—
whereas P [Xi |Θ] captures remaining, independent randomness in each observation.

2.1 De Finetti-type representations for random arrays

To specify Bayesian models for graph- or array-valued data, we need a suitable counterpart to de
Finetti’s theorem that is applicable when the random sequences in (2.1) are substituted by random
arrays X = (Xij )i,j∈N . For such data, the invariance assumption (2.1) applied to all elements of X
is typically too restrictive: In the graph case Xij ∈ {0, 1}, for example, the probability of X would
then depend only on the proportion of edges present in the graph, but not on the graph structure.
Instead, we deﬁne exchangeability of random 2-arrays in terms of the simultaneous application of a
permutation to rows and columns. More precisely:
Deﬁnition 2.1. An array X = (Xij )i,j∈N is called an exchangeable array if
for every π ∈ S∞ .
(Xij ) d= (Xπ(i)π(j ) )

(2.2)

2

Since this weakens the hypothesis (2.1) by demanding invariance only under a subset of all permuta-
tions of N2—those of the form (i, j ) (cid:55)→ (π(i), π(j ))—we can no longer expect de Finetti’s theorem
to hold. The relevant generalization of the de Finetti theorem to this case is the following:
Theorem 2.2 (Aldous, Hoover). A random 2-array (Xij ) is exchangeable if and only if there is a
random (measurable) function F : [0, 1]3 → X such that
d
(2.3)
= (F (Ui , Uj , Uij )).
(Xij )
for every collection (Ui )i∈N and (Uij )i≤j∈N of i.i.d. Uniform[0, 1] random variables, where Uj i =
Uij for j < i ∈ N.

there is a random function

2.2 Random graphs
The graph-valued data case X = {0, 1} is of particular interest. Here, the array X , interpreted
as an adjacency matrix, speciﬁes a random graph with vertex set N. For undirected graphs, X is
symmetric. We call a random graph exchangeable if X satisﬁes (2.2).
For undirected graphs, the representation (2.3) simpliﬁes further:
(cid:26)1
Θ : [0, 1]2 → [0, 1], symmetric in its arguments, such that
if Uij < Θ(Ui , Uj )
otherwise
0
satisﬁes (2.3). Each variable Ui is associated with a vertex, each variable Uij with an edge. The
representation (2.4) is equivalent to the sampling scheme
U1 , U2 , . . . ∼iid Uniform[0, 1]
Xij = Xj i ∼ Bernoulli(Θ(Ui , Uj )) ,
and
which is illustrated in Fig. 1.
Recent work in discrete analysis shows that any symmetric measurable function [0, 1]2 → [0, 1]
can be regarded as a (suitably deﬁned) limit of adjacency matrices of graphs of increasing size
[13]—intuitively speaking, as the number of rows and columns increases, the array in Fig. 1 (right)
converges to the heat map in Fig. 1 (middle) (up to a reordering of rows and columns).

F (Ui , Uj , Uij ) :=

(2.4)

(2.5)

2.3 The general case: d-arrays

Theorem 2.2 can in fact be stated in a more general setting than 2-arrays, namely for random d-
arrays, which are collections of random variables of the form (Xi1 ...id )i1 ,...,id∈N . Thus, a sequence
is a 1-array, a matrix a 2-array. A d-array can be interpreted as an encoding of a relation between
d-tuples.
In this general case, an analogous theorem holds, but the random function F in (2.3)
is in general more complex: In addition to the collections U{i} and U{ij} of uniform variables, the
representation requires an additional collection U{ij }j∈I for every non-empty subset I ⊆ {1, . . . , d};
e.g., U{i1 i3 i4 } for d ≥ 4 and I = {1, 3, 4}. The representation (2.3) is then substituted by
F : [0, 1]2d−1 −→ X
and
(Xi1 ,...,id ) d= (F (UI1 , . . . , UI(2d−1)
For d = 1, we recover a version of de Finetti’s theorem. For a discussion of convergence properties
of general arrays similar to those sketched above for random graphs, see [3].
Because we do not explicitly consider the case d > 2 in our experiments, we restrict our presenta-
tion of the model to the 2-array-valued case for simplicity. We note, however, that the model and
inference algorithms described in the following extend immediately to general d-array-valued data.

(2.6)

)) .

3 Model

To deﬁne a Bayesian model for exchangeable graphs or arrays, we start with Theorem 2.2: A distri-
bution on exchangeable arrays can be speciﬁed by a distribution on measurable functions [0, 1]3 →
X . We decompose the function F into two functions Θ : [0, 1]2 → W and H : [0, 1] × W → X for
a suitable space W , such that
(Xij ) d= (F (Ui , Uj , Uij )) = (H (Uij , Θ(Ui , Uj ))) .

(3.1)

3

Such a decomposition always exists—trivially, choose W = [0, 1]2 . The decomposition introduces
a natural hierarchical structure. We initially sample a random function Θ—the model parameter in
terms of Bayesian statistics—which captures the structure of the underlying graph or array. The
(Ui ) then represent attributes of nodes or objects and H and the array (Uij ) model the remaining
noise in the observed relations.
Model deﬁnition. For the purpose of deﬁning a Bayesian model, we will model Θ as a continuous
function with a Gaussian process prior. More precisely, we take W = R and consider a zero-mean
Gaussian process prior on CW := C([0, 1]2 , W ), the space of continuous functions from[0, 1]2 to
W , with kernel function κ : [0, 1]2 × [0, 1]2 → W . The full generative model is then:
Θ ∼ GP (0, κ)
U1 , U2 , . . . ∼iid Uniform[0, 1]
Xij |Wij ∼ P [ . |Wij ]
where Wij = Θ(Ui , Uj ) .
The parameter space of our the model is the inﬁnite-dimensional space CW . Hence, the model is
nonparametric.
Graphs and real-valued arrays require different choices of P . In either case, the model ﬁrst generates
the latent array W = (Wij ). Observations are then generated as follows:
Sample space P [Xij ∈ . |Wij ]
X = {0, 1}
Bernoulli(φ(Wij ))
X = R
Normal(Wij , σ2X )

Observed data
Graph
Real array

(3.2)

where φ is the logistic function, and σ2X is a noise variance parameter.
The Gaussian process prior favors smooth functions, which will in general result in more inter-
pretable latent space embeddings. Inference in Gaussian processes is a well-understood problem,
and the choice of a Gaussian prior allows us to leverage the full range of inference methods available
for these models.
Discussion of modeling assumptions. In addition to exchangeability, our model assumes (i) that
the function Θ is continuous—which implies measurability as in Theorem 2.2 but is a stronger
requirement—and (ii) that its law is Gaussian. Exchangeable, undirected graphs are always rep-
resentable using a Bernoulli distribution for P [Xij ∈ . |Wij ]. Hence, in this case, (i) and (ii) are
indeed the only assumptions imposed by the model. In the case of real-valued matrices, the model
additionally assumes that the function H in (3.1) is of the form
εij ∼iid Normal(0, σ) .
H (Uij , Θ(Ui , Uj )) d= Θ(Ui , Uj ) + εij
(3.3)
where
Another rather subtle assumption arises implicitly when the array X is not symmetric, i.e., not
guaranteed to satisfy Xij = Xj i , for example, if X is a directed graph: In Theorem 2.2, the array
(Uij ) is symmetric even if X is not. The randomness in Uij accounts for both Xij and Xj i which
means the conditional variables Xij |Wij and Xj i |Wj i are dependent, and a precise representation
would have to sample (Xij , Xj i )|Wij , Wj i jointly, a fact our model neglects in (3.2). However, it
can be shown that any exchangeable array can be arbitrarily well approximated by arrays which treat
Xij |Wij and Xj i |Wj i as independent [8, Thm. 2].
Remark 3.1 (Dense vs. sparse data). The methods described here address random arrays that are
of Theorem 2.2: For graph data the asymptotic proportion of present edges is p := (cid:82) Θ(x, y)dxdy ,
dense, i.e., as the size of an n × n array increases the number of non-zero entries grows as O(n2 ).
Network data is typically sparse, with O(n) non-zero entries. Density is an immediate consequence
and the graph is hence either empty (for p = 0) or dense (since O(pn2 ) = O(n2 )). Analogous
representation theorems for sparse random graphs are to date an open problem in probability.

4 Related work

Our model has some noteworthy relations to the Gaussian process latent variable model (GPLVM);
a dimensionality-reduction technique [e.g. 11]. GPLVMs can be applied to 2-arrays, but doing so
makes the assumption that either the rows or the columns of the random array are independent [12].
In terms of our model, this corresponds to choosing kernels of the form κU ⊗ δ , where ⊗ represents

4

a tensor product1 and δ represents an ‘identity’ kernel (i.e., the corresponding kernel matrix is the
identity matrix). From this perspective, the application of our model to exchangeable real-valued
arrays can be interpreted as a form of co-dimensionality reduction.
For graph data, a related parametric model is the eigenmodel of Hoff [4]. This model, also justiﬁed
by exchangeability arguments, approximates an array with a bilinear form, followed by some link
function and conditional probability distribution.
Available nonparametric models include the inﬁnite relational model (IRM) [10], latent feature re-
lational model (LFRM) [14], inﬁnite latent attribute model (ILA) [17] and many others. A recent
development is the sparse matrix-variate Gaussian process blockmodel (SMGB) of Yan et al. [28].
Although not motivated in terms of exchangeability, this model does not impose an independence
assumptions on either rows or columns, in contrast to the GPLVM. The model uses kernels of the
form κ1 ⊗ κ2 ; our work suggests that it may not be necessary to impose tensor product struc-
ture, which allows for inference with improved scaling. Roy and Teh [20] present a nonparametric
Bayesian model of relational data that approximates Θ by a piece-wise constant function with a
speciﬁc hierarchical structure, which is called a Mondrian process in [20].
Some examples of the various available models can be succinctly summarized as follows:

Graph data
∼ GP (0, κ)
Θ
Wij = mUiUj where Ui ∈ {1, . . . , K }
Wij = mUiUj where Ui ∈ {1, . . . , ∞}
Wij = −|Ui − Uj |
Wij = U (cid:48)
Wij = (cid:80)
i ΛUj
i ΛUj where Ui ∈ {0, 1}∞
Wij = U (cid:48)
where Ui ∈ {0, . . . , ∞}∞
IUid
IUjd Λ(d)
∼ GP (0, κ1 ⊗ κ2 )
UidUjd
d
Θ
Real-valued array data
∼ GP (0, κ)
Θ
=
piece-wise constant random function
Θ
Wij = U (cid:48)
i Vj
∼ GP (0, κ ⊗ δ)
Θ

Random function model
Latent class [26]
IRM [10]
Latent distance [5]
Eigenmodel [4]
LFRM [14]
ILA [17]
SMGB [28]

Random function model
Mondrian process based [20]
PMF [21]
GPLVM [12]

5 Posterior computation

We describe Markov Chain Monte Carlo (MCMC) algorithms for generating approximate samples
from the posterior distribution of the model parameters given a partially observed array. Most impor-
tantly, we describe a random subset-of-regressors approximation that scales to graphs with hundreds
of nodes. Given the relatively straightforward nature of the proposed algorithms and approximations,
we refer the reader to other papers whenever appropriate.

5.1 Latent space and kernel

Theorem 2.2 is not restricted to the use of uniform distributions for the variables Ui and Uij . The
proof remains unchanged if one replaces the uniform distributions with any non-atomic probability
measure on a Borel space. For the purposes of inference, normal distributions are more convenient,
and we henceforth use U1 , U2 , . . . ∼iid N (0, Ir ) for some integer r .
Since we focus on undirected graphical data, we require the symmetry condition Wij = Wj i . This
(cid:0)¯κ(ξ1 , ξ2 ) + ¯κ(ξ1 , ¯ξ2 )(cid:1) + σ2 I
can be achieved by constructing the kernel function in the following way
1
κ(ξ1 , ξ2 ) =
2
¯κ(ξ1 , ξ2 ) = s2 exp(−|ξ1 − ξ2 |2/(2(cid:96)2 ))
1We deﬁne the tensor product of kernel functions as follows:
κU (u1 , u2 ) × κV (v1 , v2 ).

(RBF kernel)
(5.2)
(κU ⊗ κV )((u1 , v1 ), (u2 , v2 )) =

(Symmetry + noise)

(5.1)

5

where ξk = (Uik , Ujk ), ¯ξk = (Ujk , Uik ) and s, (cid:96), σ represent a scale factor, length scale and noise
respectively (see [e.g. 19] for a discussion of kernel functions). We collectively denote the kernel
parameters by ψ .

5.2 Sampling without approximating the model

In the simpler case of a real-valued array X , we construct an MCMC algorithm over the variables
(U, ψ , σX ) by repeatedly slice sampling [16] from the conditional distributions
ψi | ψ−i , σX , U, X
σX | ψ , U, X
Uj | U−j , ψ , σX , X
and
(5.3)
where σX is the noise variance parameter used when modelling real valued data introduced in section
3. Let N = |U{i} | denote the number of rows in the observed array, let ξ be the set of all pairs
(Ui , Uj ) for all observed relations Xij , let O = |ξ | denote the number of observed relations, and let
K represent the O × O kernel matrix between all points in ξ . Changes to ψ affect every entry in
the kernel matrix K and so, naively, the computation of the Gaussian likelihood of X takes O(O3 )
time. The cubic dependence on O seems unavoidable, and thus this naive algorithm is unusable for
all but small data sets.

5.3 A random subset-of-regressor approximation

To scale the method to larger graphs, we apply a variation of a method known as Subsets-of-
Regressors (SoR) [22, 23, 25]. (See [18] for an excellent survey of this and other sparse approx-
imations.) The SoR approximation replaces the inﬁnite dimensional GP with a ﬁnite dimensional
approximation. Our approach is to treat both the inputs and outputs of the GP as latent variables.
In particular, we introduce k Gaussian distributed pseudoinputs η = (η1 , . . . , ηk ) and deﬁne target
values Tj = Θ(ηj ). Writing Kηη for the kernel matrix formed from the pseudoinputs η , we have
(ηi ) ∼iid N (0, I2r )
T | η ∼ N (0, Kηη ).
and
(5.4)
The idea of the SoR approximation is to replace Wij with the posterior mean conditioned on (η , T ),
W = KξηK −1
(5.5)
ηη T ,
where Kξη is the kernel matrix between the latent embeddings ξ and the pseudoinputs η . By consid-
ering random pseudoinputs, we construct an MCMC analogue of the techniques proposed in [24].
The conditional distribution T | U, η , ψ , (σX ), X is amenable to elliptical slice sampling [15]. All
other random parameters, including the (Ui ), can again be sampled from their full conditional dis-
tributions using slice sampling. The sampling algorithms require that one computes expressions
involving (5.5). As a result they cost at most O(k3O) time.

6 Experiments

We evaluate the model on three different network data sets. Two of these data sets—the high school
and NIPS co-authorship data—have been extensively analyzed in the literature. The third data set,
a protein interactome, was previously noted by Hoff [4] to be of interest since it exhibits both block
structure and transitivity.

Data set
High school
NIPS
Protein

Recorded data
high school social network
densely connected subset of coauthorship network
protein interactome

Vertices Reference
e.g. [4]
90
e.g. [14]
234
230
e.g. [4]

We compare performance of our model on these data sets to three other models, probabilistic matrix
factorization (PMF) [21], Hoff ’s eigenmodel, and the GPLVM (see also Sec. 4). The models are
chosen for comparability, since they all embed nodes into a Euclidean latent space. Experiments for
all three models were performed using reference implementations by the respective authors.2

2 Implementations are available for PMF at http://www.mit.edu/~rsalakhu/software.html;
the
for
eignmodel at http://cran.r-project.org/src/contrib/Descriptions/eigenmodel.html; and for the GPLVM at
http://www.cs.man.ac.uk/~neill/collab/ .

6

Figure 2: Protein interactome data. Left: Interactome network. Middle: Sorted adjacency matrix.
The network exhibits stochastic equivalence (visible as block structure in the matrix) and homophily
(concentration of points around the diagonal). Right: Maximum a posteriori estimate of the function
Θ, corresponding to the function in Fig. 1 (middle).

Method
Model
stochastic gradient
PMF [21]
MCMC
Eigenmodel [4]
GPLVM [12]
stochastic gradient
Random function model MCMC

Iterations [burn-in] Algorithm parameters
author defaults
1000
10000 [250]
author defaults
author defaults
20 sweeps
1000 [200]
(see below)

length scale
scale factor
target noise
U
η

log mean
1
2
0.1
-
-

We use standard normal priors on the latent variables
U and pseudo points η , and log normal priors for ker-
nel parameters. Parameters are chosen to favor slice
sampling acceptance after a reasonable number of it-
erations, as evaluated over a range of data sets, sum-
marized in the table on the right. Balancing computa-
tional demands, we sampled T 50 times per iteration
whilst all other variables were sampled once per itera-
tion.
We performed 5-fold cross validation, predicting links in a held out partition given 4 others. Where
the models did not restrict their outputs to values between 0 and 1, we truncated any predictions
lying outside this range. The following table reports average AUC (area under receiver operating
characteristic) for the various models, with numbers for the top performing model set in bold. Sig-
niﬁcance of results is evaluated by means of a t-test with a p-value of 0.05; results for models not
distinguishable from the top performing model in terms of this t-test are also set in bold.

std width
0.5
0.5
0.5
0.5
0.1
0.5
4
-
-
2

AUC results

Data set
High school
1
2
Latent dimensions
0.792
0.747
PMF
0.806
0.742
Eigenmodel
0.775
GPLVM 0.744
RFM 0.815
0.827

3
0.792
0.806
0.782
0.820

1
0.729
0.789
0.888
0.907

NIPS
2
0.789
0.818
0.876
0.914

3
0.820
0.845
0.883
0.919

1
0.787
0.805
0.877
0.903

Protein
2
0.810
0.866
0.883
0.910

3
0.841
0.882
0.873
0.912

The random function model outperforms the other models in all tests. We also note that in all
experiments, a single latent dimension sufﬁces to achieve better performance, even when the other
models use additional latent dimensions.
The posterior distribution of Θ favors functions deﬁning random array distributions that explain the
data well. In this sense, our model ﬁts a probability distribution. The standard inference methods
for GPLVM and PMF applied to relational data, in contrast, are designed to ﬁt mean squared error,
and should therefore be expected to show stronger performance under a mean squared error metric.
As the following table shows, this is indeed the case.

7

RMSE results

Data set
High school
Latent dimensions
1
2
0.242
PMF
0.245
Eigenmodel
0.244
0.238
0.241
GPLVM 0.244
RFM 0.239
0.234

3
0.240
0.236
0.239
0.235

1
0.141
0.141
0.112
0.114

NIPS
2
0.135
0.132
0.109
0.111

3
0.130
0.124
0.106
0.110

1
0.151
0.149
0.139
0.138

Protein
2
0.142
0.142
0.137
0.136

3
0.139
0.138
0.138
0.136

An arguably more suitable metric is comparison in terms of conditional edge probability i.e.,
P (X{ij} | W{ij} ) for all i, j in the held out data. These cannot, however, be computed in a meaning-
ful manner for models such as PMF and GPLVM, which assign a Gaussian likelihood to data. The
next table hence reports only comparisons to the eigenmodel.

Negative log conditional edge probability3
NIPS
High school
Data set
2
2
Latent dimensions
1
81
210
Eigenmodel
220
RFM 205
199
57

3
200
201

1
88
65

Protein
2
92
75

1
96
78

3
86
75

3
75
56

Remark 6.1 (Model complexity and lengthscales). Figure 2 provides a visualisation of Θ when
modeling the protein interactome data using 1 latent dimension. The likelihood of the smooth peak
is sensitive to the lengthscale of the Gaussian process representation of Θ. A Gaussian process prior
introduces the assumption that Θ is continuous. Continuous functions are dense in the space of mea-
surable functions, i.e., any measurable function can be arbitrarily well approximated by a continuous
one. The assumption of continuity is therefore not restrictive, but rather the lengthscale of the Gaus-
sian process determines the complexity of the model a priori. The nonparametric prior placed on
Θ allows the posterior to approximate any function if supported by the data, but by sampling the
lengthscale we allow the model to quickly select an appropriate level of complexity.

7 Discussion and conclusions

There has been a tremendous amount of research into modelling matrices, arrays, graphs and rela-
tional data, but nonparametric Bayesian modeling of such data is essentially uncharted territory. In
most modelling circumstances, the assumption of exchangeability amongst data objects is natural
and fundamental to the model. In this case, the representation results [2, 6, 7] precisely map out the
scope of possible Bayesian models for exchangeable arrays: Any such model can be interpreted as
a prior on random measurable functions on a suitable space.
Nonparametric Bayesian statistics provides a number of possible priors on random functions, but the
Gaussian process and its modiﬁcations are the only well-studied model for almost surely continuous
functions. For this choice of prior, our work provides a general and simple modeling approach
that can be motivated directly by the relevant representation results. The model results in both
interpretable representations for networks, such as a visualisation of a protein interactome, and has
competitive predictive performance on benchmark data.

Acknowledgments

The authors would like to thank David Duvenaud, David Knowles and Konstantina Palla for helpful
discussions. PO was supported by an EPSRC Mathematical Sciences Postdoctoral Research Fel-
lowship (EP/I026827/1). ZG is supported by EPSRC grant EP/I036575/1. DMR is supported by a
Newton International Fellowship and Emmanuel College.

3The precise calculation implemented is − log(P (X{ij} | W{ij} )) × 1000 / (Number of held out edges).

8

In

References
[1] Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). Mixed Membership Stochastic Block-
models. Journal of Machine Learning Research (JMLR), 9, 1981–2014.
[2] Aldous, D. J. (1981). Representations for partially exchangeable arrays of random variables. Journal of
Multivariate Analysis, 11(4), 581–598.
[3] Aldous, D. J. (2010). More uses of exchangeability: Representations of complex random structures. In
Probability and Mathematical Genetics: Papers in Honour of Sir John Kingman.
[4] Hoff, P. D. (2007). Modeling homophily and stochastic equivalence in symmetric relational data.
Advances in Neural Information Processing Systems (NIPS), volume 20, pages 657–664.
[5] Hoff, P. D., Raftery, A. E., and Handcock, M. S. (2002). Latent Space Approaches to Social Network
Analysis. Journal of the American Statistical Association, 97(460), 1090–1098.
[6] Hoover, D. N. (1979). Relations on probability spaces and arrays of random variables. Technical report,
Institute for Advanced Study, Princeton.
[7] Kallenberg, O. (1992). Symmetries on random arrays and set-indexed processes. Journal of Theoretical
Probability, 5(4), 727–765.
[8] Kallenberg, O. (1999). Multivariate Sampling and the Estimation Problem for Exchangeable Arrays. Jour-
nal of Theoretical Probability, 12(3), 859–883.
[9] Kallenberg, O. (2005). Probabilistic Symmetries and Invariance Principles. Springer.
[10] Kemp, C., Tenenbaum, J., Grifﬁths, T., Yamada, T., and Ueda, N. (2006). Learning systems of concepts
In Proceedings of the National Conference on Artiﬁcial Intelligence,
with an inﬁnite relational model.
volume 21.
[11] Lawrence, N. D. (2005). Probabilistic non-linear principal component analysis with Gaussian process
latent variable models. Journal of Machine Learning Research (JMLR), 6, 1783–1816.
[12] Lawrence, N. D. and Urtasun, R. (2009). Non-linear matrix factorization with Gaussian processes. In
Proceedings of the International Conference on Machine Learning (ICML), pages 1–8. ACM Press.
[13] Lov ´asz, L. and Szegedy, B. (2006). Limits of dense graph sequences. Journal of Combinatorial Theory
Series B, 96, 933–957.
[14] Miller, K. T., Grifﬁths, T. L., and Jordan, M. I. (2009). Nonparametric latent feature models for link
prediction. Advances in Neural Information Processing Systems (NIPS), pages 1276–1284.
[15] Murray, I., Adams, R. P., and Mackay, D. J. C. (2010). Elliptical slice sampling. Journal of Machine
Learning Research (JMLR), 9, 541–548.
[16] Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31(3), 705–767. With discussions and a
rejoinder by the author.
[17] Palla, K., Knowles, D. A., and Ghahramani, Z. (2012). An Inﬁnite Latent Attribute Model for Network
Data. In Proceedings of the International Conference on Machine Learning (ICML).
[18] Qui ˜nonero Candela, J. and Rasmussen, C. E. (2005). A unifying view of sparse approximate gaussian
process regression. Journal of Machine Learning Research (JMLR), 6, 1939–1959.
[19] Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.
[20] Roy, D. M. and Teh, Y. W. (2009). The Mondrian process. In Advances in Neural Information Processing
Systems (NIPS).
[21] Salakhutdinov, R. (2008). Probabilistic Matrix Factorisation. In Advances in neural information process-
ing systems (NIPS).
[22] Silverman, B. W. (1985). Some aspects of the spline smoothing approach to non-parametric regression
curve ﬁtting. Journal of the Royal Statistical Society. Series B (Methodological), 47(1), 1–52.
[23] Smola, A. J. and Bartlett, P. (2001). Sparse greedy gaussian process regression. In Advances in Neural
Information Processing Systems (NIPS). MIT Press.
[24] Titsias, M. K. and Lawrence, N. D. (2008). Efﬁcient sampling for Gaussian process inference using
control variables. In Advances in Neural Information Processing Systems (NIPS), pages 1681–1688.
[25] Wahba, G., Lin, X., Gao, F., Xiang, D., Klein, R., and Klein, B. (1999). The bias-variance tradeoff and
the randomized gacv. In Advances in Neural Information Processing Systems (NIPS).
[26] Wang, Y. J. and Wong, G. Y. (1987). Stochastic Blockmodels for Directed Graphs. Journal of the
American Statistical Association, 82(397), 8–19.
[27] Xu, Z., Yan, F., and Qi, Y. (2012). Inﬁnite Tucker Decomposition: Nonparametric Bayesian Models for
Multiway Data Analysis. In Proceedings of the International Conference on Machine Learning (ICML).
[28] Yan, F., Xu, Z., and Qi, Y. (2011). Sparse matrix-variate Gaussian process blockmodels for network
modeling. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence (UAI).

9


Distributed Probabilistic Learning
for Camera Networks with Missing Data

Sejong Yoon
Department of Computer Science
Rutgers University
sjyoon@cs.rutgers.edu

Vladimir Pavlovic
Department of Computer Science
Rutgers University
vladimir@cs.rutgers.edu
Abstract

Probabilistic approaches to computer vision typically assume a centralized setting,
with the algorithm granted access to all observed data points. However, many
problems in wide-area surveillance can beneﬁt from distributed modeling, either
because of physical or computational constraints. Most distributed models to date
use algebraic approaches (such as distributed SVD) and as a result cannot explic-
itly deal with missing data. In this work we present an approach to estimation and
learning of generative probabilistic models in a distributed context where certain
sensor data can be missing. In particular, we show how traditional centralized
models, such as probabilistic PCA and missing-data PPCA, can be learned when
the data is distributed across a network of sensors. We demonstrate the utility
of this approach on the problem of distributed afﬁne structure from motion. Our
experiments suggest that the accuracy of the learned probabilistic structure and
motion models rivals that of traditional centralized factorization methods while
being able to handle challenging situations such as missing or noisy observations.

1

Introduction

Traditional computer vision algorithms, particularly those that exploit various probabilistic and
learning-based approaches, are often formulated in centralized settings. A scene or an object is
observed by a single camera with all acquired information centrally processed and stored in a single
knowledge base (e.g., a classiﬁcation model). Even if the problem setting relies on multiple cameras,
as may be the case in multi-view or structure from motion (SfM) tasks, all collected information is
still processed and organized in a centralized fashion. Increasingly modern computational settings
are becoming characterized by networks of peer-to-peer connected devices, with local data process-
ing abilities. Nevertheless, the overall goal of such distributed device (camera) networks may still
be to exchange information and form a consensus interpretation of the visual scene. For instance,
even if a camera observes a limited set of object views, one would like its local computational model
to reﬂect a general 3D appearance of the object visible by other cameras in the network.
A number of distributed algorithms have been proposed to address the problems such as calibration,
pose estimation, tracking, object and activity recognition in large camera networks [1–3]. In order
to deal with high dimensionality of vision problems, distributed latent space search such as decen-
tralized variants of PCA have been studied in [4, 5]. A more general framework using distributed
least squares [6] based on distributed averaging of sensor fusions [7] was introduced for PCA, tri-
angulation, pose estimation and SfM. Similar approaches have been extended to settings such as the
distributed object tracking and activity interpretation [8,9]. Even though the methods such as PCA or
Kalman ﬁltering have their well-known probabilistic counterparts, the aforementioned approaches
do not use probabilistic formulation when dealing with the distributed setting.
One critical challenge in distributed data analysis includes dealing with missing data. In camera
networks, different nodes will only have access to a partial set of data features because of varying
camera views or object movement. For instance, object points used for SfM may be visible only

1

in some cameras and only in particular object poses. As a consequence, different nodes will be
frequently exposed to missing data. However, most current distributed data analysis methods are
algebraic in nature and cannot seamlessly handle such missing data.
In this work we propose a distributed consensus learning approach for parametric probabilistic mod-
els with latent variables that can effectively deal with missing data. We assume that each node in
a network can observe only a fraction of the data (e.g., object views in camera networks). Further-
more, we assume that some of the data features may be missing across different nodes. The goal of
the network of sensors is to learn a single consensus probabilistic model (e.g., 3D object structure)
without ever resorting to a centralized data pooling and centralized computation. We will demon-
strate that this task can be accomplished in a principled manner by local probabilistic models and
in-network information sharing, implemented as recursive distributed probabilistic learning.
In particular, we focus on probabilistic PCA (PPCA) as a prototypical example and derive its dis-
tributed version, the D-PPCA. We then suggest how missing data can be handled in this setting
using a missing-data PPCA and apply this model to solve the distributed SfM task in a camera net-
work. Our model is inspired by the consensus-based distributed Expectation-Maximization (EM)
algorithm for Gaussian mixtures [10], which we extend to deal with generalized linear Gaussian
models [11]. Unlike other recently proposed decomposable Gaussian graphical models [4, 12], our
model does not depend on any speciﬁc type of graphs. Our network, of arbitrary topology, is as-
sumed to be static with a single connected component. These assumptions are reasonably applicable
to many real world camera network settings.
In Section 2, we ﬁrst explain the general distributed probabilistic model. Section 3 shows how D-
PPCA can be formulated as a special case of the probabilistic framework and propose the means for
handling missing data. We then explain how D-PPCA can be modiﬁed for the application in afﬁne
SfM. In Section 5, we report experimental results of our model using both synthetic and real data.
Finally, we discuss our approach including its limitations and possible solutions in Section 6.

2 Distributed Probabilistic Model

We start our discussion by ﬁrst considering a general parametric probabilistic model in a centralized
setting and then we show how to derive its distributed form.

2.1 Centralized Setting
Let X = {xn |xn ∈ RD } be a set of iid multivariate data points with the corresponding latent
variables Z = {zn |zn ∈ RM }, n = 1...N . Our model is a joint density deﬁned on (xn , zn ) with a
with p(X, Z|θ) = (cid:81)
global parameter θ
(xn , zn ) ∼ p(xn , zn |θ),
n p(xn , zn |θ), as depicted in Fig. 1a. In this general model, we can ﬁnd an
optimal global parameter ˆθ (in a MAP sense) by applying standard EM learning. The EM follows a
recursive two-step procedure: (a) E-step, where the posterior density p(zn |xn , θ) is estimated, and
(b) M-step: parametric optimization ˆθ = arg maxθ EZ|X [log p(X, Z|θ)]. It is important to point out
that each posterior density estimate at point n depends solely on the corresponding measurement xn
and does not depend on any other xk , k (cid:54)= n. This means that even if we partition independent mea-
surements into arbitrary subsets, posterior density estimation is accomplished locally, within each
subset. However, in the M-step all measurements X affect the choice of ˆθ because of the depen-
dence of each term in the completed log likelihood on the same ˆθ . This is a typical characteristic of
parametric models where the optimal parameters depend on summary data statistics.

2.2 Distributed Setting
Let G = (V , E ) be an undirected connected graph with vertices i, j ∈ V and edges eij = (i, j ) ∈ E
connecting the two vertices. Each i-th node is directly connected with 1-hop neighbors in Bi =
{j |eij ∈ E }. Suppose the set of data samples at i-th node is Xi = {xin |n = 1, ..., Ni}, where
xin ∈ RD is n-th measurement vector and Ni is the number of samples collected in i-th node.
Likewise, we deﬁne the latent variable set for node i as Zi = {zin |n = 1, ..., Ni }.

2

(a) Centralized

(b) Distributed

(c) Augmented

Figure 1: Centralized, distributed and augmented models for probabilistic PCA.

s.t.

As observed previously, each posterior estimation is decentralized. Learning the model parameter
would be decentralized if each node had its own independent parameter θi . Still, the centralized
model can be equivalently deﬁned using the set of local parameters, with an additional constraint on
their consensus, θ1 = θ2 = · · · = θ|V | . This is illustrated in Fig. 1b where the local node models
are constrained using ties deﬁned on the underlying graph. The simple consensus tying can be more
conveniently deﬁned using a set of auxiliary variables ρij , one for each edge eij (Fig. 1c). This now
leads to the ﬁnal distributed consensus learning formulation, similar to [10]:
− log p(X|θ , G)
θi = ρij , ρij = θj , i ∈ V , j ∈ Bi
ˆθ = arg min
{θi :i∈V }
where we marginalized on X. This is a constrained optimization task that can be solved in a prin-
cipal manner using the Alternating Direction Method of Multipliers (ADMM) [13–15]. ADMM
(cid:110)
(cid:111)
(cid:88)
(cid:88)
iteratively, in a block-coordinate fashion, solves maxλ minθ L(·) on the augmented Lagrangian
L(θ , ρ, λ) = − log p(X|θ1 , θ2 , ..., θ|V | , G) +
ij2 (ρij − θj )
ij1 (θi − ρij ) + λT
λT
(cid:88)
(cid:88)
(cid:8)||θi − ρij ||2 + ||ρij − θj ||2(cid:9)
j∈Bi
i∈V
η
(2)
+
2
i∈V
j∈Bi
where λij 1 , λij 2 , i, j ∈ V are the Lagrange multipliers, η is some positive scalar parameter and
|| · || is induced norm. The last term (modulated by η ) is not strictly necessary for consensus but
introduces additional regularization. Further discussions on this term and the parameter can be found
in [15] and [16]. The auxiliary ρij play a critical decoupling role and separate estimation of local
θi during block-coordinate ascent/descent. This classic (ﬁrst introduced in 1970s) meta decompose
algorithm can be used to devise a distributed counterpart for any centralized problem that attempts
to maximize a global log likehood function over a connected network.

(1)

3 Distributed Probabilistic PCA (D-PPCA)

We now apply the general distributed probabilistic learning explained above to the speciﬁc case of
distributed PPCA. Traditional centralized formulation of probabilistic PCA (PPCA) [17] assumes
that latent variable zin ∼ N (zin |0, I), with a generative relation
xin = Wizin + µi + i ,
where i ∼ N (i |0, a−1
i I) and ai is the noise precision. Inference then yields
p(zin |xin ) = N (zin |L−1
i (xin − µi ), a−1
i L−1
i WT
(4)
),
i
i Wi + a−1
where Li = WT
i I. We can ﬁnd optimal parameters Wi , µi , ai by ﬁnding the maximum
likelihood estimates of the marginal data likelihood or by applying the EM algorithm on expected
complete data log likelihood with respect to the posterior density p(Zi |Xi ).

(3)

3.1 Distributed Formulation

The distributed algorithm developed in Section 2 can be directly applied to this PPCA model. The
basic idea is to assign each subset of samples as evidence for the local generative models with

3

parameters Wi , µi , a−1
. The inference is accomplished locally in each node. The local parameter
i
estimates are then computed using the consensus updates that combine local summary data statistics
with the information about the model conveyed through neighboring network nodes. Below, we
outline speciﬁc details of this approach.
Let Θi = {Wi , µi , ai } be the set of parameters for each node i. The global constrained consensus
optimization now becomes

s.t.

(5)

log p(xin |Wi , µi , a−1
i

i ∈ V , j ∈ Bi ,
Wi = ρij , ρij = Wj ,
i ∈ V , j ∈ Bi ,
µi = φij , φij = µj ,
i ∈ V , j ∈ Bi
ai = ψij , ψij = aj ,

min{Wi ,µi ,ai :i∈V } −F (Θi )
Ni(cid:80)
). The augmented Lagrangian is
where F (Θi ) =
n=1
(cid:16)
(cid:16)
(cid:17)
(cid:88)
(cid:88)
L(Φi ) = −F (Θi )+(cid:88)
(cid:88)
ij1 (µi − φij ) + γ T
ij2 (φij − µj )
ij2 (ρij − Wj )
ij1 (Wi − ρij ) + λT
γ T
λT
(cid:88)
(cid:88)
(cid:88)
(cid:88)
+
i∈V
j∈Bi
i∈V
j∈Bi
(βij1 (ai − ψij ) + βij2 (ψij − aj )) +
η
(cid:88)
(cid:88)
(cid:88)
(cid:88)
2
j∈Bi
i∈V
j∈Bi
i∈V
((ai − ψij )2 + (ψij − aj )2 )
η
η
2
2
i∈V
j∈Bi
i∈V
j∈Bi
where Φi = {Wi , µi , ai , ρij , φij , ψij ; i ∈ V , j ∈ Bi } and {λijk }, {γijk }, {βijk } with k = 1, 2
are the Lagrange multipliers. The scalar value η gives us control over the convergence speed of the
algorithm. With reasonably large positive η , the overall optimization converges fairly quickly [10].
We will explore the converging behaviour with respect to various η in synthetic data experiments.
Just like in a standard EM approach, we minimize the upper bound of L(Φi ). Exploiting the pos-
terior density in (4), we compute the expected mean and variance of latent variables in each node
as

(||µi − φij ||2 + ||φij − µj ||2 ) +

(||Wi − ρij ||2 + ||ρij − Wj ||2 )

(cid:17)

(6)

+

+

=

ai

(7)
(cid:33)−1

,

λ(t+1)
i

(8)

(9)

µ(t+1)
i

=

W(t+1)
i

W(t+1)
i

= λ(t)
i +

in ] + 2η |Bi |I
E[zin zT

· (Ni ai + 2η |Bi |)
−1 ,

i (xin − µi ),
E[zin ] = L−1
in ] = a−1
i L−1
E[zinzT
i + E[zin ]E[zin ]T .
i WT
(cid:32)
(cid:19)(cid:27)
(cid:18)
(cid:26)
Maximization of the completed likelihood Lagrangian derived from (6) yields
Ni(cid:88)
Ni(cid:88)
(cid:88)
·
(xin − µi )E[zin ]T − 2λ(t)
i + W(t)
W(t)
i + η
ai
j
j∈Bi
(cid:19)(cid:27)
(cid:18)
(cid:19)
(cid:18)
(cid:26)
n=1
n=1
(cid:88)
Ni(cid:88)
− 2γ (t)
xin − WiE[zin ]
(cid:111)
(cid:110)
(cid:88)
i + µ(t)
µ(t)
j )
i + η
ai
j∈Bi
n=1
− W(t+1)
η
(cid:111)
(cid:110)
(cid:88)
j
2
j∈Bi
− µ(t+1)
(cid:111)
(cid:110)
(cid:88)
µ(t+1)
j
i
j∈Bi
a(t+1)
.
i
j∈Bi
(cid:40)
For ai , we solve the quadratic equation
0 = − NiD
·
+ 2η |Bi |a(t+1)
i − η
2
2β (t)
+ a(t+1)
(cid:104)E[zin zT
(cid:110)||xin − µi ||2 + tr
Ni(cid:88)
i
i
2
in ]WT
i Wi
n=1
The overall distributed EM algorithm for D-PPCA is summarized in Algorithm 1. Detailed deriva-
tion can be found in the supplementary material.

(cid:16)
(cid:88)
(cid:105)(cid:111) (cid:41)
j∈Bi

(cid:17) − Ni(cid:88)
n=1

i (xin − µi )
E[zin ]TWT

− a(t+1)
j

i + a(t)
a(t)
j

= γ (t)
i +

β (t+1)
i

= β (t)
i +

+

1
2

(10)

(11)

(12)

,

,

γ (t+1)
i

.

(13)

η
2

η
2

4

Algorithm 1 Distributed Probabilistic PCA (D-PPCA)
Require: For every node i initialize W(0)
, µ(0)
, a(0)
randomly and set λ(0)
i = 0, γ (0)
i = 0, β (0)
i = 0.
i
i
i
for t = 0, 1, 2, ... until convergence do
for all i ∈ V do
[E-step] Compute E[zin ] and E[zin zT
in ] via (7).
[M-step] Compute W(t+1)
, µ(t+1)
, a(t+1)
via (8,9,13).
i
i
i
end for
for all i ∈ V do
Broadcast W(t+1)
i
end for
for all i ∈ V do
Compute λ(t+1)
i
end for
end for

to all neighbors of i ∈ Bi .

, and a(t+1)
i

, µ(t+1)
i

, γ (t+1)
i

, and β (t+1)
i

via (10-12).

3.2 Missing Data D-PPCA

Traditional PPCA is an effective tool for dealing with data missing-at-random (MAR) in traditional
PCA [18]. While more sophisticated methods including variational approximations, c.f., [18] are
possible direct use of PPCA is often sufﬁcient in practice. Hence, we adopt D-PPCA as a method to
deal with missing data in a distributed consensus setting.
terms for local data summaries. For instance, in (9) the data summary term (cid:80)Ni
Generalization to missing data D-PPCA from D-PPCA is straightforward and follows [18]. From
the perspective of ADMM-based learning the only modiﬁcations comes in the form of adjusted
n=1 (xin −WiE[zin ])
(cid:88)
becomes
xi,n,f − wT
E[zin ],
i,f
n∈Oi,f
where f = 1, . . . , D is the index of feature, Oi,f is the set of samples in node i that have the feature
f present, xi,n,f is the value of the present feature, and wT
i,f is the f -th row of matrix Wi . Similar
expressions can be derived for other local parameters. Note that (10-12) incur no changes.

(14)

4 D-PPCA for Structure from Motion (SfM)

In this section, we consider a speciﬁc formulation of the modiﬁed distributed probabilistic PCA
for application in afﬁne SfM. In SfM, our goal is to estimate the 3D location of N points on a
rigid object based on corresponding 2-D points observed from multiple cameras (or views). The
dimension D of our measurement matrix is thus twice the number of frames each camera observed.
A simple and effective way to solve this problem is the factorization method [19]. Given a 2D (image
coordinate) measurement matrix X, of size 2 · #f rames × #points, the matrix is factorized into a
2 · #f rames × 3 motion matrix M and the 3 × #points 3D structure matrix S. In the centralized
setting this can be easily computed using SVD on X. Equivalently, the estimates of M and S can
be found using inference and learning in a centralized PPCA, where M is treated as the PPCA
parameter and S is the latent structure. There we obtain additional estimates of the variance of
structure S, which are not immediately available from the factorization approach (although, they
can be found).
However, the above deﬁned (2 · #f rames × #points) data structure of X is not amenable to
distribution of different views (cameras, nodes), as considered in Section 3 of D-PPCA. Namely,
D-PPCA assumes that the distribution is accomplished by splitting the data matrix X into sets of
non-overlapping columns, one for each node. Here, however, we seek to distribute the rows of
matrix X, i.e., a set of (subsequent) frames is to be assigned to each node/camera.
Hence, to apply the D-PPCA framework to SfM we need to swap the role of rows and columns,
i.e., consider modeling of XT . This, subsequently, means that the 3D scene structure (which is to
be shared across all nodes in the network) will be treated as the D-PPCA parameter. The latent
D-PPCA variables will model the unknown and uncertain motion of each camera (and/or object in
its view).

5

Speciﬁcally, we will consider the model
i = W · Zi + Ei
XT
(15)
i is the matrix of image coordinates of all points in node (camera) i of size #points × 2 ·
where XT
#f rames in node i, W is the #points × 3 3D structure (D-PPCA parameter) matrix and Zi is the
3 × 2 · #f rames motion matrix of node i.
One should note that we have implicitly assumed, in a standard D-PPCA manner, that each column
of Zi is iid and distributed as N (0, I). However, each pair of subsequent Zi columns represents
one 3 × 2 afﬁne motion matrix. While those columns are not truly independent our experiments (as
demonstrated in Section 5) show that this assumption is not detrimental in practice. Remaining task
is simply following the same process we did to derive D-PPCA.
Missing data in SfM will be handled using the formalism presented in Sec. 3.2. Strictly speaking,
the model of data missing-at-random is not always applicable to SfM. The reason is that occlusions,
the main source of missing data, cannot be treated as a random process. Instead, this setting corre-
sponds to data missing-not-at-random [18] (MNAR). If treated blindly, this may introduce bias in the
estimated models. However, as we demonstrate in experiments this assumption does not adversely
affect SfM when the number of missing points is within a reasonable range.

5 Experiments

In our experiments we ﬁrst study the general convergence properties of the D-PPCA algorithm in a
controlled synthetic setting. We then apply the D-PPCA to a set of SfM problems, both on synthetic
and on real data.

5.1 Empirical Convergence Analysis

Using synthetic data generated from Gaussian distribution, we observed that D-PPCA works well
regardless of the number of network nodes, topology, choice of the parameter η or even with missing
values in both MAR and MNAR cases. Detailed results for the syntehtic data is provided in the
supplementary materials.

5.2 Afﬁne Structure from Motion

We now show that the modiﬁed D-PPCA can be used as an effective framework for distributed afﬁne
SfM. We ﬁrst show results in a controlled environment with synthetic data and then report results
on data from real video sequences. We assume that correspondences across frames and cameras
are known. For the missing values of MNAR case, we either used the actual occlusions to induce
missing points or simulated consistently missing points over several frames.

5.2.1 Synthetic Data (Cube)

We ﬁrst generated synthetic data with a rotating unit cube and 5 cameras facing the cube in a 3D
space, similar to synthetic experiments in [6]. The cube is centered at the origin of the space and
rotates 30◦ counterclockwise. We extracted 8 cube points projected on each camera view every 6◦ ,
i.e. each camera observed 5 frames. Cameras are placed on a skewed plane, making elevation along
z -axis as shown in Fig. 2a. For all synthetic and real SfM experiments, we picked η = 10 and
initialized Wi matrix with feature point coordinates of the ﬁrst frame visible in the i-th camera with
some small noise. The convergence criterion for D-PPCA for SfM was set as 10−3 relative error.
To measure the performance, we computed maximum subspace angle between the ground truth
3D coordinates and our estimated 3D structure matrix. For comparison, we conducted traditional
SVD-based SfM on the same data. In noise free case, D-PPCA for SfM always yielded the same
performance as SVD-based SfM with near 0◦ .
We also tested D-PPCA for SfM with noisy and missing-value cases. First, we generated 20 inde-
pendent samples of all 25 frames with 10 different noise levels. Then we ran D-PPCA 20 times on
each of the independent sample and averaged the ﬁnal structure estimates. As Fig. 2b shows, we
found that D-PPCA for SfM is fairly robust to noise and tends to stabilize even as the noise level

6

(a) Camera Setting

(b) Subspace Angle vs. Ground Truth

Figure 2: Rotating unit cube with multiple cameras. Red circles are camera locations and blue
arrows indicate each camera’s facing direction. Green and red crosses in the right plot are outliers
for centralized SVD-based SfM and D-PPCA for SfM, respectively.

increases. The mean subspace angle tends to be slightly larger than that estimated by the central-
ized SVD SfM, however both reside within the overlapping conﬁdence intervals. Considering MAR
missing values, we obtained 1.66◦ for 20% missing points averaged over 10 different missing point
samples. In MNAR case with actual occlusions considered, D-PPCA yielded, relatively larger, 20◦
error. Intuitively, this is because the missing points in the scene are naturally not random. However,
we argue that D-PPCA can still handle missing points given the evidence below.

5.2.2 Real Data

For real data experiement, we ﬁrst applied D-PPCA for SfM on the Caltech 3D Objects on Turntable
dataset [20]. The dataset provides various objects rotating on a turntable under different lighting
conditions. The views of most objects were taken every 5◦ which make it challenging to extract
feature points with correspondence across frames. Instead, we used a subset of the dataset which
provides views taken every degree. This subset contains images of 5 objects. To simulate multiple
cameras, we adopted a setting similar to that of [6]. We ﬁrst extracted ﬁrst 30◦ images of each
object. We then used KLT [21] implementation in Voodoo Camera Tracker1 to extract feature points
with correspondence. Lastly, we sequentially and equally partitioned the 30 images into 5 nodes to
simulate 5 cameras. Thus, each camera observes 6 frames. Table 1 shows the 5 objects and statistics
of feature points we extracted from the objects. We used η = 10 and convergence criterion 10−3 .
Due to the lack of the ground truth 3D coordinates, we compared the subspace angles between the
structure inferred using the traditional centralized SVD-based SfM and the D-PPCA-based SfM.
Results are shown in Table 1 as the mean and variance of 20 independent runs. 10% MAR and
MNAR results are also provided in the table.
Experimenal results indicate existance of differences between the reconstructions obtained by cen-
tralized factorization approach and that of D-PPCA. However, the differences are small, depend on
the object in question, and almost always include, within their conﬁdence, the factorization result.
Qualitative examination reveals no noticable differences. Moreover, re-projecting back to the cam-
era coordinate space resulted in close matching with the tracked feature points, as shown in videos
provided in supplementary materials.
We also tested the utility of D-PPCA for SfM on the Hopkins155 dataset [22]. We adopted virtually
identical experimental setting as in [6]. We collected 135 single-object sequences containing image
coordinates of points and we simulated multi-camera setting by partitioning the frames sequentially
and almost equally for 5 nodes and the network was connected using ring topology. Again, we
computed maximum subspace angle between centralized SVD-based SfM and distributed D-PPCA
for SfM. We chose the convergence criterion as 10−3 . Average maximum subspace angle between
1http://www.digilab.uni-hannover.de/docs/manual.html

7

01234501234−0.500.511.5200.20.40.60.811.21.41.6Noise level (%)Subspace angle (degree)012345678910D−PPCACentralizedSVD−basedSfMTable 1: Caltech 3D Objects on Turntable dataset statistics and quantitative results. Green dots indi-
cate feature points tracked with correspondance across all 30 frames. All results ran 20 independent
initializations. MAR results provide variances over both various initializations and missing value
settings.
Object

StorageBin

Standing

BallSander

BoxStuff

Rooster

102
30

# Points
# Frames

310
189
67
62
30
30
30
30
Subspace angle b/w centralized SVD SfM and D-PPCA (degree)
0.4463
2.6221
1.4767
1.4397
1.4848
Mean
Variance
1.2002
0.4159
0.4567
0.9448
1.6924
Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MAR (degree)
Mean
6.2991
2.1556
5.2506
7.6492
2.8358
1.3591
6.6424
3.8810
0.1351
4.3562
Var.(init)
Var.(miss)
0.5729
0.0161
0.1755
0.7603
0.0444
Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MNAR (degree)
Mean
3.1405
6.4664
5.8027
9.2661
3.7965
0.0089
2.9720
2.4333
3.1955
0.0124
Variance

D-PPCA for SfM and SVD-based SfM for all objects was 3.97◦ with variance 7.06. However,
looking into the result more carefully, we found that even with substantially larger subspace angle,
3D structure estimates were similar to that of SVD-based SfM only with orthogonal ambiguity issue.
Moreover, more than 53% of all objects yielded the subspace angle below 1◦ , 77% of them below
5◦ and more than 94% were less than 15◦ . With 10% MAR, we obtained the mean 20.07◦ with
variance 27.94◦ with about 18% of them below 1◦ , 56% of them below 5◦ and more than 70% of
them less than the mean. We could not perform MNAR experiments on Hopkins as the ground truth
occlusion information is not provided with the dataset.

6 Discussion and Future Work

In this work we introduced a general approach for learning parameters of traditional centralized
probabilistic models, such as PPCA, in a distributed setting. Our synthetic data experiments showed
that the proposed algorithm is robust to choices of initial parameters and, more importantly, is not
adversely affected by variations in network size, topology or missing values. In the SfM problems,
the algorithm can be effectively used to distribute computation of 3D structure and motion in camera
networks, while retaining the probabilistic nature of the original model.
Despite its promising performance D-PPCA for SfM exhibits some limitations. In particular, we
assume the independence of the afﬁne motion matrix parameters in (15). The assumption is clearly
inconsistent with the modeling of motion on the SE(3) manifold. However, our experiments demon-
strate that, in practice, this violation is not crucial. This shortcoming can be amended in one of sev-
eral possible ways. One can reduce the iid assumption of individual samples to that of subsequent
columns (i.e., full 3x2 motion matrices). Our additional experiments, not reported here, indicate
no discernable utility of this approach. A more principled approach would be to deﬁne priors for
motion matrices compatible with SE(3), using e.g., [23]. While appealing, the priors would render
the overall model non-linear and would require additional algorithmic considerations, perhaps in the
spirit of [1].

Acknowledgments

This work was supported in part by the National Science Foundation under Grant No. IIS 0916812.

8

References
[1] Roberto Tron and Rene Vidal. Distributed Computer Vision Algorithms. IEEE Signal Processing Maga-
zine, 28:32–45, 2011.
[2] A.Y. Yang, S. Maji, C.M. Christoudias, T. Darrell, J. Malik, and S.S. Sastry. Multiple-view Object Recog-
nition in Band-limited Distributed Camera Networks. In Distributed Smart Cameras, 2009. ICDSC 2009.
Third ACM/IEEE International Conference on, 30 2009-sept. 2 2009.
[3] Richard J. Radke. A Survey of Distributed Computer Vision Algorithms. In Hideyuki Nakashima, Hamid
Aghajan, and Juan Carlos Augusto, editors, Handbook of Ambient Intelligence and Smart Environments.
Springer Science+Business Media, LLC, 2010.
[4] A. Wiesel and A.O. Hero. Decomposable Principal Component Analysis. Signal Processing, IEEE
Transactions on, 57(11):4369–4377, 2009.
[5] Sergio V. Macua, Pavle Belanovic, and Santiago Zazo. Consensus-based Distributed Principal Component
In Signal Processing Advances in Wireless Communications
Analysis in Wireless Sensor Networks.
(SPAWC), 2010 IEEE Eleventh International Workshop on, pages 1–5, June 2010.
[6] Roberto Tron and Rene Vidal. Distributed Computer Vision Algorithms Through Distributed Averaging.
In IEEE Conference on Computer Vision and Pattern Recognition, pages 57–63, 2011.
[7] Lin Xiao, Stephen Boyd, and Sanjay Lall. A Scheme for Robust Distributed Sensor Fusion Based on
Average Consensus. In International Conference on Information Processing in Sensor Networks, pages
63–70, April 2005.
[8] R. Olfati-Saber. Distributed Kalman Filtering for Sensor Networks. In Decision and Control, 2007 46th
IEEE Conference on, pages 5492 –5498, dec. 2007.
[9] Bi Song, A.T. Kamal, C. Soto, Chong Ding, J.A. Farrell, and A.K. Roy-Chowdhury. Tracking and Activity
Recognition Through Consensus in Distributed Camera Networks. Image Processing, IEEE Transactions
on, 19(10):2564 –2579, oct. 2010.
[10] P.A. Forero, A. Cano, and G.B. Giannakis. Distributed Clustering Using Wireless Sensor Networks.
Selected Topics in Signal Processing, IEEE Journal of, 5(4):707 –724, aug. 2011.
[11] Sam Roweis and Zoubin Ghahramani. A Unifying Review of Linear Gaussian Models. Neural Compu-
tation, 11:305–345, 1999.
[12] Ami Wiesel, Yonina C. Eldar, and Alfred O. Hero. Covariance Estimation in Decomposable Gaussian
Graphical Models. IEEE Transactions on Signal Processing, 58(3):1482–1492, 2010.
[13] Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint. A globally convergent augmented La-
grangian algorithm for optimization with general constraints and simple bounds. SIAM J. Numer. Anal.,
28:545–572, February 1991.
[14] Robert Michael Lewis and Virginia Torczon. A Globally Convergent Augmented Lagrangian Pattern
Search Algorithm for Optimization with General Constraints and Simple Bounds. SIAM J. on Optimiza-
tion, 12:1075–1089, April 2002.
[15] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed Optimization
and Statistical Learning via the Alternating Direction Method of Multipliers. In Michael Jordan, editor,
Foundations and Trends in Machine Learning, volume 3, pages 1–122. Now Publishers, 2011.
[16] Pedro A. Forero, Alfonso Cano, and Geogios B. Giannakis. Consensus-Based Distributed Support Vector
Machines. Journal of Machine Learning Research, 11:1663–1707, 2010.
[17] Michael E. Tipping and Chris M. Bishop. Probabilistic Principal Component Analysis. Journal of the
Royal Statistical Society, Series B, 61:611–622, 1999.
[18] Alexander Ilin and Tapani Raiko. Practical Approaches to Principal Component Analysis in the Presence
of Missing Values. Journal of Machine Learning Research, 11:1957–2000, 2010.
[19] Carlo Tomasi and Takeo Kanade. Shape and motion from image streams under orthography: a factoriza-
tion method. International Journal of Computer Vision, 9:137–154, 1992. 10.1007/BF00129684.
[20] Pierre Moreels and Pietro Perona. Evaluation of Features Detectors and Descriptors based on 3D Objects.
International Journal of Computer Vision, 73(3):263–284, July 2007.
[21] Carlo Tomasi and Takeo Kanade. Detection and Tracking of Point Features. Technical Report CMU-CS-
91-132, Carnegie Mellon University, April 1991.
[22] Roberto Tron and Rene Vidal. A Benchmark for the Comparison of 3-D Motion Segmentation Algo-
rithms. In IEEE International Conference on Computer Vision and Pattern Recognition, 2007.
[23] Yasuko Chikuse. Statistics on Special Manifolds, volume 174 of Lecture Notes in Statistics. Springer, 1
edition, February 2003.

9


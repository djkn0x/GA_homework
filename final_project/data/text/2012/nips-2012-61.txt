A Conditional Multinomial Mixture Model for
Superset Label Learning

Li-Ping Liu
EECS, Oregon State University
Corvallis, OR 97331
liuli@eecs.oregonstate.edu

Thomas G. Dietterich
EECS, Oregon State University
Corvallis, OR 97331
tgd@cs.orst.edu

Abstract

In the superset label learning problem (SLL), each training instance provides a set
of candidate labels of which one is the true label of the instance. As in ordinary
regression, the candidate label set is a noisy version of the true label.
In this
work, we solve the problem by maximizing the likelihood of the candidate label
sets of training instances. We propose a probabilistic model, the Logistic Stick-
Breaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSB-
CMM is derived from the logistic stick-breaking process. It ﬁrst maps data points
to mixture components and then assigns to each mixture component a label drawn
from a component-speciﬁc multinomial distribution. The mixture components can
capture underlying structure in the data, which is very useful when the model is
weakly supervised. This advantage comes at little cost, since the model introduces
few additional parameters. Experimental tests on several real-world problems with
superset labels show results that are competitive or superior to the state of the art.
The discovered underlying structures also provide improved explanations of the
classiﬁcation predictions.

1

Introduction

In supervised classiﬁcation, the goal is to learn a classiﬁer from a collection of training instances,
where each instance has a unique class label. However, in many settings, it is difﬁcult to obtain such
precisely-labeled data. Fortunately, it is often possible to obtain a set of labels for each instance,
where the correct label is one of the elements of the set.
For example, captions on pictures (in newspapers, facebook, etc.) typically identify all of the people
the picture but do not necessarily indicate which face belongs to each person. Imprecisely-labeled
training examples can be created by detecting each face in the image and deﬁning a label set contain-
ing all of the names mentioned in the caption. A similar case arises in bird song classiﬁcation [2]. In
this task, a ﬁeld recording of multiple birds singing is divided into 10-second segments, and experts
identify the species of all of the birds singing in each segment without localizing each species to a
speciﬁc part of the spectrogram. These examples show that superset-labeled data are typically much
cheaper to acquire than standard single-labeled data. If effective learning algorithms can be devised
for superset-labeled data, then they would have wide application.
The superset label learning problem has been studied under two main formulations. In the multi-
instance multi-label (MIML) formulation [15], the training data consist of pairs (Bi , Yi ), where
Bi = {xi,1 , . . . , xi,ni } is a set of instances and Yi is a set of labels. The assumption is that for every
instance xi,j ∈ Bi , its true label yi,j ∈ Yi . The work of Jie et al. [9] and Briggs et al. [2] learn
classiﬁers from such set-labeled bags.
In the superset label formulation (which has sometimes been confusingly called the “partial label”
problem) [7, 10, 8, 12, 4, 5], each instance xn has a candidate label set Yn that contains the unknown

1

true label yn . This formulation ignores any bag structure and views each instance independently. It
is more general than the MIML formulation, since any MIML problem can be converted to a super-
set label problem (with loss of the bag information). Furthermore, the superset label formulation is
natural in many applications that do not involve bags of instances. For example, in some applica-
tions, annotators may be unsure of the correct label, so permitting them to provide a superset of the
correct label avoids the risk of mislabeling. In this paper, we employ the superset label formulation.
Other relevant work includes Nguyen et al. [12] and Cour et al. [5] who extend SVMs to handle
superset labeled data.
In the superset label problem, the label set Yn can be viewed as a corruption of the true label.
The standard approach to learning with corrupted labels is to assume a generic noise process and
incorporate it into the likelihood function. In standard supervised learning, it is common to assume
that the observed label is sampled from a Bernoulli random variable whose most likely outcome is
equal to the true label. In ordinary least-squares regression, the assumption is that the observed value
is drawn from a Gaussian distribution whose mean is equal to the true value and whose variance is
a constant σ2 . In the superset label problem, we will assume that the observed label set Yn is drawn
from a set-valued distribution p(Yn |yn ) that depends only on the true label. When computing the
likelihood, this will allow us to treat the true label as a latent variable that can be marginalized away.
When the label information is imprecise, the learning algorithm has to depend more on underlying
structure in the data. Indeed, many semi-supervised learning methods [16] model cluster structure
of the training data explicitly or implicitly. This suggests that the underlying structure of the data
should also play important role in the superset label problem.
In this paper, we propose the Logistic Stick-Breaking Conditional Multinomial Model (LSB-CMM)
for the superset label learning problem. The model has two components: the mapping component
and the coding component. Given an input xn , the mapping component maps xn to a region k . Then
the coding component generates the label according to a multinomial distribution associated with k .
The mapping component is implemented by the Logistic Stick Breaking Process(LSBP) [13] whose
Bernoulli probabilities are from discriminative functions. The mapping and coding components are
optimized simultaneously with the variational EM algorithm.
LSB-CMM addresses the superset label problem in several aspects. First, the mapping component
models the cluster structure with a set of regions. The fact that instances in the same region often
have the same label is important for inferring the true label from noisy candidate label sets. Second,
the regions do not directly correspond to classes. Instead, the number of regions is automatically
determined by data, and it can be much larger than the number of classes. Third, the results of the
LSB-CMM model can be more easily interpreted than the approaches based on SVMs [5, 2]. The
regions provide information about how data are organized in the classiﬁcation problem.

2 The Logistic Stick Breaking Conditional Multinomial Model
The superset label learning problem seeks to train a classiﬁer f : Rd (cid:55)→ {1, · · · , L} on a given
n=1 , where each instance xn ∈ Rd has a candidate label set Yn ⊂
dataset (x, Y ) = {(xn , Yn )}N
{1, · · · , L}. The true labels y = {yn}N
n=1 are not directly observed. The only information is that
the true label yn of instance xn is in the candidate set Yn . The extra labels {l|l (cid:54)= yn , l ∈ Yn}
causing ambiguity will be called the distractor labels. For any test instance (xt , yt ) drawn from the
same distribution as {(xn , yn )}N
n=1 , the trained classiﬁer f should be able to map xt to yt with high
probability. When |Yn | = 1 for all n, the problem is a supervised classiﬁcation problem. We require
|Yn | < L for all n; that is, every candidate label set must provide at least some information about
the true label of the instance.

2.1 The Model
as p(Yn |xn ) = (cid:80)L
As stated in the introduction, the candidate label set is a noisy version of the true label. To train
a classiﬁer, we ﬁrst need a likelihood function p(Yn |xn ). The key to our approach is to write this
yn=1 p(Yn |yn )p(yn |xn ), where each term is the product of the underlying true
classiﬁer, p(yn |xn ), and the noise model p(Yn |yn ). We then make the following assumption about
the noise distribution:

2

Figure 1: The LSB-CMM. Square nodes are discrete, circle nodes are continuous, and double-circle
nodes are deterministic.

Assumption: All labels in the candidate label set Yn have the same probability of generating Yn ,
(cid:26) λ(Yn )
but no label outside of Yn can generate Yn
p(Yn |yn = l) =
0

if l ∈ Yn
if l /∈ Yn

(1)

.

= arg max
f

f ∗ = arg max
f

p(yn |xn ; f )p(Yn |yn )
N(cid:88)
n=1

This assumption enforces three constraints. First, the set of labels Yn is conditionally independent
of the input xn given yn . Second, labels that do not appear in Yn have probability 0 of generating
Yn . Third, all of the labels in Yn have equal probability of generating Yn (symmetry). Note that
these constraints do not imply that the training data are correctly labeled. That is, suppose that the
most likely label for a particular input xn is yn = l. Because p(yn |xn ) is a multinomial distribution,
a different label yn = l(cid:48) might be assigned to xn by the labeling process. Then this label is further
corrupted by adding distractor labels to produce Yn . Hence, it could be that l (cid:54)∈ Yn . In short, in
this model, we have the usual “multinomial noise” in the labels which is then further compounded
by “superset noise”. The third constraint can be criticized for being simplistic; we believe it can be
replaced with a learned noise model in future work.
Given (1), we can marginalize away yn in the following optimization problem maximizing the like-
N(cid:88)
L(cid:88)
lihood of observed candidate labels.
log
N(cid:88)
log (cid:88)
yn=1
n=1
yn∈Yn
n=1
Under the conditional independence and symmetry assumptions, the last term does not depend on f
and so can be ignored in the optimization. This result is consistent with the formulation in [10].
We propose the Logistic Stick-Breaking Conditional Multinomial Model to instantiate f (see
In LSB-CMM, we introduce a set of K regions (mixture components) {1, . . . , K }.
Figure 1).
LSB-CMM has two components. The mapping component maps each instance xn to a region
zn , zn ∈ {1, . . . , K }. Then the coding component draws a label yn from the multinomial distri-
bution indexed by zn with parameter θzn . We denote the region indexes of the training instances by
z = (zn )N
n=1 .
In the mapping component, we employ the Logistic Stick Breaking Process(LSBP) [13] to model the
instance-region relationship. LSBP is a modiﬁcation of the Dirichlet Process (DP) [14]. In LSBP,
the sequence of Bernoulli probabilities are the outputs of a sequence of logistic functions instead of
being random draws from a Beta distribution as in the Dirichlet process. The input to the k-th logistic
function is the dot product of xn and a learned weight vector wk ∈ Rd+1 . (The added dimension
corresponds to a zeroth feature ﬁxed to be 1 to provide an intercept term.) To regularize these
logistic functions, we posit that each wk is drawn from a Gaussian distribution Normal(0, Σ), where
Σ = diag(∞, σ2 , · · · , σ2 ). This regularizes all terms in wk except the intercept. For each xn , a
sequence of probabilities {vnk }K
k=1 is generated from logistic functions, where vnk = expit(wT
k xn )
and expit(u) = 1/(1 + exp(−u)) is the logistic function. We truncate k at K by setting wK =
(+∞, 0, · · · , 0) and thus vnK = 1. Let w denote the collection of all K wk . Given the probabilities

p(yn |xn ; f ) +

log(λ(Yn )).

(2)

3

(3)

p(zn = k) = φnk = vnk

k−1(cid:89)
vn1 , . . . , vnK computed from xn , we choose the region zn according to a stick-breaking procedure:
(1 − vni ).
i=1
Here we stipulate that the product is 1 when k = 1. Let φn = (φn1 , · · · , φnK ) constitute the
parameter of a multinomial distribution. Then zn is drawn from this distribution.
In the coding component of LSB-CMM, we ﬁrst draw K L-dimensional multinomial probabilities
θ = {θk }K
k=1 from the prior Dirichlet distribution with parameter α. Then, for each instance xn
with mixture zn , its label yn is drawn from the multinomial distribution with θzn . In the traditional
multi-class problem, yn is observed. However, in the SLL problem yn is not observed and Yn is
generated from yn .
The generative process of the whole model is summarized below:
k−1(cid:89)
wk ∼ Normal(0, Σ), 1 ≤ k ≤ K − 1, wK = (+∞, 0, · · · , 0)
(1 − expit(wT
zn ∼ Mult(φn ), φnk = expit(wT
k xn )
i xn ))
i=1
θk ∼ Dirichlet(α)
yn ∼ Mult(θzn )
Yn ∼ Dist1(yn )
(Dist1 is some distribution satisfying (1))
 + log(p(w|0, Σ)).
 (cid:88)
As shown in (2), the model needs to maximize the likelihood that each yn is in Yn . After incorpo-
N(cid:88)
rating the priors, we can write the penalized maximum likelihood objective as
yn∈Yn
n=1
This cannot be solved directly, so we apply variational EM [1].

p(yn |xn , w, α)

max LL =

(6)
(7)
(8)

log

(5)

(4)

(9)

2.2 Variational EM

q(θk | ˆαk ),

q(zn , yn | ˆφn )

q(z , y , θ | ˆφ, ˆα) =

The hidden variables in the model are y , z , and θ . For these hidden variables, we introduce the
variational distribution q(y , z , θ | ˆφ, ˆα), where ˆφ = { ˆφn}N
n=1 and ˆα = { ˆαk }K
k=1 are the parameters.
K(cid:89)
N(cid:89)
Then we factorize q as
n=1
k=1
where ˆφn is a K × L matrix and q(zn , yn | ˆφn ) is a multinomial distribution in which p(zn = k , yn =
l) = ˆφnkl . This distribution is constrained by the candidate label set: if a label l /∈ Yn , then ˆφnkl = 0
for any value of k . The distribution q(θk | ˆαk ) is a Dirichlet distribution with parameter ˆαk .
After we set the distribution q(z , y , θ), our variational EM follows standard methods. The detailed
derivation can be found in the supplementary materials [11]. Here we only show the ﬁnal updating
step with some analysis.
(cid:26) φnk exp (cid:0)Eq(θk | ˆαk ) [log(θkl )](cid:1) ,
In the E step, the parameters of variational distribution are updated as (11) and (12).
if l ∈ Yn
ˆφnkl ∝
if l /∈ Yn
N(cid:88)
0,
ˆφnkl .
n=1

ˆαk = α +

(11)

(10)

(12)

,

The update of ˆφn in (11) indicates the key difference between the LSB-CMM model and traditional
clustering models. The formation of regions is directed by both instance similarities and class labels.

4

If the instance xn wants to join region k (i.e., (cid:80)
ˆφnkl is large), then it must be similar to wk as
l
well as to instances in that region in order to make φnk large. Simultaneously, its candidate labels
must ﬁt the “label ﬂavor” of region k , where the “label ﬂavor” means region k prefers labels having
large values in ˆαk . The update of ˆα in (12) can be interpreted as having each instance xn vote for
the label l for region k with weight ˆφnkl .
In the M step, we need to solve the maximization problem in (13) for each wk , 1 ≤ k ≤ K − 1.
Note that wK is ﬁxed. Each wk can be optimized separately. The optimization problem is similar to
the problem of logistic regression and is also a concave maximization problem, which can be solved
(cid:105)
(cid:104) ˆφnk log(expit(wT
N(cid:88)
by any gradient-based method, such as BFGS.
− 1
k xn )) + ˆψnk log(1 − expit(wT
k Σ−1wk +
where ˆφnk = (cid:80)L
ˆφnkl and ˆψnk = (cid:80)K
max
k xn ))
wT
,
2
wk
n=1
ˆφnj .
Intuitively, the variable ˆφnk is the proba-
j=k+1
l=1
bility that instance xn belongs to region k , and ˆψnk is the probability that xn belongs to region
{k + 1, · · · , K }. Therefore, the optimal wk discriminates instances in region k against instances in
regions ≥ k .

(13)

2.3 Prediction

For a test instance xt , we predict the label with maximum posterior probability. The test instance
can be mapped to a region with w, but the coding matrix θ is marginalized out in the EM. We use
the variational distribution p(θk | ˆαk ) as the prior of each θk and integrate out all θk -s. Given a test
point xt , the prediction is the label l that maximizes the probability p(yt = l|xt , w, ˆα) calculated as
K(cid:88)
(14). The detailed derivation is also in the supplementary materials [11].
ˆαkl(cid:80)
p(yt = l|xt , w, ˆα) =
(cid:16)
(cid:17)
k xt ) (cid:81)k−1
l ˆαkl
k=1
i=1 (1 − expit(wT
where φtk =
expit(wT
i xt ))
. The test instance goes to region k with
probability φtk , and its label is decided by the votes ( ˆαk ) in that region.

(14)

φtk

,

2.4 Complexity Analysis and Practical Issues

In the E step, for each region k , the algorithm iterates over all candidate labels of all instances, so the
complexity is O(N K L). In the M step, the algorithm solves K − 1 separate optimization problems.
Suppose each optimization problem takes O(V N d) time, where V is the number of BFGS iterations.
Then the complexity is O(K V N d). Since V is usually larger than L, the overall complexity of one
we only store the matrix (cid:80)L
EM iteration is O(K V N d). Suppose the EM steps converge within m iterations, where m is usually
less than 50. Then the overall complexity is O(mK V N d). The space complexity is O(N K ), since
ˆφnkl and the matrix ˆα.
l=1
In prediction, the mapping phase requires O(K d) time to multiply w and the test instance. Af-
ter the stick breaking process, which takes O(K ) calculations, the coding phase requires O(K L)
calculation. Thus the overall time complexity is O(K max{d, L}). Hence, the prediction time is
comparable to that of logistic regression.
There are several practical issues that affect the performance of the model. Initialization: From
the model design, we can expect that instances in the same region have the same label. Therefore,
it is reasonable to initialize ˆα to have each region prefer only one label, that is, each ˆαk has one
element with large value and all others with small values. We initialize φ to φnk = 1
K , so that
all regions have equal probability to be chosen at the start. Initialization of these two variables is
enough to begin the EM iterations. We ﬁnd that such initialization works well for our model and
generally is better than random initialization. Calculation of Eq(θk | ˆαk ) [log(θkl )] in (11): Although
it has a closed-form solution, we encountered numerical issues, so we calculate it via Monte Carlo
sampling. This does not change complexity analysis above, since the training is dominated by M
step. Priors: We found that using a non-informative prior for Dirichlet(α) worked best. From (12)
and (14), we can see that when θ is marginalized, the distribution is non-informative when α is set
to small values. We use α = 0.05 in our experiments.

5

Figure 2: Decision boundaries of LSB-CMM on a linearly-inseparable problem. Left: all data points
have true labels. Right: labels of gray data points are corrupted.

3 Experiments

In this section, we describe the results of several experiments we conducted to study the behavior of
our proposed model. First, we experiment with a toy problem to show that our algorithm can solve
problems with linearly-inseparable classes. Second, we perform controlled experiments on three
synthetic datasets to study the robustness of LSB-CMM with respect to the degree of ambiguity of
the label sets. Third, we experiment with three real-world datasets.
LSB-CMM Model: The LSB-CMM model has three parameters K, σ2 , α. We ﬁnd that the model
is insensitive to K if it is sufﬁciently large. We set K = 10 for the toy problems and K = 5L for
other problems. α is set to 0.05 for all experiments. When the data is standardized, the regularization
parameter σ2 = 1 generally gives good results, so σ2 is set to 1 in all superset label tasks.
Baselines: We compared the LSB-CMM model with three state-of-the-art methods. Supervised
SVM: the SVM is always trained with the true labels. Its performance can be viewed as an upper
bound on the performance of any SSL algorithm. LIBSVM [3] with RBF kernel was run to construct
a multi-class classiﬁer in one-vs-one mode. One third of the training data was used to tune the C
parameter and the RBF kernel parameter γ . CLPL: CLPL [5] is a linear model that encourages
large average scores of candidate labels. The model is insensitive to the C parameter, so we set
the C value to 1000 (the default value in their code). SIM: SIM [2] minimizes the ranking loss of
instances in a bag. In controlled experiments and in one of the real-world problems, we could not
make the comparison to LSB-CMM because of the lack of bag information. The λ parameter is set
to 10−8 based on authors’ recommendation.

3.1 A Toy Problems

In this experiment, we generate a linearly-inseparable SLL problem. The data has two dimensions
and six clusters drawn from six normal distributions with means at the corners of a hexagon. We
assign a label to each cluster so that the problem is linearly-inseparable (see (2)). In the ﬁrst task,
we give the model the true labels. In the second task, we add a distractor label for two thirds of
all instances (gray data points in the ﬁgure). The distractor label is randomly chosen from the two
labels other than the true label. The decision boundaries found by LSB-CMM in both tasks are
shown in (2)). We can see that LSB-CMM can successfully give nonlinear decision boundaries for
this problem. After injecting distractor labels, LSB-CMM still recovers the boundaries between
classes. There is minor change of the boundary at the edge of the cluster, while the main part of
each cluster is classiﬁed correctly.

3.2 Controlled Experiments
We conducted controlled experiments on three UCI [6] datasets: {segment (2310 instances, 7
classes), pendig its (10992 instances, 10 classes), and usps (9298 instances, 10 classes)}. Ten-
fold cross validation is performed on all three datasets. For each training instance, we add distractor
labels with controlled probability. As in [5], we use p, q , and ε to control the ambiguity level of

6

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.6−0.800.81.6−1.6−0.800.81.6lclass 1class 2class 3llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.6−0.800.81.6−1.6−0.800.81.6lclass 1class 2class 3Figure 3: Three regions learned by the model on usps

candidate label sets. The roles and values of these three variables are as follows: p is the proba-
bility that an instance has distractor labels (p = 1 for all controlled experiments); q ∈ {1, 2, 3, 4}
is the number of distractor labels; and ε ∈ {0.3, 0.7, 0.9, 0.95} is the maximum probability that a
distractor label co-occurs with the true label [5], also called the ambiguity degree.
We have two settings for these three variables. In the ﬁrst setting, we hold q = 1 and vary ε, that is,
for each label l, we choose a speciﬁc label l(cid:48) (cid:54)= l as the (unique) distractor label with probability ε
or choose any other label with probability 1 − ε. In the extreme case when ε = 1, l(cid:48) and l always
co-occur, and they cannot be distinguished by any classiﬁer. In the second setting, we vary q and
pick distractor labels randomly for each candidate label set.
The results are shown in Figure (4). Our LSB-CMM model signiﬁcantly outperforms the CLPL
approach. As the number of distractor labels increases, performance of both methods goes down, but
not too much. When the true label is combined with different distractor labels, the disambiguation is
easy. The co-occurring distractor labels provide much less disambiguation. This explains why large
ambiguity degree hurts the performance of both methods. The small dataset (segment) suffers even
more from large ambiguity degree, because there are fewer data points that can “break” the strong
correlation between the true label and the distractors.
To explore why the LSB-CMM model has good performance, we investigated the regions learned
by the model. Recall that φnk is the probability that xn is sent to region k . In each region k , the
representative instances have large values of φnk . We examined all φnk from the model trained
on the usps dataset with 3 random distractor labels. For each region k , we selected the 9 most
representative instances. Figure (3) shows representative instances for three regions. These are all
from class “2” but are written in different styles. This shows that the LSB-CMM model can discover
the sub-classes in the data. In some applications, the whole class is not easy to discriminate from
other classes, but sometimes each sub-class can be easily identiﬁed. In such cases, LSB-CMM will
be very useful and can improve performance.
Explanation of the results via regions can also give better understanding of the learned classiﬁer.
In order to analyze the performance of the classiﬁer learned from data with either superset labels
or fully observed labels, one traditional method is to compute the confusion matrix. While the
confusion matrix can only tell the relationships between classes, the mixture analysis can indicate
precisely which subclass of a class are confused with which subclasses of other classes. The regions
can also help the user identify and deﬁne new classes as reﬁnements of existing ones.

3.3 Real-World Problems

We apply our model on three real-world problems. 1) BirdSong dataset [2]: This contains 548
10-second bird song recordings. Each recording contains 1-40 syllables. In total there are 4998
syllables. Each syllable is described by 38 features. The labels of each recording are the bird species
that were singing during that 10-second period, and these species become candidate labels set of each
syllable in the recording. 2) MSRCv2 dataset: This dataset contains 591 images with 23 classes.
The ground truth segmentations (regions with labels) are given. The labels of all segmentations
in an image are treated as candidate labels for each segmentation. Each segmentation is described
by 48-dimensional gradient and color histograms. 3) Lost dataset [5]: This dataset contains 1122
faces, and each face has the true label and a set of candidate labels. Each face is described by 108
PCA components. Since the bag information (i.e., which faces are in the same scene) is missing,

7

Figure 4: Classiﬁcation performance on synthetic data (red: LSB-CMM; blue: CLPL). The dot-dash
line is for different q values (number of distractor labels) as shown on the top x-axis. The dashed
line is for different ε (ambiguity degree) values as shown on the bottom x-axis.

Table 1: Classiﬁcation Accuracies for Superset Label Problems
SVM
SIM
LSB-CMM
CLPL
0.715(0.042)
0.790(0.027)
0.637(0.034)
0.589(0.035)
0.459(0.032)
0.454(0.043)
0.411(0.044)
0.673(0.043)
0.710(0.045)
0.703(0.058)
0.817(0.038)
-

BirdSong
MSRCv2
Lost

SIM is not compared to our model on this dataset. We run 10-fold cross validation on these three
datasets. The BirdSong and MSRCv2 datasets are split by recordings/images, and the Lost dataset
is split by faces.
The classiﬁcation accuracies are shown in Table (1). Accuracies of the three superset label learning
algorithms are compared using the paired t-test at the 95% conﬁdence level. Values statistically
indistinguishable from the best performance are shown in bold. Our LSB-CMM model out-performs
the other two methods on the BirdSong database, and its performance is comparable to SIM on the
MSRCv2 dataset and to CLPL on the Lost dataset. It should be noted that the input features are
very coarse, which means that the cluster structure of the data is not well maintained. The relatively
low performance of the SVM conﬁrms this. If the instances were more precisely described by ﬁner
features, one would expect our model to perform better in those cases as well.

4 Conclusions

This paper introduced the Logistic Stick-Breaking Conditional Multinomial Model to address the
superset label learning problem. The mixture representation allows LSB-CMM to discover cluster
structure that has predictive power for the superset labels in the training data. Hence, if two labels
co-occur, LSB-CMM is not forced to choose one of them to assign to the training example but
instead can create a region that maps to both of them. Nonetheless, each region does predict from a
multinomial, so the model still ultimately seeks to predict a single label. Our experiments show that
the performance of the model is either better than or comparable to state-of-the-art methods.

Acknowledgment

This material is based upon work supported by the National Science Foundation under Grant
No. 1125228. The code as an R package is available at:
http://web.engr.oregonstate.edu/˜liuli/files/LSB-CMM_1.0.tar.gz.

8

accuracyllll0.30.70.90.950.70.80.91.01234lSVMLSB−CMM, vary qLSB−CMM, vary eCLPL, vary qCLPL, vary enumber of ambiguous labelsambiguity degree(a) segmentllll0.30.70.90.950.70.80.91.01234lSVMLSB−CMM, vary qLSB−CMM, vary eCLPL, vary qCLPL, vary enumber of ambiguous labelsambiguity degree(b) pendigitsllll0.30.70.90.950.70.80.91.01234lSVMLSB−CMM, vary qLSB−CMM, vary eCLPL, vary qCLPL, vary enumber of ambiguous labelsambiguity degree(c) uspsReferences

[1] C. M. Bishop. Pattern recognition and machine learning. Springer, 2006.
[2] F. Briggs & X. F. Fern & R. Raich. Rank-Loss Support Instance Machines for MIML Instance Annotation.
In proc. KDD, 2012.
[3] C.-C. Chang & C.-J. Lin. LIBSVM: A Library for Support Vector Machines. ACM Trans. on Intelligent
Systems and Technology, 2(3):1-27, 2011.
[4] T. Cour & B. Sapp & C. Jordan & B. Taskar. Learning From Ambiguously Labeled Images. In Proc.
CVPR 2009.
[5] T. Cour & B. Sapp & B. Taskar. Learning from Partial Labels. Journal of Machine Learning Research,
12:1225-1261, 2011.
[6] A. Frank & A. Asuncion. UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].
[7] Y. Grandvalet. Logistic Regression for Partial Labels. In Proc. IPMU, 2002.
[8] E. Hullermeier & J. Beringer. Learning from Ambiguously Labeled Examples. In Proc. IDA-05, 6th
International Symposium on Intelligent Data Analysis Madrid, 2005.
[9] L. Jie & F. Orabona. Learning from Candidate Labeling Sets. In Proc. NIPS, 2010.
[10] R. Jin & Z. Ghahramani. Learning with Multiple Labels. In Proc. NIPS, 2002.
[11] L-P. Liu & T. Dietterich. A Conditional Multinomial Mixture Model for Superset Label Learning (Sup-
plementary Materials),
http://web.engr.oregonstate.edu/˜liuli/pdf/lsb_cmm_supp.pdf .
[12] N. Nguyen & R. Caruana. Classiﬁcation with Partial Labels. In Proc. KDD, 2008.
[13] L. Ren & L. Du & L. Carin & D. B. Dunson. Logistic Stick-Breaking Process. Journal of Machine
Learning Research, 12:203-239, 2011.
[14] Y. W. Teh. Dirichlet Processes. Encyclopedia of Machine Learning, to appear. Springer.
[15] Z.-H. Zhou & M.-L. Zhang. Multi-Instance Multi-Label Learning with Application To Scene Classiﬁca-
tion. Advances in Neural Information Processing Systems, 19, 2007
[16] X. Zhu & A. B. Goldberg. Introduction to Semi-Supervised Learning. Synthesis Lectures on Artiﬁcial
Intelligence and Machine Learning, 3(1):1-130, 2009.

9


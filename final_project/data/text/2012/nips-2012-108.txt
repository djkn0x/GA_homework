Perfect Dimensionality Recovery
by Variational Bayesian PCA

Shinichi Nakajima
Nikon Corporation
Tokyo, 140-8601, Japan
nakajima.s@nikon.co.jp

Ryota Tomioka
The University of Tokyo
Tokyo 113-8685, Japan
tomioka@mist.i.u-tokyo.ac.jp

Masashi Sugiyama
Tokyo Institute of Technology
Tokyo 152-8552, Japan
sugi@cs.titech.ac.jp

S. Derin Babacan
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
dbabacan@illinois.edu

Abstract

The variational Bayesian (VB) approach is one of the best tractable approxima-
tions to the Bayesian estimation, and it was demonstrated to perform well in many
applications. However, its good performance was not fully understood theoreti-
cally. For example, VB sometimes produces a sparse solution, which is regarded
as a practical advantage of VB, but such sparsity is hardly observed in the rigorous
Bayesian estimation. In this paper, we focus on probabilistic PCA and give more
theoretical insight into the empirical success of VB. More speciﬁcally, for the sit-
uation where the noise variance is unknown, we derive a sufﬁcient condition for
perfect recovery of the true PCA dimensionality in the large-scale limit when the
size of an observed matrix goes to inﬁnity. In our analysis, we obtain bounds for
a noise variance estimator and simple closed-form solutions for other parameters,
which themselves are actually very useful for better implementation of VB-PCA.

1 Introduction

Variational Bayesian (VB) approximation [1] was proposed as a computationally efﬁcient alternative
to rigorous Bayesian estimation. The key idea is to force the posterior to be factorized, so that the
integration—a typical intractable operation in Bayesian methods—can be analytically performed
over each parameter with the other parameters ﬁxed. VB has been successfully applied to many
applications [4, 7, 20, 11].
Typically, VB solves a non-convex optimization problem with an iterative algorithm [3], which
makes theoretical analysis difﬁcult. An important exceptional case is the matrix factorization (MF)
model [11, 6, 19] with no missing entry in the observed matrix. Recently, the global analytic solution
of VBMF has been derived and theoretical properties such as the mechanism of sparsity induction
have been revealed [15, 16]. These works also posed thought-provoking relations between VB
and rigorous Bayesian estimation: The VB posterior is actually quite different from the true Bayes
posterior (compare the left and the middle graphs in Fig. 1), and VB induces sparsity in its solution
but such sparsity is hardly observed in rigorous Bayesian estimation (see the right graph in Fig. 1).1
These facts might deprive the justiﬁcation of VB based solely on the fact that it is one of the best
tractable approximations to the Bayesian estimation.

1Also in mixture models, inappropriate model pruning by VB approximation was discussed [12].

1


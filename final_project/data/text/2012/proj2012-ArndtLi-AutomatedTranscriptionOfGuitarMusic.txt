Automated Transcription of Guitar Music

Carl-Fredrik Arndt
Institute for Computational & Mathematical Engineering
Stanford University
Stanford, California 94305
Email: cfarndt@stanford.edu

Lewis Li
Electrical Engineering
Stanford University
Stanford, California 94305
Email: lewisli@stanford.edu

Abstract—In this report, we present our methodology of analyzing
music to extract and transcribe guitar notes. Our approach is divided
into three steps, the ﬁrst isolates the guitar track from the music using
Independent Subspance Analysis (ISA). Meanwhile, the second step uses
both frequency and time domain based approaches to transcribe the
guitar track into the corresponding notes. A duration detection algorithm
is then used to isolate the length of each note. We will apply this
methodology to a series of test data ranging from diatonic scales to
segments of classic rock songs. One important distinction is that we do
not attempt transcription of guitar chords, focusing only on monophonic
scores.

I . IN TRODUC T ION

A. Motivation

The concept of Automated Music Transcription (AMT) has been
extensively studied in recent years. The premise is the autonomous
translation of digital audio waveforms into a symbolic music rep-
resentation akin to what would be on a score. This can be almost
viewed as a dis-assembler routine for acoustic music. Such a program
would have widespread applications, allowing the modiﬁcation, and
re-arrangement of music, in addition to providing an easier platform
for aspiring musicians to learn how to play speciﬁc songs. From a
different perspective, music transcription is presently performed by
trained humans. The development of an effective automated transcrip-
tion algorithm could provide insight into complicated processes of
human perception of music. Another application could be improved
identiﬁcation of musical genres through the analysis of the notes rather
than just the frequency signature. Consequently, the possibility of
automated music transcription has piqued the interest of individual
in a wide variety of ﬁelds.

B. Prior and Related Work

With the increase in computing power, research into AMT has
grown substantially over the past decade. A closely related ﬁeld is
that of automated speech recognition, which has enjoyed considerably
greater research efforts, mainly due to the commercial implications
of such products. Real time speech recognition and natural language
processing has now entered the mainstream with products such as Siri.
While AMT lags behind in research exertion, many of the ideas from
speech recognition can be borrowed [14].
Music is separated into two main categories: monophonic and poly-
phonic. The former refers to music where no more than one note
is simultaneously being played, while latter describes music where
no such constraints are deﬁned. Transcribing monophonic music has
been studied previously in [2]. The basic steps require disseminating
between silence and noise, determining a pitch’s tone, and ﬁnally
mapping to an appropriate note. A variety of techniques such as zero-
crossing [18] or auto-correlation have been shown to provide accurate
results [16]. Alternatively, frequency domain methods such as Har-
monic Product Spectrum (HPS) [20], and Cepstrum [26] have shown

comparably favorable results. Conversely, automated transcription of
polymorphic music is a much more challenging problem as many
spectral peaks arise in the frequency domain corresponding to the
various harmonies and fundamentals being played [21]. A number
of recent PhD and Masters thesis have been dedicated to addressing
this problem. The most recent general approach has centered upon
non-negative matrix factorization [25]. Approaches such as [8] have
used prerecorded note samples to build a library of note templates,
which incoming notes are then mapped to. Despite such efforts, no
general-purpose system has been developed that can be on par with
a trained human musician. Generally, constraints such as the drums
and percussive instruments are not allowed, or the number of other
instruments must be known at run-time.
A related problem is that of source separation, commonly referred to
as the Cocktail Part Problem. This is far from being a solved problem,
but approaches such as Principal Component Analysis (PCA) together
with Independent Component Analysis (ICA) have demonstrated
success on simpliﬁed problems [11]. There have been attempts to
use pitch detection to aid in source separation [12], but very limited
proposals to combine the two to transcribe polyphonic music.

C. Goals

This project intends on developing an implementation that combines
the two aforementioned techniques to provide an attempt at AMT.
Obviously, the development of a general purpose AMT is not possible,
thus a secondary goal is identify the constraints under which such
a system would work. It is well known that chord recognition is a
very challenging problem [24], hence the scope of this project will
focus on single guitar notes. Furthermore, it is known that delays and
echos makes source separation very difﬁcult [19]. Consequently, this
will complicate separating tracks where electric guitar effects such as
ﬂanger, reverb, and chorus are used. In the following sections, we will
present out methodology for track separation and music transcription,
describe the method of which we collected test data and present and
discuss the experimental results before delineating any future steps.

I I . M ETHODO LOGY

Fig. 1. Overview of methodology

Music is consisted of notes, each with characteristics such as
pitch, duration, loudness and timbre. Pitch refers to the perception
of how ”high” or ”low” a note is. Notes are classiﬁed into sets of

12 semi-tones, termed an octave. Pitch is determined primarily by the
fundamental frequency of the note. Table II shows the fundamental
frequencies of the notes a guitar with standard tuning is capable of
producing. Duration refers to the temporal length of each note which
is measured in beats. Loudness is measured by the amplitude of the
waveform. Finally, timber describes the ”color” of a note, which
allows humans to desiminate between different instruments playing
the same note. In reality, notes are comprised of the fundamental
frequency f0 as well as harmonics, frequencies manifesting at integer
multiples of f0 . The initial step of our methodology uses PCA and ICA
to isolate guitar tracks. The preceding phase demarcate the portions of
the track corresponding to rests; allowing us to evaluate the duration
of each note. This involves extracting features of the waveform and
applying an appropiate threshold. The penultimate step estimates the
pitch of each individual note using both time and frequency domain
techniques. Finally, we match the estimated pitch to Table II, and
produce a Musical Instrument Digital Interface (MIDI) version of our
initial track to verify correctness.

Octave

2

3

4

5

6

E
F
F(cid:93)
G
G(cid:93)
A
B(cid:91)
B
C
C(cid:93)
D
E(cid:91)

82.41
87.31
92.50
98.00
103.8
110.0
116.5
123.5
-
-
-
-

164.8
174.6
185.0
196.0
207.7
220.0
233.1
246.9
130.8
138.6
146.8
155.6

329.6
349.2
370.0
392.0
415.3
440.0
466.2
493.9
261.6
277.2
293.7
311.1

659.3
698.5
740.0
784.0
830.6
880.0
932.3
987.8
523.3
554.4
587.3
622.3

-
-
-
-
-
-
-
-
1047
1109
1175
1245

TABLE I
FUNDAM ENTA L FREQUENC I E S O F NOTE S THAT A GU I TAR CAN PRODUC E W I TH
STANDARD TUN ING [1 ]

A. Track Separation

The track separation will be done by a method called Independent
Subspace Analysis (ISA) which was proposed in [3]. ISA uses ICA
create number of different signals, where each signal is matched to a
source (instrument).
We here look at a setting where we assume that the number of
sources in the audio signal is stationary, but the extension to the
non-stationary case is trivial. We now describe all the steps of ISA
separately.

1) Divide Signal:
If we are given an audio signal x we split
this signal into m separate pieces of length w . Here the ﬁrst piece
corresponds to the ﬁrst part of the song and etc. Denote each of these
pieces xk , 1 ≤ k ≤ m and put them into a w × m matrix X.
2) Spectral Transformation: Since we are interested in the spectral
properties of the signal we apply a linear spectral transformation (in
out implementation we used the Short Term Fourier Transform) to the
signal. For each xk , this means that we are using a linear operator M
(an n × w matrix where n is the number of spectral components) and
ﬁnd a representation sk such that xk = M sk . A n × k matrix S in
then created in the same way as X .

3) Feature extraction: To extract features from the one dimensional
signals we apply the SVD to S , so that S = U ΣV T . Here the V -
vectors corresponds to our principal components/features. To deter-
mine how many principal components to use, we put a lower threshold
on the information ratio. Assuming ρ is the number of principals

(cid:80)ρ
(cid:80)n
i=1 σi
, (here σi tea the singular
components used, this means that
i=1 σi
values) needs to be greater then some given number (chosen to be 0.7
in [3]).

· · ·
· · ·
. . .
· · ·

D =

H (M ; D) =

δKL (z1 , z1 )
δKL (z2 , z1 )
...
δKL (zρ , z1 )

δKL (z1 , z2 )
δKL (z2 , z2 )
...
δKL (zρ , z2 )

δKL (z1 , zρ )
δKL (z2 , zρ )
...
δKL (zρ , zρ )

4) Independent Feature Creation: If we group all the ρ vectors
from previous step into a ρ × n observation matrix V ρ we can now
use the standard case ICA on this. This will create an independent set
of feature vectors zi , 1 ≤ i ≤ ρ.
5) Subspace Identiﬁcation: To create clusters containing separate
instruments, we will use the Kullback-Leibler divergence. First calcu-
late the so called IXEGRAM matrix, deﬁned in [3], as

 .
Here δKL (zi , zj ) denotes the Kullback-Leibler divergence between
zi and zj . Since the Kullback-Leibler divergence is calculated using
a distribution function, and we only have access to a samples, these
has to be approximated numerically. Next introduce the cost-function
ρ(cid:88)
ρ(cid:88)
κ(cid:88)
1(cid:80)ρ
j=1 Mj c
c=1
i=1
k=1
Here M is a ρ × κ binary valued matrix where κ is the number
of clusters and Mi,j denotes whether signal i is in cluster κ or not.
Note that the number of clusters is prespeciﬁed, so we have to run
this part of the algorithm multiple times to approximate right number
of clusters. To ﬁnd M for a ﬁxed k we minimize the cost function (1).
This is be done by using a deterministic annealing algorithm (basically
a ﬁxed point algorithm) which solves
(cid:17)
(cid:16)
(cid:80)ρ
(cid:80)κ
2 (cid:80)n
1 + (cid:80)ρ
Di,k −
j=1 Mj,λDj,k
k=1
j=1,j (cid:54)=i Mj,λ
(cid:80)κ
j=1,j (cid:54)=i Mj,λ
− exp(i,λ/T )
µ=1 exp(−i,µ/T )
where T a variable parameter. This is described in [10].

MicMkcDik .

(1)

i,λ =

Mi,λ =

,

,

6) Retrieve signals: In the last step we simply have to retrieve
the κ audio signals of the different instruments. This is done by ﬁrst
inverting the ICA step by using the mixing matrix, then inverting
the SVD and at
last using the inverse transform of the spectral
transformation.

B. Duration Detection

Silence detection is a commonly studied problem in Voice Activity
Detection (VAD) [13], and we will borrow some of the commonly
used audio features to detect rest beats in music.
M(cid:88)
1) Short Term Energy: The short term energy is deﬁned as:
m=1
where w refers to a Hamming window. The short term energy gives a
representation of the temporal amplitude variation. This is perhaps the
best feature for detecting periods of silence in a an audio sequence.

x2 [m]w2 [ˆn − m]

Xi =

Zi =

F Li =

(EXi [m] − EXi−1 [m])2

0.5| sgn(x[m]) − sgn(x[m − 1])|w[ˆn − m]

M(cid:88)
2) Zero-Crossing Rate: The zero-crossing rate is deﬁned as:
m=1
Zero-crossing measures the noisiness of a signal, and has been used
to discern between music and other noises (background, vocals etc)
[7]
M(cid:88)
3) Spectral Flux: The spectral ﬂux is deﬁned as:
m=1
where EXi is the normalized DFT coefﬁcients of the ith window.
This measures the local spectral change which is notably higher due
to noise during silence.
(cid:80)∞
4) Spectral Centroid: The spectral centroid is deﬁned as:
(cid:80)∞
m=−∞ mX [m]
m=−∞ X [m]
Here X refers to the DFT of x. The spectral centroid gives a metric
for the ’brightness’ of the sound [23].
(cid:16) 1
(cid:17)
(cid:80)M −1
5) Weiner Entropy: The Weiner Entropy is deﬁned as:
(cid:80)M −1
m=0 ln x[n]
M
1
m=0 x[n]
M
Weiner Entropy is used as a metric for determining if a sound is more
tone like or noise like [5].

C ˆn =

exp

Wi =

6) Thresholding: With these features,
is possible to train a
it
Support Vector Machine (SVM) or another other classiﬁer to label
portions as noise or music. However, it has been shown that much
simpler techniques can be employed to provide excellent separation
[17]. We adapted the following steps from [6].

1) Compute histogram of feature values
2) Detect histogram local maximas
3) Compute threshold T = W M1+M2
, where W is a user deﬁned
W +1
weight, and M1 and M2 are the ﬁrst and second maximas.

We will use a combination of the features deﬁned above and a
smaller window size (10ms to 25ms), since rests in music are generally
shorter than that of human speech.

C. Music Transcription

Once the tone portions of the track have been isolated, a pitch
detection algorithm can be deployed to compute the notes themselves.
Pitch detection is also a common problem in voice recognition and a
variety of algorithms have been proposed [4]. As there is no overall
consensus of an ideal pitch recognition algorithm, we have elected to
implement the a few of the most popular. This can offer us insight to
any causable relationships between dataset and an optimal algorithm.

1) Harmonic Product Spectrum: Harmonic Product Spectrum
(HPS) is a frequency domain algorithm [15]. The basic idea is to take
the Fourier Transform of a windowed portion of the waveform. The
windowing function that was proposed was the Hamming Window.
Ideally, this will yield a set of peaks at the fundamental frequency and
its associated harmonics. f0 can be abstracted simply by ﬁnding the
greatest common denominator of peak frequencies. A computationally
efﬁcient method of evaluating this is to down-sample the spectrum by
an integer factor, and then computing the element wise product with

the original spectrum. This will eliminate high frequency harmonics,
while magnifying the fundamental frequency. Figure 2 gives a visual
representation of this algorithm. A few iterations of the downsample-
multiply procedure will yield a spectrum that has a maxima at the
fundamental frequency.

Fig. 2. Overview of Harmonic Product Spectrum

2) Cepstrum: Cepstrum is another Fourier analysis technique
which involves evaluating the logarithmic amplitude spectrum of the
signal.That is:

C = |F {log X }|2

Assuming this log amplitude spectrum contains many regularly
space harmonics, the Fourier analysis will yield peaks corresponding
to the fundamental frequency. In essense, the agorithm searches for
periodicity within the spectrum.
The cepstrum has units of quefrency, and peaks within the cepstrum
are known as rahmonics. These correspond to periodicies within the
spectrum. Therefore,
to obtain an estimate of f0 , we search for
peaks within the quefrency regions corresponding to fundamental
frequencies of notes a guitar can produce. Quefrency and frequency
are inverses of one another.

3) Auto-Correlation: Auto-correlation is the most commonly used
time-series pitch detection algorithm in speech recognition [9]. The
idea relies on the fact that the correlation signal will have a peak at
the lag corresponding to the pitch period. The autocorrelation for a
discrete signal is given as:
N(cid:88)
m=0

−k − 1s[m]s[m + k ]

R[k ] =

Here N refers to the number of samples in the window, and k is the
lag index. The choice of N is important in the sense that it must cover
2 or more periods such that periodicity can be seen. Conversely, a
larger window will dimish the algorithm’s capability to detect temporal
variations in pitch. Now to ﬁnd the fundamental frequency we use the
following:

f0 =

1
kmax fs

, R[kmax ] = arg max
k

R[k ]

Here fs is the sampling frequency. The key assumption is that if
s[m] is periodic, R[k ] should exhibit peaks at integer multiples of
the period. The main peak in the autocorrelation function will be at
k = 0 (since a signal should always correlate to itself). The location
of the next peak gives an estimate of the period. The method usually
require a number of periods of data to form a reliable estimate, and
thus some averaging of the frequency signal is unavoidable.

D. Post-Processing & MIDI Synthesis

Pitch detection and duration detection will both yield a vector
corresponding to the number of windows. The pitch detection vector

will contain the estimated fundamental frequencies. By ﬁnding the
closest match for each frequency in Table II, we can obtain a vector
of semi-tone notes. This vector is then multiplied element wise by the
boolean vector generated by duraction detection to yield the estimated
transcription, where rests correspond to 0 frequency. To generate an
audio waveform, we transformed the appropiate notes into MIDI notes,
allowing for playback.

I I I . DATA CO LL EC T ION

For the intial testing of pitch detection and duration detection,
we recorded monophonic guitar tracks using iJam and Apple’s
Garageband. The test cases were comprised of a E-Major scale,
the introduction to Led Zeppelin’s Stairway To Heaven, and Guns
N’Roses’ Sweet Child O’ Mine. To test the track seperation we used
Red Hot Chilli Peppers’ Otherside, which features drums, guitar, bass
and vocals.

IV. EX PER IM EN TA L R E SU LT S

A. Track Separation

We ran the previously described algorithm on approximately 10
seconds of the initial part of Otherside. This part of the song only
contains a bass guitar and a regular guitar and thus we could set
the number of clusters, κ, to be ﬁxed to two. After doing this we
transformed the received clusters back to Wav ﬁles and played them.
One could clearly here which part contained the bass note versus the
regular guitar. In ﬁgure 3 we show the initial Wav ﬁle and then both
the ﬁrst (regular guitar) and second (bass) clusters. The main problem
with the algorithm is that it is build to separate pitched instruments
and thus fail to separate the song from the instruments as noted in
[3].

Fig. 4. Duration detection algorithm performed on Stairway To Heaven. A) Short
time energy with median smoothing. B) Spectral centroid with median smoothing. C)
Duration detection result. D) Waveform of the original track.

C. Music Transcription

We attempted music transcription using HPS, Cepstrum and Auto-
Correlation on the various sets of test data. For our selected test cases,
HPS appeared to yield the best results.

1) E-Major Scale: This was a simple test case where we played
8 notes that are part of a E-Major scale. All three of the algorithms
were able to recognize all notes as shown in Figure 5.

Fig. 3. Time-series plot of the original Wav ﬁle(top), separated guitar part(middle)
and bass guitar part(bottom).

Pitch detection result combined with duration detection for E-Major scale
Fig. 5.
using HPS

B. Duration Detection

We attempted duration detection using permutations of the features
described in Section II-B. It was found that short time energy and
spectral centroid provided acceptable results, while additional features
did not necessarily yield any improvements. Figure 4 shows the
features after a median ﬁlter has been applied. The bottom two plot
indicates the periods during which a note is being played compared
to the actual waveform. As we can see, the duration detection is
reasonably successful at identifying the rests and notes.

2) Stairway to Heaven: Stairway to Heaven is an slow tempo song
(approximately 80bpm in 4-4 time). We recorded a version that was
strictly monophonic. All three algorithms generated small incorrect
spikes in frequency at the begining of each note. This can be rectiﬁed
by using a median ﬁlter of 3 elements.

3) Sweet Child O’ Mine: Sweet Child O’ Mine is a fast tempo
song (200 bpm), that was originally played by Guns N’ Roses using an
overdriven electric guitar. For the purpose of this project, we recorded
a version played using a clean guitar. The output is shown in Figure
6.

[7] Fabien Gouyon, Francois Pachet, and Olivier Delerue. On the use of zero-crossing
rate for an application of classiﬁcation of percussive sounds. In Proceedings of
the COST G-6 Conference on Digital Audio Effects (DAFX-00, 2000.
[8] John E. Hartquist. Real-time musical analysis of polyphonic guitar audio. Master’s
thesis, California Polytechnic State University, 2012.
[9] W.J Hess. pitch and voicing determination”. Advancesin Speech Signal Process-
ing, 1992.
[10] T. Hofmann and J. M. Buhmann. Pairwise data clustering by deterministic
annealing. IEEE T. Pattern Anal, pages 1–14, 1997.
[11] Aapo Hyvrinen and Erkki Oja. Independent component analysis: algorithms and
applications. Neural Networks, 13:411–430, 2000.
[12] M. Karjalainen and T. Tolonen. Multi-pitch and periodicity analysis model for
sound separation and auditory scene analysis. In Proceedings of the Acoustics,
Speech, and Signal Processing, 1999. on 1999 IEEE International Conference
- Volume 02, ICASSP ’99, pages 929–932, Washington, DC, USA, 1999. IEEE
Computer Society.
[13] Tomi Kinnunen, Evgenia Chernenko, Marko Tuononen, Pasi Frnti, and Haizhou
Li. Voice activity detection using mfcc features and support vector machine, 2007.
[14] Anssi P. Klapuri. Automatic music transcription as we know it today. Journal of
New Music Research, pages 269–282, 2004.
[15] Gopala Krishna Koduri, Joan Serr, and Xavier Serra. Characterization of
intonation in carnatic music by parametrizing pitch histograms. In Proceedings
of the 13th International Society for Music Information Retrieval Conference,
Porto, Portugal, October 8-12 2012.
http://ismir2012.ismir.net/event/papers/
199- ismir- 2012.pdf.
[16] Philip Mcleod, Damon Simpson, Robert Visser, Mike Phillips, Ignas Kukenys,
Yaoyao Wang, Arthur Melissen, Natalie Zhao, and All The. Fast, accurate pitch
detection tools for music analysis.
[17] M. H. Moattar and M. M. Homayounpour. A simple but efﬁcient real-time voice
activity detection algorithm, 2009.
[18] C. Panagiotakis and G. Tziritas. A speech/music discriminator based on rms and
zero-crossings. IEEE Transactions on Multimedia, 7:155–166, 2004.
[19] Barak Pearlmutter and Lucas Parra. A context-sensitive generalization of ica,
1996.
[20] L. Rabiner and R. Schafer. Digital Processing of Speech Signals. Englewood
Cliffs: Prentice Hall, 1978.
[21] Matti Ryynnen and Anssi Klapuri. Transcription of the singing melody in
polyphonic music. In in Proc. 7th International Conference on Music Information
Retrieval, 2006.
[22] Eric D. Scheirer. Tempo and beat analysis of acoustic musical signals. Journal
of the Acoustical Society of America, 103(1):588–601, 1998.
[23] Emery Schubert and Joe Wolfe. Does timbral brightness scale with frequency
and spectral centroid?
[24] Alexander Sheh and Daniel Ellis. Chord segmentation and recognition using
em-trained hidden markov models, 2003.
[25] Paris Smaragdis and Judith C. Brown. Non-negative matrix factorization for
polyphonic music transcription. In In IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, pages 177–180, 2003.
[26] Christopher Wendt and Athina P. Petropulu. Pitch determination and speech
segmentation using the discrete wavelet transform. In in Proceedings of the IEEE
International Conference on Acoustics, Speech, and Signal Processing, pages 45–
48, 1996.

Fig. 6.
Mine

Pitch detection result combined with duration detection for Sweet Child O

D. Summary

Song
E-Major
Stairway
SChild
OSide
Sandman
Fad2Blk

Cepstrum Auto-Corr
HPS
Notes  % 
%
%

100
8
100
8
100
8
8
89
23
85
22
92
24
26
66
24
56
18
90
29
32
50
48
96
42
84
44
88
96
23
92
22
100
24
24
32
30
93
29
91
27
84

In our test cases, we were able to demonstrate over 90% success
rates using HPS. It should however be noted that we selected our test
cases in order to match the capabilities of our algorithms. That is,
monotonic tracks with no chords. In order to cater to all songs, this
restriction will have to be lifted.

V. FU TUR E WORK

In this project, we have demonstrated that ISA and HPS can be
used to achieve music transcription at least on select test data. The
greatest challenge is the transcription of chords. Bayesian approaches
such as [24], and generated success rates of over 70% and could be
adapted to serve an appropiate purpose here. Furthermore, to generate
an actual score the beat of the music will have to be determined.
Certain attempts such as [22] have also achieved around 70% success
rates. Once the tempo is established, it is simple to transform the
durations into note lengths. Finally, certain aspects of a score such
as dynamics should also be detected. The combination of all these
components would result in a complete automated music transcription
program.

ACKNOW L EDG EM EN T S

The authors would like to thank Prof. Andrew Ng and the CS229
Teaching Staff for their Machine Learning course.

R E F ER ENC E S

[1]
[2] Juan Pablo Bello, Giuliano Monti, Mark Sandler, and Mark S. Techniques
In in International Symposium on Music
for automatic music transcription.
Information Retrieval, pages 23–25, 2000.
[3] M. A. Casey and A. Westner. Separation of mixed audio sources by independent
subspace analysis. In Proc. Int. Comput. Music Conf., July -00 2000.
[4] Patricio De La Cuadra and Aaron Master. Efﬁcient pitch detection techniques for
interactive music. In In Proceedings of the 2001 International Computer Music
Conference, La Habana, 2001.
[5] S. Dubnov. Generalization of spectral ﬂatness measure for non-gaussian linear
processes. Signal Processing Letters, IEEE, 11(8):698 – 701, aug. 2004.
[6] Theodoros Giannakopoulos. Silence removal in speech signals, 2010.


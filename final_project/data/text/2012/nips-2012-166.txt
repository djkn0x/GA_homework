On-line Reinforcement Learning Using Incremental
Kernel-Based Stochastic Factorization

Andr ´e M. S. Barreto
School of Computer Science
McGill University
Montreal, Canada
amsb@cs.mcgill.ca

Doina Precup
School of Computer Science
McGill University
Montreal, Canada
dprecup@cs.mcgill.ca

Joelle Pineau
School of Computer Science
McGill University
Montreal, Canada
jpineau@cs.mcgill.ca

Abstract

Kernel-based stochastic factorization (KBSF) is an algorithm for solving rein-
forcement learning tasks with continuous state spaces which builds a Markov de-
cision process (MDP) based on a set of sample transitions. What sets KBSF apart
from other kernel-based approaches is the fact that the size of its MDP is indepen-
dent of the number of transitions, which makes it possible to control the trade-off
between the quality of the resulting approximation and the associated computa-
tional cost. However, KBSF’s memory usage grows linearly with the number of
transitions, precluding its application in scenarios where a large amount of data
must be processed. In this paper we show that it is possible to construct KBSF’s
MDP in a fully incremental way, thus freeing the space complexity of this algo-
rithm from its dependence on the number of sample transitions. The incremental
version of KBSF is able to process an arbitrary amount of data, which results in
a model-based reinforcement learning algorithm that can be used to solve contin-
uous MDPs in both off-line and on-line regimes. We present theoretical results
showing that KBSF can approximate the value function that would be computed
by conventional kernel-based learning with arbitrary precision. We empirically
demonstrate the effectiveness of the proposed algorithm in the challenging three-
pole balancing task, in which the ability to process a large number of transitions
is crucial for success.

Introduction
1
The task of learning a policy for a sequential decision problem with continuous state space is a
long-standing challenge that has attracted the attention of the reinforcement learning community for
years. Among the many approaches that have been proposed to solve this problem, kernel-based
reinforcement learning (KBRL) stands out for its good theoretical guarantees [1, 2]. KBRL solves
a continuous state-space Markov decision process (MDP) using a ﬁnite model constructed based on
sample transitions only. By casting the problem as a non-parametric approximation, it provides a
statistically consistent way of approximating an MDP’s value function. Moreover, since it comes
down to the solution of a ﬁnite model, KBRL always converges to a unique solution.

Unfortunately, the good theoretical properties of kernel-based learning come at a price: since the
model constructed by KBRL grows with the amount of sample transitions, the number of operations
performed by this algorithm quickly becomes prohibitively large as more data become available.
Such a computational burden severely limits the applicability of KBRL to real reinforcement learn-
ing (RL) problems. Realizing that, many researchers have proposed ways of turning KBRL into a
more practical tool [3, 4, 5]. In this paper we focus on our own approach to leverage KBRL, an
algorithm called kernel-based stochastic factorization (KBSF) [4].

KBSF uses KBRL’s kernel-based strategy to perform a soft aggregation of the states of its MDP.
By doing so, our algorithm is able to summarize the information contained in KBRL’s model in an
MDP whose size is independent of the number of sample transitions. KBSF enjoys good theoretical

1

guarantees and has shown excellent performance on several tasks [4]. The main limitation of the
algorithm is the fact that, in order to construct its model, it uses an amount of memory that grows
linearly with the number of sample transitions. Although this is a signiﬁcant improvement over
KBRL, it still hinders the application of KBSF in scenarios in which a large amount of data must be
processed, such as in complex domains or in on-line reinforcement learning.

In this paper we show that it is possible to construct KBSF’s MDP in a fully incremental way,
thus freeing the space complexity of this algorithm from its dependence on the number of sample
transitions. In order to distinguish it from its original, batch counterpart, we call this new version of
our algorithm incremental KBSF, or iKBSF for short. As will be seen, iKBSF is able to process an
arbitrary number of sample transitions. This results in a model-based RL algorithm that can be used
to solve continuous MDPs in both off-line and on-line regimes.

A second important contribution of this paper is a theoretical analysis showing that it is possible to
control the error in the value-function approximation performed by KBSF. In our previous experi-
ments with KBSF, we deﬁned the model used by this algorithm by clustering the sample transitions
and then using the clusters’s centers as the representative states in the reduced MDP [4]. However,
we did not provide a theoretical justiﬁcation for such a strategy. In this paper we ﬁll this gap by
showing that we can approximate KBRL’s value function at any desired level of accuracy by mini-
mizing the distance from a sampled state to the nearest representative state. Besides its theoretical
interest, the bound is also relevant from a practical point of view, since it can be used in iKBSF to
guide the on-line selection of representative states.

Finally, a third contribution of this paper is an empirical demonstration of the performance of iKBSF
in a new, challenging control problem: the triple pole-balancing task, an extension of the well-known
double pole-balancing domain. Here, iKBSF’s ability to process a large number of transitions is
crucial for achieving a high success rate, which cannot be easily replicated with batch methods.

2 Background

In reinforcement learning, an agent interacts with an environment in order to ﬁnd a policy that
maximizes the discounted sum of rewards [6]. As usual, we assume that such an interaction can be
modeled as a Markov decision process (MDP, [7]). An MDP is a tuple M ≡ (S, A, Pa , ra , γ ), where S
is the state space and A is the (ﬁnite) action set. In this paper we are mostly concerned with MDPs
with continuous state spaces, but our strategy will be to approximate such models as ﬁnite MDPs. In
a ﬁnite MDP the matrix Pa ∈ R|S|×|S| gives the transition probabilities associated with action a ∈ A
and the vector ra ∈ R|S| stores the corresponding expected rewards. The discount factor γ ∈ [0, 1) is
used to give smaller weights to rewards received further in the future.

Consider an MDP M with continuous state space S ⊂ [0, 1]d . Kernel-based reinforcement learning
(KBRL) uses sample transitions to derive a ﬁnite MDP that approximates the continuous model [1,
2]. Let Sa = {(sa
k , ra
k , ˆsa
k )|k = 1, 2, ..., na} be sample transitions associated with action a ∈ A, where
k ∈ R. Let φ : R+ 7→ R+ be a Lipschitz continuous function and let kτ (s, s′ ) be a
sa
k , ˆsa
k ∈ S and ra
kernel function deﬁned as kτ (s, s′ ) = φ (k s − s′ k /τ ), where k · k is a norm in Rd and τ > 0. Finally,
i )/∑na
deﬁne the normalized kernel function associated with action a as κ a
τ (s, sa
i ) = kτ (s, sa
j=1 kτ (s, sa
j ).
The model constructed by KBRL has the following transition and reward functions:
i , if s′ = ˆsa
i ), if s′ = ˆsa
ˆRa (s, s′ ) = (cid:26) ra
ˆPa (s′ |s) = (cid:26) κ a
τ (s, sa
i ,
i ,
0, otherwise.
0, otherwise
Since only transitions ending in the states ˆsa
i have a non-zero probability of occurrence, one can
deﬁne a ﬁnite MDP ˆM composed solely of these n = ∑a na states [2, 3]. After ˆV ∗ , the optimal
ˆM , has been computed, the value of any state-action pair can be determined as:
value function of
Q(s, a) = ∑na
i + γ ˆV ∗ ( ˆsa
i ) (cid:2)ra
τ (s, sa
i=1 κ a
i )(cid:3) , where s ∈ S and a ∈ A. Ormoneit and Sen [1] proved that,
if na → ∞ for all a ∈ A and the widths of the kernels τ shrink at an “admissible” rate, the probability
of choosing a suboptimal action based on Q(s, a) converges to zero.

and

(1)

ˆM , but the time and
Using dynamic programming, one can compute the optimal value function of
space required to do so grow fast with the number of states n [7, 8]. Therefore, the use of KBRL
leads to a dilemma: on the one hand, one wants to use as many transitions as possible to capture the
dynamics of M , but on the other hand one wants to have an MDP ˆM of manageable size.

2

Kernel-based stochastic factorization (KBSF) provides a practical way of weighing these two con-
ˆM
ﬂicting objectives [4]. Our algorithm compresses the information contained in KBRL’s model
in an MDP ¯M whose size is independent of the number of transitions n. The fundamental idea
behind KBSF is the “stochastic-factorization trick”, which we now summarize. Let P ∈ Rn×n be a
transition-probability matrix and let P = DK be a factorization in which D ∈ Rn×m and K ∈ Rm×n are
stochastic matrices. Then, swapping the factors D and K yields another transition matrix ¯P = KD
that retains the basic topology of P—that is, the number of recurrent classes and their respective
reducibilities and periodicities [9]. The insight is that, in some cases, one can work with ¯P instead
of P; when m ≪ n, this replacement affects signiﬁcantly the memory usage and computing time.

KBSF results from the application of the stochastic-factorization trick to ˆM . Let ¯S ≡ { ¯s1 , ¯s2 , ..., ¯sm }
be a set of representative states in S. KBSF computes matrices ˙Da ∈ Rna×m and ˙Ka ∈ Rm×na with el-
ements ˙d a
i , ¯s j ) and ˙ka
j ), where κ ¯τ is deﬁned as κ ¯τ (s, ¯si ) = k ¯τ (s, ¯si )/∑m
i j = κ ¯τ ( ˆsa
τ ( ¯si , sa
i j = κ a
j=1 k ¯τ (s, ¯s j ).
The basic idea of the algorithm is to replace the MDP ˆM with ¯M ≡ ( ¯S, A, ¯Pa , ¯ra , γ ), where ¯Pa = ˙Ka ˙Da
and ¯ra = ˙Ka ra (ra ∈ Rna is the vector composed of sample rewards ra
i ). Thus, instead of solving an
MDP with n states, one solves a model with m states only. Let D⊺ ≡ [(cid:0) ˙D1 (cid:1)⊺ (cid:0) ˙D2 (cid:1)⊺ ... (cid:0) ˙D|A| (cid:1)⊺ ] ∈ Rm×n
and let K ≡ [ ˙K1 ˙K2 ... ˙K|A| ] ∈ Rm×n . Based on ¯Q∗ ∈ Rm×|A| , the optimal action-value function of ¯M ,
ˆM as ˜v = ΓD ¯Q∗ , where Γ is the ‘max’ operator
one can obtain an approximate value function for
applied row wise, that is, ˜vi = maxa (D ¯Q∗ )ia . We have showed that the error in ˜v is bounded by:
ˆPa − DKa (cid:13)(cid:13)∞(cid:19) ,
a (cid:13)(cid:13)
max
where k·k∞ is the inﬁnity norm, ˆv∗ ∈ Rn is the optimal value function of KBRL’s MDP,
ˆC =
i , ¯C = maxa,i ¯ra
maxa,i ˆra
i − mina,i ˆra
i − mina,i ¯ra
i , and Ka is matrix K with all elements equal to zero
except for those corresponding to matrix ˙Ka (see [4] for details).

(1 − γ )2 (cid:18) ¯Cmax
1
i

k ˆra − D ¯ra k∞ +

(1 − max
j

di j ) +

ˆCγ
2

k ˆv∗ − ˜vk∞ ≤

1
1 − γ

max
a

(2)

3

Incremental kernel-based stochastic factorization

In the batch version of KBSF, described in Section 2, the matrices ¯Pa and vectors ¯ra are determined
using all the transitions in the corresponding sets Sa simultaneously. This has two undesirable conse-
quences. First, the construction of the MDP ¯M requires an amount of memory of O(nmaxm), where
nmax = maxa na . Although this is a signiﬁcant improvement over KBRL’s memory usage, which
is O(n2
max ), in more challenging domains even a linear dependence on nmax may be impractical.
Second, with batch KBSF the only way to incorporate new data into the model ¯M is to recompute
the multiplication ¯Pa = ˙Ka ˙Da for all actions a for which there are new sample transitions available.
Even if we ignore the issue of memory usage, this is clearly inefﬁcient in terms of computation. In
this section we present an incremental version of KBSF that circumvents these important limitations.

Suppose we split the set of sample transitions Sa in two subsets S1 and S2 such that S1 ∩ S2 = /0
and S1 ∪ S2 = Sa . Without loss of generality, suppose that the sample transitions are indexed so that
k )|k = n1 + 1, n1 + 2, ..., n1 + n2 = na }. Let ¯PS1
S1 ≡ {(sa
k , ra
k , ˆsa
k )|k = 1, 2, ..., n1} and S2 ≡ {(sa
k , ra
k , ˆsa
and ¯rS1 be matrix ¯Pa and vector ¯ra computed by KBSF using only the n1 transitions in S1 (if n1 = 0,
we deﬁne ¯PS1 = 0 ∈ Rm×m and ¯rS1 = 0 ∈ Rm for all a ∈ A). We want to compute ¯PS1 ∪S2 and ¯rS1 ∪S2
from ¯PS1 , ¯rS1 , and S2 , without using the set of sample transition S1 .
We start with the transition matrices ¯Pa . We know that

¯pS1
i j =

n1
∑
t=1

˙ka
it

˙d a
t j =

n1
∑
t=1

kτ ( ¯si , sa
t )
∑n1
l=1 kτ ( ¯si , sa
l )

k ¯τ ( ˆsa
t , ¯s j )
∑m
l=1 k ¯τ ( ˆsa
t , ¯sl )

=

1
∑n1
l=1 kτ ( ¯si , sa
l )

n1
∑
t=1

kτ ( ¯si , sa
t )k ¯τ ( ˆsa
t , ¯s j )
∑m
l=1 k ¯τ ( ˆsa
t , ¯sl )

.

i = ∑n1+n2
i = ∑n1
To simplify the notation, deﬁne wS1
l ), wS2
l ), and ct
l=n1+1 kτ ( ¯si , sa
l=1 kτ ( ¯si , sa
i j =
kτ ( ¯si ,sa
t )k ¯τ ( ˆsa
t , ¯s j )
, with t ∈ {1, 2, ..., n1 + n2 }. Then,
∑m
l=1 k ¯τ ( ˆsa
t , ¯sl )

¯pS1 ∪S2
i j =

1
i j (cid:17) =
i (cid:16)∑n1
i j + ∑n1+n2
t=n1+1 ct
t=1 ct
wS1
i + wS2

1
i j (cid:17) .
i (cid:16) ¯pS1
i + ∑n1+n2
i j wS1
t=n1+1 ct
wS1
i + wS2

3

¯pS1 ∪S2
i j =

i j = ∑n1+n2
Now, deﬁning bS2
t=n1+1 ct
i j , we have the simple update rule:
1
i (cid:16)bS2
i (cid:17) .
i j wS1
i j + ¯pS1
i + wS2
wS1
We can apply similar reasoning to derive an update rule for the rewards ¯ra
i . We know that
n1
n1
1
∑
∑
∑n1
l=1 kτ ( ¯si , sa
l )
t=1
t=1
Let ht
t )ra
i = kτ ( ¯si , sa
t , with t ∈ {1, 2, ..., n1 + n2}. Then,
1
1
i (cid:16)wS1
i (cid:16)∑n1
i (cid:17) =
i (cid:17) .
i + ∑n1+n2
i + ∑n1+n2
¯rS1 ∪S2
i ¯rS1
t=n1+1 ht
t=1 ht
t=n1+1 ht
i
i + wS2
i + wS2
wS1
wS1
i = ∑n1+n2
Deﬁning eS2
t=n1+1 ht
i , we have the following update rule:

t )ra
kτ ( ¯si , sa
t =

t )ra
kτ ( ¯si , sa
t .

1
wS1
i

¯rS1
i =

=

(3)

=

¯rS1 ∪S2
i

1
i (cid:16)eS2
i (cid:17) .
i + ¯rS1
i wS1
wS1
i + wS2
, and wS2
i j , eS2
Since bS2
i can be computed based on S2 only, we can discard the sample transitions in S1
i
after computing ¯PS1 and ¯rS1 . To do that, we only have to keep the variables wS1
. These variables can
i
be stored in |A| vectors wa ∈ Rm , resulting in a modest memory overhead. Note that we can apply
the ideas above recursively, further splitting the sets S1 and S2 in subsets of smaller size. Thus, we
have a fully incremental way of computing KBSF’s MDP which requires almost no extra memory.

(4)

Algorithm 1 shows a step-by-step description of how to update ¯M based on a set of sample tran-
sitions. Using this method to update its model, KBSF’s space complexity drops from O(nm) to
O(m2 ). Since the amount of memory used by KBSF is now independent of n, it can process an
arbitrary number of sample transitions.

Algorithm 1 Update KBSF’s MDP
¯Pa , ¯ra , wa for all a ∈ A
Sa
for all a ∈ A
Output: Updated ¯M and wa

Input:

for a ∈ A do
for t = 1, ..., na do zt ← ∑m
l=1 k ¯τ ( ˆsa
t , ¯sl )
na ← |Sa |
for i = 1, 2, ..., m do
w′ ← ∑na
t=1 kτ ( ¯si , sa
t )
for j = 1, 2, ..., m do
b ← ∑na
t )k ¯τ ( ˆsa
t=1 kτ ( ¯si , sa
t , ¯s j )/zt
¯pi j ← 1
i +w′ (b + ¯pi j wa
i )
wa
e ← ∑na
t )ra
t=1 kτ ( ¯si , sa
t
¯ri ← 1
i +w′ (e + ¯riwa
i )
wa
i + w′
i ← wa
wa

Algorithm 2 Incremental KBSF (iKBSF)

Input:

¯si Representative states, i = 1, 2, ..., m
tm Interval to update model
tv Interval to update value function
n Total number of sample transitions
Output: Approximate value function ˜Q(s, a)
¯Q ← arbitrary matrix in Rm×|A|
¯Pa ← 0 ∈ Rm×m , ¯ra ← 0 ∈ Rm , wa ← 0 ∈ Rm , ∀a ∈ A
for t = 1, 2, ..., n do
Select a based on ˜Q(st , a) = ∑m
i=1 κ ¯τ (st , ¯si ) ¯qia
Execute a in st and observe rt and ˆst
Sa ← Sa S{(st , rt , ˆst )}
if (t mod tm = 0) then
Add new representative states to ¯M using Sa
Update ¯M and wa using Algorithm 1 and Sa
Sa ← /0 for all a ∈ A
if (t mod tv = 0) update ¯Q

Instead of assuming that S1 and S2 are a partition of a ﬁxed dataset Sa , we can consider that S2 was
generated based on the policy learned by KBSF using the transitions in S1 . Thus, Algorithm 1 pro-
vides a ﬂexible framework for integrating learning and planning within KBSF. A general description
of the incremental version of KBSF is given in Algorithm 2. iKBSF updates the model ¯M and the
value function ¯Q at ﬁxed intervals tm and tv , respectively. When tm = tv = n, we recover the batch
version of KBSF; when tm = tv = 1, we have an on-line method which stores no sample transitions.
¯M .
Note that Algorithm 2 also allows for the inclusion of new representative states to the model
Using Algorithm 1 this is easy to do: given a new representative state ¯sm+1 , it sufﬁces to set wa
m+1 =
0, ¯ra
m+1 = 0, and ¯pm+1, j = ¯p j,m+1 = 0 for j = 1, 2, ..., m + 1 and all a ∈ A. Then, in the following
applications of Eqns (3) and (4), the dynamics of ¯M will naturally reﬂect the existence of state ¯sm+1 .

4

4 Theoretical Results

Our previous experiments with KBSF suggest that, at least empirically, the algorithm’s performance
improves as m → n [4] . In this section we present theoretical results that conﬁrm this property. The
results below are particularly useful for iKBSF because they provide practical guidance towards
where and when to add new representative states.

Suppose we have a ﬁxed set of sample transitions Sa . We will show that, if we are free to deﬁne the
representative states, then we can use KBSF to approximate KBRL’s solution to any desired level of
accuracy. To be more precise, let d∗ ≡ maxa,i min j k ˆsa
i − ¯s j k, that is, d∗ is the maximum distance
i to the closest representative state. We will show that, by minimizing d∗ , we
from a sampled state ˆsa
can make k ˆv∗ − ˜vk∞ as small as desired (cf. Eqn (2)).
∗ − ¯s j k, that is, ˆsa
∗ ≡ ¯sh where h = argmin j k ˆsa
i − ¯s j k and ¯sa
∗ ≡ ˆsa
k with k = argmaxi min j k ˆsa
Let ˆsa
∗
is the sampled state in Sa whose distance to the closest representative state is maximal, and ¯sa
∗ is the
∗ , ¯sa
∗ . Using these deﬁnitions, we can select the pair ( ˆsa
representative state that is closest to ˆsa
∗ ) that
∗ k. Obviously, k ˆs∗ − ¯s∗ k= d∗ .
∗ − ¯sa
∗ where b = argmaxa k ˆsa
∗ and ¯s∗ ≡ ¯sb
∗ k: ˆs∗ ≡ ˆsb
∗ − ¯sa
maximizes k ˆsa
∗ are unique for all a ∈ A, (ii) R ∞
∗ and ¯sa
We make the following simple assumptions: (i) ˆsa
0 φ (x)d x ≤
Lφ < ∞, (iii) φ (x) ≥ φ (y) if x < y, (iv) ∃ Aφ , λφ > 0, ∃ Bφ ≥ 0 such that Aφ exp(−x) ≤ φ (x) ≤
λφ Aφ exp(−x) if x ≥ Bφ . Assumption (iv) implies that the kernel function φ will eventually decay
exponentially. We start by introducing the following deﬁnition:
Deﬁnition 1. Given α ∈ (0, 1] and s, s′ ∈ S, the α -radius of kτ with respect to s and s′ is deﬁned as
ρ (kτ , s, s′ , α ) = max{x ∈ R+ |φ (x/τ ) = α kτ (s, s′ )}.

The existence of ρ (kτ , s, s′ , α ) is guaranteed by assumptions (ii) and (iii) and the fact that φ is
continuous [1]. To provide some intuition on the meaning of the α -radius of kτ , suppose that φ is
strictly decreasing and let c = φ (k s − s′ k /τ ). Then, there is a s′′ ∈ S such that φ (k s − s′′ k /τ ) = α c.
The radius of kτ in this case is k s − s′′ k. It should be thus obvious that ρ (kτ , s, s′ , α ) ≥k s − s′ k.
We can show that ρ has the following properties (proved in the supplementary material):
Property 1. If k s − s′ k<k s − s′′ k, then ρ (kτ , s, s′ , α ) ≤ ρ (kτ , s, s′′ , α ).
Property 2. If α < α ′ , then ρ (kτ , s, s′ , α ) > ρ (kτ , s, s′ , α ′ ).
Property 3. For α ∈ (0, 1) and ε > 0, there is a δ > 0 such that ρ (kτ , s, s′ , α )− k s − s′ k< ε if τ < δ .

We now introduce a notion of dissimilarity between two states s, s′ ∈ S which is induced by a speciﬁc
set of sample transitions Sa and the choice of kernel function:
Deﬁnition 2. Given β > 0, theβ -dissimilarity between s and s′ with respect to κ a
τ is deﬁned as
τ , s, s′ , β ) = (cid:26) ∑na
τ (s′ , sa
k )|, if k s − s′ k≤ β ,
τ (s, sa
k ) − κ a
k=1 |κ a
ψ (κ a
0, otherwise.

The parameter β deﬁnes the volume of the ball within which we want to compare states. As we will
τ , s, s′ , β ) ∈ [0, 2]. It is possible to show
see, this parameter links Deﬁnitions 1 and 2. Note that ψ (κ a
that ψ satisﬁes the following property (see supplementary material):
τ , s, s′ , β ) < ε if k s − s′ k< δ .
Property 4. For β > 0 and ε > 0, there is a δ > 0 such that ψ (κ a

Deﬁnitions 1 and 2 allow us to enunciate the following result:
∗ , ¯sa
let ρ a = ρ (k ¯τ , ˆsa
∗ , α /t ),
Lemma 1. For any α ∈ (0, 1] and any t ≥ m − 1,
τ , ˆsa
τ , ˆsa
ψ (κ a
i , ¯s j , ρ a ), and let ψ a
ψ (κ a
i , ¯s j , ∞). Then,
max = max
max
i, j
i, j

let ψ a
ρ =

kPa − DKak∞ ≤

1
1 + α

ψ a
ρ +

α
1 + α

ψ a
max .

(5)

Proof. See supplementary material.
Since ψ a
max ≥ ψ a
ρ , one might think at ﬁrst that the right-hand side of Eqn (5) decreases monotonically
as α → 0. This is not necessarily true, though, because ψ a
ρ → ψ a
max as α → 0 (see Property 2). We
are ﬁnally ready to prove the main result of this section.

5

Proposition 1. For any ε > 0, there are δ1 , δ2 > 0 such that k ˆv∗ − ˜vk∞ < ε if d∗ < δ1 and ¯τ < δ2 .
Proof. Let ˇr ≡ [(r1 )⊺ , (r2 )⊺ , ..., (r|A| )⊺ ]⊺ ∈ Rn . From Eqn (1) and the deﬁnition of ¯ra , we can write
ˆPa ˇr − DKa ˇr(cid:13)(cid:13)∞ = (cid:13)(cid:13)( ˆPa − DKa ) ˇr(cid:13)(cid:13)∞ ≤ (cid:13)(cid:13)
ˆPa − DKa (cid:13)(cid:13)∞ k ˇrk∞ .
k ˆra − D ¯ra k∞ = (cid:13)(cid:13)
ˆPa ˇr − D ˙Ka ra (cid:13)(cid:13)∞ = (cid:13)(cid:13)
Thus, plugging Eqn (6) back into Eqn (2), it is clear that there is a η > 0 such that k ˆv∗ − ˜vk∞ < ε
ˆPa − DKa(cid:13)(cid:13)∞ < η and maxi (1 − max j di j ) < η . We start by showing that if d∗ and ¯τ are
if maxa (cid:13)(cid:13)
ˆPa − DKa(cid:13)(cid:13)∞ < η . From Lemma 1 we know that, for any set of m ≤ n
small enough, then maxa (cid:13)(cid:13)
representative states, and for any α ∈ (0, 1], the following must hold:
kPa − DKak∞ ≤ (1 + α )−1ψρ + α (1 + α )−1ψMAX ,
max
a

(6)

τ , ˆsa
where ψMAX = maxa,i,s ψ (kτ , ˆsa
ρ = maxa,i, j ψ (κ a
i , s, ∞) and ψρ = maxa ψ a
i , ¯s j , ρ a ), with ρ a =
∗ , ¯sa
ρ (k ¯τ , ˆsa
∗ , α /(n − 1)). Note that ψMAX is independent of the representative states. Deﬁne α such
that α /(1 + α )ψMAX < η . We have to show that, if we deﬁne the representative states in such a way
that d∗ is small enough, and set ¯τ accordingly, then we can make ψρ < (1 − α )η − α ψMAX ≡ η ′ .
From Property 4 we know that there is a δ1 > 0 such that ψρ < η ′ if ρ a < δ1 for all a ∈ A. From
Property 1 we know that ρ a ≤ ρ (k ¯τ , ˆs∗ , ¯s∗ , α /(n − 1)) for all a ∈ A. From Property 3 we know that,
for any ε ′ > 0, there is a δ ′ > 0 such that ρ (k ¯τ , ˆs∗ , ¯s∗ , α /(n − 1)) < d∗ + ε ′ if ¯τ < δ ′ . Therefore, if
d∗ < δ1 , we can take any ε ′ < δ1 − d∗ to have an upper bound δ ′ for ¯τ . It remains to show that there
is a δ > 0 such that mini max j di j > 1 − η if ¯τ < δ . Recalling that ˙d a
i , ¯s j )/∑m
i j = k ¯τ ( ˆsa
k=1 k ¯τ ( ˆsa
i , ¯sk ),
i , ¯s j ), and let ya
i , ¯sh ) and ˇya
let h = argmax j k ¯τ ( ˆsa
i = k ¯τ ( ˆsa
i = max j 6=h k ¯τ ( ˆsa
i , ¯s j ). Then, for any i,
max j ˙d a
i / (cid:0)ya
i j = ya
i + (m − 1) ˇya
i /(ya
i , ¯s j )(cid:1) ≥ ya
i + ∑ j 6=h k ¯τ ( ˆsa
i ). From Assump. (i) and Prop. 3 we know
i > (m − 1)(1 − η ) ˇya
i > 0 such that ya
that there is a δ a
i /η if ¯τ < δ a
i . Thus, by making δ = mina,i δ a
i ,
we can guarantee that mini max j di j > 1 − η . If we take δ2 = min(δ , δ ′ ), the result follows.

Proposition 1 tells us that, regardless of the speciﬁc reinforcement-learning problem at hand, if the
distances between sampled states and the respective nearest representative states are small enough,
then we can make KBSF’s approximation of KBRL’s value function as accurate as desired by setting
¯τ to a small value. How small d∗ and ¯τ should be depends on the particular choice of kernel kτ and on
the characteristics of the sets of transitions Sa . Of course, a ﬁxed number m of representative states
imposes a minimum possible value for d∗ , and if this value is not small enough decreasing ¯τ may
actually hurt the approximation. Again, the optimal value for ¯τ in this case is problem-dependent.

Our result supports the use of a local approximation based on representative states spread over
the state space S. This is in line with the quantization strategies used in batch-mode kernel-based
reinforcement learning to deﬁne the states ¯s j [4, 5]. In the case of on-line learning, we have to
adaptively deﬁne the representative states ¯s j as the sample transitions come in. One can think of
several ways of doing so [10]. In the next section we show a simple strategy for adding representative
states which is based on the theoretical results presented in this section.

5 Empirical Results

We now investigate the empirical performance of the incremental version of KBSF. We start with a
simple task in which iKBSF is contrasted with batch KBSF. Next we exploit the scalability of iKBSF
to solve a difﬁcult control task that, to the best of our knowledge, has never been solved before.

We use the “puddle world” problem as a proof of concept [11]. In this ﬁrst experiment we show
that iKBSF is able to recover the model that would be computed by its batch counterpart. In order
to do so, we applied Algorithm 2 to the puddle-world task using a random policy to select actions.
Figure 1a shows the result of such an experiment when we vary the parameters tm and tv . Note
that the case in which tm = tv = 8000 corresponds to the batch version of KBSF. As expected, the
performance of KBSF decision policies improves gradually as the algorithm goes through more
sample transitions, and in general the intensity of the improvement is proportional to the amount of
data processed. More important, the performance of the decision policies after all sample transitions
have been processed is essentially the same for all values of tm and tv , which shows that iKBSF
can be used as a tool to circumvent KBSF’s memory demand (which is linear in n). Thus, if one
has a batch of sample transitions that does not ﬁt in the available memory, it is possible to split
the data in chunks of smaller sizes and still get the same value-function approximation that would

6

be computed if the entire data set were processed at once. As shown in Figure 1b, there is only a
small computational overhead associated with such a strategy (this results from unnormalizing and
normalizing the elements of ¯Pa and ¯ra several times through update rules (3) and (4)).

ι = 1000
ι = 2000
ι = 4000
ι = 8000

n
r
u
t
e
R

3

2

1

0

1
−

2
−

3
−

ι = 1000
ι = 2000
ι = 4000
ι = 8000

s
d
n
o
c
e
S

5
.
1

0
.
1

5
.
0

0
.
0

0

2000

4000

6000

8000

0

2000

4000

6000

8000

Number of sample transitions

Number of sample transitions

(a) Performance

(b) Run times

Figure 1: Results on the puddle-world task averaged over 50 runs. iKBSF used 100 representative
states evenly distributed over the state space and tm = tv = ι (see legends). Sample transitions were
collected by a random policy. The agents were tested on two sets of states surrounding the “puddles”:
a 3 × 3 grid over [0.1, 0.3] × [0.3, 0.5] and the four states {0.1, 0.3} × {0.9, 1.0}.

But iKBSF is more than just a tool for avoiding the memory limitations associated with batch learn-
ing. We illustrate this fact with a more challenging RL task. Pole balancing has a long history
as a benchmark problem because it represents a rich class of unstable systems [12, 13, 14]. The
objective in this task is to apply forces to a wheeled cart moving along a limited track in order to
keep one or more poles hinged to the cart from falling over [15]. There are several variations of
the problem with different levels of difﬁculty; among them, balancing two poles at the same time
is particularly hard [16]. In this paper we raise the bar, and add a third pole to the pole-balancing
task. We performed our simulations using the parameters usually adopted with the double pole task,
except that we added a third pole with the same length and mass as the longer pole [15]. This results
in a problem with an 8-dimensional state space S.

In our experiments with the double-pole task, we used 200 representative states and 106 sample
transitions collected by a random policy [4]. Here we start our experiment with triple pole-balancing
¯M by incorporating
using exactly the same conﬁguration, and then we let KBSF reﬁne its model
more sample transitions through update rules (3) and (4). Speciﬁcally, we used Algorithm 2 with a
0.3-greedy policy, tm = tv = 106 , and n = 107 . Policy iteration was used to compute ¯Q∗ at each value-
function update. As for the kernels, we adopted Gaussian functions with widths τ = 100 and ¯τ = 1
(to improve efﬁciency, we used a KD-tree to only compute the 50 largest values of kτ ( ¯si , ·) and the
10 largest values of k ¯τ ( ˆsa
i , ·)). Representative states were added to the model on-line every time the
agent encountered a sample state ˆsa
i for which k ¯τ ( ˆsa
i , ¯s j ) < 0.01 for all j ∈ 1, 2, ..., m (this corresponds
to setting the maximum allowed distance d∗ from a sampled state to the closest representative state).

We compare iKBSF with ﬁtted Q-iteration using an ensemble of 30 trees generated by Ernst et al.’s
extra-trees algorithm [17]. We chose this algorithm because it has shown excellent performance in
both benchmark and real-world reinforcement-learning tasks [17, 18].1 Since this is a batch-mode
learning method, we used its result on the initial set of 106 sample transitions as a baseline for our
empirical evaluation. To build the trees, the number of cut-directions evaluated at each node was
ﬁxed at dim(S) = 8, and the minimum number of elements required to split a node, denoted here
by ηmin , was ﬁrst set to 1000 and then to 100. The algorithm was run for 50 iterations, with the
structure of the trees ﬁxed after the 10th iteration.

As shown in Figure 2a, both ﬁtted Q-iteration and batch KBSF perform poorly in the triple pole-
balancing task, with average success rates below 55%. This suggests that the amount of data used

1Another reason for choosing ﬁtted Q-iteration was that some of the most natural competitors of iKBSF
have already been tested on the simpler double pole-balancing task, with disappointing results [19, 4].

7

by these algorithms is insufﬁcient to describe the dynamics of the control task. Of course, we could
give more sample transitions to ﬁtted Q-iteration and batch KBSF. Note however that, since they
are batch-learning methods, there is an inherent limit on the amount of data that these algorithms
can use to construct their approximation. In contrast, the amount of memory required by iKBSF
is independent of the number of sample transitions n. This fact together with the fact that KBSF’s
computational complexity is only linear in n allow our algorithm to process a large amount of data
within a reasonable time. This can be observed in Figure 2b, which shows that iKBSF can build
an approximation using 107 transitions in under 20 minutes. As a reference for comparison, ﬁtted
Q-iteration using ηmin = 1000 took an average of 1 hour and 18 minutes to process 10 times less data.

s
e
d
o
s
i
p
e
 
l
u
f
s
s
e
c
c
u
S

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

3
.
0

●

●

●

●

●

●

iKBSF
TREE−1000
TREE−100

●

●

●

●

●

0
0
0
0
5

0
0
0
0
1

0
0
0
2

0
0
5

0
0
2

)
g
o
l
(
 
s
d
n
o
c
e
S

●

●

●

Batch KBSF

0
5

●

Batch KBSF

●

●

●

●

●

●

●

iKBSF
TREE−1000
TREE−100

●

●

●

●

●

●

●

●

0
0
0
4

0
0
0
3

0
0
0
2

0
0
0
1

s
e
t
a
t
s
 
e
v
i
t
a
t
n
e
s
e
r
p
e
r
 
f
o
 
r
e
b
m
u
N

●

●

2e+06

4e+06

6e+06

8e+06

1e+07

2e+06

4e+06

6e+06

8e+06

1e+07

2e+06

6e+06

1e+07

Number of sample transitions

Number of sample transitions

Number of sample transitions

(a) Performance

(b) Run times

(c) Size of KBSF’s MDP

Figure 2: Results on the triple pole-balancing task averaged over 50 runs. The values correspond to
the fraction of episodes initiated from the test states in which the 3 poles could be balanced for 3000
steps (one minute of simulated time). The test set was composed of 256 states equally distributed
over the hypercube deﬁned by ±[1.2 m, 0.24 m/s, 18o , 75o /s, 18o , 150o /s, 18o , 75o/s]. Shadowed re-
gions represent 99% conﬁdence intervals.

As shown in Figure 2a, the ability of iKBSF to process a large number of sample transitions allows
our algorithm to achieve a success rate of approximately 80%. This is similar to the performance
of batch KBSF on the double-pole version of the problem [4]. The good performance of iKBSF on
the triple pole-balancing task is especially impressive when we recall that the decision policies were
evaluated on a set of test states representing all possible directions of inclination of the three poles.
In order to achieve the same level of performance with KBSF, approximately 2 Gb of memory would
be necessary, even using sparse kernels, whereas iKBSF used less than 0.03 Gb of memory.

To conclude, observe in Figure 2c how the number of representative states m grows as a function of
the number of sample transitions processed by KBSF. As expected, in the beginning of the learning
process m grows fast, reﬂecting the fact that some relevant regions of the state space have not been
visited yet. As more and more data come in, the number of representative states starts to stabilize.

6 Conclusion

This paper presented two contributions, one practical and one theoretical. The practical contribution
is iKBSF, the incremental version of KBSF. iKBSF retains all the nice properties of its precursor:
it is simple, fast, and enjoys good theoretical guarantees. However, since its memory complexity
is independent of the number of sample transitions, iKBSF can be applied to datasets of any size,
and it can also be used on-line. To show how iKBSF’s ability to process large amounts of data can
be useful in practice, we used the proposed algorithm to learn how to simultaneously balance three
poles, a difﬁcult control task that had never been solved before.

As for the theoretical contribution, we showed that KBSF can approximate KBRL’s value function
at any level of accuracy by minimizing the distance between sampled states and the closest repre-
sentative state. This supports the quantization strategies usually adopted in kernel-based RL, and
also offers guidance towards where and when to add new representative states in on-line learning.

Acknowledgments The authors would like to thank Amir massoud Farahmand for helpful discussions re-
garding this work. Funding for this research was provided by the National Institutes of Health (grant R21
DA019800) and the NSERC Discovery Grant program.

8

References

[1] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning, 49 (2–3):
161–178, 2002.

[2] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems.
IEEE Transactions on Automatic Control, 47(10):1624–1636, 2002.

[3] N. Jong and P. Stone. Kernel-based models for reinforcement learning in continuous state
spaces.
In Proceedings of the International Conference on Machine Learning (ICML)—
Workshop on Kernel Machines and Reinforcement Learning, 2006.

[4] A. M. S. Barreto, D. Precup, and J. Pineau. Reinforcement learning using kernel-based stochas-
tic factorization. In Advances in Neural Information Processing Systems (NIPS), pages 720–
728, 2011.

[5] B. Kveton and G. Theocharous. Kernel-based reinforcement learning on representative states.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 124–131, 2012.

[6] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.

[7] M. L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., 1994.

[8] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision
problems. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pages 394–402, 1995.

[9] A. M. S. Barreto and M. D. Fragoso. Computing the stationary distribution of a ﬁnite Markov
chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applications, 32:
1513–1523, 2011.

[10] Y. Engel, S. Mannor, and R. Meir. The kernel recursive least squares algorithm. IEEE Trans-
actions on Signal Processing, 52:2275–2285, 2003.

[11] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse
coarse coding. In Advances in Neural Information Processing Systems (NIPS), pages 1038–
1044, 1996.

[12] D. Michie and R. Chambers. BOXES: An experiment in adaptive control. Machine Intelligence
2, pages 125–133, 1968.

[13] C. W. Anderson. Learning and Problem Solving with Multilayer Connectionist Systems. PhD
thesis, Computer and Information Science, University of Massachusetts, 1986.

[14] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve
difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13:
834–846, 1983.

[15] F. J. Gomez. Robust non-linear control through neuroevolution. PhD thesis, The University of
Texas at Austin, 2003.

[16] A. P. Wieland. Evolving neural network controllers for unstable systems. In Proceedings of
the International Joint Conference on Neural Networks (IJCNN), pages 667–673, 1991.

[17] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research, 6:503–556, 2005.

[18] D. Ernst, G. B. Stan, J. Gonc¸ alves, and L. Wehenkel. Clinical data based optimal STI strate-
gies for HIV: a reinforcement learning approach. In Proceedings of the IEEE Conference on
Decision and Control (CDC), pages 124–131, 2006.

[19] F. Gomez, J. Schmidhuber, and R. Miikkulainen. Efﬁcient non-linear control through neu-
roevolution. In Proceedings of the European Conference on Machine Learning (ECML), pages
654–662, 2006.

9


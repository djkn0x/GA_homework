Noname manuscript No.
(will be inserted by the editor)

Side Channel Cryptanalysis Using Machine Learning
Using an SVM to recover DES keys from a smart card.

Hera He · Josh Jaﬀe · Long Zou

December 14, 2012

Abstract Cryptographic devices use power as they op-
erate. Variations in the amount of power consumed dur-
ing an operation may leak information about data val-
ues being processed. Methods such as diﬀerential power
analysis (DPA) [1] have been used to recover keys by
exploiting such leaks. But DPA is tougher to apply to
leakage that depends only on the key. For this pro ject
we attacked leakage from an 8-bit smart card perform-
ing DES1 decryptions with a 56-bit key. We explored
using an SVM to exploit the side channel leakage.
Using an SVM classiﬁer we reached near 100% clas-
siﬁcation accuracy using features drawn from measure-
ments during the DES key schedule, after training with
at least 1000 samples out of our set of 11000. This is
the ﬁrst publication in which an SVM has been used to
recover an entire key.

1 Introduction

Many devices today use cryptography to perform secu-
rity tasks. For example, most automobiles can be un-
locked using a wireless key fob. Parking meters in San
Francisco accept payment from smart cards. SIM cards
in mobile phones authenticate the phone to the base
tower. The security of such systems requires ‘tamper re-
sistance’ to prevent attackers from simply opening the
device, reading all the secrets from its memory, and
then using them to create clones.
But tampering is not the only threat. Physical cir-
cuits use power and generate electromagnetic emissions
as they operate. Power consumption is a property that
an attacker can measure—for example, by connecting

yhe1@stanford.edu,
pz73@stanford.edu

joshj@stanford.edu,

1 Much of our analysis also applies to 3DES.

an oscilloscope to a target device’s ground pin and mon-
itoring the voltage as it rises or falls as the amount of
current through the chip varies with time. These varia-
tions carry information beyond what is communicated
by the ciphertext. So the power measurements consti-
tute a side channel—and this has traditionally been
exploited with attacks such as simple power analysis
(SPA) [1] and DPA. While DPA is often applied as a
‘black box’ attack method, template attacks (TA) [2]
are a related approach that use a characterization stage
requiring known keys.
Related work. It has recently been noted [4] [5]
that the TA model is amenable to supervised learning.
One paper [4] used a Least Squares SVM to extract
information from a byte operation during AES decryp-
tion. Although not oriented towards key-recovery, the
authors built a one byte classiﬁer that achieved up to
99.3% accuracy for certain parameter choices. Another
paper [5] looked at 3DES in an FPGA, and found that
an SVM could reliably recover a few bits of the key—
but would require a 70 bit search to complete the attack
on a 112 bit key. Both papers found feature selection to
be critically important.
In this pro ject were interested to see whether super-
vised learning could automate the process of decoding
SPA leaks and key dependent statistical leaks. We also
revisited the question of feature selectionand found, in
contrast to previous results, that aggressive feature se-
lection was not necessarily helpful.
The rest of this paper is structured as follows. In
section 2 we give an overview of the target device and
data collection process, and describe the features that
were used for supervised learning. In sections 3 and 4
we discuss the problem formulation and how we arrived
at the ﬁnal SVM model. In section 5 we present the
results of our model. We conclude with some remarks

2

Hera He et al.

Fig. 1: Start of DES decryption. Each point represents 6 clock cycles. PC1, IP, PC2, and XOR regions indicated.

about the signiﬁcance of these results, lessons learned,
and future steps that could make this publishable in a
top tier side-channel-crypto conference.

2 Data Set Overview: the DES smart card

This pro ject focused on extracting a key from an 8-
bit Atmel smart card performing decryptions using the
Data Encryption Standard (DES)[3]. DES is a block
cipher that uses a 56-bit key and has a 64-bit block
size. For this pro ject, it is relevant to know that DES
performs bit permutations on the input message (“IP”)
and key (“PC1”), and then applies a round function 16
times in a Feistel structure. The round function per-
forms a second key permutation (“PC2”), then com-
putes the exclusive or of a 48-bit round key with some
bits of the round input (“XOR”). A 6-bit to 4-bit sub-
stitution (“SBOX”) is performed on the XOR output.
The DES key schedule includes PC1 and PC2. This
pro ject focused on data-dependent variations in power
measurements during clock cycles in which the PC1,
PC2, and XOR operations were performed.
Power measurement traces were collected using:
• A ZeitControl BASIC card version 3.9 (programmed
for DES decryption with chosen key)
• A DPA Workstation smart card reader with 5Ω re-
sistor in series with ground provided to smart card
• A Tektronix DPO7104 oscilloscope connected to mea-
sure the voltage drop across the ground resistor

Figure 1 shows a trace of power measurements over
one DES decryption.
The smart card ran at 4MHz. Power measurements
were recorded at 200MHz for 50 samples per clock cycle—
or over one million power measurements per trace. We
stored a subset of each trace covering the DES initializa-
tion and ﬁrst four decryption rounds. The data collec-
tion spanned 4000 unique keys and 16000 ciphertexts2 .

2 We primarily worked with the ﬁrst 11000.

Each key and ciphertext combination was repeated ﬁve
times. The traces were then aligned and each clock cy-
cle was synchronized with a reference (to remove timing
jitter). The data were further reduced by averaging to-
gether all traces that correspond to a single key and
ciphertext combination, then selecting the 12th sample
of each clock cycle as representative for that cycle.3

3 Problem Formulation

3.1 Problem Description

We collected N = 11000 instances and held out N1 =
1000 of them to be test data, leaving N0 = 10000 to
be training examples. Each training example consisted
of Y , C, X1 , X2 , X3 , where the key Y is a binary vec-
tor of length pk = 56, input ciphertext C is a binary
vector of length pc = 64, X1 , X2 , X3 are power trace
measurements in PC1 region, PC2 region and XOR re-
gion, vectors with length p1 = 3039, p2 = 904, p3 = 516
respectively. We used SPA and DPA to interpret the
portions of the power traces as labeled in Figure 1 and
determine exactly where the PC1, PC2, and XOR op-
erations occur. We hope to recover keys Y in the test
set.

We expect leaks during the PC1 and PC2 stages
to depend directly on bits of the key, while the power
measurements in the XOR stage may depend on bits
of input ciphertext as well as bits of key. The analysis
focused on these regions separately, i.e. we seek to ﬁnd
three separate multi-label classiﬁers Y = G1 (X1 ), Y =
G2 (X2 ) and Y = G3 (C, X3 ).

3 We also performed a comparison between this ’12th cycle’
approach and that of using the average of all 50 measurements
over a cycle as representing that cycle. Diﬀerences were minor.

Side Channel Cryptanalysis Using Machine Learning

3

3.2 Problem Transformation: From Multi-Label
Classiﬁcation To Binary Classiﬁcation

We are looking for a classiﬁer that assigns each instance
to a set of d = 56 labels with binary classes. Two main
methods have been proposed for tackling such problems
[6]: the binary relevance (BR) approach and the la-
bel power set (LP) approach. In the BR approach,
the multi-label problem is transformed into d separate
binary classiﬁcation problems. This breakdown of the
classes may overlook possible interactions among the la-
bels, however. The label power set method transforms
the multi-binary-label problem into single-compound-
label problem where the single label has 2d possible
classes. Here the interactions are encoded to the classi-
ﬁcation implicitly, but the exponential increase of the
number of classes makes the computation intractable
for large d.
The DES block cipher tends to manipulate the key
primarily in 1, and 6 bit groupings. We decided to use
BR ﬁrst, ﬁguring that operations that involve only one
bit of the key are likely to be have minimal interaction
and may be suﬃcient to recover the key. The problem
was therefore transformed into ﬁnding 56 separate bi-
nary label classiﬁers for each of the three regions.4

achieving near 100% prediction accuracy in PC1 and
PC2 regions, and above 90% accuracy in XOR region.
In order to get an eﬀective SVM classiﬁer it is essen-
tial to choose an eﬀective kernel function, ﬁnd a good
set of kernel parameters, optimize the soft margin C ,
and identify an informative set of features. In this sec-
tion, we will discuss our choice of these variables and
how we arrived at our ﬁnal SVM model.
Our ﬁrst step was to ﬁt a linear kernel SVM with
L2 regularization to all three regions. We quickly ﬁg-
ured out that standardizing the features before feeding
into the SVM classiﬁer both reduced the model train-
ing time and improved the prediction accuracy. Stan-
dardizing is especially important in the XOR region to
normalize the binary valued ciphertexts and real valued
power trace measurements to the same scale.
This model is very successful for PC1 and PC2 re-
gions. We reached 100% prediction accuracy with about
1000 training samples (see Figure 2). However, SVM
with linear kernel is not as successful in the XOR re-
gion. As we increase the sample size towards 10000,
the XOR training accuracy and test accuracy converge
to around 65% (see Figure 2). This indicates an inad-
equacy of the linear kernel for XOR region with just
standardized feature matrix [C X3 ].

3.3 Choice of evaluation metric

4.2 Kernels and parameters

Ideally we want to ﬁnd a model that is accurate for most
target bits. Preliminary trials with the SVM showed
similar performance trend on all key bits, suggesting
that performance for any one bit was an acceptable
approximation for performance over all 56. Hence to
reduce the training time, during the model selection
stage we evaluated models based only on the accuracy
for the ﬁrst bit.

4 Evolution of our SVM model

4.1 Overview of SVM results

Once the problem has been broken into ﬁnding a binary
classiﬁer for each bit of the key, we could use standard
classiﬁcation methods like SVM. With SVM, we were
able to recover most of the key bits with high accuracy,

The failure of the linear kernel in XOR indicates pos-
sible nonlinear relationship between the labels and the
input features. Hence we experimented with diﬀerent
kernels for SVM, including polynomial kernels with dif-
ferent degrees, Gaussian kernel and sigmoid kernel. Ta-
ble 1 shows the hold-out cross validation accuracy for
diﬀerent kernel. Polynomial kernel with degree 2 has
slightly better performance than the linear kernel, while
other kernels made it worse—little better than random
guessing. We also tried optimizing the kernel parame-
ters and the soft margin parameter C by grid searching
and cross validation, but the search in a wide range
still didn’t save us from the poor performance. Hence
we conclude the inappropriateness of all kernels other
than the polynomial kernel of degree 2.

4.3 Feature engineering for XOR region

4 Note that DES is unusual in manipulating individual bits.
With other ciphers, bit groupings of 8 or more bits may be
more common. When operations involve several bits of a key
simultaneously, more complex models should probably be ap-
plied to capture the interactions among diﬀerent classes. An
intermediate form of LP that focuses on k bit groupings limits
the exponential increase in classes to 2k .

Our next goal was to push prediction accuracy in the
XOR region to higher than 69.5%. Polynomial kernel
with 2 degrees expands the dimension of the feature
space from p3 + pc = 636 to (p3 + pc )2 > 4 × 105 , where
features are the pairwise product of the original inputs.
With only 10000 training samples, overﬁtting is clearly

4

Hera He et al.

Fig. 2: Training and Test accuracy of SVM with linear kernel for PC1, PC2, and XOR regions respectively.
Features for PC1 and PC2 only contains power traces X1 and X2 respectively. For XOR regions features contain
both ciphertext C and power traces X3

Kernel
Polynomial d = 2
Polynomial d = 3
Polynomial d = 4
Gaussian
Sigmoid

NVEC TRAIN ACC TEST ACC
69.5%
100%
9014
59.2%
100%
9966
57.8%
100%
9995
10000
100%
52%
50.2%
100%
6247

Table 1: Comparisons of SVM with diﬀerent kernels.
NVEC = # of support vectors. N0 = 10000, N1 = 1000.

a problem: we have training accuracy of 100%, and 90%
of our inputs became support vectors. The high dimen-
sion of the new feature space makes the data set linearly
separable—but clearly contains a lot of noise that is be-
ing ﬁt by the model. We should not criticize this feature
mapping too much, because 69.5% is signiﬁcantly bet-
ter than random guessing. Some of new features are
indeed informative. But we may be able to obtain a
better classiﬁer by retaining the informative features
while removing some of the others. We accomplished
this with some judicious feature engineering.5
The engineered features are obtained in the follow-
ing way. Beginning with the X3 power measurements
standardized to have zero mean and unit variance, and
using ciphertext bits C0..63 , we computed: φ(C, X3 ) =
φ(cid:48) (C0 , X3 ) || φ(cid:48) (C1 , X3 ) || ... || φ(cid:48) (C63 , X3 ). The function
φ(cid:48) is deﬁned by φ(cid:48) (1, X ) = X and φ(cid:48) (0, X ) = −1 · X .
The result has 64 · 572 > 36000 elements, and cor-
responds to all terms in the degree 2 polynomial that
are a product of one bit of ciphertext and one power
measurement. Training with this feature vector, how-
ever, was extremely slow due to the large number of
features—and prediction accuracy remained low!

5 We could have used standard feature selection techniques,
based on forward search or looking at mutual information.
Instead, we chose a feature engineering strategy based on ra-
tionale that we will lay out towards the end of this section.

(1)

So we next worked separately with each engineered
feature matrix φ(cid:48) (Ci , X3 ). Training each bit of the key
with all 64 engineered feature matrices reveals that for
each key bit only one engineered feature matrix is in-
formative. This fact is expected, given the structure of
the DES algorithm.6
This success of this engineered feature matrix can
also be justiﬁed in the following way. After the XOR
operation Ih = Ci ⊕ Kj the power features may leak
information about Ih . If the power consumption is lin-
early proportional to Ih , this could be expressed as:
Pt = a · (Kj ⊕ Ci ) + b
for some power measurement (Pt ) at time t. Using the
following fact7 for binary Kj and Ci :
Kj ⊕ Ci =
(1 − (2Kj − 1) · (2Ci − 1))
1
2
(cid:16)(cid:16)
(cid:17) · (2Ci − 1) − a
(cid:17)
and solving (1) for Kj reveals that:
Pt − a
− b
Kj = − 1
.
a
2
2
2 + b with (cid:104)Pt (cid:105) gives
Substituting the unknown a
Kj ∝ (Pt − (cid:104)Pt (cid:105)) · (2Ci − 1) + .
But calculating the product of the centered power
trace (Pt−(cid:104)Pt (cid:105)) and (2Ci−1) is exactly the same as ﬂip-
ping the sign of the standardized power trace according
to the value of Ci—which is how we obtained our en-
gineered feature matrix. So if the power measurements
X3 can be linearly separated by an SVM according to
the value of a bit Ih , then the feature matrix engineered
with bit Ci can be linearly separated by an SVM to re-
veal Kj = Ci ⊕ Ih .
6 We also expect only 32 of the engineered feature matrices
to be informative, corresponding to the bits of ’R’ in round
1. And we expect only a 48-bit round key (not the full 56-bit
key) to be be used in XOR operations.
7 For explanation, see the extended version of this paper

(2)

0500100015002000556065707580859095100105Number of training samplesAccuracy (%)First key bit, PC1 Region  Training (standardized)Test (standardized)Training (unstandardized)Test (unstandardized)0500100015002000556065707580859095100105Number of training samplesAccuracy (%)First key bit, PC2 Region  Training (standardized)Test (standardized)Training (unstandardized)Test (unstandardized)0200040006000800010000556065707580859095100105Number of training samplesAccuracy (%)First key bit, XOR Region  Training (standardized)Test (standardized)Training (unstandardized)Test (unstandardized)Side Channel Cryptanalysis Using Machine Learning

5

Fig. 3: Training and Prediction Accuracy for the ﬁrst
key bit based on 64 engineered feature matrices, the
numbers being displayed in the plot correspond to the
index of engineered feature matrix, being x in this case.

Fig. 4: Training and Prediction Accuracy for all key
bits based on the most informative engineered feature
matrix. The 48 key bits that are part of the decryp-
tion round key are all predicted with greater than 80%
accuracy.

5 Results With Engineered Features In XOR
Region

We present the result of training for the 1st key bit with
our engineered feature matrix in Figure 3. All models
were trained with 10000 samples and tested on 1000
samples. As shown in the plot, for the 1st key bit, the
27th ciphertext is leaking information, giving a predic-
tion accuracy over 90%, while results trained from en-
gineered feature matrices based on other bits of cipher-
texts are basically random. This analysis was repeated
for all bits of the key. The leakiest bit of ciphertext cor-
responding to each key was identiﬁed, and the results
are reported in Figure 4. Note that in the XOR region,
8 bits of the key are not used. This is in accordance
with our result of 8 unidentiﬁably key bits with test
accuracy below 55%.

6 Conclusion

Initial power analysis of the DES card shows high am-
plitude power leaks during the PC1 and PC2 opera-
tions. Such leaks are challenging to exploit using tradi-
tional DPA (because it can’t vary the keys) and SPA
methods (because the signals are overlapping and com-
plicated). Template attacks provide an alternative, but
traditional TA methods are also complicated. In this
pro ject we found that a very straightforward SVM was
able to decode these leaks and recover the keys with
near perfect accuracy—after a relatively short training
process.
Our results are compare favorably to previous re-
search, in that we extracted a key, and also found fea-
ture selection to be less important (in the PC1 and PC2

regions). In the XOR region we found that either using
a degree 2 polynomial kernel or using judicious feature
engineering could draw out the relationship between
key bit, ciphertext bit, and power measurement.
This work can be continued by applying these tech-
niques to recover a key from a more modern device
(such as the ASIC in an FPGA bitstream encryption
core). Another interesting area to explore is applying
these techniques to evaluating leakage using a sequence
of speciﬁc test vectors.

References

1. Kocher, Paul, Josh Jaﬀe, Benjamin Jun. “Diﬀerential
Power Analysis,” CRYPTO 1999, pages 388-397.
2. Chari, Suresh, Josyula Rao, and Panka j Rohatgi. “Tem-
plate attacks.” CHES 2002 (2003): 51-62.
3. FIPS PUB 46-3, “Data Encryption Standard,” National
Bureau of Standards, US Dept. of Commerce, Jan. 1977.
4. Hospodar, Gabriel, Benedikt Gierlichs, Elke de Mulder,
Ingrid Verbauwhede, Joos Vandewalle. “Machine Learn-
ing in Side-Channel Analysis: A First Study.” JCEN
2011, Vol 1, Issue 4, 293-302.
5. Lerman, L., Bontempi, G., Markowitch, O. “Side chan-
nel attack: an approach based on machine learning.”
COSADE (2011).
6. Tsoumakas, G., and I. Katakis. ”Multi-label classiﬁca-
tion: An overview.”
7. R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. LIBLINEAR: A Library for Large Linear Clas-
siﬁcation, Journal of Machine Learning Research 9(2008),
1871-1874.
8. Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a li-
brary for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1–27:27, 2011.
IJDWM 3, no. 3 (2007): 1-13.

010203040506070405060708090100i−th engineered feature matrix, i = 1,...,64Accuracy(%)First key bit, XOR region, 64 sets of engineered features   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64Training AccuracyTest Accuracy010203040506050556065707580859095100k−th key bit, k = 1,...,56Accuracy(%)All key bits, XOR regions, most informative set of engineered features   27 35 41 47 61 5 15 27 17 1 29 21 30 29 9 7 13 29 23 31 39 25 3 1 31 63 3 5 43 51 11 25 63 29 57 33 49 59 19 37 55 53 61 59 24 35 39 20 37 33 5 57 40 13 45 7Training, IdentifiableTest, IdentifiableTraining, NonidentifiableTest, Nonidentifiable
Active Comparison of Prediction Models

Christoph Sawade, Niels Landwehr, and Tobias Scheffer
University of Potsdam
Department of Computer Science
August-Bebel-Strasse 89, 14482 Potsdam, Germany
{sawade, landwehr, scheffer}@cs.uni-potsdam.de

Abstract

We address the problem of comparing the risks of two given predictive
models—for instance, a baseline model and a challenger—as conﬁdently as pos-
sible on a ﬁxed labeling budget. This problem occurs whenever models cannot
be compared on held-out training data, possibly because the training data are un-
available or do not reﬂect the desired test distribution. In this case, new test in-
stances have to be drawn and labeled at a cost. We devise an active comparison
method that selects instances according to an instrumental sampling distribution.
We derive the sampling distribution that maximizes the power of a statistical test
applied to the observed empirical risks, and thereby minimizes the likelihood of
choosing the inferior model. Empirically, we investigate model selection prob-
lems on several classiﬁcation and regression tasks and study the accuracy of the
resulting p-values.

1

Introduction

We address situations in which an informed choice between candidate predictive models—for in-
stance, a baseline method and a challenger—has to be made. In practice, it is not always possible to
compare the models’ risks on held-out training data. For example, in computer vision it is common
to acquire pre-trained object or face recognizers from third parties. Such recognizers do not typi-
cally come with the image databases that have been used to train them. The suppliers of the models
could provide risk estimates based on held-out training data; however, such estimates might be bi-
ased because the training data would not necessarily reﬂect the distribution of images the deployed
models will be exposed to. Another example are domains where the input distribution changes
over a period of time in which a baseline model, e.g., a spam ﬁlter, has been employed. By the
time a new predictive model is considered, a previous risk estimate of the baseline model may no
longer be accurate.
In these example scenarios, new test data have to be drawn and labeled. The standard approach
to comparing models would be to draw n test instances according to the test distribution which
the model is exposed to in practice, label these data, and calculate the difference of the empirical
√
n ˆ∆n
risks ˆ∆n and the sample variance S 2
n . Then, under the null hypothesis of identical risks,
is
Sn
asymptotically governed by a standard normal distribution, and we can compute a p-value which
quantiﬁes the likelihood that an observed empirical difference is due to chance, indicating how
conﬁdently the decision to prefer the apparently better model can be made.
In many application scenarios, unlabeled test instances are readily available whereas the process
of labeling data is costly. We study an active model comparison process that, in analogy to active
learning, selects instances from a pool of unlabeled test data and queries their labels. Instances
are selected according to an instrumental sampling distribution q . The empirical difference of the
models’ risks is weighted appropriately to compensate for the discrepancy between instrumental
and test distributions which leads to consistent—that is, asymptotically unbiased—risk estimates.

1

The principal theoretical contribution of this paper is the derivation of a sampling distribution q that
allows us to make the decision to prefer the superior model as conﬁdently as possible given a ﬁxed
labeling budget n, if one of the models is in fact superior. Equivalently, one may use q to minimize
the labeling costs n required to reach a correct decision at a prescribed level of conﬁdence.
The active comparison problem that we study can be seen as an extreme case of active learning, in
which the model space contains only two (or, more generally, a small number of) models. For the
special case of classiﬁcation with zero-one loss and two models under study, a simpliﬁed version
of the sampling distribution we derive coincides with the sampling distribution used in the A 2 and
IWAL active learning algorithms proposed by Balcan et al. [1] and Beygelzimer et al. [2]. For A 2
and IWAL , the derivation of this distribution is based on ﬁnite-sample complexity bounds, while in
our approach, it is based on maximizing the power of a statistical test comparing the models under
study. The latter approach has the advantage that it directly generalizes to regression problems. A
further difference to active learning is that our goal is not only to choose the best model, but also to
obtain a well-calibrated p-value indicating the conﬁdence with which this decision can be made.
Our method is also related to recent work on active data acquisition strategies for the evaluation
of a single predictive model, in terms of standard risks [8] or generalized risks that subsume pre-
cision, recall, and f-measure [9]. The problem addressed in this paper is different in that we seek
to assess the relative performance of two models, without necessarily determining absolute risks
precisely. Madani et al. have studied active model selection, where the goal is also to identify a
model with lowest risk [5]. However, in their setting costs are associated with obtaining predictions
ˆy = f (x), while in our setting costs are associated with obtaining labels y ∼ p(y |x). Hoeffding
races [6] and sequential sampling algorithms [10] perform efﬁcient model selection by keeping
track of risk bounds for candidate models and removing models that are clearly outperformed from
consideration. The goal of these methods is to reduce computational complexity, not labeling effort.
The rest of this paper is organized as follows. The problem setting is laid out in Section 2. Section 3
derives the instrumental distribution and details our theoretical ﬁndings. Section 4 explores active
model comparison experimentally. Section 5 concludes.

2 Problem Setting
Let X denote the feature space and Y the label space; an unknown test distribution p(x, y) is deﬁned
over X × Y . Let p(y |x; θ1 ) and p(y |x; θ2 ) be given θ-parameterized models of p(y |x) and let
fj : X → Y with fj (x) = arg maxy p(y |x; θj ) be the corresponding predictive functions.
(cid:90) (cid:90)
The risks of f1 , f2 are given by
(1)
R[fj ] =
(cid:96)(fj (x), y)p(x, y)dy dx
for a loss function (cid:96) : Y × Y → R. In a classiﬁcation setting, the integral over Y reduces to a sum.
n(cid:88)
The standard approach to comparing models is to compare empirical risk estimates
1
n
i=1
where n test instances (xi , yi ) are drawn from p(x, y) = p(x)p(y |x). We assume that unlabeled
data are readily available, but acquiring labels y for selected instances x according to p(y |x) is a
costly process that may involve a query to a human labeler.
Test instances need not necessarily be drawn according to the input distribution p(x). We will focus
on a data labeling process that draws test instances according to an instrumental distribution q(x)
rather than p(x).
Intuitively, q(x) should be designed such as to prefer instances that highlight
differences between the models f1 and f2 . Let q(x) denote an instrumental distribution with the
property that p(x) > 0 implies q(x) > 0 for all x ∈ X . A consistent risk estimate is then given by
n(cid:88)
i=1

ˆRn,q [fj ] =

(cid:96)(fj (xi ), yi ),

ˆRn [fj ] =

(cid:96)(fj (xi ), yi ),

1
W

p(xi )
q(xi )

(2)

(3)

2

where (xi , yi ) ∼ q(x)p(y |x) and W = (cid:80)n
q(xi ) . Weighting factors p(xi )
p(xi )
q(xi ) compensate for the
i=1
discrepancy between test and instrumental distribution, and the normalizer is the sum of weights.
Because of the weighting factors, Equation 3 deﬁnes a consistent risk estimate (see [4], Chapter 2).
Consistency means that the expected value of ˆRn,q [fj ] converges to the true risk R[fj ] for n → ∞.
Given estimates ˆRn,q [f1 ] and ˆRn,q [f2 ], the difference ˆ∆n,q = ˆRn,q [f1 ]− ˆRn,q [f2 ] provides evidence
on which model is preferable; a positive ˆ∆n,q argues in favor of f2 . In preferring one model over the
other, one rejects the null hypothesis that the observed difference ˆ∆n,q is only a random effect, and
R[f1 ] = R[f2 ] holds. The null hypothesis implies that the mean of ˆ∆n,q is asymptotically zero.
Because ˆ∆n,q is asymptotically normally distributed (see, e.g., [3]), it further implies that the statistic
√

n

ˆ∆n,q
σn,q

∼ N (0, 1)

n,q = Var[ ˆ∆n,q ] denotes the variance
is asymptotically standard-normally distributed, where 1
n σ2
(cid:16)
(cid:17)2
of ˆ∆n,q . In practice, σ2
n(cid:88)
n,q is unknown. A consistent estimator of σ2
n,q is given by
i=1

(cid:96)(f1 (xi ), yi ) − (cid:96)(f2 (xi ), yi ) − ˆ∆n,q

p(xi )2
q(xi )2

S 2
n,q =

1
W

(4)

,

as shown, for example, by Geweke [3]. Substituting the empirical for the true standard deviation
√
ˆ∆n,q
n,q , the null hypothesis
n,q consistently estimates σ2
. Because S 2
yields an observable statistic
n
Sn,q
also implies that the observable statistic is asymptotically standard normally distributed,
√
ˆ∆n,q
∼ N (0, 1).
Sn,q
(cid:33)(cid:33)
(cid:32)
(cid:32)√
Let Φ denote the cumulative distribution function of the standard normal distribution. Then,
| ˆ∆n,q |
Sn,q

1 − Φ

(5)

n

n

2

is called the p-value of a two-sided paired Wald test (see, e.g., [12], Chapter 10). The p-value quan-
tiﬁes the likelihood of observing the given absolute value of the test statistic, or a higher value, by
chance under the null hypothesis. Student’s t-distribution can serve as a more popular approximation
of the distribution of a test statistic under the null hypothesis, resulting in the common t-test. Note,
however, that Sn,q would have to be a sum of squared, normally distributed random variables for the
test statistic to be asymptotically governed by the t-distribution. This assumption is reasonable for
regression, but not for classiﬁcation, and only for the case of p = q .
If the null hypothesis does not hold and the two models incur different risks, the distribution of the
test statistic depends on the chosen sampling distribution q(x). Our goal is to ﬁnd a distribution q(x)
that allows us to tell the risks of f1 and f2 apart with high conﬁdence. More formally, the power of
a test when sampling from q(x) is the likelihood that the null hypothesis can be rejected, that is, the
likelihood that the p-value falls below a pre-speciﬁed conﬁdence threshold α. Our goal is to ﬁnd the
(cid:32)
(cid:32)
(cid:32)√
(cid:33)(cid:33)
(cid:33)
sampling distribution q that maximizes test power:
1 − Φ
2

≤ α

n

(6)

.

q∗ = arg max
q

p

| ˆ∆n,q |
Sn,q

3 Active Model Comparison

We now turn towards deriving an optimal sampling distribution q∗ according to Equation 6. Sec-
tion 3.1 analytically derives an asymptotically optimal sampling distribution. Section 3.2 discusses
the sampling distribution in a pool-based setting and presents the active comparison algorithm.

3

3.1 Asymptotically Optimal Sampling
the corresponding critical value zα = Φ−1 (cid:0)1 − α
(cid:1):
Let ∆ = R[f1 ] − R[f2 ] denote the true risk difference, and assume ∆ (cid:54)= 0. Given a conﬁdence
threshold α, the test power equals the probability that the absolute value of the test statistic exceeds
(cid:32)√
(cid:33)
(cid:33)
(cid:33)
(cid:32)√
(cid:32)
2
| ˆ∆n,q |
≤ α
2 − 2Φ
n
p
Sn,q

| ˆ∆n,q |
Sn,q

≥ zα

= p

(7)

n

.

Asymptotically, it holds that

√

n( ˆ∆n,q − ∆)
σn,q

∼ N (0, 1).

Since Sn,q consistently estimates σn,q , it follows that for large n the statistic
√
(cid:19)
(cid:18) √
n∆
and unit variance,
distributed with mean
σn,q
√
n∆
σn,q

n ˆ∆n,q
Sn,q

∼ N

, 1

.

√

n

ˆ∆n,q
Sn,q

is normally

(8)

p

n

f

√

(9)

dT ,

exp

≤ α

where

f (T ; µ, 1) =

≈ 1 −
(cid:19)

| ˆ∆n,q |
of the test statistic follows a folded normal dis-
Equation 8 implies that the absolute value
n
√
Sn,q
n∆
tribution with location parameter
and scale parameter one. According to Equation 7, test power
(cid:32)
(cid:32)√
(cid:33)
(cid:33)
(cid:19)
(cid:18)
(cid:90) zα
σn,q
can thus be approximated in terms of the cumulative distribution of this folded normal distribution,
√
| ˆ∆n,q |
2 − 2Φ
n∆
, 1
T ;
σn,q
Sn,q
0
(cid:18)
(cid:18)
1√
1√
− 1
− 1
(T + µ)2
exp
2
2
2π
2π
(cid:18)
(cid:19)
(cid:90) zα
denotes the density of a folded normal distribution with location parameter µ and scale parameter
one. We deﬁne the shorthand
√
n∆
, 1
σn,q
0
for the approximation of test power given by Equation 9. In the following, we derive a sampling dis-
tribution maximizing βn,q , thereby approximately solving the optimization problem of Equation 6.
(cid:115)(cid:90)
Theorem 1 (Optimal Sampling Distribution). Let ∆ = R[f1 ] − R[f2 ] with ∆ (cid:54)= 0. The distribution
((cid:96)(f1 (x), y) − (cid:96)(f2 (x), y) − ∆)2 p(y |x)dy
asymptotically maximizes βn,q ; that is, for any other sampling distribution q (cid:54)= q∗ it holds that
βn,q < βn,q∗ for sufﬁciently large n.

q∗ (x) ∝ p(x)

βn,q = 1 −

(T − µ)2

(cid:19)

dT

+

f

T ;

Before we prove Theorem 1, we show that a sampling distribution asymptotically maximizes βn,q if
and only if it minimizes the asymptotic variance of the estimator ˆ∆n,q .
Lemma 2 (Variance Optimality). Let q , q (cid:48) denote two sampling distributions. Then it holds that
(cid:104) ˆ∆n,q
(cid:105)
(cid:104) ˆ∆n,q (cid:48)
(cid:105)
βn,q > βn,q (cid:48) for sufﬁciently large n if and only if
A proof is included in the online appendix. Lemma 2 shows that in order to solve the optimization
problem given by Equation 6, we need to ﬁnd the sampling distribution minimizing the asymptotic
variance of the estimator ˆ∆n,q . This asymptotic variance is characterized by the following Lemma.

< lim
n→∞ n Var

lim
n→∞ n Var

(10)

.

4

σ2
q =

Lemma 3 (Asymptotic Variance). The asymptotic variance σ2
n→∞n Var[ ˆ∆n,q ] of ˆ∆n,q is
(cid:90) (cid:90) p(x)2
q = lim
given by
q(x)2 ((cid:96)(f1 (x), y) − (cid:96)(f2 (x), y) − ∆)2 p(y |x)q(x)dy dx.
A proof of Lemma 3 is included in the online appendix.
the constraint (cid:82) q(x)dx = 1 using a Lagrange multiplier β .
Proof of Theorem 1. We can now prove Theorem 1 by deriving the distribution q∗ that minimizes the
(cid:18)(cid:90)
(cid:19)
(cid:90) c(x)
asymptotic variance σ2
q as given by Lemma 3. We minimize the functional σ2
q in terms of q under
where c(x) = p(x)2 (cid:82) ((cid:96)(f1 (x), y) − (cid:96)(f2 (x), y) − ∆)2 p(y |x)dy . The optimal point for the con-
L [q , β ] = σ2
q(x)dx − 1
+ β (q(x) − p(x)) dx
=
q + β
q(x)
(cid:18) c(x)
(cid:19)
strained problem satisﬁes the Euler-Lagrange equation
= − c(x)
+ β (q(x) − p(x))
∂
(11)
q(x)2 + β = 0.
(cid:112)c(x)
∂ q(x)
q(x)
A solution for Equation 11 with respect to the normalization constraint is given by
(cid:82) (cid:112)c(x)dx
q∗ (x) =
Resubstitution of c(x) into Equation 12 implies the theorem.

.

(12)

3.2 Empirical Sampling Distribution
The distribution q∗ also depends on the true conditional p(y |x) and the true difference in risks ∆.
In order to implement the method, we have to approximate these quantities. Note that as long as
p(x) > 0 implies q(x) > 0, any choice of q will yield consistent risk estimates because weighting
factors account for the discrepancy between sampling and test distribution (Equation 3). That is,
ˆ∆n,q is guaranteed to converge to ∆ as n grows large; any approximation employed to compute q∗
will only affect the number of test examples required to reach a certain level of estimation accu-
racy. To approximate the true conditional p(y |x), we use the given predictive models p(y |x; θ1 ) and
p(y |x; θ2 ), and assume a mixture distribution giving equal weight to both models:
p(y |x; θ1 ) +
p(y |x; θ2 ).
p(y |x) ≈ 1
1
(13)
2
2
The risk difference ∆ is replaced by a difference ∆θ of introspective risks calculated from Equa-
tion 1, where the integral over X is replaced by a sum over the pool, p(x) = 1
m , and p(y |x) is
approximated by Equation 13.
We will now derive the empirical sampling distribution for two standard loss functions.
Derivation 4 (Sampling for Zero-one Loss). Let (cid:96) be the zero-one loss for a binary prediction
problem with label space Y = {0, 1}. When p(y |x) is approximated as in Equation 13, the sampling

(cid:113)
distribution asymptotically maximizing βn,q in a pool-based setting resolves to
|∆θ |
(cid:113)
: f1 (x) = f2 (x)
1 − 2∆θ (1 − 2p(y = 1|x; θ)) + ∆θ
2 : f1 (x) > f2 (x)
1 + 2∆θ (1 − 2p(y = 1|x; θ)) + ∆θ
2 : f1 (x) < f2 (x)
for all x ∈ D .
A proof is included in the online appendix. Instead of using Approximation 13, an uninformative
approximation p(y = 1|x) ≈ 0.5 may be used. In this case q∗ degenerates to uniform sampling from
the subset of the pool where f1 (x) (cid:54)= f2 (x). We denote this baseline as active (cid:54)= . This baseline
coincides with the A 2 as well as the IWAL active learning algorithms, applied to the model space
{f1 , f2}, as can be seen from inspection of Algorithm 1 in [1] and Algorithms 1 and 2 in [2].
We now derive the optimal sampling distribution for regression problems with a squared loss func-
tion, assuming that the predictive distributions p(y |x; θ1 ) and p(y |x; θ2 ) are Gaussian:

q∗ (x) ∝

5

Algorithm 1 Active Model Comparison
input Models f1 , f2 with distributions p(y |x; θ1 ), p(y |x; θ2 ); pool D ; labeling budget n.
1: Compute sampling distribution q∗ (Derivation 4 or 5).
2: for i = 1, . . . , n do
Draw xi ∼ q∗ (x) from D with replacement.
3:
Query label yi ∼ p(y |xi ) from oracle.
4:
5: end for
6: Compute ˆRn,q [f1 ] and ˆRn,q [f2 ] (Equation 3).
7: Determine f ∗ ← arg minf ∈{f1 ,f2 } ˆRn,q [f ], compute p-value for sample (Equation 5)
output f ∗ , p-value.

Derivation 5 (Sampling for Squared Loss). Let (cid:96) be the squared loss, and let p(y |x; θ1 ) and
p(y |x; θ2 ) be Gaussian. When p(y |x) is approximated as in Equation 13, then the sampling dis-
(cid:115)
tribution asymptotically maximizing βn,q in a pool-based setting resolves to
q∗ (x) ∝
1 (x) − f 2
2 (f1 (x) − f2 (x))2 (f 2
x )− (f 2
2 (x))2
(14)
1 (x) + f 2
2 (x) + τ 2
x denotes the sum of the variances of the predictive distributions at x ∈ D .
for all x ∈ D , where τ 2
A proof is given in the online appendix. Variances of predictive distributions at instance x would be
available from a probabilistic model such as a Gaussian process [7]. If only predictions fj (x) but
x → 0, leading to
no predictive distribution is available, we can assume peaked distributions with τ 2
q∗ (x) ∝ (f1 (x) − f2 (x))2 ,
x → ∞, leading to
or we can assume inﬁnitely broad predictive distributions with τ 2
q∗ (x) ∝ |f1 (x) − f2 (x)|.
We refer to these baselines as active0 and active∞ .
Algorithm 1 summarizes the active model comparison algorithm. It samples n instances with re-
placement from the pool according to the distribution prescribed by Derivations 4 (for zero-one
loss) or 5 (for squared loss) and queries their label. Note that instances can be drawn more than
once; in the special case that the labeling process is deterministic, the actual labeling costs may thus
stay below the sample size. In this case, the loop is continued until the labeling budget is exhausted.
We have so far focused on the problem of comparing the risks of two prediction models, such as
a baseline and a challenger. We might also consider several alternative models; the objective of
an evaluation could be to rank the models according to the risk incurred or to identify the model
with lowest risk. Standard generalizations of the Wald test that compare multiple alternatives—for
instance, within-subject ANOVA [11]—try to reject the null hypothesis that the means of all con-
sidered alternatives are equal. Rejection does not imply that all empirically observed differences are
signiﬁcant; for instance, the test could become signiﬁcant because one of the alternatives performs
clearly worst. Choosing a sampling distribution q that maximizes the power of such a test would
thus in general not reﬂect the objectives of the empirical evaluation.
In practice, researchers often resort to pairwise hypothesis testing when comparing multiple predic-
tion models. Accordingly, we derive a heuristic sampling distribution for the comparison of multiple
(cid:88)
models θ1 , ..., θk as a mixture of pairwise-optimal sampling distributions,
q∗ (x) =
q∗
1
k(k − 1)
i,j (x),
i (cid:54)=j
where q∗
i,j denotes the optimal distribution for comparing the models θi and θj given by Theorem 1.
When comparing multiple models, we replace Equation 13 by a mixture over all models θ1 , ..., θk .

(15)

4 Empirical Results

We study the empirical behavior of active comparison (Algorithm 1, labeled active in all diagrams)
relative to a risk comparison based on a test sample drawn uniformly from the pool (labeled passive)

6

Figure 1: Model selection accuracy over labeling costs for comparison of two prediction models
(top) and multiple prediction models (bottom). Error bars indicate the standard error.

and the baselines active (cid:54)= , active0 , and active∞ discussed in Section 3.2. We also include the active
risk estimator presented in [8] in our study, which infers optimal sampling distributions q∗
1 and q∗
2 for
individually estimating the risks of the models θ1 and θ2 . Test instances are sampled from a mixture
2 q∗
2 q∗
distribution q∗ (x) = 1
2 (x) (labeled ARE). Each comparison method returns the model
1 (x) + 1
with lower empirical risk and the p-value of a paired two-sided test. When studying classiﬁcation,
we also include the active learning algorithms A 2 [1] and IWAL [2] as baselines by using them to
sample test instances. Their model space is the set of predictive models that are to be compared.
We conduct experiments in two classiﬁcation domains (spam ﬁltering, object recognition) and two
regression domains (inverse dynamics, Abalone) ranging from 4,109 to 169,612 instances. Kernel-
ized logistic regression is employed for classiﬁcation, Gaussian processes are employed for regres-
sion. In the spam ﬁltering domain, we compare models that differ in the recency of their training
data. In the object recognition domain, we compare SIFT-based recognizers using different interest
point detectors (Harris operator, Canny edge detector, F ¨orstner operator) and visual vocabularies.
For regression, we compare models that differ in the choice of their kernel function (linear versus
Matern, polynomial kernels of different degrees). Models are trained on part of the available data;
the rest of the data serve as the pool of unlabeled test instances for which labels can be queried.
Results are averaged over 5,000 repetitions of the evaluation process. Further details on the datasets
and experimental setup are included in the online appendix.

4.1

Identifying the Model With Lower True Risk

We measure model selection accuracy, deﬁned as the fraction of experiments in which an evaluation
method correctly identiﬁes the model with lower true risk. The true risk is taken to be the risk over
all test instances in the pool. Figure 1 (top) shows that for the comparison of two models active
results in signiﬁcantly higher model selection accuracy than passive, or, equivalently, saves between
70% and 90% of labeling effort. Differences between active and the simpliﬁed variants active0 ,
active∞ , and active(cid:54)= are marginal. These variants do not require an estimate of p(y |x), thus the
method is applicable even if no such estimate is available. A 2 and IWAL coincide with active(cid:54)=
(cf. Section 3.2). Figure 1 (bottom) shows results when comparing multiple models. In the object
recognition domain, active saves approximately 70% of labeling effort compared to passive. A 2 and
IWAL outperform passive but are less accurate than active. For the regression domains, active saves
between 60% and 85% of labeling effort compared to passive.

4.2 Signiﬁcance Testing: Type I and Type II Errors

We now study how often a comparison method is able to reject the null hypothesis that two predictive
models incur identical risks, and the calibration of the resulting p-values. For classiﬁcation, the

7

501001502000.40.50.60.70.80.9labeling costs nmodel selection accuracySpam Filtering(Classification, 2 Models)active≠passiveAREA2activeIWAL2004006008000.650.70.750.80.850.90.95labeling costs nmodel selection accuracyAbalone(Regression, 2 Models)active∞active0AREpassiveactive2004006008000.70.750.80.850.90.951labeling costs nmodel selection accuracyInverse Dynamics(Regression, 2 Models)active∞active0AREpassiveactive5010015020000.20.40.60.81labeling costs nmodel selection accuracyObject Recognition(Classification, 13 Models)active≠passiveAREA2activeIWAL2004006008000.40.50.60.70.80.9labeling costs nmodel selection accuracyAbalone(Regression, 5 Models)active∞active0AREpassiveactive2004006008000.40.50.60.70.8labeling costs nmodel selection accuracyInverse Dynamics(Regression, 5 Models)active∞active0AREpassiveactiveFigure 2: True-positive signiﬁcance rate for different test levels α (left, left-center). Average p-value
over labeling costs n (right-center, right). Error bars indicate the standard error.

Figure 3: False-positive signiﬁcance rate over test level α (left, left-center). False-positive signiﬁ-
cance rate over labeling costs n (right-center, right). Error bars indicate the standard error.
method active(cid:54)= is equivalent to passive applied to D(cid:54)= = {x ∈ D |f1 (x) (cid:54)= f2 (x)} (see Section 3.2).
Labeling effort is thus simply reduced by a factor of |D(cid:54)= |/|D |. For regression, the analysis is less
straightforward as typically D = D(cid:54)= . In this section we therefore focus on regression problems.
Figure 2 (left, left-center) shows how often the active and passive comparison methods are able to
reject the null hypothesis that the two models incur identical risk. The true risks incurred are never
equal in these experiments. We observe that active is able to reject the null hypothesis more often
and with a higher conﬁdence. In the Abalone domain, active rejects the null hypothesis at α = 0.001
more often than passive is able to reject it at α = 0.1. Figure 2 (right-center, right) shows that active
comparison also results in lower average p-values, in particular for large n.
We also conduct experiments under the null hypothesis. Whenever a test instance x is sampled and
the predictions y = f1 (x) and y (cid:48) = f2 (x) are queried, the predicted labels y and y (cid:48) are swapped
with probability 0.5; this ensures that the true risks of f1 and f2 coincide. Figure 3 (left, left-center)
shows that Type I errors are well calibrated for both tests, as the false-positive rate stays below the
(ideal) diagonal line when plotted against α. Figure 3 (right-center, right) shows that both tests are
slightly conservative for small n, and approach the expected false-positive rate as n grows larger.
We ﬁnally study a protocol in which test instances are drawn and labeled until the null hypothesis can
be rejected or the labeling budget is exhausted. Results (included in the online appendix) indicate
that active incurs the lowest average labeling costs, obtains signiﬁcance results most often, and has
the lowest likelihood of incorrectly choosing the model with higher true risk.

5 Conclusion

We have derived the sampling distribution that asymptotically maximizes the power of a statistical
test that compares the risk of two predictive models. The sampling distribution intuitively gives
preference to test instances on which the models disagree strongly.
Empirically, we observed that the resulting active comparison method consistently outperforms a
traditional comparison based on a uniform sample of test instances. Active comparison identiﬁes
the model with lower true risk more often, and is able to detect signiﬁcant differences between
the risks of two given models more quickly.
In the four experimental domains that we studied,
performing active comparison resulted in a saved labeling effort of between 60% and over 90%. We
also performed experiments under the null hypothesis that both models incur identical risks, and
veriﬁed that active comparison does not lead to increased false-positive signiﬁcance results.

Acknowledgements

We wish to thank Paul Prasse for his help with the experiments on object recognition data.

8

0.0010.010.050.100.20.40.60.81True Positive SignificanceInverse Dynamics (Regression, n=800)α−levelfrequency  passiveactive0.0010.010.050.100.20.40.60.81True Positive SignificanceAbalone (Regression, n=800)α−levelfrequency  passiveactive20040060080000.10.20.3labeling costs naverage p−valueAverage p−valueInverse Dynamics (Regression)  passiveactive20040060080000.10.20.3labeling costs naverage p−valueAverage p−valueAbalone (Regression)  passiveactive00.050.10.150.200.050.10.150.2α−levelfrequencyFalse Positive SignificanceInverse Dynamics (Regression, n=800)  passiveactive00.050.10.150.200.050.10.150.2α−levelfrequencyFalse Positive SignificanceAbalone (Regression, n=800)  passiveactive20040060080000.020.04labeling costs nfrequencyFalse Positive SignificanceInverse Dynamics (Regression, α=0.05)  passiveactive20040060080000.020.04labeling costs nfrequencyFalse Positive SignificanceAbalone (Regression, α=0.05)  passiveactiveReferences
[1] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proceedings of the
23rd International Conference on Machine Learning, 2006.
[2] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Pro-
ceedings of the 26th International Conference on Machine Learning, 2009.
[3] J. Geweke. Bayesian inference in econometric models using monte carlo integration. Econo-
metrica, 57(6):1317–1339, 1989.
[4] J. S. Liu. Monte Carlo Strategies in Scientiﬁc Computing. Springer, 2001.
[5] O. Madani, D. J. Lizotte, and R. Greiner. Active model selection. In Proceedings of the 20th
Conference on Uncertainty in Artiﬁcial Intelligence, 2004.
[6] O. Maron and A. W. Moore. Hoeffding races: Accelerating model selection search for classi-
ﬁcation and function approximation. In Proceedings of the 6th Annual Conference on Neural
Information Processing Systems, 1993.
[7] Carl Edward Rasmussen and Christopher Williams. Gaussian Processes for Machine Learn-
ing. MIT Press, 2006.
[8] C. Sawade, N. Landwehr, S. Bickel, and T. Scheffer. Active risk estimation. In Proceedings of
the 27th International Conference on Machine Learning, 2010.
[9] C. Sawade, N. Landwehr, and T. Scheffer. Active estimation of f-measures. In Proceedings of
the 23rd Annual Conference on Neural Information Processing Systems, 2010.
[10] T. Scheffer and S. Wrobel. Finding the most interesting patterns in a database quickly by using
sequential sampling. Journal of Machine Learning Research, 3:833–862, 2003.
[11] D. Sheskin. Handbook of Parametric and Nonparametric Statistical Procedures. Chapman &
Hall, 2004.
[12] L. Wasserman. All of Statistics: a Concise Course in Statistical Inference. Springer, 2004.

9


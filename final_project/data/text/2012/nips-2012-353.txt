Small-Variance Asymptotics for Exponential Family
Dirichlet Process Mixture Models

Ke Jiang, Brian Kulis
Department of CSE
The Ohio State University
{jiangk,kulis}@cse.ohio-state.edu

Michael I. Jordan
Departments of EECS and Statistics
University of California at Berkeley
jordan@cs.berkeley.edu

Abstract

Sampling and variational inference techniques are two standard methods for in-
ference in probabilistic models, but for many problems, neither approach scales
effectively to large-scale data. An alternative is to relax the probabilistic model
into a non-probabilistic formulation which has a scalable associated algorithm.
This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting
the variance of particular distributions in the model go to zero. For instance, in
the context of clustering, such an approach yields connections between the k-
means and EM algorithms. In this paper, we explore small-variance asymptotics
for exponential family Dirichlet process (DP) and hierarchical Dirichlet process
(HDP) mixture models. Utilizing connections between exponential family distri-
butions and Bregman divergences, we derive novel clustering algorithms from the
asymptotic limit of the DP and HDP mixtures that features the scalability of exist-
ing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric
models. We focus on special cases of our analysis for discrete-data problems, in-
cluding topic modeling, and we demonstrate the utility of our results by applying
variants of our algorithms to problems arising in vision and document analysis.

1
Introduction
An enduring challenge for machine learning is in the development of algorithms that scale to truly
large data sets. While probabilistic approaches—particularly Bayesian models—are ﬂexible from
a modeling perspective, lack of scalable inference methods can limit applicability on some data.
For example, in clustering, algorithms such as k-means are often preferred in large-scale settings
over probabilistic approaches such as Gaussian mixtures or Dirichlet process (DP) mixtures, as the
k-means algorithm is easy to implement and scales to large data sets.
In some cases, links between probabilistic and non-probabilistic models can be made by applying
asymptotics to the variance (or covariance) of distributions within the model. For instance, con-
nections between probabilistic and standard PCA can be made by letting the covariance of the data
likelihood in probabilistic PCA tend toward zero [1, 2]; similarly, the k-means algorithm may be
obtained as a limit of the EM algorithm when the covariances of the Gaussians corresponding to
each cluster goes to zero. Besides providing a conceptual link between seemingly quite different
approaches, small-variance asymptotics can yield useful alternatives to probabilistic models when
the data size becomes large, as the non-probabilistic models often exhibit more favorable scaling
properties. The use of such techniques to derive scalable algorithms from rich probabilistic models
is still emerging, but provides a promising approach to developing scalable learning algorithms.
This paper explores such small-variance asymptotics for clustering, focusing on the DP mixture.
Existing work has considered asymptotics over the Gaussian DP mixture [3], leading to k-means-
like algorithms that do not ﬁx the number of clusters upfront. This approach, while an important
ﬁrst step, raises the question of whether we can perform similar asymptotics over distributions other

1

than the Gaussian. We answer in the afﬁrmative by showing how such asymptotics may be applied
to the exponential family distributions for DP mixtures; such analysis opens the door to a new class
of scalable clustering algorithms and utilizes connections between Bregman divergences and expo-
nential families. We further extend our approach to hierarchical nonparametric models (speciﬁcally,
the hierarchical Dirichlet process (HDP) [4]), and we view a major contribution of our analysis to
be the development of a general hard clustering algorithm for grouped data.
One of the primary advantages of generalizing beyond the Gaussian case is that it opens the door
to novel scalable algorithms for discrete-data problems. For instance, visual bag-of-words [5] have
become a standard representation for images in a variety of computer vision tasks, but many existing
probabilistic models in vision cannot scale to the size of data sets now commonly available. Simi-
larly, text document analysis models (e.g., LDA [6]) are almost exclusively discrete-data problems.
Our analysis covers such problems; for instance, a particular special case of our analysis is a hard
version of HDP topic modeling. We demonstrate the utility of our methods by exploring applications
in text and vision.
Related Work: In the non-Bayesian setting, asymptotics for the expectation-maximization algo-
rithm for exponential family distributions were studied in [7]. The authors showed a connection be-
tween EM and a general k-means-like algorithm, where the squared Euclidean distance is replaced
by the Bregman divergence corresponding to exponential family distribution of interest. Our results
may be viewed as generalizing this approach to the Bayesian nonparametric setting. As discussed
above, our results may also be viewed as generalizing the approach of [3], where the asymptotics
were performed for the DP mixture with a Gaussian likelihood, leading to a k-means-like algo-
rithm where the number of clusters is not ﬁxed upfront. Note that our setting is considerably more
involved than either of these previous works, particularly since we will require an appropriate tech-
nique for computing an asymptotic marginal likelihood. Other connections between hard clustering
and probabilistic models were explored in [8], which proposes a “Bayesian k-means” algorithm by
performing a maximization-expectation algorithm.

2 Background
In this section, we brieﬂy review exponential family distributions, Bregman divergences, and the
Dirichlet process mixture model.

2.1 The Exponential Family
Consider the exponential family with natural parameter θ = {θj }d
j=1 ∈ Rd ; then the exponential
p(x | θ) = exp (cid:0)(cid:104)x, θ(cid:105) − ψ(θ) − h(x)(cid:1),
family probability density function can be written as [9]:
where ψ(θ) = log (cid:82) exp((cid:104)x, θ(cid:105) − h(x))dx is the log-partition function. Here we assume for
simplicity that x is a minimal sufﬁcient statistic for the natural parameter θ . ψ(θ) can be utilized to
compute the mean and covariance of p(x | θ); in particular, the expected value is given by ∇ψ(θ),
and the covariance is ∇2ψ(θ).
Conjugate Priors: In a Bayesian setting, we will require a prior distribution over the natural pa-
rameter θ . A convenient property of the exponential family is that a conjugate prior distribution of
θ exists; in particular, given any speciﬁc distribution in the exponential family, the conjugate prior
p(θ | τ , η) = exp (cid:0)(cid:104)θ , τ (cid:105) − ηψ(θ) − m(τ , η)(cid:1).
can be parametrized as [11]:
Here, the ψ(·) function is the same as that of the likelihood function. Given a data point xi , the
posterior distribution of θ has the same form as the prior, with τ → τ + xi and η → η + 1.
Relationship to Bregman Divergences: Let φ : S → R be a differentiable, strictly convex function
deﬁned on a convex set S ⊆ Rd . The Bregman divergence for any pair of points x, y ∈ S is deﬁned
as Dφ (x, y) = φ(x) − φ(y) − (cid:104)x − y , ∇φ(y)(cid:105), and can be viewed as a generalized distortion mea-
sure. An important result connecting Bregman divergences and exponential families was discussed
in [7] (see also [10, 11]), where a bijection between the two was established. A key consequence
of this result is that we can equivalently parameterize both p(x | θ) and p(θ | τ , η) in terms of the

2

expectation µ:

(cid:18)
(cid:18) τ
(cid:19)(cid:19)
p(x | θ) = p(x | µ) = exp(−Dφ (x, µ))fφ (x),
p(θ | τ , η) = p(µ | τ , η) = exp
− ηDφ
, µ
gφ (τ , η),
η
where φ(·) is the Legendre-conjugate function of ψ(·) (denoted as φ = ψ∗ ), fφ (x) = exp(φ(x) −
h(x)), and µ is the expectation parameter which satisﬁes µ = ∇ψ(θ) (and also µ = θ∗ ). The
Bregman divergence representation provides a natural way to parametrize the exponential family
distributions with its expectation parameter and, as we will see, we will ﬁnd it convenient to work
with this form.

πcp(x | µc ) =

πc exp(−Dφ (x, µc ))fφ (x).

p(x) =

2.2 Dirichlet Process Mixture Models
The Dirichlet Process (DP) mixture model is a Bayesian nonparametric mixture model [12]; unlike
most parametric mixture models (Bayesian or otherwise), the number of clusters in a DP mixture is
not ﬁxed upfront. Using the exponential family parameterized by the expectation µc , the likelihood
∞(cid:88)
∞(cid:88)
for a data point can be expressed as the following inﬁnite mixture:
c=1
c=1
Even though there are conceptually an inﬁnite number of clusters, the nonparametric prior over the
mixing weights causes the weights πc to decay exponentially. Moreover, a simple collapsed Gibbs
sampler can be employed for performing inference in this model [13]; this Gibbs sampler will form
the basis of our asymptotic analysis. Given a data set {x1 , ..., xn}, the state of the Markov chain
is the set of cluster indicators {z1 , ..., zn} as well as the cluster means of the currently-occupied
clusters (the mixing weights have been integrated out). The Gibbs updates for zi , (i = 1, . . . , n),
are given by the following conditional probabilities:
(cid:90)
P (zi = c | z−i , xi , µ) =
p(xi | µc )
p(xi | µ)dG0 ,
P (zi = cnew | z−i , xi , µ) =

n−i,c
Z(n − 1 + α)
α
Z(n − 1 + α)
where Z is the normalizing constant, n−i,c is the number of data points (excluding xi ) that are
currently assigned to cluster c, G0 is a prior over µ, and α is the concentration parameter that
determines how likely we are to start a new cluster. If we choose to start a new cluster during the
Gibbs update, we sample its mean from the posterior distribution obtained from the prior distribution
G0 and the single observation xi . After performing Gibbs moves on the cluster indicators, we update
the cluster means µc by sampling from the posterior of µc given the data points assigned to cluster c.

3 Hard Clustering for Exponential Family DP Mixtures
Our goal is to analyze what happens as we perform small-variance asymptotics on the exponential
family DP mixture when running the collapsed Gibbs sampler described earlier, and we begin by
considering how to scale the covariance in an exponential family distribution. Given an exponential
family distribution p(x | θ) with natural parameter θ and log-partition function ψ(θ), consider a
scaled exponential family distribution whose natural parameter is ˜θ = βθ and log-partition function
is ˜ψ( ˜θ) = βψ( ˜θ/β ), where β > 0. The following result characterizes the relationship between the
mean and covariance of the original and scaled exponential family distributions.
Lemma 3.1. Denote µ(θ) as the mean, and cov(θ) as the covariance, of p(x | θ) with log-partition
ψ(θ). Given a scaled exponential family with ˜θ = βθ and ˜ψ( ˜θ) = βψ( ˜θ/β ), the mean ˜µ( ˜θ) of the
scaled distribution is µ(θ) and the covariance, ˜cov( ˜θ), is cov(θ)/β .
This lemma follows directly from ˜µ( ˜θ) = ∇ ˜θ
˜ψ( ˜θ) = β∇ ˜θ ψ( ˜θ/β ) = ∇θ ψ( ˜θ/β ) = ∇θ ψ(θ) =
β × ∇2
β × ∇2
µ(θ), and ˜cov( ˜θ) = ∇2
( ˜ψ( ˜θ)) = β∇ ˜θ (∇ ˜θ ψ( ˜θ/β )) = 1
θ ψ( ˜θ/β ) = 1
θ ψ(θ) =
˜θ
cov(θ)/β . It is perhaps intuitively simpler to observe what happens to the distribution using the

3

D ˜φ

g ˜φ

,

g ˜φ

,

.

= exp

− ηDφ

Bregman divergence representation. Recall that the generating function φ for the Bregman diver-
gence is given by the Legendre-conjugate of ψ . Using standard properties of convex conjugates, we
see that the conjugate of ˜ψ is simply ˜φ = βφ. The Bregman divergence representation for the scaled
distribution is given by
p(x | ˜θ) = p(x | ˜µ) = exp(−D ˜φ (x, ˜µ))f ˜φ (x) = exp(−βDφ (x, µ))fβφ (x),
where the last equality follows from Lemma 3.1 and the fact that, for a Bregman divergence,
Dβφ (·, ·) = βDφ (·, ·). Thus, as β increases under the above scaling, the mean is ﬁxed while the
distribution becomes increasingly concentrated around the mean.
Next we consider the prior distribution under the scaled exponential family. When scaling by β , we
also need to scale the hyper-parameters τ and η , namely τ → τ /β and η → η/β . This gives the
(cid:18)
(cid:19)
(cid:19)(cid:19)
(cid:18) τ
(cid:18)
(cid:19)
(cid:19)(cid:19)
(cid:18) τ
(cid:18) τ
(cid:18) τ /β
following prior written using the Bregman divergence, where we are now explicitly conditioning on
β :
p( ˜θ | τ , η , β ) = exp
− η
η
η
, µ
, µ
η/β
β
η
β
β
β
β
(cid:90)
Finally, we compute the marginal likelihood for x by integrating out ˜θ , as it will be necessary for
the Gibbs sampler. Standard algebraic manipulations yield the following:
(cid:19)
(cid:18)
(cid:19)(cid:19)
(cid:18) τ
(cid:18) βx + τ
(cid:90)
p(x | τ , η , β ) =
p(x | ˜θ) × p( ˜θ | τ , η , β )d ˜θ
(cid:19)(cid:19)
(cid:19)
(cid:18) τ
(cid:18)
(cid:18) βx + τ
(cid:90)
= f ˜φ (x) · g ˜φ
− (β + η)Dφ
η
d ˜θ
, ˜µ( ˜θ)
exp
A( ˜φ,τ ,η ,β ) (x)
β + η
β
β
− (β + η)Dφ
A( ˜φ,τ ,η ,β ) (x) · β d ·
= f ˜φ (x) · g ˜φ
η
dθ .
, µ(θ)
,
exp
Here, A( ˜φ,τ ,η ,β ) (x) = exp (cid:0) − (βφ(x) + ηφ( τ
β+η ))(cid:1), which arises when combining
β
β
β + η
(1)
η ) − (β + η)φ( βx+τ
the Bregman divergences from the likelihood and the prior.
Now we make the following key insight, which will allow us to perform the necessary asymptotics.
We can write the integral from the last line above (denoted I below) via Laplace’s method. Since
(cid:19)d/2 (cid:12)(cid:12)(cid:12)(cid:12) ∂ 2Dφ ( βx+τ
(cid:12)(cid:12)(cid:12)(cid:12)−1/2
(cid:19)
(cid:18)
(cid:18) βx + τ
(cid:19)(cid:19)(cid:18) 2π
(cid:18) 1
β+η , µ) has a local minimum (which is global in this case) at ˆθ = ˆµ∗ = ( βx+τ
β+η )∗ , we have:
Dφ ( βx+τ
β+η , ˆµ)
− (β + η)Dφ
(cid:19)d/2 (cid:12)(cid:12)(cid:12)(cid:12) ∂ 2Dφ ( βx+τ
(cid:12)(cid:12)(cid:12)(cid:12)−1/2
(cid:18) 1
(cid:18) 2π
(cid:19)
, ˆµ
I = exp
∂θ∂θT
β + η
β + η
β
β+η , ˆµ)
∂θ∂θT
β
β + η
where ∂ 2Dφ ( βx+τ
β+η , ˆµ)
= cov( ˆθ) is the covariance matrix of the likelihood function instantiated at ˆθ
and approaches cov(x∗ ) when β goes to ∞. Note that the exponential term equals one since the
∂θ∂θT
divergence inside is 0.
3.1 Asymptotic Behavior of the Gibbs Sampler
We now have the tools to consider the Gibbs sampler for the exponential family DP mixture as we
let β → ∞. As we will see, we will obtain a general k-means-like hard clustering algorithm which
utilizes the appropriate Bregman divergence in place of the squared Euclidean distance, and also can
vary the number of clusters. Recall the conditional probabilities for performing Gibbs moves on the
cluster indicators zi , where we now are considering the scaled distributions:
n−i,c
exp(−βDφ (xi , µc ))f ˜φ (xi )
P (zi = c | z−i , xi , β , µ) =
Z
p(xi | τ , η , β ),
P (zi = cnew | z−i , xi , β , µ) =
α
Z
where Z is a normalization factor, and the marginal probability p(xi | τ , η , β ) is given by the deriva-
tions in (1) and (2). Now, we consider the asymptotic behavior of these probabilities as β → ∞. We

+ O

+ O

=

(2)

,

4

note that

α =

g ˜φ

,

·

P (zi = cnew | z−i , xi , β , µ) =

β→∞ A( ˜φ,τ ,η ,β ) (xi ) = exp(−η(φ(τ /η) − φ(xi ))),
βxi + τ
and
lim
lim
= xi ,
β→∞
β + η
and that the Laplace approximation error term goes to zero as β → ∞. Further, we deﬁne α as a
(cid:18) 2π
(cid:18) τ
(cid:18)
(cid:19)
(cid:19)d/2 · β d
(cid:19)−1 · exp(−βλ),
function of β , η , and τ (but independent of the data):
η
β
β + η
β
for some λ. After canceling out the f ˜φ (xi ) terms from all probabilities, we can then write the Gibbs
probabilities as
Cxi · exp(−βλ) + (cid:80)k
n−i,c · exp(−βDφ (xi , µc ))
P (zi = c | z−i , xi , β , µ) =
j=1 n−i,j · exp(−βDφ (xi , µj ))
Cxi · exp(−βλ) + (cid:80)k
Cxi · exp(−βλ)
j=1 n−i,j · exp(−βDφ (xi , µj ))
where Cxi approaches a positive, ﬁnite constant for a given xi as β → ∞. Now, all of the above
probabilities will become binary as β → ∞. More speciﬁcally, all the k + 1 values will be in-
creasingly dominated by the smallest value of {Dφ (xi , µ1 ), . . . , Dφ (xi , µk ), λ}. As β → ∞, only
the smallest of these values will receive a non-zero probability. That is, the data point xi will be
assigned to the nearest cluster with a divergence at most λ. If the closest mean has a divergence
greater than λ, we start a new cluster containing only xi .
Next, we show that sampling µc from the posterior distribution is achieved by simply computing
the empirical mean of a cluster in the limit. During Gibbs sampling, once we have performed one
complete set of Gibbs moves on the cluster assignments, we need to sample the µc conditioned on
all assignments and observations. If we let nc be the number of points assigned to cluster c, then the
(cid:18) (cid:80)nc
(cid:19)(cid:19)
(cid:18)
posterior distribution (parameterized by the expectation parameter) for cluster c is
i=1 βxc
p(µc | X, z , τ , η , β ) ∝ p(Xc | µc , β )×p(µc | τ , η , β ) ∝ exp
−(βnc+η)Dφ
i + τ
, µ
βnc + η
} is the set of points currently assigned to cluster c, and z
where X is all the data, Xc = {xc
1 , ..., xc
(cid:80)nc
nc
is the set of all current assignments. We can see that the mass of the posterior distribution becomes
as β → ∞. In other words, after we determine the
i=1 xi
concentrated around the sample mean
nc
assignments of data points to clusters, we update the means as the sample mean of the data points in
each cluster. This is equivalent to the standard k-means cluster mean update step.
3.2 Objective function and algorithm
From the above asymptotic analysis of the Gibbs sampler, we observe a new algorithm which can
be utilized for hard clustering. It is as simple as the popular k-means algorithm, but also provides
the ability to adapt the number of clusters depending on the data as well as incorporate different
(cid:80)n
distortion measures. The algorithm description is as follows:
• Initialization: input data x1 , . . . , xn , λ > 0, and µ1 = 1
i=1 xn
n
• Assignment: for each data point xi , compute the Bregman divergence Dφ (xi , µc ) to all
existing clusters. If minc Dφ (xi , µc ) ≤ λ, then zi,c0 = 1 where c0 = argmincDφ (xi , µc );
(cid:80)
otherwise, start a new cluster and set zi,cnew = 1;
• Mean Update: compute the cluster mean for each cluster, µj = 1|lj |
x∈lj
the set of points in the j -th cluster.

x, where lj is

,

,

We iterate between the assignment and mean update steps until local convergence. Note that the
initialization used here—placing all data points into a single cluster—is not necessary, but is one
natural way to initialize the algorithm. Also note that the algorithm depends heavily on the choice
of λ; heuristics for selecting λ were brieﬂy discussed for the Gaussian case in [3], and we will follow
this approach (generalized in the obvious way to Bregman divergences) for our experiments.

5

We can easily show that the underlying objective function for our algorithm is quite similar to that
in [3], replacing the squared Euclidean distance with an appropriate Bregman divergence. Recall
that the squared Euclidean distance is the Bregman divergence corresponding to the Gaussian distri-
bution. Thus, the objective function in [3] can be seen as a special case of our work. The objective
(cid:88)
k(cid:88)
function optimized by our derived algorithm is the following:
x∈lc
c=1
where k is the total number of clusters, φ is the conjugate function of the log-partition function of
the chosen exponential family distribution, and µc is the sample mean of cluster c. The penalty term
λ controls the tradeoff between the likelihood and the model complexity, where a large λ favors
small model complexity (i.e., fewer clusters) while a small λ favors more clusters. Given the above
objective function, our algorithm can be shown to monotonically decrease the objective function
value until convergence to some local minima. We omit the proof here as it is almost identical as the
proof for Theorem 3.1 in [3].

Dφ (x, µc ) + λk

min
{lc }k
c=1

(3)

4 Extension to Hierarchies
A key beneﬁt of the Bayesian approach is its natural ability to form hierarchical models. In the con-
text of clustering, a hierarchical mixture allows one to cluster multiple groups of data—each group
is clustered into a set of local clusters, but these local clusters are shared among the groups (i.e.,
sets of local clusters across groups form global clusters, with a shared global mean). For Bayesian
nonparametric mixture models, one way of achieving such hierarchies arises via the hierarchical
Dirichlet Process (HDP) [4], which provides a nonparametric approach to allow sharing of clusters
among a set of DP mixtures.
In this section, we will brieﬂy sketch out the extension of our analysis to the HDP mixture, which
yields a natural extension of our methods to groups of data. Given space considerations, and the fact
that the resulting algorithm turns out to reduce to Algorithm 2 from [3] with the squared Euclidean
distance replaced by an appropriate Bregman divergence, we will omit the full speciﬁcation of the
algorithm here. However, despite the similarity to the existing Gaussian case, we do view the ex-
tension to hierarchies as a promising application of our analysis. In particular, our approach opens
the door to hard hierarchical algorithms over discrete data, such as text, and we brieﬂy discuss an
application of our derived algorithm to topic modeling.
We assume that there are J data sets (groups) which we index by j = 1, ..., J . Data point xij
refers to data point i from set j . The HDP model can be viewed as clustering each data set into
local clusters, but where each local cluster is associated to a global mean. Global means may be
shared across data sets. When performing the asymptotics, we require variables for the global means
(µ1 , ..., µg ), the associations of data points to local clusters, zij , and the associations of local clusters
to global means, vj t , where t indexes the local clusters for a data set. A standard Gibbs sampler
considers updates on all of these variables, and in the nonparametric setting does not ﬁx the number
of local or global clusters.
The tools from the previous section may be nearly directly applied to the hierarchical case. As
opposed to the ﬂat model, the hard HDP requires two parameters: a value λtop which is utilized
when starting a global (top-level) cluster, and a value λbottom which is utilized when starting a local
cluster. The resulting hard clustering algorithm ﬁrst performs local assignment moves on the zij ,
then updates the local cluster assignments, and ﬁnally updates all global means.
(cid:88)
k(cid:88)
The resulting objective function that is monotonically minimized by our algorithm is given as fol-
lows:
xij ∈lc
c=1
where k is the total number of global clusters and t is the total number of local clusters. The bottom-
level penalty term λbottom controls both the number of local and top-level clusters, where larger
λbottom tends to give fewer local clusters and more top-level clusters. Meanwhile, the top-level
penalty term λtop , as in the one-level case, controls the tradeoff between the likelihood and model
complexity.

Dφ (xij , µc ) + λbottom t + λtopk ,

min
{lc }k
c=1

(4)

6

Figure 1: (Left) Example images from the ImageNet data (Persian cat and elephant categories). Each
image is represented via a discrete visual-bag-of-words histogram. Clustering via an asymptotic
multinomial DP mixture considerably outperforms the asymptotic Gaussian DP mixture; see text
for details. (Right) Elapsed time per iteration in seconds of our topic modeling algorithm when
running on the NIPS data, as a function of the number of topics.

5 Experiments
We conclude with a brief set of experiments highlighting applications of our analysis to discrete-data
problems, namely image clustering and topic modeling. For all experiments, we randomly permute
the data points at each iteration, as this tends to improve results (as discussed previously, unlike
standard k-means, the order in which the data points are processed impacts the resulting clusters).

Image Clustering. We ﬁrst explore an application of our techniques to image clustering, focusing
on the ImageNet data [14]. We utilize a subset of this data for quantitative experiments, sampling
100 images from 10 different categories of this data set (Persian cat, African elephant, ﬁre engine,
motor scooter, wheelchair, park bench, cello, French horn, television, and goblet), for a total of 1000
images. Each image is processed via standard visual-bag-of-words: SIFT is densely applied on top
of image patches in image, and the resulting SIFT vectors are quantized into 1000 visual words.
We use the resulting histograms as our discrete representation for an image, as is standard. Some
example images from this data set are shown in Figure 1.
We explore whether the discrete version of our hard clustering algorithm based on a multinomial
DP mixture outperforms the Gaussian mixture version (i.e., DP-means); this will validate our gen-
eralization beyond the Gaussian setting. For both the Gaussian and multinomial cases, we utilize a
farthest-ﬁrst approach for both selecting λ as well as initializing the clusters (see [3] for a discussion
of farthest-ﬁrst for selecting λ).
We compute the normalized mutual information (NMI) between the true clusters and the results of
the two algorithms on this difﬁcult data set. The Gaussian version performs poorly, achieving an
NMI of .06 on this data, whereas the hard multinomial version achieves a score of .27. While the
multinomial version is far from perfect, it performs signiﬁcantly better than DP-means. Scalability
to large data sets is clearly feasible, given that the method scales linearly in the number of data
points. Note that comparisons between the Gibbs sampler and the corresponding hard clustering
algorithm for the Gaussian case were considered in [3], where experiments on several data sets
showed comparable clustering accuracy results between the sampler and the hard clustering method.
Furthermore, for a fully Bayesian model that places a prior on the concentration parameter, the
sampler was shown to be considerably slower than the corresponding hard clustering method. Given
the similarity of the sampler for the Gaussian and multinomial case, we expect similar behavior
with the multinomial Gibbs sampler.

Illustration: Scalable Hard Topic Models. We also highlight an application to topic modeling,
by providing some qualitative results over two common document collections. Utilizing our general
algorithm for a hard version of the multinomial HDP is straightforward. In order to apply the hard
hierarchical algorithm to topic modeling, we simply utilize the discrete KL-divergence in the hard
exponential family HDP, since topic modeling for text uses a multinomial distribution for the data
likelihood.
To test topic modeling using our asymptotic approach, we performed analyses using the NIPS 1-121
and the NYTimes [15] datasets. For the NIPS dataset, we use the whole dataset, which contains
1740 total documents, 13649 words in the vocabulary, and 2,301,375 total words. For the NYTimes

1 http://www.cs.nyu.edu/ roweis/data.html

7

1

2

3

4

5

6

NIPS
neurons, memory, patterns, activity, re-
sponse, neuron, stimulus, ﬁring, cortex, re-
current, pattern, spike, stimuli, delay, re-
sponses
neural, networks, state, weight, states, re-
sults, synaptic, threshold, large, time, sys-
tems, activation, small, work, weights
training, hidden, recognition, layer, per-
formance, probability, parameter, error,
speech, class, weights, trained, algorithm,
approach, order
cells, visual, cell, orientation, cortical, con-
nection, receptive, ﬁeld, center,
tuning,
low, ocular, present, dominance, ﬁelds
energy, solution, methods, function, solu-
tions, local, equations, minimum, hopﬁeld,
temperature, adaptation,
term, optimiza-
tion, computational, procedure
noise, classiﬁer, classiﬁers, note, margin,
noisy, regularization, generalization, hy-
pothesis, multiclasses, prior, cases, boost-
ing, ﬁg, pattern

NYTimes
team, game, season, play, games, point,
player, coach, win, won, guy, played, play-
ing, record, ﬁnal

percent, campaign, money, fund, quarter,
federal, public, pay, cost, according, in-
come, half, term, program, increase
president, power, government, country,
peace, trial, public, reform, patriot, eco-
nomic, past, clear,
interview, religious,
early
family, father, room, line, shares, recount,
told, mother, friend, speech, expression,
won, offer, card, real
company, companies, stock, market, busi-
ness, billion, ﬁrm, computer, analyst, in-
dustry,
internet, chief,
technology, cus-
tomer, number
right, human, decision, need, leadership,
foundation, number, question, country,
strike, set, called, support, law, train

Table 1: Sample topics inferred from the NIPS and NYTimes datasets by our hard multinomial HDP
algorithm.

dataset, we randomly sampled 2971 documents with 10171 vocabulary words, and 853,451 words in
total; we also eliminated low-frequency words (those with less than ten occurrences). The prevailing
metric to measure the goodness of topic models is perplexity; however, this is based on the predictive
probability, which has no counterpart in the hard clustering case. Furthermore, ground truth for topic
models is difﬁcult to obtain. This makes quantitative comparisons difﬁcult for topic modeling, and
so we therefore focus on qualitative results. Some sample topics (with the corresponding top 15
terms) discovered by our approach from both the NIPS and NYTimes datasets are given in Table 1;
we can see that the topics appear to be quite reasonable. Also, we highlight the scalability of our
approach: the number of iterations needed for convergence on these data sets ranges from 13 to 25,
and each iteration completes in under one minute (see the right side of Figure 1). In contrast, for
sampling methods, it is notoriously difﬁcult to detect convergence, and generally a large number of
iterations is required. Thus, we expect this approach to scale favorably to large data sets.

6 Conclusion
We considered a general small-variance asymptotic analysis for the exponential family DP and
HDP mixture model. Crucially, this analysis allows us to move beyond the Gaussian distribution
in such models, and opens the door to new clustering applications, such as those involving discrete
data. Our analysis utilizes connections between Bregman divergences and exponential families,
and results in a simple and scalable hard clustering algorithm which may be viewed as generalizing
existing non-Bayesian Bregman clustering algorithms [7] as well as the DP-means algorithm [3].
Due to the prevalence of discrete data in modern computer vision and information retrieval, we
hope our algorithms will ﬁnd use for a variety of large-scale data analysis tasks. We plan to
continue to focus on the difﬁcult problem of quantitative evaluations comparing probabilistic and
non-probabilistic methods for clustering, particularly for topic models. We also plan to compare
our algorithms with recent online inference schemes for topic modeling, particularly the online
LDA [16] and online HDP [17] algorithms.

Acknowledgements. This work was supported by NSF award IIS-1217433 and by the ONR under
grant number N00014-11-1-0688.

8

References
[1] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the
Royal Statistical Society, Series B, 21(3):611–622, 1999.
[2] S. Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing
Systems, 1998.
[3] B. Kulis and M. I. Jordan. Revisiting k-means: New algorithms via Bayesian nonparametrics.
In Proceedings of the 29th International Conference on Machine Learning, 2012.
[4] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101(476):1566–1581, 2006.
[5] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories.
In IEEE Conference on Computer Vision and Patterns Recognition, 2005.
[6] D. Blei, A. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[7] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with Bregman divergences.
Journal of Machine Learning Research, 6:1705–1749, 2005.
[8] K. Kurihara and M. Welling. Bayesian k-means as a “Maximization-Expectation” algorithm.
Neural Computation, 21(4):1145–1172, 2008.
Information and Exponential Families in Statistical Theory. Wiley
[9] O. Barndorff-Nielsen.
Publishers, 1978.
[10] J. Forster and M. K. Warmuth. Relative expected instantaneous loss bounds. In Proceedings
of 13th Conference on Computational Learning Theory, 2000.
[11] A. Agarwal and H. Daume. A geometric view of conjugate priors. Machine Learning,
81(1):99–113, 2010.
[12] N. Hjort, C. Holmes, P. Mueller, and S. Walker. Bayesian Nonparametrics: Principles and
Practice. Cambridge University Press, Cambridge, UK, 2010.
[13] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9:249–265, 2000.
[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hier-
archical image database. In IEEE Conference on Computer Vision and Patterns Recognition,
2009.
[15] A. Frank and A. Asuncion. UCI Machine Learning Repository, 2010.
[16] M. D. Hoffman, D. M. Blei, and F. Bach. Online learning for Latent Dirichlet Allocation. In
Advances in Neural Information Processing Systems, 2010.
[17] C. Wang, J. Paisley, and D. M. Blei. Online variational inference for the hierarchical Dirichlet
In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and
process.
Statistics, 2011.

9


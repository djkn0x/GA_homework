Selective Labeling via Error Bound Minimization

Quanquan Gu† , Tong Zhang‡ , Chris Ding§ , Jiawei Han†
†Department of Computer Science, University of Illinois at Urbana-Champaign
‡Department. of Statistics, Rutgers University
§Department. of Computer Science & Engineering, University of Texas at Arlington
qgu3@illinois.edu, tzhang@stat.rutgers.edu, chqding@uta.edu, hanj@cs.uiuc.edu

Abstract

In many practical machine learning problems, the acquisition of labeled data is of-
ten expensive and/or time consuming. This motivates us to study a problem as fol-
lows: given a label budget, how to select data points to label such that the learning
performance is optimized. We propose a selective labeling method by analyzing
the out-of-sample error of Laplacian regularized Least Squares (LapRLS). In par-
ticular, we derive a deterministic out-of-sample error bound for LapRLS trained
on subsampled data, and propose to select a subset of data points to label by min-
imizing this upper bound. Since the minimization is a combinational problem, we
relax it into continuous domain and solve it by projected gradient descent. Ex-
periments on benchmark datasets show that the proposed method outperforms the
state-of-the-art methods.

1 Introduction

The performance of (semi-)supervised learning methods typically depends on the amount of labeled
data. Roughly speaking, the more the labeled data, the better the learning performance will be.
However, in many practical machine learning problems, the acquisition of labeled data is often
expensive and/or time consuming. To overcome this problem, active learning [9, 10] was proposed,
which iteratively queries the oracle (labeler) to obtain the labels at new data points. Representative
methods include support vector machine (SVM) active learning [19, 18], agnostic active learning
[2, 5, 14], etc. Due to the close interaction between the learner and the oracle, active learning can be
advantageous to achieve better learning performance. Nevertheless, in many real-world applications,
such an interaction may not be feasible. For example, when one turns to Amazon Mechanical Turk1
to label data, the interaction between the learner and the labeling workers is very limited. Therefore,
standard active learning is not very practical in this case.
Another potential solution to the label deﬁciency problem is semi-supervised learning [7, 22, 21, 4],
which aims at combining a small number of labeled data and a large amount of unlabeled data to
improve the learning performance. In a typical setting of semi-supervised learning, a small set of
labeled data is assumed to be given at hand or randomly generated in practice. However, randomly
selecting (uniformly sampling) data points to label is unwise because not all the data points are
equally informative. It is desirable to obtain a labeled subset which is most beneﬁcial for semi-
supervised learning.
In this paper, based on the above motivation, we investigate a problem as follows: given a ﬁxed label
budget, how to select a subset of data points to label such that the learning performance is optimized.
We refer to this problem as selective labeling, in contrast to conventional random labeling. To
achieve the goal of selective labeling, it is crucial to consider the out-of-sample error of a speciﬁc
learner. We choose Laplacian Regularized Least Squares (LapRLS) as the learner [4] because it is a

1 https://www.mturk.com/

1

state-the-art semi-supervised learning method, and takes many linear regression methods as special
cases (e.g., ridge regression [15]). We derive a deterministic out-of-sample error bound for LapRLS
trained on subsampled data, which suggests to select the data points to label by minimizing this
upper bound. The resulting selective labeling method is a combinatorial optimization problem. In
order to optimize it effectively and efﬁciently, we relax it into a continuous optimization problem,
and solve it by projected gradient descent algorithm followed by discretization. Experiments on
benchmark datasets show that the proposed method outperforms the state-of-the-art methods.
The remainder of this paper is organized as follows.
In Section 2, we brieﬂy review manifold
regularization and LapRLS. In Section 3, we derive an out-of-sample error bound for LapRLS on
subsampled data, and present a selective labeling criterion by minimizing the this bound, followed
by its optimization algorithm. We discuss the connections between the proposed method and several
existing experimental design approaches in Section 4. The experiments are demonstrated in Section
5. We conclude this paper in Section 6.

2 Review of Laplacian Regularized Least Squares
Given a data set {(x1 , y1 ), . . . , (xn , yn )} where xi ∈ Rd and yi ∈ {±1}, Laplacian Regularized
Least Squares (LapRLS) [4] aims to learn a linear function f (x) = wT x. In order to estimate and
∫
preserve the geometrical and topological properties of the data, LapRLS [4] assumes that if two data
points xi and xj are close in the intrinsic geometry of the data distribution, the labels of this two
points are also close to each other. Let f (x) be a function that maps the original data point x in a
compact submanifold M to R, we use ||f ||2M =
x∈M || ▽M f ||2dx to measure the smoothness of
f along the geodesics in the intrinsic manifold of the data, where ▽M f is the gradient of f along
the manifold M. Recent study on spectral graph theory [8] has demonstrated that ||f ||2M can be
∑
discretely approximated through a nearest neighbor graph on a set of data points. Given an afﬁnity
matrix W ∈ Rn×n of the graph, ||f ||2M is approximated as:
||f ||2M ≈ 1
||fi − fj ||2
2Wij = f T Lf ,
∑
2
ij
where fi is a shorthand for f (xi ), f = [f1 , . . . , fn ]T , D is a diagonal matrix, called degree matrix,
j=1 Wij , and L = D − W is the combinatorial graph Laplacian [8]. Eq. (1) is called
n
with Dii =
Manifold Regularization. Intuitively, the regularization incurs a heavy penalty if neighboring points
xi and xj are mapped far apart.
Based on manifold regularization, LapRLS solves the following optimization problem,
||XT w − y||2
||w||2
2 +
2 +

λA
λI
arg min
2
2
w
where λA , λI > 0 are positive regularization parameters, X = [x1 , . . . , xn ] is the design ma-
trix, y = [y1 , . . . , yn ]T is the response vector, ||w||2 is ℓ2 regularization of linear function, and
wT XLXT w is manifold regularization of f (x) = wT x. When λI = 0, LapRLS reduces to ridge
regression [15]. A bias term b can be incorporated into the form by expanding the weight vector and
input feature vector as w ← [w; b] and x ← [x; 1]. Note that Eq. (2) is a supervised version of
LapRLS, because only labeled data are used in manifold regularization. Although our derivations
are based on this version in the rest of the paper, the results can be extended to semi-supervised
version of LapRLS straightforwardly.

(1)

wT XLXT w,

(2)

3 The Proposed Method

3.1 Problem Formulation
The generic problem of selective labeling is as follows. Given a set of data points X =
{x1 , . . . , xn }, namely the pool of candidate data points, our goal is to ﬁnd a subsample L ⊂
{1, . . . , n}, which contains the most informative |L| = l points.
To derive a selective labeling approach for LapRLS, we ﬁrst derive an out-of-sample error bound of
LapRLS.

2

3.2 Out-of-Sample Error Bound of LapRLS

∗

∗

We deﬁne the function class of LapRLS as follows.
Deﬁnition 1. The function class of LapRLS is FB = {x → wT x | λA ||w||2
2 + λI wT XLXT w ≤
B}, where X = [x1 , . . . , xn ], and B > 0 is a constant.
Consider the following linear regression model,
(3)
y = XT w
+ ϵ,
∗ is the
where X = [x1 , . . . , xn ] is the design matrix, y = [y1 , . . . , yn ]T is the response vector, w
true weight vector which is unknown, and ϵ = [ϵ1 , . . . , ϵn ]T is the noise vector with ϵi an unknown
noise with zero mean. We assume that different observations have noises that are independent, but
with equal variance σ2 .
∗ satisﬁes
Moreover, we assume that the true weight vector w
∗ ≤ B ,
∗ ||2
λA ||w
∗
(4)
)T XLXT w
2 + λI (w
which implies that the true hypothesis belongs to the function class of LapRLS in Deﬁnition 1. In
this case, the approximation error vanishes and the excess error equals to the estimation error. Note
that this assumption can be relaxed with more effort, under which we can derive a similar error
bound as below. For simplicity, the following derivations are built upon the assumption in Eq. (4).
∗ using LapRLS in Eq. (2) from a subsample
In selective labeling, we are interested in estimating w
L ∈ {1, . . . , n}. Denote the subsample of X by XL , the subsample of y by yL , and the subsample
of ϵ by ϵL . The solution of LapRLS is given by
−1XLyL ,
^wL = (XLXTL + λA I + λI XLLLXTL )
(5)
where I is an identity matrix, LL is the graph Laplacian computed based on XL , which is a principal
submatrix of L.
In the following, we will present a deterministic out-of-sample error bound for LapRLS trained on
the subsampled data, which is among the main contributions of this paper.
(
)
Theorem 2. For any ﬁxed V = [v1 , . . . , vm ] and X = [x1 , . . . , xn ], and a subsample L of X, the
expected error of LapRLS trained on L in predicting the true response VT w
∗ is upper bounded as
E||VT ^wL − VT w
≤ (B + σ2 )tr
∗ ||2
−1V
VT (XLXTL + λA I + λI XLLLXTL )
(6)
.
2
Proof. Let ML = λA I + λI XLLLXTL . Given L, the expected error (where the expectation is w.r.t.
ϵL ) is given by
E||VT ^wL − VT w
∗ ||2
2
∗ ||2
−1XLyL − VT w
= E||VT (XLXTL + ML )
|
|
}
{z
}
{z
2
∗ ||2
∗ − VT w
= ||VT (XLXTL + ML )
−1XL ϵL ||2
+ E||VT (XLXTL + ML )
−1XLXTLw
,(7)
2
2
A1
A2
where the second equality follows from yL = XLw
+ ϵL . Now we bound the two terms in the
right hand side respectively.
(
)
The ﬁrst term is bounded by
A1 = ||VT (XLXTL + ML )
≤ ||VT (XLXTL + ML )
∗ ||2
−1MLw
(
)
2
−1ML (XLXTL + ML )
−1V
VT (XLXTL + ML )
= B tr
≤ B tr
−1V
VT (XLXTL + ML )
(8)
where the ﬁrst inequality is due to Cauchy Schwarz’s inequality, and the second inequality follows
(
)
from dropping the negative term.
)
(
Similarly, the second term can be bounded by
A2 ≤ σ2 tr
−1XLXTL (XLXTL + ML )
VT (XLXTL + ML )
≤ σ2 tr
−1V
VT (XLXTL + ML )
(9)
,
where the ﬁrst equality uses E[ϵL ϵTL ] ≤ σ2 I, and it becomes equality if ϵi are independent and
identically distributed (i.i.d.). Combing Eqs. (8) and (9) completes the proof.

−1M

−1V

∗ ||2
2

2L ||2
1
F

||M

1
2Lw

3

Note that in the above theorem, the sample V could be either the same as or different from the
∗ as follows.
sample X. Sometimes, we are also interested in the expected estimation error of w
(
)
Theorem 3. For any ﬁxed X, and a subsample L of X, the expected error of LapRLS trained on L
∗ is upper bounded as
in estimating the true weight vector w
≤ (B + σ2 )tr
∗ ||2
E|| ^wL − w
−1
(XLXTL + λA I + λI XLLLXTL )
2

(10)

The proof of this theorem follows similar derivations of Theorem 2.

(11)

−1X

,

3.3 The Criterion of Selective Labeling
From Theorem 2, we can see that given a subsample L of X, the expected prediction error of
LapRLS on V is upper bounded by Eq. (6). In addition, the right hand side of Eq. (6) does not
depend on the labels, i.e., y. More importantly, the error bound derived in this paper is deterministic,
which is unlike those probabilistic error bounds derived based on Rademacher complexity [3] or
algorithmic stability [6]. Since those probabilistic error bounds only hold for i.i.d. sample rather
than a particular sample, they cannot provide a criterion to choose a subsample set for labeling due
to the correlation between the pool of candidate points and the i.i.d. sample. On the contrary, the
deterministic error bound does not suffer from such a kind of problem. Therefore, it provides a
natural criterion for selective labeling.
In detail, given a pool of candidate data points, i.e., X, we propose to ﬁnd a subsample L of
)
(
{1, . . . , n}, by minimizing the follow objective function
XT (XLXTL + λI XLLLXTL + λA I)
L⊂{1;:::;n} tr
arg min
where we simply assume V = X. The above problem is a combinatorial optimization problem.
Finding the global optimal solution is NP-hard. One potential way to solve it is greedy forward
(or backward) selection. However, it is inefﬁcient. Here we propose an efﬁcient algorithm, which
solves its continuous relaxation.
3.4 Reformulation
{
We introduce a selection matrix S ∈ Rn×l , which is deﬁned as
if xi is selected as the j -point in L
1,
otherwise.
0,
It is easy to check that each column of S has one and only one 1, and each row has at most one 1.
The constraint set for S can be deﬁned as
S1 = {S|S ∈ {0, 1}n×l , ST 1 = 1, S1 ≤ 1},
where 1 is a vector of all ones, or equivalently,
S2 = {S|S ∈ {0, 1}n×l , ST S = I},
where I is an identity matrix.
(
)
Based on S, we have XL = XS and LL = ST LS. Thus, Eq. (11) can be equivalently reformulated
as
)
(
−1X
XT (XSST XT + λI XSST LSST XT + λA I)
−1X
′
SST XT + λA I)
XT (XSST L

arg min
S∈S2
= arg min
S∈S2
= I + λI L. The above optimization problem is still a discrete optimization. Let
S3 = {S|S ≥ 0, ST S = I},
(16)
where we relax the binary constraint on S into nonnegative constraint. Note that S3 is a matching
(
)
polytope [17]. Then we solve the following continuous optimization,
′
−1X
tr
XT (XSST L
SST XT + λA I)

′
where L

Sij =

tr

tr

(12)

(13)

(14)

,

.

(15)

(17)

arg min
S∈S3

4

We derive a projected gradient descent algorithm to ﬁnd a local optimum of Eq. (17). We ﬁrst ignore
(
)
(
)
the nonnegative constraint on S. Since ST S = I, we introduce a Lagrange multiplier (cid:3) ∈ Rl×l ,
thus the Lagrangian function is
(cid:3)(ST S − I)
−1X
′
L(S) = tr
SST XT + λA I)
XT (XSST L

(18)

+ tr

.

(19)

The derivative of L(S) with respect to S is2
= −2(XT BXSST L
′
′
∂L
SST XT BXS) + 2S(cid:3),
S + L
∂S
−1 and A = XSST L
−1 (XXT )A
′
where B = A
SST XT + λI. Note that the computational burden
−1 , which is the inverse of a d × d matrix. To overcome this problem, we use
(
)−1
of the derivative is A
−1 can be computed as
the Woodbury matrix identity [12]. Then A
I − 1
−1 +
−1 =
′
1
1
ST XT ,
ST XT XS
(ST L
A
S)
λ2 XS
λ
λ
S is a l × l matrix, whose inverse can be solved efﬁciently when l ≪ d.
′
where ST L
To determine the Lagrange multiplier (cid:3), left multiplying Eq. (19) by ST , and using the fact that
ST S = I, we obtain

′
′
(21)
SST XT BXS.
S + ST L
(cid:3) = ST XT BXSST L
Substituting the Lagrange multiplier (cid:3) back into Eq. (19), we can obtain the derivative depending
only on S. Thus we can use projected gradient descent to ﬁnd a local optimal solution for Eq. (17).
In each iteration, it takes a step proportional to the negative of the gradient of the function at the
current point, followed by a projection back into the nonnegative set.

(20)

3.5 Discretization

∗ by projected gradient descent. However,
Till now, we have obtained a local optimal solution S
∗ ∈ S3 . In order to determine which l data
∗ contains continuous values. In other words, S
this S
∗ into S1 . We use a simple greedy procedure to conduct the
points to select, we need to project S
discretization: we ﬁrst ﬁnd the largest element in S (if there exist multiple largest elements, we
choose any one of them), and mark its row and column; then from the unmarked columns and rows
we ﬁnd the largest element and also mark it; this procedure is repeated until we ﬁnd l elements.

4 Related Work

We notice that our proposed method shares similar spirit with optimal experimental design3 in statis-
tics [1, 20, 16], whose intent is to select the most informative data points to learn a function which
has minimum variance of estimation, or minimum variance of prediction.
(
)
For example, A-Optimal Design (AOD) minimizes the expected variance of the model parameter. In
particular, for ridge regression, it optimizes the following criterion,
−1
(XLXTL + λA I)
L⊂{1;:::;n} tr
arg min
where I is an identity matrix. We can recover this criterion by setting λI = 0 in Theorem 3.
However, the pitfall of AOD is that it does not characterize the quality of predictions on the data,
which is essential for classiﬁcation or regression.
To overcome the shortcoming of A-optimal design, Yu et al. [20] proposed a Transdutive Experi-
(
)
mental Design (TED) approach. TED selects the samples which minimize the expected predictive
variance of ridge regression on the data,
L⊂{1;:::;n} tr
arg min
2The calculation of the derivative is non-trivial, please refer to the supplementary material for detail.
3 Some literature also call it active learning, while our understand is there is no adaptive interaction between
the learner and the oracle within optimal experimental design. Therefore, it is better to call it nonadaptive
active learning.

−1X
XT (XLXTL + λA I)

(23)

(22)

.

,

5

(
)
Although TED is motivated by minimizing the variance of the prediction, it is very interesting to
demonstrate that the above criterion is coinciding with minimizing the out-of-sample error bound
in Theorem 2 with λI = 0. The reason is that for ridge regression, the upper bounds of the bias
−1X
XT (XLXTL + λA I)
and variance terms share a common factor tr
. This is a very important
observation because it explains why TED performs very well even though its criterion is minimizing
the variance of the prediction. Furthermore, TED can be seen as a special case of our proposed
method.
)
(
He et al. [16] proposed Laplacian Optimal Design (LOD), which selects data points that minimize
the expected predictive variance of Laplacian regularized least squares [4] on the data,
−1X
L⊂{1;:::;n} tr
arg min
where the graph Laplacian L is computed on all the data points in the pool, i.e., X. LOD selects
the points by XLXTL while leaving the graph Laplacian term XLXT ﬁxed. However, our method
selects the points by XLXTL as well as the graph Laplacian term i.e., XLLLXTL . This difference
is essential, because our criterion has a strong theoretical foundation, i.e., minimizing the out-of-
sample error bound of LapRLS. This explains the non-signiﬁcant improvement of LOD over TED.
Admittedly, the term XLLLXTL in our method raised a challenge for optimization. Yet it has been
well-solved by the projected gradient descent algorithm derived in previous section.
We also notice that similar problem was studied for graphs [13]. However, their method cannot be
applied to our setting, because their input is restricted to the adjacency matrix of a graph.

XT (λI XLXT + XLXTL + λA I)

,

(24)

5 Experiments

In this section, we evaluate the proposed method on both synthetic and real-world datasets, and
compare it with the state-of-the-art methods. All the experiments are conducted in Matlab.

5.1 Compared Methods

To demonstrate the effectiveness of our proposed method, we compare it with the following baseline
approaches: Random Sampling (Random) uniformly selects data points from the pool as training
data. It is the simplest baseline for label selection. A-Optimal Design (AOD) is a classic exper-
imental design method proposed in the community of statistics. There is a parameter λA to be
tuned. Transductive Experiment Design (TED) is proposed in [20], which is the state-of-the-art
(non-adaptive) active learning method. There is a parameter λA to be tuned. Laplacian Optimal
Design (LOD) [16] is an extension of TED, which incorporates the manifold structure of the data.
Selective Labeling via Error Bound Minimization (Bound) is the proposed method. There are two
tunable parameters λA and λI in both LOD and Bound.
Both LOD and Bound use graph Laplacian. To compute it, we ﬁrst normalize each data point into a
vector with unit ℓ2 -norm. Then we construct a 5-NN graph and use the cosine distance to measure
the similarity between data points throughout of our experiments.
Note that the problem setting of our study is to select a batch of data points to label without training
a classiﬁer. Therefore, we do not compare our method with typical active learning methods such as
SVM active learning [19, 18] and agnostic active learning [2].
After selecting the data points by the above methods, we train a LapRLS [4] as the learner to do
classiﬁcation. There are two parameters in LapRLS, i.e., λA and λI .

5.2 Synthetic Dataset

To get an intuitive picture of how the above methods (except random sampling, which is trivial)
work differently, we show their experimental results on a synthetic dataset in Figure 1. This dataset
contains two circles, each of which constitutes a class. It has strong manifold structure. We let
the compared methods select 8 data points. As can be seen, the data points selected by AOD are
concentrated on the inner circle (belonging to one class), which are not able to train a classiﬁer.
The data points selected by TED, LapIOD and Bound are distributed on both inner and outer circles

6

(a) AOD

(b) TED

(c) LOD

(d) Bound

Figure 1: Selected points (the red marks) on the two circles dataset by (a) AOD; (b) TED; (c) LOD;
and (d) Bound.

(belonging to different classes), which are good at training a learner. Furthermore, the 8 data points
selected by Bound are uniformly distributed on the two circles, four from the inner circle, and the
other four from the outer circle, which can better represent the original data.

5.3 Real Datasets & Parameter Settings

In the following, we use three real-world benchmark datasets to evaluate the compared methods.
wdbc is the Wisconsin Diagnostic Breast Cancer data set, which is from UCI machine learning
repository4 . It aims at predicting the breast cancer as benign or malignant based on the digitalized
images. There are 357 positive samples and 212 negative samples. Each sample has 32 attributes.
ORL face database5 contains 10 images for each of the 40 human subjects, which were taken at
different times, varying the lighting, facial expressions and facial details. The original images (with
256 gray levels) have size 92 × 112, which are resized to 32 × 32 for efﬁciency.
Isolet was ﬁrst used in [11]. It contains 150 people who spoke each letter of the alphabet twice. The
speakers are grouped into sets of 30 speakers each, and we use the ﬁrst group, referred to Isolet1.
Each sample is represented by a 617-dimensional feature vector.
For each data set, we randomly select 20% data as held-out set for model selection, and the rest 80%
data as work set. In order to randomize the experiments, in each run of experiments, we restrict the
training data (pool of candidate data points) to be selected from a random sampling of 50% work
set (which accounts for 40% of the total data). The remaining half data (40% of the total data) is
used as test set. Once the labeled data are selected, we train a semi-supervised version of LapRLS,
which uses both labeled and unlabeled data (all the training data) for manifold regularization. We
report the classiﬁcation result on the test set. This random split was repeated 10 times, thus we can
compute the mean and standard deviation of the classiﬁcation accuracy.
The parameters of compared methods (See Section 5.1) are tuned by 2-fold cross validation on the
held-out set. For the parameters of LapRLS, we use the same parameters of LOD (or Bound) for
LapRLS. For the wdbc dataset, the chosen parameters are λA = 0.001, λI = 0.01. For ORL,
λA = 0.0001, λI = 0.001. For Isolet1, λA = 0.01, λI = 0.001.
For wdbc, we let the compared methods incrementally choose {2, 4, . . . , 20} points to label, for
ORL, we incrementally choose {80, 90, . . . , 150} points for labeling, and for Isolet1, we choose
{30, 40, . . . , 120} points to query.

5.4 Results on Real Datasets

The experimental results are shown in Figure 2. In all subﬁgures, the x-axis represents the number of
labeled points, while the y-axis is the averaged classiﬁcation accuracy on the test data over 10 runs.
In order to show some concrete results, we also list the accuracy and running time (in second) of
all the compared methods on the three datasets with 2, 80 and 30 labeled data points respectively in

4 http://archive.ics.uci.edu/ml/
5 http://www.cl.cam.ac.uk/Research/DTG/attarchive:pub/data

7

-2.5-2-1.5-1-0.500.511.522.5-2.5-2-1.5-1-0.500.511.522.5 1 2 3 4 5 6 7 8-2.5-2-1.5-1-0.500.511.522.5-2.5-2-1.5-1-0.500.511.522.5 1 2 3 4 5 6 7 8-2.5-2-1.5-1-0.500.511.522.5-2.5-2-1.5-1-0.500.511.522.5 1 2 3 4 5 6 7 8-2.5-2-1.5-1-0.500.511.522.5-2.5-2-1.5-1-0.500.511.522.5 1 2 3 4 5 6 7 8(a) wdbc

(b) ORL

(c) Isolet1

Figure 2: Comparison of different methods on (a) wdbc; (b) ORL; and (c) Isolet1 using LapRLS.

Table 1: Classiﬁcation accuracy (%) and running time (in second) of compared methods on the three
datasets.

wdbc (2 labeled)
Dataset
Acc
time
Random 69.47±14.56
–
68.59±12.46
0.0
AOD
68.33±10.68
0.0
TED
63.48±8.38
LOD
0.1
88.68±2.82
0.3
Bound

ORL (80 labeled)
Acc
time
72.00±4.05
–
65.17±3.14
32.2
80.33±2.94
39.6
80.25±2.64
41.7
83.25±3.17
23.4

Isolet1 (30 labeled)
Acc
time
44.36±3.09
–
40.27±2.24
7.4
55.98±2.54
41.1
57.79±1.87
41.5
61.99±2.14
17.4

Table 1. For each dataset, we did paired t-tests between the proposed method and the other methods
in the 95% conﬁdence interval. If it is signiﬁcant over all the other methods, the corresponding entry
of Bound is bolded.
We observe that the proposed selective labeling method greatly outperforms the other methods at
most cases. AOD is usually worse than random sampling. The reason is that minimizing the variance
of model parameter does not guarantee the quality of predictions on the data. TED performs very
well. As we mentioned before, the criterion of TED coincides with minimizing the out-of-sample
error bound of ridge regression. This explains its good empirical performance. The performance
of LOD is slightly better than TED. This is because LOD incorporates the geometric structure into
TED. The superior performance of our method is attributed to its theoretical foundation, which
guarantees that the learner (LapRLS) can achieve small error on the test data. In addition, the running
time of our method is comparable to or even less than the running time of the other methods.
One may argue that the above comparison is not fair because we use LapRLS as the learner, which
tends to ﬁt the proposed method. Therefore, we also compare different methods using ridge re-
gression (RR) as the learner. We ﬁnd that our proposed method is also much better than the other
methods using RR. For the space limit, we omit the results here and put them in the supplemental
material.

6 Conclusions

The main contributions of this paper are: (1) We present a deterministic out-of-sample error bound
for LapRLS; (2) we present a selective labeling method by minimizing this upper bound; and (3) we
present a simple yet effective algorithm to optimize the criterion for selective labeling.

Acknowledgement

The work was supported in part by U.S. National Science Foundation grants IIS-0905215, CNS-
0931975, the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-
0053 (NS-CTA), the U.S. Air Force Ofﬁce of Scientiﬁc Research MURI award FA9550-08-1-0265,
and MIAS, a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC. We
would like to thank the anonymous reviewers for their helpful comments.

8

2468101214161820657075808590#Labeled dataAccuracy  RandomAODTEDLODBound809010011012013014015016065707580859095#Labeled dataAccuracy  RandomAODTEDLODBound30405060708090100110120404550556065707580#Labeled dataAccuracy  RandomAODTEDLODBoundReferences
[1] A. D. Anthony Atkinson and R. Tobias. Optimum Experimental Designs. Oxford University
Press, 2007.
[2] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages
65–72, 2006.
[3] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463–482, 2002.
[4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework
for learning from labeled and unlabeled examples. Journal of Machine Learning Research,
7:2399–2434, 2006.
[5] A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without con-
straints. In NIPS, pages 199–207, 2010.
[6] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2:499–526, 2002.
[7] O. Chapelle, B. Sch ¨olkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press,
Cambridge, MA, 2006.
[8] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, February 1997.
[9] D. A. Cohn, L. E. Atlas, and R. E. Ladner.
Improving generalization with active learning.
Machine Learning, 15(2):201–221, 1994.
[10] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. In NIPS,
pages 705–712, 1994.
[11] M. A. Fanty and R. A. Cole. Spoken letter recognition. In NIPS, pages 220–226, 1990.
[12] G. H. Golub and C. F. V. Loan. Matrix computations (3rd ed.). Johns Hopkins University
Press, Baltimore, MD, USA, 1996.
[13] A. Guillory and J. Bilmes. Active semi-supervised learning using submodular functions. In
UAI, pages 274–282, 2011.
[14] S. Hanneke. Rates of convergence in active learning. The Annals of Statistics, 39(1):333–361,
2011.
[15] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of statistical learning: data mining,
inference, and prediction. New York: Springer-Verlag, 2001.
[16] X. He, W. Min, D. Cai, and K. Zhou. Laplacian optimal design for image retrieval. In SIGIR,
pages 119–126, 2007.
[17] B. Korte and J. Vygen. Combinatorial Optimization: Theory and Algorithms. Springer Pub-
lishing Company, Incorporated, 4th edition, 2007.
[18] G. Schohn and D. Cohn. Less is more: Active learning with support vector machines.
ICML, pages 839–846, 2000.
[19] S. Tong and D. Koller. Support vector machine active learning with applications to text classi-
ﬁcation. In ICML, pages 999–1006, 2000.
[20] K. Yu, J. Bi, and V. Tresp. Active learning via transductive experimental design. In ICML,
pages 1081–1088, 2006.
[21] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch ¨olkopf. Learning with local and global
consistency. In NIPS, 2003.
[22] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds and
harmonic functions. In ICML, pages 912–919, 2003.

In

9


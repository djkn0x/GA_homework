Stochastic optimization and sparse statistical
recovery: Optimal algorithms for high dimensions

Alekh Agarwal
Microsoft Research
New York NY
alekha@microsoft.com

Sahand N. Negahban
Dept. of EECS
MIT
sahandn@mit.edu

Martin J. Wainwright
Dept. of EECS and Statistics
UC Berkeley
wainwrig@stat.berkeley.edu

Abstract

We develop and analyze stochastic optimization algorithms for problems in which
the expected loss is strongly convex, and the optimum is (approximately) sparse.
Previous approaches are able to exploit only one of these two structures, yielding
a O(d/T ) convergence rate for strongly convex objectives in d dimensions and
O(ps(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm
is based on successively solving a series of ℓ1 -regularized optimization problems
using Nesterov’s dual averaging algorithm. We establish that the error of our solu-
tion after T iterations is at most O(s(log d)/T ), with natural extensions to approx-
imate sparsity. Our results apply to locally Lipschitz losses including the logistic,
exponential, hinge and least-squares losses. By recourse to statistical minimax
results, we show that our convergence rates are optimal up to constants. The ef-
fectiveness of our approach is also con ﬁrmed in numerical si mulations where we
compare to several baselines on a least-squares regression problem.

1 Introduction

Stochastic optimization algorithms have many desirable features for large-scale machine learning,
and have been studied intensively in the last few years (e.g., [18, 4, 8, 22]). The empirical efﬁciency
of these methods is backed with strong theoretical guarantees on their convergence rates, which
depend on various structural properties of the objective function. More precisely, for an objective
function that is strongly convex, stochastic gradient descent enjoys a convergence rate ranging from
O(1/T ), when features vectors are extremely sparse, to O(d/T ), when feature vectors are dense [9,
14, 10]. This strong convexity condition is satisﬁed for man y common machine learning problems,
including boosting, least squares regression, SVMs and generalized linear models among others.

A complementary condition is that of (approximate) sparsity in the optimal solution. Sparse models
have proven useful in many applications (see e.g., [6, 5] and references therein), and many statistical
procedures seek to exploit such sparsity. It has been shown [15, 19] that when the optimal solution θ∗
is s-sparse, appropriate versions of the mirror descent algorithm converge at a rate O(sp(log d)/T ).
Srebro et al. [20] exploit the smoothness of common loss functions, and obtain improved rates of
the form O(ηp(s log d)/T ), where η is the noise variance. While the √log d scaling makes these
methods attractive in high dimensions, their scaling with respect to the iterations T is relatively
slow —namely, O(1/√T ) as opposed to O(1/T ) for strongly convex problems.
Many optimization problems encountered in practice exhibit both features: the objective function is
strongly convex, and the optimum is (approximately) sparse. This fact leads to the natural question:
is it possible to design algorithms for stochastic optimization that enjoy the best features of both
types of structure? More speciﬁcally, an algorithm should h ave a O(1/T ) convergence rate, as well
as a logarithmic dependence on dimension. The main contribution of this paper is to answer this
question in the afﬁrmative, and to analyze a new algorithm th at has convergence rate O((s log d)/T )

1

for a strongly convex problem with an s-sparse optimum in d dimensions. This rate is unimprovable
(up to constants) in our setting, meaning that no algorithm can converge at a substantially faster rate.
Our analysis also yields optimal rates when the optimum is only approximately sparse.

The algorithm proposed in this paper builds off recent work on multi-step methods for strongly
convex problems [11, 10, 12], but involves some new ingredients so as to obtain optimal rates for
statistical problems with sparse optima. In particular, we form a sequence of objective functions
by decreasing the amount of regularization as the optimization algorithm proceeds which is quite
natural from a statistical viewpoint. Each step of our algorithm can be computed efﬁciently, with a
closed form update rule in many common examples. In summary, the outcome of our development
is an optimal one-pass algorithm for many structured statistical problems in high dimensions, and
with computational complexity linear in the sample size. Numerical simulations con ﬁrm our theo-
retical predictions regarding the convergence rate of the algorithm, and also establish its superiority
compared to regularized dual averaging [22] and stochastic gradient descent algorithms. They also
con ﬁrm that a direct application of the multi-step method of
Juditsky and Nesterov [11] is inferior
to our algorithm, meaning that our gradual decrease of regularization is quite critical. More details
on our results and their proofs can be found in the full-length version of this paper [2].

2 Problem set-up and algorithm description

Given a subset Ω ⊆ Rd and a random variable Z taking values in a space Z , we consider an
optimization problem of the form
θ∗ ∈ arg min
(1)
E[L(θ; Z )],
θ∈Ω
where L : Ω × Z → R is a given loss function. As is standard in stochastic optimization, we do
not have direct access to the expected loss function L(θ) := E[L(θ; Z )], nor to its subgradients.
Rather, for a given query point θ ∈ Ω, we observe a stochastic subgradient, meaning a random
vector g (θ) ∈ Rd such that E[g (θ)] ∈ ∂L(θ). The goal of this paper is to design algorithms that are
suitable for solving the problem (1) when the optimum θ∗ is (approximately) sparse.

(2)

Algorithm description:
In order to solve a sparse version of the problem (1), our strategy is to
consider a sequence of regularized problems of the form
θ∈Ω′ (cid:8)L(θ) + λkθk1(cid:9).
min
Our algorithm involves a sequence of KT different epochs, where the regularization parameter λ > 0
and the constraint set Ω′ ⊂ Ω change from epoch to epoch. The epochs are speciﬁed by:
• a sequence of natural numbers {Ti}KT
i=1 , where Ti speciﬁes the length of the ith epoch,
• a sequence of positive regularization weights {λi}KT
i=1 , and
i=1 and d-dimensional vectors {yi}KT
• a sequence of positive radii {Ri}KT
i=1 , which specify the con-
straint set, Ω(Ri ) := (cid:8)θ ∈ Ω | kθ − yi kp ≤ Ri(cid:9), that is used throughout the ith epoch.
We initialize the algorithm in the ﬁrst epoch with y1 = 0, and with any radius R1 that is an up-
per bound on kθ∗k1 . The norm k · kp used in de ﬁning the constraint set Ω(Ri ) is speciﬁed by
p = 2 log d/(2 log d − 1), a choice that will be clariﬁed momentarily.
7→ yi+1 , in such a way that we are guaranteed that
The goal of the ith epoch is to update yi
i+1 for each i = 1, 2, . . .. We choose the radii such that R2
i /2, so
kyi+1 − θ∗k2
1 ≤ R2
i+1 = R2
1 /2KT −1 . In order to update yi 7→ yi+1 , we run Ti rounds
that upon termination, kyKT − θ∗k2
1 ≤ R2
of the stochastic dual averaging algorithm [17] (henceforth DA) on the regularized objective
θ∈Ω(Ri ) (cid:8)L(θ) + λi kθk1(cid:9).
(3)
min
The DA method generates two sequences of vectors {µt}Ti
t=0 and {θt}Ti
t=0 initialized as µ0 = 0
and θ0 = yi , using a sequence of step sizes {αt}Ti
t=0 . At iteration t = 0, 1, . . . , Ti , we let g t be a
stochastic subgradient of L at θt , and we let ν t be any element of the subdifferential of the ℓ1 -norm
k · k1 at θt . The DA update at time t maps (µt , θt ) 7→ (µt+1 , θt+1 ) via the recursions
θ∈Ω(Ri ) (cid:8)αt+1 hµt+1 , θi + ψyi ,Ri (θ)(cid:9),
µt+1 = µt + g t + λi ν t , and θt+1 = arg min
(4)

2

.

(5)

ψyi ,Ri (θ) =

θt+1 = yi +

where the prox function ψ is speciﬁed below (5). The pseudocode describing the overal l procedure
is given in Algorithm 1. In the stochastic dual averaging updates (4), we use the prox function
2 log d
1
i (p − 1) kθ − yi k2
p , where p =
2R2
2 log d − 1
This particular choice of the prox-function and the speciﬁc value of p ensure that the function ψ
is strongly convex with respect to the ℓ1 -norm, and has been previously used for sparse stochastic
optimization (see e.g. [15, 19, 7]). In most of our examples, Ω = Rd and owing to our choice of the
prox-function and the feasible set in the update (4), we can compute θt+1 from µt+1 in closed form.
Some algebra yields that the update (4) with Ω = Rd is equivalent to
− 1(cid:27) .
, where ξ = max (cid:26)0,
|µt+1 |(q−1) sign(µt+1 )
R2
i αt+1
αt+1kµt+1kqRi
kµt+1k(q−2)
p − 1
(p − 1)(1 + ξ )
q
Here |µt+1 |(q−1) refers to elementwise operations and q = p/(p − 1) is the conjugate exponent to
p. We observe that our update (4) computes a subgradient of the ℓ1 -norm rather than computing an
exact prox-mapping as in some previous methods [16, 7, 22]. Computing such a prox-mapping for
yi 6= 0 requires O(d2 ) computation, which is why we adopt the update (4) with a complexity O(d).
Algorithm 1 Regularization Annealed epoch Dual AveRaging (RADAR)
Require: Epoch length schedule {Ti}KT
i=1 , initial radius R1 , step-size multiplier α, prox-function
ψ , initial prox-center y1 , regularization parameters λi .
for Epoch i = 1, 2, . . . , KT do
Initialize µ0 = 0 and θ0 = yi .
for Iteration t = 0, 1, . . . , Ti − 1 do
Update (µt , θt ) 7→ (µt+1 , θt+1 ) according to rule (4) with step size αt = α/√t.
end for
PTi
t=1 θ t
Set yi+1 =
.
Ti
Update R2
i /2.
i+1 = R2
end for
Return yKT +1

(6)

Conditions: Having de ﬁned our algorithm, we now discuss the conditions o n the objective func-
tion L(θ) and stochastic gradients that underlie our analysis.
Assumption 1 (Locally Lipschitz). For each R > 0, there is a constant G = G(R) such that
|L(θ) − L( ˜θ)| ≤ G kθ − ˜θk1
for all pairs θ, ˜θ ∈ Ω such that kθ − θ∗k1 ≤ R and k ˜θ − θ∗k1 ≤ R.
We note that it sufﬁces to have k∇L(θ)k∞ ≤ G(R) for the above condition. As mentioned, our
goal is to obtain fast rates for objectives satisfying a local strong convexity condition, de ﬁned below.
Assumption 2 (Local strong convexity (LSC)). The function L : Ω → R satisﬁes a R-local form
of strong convexity (LSC) if there is a non-negative constant γ = γ (R) such that
γ
2 kθ − ˜θk2
2 ∀θ, ˜θ ∈ Ω with kθk1 ≤ R and k ˜θk1 ≤ R.
L( ˜θ) ≥ L(θ) + h∇L(θ), ˜θ − θi +
Some of our results regarding stochastic optimization from a ﬁnite sample will use a weaker form of
the assumption, called local RSC, exploited in our recent work on statistics and optimization [1, 13].
Our ﬁnal assumption is a tail condition on the error in stocha stic gradients: e(θ) := g (θ) − E[g (θ)].
Assumption 3 (Sub-Gaussian stochastic gradients). There is a constant σ = σ(R) such that
E(cid:2) exp(ke(θ)k2
∞/σ2 )(cid:3) ≤ exp(1) for all θ such that kθ − θ∗ k1 ≤ R.
Clearly, this condition holds whenever the error vector e(θ) has bounded components. More gener-
ally, the bound (8) holds whenever each component of the error vector has sub-Gaussian tails.

(7)

(8)

3

Some illustrative examples: We now describe some examples that satisfy the above conditions to
illustrate how the various parameters of interest might be obtained in different scenarios.
Example 1 (Classiﬁcation under Lipschitz losses) . In binary classiﬁcation, the samples consist of
pairs z = (x, y ) ∈ Rd × {−1, 1}. Common choices for the loss function L(θ; z ) are the hinge loss
max(0, 1 − y hθ, xi) or the logistic loss log(1 + exp(−y hθ, xi). Given a distribution P over Z (either
the population or the empirical distribution), a common strategy is to draw (xt , yt ) ∼ P at iteration
t and use g t = ∇L(θ; (xt , yt )). We now illustrate how our conditions are satisﬁed in this se tting.
• Locally Lipschitz: Both the above examples actually satisfy a stronger global Lipschitz condition
since we have the bound G ≤ k∇L(θ)k∞ ≤ Ekxk∞ . Often, the data satisﬁes the normalization
kxk∞ ≤ B , in which case we get G ≤ B . More generally, tail conditions on the marginal
distribution of each coordinate of x ensure G = O(√log d)) is valid with high probability.
• LSC: When the expectation in the objective (1) is under the population distribution, the above
examples satisfy LSC. Here we focus on the example of the logistic loss, where we de ﬁne the link
function ψ(α) = exp(α)/(1 + exp(α))2 . We also de ﬁne Σ = E[xxT ] to be the covariance matrix
and let σmin (Σ) denote its minimum singular value. Then a second-order Taylor expansion yields
ψ(heθ , xi)
ψ(BR)σmin (Σ)
L( ˜θ) − L(θ) − h∇L(θ), ˜θ − θi =
kθ − ˜θk2
kΣ1/2 (θ − ˜θ)k2
2 ≥
2 ,
2
2
where eθ = aθ + (1 − a) ˜θ for some a ∈ (0, 1). Hence γ ≥ ψ(BR)σmin (Σ) in this example.
• Sub-Gaussian gradients: Assuming the bound Ekxk∞ ≤ B , this condition is easily veriﬁed. A
simple calculation yields σ = 2B , since
ke(θ)k∞ = k∇L(θ; (x, y )) − ∇L(θ)k∞ ≤ k∇L(θ; (x, y ))k∞ + k∇L(θ)k∞ ≤ 2B .
Example 2 (Least-squares regression). In the regression setup, we are given samples of the form
z = (x, y ) ∈ Rd × R. The loss function of interest is L(θ; (x, y )) = (y − hθ, xi)2 /2. To illustrate
the conditions more clearly, we assume that our samples are generated as y = hx, θ∗ i + w, where
2/2.
w ∼ N (0, η2 ) and ExxT = Σ so that EL(θ; (x, y )) = kΣ1/2 (θ − θ∗ )k2
• Locally Lipschitz: For this example, the Lipschitz parameter G(R) depends on the bound R.
If we de ﬁne ρ(Σ) = maxi Σii to be the largest variance of a coordinate of x, then a direct
calculation yields the bound G(R) ≤ ρ(Σ)R.
• LSC: Again we focus on the case where the expectation is taken under the population distribu-
tion, where we have γ = σmin (Σ).
• Sub-Gaussian gradients: Once again we assume that kxk∞ ≤ B . It can be shown with some
work that Assumption 3 is satisﬁed with σ2 (R) = 8ρ(Σ)2R2 + 4B 4R2 + 10B 2η2 .

3 Main results and their consequences

In this section we state our main results, regarding the convergence of Algorithm 1. We focus on
the cases where Assumptions 1 and 3 hold over the entire set Ω, and RSC holds uniformly for
all kθk1 ≤ R1 ; key examples being the hinge and logistic losses from Example 1. Extensions to
examples such as least-squares loss, which are not Lipschitz on all of Ω require a more delicate
treatment and these results as well the proofs of our results can be found in the long version [2].
Formally, we assume that G(R) ≡ G and σ(R) ≡ σ in Assumptions 1 and 3. We also use γ to
denote γ (R1 ) in Assumption 2. For a constant ω > 0 governing the error probability in our results,
we also de ﬁne ω 2
i = ω 2 + 24 log i at epoch i. Our results assume that we run Algorithm 1 with
Ti ≥ c1 (cid:20) s2
i σ2 (cid:1) + log d(cid:21) ,
i (cid:0)(G2 + σ2 ) log d + ω 2
γ 2R2
where c1 is a universal constant. For a total of T iterations in Algorithm 1, we state our results for
the parameter bθT = y(KT +1) where KT is the last epoch completed in T iterations.
3.1 Main theorem and some remarks
We start with our main result which shows an overall convergence rate of O(1/T ) after T iterations.
This O(1/T ) convergence is analogous to earlier work on multi-step methods for strongly convex

(9)

4

(11)

(13)

(14)

.

(12)

κT
log d

G(R) ≡ G,

objectives [11, 12, 10]. For each subset S ⊆ {1, 2, . . . , d} of cardinality s, we de ﬁne
ε2 (θ∗ ; S ) := kθ∗S c k2
(10)
1/s.
This quantity captures the degree of sparsity in the optimum θ∗ ; for instance, ε2 (θ∗ ; S ) = 0 if and
only if θ∗ is supported on S . Given the probability parameter ω > 0, we also de ﬁne the shorthand
κT = log2 (cid:20)
s2 ((G2 + σ2 ) log d + ω 2σ2 ) (cid:21) log d.
γ 2R2
1 T
Theorem 1. Suppose the expected loss L satisﬁes Assumptions 1 — 3 with parameters
γ and σ(R) ≡ σ , and we perform updates (4) with epoch lengths (9) and parameters
and α(t) = 5Ris
s√Ti q(G2 + σ2 ) log d + ω 2
Riγ
log d
λ2
i σ2
i =
(G2 + λ2
i + σ2 )t
Then for any subset S ⊆ {1, . . . , d} of cardinality s and any T ≥ 2κT , there is a universal constant
c0 such that with probability at least 1 − 6 exp(−ω 2/12) we have
2 ≤ c3 (cid:20) s
)) + ε2 (θ∗ ; S )(cid:21) .
kbθT − θ∗k2
((G2 + σ2 ) log d + σ2 (ω 2 + log
γ 2T
Consequently, the theorem predicts a convergence rate of O(1/γ 2T ) which is the best possible under
our assumptions. Under the setup of Example 1, the error bound of Theorem 1 further simpliﬁes to
2 = O (cid:18) sB 2
(log d + ω 2) + ε2 (θ∗ ; S )(cid:19) .
kbθT − θ∗ k2
γ 2T
We note that for an approximately sparse θ∗ , Theorem 1 guarantees convergence only to a toler-
ance ε2 (θ∗ ; S ) due to the error terms arising out of the approximate sparsity. Overall, the theorem
provides a family of upper bounds, one for each choice of S . The best bound can be obtained by
optimizing this choice, trading off the competing contributions of s and kθ∗S c k1 .
At this point, we can compare the result of Theorem 1 to some of the previous work. One approach
to minimize the objective (1) is to perform stochastic gradient descent on the objective, which has a
convergence rate of O(( eG2 + eσ2 )/(γ 2T )) [10, 14], where k∇L(θ)k2 ≤ eG and E exp (cid:16) ke(θ)k2
eσ2 (cid:17) ≤
2
exp(1). In the setup of Example 1, eG2 = Bd and similarly for eσ ; giving an exponentially worse
scaling in the dimension d. An alternative is to perform mirror descent [15, 19] or regularized dual
averaging [22] using the same prox-function as Algorithm 1 but without breaking it up into epochs.
As mentioned in the introduction, this single-step method fails to exploit the strong convexity of our
problem and obtains inferior convergence rates of O(splog d/T ) [19, 22, 7].
A proposal closer to our approach is to minimize the regularized objective (3), but with a ﬁxed
value of λ instead of the decreasing schedule of λi used in Theorem 1. This amounts to using the
method of Juditsky and Nesterov [11] on the regularized problem, and by using the proof techniques
developed in this paper, it can be shown that setting λ = σplog d/T leads to an overall convergence
γ 2T (log d + ω 2 )(cid:17), which exhibits the same scaling as Theorem 1. However, with this
rate of eO (cid:16) sB 2
ﬁxed setting of λ, the initial epochs tend to be much longer than needed for halving the error. Indeed,
our setting of λi is based on minimizing the upper bound at each epoch, and leads to an improved
performance in our numerical simulations. The bene ﬁts of sl owly decreasing the regularization in
the context of deterministic optimization were also noted in the recent work of Xiao and Zhang [23].

3.2 Some illustrative corollaries

We now present some consequences of Theorem 1 by making speciﬁc assumptions regarding the
sparsity of θ∗ . The simplest situation is when θ∗ is supported on some subset S of size s. More
generally, Theorem 1 also applies to the case when the optimum θ∗ is only approximately sparse.
One natural form of approximate sparsity is to assume that θ∗ ∈ Bq (Rq ) for 0 < q ≤ 1, where
|θi |q ≤ Rq) .
Bq (Rq ) := (θ ∈ Rd |
dXi=1
5

θ∗ is s-sparse,

θ∗ ∈ Bq (Rq ).

For 0 < q ≤ 1, membership in the set Bq (Rq ) enforces a decay rate on the components of the
vector θ. We now present a corollary of Theorem 1 under such an approximate sparsity condition.
To facilitate comparison with minimax lower bounds, we set ω 2 = δ log d in the corollaries.
Corollary 1. Under the conditions of Theorem 1, for all T > 2κT with probability at least 1 −
6 exp(−δ log d/12), there is a universal constant c0 such that
c0 h G2+σ2 (1+δ)
log d i
2 ≤ 
T + sσ2
s log d
γ 2T log κT
γ 2
2 (cid:21)
c0Rq (cid:20)n (G2+σ2 (1+δ)) log d
kbθT − θ∗k2
γ 2T (cid:17) 2−q
o 2−q
+ (cid:16) σ2
log κT
2
2
log d
q
γ 2T
((1+δ) log d)
The ﬁrst part of the corollary follows directly from Theorem 1 by noting that ε2 (θ∗ ; S ) = 0 under
our assumptions. Note that as q ranges over the interval [0, 1], re ﬂecting the degree of sparsity, the
convergence rate ranges from from eO(1/T ) (for q = 0 corresponding to exact sparsity) to eO(1/√T )
(for q = 1). This is a rather interesting trade-off, showing in a precise sense how convergence rates
vary quantitatively as a function of the underlying sparsity.
It is useful to note that the results on recovery for generalized linear models presented here exactly
match those that have been developed in the statistics literature [13, 21], which are optimal under our
assumptions on the design vectors. Concretely, ignoring factors of O(log T ), we get a parameter bθT
having error at most O(s log d/(γ 2T ) with an error probability decyaing to zero with d. Moreover,
in doing so our algorithm only goes over at most T data samples, as each stochastic gradient can be
evaluated with one fresh data sample drawn from the underlying distribution. Since the statistical
minimax lower bounds [13, 21] demonstrate that this is the smallest possible error that any method
can attain from T samples, our method is statistically optimal in the scaling of the estimation error
with the number of samples. We also observe that it is easy to instead set the error probability to
δ = ω 2 log T , if an error probability decaying with T is desired, incurring at most additional log T
factors in the error bound. Finally, we also remark that our techniques extend to handle examples
such as the least-squares loss that are not uniformly Lipschitz. The details of this extension are
deferred to the long version of this paper [2].

1
n

(15)

L(θ; Zi )

θ∗ = arg min
θ∈Ω

Stochastic optimization over ﬁnite pools:
A common setting for the application of stochastic op-
timization methods in machine learning is when one has a ﬁnit e pool of examples, say {Z1 , . . . , Zn},
and the objective (1) takes the form
nXi=1
In this setting, a stochastic gradient g (θ) can be obtained by drawing a sample Zj at random with
replacement from the pool {Z1 , . . . , Zn}, and returning the gradient ∇L(θ; Zj ).
In high-dimensional problems where d ≫ n, the sample loss is not strongly convex. However, it has
been shown by many researchers [3, 13, 1] that under suitable conditions, this objective does satisfy
restricted forms of the LSC assumption, allowing us to appeal to a generalized form of Theorem 1.
We will present this corollary only for settings where θ∗ is exactly sparse and also specialize to the
logistic loss, L(θ; (x, y )) = log(1 + exp(−y hθ, xi)) to illustrate the key aspects of the result. We
recall the de ﬁnition of the link function ψ(α) = exp(α)/(1 + exp(α))2 . We will state the result
for sub-Gaussian data design with parameters (Σ, η2
x ), meaning that the E[xi xT
i ] = Σ and hu, xi i is
ηx -sub-Gaussian for any unit norm vector u ∈ Rd .
Corollary 2. Consider the ﬁnite-pool loss
(15), based on n i.i.d. samples from a sub-Gaussian
design with parameters (Σ, η2
x ). Suppose that Assumptions 1-3 are satisﬁed and the optimum θ∗
of (15) is s-sparse. Then there are universal constants (c0 , c1 , c2 , c3 ) such that for all T ≥ 2κT and
log d
min (Σ), η4
min (Σ) max(σ2
x ), we have
n ≥ c3
σ2
ψ2 (2BR1 ) (cid:8)B 2 (1 + δ)(cid:9)o + c0
T n
sσ2
c0
s log d
1
kbθT − θ∗k2
2 ≤
σ2
σ2
min (Σ)ψ2 (2BR1 )T
min (Σ)
min (Σ)/η4
with probability at least 1 − 2 exp(−c1n min(σ2
x , 1)) − 6 exp(−δ log d/12).

log

κT
log d

.

6

We observe that the bound only holds when the number of samples n in the objective (15) is large
enough, which is necessary for the restricted form of the LSC condition to hold with non-trivial
parameters in the ﬁnite sample setting.

A modiﬁed method with constant epoch lengths:
Algorithm 1 as described is efﬁcient and sim-
ple to implement. However, the convergence results critically rely on the epoch length Ti to be
set appropriately in a doubling manner. This could be problematic in practice, where it might be
tricky to know when an epoch should be terminated. Following Juditsky and Nesterov [11], we
next demonstrate how a variant of our algorithm with constant epoch lengths enjoys similar rates of
convergence. The key challenge here is that unlike the previous set-up [11], our objective function
changes at each epoch which leads to signiﬁcant technical di fﬁculties. At a very coarse level, if
we have a total budget of T iterations, then this version of our algorithm allows us to set the epoch
lengths to O(log T ), and guarantees convergence rates that are O((log T )/T ).
Theorem 2. Suppose the expected loss satisﬁes Assumptions 1- 3 with par ameters G, γ , and σ
resp. Let S be any subset of {1, . . . , d} of cardinality s. Suppose we run Algorithm 1 for a total of
T iterations with epoch length Ti ≡ T log d/κT and with parameters as in Equation 12. Assuming
that this setting ensures Ti = O(log d), for any set S , with probability at least 1 − 3 exp(ω 2/12)
2 = O (cid:18)s
κ (cid:19) .
(G2 + σ2 ) log d + (ω 2 + log(κ/ log d))σ2
log d
kbθT − θ∗ k2
T
The theorem shows that up to logarithmic factors in T , setting the epoch lengths optimally is not
critical. A similar result can also be proved for the case of least-squares regression.

4 Simulations

In this section we will present numerical simulations that back our theoretical convergence results.
We focus on least-squares regression, discussed in Example 2. Speciﬁcally, we generate samples
(xt , yt ) with each coordinate of xt distributed as Unif[−B , B ] and yt = hθ∗ , xt i + wt . We pick θ∗
to be s-sparse vector with s = ⌈log d⌉, and wt ∼ N (0, η2 ) with η2 = 0.5. Given an iterate θt , we
generate a stochastic gradient of the expected loss (1) at (xt , yt ). For the ℓ1 -norm, we pick the sign
vector of θt , with 0 for any component that is zero, a member of the ℓ1 -sub-differential.
Our ﬁrst set of results evaluate Algorithm 1 against other st ochastic optimization baselines assuming
a complete knowledge of problem parameters. Speciﬁcally, w e epoch i is terminated once
p/2. This ensures that θ∗ remains feasible throughout, and tests the per-
p ≤ kyi − θ∗k2
kyi+1 − θ∗ k2
formance of Algorithm 1 in the most favorable scenario. We compare the algorithm against two
baselines. The ﬁrst baseline is the regularized dual averag ing (RDA) algorithm [22], applied to the
regularized objective (3) with λ = 4ηplog d/T , which is the statistically optimal regularization pa-
rameter with T samples. We use the same prox-function ψ(θ) = kθk2
2(p−1) , so that the theory for RDA
p
predicts a convergence rate of O(splog d/T ) [22]. Our second baseline is the stochastic gradient
(SGD) algorithm which exploits the strong convexity but not the sparsity of the problem (1). Since
the squared loss is not uniformly Lipschitz, we impose an additional constraint kθk1 ≤ R1 , without
which the algorithm does not converge. The results of this comparison are shown in Figure 1(a),
where we present the error kθt − θ∗ k2
2 averaged over 5 random trials. We observe that RADAR
comprehensively outperforms both the baselines, con ﬁrmin g the predictions of our theory.

The second set of results focuses on evaluating algorithms better tailored for our assumptions. Our
ﬁrst baseline here is the approach that we described in our re marks following Theorem 1. In this
approach we use the same multi-step strategy as Algorithm 1 but keep λ ﬁxed. We refer to this as
Epoch Dual Averaging (henceforth EDA), and again employ λ = 4ηplog d/T with this strategy.
Our epochs are again determined by halving of the squared ℓp -error measured relative to θ∗ .
Finally, we also evaluate the version of our algorithm with constant epoch lengths that we analyzed in
Theorem 2 (henceforth RADAR-CONST), using epochs of length log(T ). As shown in Figure 1(b),
the RADAR-CONST has relatively large error during the initial epochs, before converging quite

7

rapidly, a phenomenon consistent with our theory.1 Even though the RADAR-CONST method does
not use the knowledge of θ∗ to set epochs, all three methods exhibit the same eventual convergence
rates, with RADAR (set with optimal epoch lengths) performing the best, as expected. Although
RADAR-CONST is very slow in initial iterations, its convergence rate remains competitive with
EDA (even though EDA does exploit knowledge of θ∗ ), but is worse than RADAR as expected.
Overall, our experiments demonstrate that RADAR and RADAR-CONST have practical perfor-
mance consistent with our theoretical predictions. Although optimal epoch length setting is not
too critical for our approach, better data-dependent empirical rules for determining epoch lengths
remains an interesting question for future research. The relatively poorer performance of EDA
demonstrates the importance of our decreasing regularization schedule.

2 2
k
∗
θ
−
t
θ
k

6

5

4

3

2

1

0
 
0

Error vs. iterations

 

RADAR
SGD
RDA

0.5

1
Iterations

1.5

2
x 104

2 2
k
∗
θ
−
t
θ
k

5

4

3

2

1

0
 
0

Error vs. iterations

 

RADAR
EDA
RADAR-CONST

0.5

1
Iterations

1.5

2
x 104

(b)
(a)
Figure 1. A comparison of RADAR with other stochastic optimization algorithms for d = 40000 and
s = ⌈log d⌉. The left plot compares RADAR with the RDA and SGD algorithms, neither of which
exploits both the sparsity and the strong convexity structures simultaneously. The right one compares
RADAR with the EDA and RADAR-CONST algorithms, all of which exploit the problem structure
but with varying degrees of effectiveness. We plot kθt − θ∗ k2
2 averaged over 5 random trials versus the
number of iterations.

5 Discussion

In this paper we present an algorithm that is able to take advantage of the strong convexity and spar-
sity conditions that are satsiﬁed by many common problems in machine learning. Our algorithm
is simple and efﬁcient to implement, and for a d-dimensional objective with an s-sparse optima, it
achieves the minimax-optimal convergence rate O(s log d/T ). We also demonstrate optimal con-
vergence rates for problems that have weakly sparse optima, with implications for problems such as
sparse linear regression and sparse logistic regression. While we focus our attention exclusively on
sparse vector recovery due to space constraints, the ideas naturally extend to other structures such as
group sparse vectors and low-rank matrices. It would be interesting to study similar developments
for other algorithms such as mirror descent or Nesterov’s accelerated gradient methods, leading to
multi-step variants of those methods with optimal convergence rates in our setting.

Acknowledgements

The work of all three authors was partially supported by ONR MURI grant N00014-11-1-0688 to
MJW. In addition, AA was partially supported by a Google Fellowship, and SNN was partially
supported by the Yahoo KSC award.

1 To clarify, the epoch lengths in RADAR-CONST are set large enough to guarantee that we can attain an overall error bound of O(1/T ),
meaning that the initial epochs for RADAR-CONST are much longer than for RADAR. Thus, after roughly 500 iterations, RADAR-CONST
has done only 2 epochs and operates with a crude constraint set Ω(R1 /4). During epoch i, the step size scales proportionally to Ri /√t,
where t is the iteration number within the epoch; hence the relatively large initial steps in an epoch can take us to a bad solution even when
we start with a good solution yi when Ri is large. As Ri decreases further with more epochs, this effect is mitigated and the error of
RADAR-CONST does rapidly decrease like our theory predicts.

8

References

[1] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient methods
for high-dimensional statistical recovery. To appear in The Annals of Statistics, 2012. Full-length version
http://arxiv.org/pdf/1104.4824v2.
[2] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Stochastic optimization and sparse statistical recov-
ery: An optimal algorithm for high dimensions. 2012. URL http://arxiv.org/abs/1207.4421.
[3] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Ann.
Stat., 37(4):1705–1732, 2009.
[4] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS, 2007.
[5] P. B ¨uhlmann and S. Van De Geer. Statistics for High-Dimensional Data: Methods, Theory and Applica-
tions. Springer Series in Statistics. Springer, 2011.
[6] D. L. Donoho. High-dimensional data analysis: The curses and blessings of dimensionality, 2000.
[7] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceed-
ings of the 23rd Annual Conference on Learning Theory, pages 14–26. Omnipress, 2010.
[8] J. Duchi and Y. Singer. Efﬁcient online and batch learnin g using forward-backward splitting. Journal of
Machine Learning Research, 10:2873–2898, 2009.
[9] E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimiza-
tion. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2006.
[10] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic
strongly-convex optimization. Journal of Machine Learning Research - Proceedings Track, 19:421–436,
2011.
[11] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex func-
tions. Available online http://hal.archives-ouvertes.fr/docs/00/50/89/33/PDF/Strong-hal.pdf, 2010.
[12] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization, part ii: shrinking procedures and optimal algorithms. 2010.
[13] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uni ﬁed framework for high-dimensional
analysis of M-estimators with decomposable regularizers. In NIPS Conference, Vancouver, Canada, De-
cember 2009. Full length version arxiv:1010.2731v1.
[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[15] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization . Wiley, New
York, 1983.
[16] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center
for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL), 2007.
[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming A,
120(1):261–283, 2009.
[18] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In
Proceedings of the 24th International Conference on Machine Learning, 2007.
[19] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1 regularized loss minimization. Journal of
Machine Learning Research, 12:1865–1892, June 2011.
[20] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise, and fast rates. In Advances in Neural
Information Processing Systems 23, pages 2199–2207, 2010.
[21] S. A. van de Geer. High-dimensional generalized linear models and the lasso. The Annals of Statistics,
36:614–645, 2008.
[22] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of
Machine Learning Research, 11:2543–2596, 2010.
[23] L. Xiao and T. Zhang. A proximal-gradient homotopy method for the sparse least-squares problem.
ICML, 2012. URL http://arxiv.org/abs/1203.3002.

9


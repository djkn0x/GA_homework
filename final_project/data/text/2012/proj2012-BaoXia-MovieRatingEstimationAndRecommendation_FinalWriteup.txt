MOVIE RATING ESTIMATION AND RECOMMENDATION 
 
Zhouxiao Bao, Haiying Xia 
 

 

ABSTRACT 

 
In  this  paper,  we  build  a  movie  rating  prediction  system 
based  on  selected  training  sets  provided  by  MovieLens. 
Several  machine  learning  related  algorithms  –  baseline 
predictor, KNN, Stochastic Gradient Descent, SVD, SVD++, 
asymmetric  SVD,  integrated  model  and  NMTF  are  used  to 
predict  the  rating  from  particular  users  for  unrated  movies. 
RMSE  (Root-Mean-Square-Error)  is  applied  as  the  main 
criteria  to  evaluate  their  performance.  The  simulation  result 
shows  distinct  performance  due  to  selected  algorithms  as 
well as the corresponding learning rates. 
 

Index  Terms—  Movie  rating  predictor,  SVD,  RMSE, 
Matrix Factorization 
 

from  943  users  on  1682  movies,  in  which  each  user  has 
rated  at  least  20 movies. Among MovieLens  100k Data Set, 
we  choose  ua.base/test  and  ub.base/test 
for 
result 
comparison  between  different  training  sets  and  testing  sets; 
and  ua.base/test  for  model  comparison  between  different 
algorithms.  The  whole  set  u  data  is  split  into  a  training  set 
and  a  test  set with  exactly 10  ratings per user  in  the  test  set. 
The  sets  ua.test  and  ub.test  are  disjoint.  Also,  in  order  to 
apply  cross  validation  to  select  from  different  models,  we 
again  split  the  data  set  ua.base  into  10  small  disjoint  sets. 
This  allows  us  to  perform  10-fold  cross  validation  on 
ua1.base - ua10.base and evaluate  the  finally selected model 
on test set ua.test. 
Each  element (cid:1827)(cid:3048)(cid:3036) denotes  the  rating  scored  by  the  u-th  user 
The  training  and  testing  data  are  pre-processed  as  follows: 
we apply the database to build a U by I matrix A, where U is 
the  number  of  users,  and  I  is  the  number  of  rated  movies. 
for  the  i-th  movie.  It  is  easy  to  find  that  the  majority  of 
movies don’t obtain a sufficient number of ratings, and also, 
there  only  exist  common  ratings  for  general  user.  So  A  is  a 
very sparse matrix. 
 
A ∈ (cid:1337)(cid:2898)(cid:3179)(cid:3400)(cid:2898)(cid:3167) , where N(cid:2931)  is the number of users and N(cid:2919)  is the 
3. PROBLEM DESCRIPTION 
 
After pre-processing, we obtain a large user-item matrix 
A(cid:2931),(cid:2919) (cid:3404) (cid:3420)R (cid:2931)(cid:2919) ,
existent	rating
0,
no	such	rating  
total number of items (movies). So we have: 
 
prediction rating of user u on item i is T(cid:2931)(cid:2919) (cid:4666)R(cid:2931)(cid:2919) (cid:3404) 0(cid:4667). 
 
Thus our work is to fill in all the zero-entries in the matrix 
based on the training set of existing ratings. Assume the 
In this project the widely used RMSE (Root-Mean-Square 
RMSE (cid:3404) 	 (cid:3496) 1|S(cid:2930)(cid:2915)(cid:2929)(cid:2930) | (cid:3533)
Error) criteria is applied to evaluate the performance of each 
(cid:4666)R (cid:2931)(cid:2919) (cid:3398) T(cid:2931)(cid:2919) (cid:4667)(cid:2870)
algorithm, it could be calculated as: 
 
(cid:4666)(cid:2931),(cid:2919)(cid:4667)∈(cid:2903)(cid:3178)(cid:3163)(cid:3177)(cid:3178)
As is mentioned above, (cid:4666)u, i(cid:4667) ∈ S(cid:2930)(cid:2915)(cid:2929)(cid:2930)  actually means that 
 
 
user u has rated item i before. 
 
4. ALGORITHMS 

 
4.1 Baseline predictors 

1. INTRODUCTION 

 
Recommender  systems  provide  users  with  personalized 
suggestions  for  products  or  services.  They  are  becoming 
more  and  more  important  in  the  success  of  electronic 
commerce, and being utilized in various applications such as 
Amazon, YouTube  and Google news. Generally  speaking,  a 
recommendation system builds up items’ profiles, and users’ 
profiles  based  on  their  previous  behavior  recorded.  Then  it 
makes  a  prediction  on  the  rating  given  by  certain  user  on 
certain  item  which  he/she  has  not  yet  evaluated.  Based  on 
the prediction,  the system makes recommendations. Various 
techniques  for  recommendation  generation  have  been 
proposed  and 
successfully  deployed 
in  commercial 
environments, among which collaborative  filtering (CF) and 
content-based methods are most commonly used [1, 2].  
 
Movie  is  now  an  indispensable  entertainment  in  human  life. 
Most  video  websites  such  as  YouTube  and  Hulu  and  a 
number  of 
social  networks  allow  users 
rate  on 
videos/movies.  In  this  project,  we  build  a  movie  rating 
prediction  system  based  on  several  selected  training  sets, 
which  estimates 
the  movie  ratings  from  each  user. 
According  to  this  result,  we  are  able  to  make  personalized 
movie  recommendations  for  every  user,  which  would  more 
likely  satisfy  users’  expectation  and 
improve  user 
experience.  
 

2. DATASET AND PRE-PROCESSING 

 
Considering  about  the  simulation  efficiency,  we  choose 
MovieLens  100k  Data  Set1  as  our  training  and  testing  set. 
Such  set  consists  of  100,000  ratings  (rating  score  from  1-5) 

is  estimated  as  b(cid:2931)(cid:2919) (cid:3404) μ (cid:3397) b(cid:2931) (cid:3397) b(cid:2919)  ,  where  μ  is  the  overall 
We  use  a  simple  baseline  predictor  to  estimate  the  movie 
average  score, b(cid:2931)  and b(cid:2919)  is  the  training  deviations  of  user  u 
ratings  from  each particular  user. The  predictor  algorithm  is 
and  item  i,  respectively.  The  parameters,  b(cid:2931)  and  b(cid:2919)  are 
described  in  [2].  In  this  approach,  the  unknown  rating  score 
complexity  but  less  accurate.  We  choose λ(cid:2870) (cid:3404) 25, λ(cid:2871) (cid:3404) 10, 
which  are  parameters  applied  to  estimate  b(cid:2931)  and  b(cid:2919) , 
estimated  using  a  decoupling  method,  which  requires  less 
respectively.  
 
By applying ua.base, ub.base as two training sets, and 
ua.test, ub.test as two test sets for them, respectively.  
 
Running the baseline predictor, we get the result listed in 
Table 1. 

Table 1: Performance of Baseline 
 
ub Data 
ua Data 
 
It  shows  that  by  choosing  proper  parameters λ(cid:2870) , λ(cid:2871) ,  we  can 
RMSE 
0.96648 
0.97737 
different (cid:2019)(cid:2870) , (cid:2019)(cid:2871) ’s will result in distinct RMSE. Our goal here 
 
obtain  reasonable  results  on  rating  prediction.  Note  that 
is  just  to  provide  some  values  to  compare  with  following 
results,  so  only  one  pair  of  reasonable  parameters  is  used 
here. 
 
4.2 KNN (K-Nearest Neighbor) 
 
In  the  problem  that  we  apply  KNN  to  estimate  the  movie 
rates,  we  use  a  similarity  matrix  to  measure  the  “distance” 
between  each  item.  So,  this  method  should  be  more 
The  similarity  between  item i(cid:2869)  and  item i(cid:2870)  can  be  calculated 
precisely  named  as 
item-based  KNN  algorithm.  The 
approach  is  described  in  detail  in  [3].  To  measure  the 
similarity  between  items,  we  choose  Pearson  correlation. 
Similarity(cid:4666)i(cid:2869) , i(cid:2870)(cid:4667)
(cid:4666)R(cid:2931),(cid:2919)(cid:3117) (cid:3398) R(cid:3365) (cid:2931)(cid:4667)(cid:4666)R(cid:2931),(cid:2919)(cid:3118) (cid:3398) R(cid:3365) (cid:2931)(cid:4667)
∑
as follows: 
(cid:3404)
 
(cid:2931)∈(cid:2905)(cid:4666)(cid:2919)(cid:3117)(cid:4667)∩(cid:2905)(cid:4666)(cid:2919)(cid:3118)(cid:4667)
(cid:3435)R(cid:2931),(cid:2919)(cid:3118) (cid:3398) R(cid:3365) (cid:2931) (cid:3439)(cid:2870)
(cid:3435)R(cid:2931),(cid:2919)(cid:3117) (cid:3398) R(cid:3365) (cid:2931) (cid:3439)(cid:2870)
(cid:3495)(cid:4672)∑
(cid:4673) (cid:4672)∑
(cid:4673) 
(cid:2931)∈(cid:2905)(cid:4666)(cid:2919)(cid:3117)(cid:4667)∩(cid:2905)(cid:4666)(cid:2919)(cid:3118)(cid:4667)
(cid:2931)∈(cid:2905)(cid:4666)(cid:2919)(cid:3117)(cid:4667)∩(cid:2905)(cid:4666)(cid:2919)(cid:3118)(cid:4667)
Here, U(cid:4666)i(cid:2869) (cid:4667) is the set of all users who has rated on i(cid:2869)  before; 
U(cid:4666)i(cid:2870) (cid:4667)  is  the  set  of  all  users  who  has  rated  on  i(cid:2870) ; U(cid:4666)i(cid:2869) (cid:4667) ∩
U(cid:4666)i(cid:2870) (cid:4667) is  a  set  of  users  who  has  rated  both  items; R(cid:3365) (cid:2931)  is  the 
 
average rating of all ratings given by user u. 
 
Since  the  performance  depends  on  k,  number  of  nearest 
neighbors,  we  try  different  k’s  in  the  same  training  set,  and 
evaluate  the  corresponding  performance  by  measuring  the 
RMSE.  Using  two  database  ua  and  ub,  the  RMSE  curve 
based  on  different  k-values  is  shown  in  Fig.1(a),  while  the 
corresponding timing curve is illustrated in Fig.1(b). 

RMSE curve  for different  k -values  based on  two datasets

200

600

400

1200

1000
800
k  value
(a) 
Timing curve  for different  k -values  based on  two datasets

1400

1600

1800

 

1.14

1.12

1.1

1.08

1.06

1.04

1.02

1

0.98

0.96

0.94

0

420

400

380

360

340

320

300

E
S
M
R

e
m
i
t
 
d
e
s
p
a
l
E

280

0

600

400

200

1600

1400

1200

1800

1000
800
k  value
 
(b) 
Fig. 1 The (a) RMSE curve and (b) timing curve based on 
different k-values. 
 
From  the  above  curves,  we  are  able  to  choose  a  proper  k-
value  to achieve reasonable estimation performance. For  the 
particular  training  set  in  our  project,  an  appropriate  k-value 
is suggested ranging from 800 to 1300. 
RMSE based on different  k -values  for various  t raining subsets

1.1

 

E
S
M
R

1.05

1

Leave out  t raining set:  1
Leave out  t raining set:  2
Leave out  t raining set:  3
Leave out  t raining set:  4
Leave out  t raining set:  5
Leave out  t raining set:  6
Leave out  t raining set:  7
Leave out  t raining set:  8
Leave out  t raining set:  9
Leave out  t raining set:  10

0.95

 
0

200

400

600

800

1000

1200

1400

1600

1800

k-value

Fig. 2 RMSE curve based on different k-values for leaving out 
different training subsets. 

 
The  above  model  is  then  further  improved  by  using  10-fold 
cross  validation.  We  split  the  data  set  ua.base  into  10  sets 
and  perform  10-fold  cross  validation  on  ua1.base  - 
ua10.base.  The  RMSE  curve  in  terms  of  different  k-values 
on  the  10  training  subsets  is  illustrated  in  Fig.2.  The 

 

suggested  k-value  according  to  the  curves  is  ranging  from 
900    to  1000,  which  is  similar  to  the  result  without  cross 
validation.  The  smallest  RMSE  value  in  the  overall 
simulation result is 0.95534. 
 
4.3 Stochastic Gradient Descent 
 
(cid:1870)̂(cid:3048)(cid:3036) (cid:3404) (cid:1869)(cid:3036)(cid:3021) (cid:1868)(cid:3048) , 
The  rating  estimation  equation  for  Stochastic  Gradient 
Descent method could be written as: 
where (cid:1869)(cid:3036) ∈ (cid:2174)(cid:3038)  describes  the  overall  interests  of  the  users  to 
 
particular  items,  and  (cid:1868)(cid:3048) ∈ (cid:2174)(cid:3038)  illustrates  the  interests  one 
 
(cid:1857)(cid:3048)(cid:3036) (cid:3404) (cid:1870)(cid:3048)(cid:3036) (cid:3398) (cid:1870)̂(cid:3048)(cid:3036) ,  then  (cid:1869)(cid:3036) , (cid:1868)(cid:3048)  are  updated  using  gradient 
particular user has  for  the  items. Denote  the prediction error  
(cid:1869)(cid:3036) ≔ (cid:1869)(cid:3036) (cid:3397) (cid:2009)(cid:4666)(cid:1857)(cid:3048)(cid:3036) ⋅ (cid:1868)(cid:3048) (cid:3398) (cid:2010) ⋅ (cid:1869)(cid:3036) (cid:4667) 
(cid:1868)(cid:3048) ≔ (cid:1868)(cid:3048) (cid:3397) (cid:2009)(cid:4666)(cid:1857)(cid:3048)(cid:3036) ⋅ (cid:1869)(cid:3036) (cid:3398) (cid:2010) ⋅ (cid:1868)(cid:3048) (cid:4667)	
descent following the equations: 
 
where (cid:2009) , (cid:2010)  are learning rates, and  the initial values for	(cid:1869)(cid:3036) , (cid:1868)(cid:3048)  
 
(cid:2009) , (cid:2010) ,  we  select (cid:2009) (cid:3404) 0.01, (cid:2010) (cid:3404) 0.05,  the  RMSE  curve  with 
are  set  as  random  vectors with  entries  uniformly  distributed 
in  [0,1].  After  several  experiments  for  different  choice  of 
different k values is shown in Fig. 3. 
RMSE vs .  K -value curve us ing S tochas t ic  Gradient  descent
0.954
 

  =  0.01,   =  0.05

0.952

0.95

E
S
M
R

0.948

0.946

0.944

0.942

 
0

50

150

100

450

400

350

500

300
250
200
K  value (dimens ion)
 
Fig. 3 RMSE curve based on different k-values(dimension) 
using Stochastic Gradient Descent method 
 
 4.4	SVD	and	its	variants	
The RMSE curve shows a better performance than both 
	
baseline predictor and KNN. 
4.4.1 SVD 
 
The  SVD  method  could  be  considered  as  combination  of 
(cid:1870)̂(cid:3048)(cid:3036) (cid:3404) (cid:2020) (cid:3397) (cid:1854)(cid:3036) (cid:3397) (cid:1854)(cid:3048) (cid:3397) (cid:1869)(cid:3036)(cid:3021) (cid:1868)(cid:3048)  
baseline  predictor  and  Stochastic  Gradient  Descent.  The 
overall prediction equation is written as in [2]: 
 

 

with  (cid:2020) , (cid:1854)(cid:3036) , (cid:1854)(cid:3048) , (cid:1869)(cid:3036) , (cid:1868)(cid:3048)  defined  in  section  4.1  and  4.3.  In  our 
project, (cid:1854)(cid:3036) , (cid:1854)(cid:3048)  are  initialized  using  decoupling  method,  and 
(cid:1869)(cid:3036) , (cid:1868)(cid:3048) ’s  initial  values  remain  to  random  vectors.  Their 
(cid:1854)(cid:3048) ≔ (cid:1854)(cid:3048) (cid:3397) (cid:2009)(cid:2869) ⋅ (cid:4666)(cid:1857)(cid:3048)(cid:3036) (cid:3398) (cid:2010)(cid:2869) ⋅ (cid:1854)(cid:3048) (cid:4667)	
(cid:1854)(cid:3036) ≔ (cid:1854)(cid:3036) (cid:3397) (cid:2009)(cid:2869) ⋅ (cid:4666)(cid:1857)(cid:3048)(cid:3036) (cid:3398) (cid:2010)(cid:2869) ⋅ (cid:1854)(cid:3036) (cid:4667)	
updating rules are: 
(cid:1869)(cid:3036) ≔ (cid:1869)(cid:3036) (cid:3397) (cid:2009)(cid:2870) ⋅ (cid:4666)(cid:1857)(cid:3048)(cid:3036) ⋅ (cid:1868)(cid:3048) (cid:3398) (cid:2010)(cid:2870) ⋅ (cid:1869)(cid:3036) (cid:4667)	
 
(cid:1868)(cid:3048) ≔ (cid:1868)(cid:3048) (cid:3397) (cid:2009)(cid:2870) ⋅ (cid:4666)(cid:1857)(cid:3048)(cid:3036) ⋅ (cid:1869)(cid:3036) (cid:3398) (cid:2010)(cid:2870) ⋅ (cid:1868)(cid:3048) (cid:4667)	
In  our  project,  we  choose  the  learning  rate  α(cid:2869),(cid:2870) (cid:3404) 0.01,
β(cid:2869),(cid:2870) (cid:3404) 0.05 for SVD method.  	
 
adding  an  item  related  factor  vector (cid:1877)(cid:3036) ∈ (cid:2174)(cid:3038) .  As  described 
4.4.2 SVD++ 
 
(cid:1870)̂(cid:3048)(cid:3036) (cid:3404) (cid:2020) (cid:3397) (cid:1854)(cid:3036) (cid:3397) (cid:1854)(cid:3048) (cid:3397) (cid:1869)(cid:3036)(cid:3021) (cid:4666)(cid:1868)(cid:3048) (cid:3397) |(cid:1844)(cid:4666)(cid:1873)(cid:4667)|(cid:2879)(cid:2869)(cid:2870) (cid:3533) (cid:1877)(cid:3037) (cid:4667)
	
Based  on  SVD  method,  SVD++  improves  the  accuracy  by 
in [2], the prediction model becomes: 
(cid:3037)∈(cid:3019)(cid:4666)(cid:3048)(cid:4667)
where (cid:1844)(cid:4666)(cid:1873)(cid:4667) is defined as  the set  in which  the corresponding 
items  are  rated  by  user  (cid:1873) .  The  updating  equation  is  also 
 
is  chosen  as  α(cid:2869) (cid:3404) 0.009, (cid:2009)(cid:2870) (cid:3404) 0.005, (cid:2010)(cid:2869) (cid:3404) 0.045, (cid:2010)(cid:2870) (cid:3404)
0.05 for a relatively good performance. 
improved as provided in [2]. In our project, the learning rate 
 
4.4.3 Asymmetric SVD 
The  asymmetric  SVD  method  improves  the  base  SVD  as 
(cid:1870)̂(cid:3048)(cid:3036) (cid:3404) (cid:2020) (cid:3397) (cid:1854)(cid:3036) (cid:3397) (cid:1854)(cid:3048) (cid:3397) (cid:1869)(cid:3036)(cid:3021) ⋅ |(cid:1844)(cid:4666)(cid:1873)(cid:4667)|(cid:2879)(cid:2869)(cid:2870) ⋅ (cid:3533) (cid:4670)(cid:3435)(cid:1870)(cid:3048)(cid:3037) (cid:3398) (cid:1854)(cid:3048)(cid:3037) (cid:3439)(cid:1876)(cid:3037) (cid:3397) (cid:1877)(cid:3037) (cid:4671)
described  in  [6].  In  our  project,  we  revise  the  estimation 
A  relatively  better  learning  rate  set  is  chosen  as:  α(cid:2869),(cid:2870) (cid:3404)
equation as in order to reduce computing complexity: 
0.001, (cid:2010)(cid:2869),(cid:2870) (cid:3404) 0.01. 
(cid:3037)∈(cid:3019)(cid:4666)(cid:3048)(cid:4667)
 
 
The  RMSE  curves  based  on  different  k-values  (dimension) 
for SVD and its variants are in Fig. 4. 
 

RMSE vs .  k -value (dimens ion) curve us ing SVD and  its  variants

 

SVD
SVD++
Asymmetric  SVD

0.98

0.97

0.96

E
S
M
R

0.95

0.94

0.93

0.92

 
0

50

150

100

300
250
200
K value (dimens ion)
 
Fig. 4 RMSE curve based on different k-values(dimension) 
using SVD and its variants 

450

500

350

400

 

 
4.5 Global Neighborhood model 
 
This  model  allows  an  efficient  global  optimization  scheme. 
It  is  able  to  integrate  implicit  user  feedback.  We  abandon 
the  user-specific  weights  in  favor  of  global  weights  which 
  r(cid:3548)(cid:2931)(cid:2919) (cid:3404) μ (cid:3397) b(cid:2931) (cid:3397) b(cid:2919) (cid:3397) |R(cid:4666)u(cid:4667)|(cid:2879)(cid:2869)/(cid:2870) (cid:3533) (cid:3427)(cid:3435)r(cid:2931)(cid:2920) (cid:3398) b(cid:2931)(cid:2920) (cid:3439)w(cid:2919)(cid:2920) (cid:3397) c(cid:2919)(cid:2920) (cid:3431)
are independent of a specific user. The estimation is: 
(cid:2920)∈(cid:2902)(cid:4666)(cid:2931)(cid:4667)
The updating rule for w(cid:2919)(cid:2920)  and c(cid:2919)(cid:2920)  is: 
 
w(cid:2919)(cid:2920) ← w(cid:2919)(cid:2920) (cid:3397) γ (cid:3436)|R(cid:4666)u(cid:4667)|(cid:2879)(cid:2869)(cid:2870)e(cid:2931)(cid:2919) (cid:3435)r(cid:2931)(cid:2920) (cid:3398) b(cid:2931)(cid:2920)(cid:3439) (cid:3398) λw(cid:2919)(cid:2920)(cid:3440) 
c(cid:2919)(cid:2920) ← c(cid:2919)(cid:2920) (cid:3397) γ (cid:3436)|R(cid:4666)u(cid:4667)|(cid:2879)(cid:2869)(cid:2870)e(cid:2931)(cid:2919) (cid:3398) λc(cid:2919)(cid:2920) (cid:3440) 
 
γ (cid:3404) 0.007, λ (cid:3404) 0.019, we can get for uatest is:  
 
Apply  this  algorithm  on  ua  and    set  the  parameters  as: 
 
RMSE = 0.931926 
 
This  is  much  better  than  the  baseline  and  KNN  predictor, 
but is worse than those matrix factorization methods. 
 
4.6 Co-clustering by BVD (Block Value Decomposition) 
 
Recently,  traditional  data  clustering  methods  such  as  K-
means  has  been  applied  in  the  transformed  space.  Different 
(cid:1858)(cid:4666)(cid:2177), (cid:2158), (cid:2169)(cid:4667) (cid:3404) ‖(cid:2157) (cid:3398) (cid:2177)(cid:2158)(cid:2169)‖(cid:2870)  
from SVD, it introduces a block value matrix. The algorithm 
	
is given by the minimization of: 
where  (cid:1795) ∈ (cid:1337)(cid:1788)(cid:1821)(cid:3400)(cid:1811)  is  user-cluster  matrix,  (cid:1787) ∈ (cid:1337)(cid:2921)(cid:3400)(cid:2898)(cid:3167)  is  
 
movie-cluster matrix  and (cid:1776) ∈ (cid:1337)(cid:2921)(cid:3400)(cid:2921)  is  block  value matrix  (k 
is the number of clusters we select). 
 
Apparently, the objective function  is convex  in U, R and M. 
So we can derive  an EM  style  algorithm  that  converges  to  a 
(cid:4666)(cid:1775)(cid:1787)(cid:3021)(cid:1776)(cid:3021) (cid:4667)(cid:3036)(cid:3037)
local  minimum  by  iteratively  updating  the  decomposition 
(cid:1795)(cid:3036)(cid:3037) ← (cid:1795)(cid:3036)(cid:3037)
(cid:4666)(cid:1795)(cid:1776)(cid:1787)(cid:1787)(cid:3021) (cid:1776)(cid:3021) (cid:4667)(cid:3036)(cid:3037)  
using a set of multiplicative updating rules: 
(cid:4666)(cid:1795)(cid:3021)(cid:1775)(cid:1787)(cid:3021) (cid:4667)(cid:3036)(cid:3037)
 
(cid:1776)(cid:3036)(cid:3037) ← (cid:1776)(cid:3036)(cid:3037)
(cid:4666)(cid:1795)(cid:3021)(cid:1795)(cid:1776)(cid:1787)(cid:1787)(cid:3021) (cid:4667)(cid:3036)(cid:3037)  
(cid:4666)(cid:1776)(cid:3021)(cid:1795)(cid:3021) (cid:1775)(cid:4667)(cid:3036)(cid:3037)
(cid:1787)(cid:3036)(cid:3037) ← (cid:1787)(cid:3036)(cid:3037)
(cid:4666)(cid:1776)(cid:3021)(cid:1795)(cid:3021)(cid:1795)(cid:1776)(cid:1787)(cid:4667)(cid:3036)(cid:3037)  
 
Running  this  algorithm  for  different  values  of  k  until 
converge,  we  can  get  the  following  curve  of  RMSE  on 
dataset ua. 

Co-c lutering us ing BVD

0.934

0.932

0.93

0.928

0.926

0.924

0.922

0.92

0.918

E
S
M
R

0.916
20

55

60

25

30

35

45

50

40
 
K
Fig. 5 RMSE vs. k-values(dimension) for Co-clustering using 
BVD 
 
This  algorithm  successfully  captures  the  clustering  feature 
of  both  users  and movies  and  the  relationship  between  their 
clusters.  From  the  result, we  can  see  that  this  algorithm  can 
generate  better  performance  than  previous  methods.  It  can 
reach  a  smallest  RMSE  at  certain  K,  and  this  value,  we 
found, depends highly on the characteristic of the dataset. 
 

5. CONCLUSION 
 
In  our  project,  various  rating  prediction  algorithms  are 
applied  using  the  MovieLens  dataset.  The  methods  based 
matrix  factorization  outperform  the  other  ones,  which  only 
use  the  stochastic  information  of  the  training  database. 
Adding  the  block  value  matrix  can  further  improve  the 
performance. 

 
6. REFERENCES 

 
[1]  J.  Breese,  D.  Heckerman  and  C.  Kadie.  Empirical  Analysis  of 
Predictive  Algorithms  for  Collaborative  Filtering,  Technical 
Report of Microsoft Research, 1998. 
 
[2]  F.  Ricci,  L.  Rokach,  B.  Shapira,  P.  Kantor,  Recommender 
Systems Handbook. 
 
[3]  B.  Sarwar, G. Karypis,  J. Konstan  and  John  Riedl,  Item-Based 
Collaborative  Filtering  Recommendation  Algorithms,  Proceedings 
of  the  10th  international  conference  on  World  Wide  Web  2001: 
285-295. 
 
[4]  B.  Long,  Z.  Zhang  and  P.  Yu,  Co-clustering  by  Block  Value 
Decomposition, SIGKDD’05, August 21-24, 2005. 
 
[5] T. Huynh and D. Vu, Using Co-clustering for Predicting Movie 
Rating in Netflix. 
 
[6]  Y.  Koren,  Factorization  Meets 
the  Neighborhood:  a 
Multifaceted Collaborative Filtering Model, KDD’08. 
 


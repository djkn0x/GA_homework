Mandatory Leaf Node Prediction in
Hierarchical Multilabel Classiﬁcation

Wei Bi
James T. Kwok
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
{weibi,jamesk}@cse.ust.hk

Abstract

In hierarchical classiﬁcation, the prediction paths may be required to always end
at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is par-
ticularly useful when the leaf nodes have much stronger semantic meaning than
the internal nodes. However, while there have been a lot of MLNP methods in hi-
erarchical multiclass classiﬁcation, performing MLNP in hierarchical multilabel
classiﬁcation is much more difﬁcult. In this paper, we propose a novel MLNP
algorithm that (i) considers the global hierarchy structure; and (ii) can be used on
hierarchies of both trees and DAGs. We show that one can efﬁciently maximize
the joint posterior probability of all the node labels by a simple greedy algorithm.
Moreover, this can be further extended to the minimization of the expected sym-
metric loss. Experiments are performed on a number of real-world data sets with
tree- and DAG-structured label hierarchies. The proposed method consistently
outperforms other hierarchical and ﬂat multilabel classiﬁcation methods.

1

Introduction

In many real-world classiﬁcation problems, the output labels are organized in a hierarchy. For
example, gene functions are arranged in a tree in the Functional Catalog (FunCat) or as a directed
acyclic graph (DAG) in the Gene Ontology (GO) [1]; musical signals are organized in an audio
taxonomy [2]; and documents in the Wikipedia hierarchy. Hierarchical classiﬁcation algorithms,
which utilize these hierarchical relationships between labels in making predictions, often lead to
better performance than traditional non-hierarchical (ﬂat) approaches.
In hierarchical classiﬁcation, the labels associated with each pattern can be on a path from the root
to a leaf (full-path prediction); or stop at an internal node (partial-path prediction [3]). Following
the terminology in the recent survey [4], when only full-path predictions are allowed, it is called
mandatory leaf node prediction (MLNP); whereas when partial-path predictions are also allowed,
it is called non-mandatory leaf node prediction (NMLNP). Depending on the application and how
the label hierarchy is generated, either one of these prediction modes may be more relevant. For
example, in the taxonomies of musical signals [2] and genes [5], the leaf nodes have much stronger
semantic/biological meanings than the internal nodes, and MLNP is more important. Besides, some-
times the label hierarchy is learned from the data, using methods like hierarchical clustering [6],
Bayesian network structure learning [7] and label tree methods [8, 9]. In these cases, the internal
nodes are only artiﬁcial, and MLNP is again more relevant. In the recent Second Pascal Challenge
on Large-scale Hierarchical Text Classiﬁcation, the tasks also require MLNP.
In this paper, we focus on hierarchical multilabel classiﬁcation (HMC), which differs from hierar-
chical multiclass classiﬁcation in that the labels of each pattern can fall on a union of paths in the
hierarchy [10]. An everyday example is that a document/image/song/video may have multiple tags.
Because of its practical signiﬁcance, HMC has been extensively studied in recent years [1, 3, 10–12].

1

While there have been a lot of MLNP methods in hierarchical multiclass classiﬁcation [4], none of
these can be easily extended for the more difﬁcult HMC setting. They all rely on training a multiclass
classiﬁer at each node, and then use a recursive strategy to predict which subtree to pursue at the next
lower level. In hierarchical multiclass classiﬁcation, exactly one subtree is to be pursued; whereas
in HMC, one has to decide at each node how many and which subtrees to pursue. Even when this
can be performed (e.g., by adjusting the classiﬁcation threshold heuristically), it is difﬁcult to ensure
that all the prediction paths will end at leaf nodes, and so a lot of partial paths may be resulted.
Alternatively, one may perform MLNP by ﬁrst predicting the number of leaf labels (k) that the test
pattern has, and then pick the k leaf labels whose posterior probabilities are the largest. Prediction of
k can be achieved by using the MetaLabeler [13], though this involves another, possibly non-trivial,
learning task. Moreover, the posterior probability computed at each leaf l corresponds to a single
k leaf nodes; and then pick the one with the largest probability. However, as there are (cid:0)N
(cid:1) such
prediction path from the root to l. However, the target multilabel in HMC can have multiple paths.
Hence, a better approach is to compute the posterior probabilities of all subtrees/subgraphs that have
k
possible subsets (where N is the number of leafs), this can be expensive when N is large.
Recently, Cerri et al. [14] proposed the HMC-label-powerset (HMC-LP), which is specially de-
signed for MLNP in HMC. Its main idea is to reduce the hierarchical problem to a non-hierarchical
problem by running the (non-hierarchical) multilabel classiﬁcation method of label-powerset [15]
at each level of the hierarchy. However, this signiﬁcantly increases the number of “meta-labels”,
making it unsuitable for large hierarchies. Moreover, as it processes the hierarchy level-by-level,
this cannot be applied on DAGs, where “levels” are not well-deﬁned.
In this paper, we propose an efﬁcient algorithm for MLNP in both tree-structured and DAG-
structured hierarchical multilabel classiﬁcation. The target multilabel is obtained by maximizing
the posterior probability among all feasible multilabels. By adopting a weak “nested approxima-
tion” assumption, we show that the resultant optimization problem can be efﬁciently solved by a
greedy algorithm. Empirical results also demonstrate that this “nested approximation” assumption
holds in general. The rest of this paper is organized as follows. Section 2 describes the proposed
framework for MLNP on tree-structured hierarchies, which is then extended to DAG-structured hi-
erarchies in Section 3. Experimental results are presented in Section 4, and the last section gives
some concluding remarks.

2 Maximum a Posteriori MLNP on Label Trees
In this section, we assume that the label hierarchy is a tree T . With a slight abuse of notation,
we will also use T to denote the set of all the tree nodes, which are indexed from 0 (for the root),
1, 2, . . . , N . Let the set of leaf nodes in T be L. For a subset A ⊆ T , its complement is denoted by
Ac = T \A. For a node i, denote its parent by pa(i), and its set of children by child(i). Moreover,
given a vector y, yA is the subvector of y with indices from A.
In HMC, we are given a set of training examples {(x, y)}, where x is the input and y =
[y0 , . . . , yN ](cid:48) ∈ {0, 1}N +1 is the multilabel denoting memberships of x to each of the nodes. Equiv-
alently, y can be represented by a set Ω ⊆ T , such that yi = 1 if i ∈ Ω; and 0 otherwise. For y (or
Ω) to respect the tree structure, we require that yi = 1 ⇒ ypa(i) = 1 for any non-root node i ∈ T .
In this paper, we assume that for any group of siblings {i1 , i2 , . . . , im}, their labels are condition-
(cid:81)m
ally independent given the label of their parent pa(i1 ) and x, i.e., p(yi1 , yi2 , . . . yim |ypa(i1 ) , x) =
j=1 p(yij |ypa(i1 ) , x). This simpliﬁcation is standard in Bayesian networks and also commonly
(cid:89)
used in HMC [16, 17]. By repeated application of the probability product rule, we have
p(yi | ypa(i) , x).
p(y0 , . . . , yN |x) = p(y0 |x)
i∈T \{0}

(1)

2.1 Training
With the simpliﬁcation in (1), we only need to train estimators for p(yi = 1 | ypa(i) = 1, x), i ∈
T \{0}. The algorithms to be proposed are independent of the way these probability estimators are
learned. In the experiments, we train a multitask lasso model for each group of sibling nodes, using
those training examples that their shared parent is labeled positive.

2

(4)

(5)

(2)

where

2.2 Prediction
For maximum a posteriori MLNP of a test pattern x, we want to ﬁnd the multilabel Ω∗ that (i)
maximizes the posterior probability in (1); and (ii) respects T . Suppose that it is also known that x
has k leaf labels. The prediction task is then:
Ω∗ = maxΩ p(yΩ = 1, yΩc = 0 | x)
y0 = 1, k of the leaves in L are labeled 1,
s.t.
Ω contains no partial path,
(3)
all yi ’s respect the label hierarchy.
Note that p(yΩ = 1, yΩc = 0 | x) considers all the node labels in the hierarchy simultaneously.
In contrast, as discussed in Section 1, existing MLNP methods in hierarchical multiclass/multilabel
classiﬁcation only considers the hierarchy information locally at each node.
Associate an indicator function ψ : T → {0, 1}N +1 with Ω, such that ψi ≡ ψ(i) = 1 if i ∈ Ω, and
0 otherwise. The following Proposition shows that (2) can be written as an integer linear program.
(cid:88)
Proposition 1. For a label tree, problem (2) can be rewritten as
s.t. (cid:88)
maxψ
wiψi
i∈T
ψi = k , ψ0 = 1, ψi ∈ {0, 1} ∀i ∈ T ,
(cid:88)
i∈L
ψj ≥ 1 ∀i ∈ Lc : ψi = 1,
j∈child(i)
ψi ≤ ψpa(i) ∀i ∈ T \{0},

(cid:80)
l∈child(i) log(1 − pl )
log pi − log(1 − pi ) + (cid:80)
i = 0
i ∈ L
log pi − log(1 − pi )
(6)
wi =
i ∈ Lc\{0} ,
l∈child(i) log(1 − pl )
and pi ≡ p(yi = 1 | ypa(i) = 1, x).
Problem (4) has (cid:0)|L|
(cid:1) candidate solutions, which can be expensive to solve when T is large. In
k
the following, we will extend the nested approximation property (NAP), ﬁrst introduced in [18] for
model-based compressed sensing, to constrain the optimal solution.
Deﬁnition 1 (k-leaf-sparse). A multilabel y is k-leaf-sparse if k of the leaf nodes are labeled one.
Deﬁnition 2 (Nested Approximation Property (NAP)). For a pattern x, let its optimal k-leaf-sparse
multilabel be Ωk . The NAP is satisﬁed if {i : i ∈ Ωk } ⊂ {i : i ∈ Ωk(cid:48) } for all k < k (cid:48) .
Note that NAP is often implicitly assumed in many HMC algorithms. For example, consider the
common approach that trains a binary classiﬁer at each node and recursively predicts from the root to
the subtrees. When the classiﬁcation threshold at each node is high, prediction stops early; whereas
when the threshold is lowered, prediction can go further down the hierarchy. Hence, nodes that
are labeled positive at a high threshold will always be labeled at a lower threshold, implying NAP.
Another example is the CSSA algorithm in [11]. Since it is greedy, a larger solution (with more
labels predicted positive) always includes the smaller solutions.
Algorithm 1 shows the proposed algorithm, which will be called MAS (MAndatory leaf node pre-
diction on Structures). Similar to [11], Algorithm 1 is also greedy and based on keeping track of the
supernodes. However, the deﬁnition of a supernode and its updating are different. Each node i ∈ T
is associated with the weight wi in (6). Initially, only the root is selected (ψ0 = 1). For each leaf l
in L, we create a supernode, which is a subset in T containing all the nodes on the path from l to the
is deﬁned as SNV(S ) = (cid:80)
root. Given |L| leaves in T , there are initially |L| supernodes. Moreover, all of them are unassigned
(i.e., each contains an unselected leaf node). Each supernode S has a supernode value (SNV) which
i∈S wi .

3

Algorithm 1 MAS (Mandatory leaf node prediction on structures).
1: Initialization: Initialize every node (except the root) with ψi ← 0; Ω ← {0}; Create a supernode
from each leaf with its ancestors.
2: for iteration=1 to k do
select the unassigned supernode S ∗ with the largest SNV;
3:
assign all unselected nodes in S ∗ with ψi ← 1;
4:
Ω ← Ω ∪ S ∗ ;
5:
for each unassigned supernode S do
6:
update the SNV of S (using Algorithm 2 for trees and Algorithm 3 for DAGs);
7:
end for
8:
9: end for

In each iteration, supernode S ∗ with the largest SNV is selected among all the unassigned supern-
will take if S is merged with Ω, i.e., SNV(S ) ← (cid:80)
i∈S∪Ω wi = (cid:80)
odes. S ∗ is then assigned, with the ψi ’s of all its constituent nodes set to 1, and Ω is updated
accordingly. For each remaining unassigned supernode S , we update its SNV to be the value that it
i∈S\Ω wi + SNV(Ω). Since each
unassigned S contains exactly one leaf and we have a tree structure, this update can be implemented
efﬁciently in O(h2 ) time, where h is the height of the tree (Algorithm 2).

Algorithm 2 Updating the SNV of an
unassigned tree supernode S , contain-
ing the leaf l.
1: node ← l;
2: SNV(S ) ← SNV(Ω);
3: repeat
SNV(S ) ← SNV(S ) + wnode ;
4:
node ← pa(node);
5:
6: until node ∈ Ω.

Algorithm 3 Updating the SNV of an unassigned DAG
supernode S , containing the leaf l.
1: insert l to T ;
2: SNV(S ) ← SNV(Ω);
3: repeat
node ← ﬁnd-max(T );
4:
delete node from T ;
5:
SNV(S ) ← SNV(S ) + wnode ;
6:
insert nodes in Pa(node)\(Ω ∪ T ) to T ;
7:
8: until T = ∅.

The following Proposition shows that MAS ﬁnds the best k-leaf-sparse prediction.
Proposition 2. Algorithm 1 obtains an optimal ψ solution of (4) under the NAP assumption.
Finally, we study the time complexity of Algorithm 1. Step 3 takes O(|L|) time; steps 4 and 5 take
O(h) time; and updating all the remaining unassigned supernodes takes O(h2 |L|) time. Therefore,
O(h2k |L|). In contrast, a brute-force search will take (cid:0)|L|
(cid:1) time.
each iteration takes O(h2 |L|) time, and the total time to obtain an optimal k-leaf-sparse solution is
k
2.2.1 Unknown Number of Labels

In practice, the value of k may not be known. The straightforward approach is to run Algorithm 1
with k = 1, . . . , |L|, and ﬁnd the Ωk ∈ {Ω1 , . . . , Ω|L| } that maximizes the posterior probability in
(1). However, recall that Ωk ⊂ Ωk+1 under the NAP assumption. Hence, we can simply set k = |L|,
and Ωi is immediately obtained as the Ω in iteration i. The total time complexity is O(h2 |L|2 ). In
contrast, a brute-force search takes O(2|L| ) time when k is unknown.

2.3 MLNP that Minimizes Risk

While maximizing the posterior probability minimizes the 0-1 loss, another loss function that has
been popularly used in hierarchical classiﬁcation is the H-loss [12]. However, along each prediction
path, H-loss only penalizes the ﬁrst classiﬁcation mistake closest to the root. On the other hand, we
are more interested in the leaf nodes in MLNP. Hence, we will adopt the symmetric loss instead,
which is deﬁned as (cid:96)(Ω, ˚Ω) = |Ω\˚Ω| + |˚Ω\Ω|, where ˚Ω is the true multilabel for the given x, and
Ω is the prediction. However, this weights mistakes in any part of the hierarchy equally; whereas in
HMC, a mistake that occurs at the higher level of the hierarchy is usually considered more crucial.

4

(7)

(8)

(9)

(cid:96)(Ω, ˚Ω) =

p(yi | yPa(i) , x),

p(y0 , y1 , . . . , yN |x) = p(y0 |x)

incorporate the hierarchy structure into (cid:96)(Ω, ˚Ω) by extending it as (cid:80)
Let I (·) be the indicator function that returns 1 when the argument holds, 0 otherwise. We thus
i ci I (i ∈ Ω\˚Ω)+ci I (i ∈ ˚Ω\Ω),
where c0 = 1, ci = cpa(i)/nsibl(i) as in [3], and nsibl(i) is the number of siblings of i (including i
itself). Finally, one can also allow different relative importance (α ≥ 0) for the false positives and
(cid:88)
negatives, and generalize (cid:96)(Ω, ˚Ω) further as
i I (i ∈ Ω\˚Ω) + c−
i I (i ∈ ˚Ω\Ω),
c+
i
1+α and c−
where c+
1+α .
i = 2ci
i = 2αci
(cid:80)
Given a loss function (cid:96)(·, ·), from Bayesian decision theory, the optimal multilabel Ω∗ is the one that
˚Ω (cid:96)(Ω, ˚Ω) p(y˚Ω = 1, y˚Ωc = 0|x). The proposed
minimizes the expected loss: Ω∗ = arg minΩ
formulation can be easily extended for this. The following Proposition shows that it leads to a
problem very similar to (4). Extension to a DAG-structured label hierarchy is analogous.
Proposition 3. With a label tree and the loss function in (7), the optimal Ω∗ that minimizes the
i )p(yi = 1|x) − c+
i + c−
expected loss can be obtained by solving (4), but with wi = (c+
i .
3 Maximum a Posteriori MLNP on Label DAGs
When the label hierarchy is a DAG G , on using the same conditional independence simpliﬁcation in
(cid:89)
Section 2, we have
i∈G\{0}
where Pa(i) is the set of parents of node i. The prediction task involves the same optimization
problem as in (2). However, there are now two interpretations on how the labels should respect the
DAG in (3) [1, 11]. The ﬁrst one requires that if a node is labeled positive, all its parents must also
be positive. In bioinformatics, this is also called the true path rule that governs the DAG-structured
GO taxonomy on gene functions. The alternative is that a node can be labeled positive if at least one
of its parents is positive. Here, we adopt the ﬁrst interpretation which is more common.
A direct maximization of p(y0 , y1 , . . . , yN |x) by (8) is NP-hard [19]. Moreover, the size of each
probability table p(yi |yPa(i) , x) in (8) grows exponentially with |Pa(i)|. Hence, it can be both im-
(cid:89)
(cid:89)
practical and inaccurate when G is large and the sample size is limited. In the following, we assume
p(y0 |x)
p(yi | yj , x),
p(y0 , y1 , . . . , yN |x) =
1
n(x)
j∈Pa(i)
i∈G\{0}
where n(x) is a normalization term. This follows from the approach of composite likelihood (or
pseudolikelihood) [20] which replaces a difﬁcult probability density function by a set of marginal or
conditional events that are easier to evaluate. In particular, (9) corresponds to the so-called pairwise
conditional likelihood that has been used in longitudinal studies and bioinformatics [21]. Composite
likelihood has been successfully used in different applications such as genetics, spatial statistics
and image analysis. The connection between composite likelihood and various (ﬂat) multilabel
classiﬁcation models is also recently discussed in [21]. Moreover, by using (9), the 2|Pa(i)| numbers
in the probability table p(yi |yPa(i) , x) are replaced by the |Pa(i)| numbers in {p(yi |yj , x)}j∈Pa(i) ,
and thus the estimates obtained are much more reliable. The following Proposition shows that
maximizing (9) can be reduced to a problem similar to (4).
(cid:88)
Proposition 4. With the assumption (9), problem (2) for the label DAG can be rewritten as
s.t. (cid:88)
maxψ
wiψi
i∈G
ψi = k , ψ0 = 1, ψi ∈ {0, 1} ∀i ∈ G ,
(cid:88)
i∈L
ψj ≥ 1 ∀i ∈ Lc : ψi = 1,
j∈child(i)
ψi ≤ ψj ∀j ∈ Pa(i), ∀i ∈ G \{0},

(10)

(11)

5

(cid:80)

(cid:80)
l∈child(0) log(1 − pl0 )
j∈Pa(i) (log pij − log(1 − pij )) + (cid:80)
(cid:80)
i = 0,
j∈Pa(i) (log pij − log(1 − pij ))
i ∈ L,
where wi=
l∈child(i) log(1 − pli ) i ∈ Lc\{0},
and pij ≡ p(yi = 1|yj = 1, x) for j ∈ Pa(i).
Problem (10) is similar to problem (4), except in the deﬁnition of wi and that the hierarchy constraint
(11) is more general than (5). When the DAG is indeed a tree, (10) reduces to (4), and Proposition 4
reduces to Proposition 1. When k is unknown, the same procedure in Section 2.2.1 applies.
In the proof of Proposition 2, we do not constrain the number of parents for each node. Hence, (10)
can be solved efﬁciently as before, except for two modiﬁcations: (i) Each initial supernode now
contains a leaf and its ancestors along all paths to the root. (ii) Since Pa(i) is a set and the hierarchy
is a DAG, updating the SNV gets more complicated. In Algorithm 3, T is a self-balancing binary
search tree (BST) that keeps track of the nodes in S \Ω using their topological order1 . To facilitate
the checking of whether a node is in Ω (step 7), Ω also stores its nodes in a self-balancing BST.
Recall that for a self-balancing BST, the operations of insert, delete, ﬁnd-max and ﬁnding an element
all take O(log V ) time, where V ≤ N is the number of nodes in the BST. Hence, updating the SNV
of one supernode by Algorithm 3 takes O(N log N ) time. As O(|L|) supernodes need to be updated
in each iteration of Algorithm 1, this step (which is the most expensive step in Algorithm 1) takes
O(|L| · N log N ) time. The total time for Algorithm 1 is O(k · |L| · N log N ).
4 Experiments

In this section, experiments are performed on a number of benchmark multilabel data sets2 , with
both tree- and DAG-structured label hierarchies (Table 1). As pre-processing, we remove examples
that contain partial label paths and nodes with fewer than 10 positive examples. At each parent node,
we then train a multitask lasso model with logistic loss using the MALSAR package [22].

4.1 Classiﬁcation Performance

The proposed MAS algorithm is compared with HMC-LP [14], the only existing algorithm that can
perform MLNP on trees (but not on DAGs). We also compare with the combined use of MetaLabeler
[13] and NMLNP methods as described in Section 1. These NMLNP methods include (i) HBR,
which is modiﬁed from the hierarchical classiﬁer H-SVM [3], by replacing its base learner SVM
with the multitask lasso as for MAS; (ii) CLUS-HMC [1]; and (iii) ﬂat BR [23], which is a popular
MLNP method but does not use the hierarchy information. For performance evaluation, we use
the hierarchical F-measure (HF) which has been commonly used in hierarchical classiﬁcation [4].
Results based on 5-fold cross-validation are shown in Table 1. As can be seen, MAS is always
among the best on almost all data sets.
Next, we compare the methods using the loss in (7), where the relative importance for false positives
vs negatives (α) is set to be the ratio of the numbers of negative and positive training labels. Results
are shown in Table 2. As can be seen, the risk-minimizing version (MASR) can always obtain the
smallest loss. We also vary α in the range { 1
2 , 1, 2, · · · , 9, 10}. As can be seen from
10 , 1
9 , . . . , 1
Figure 1, MASR consistently outperforms the other methods, sometimes by a signiﬁcant margin.
Finally, Figure 2 illustrates some example query images and their misclassiﬁcations by MAS, MASR
and BR on the caltech101 data set. As can be seen, even when MAS/MASR misclassiﬁes the image,
the hierarchy often helps to keep the prediction close to the true label.
4.2 Validating the NAP Assumption

In this section, we verify the validity of the NAP assumption. For each test pattern, we use brute-
force search to ﬁnd its best k-leaf-sparse prediction, and check if it includes the best (k − 1)-leaf-
sparse prediction. As brute-force search is very expensive, experiments are only performed on four

1We number the sorted order such that nodes nearer to the root are assigned smaller values. Note that the
topological sort only needs to be performed once as part of pre-processing.
2Downloaded from http://mulan.sourceforge.net/datasets.html and http://dtai.
cs.kuleuven.be/clus/hmcdatasets/

6

Table 1: HF values obtained by the various methods on all data sets. The best results and those
that are not statistically worse (according to paired t-test with p-value less than 0.05) are in bold.
HMC-LP and CLUS-HMC cannot be run on the caltech101 data, which is large and dense.

data set
rcv1v2 subset1
rcv1v2 subset2
rcv1v2 subset3
rcv1v2 subset4
rcv1v2 subset5
delicious
enron
wipo
caltech-101
seq (funcat)
pheno (funcat)
struc (funcat)
hom (funcat)
cellcycle (funcat)
church (funcat)
derisi (funcat)
eisen (funcat)
gasch1 (funcat)
gasch2 (funcat)
spo (funcat)
expr (funcat)
seq (GO)
pheno (GO)
struc (GO)
hom (GO)
cellcycle (GO)
church (GO)
derisi (GO)
eisen (GO)
gasch1 (GO)
gasch2 (GO)
spo (GO)
expr (GO)

#pattern
4422
4485
4513
4569
4452
768
1607
569
9144
1115
330
1065
1124
1080
1104
995
768
1038
1076
1053
1109
518
227
505
507
484
511
492
404
512
508
494
504

#leaf
42
43
46
44
45
49
24
21
102
36
14
33
35
33
35
33
29
32
33
32
32
32
19
33
29
29
28
31
28
32
32
32
35

avg #leaf
per
pattern
1.3
1.3
1.3
1.3
1.4
5.4
2.6
1
1
1.8
1.6
1.8
1.8
1.9
1.8
1.8
1.8
1.8
1.8
1.8
1.8
3.6
3.5
3.5
3.2
3.1
3.2
3.4
3.4
3.4
3.3
3.3
3.5

(hierarchical)
(with MetaLabeler)
MAS HMC-LP HBR CLUS-HMC
0.85
0.63
0.83
0.22
0.85
0.64
0.84
0.21
0.85
0.20
0.83
0.63
0.86
0.64
0.84
0.21
0.84
0.63
0.83
0.21
0.57
0.28
0.23
0.53
0.75
0.68
0.74
0.72
0.83
0.83
0.42
0.71
0.82
0.82
-
-
0.26
0.25
0.26
0.15
0.25
0.25
0.20
0.12
0.25
0.23
0.21
0.03
0.35
0.36
0.21
0.27
0.21
0.20
0.19
0.12
0.20
0.18
0.05
0.17
0.21
0.18
0.08
0.18
0.28
0.29
0.28
0.10
0.29
0.25
0.11
0.23
0.25
0.24
0.22
0.05
0.23
0.18
0.10
0.18
0.28
0.25
0.25
0.12
0.52
-
0.58
0.59
0.57
0.49
0.53
-
0.55
0.48
-
0.51
0.65
0.59
0.60
-
0.51
0.49
-
0.49
0.57
-
0.50
0.53
0.56
0.53
0.49
-
0.57
0.54
0.48
0.64
0.57
0.56
0.55
0.51
0.50
0.50
0.47
0.49
0.55
0.57
0.49

-
-
-
-

(ﬂat)

BR
0.83
0.84
0.83
0.84
0.83
0.54
0.74
0.83
0.70
0.23
0.23
0.24
0.36
0.19
0.17
0.18
0.27
0.22
0.25
0.18
0.27
0.61
0.55
0.53
0.63
0.51
0.54
0.54
0.57
0.58
0.53
0.51
0.60

smaller data sets for k = 2, . . . , 10. Figure 3 shows the percentage of test patterns satisfying the
NAP assumption at different values of k . As can be seen, the NAP holds almost 100% of the time.
5 Conclusion

In this paper, we proposed a novel hierarchical multilabel classiﬁcation (HMC) algorithm for manda-
tory leaf node prediction. Unlike many hierarchical multilabel/multiclass classiﬁcation algorithms,
it utilizes the global hierarchy information by ﬁnding the multilabel that has the largest posterior
probability over all the node labels. By adopting a weak “nested approximation” assumption, which
is already implicitly assumed in many HMC algorithms, we showed that this can be efﬁciently
optimized by a simple greedy algorithm. Moreover, it can be extended to minimize the risk asso-
ciated with the (hierarchically weighted) symmetric loss. Experiments performed on a number of
real-world data sets demonstrate that the proposed algorithms are computationally simple and more
accurate than existing HMC and ﬂat multilabel classiﬁcation methods.
Acknowledgment

This research has been partially supported by the Research Grants Council of the Hong Kong Special
Administrative Region under grant 614012.

7

(a) rcv1subset1
(c) struc(funcat)
(b) enron
Figure 1: Hierarchically weighted symmetric loss values (7) for different α’s.

Figure 2: Example misclassiﬁcations on the caltech101 data set.

Table 2: Hierarchically weighted symmetric loss values (7) on the tree-structured data sets.

data set
rcv1v2 subset1
rcv1v2 subset2
rcv1v2 subset3
rcv1v2 subset4
rcv1v2 subset5
delicious
enron
wipo
caltech-101
seq (funcat)
pheno (funcat)
struc (funcat)
hom (funcat)
cellcycle (funcat)
church (funcat)
derisi (funcat)
eisen (funcat)
gasch1 (funcat)
gasch2 (funcat)
spo (funcat)
expr (funcat)

(used with MetaLabeler)
BR
MASR MAS HMC-LP HBR CLUS-HMC
0.05
0.10
0.46
0.12
0.20
0.13
0.04
0.12
0.19
0.11
0.45
0.09
0.04
0.12
0.20
0.11
0.45
0.09
0.04
0.11
0.19
0.11
0.44
0.10
0.05
0.12
0.20
0.11
0.46
0.10
0.13
0.23
0.19
0.23
0.14
0.14
0.25
0.35
0.41
0.35
0.36
0.31
0.07
0.09
0.16
0.09
0.34
0.09
0.00
0.01
0.01
-
0.01
-
0.24
0.41
0.38
0.38
0.41
0.26
0.39
0.38
0.38
0.61
0.55
0.41
0.29
0.40
0.41
0.89
0.42
0.39
0.32
0.32
0.34
0.36
0.37
0.36
0.24
0.30
0.38
0.29
0.41
0.29
0.26
0.31
0.41
0.30
0.42
0.30
0.26
0.30
0.45
0.30
0.43
0.30
0.30
0.38
0.36
0.38
0.39
0.36
0.24
0.29
0.39
0.29
0.43
0.27
0.27
0.29
0.39
0.29
0.42
0.30
0.29
0.30
0.40
0.30
0.42
0.31
0.24
0.26
0.41
0.28
0.39
0.28

(a) pheno(funcat).

(b) pheno(GO).

(c) eisen(funcat).

(d) eisen(GO).

Figure 3: Percentage of patterns satisfying the NAP assumption at different values of k .

8

1/10                 1/5              1               5                  10 0.040.050.060.070.080.090.10.110.12(cid:95)Average Testing Loss  MASRMASHBR1/10                 1/5              1               5                  10 0.20.220.240.260.280.30.320.340.360.380.4(cid:95)Average Testing Loss  MASRMASHBR1/10                 1/5              1               5                  10 0.20.250.30.350.4(cid:95)Average Testing Loss  MASRMASHBRanimate inanimate animal water crayfish lobster insect butterfly root Query      MASR MAS BR music wind accordion animate inanimate animal water crocodile dolphin air ibis root Query           MASR MAS BR transportation air airplane animate plant flower sunflower water  lily insect butterfly root Query         MASR MAS BR human face 24681090919293949596979899100kinstances satisfying NAP(%)24681090919293949596979899100kinstances satisfying NAP(%)24681090919293949596979899100kinstances satisfying NAP(%)24681090919293949596979899100kinstances satisfying NAP(%)References
[1] C. Vens, J. Struyf, L. Schietgat, S. Dvzeroski, and H. Blockeel. Decision trees for hierarchical multi-label
classiﬁcation. Machine Learning, 73:185–214, 2008.
[2] J.J. Burred and A. Lerch. A hierarchical approach to automatic musical genre classiﬁcation. In Proceed-
ings of the 6th International Conference on Digital Audio Effects, 2003.
[3] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
Incremental algorithms for hierarchical classiﬁcation.
Journal of Machine Learning Research, 7:31–54, 2006.
[4] C.N. Silla and A.A. Freitas. A survey of hierarchical classiﬁcation across different application domains.
Data Mining and Knowledge Discovery, 22(1-2):31–72, 2011.
[5] Z. Barutcuoglu and O.G. Troyanskaya. Hierarchical multi-label prediction of gene function. Bioinfor-
matics, 22:830–836, 2006.
[6] K. Punera, S. Rajan, and J. Ghosh. Automatically learning document taxonomies for hierarchical clas-
siﬁcation. In Proceedings of the 14th International Conference on World Wide Web, pages 1010–1011,
2005.
[7] M.-L. Zhang and K. Zhang. Multi-label learning by exploiting label dependency. In Proceedings of the
16th International Conference on Knowledge Discovery and Data Mining, pages 999–1008, 2010.
[8] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In Advances in
Neural Information Processing Systems 23, pages 163–171. 2010.
[9] J. Deng, S. Satheesh, A.C. Berg, and L. Fei-Fei. Fast and balanced: Efﬁcient label tree learning for large
In Advances in Neural Information Processing Systems 24, pages 567–575.
scale object recognition.
2011.
[10] J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel
classiﬁcation models. Journal of Machine Learning Research, 7:1601–1626, 2006.
[11] W. Bi and J.T. Kwok. Multi-label classiﬁcation on tree- and DAG-structured hierarchies. In Proceedings
of the 28th International Conference on Machine Learning, pages 17–24, 2011.
[12] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Hierarchical classiﬁcation: Combining Bayes with SVM.
In Proceedings of the 23rd International Conference on Machine Learning, pages 177–184, 2006.
[13] L. Tang, S. Rajan, and V.K. Narayanan. Large scale multi-label classiﬁcation via metalabeler. In Pro-
ceedings of the 18th International Conference on World Wide Web, pages 211–220, 2009.
[14] R. Cerri, A. C. P. L. F. de Carvalho, and A. A. Freitas. Adapting non-hierarchical multilabel classiﬁcation
methods for hierarchical multilabel classiﬁcation. Intelligent Data Analysis, 15:861–887, 2011.
[15] G. Tsoumakas and I. Vlahavas. Random k-labelsets: An ensemble method for multilabel classiﬁcation.
In Proceedings of the 18th European Conference on Machine Learning, pages 406–417, Warsaw, Poland,
2007.
[16] N. Cesa-Bianchi, C. Gentile, A. Tironi, and L. Zaniboni. Incremental algorithms for hierarchical classiﬁ-
cation. In Advances in Neural Information Processing Systems 17, pages 233–240. 2005.
[17] J.H. Zaragoza, L.E. Sucar, and EF Morales. Bayesian chain classiﬁers for multidimensional classiﬁcation.
In Twenty-Second International Joint Conference on Artiﬁcial Intelligence, pages 2192–2197, 2011.
[18] R.G. Baraniuk, V. Cevher, M.F. Duarte, and C. Hegde. Model-based compressive sensing. IEEE Trans-
actions on Information Theory, 56:1982–2001, 2010.
[19] S.E. Shimony. Finding maps for belief networks is NP-hard. Artiﬁcial Intelligence, 68:399–410, 1994.
[20] C. Varin, N. Reid, and D. Firth. An overview of composite likelihood methods. Statistica Sinica, 21:5–42,
2011.
[21] Y. Zhang and J. Schneider. A composite likelihood view for multi-label classiﬁcation. In Proceedings of
the 15th International Conference on Artiﬁcial Intelligence and Statistics, pages 1407–1415, 2012.
[22] J. Zhou, J. Chen, and J. Ye. MALSAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State
University, 2012.
[23] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data. In Data Mining and Knowledge
Discovery Handbook, pages 667–685. Springer, 2010.

9


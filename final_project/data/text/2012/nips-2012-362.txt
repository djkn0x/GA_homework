Discriminative Learning of Sum-Product Networks

Robert Gens
Pedro Domingos
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{rcg,pedrod}@cs.washington.edu

Abstract

Sum-product networks are a new deep architecture that can perform fast, exact in-
ference on high-treewidth models. Only generative methods for training SPNs
have been proposed to date.
In this paper, we present the ﬁrst discriminative
training algorithms for SPNs, combining the high accuracy of the former with
the representational power and tractability of the latter. We show that the class
of tractable discriminative SPNs is broader than the class of tractable generative
ones, and propose an efﬁcient backpropagation-style algorithm for computing the
gradient of the conditional log likelihood. Standard gradient descent suffers from
the diffusion problem, but networks with many layers can be learned reliably us-
ing “hard” gradient descent, where marginal inference is replaced by MPE infer-
ence (i.e., inferring the most probable state of the non-evidence variables). The
resulting updates have a simple and intuitive form. We test discriminative SPNs
on standard image classiﬁcation tasks. We obtain the best results to date on the
CIFAR-10 dataset, using fewer features than prior methods with an SPN architec-
ture that learns local image structure discriminatively. We also report the highest
published test accuracy on STL-10 even though we only use the labeled portion
of the dataset.

1

Introduction

Probabilistic models play a crucial role in many scientiﬁc disciplines and real world applications.
Graphical models compactly represent the joint distribution of a set of variables as a product of fac-
tors normalized by the partition function. Unfortunately, inference in graphical models is generally
intractable. Low treewidth ensures tractability, but is a very restrictive condition, particularly since
the highest practical treewidth is usually 2 or 3 [2, 9]. Sum-product networks (SPNs) [23] overcome
this by exploiting context-speciﬁc independence [7] and determinism [8]. They can be viewed as a
new type of deep architecture, where sum layers alternate with product layers. Deep networks have
many layers of hidden variables, which greatly increases their representational power, but inference
with even a single layer is generally intractable, and adding layers compounds the problem [3].
SPNs are a deep architecture with full probabilistic semantics where inference is guaranteed to be
tractable, under general conditions derived by Poon and Domingos [23]. Despite their tractability,
SPNs are quite expressive [16], and have been used to solve difﬁcult problems in vision [23, 1].
Poon and Domingos introduced an algorithm for generatively training SPNs, yet it is generally
observed that discriminative training fares better. By optimizing P (Y |X) instead of P (X, Y) con-
ditional random ﬁelds retain joint inference over dependent label variables Y while allowing for
ﬂexible features over given inputs X [22]. Unfortunately, the conditional partition function Z (X)
is just as prone to intractability as with generative training. For this reason, low treewidth models
(e.g. chains and trees) of Y are commonly used. Research suggests that approximate inference can
make it harder to learn rich structured models [21]. In this paper, discriminatively training SPNs
will allow us to combine ﬂexible features with fast, exact inference over high treewidth models.

1

With inference and learning that easily scales to many layers, SPNs can be viewed as a type of
deep network. Existing deep networks employ discriminative training with backpropagation through
softmax layers or support vector machines over network variables. Most networks that are not purely
feed-forward require approximate inference. Poon and Domingos showed that deep SPNs could be
learned faster and more accurately than deep belief networks and deep Boltzmann machines on a
generative image completion task [23]. This paper contributes a discriminative training algorithm
that could be used on its own or with generative pre-training.
For the ﬁrst time we combine the advantages of SPNs with those of discriminative models. In this
paper we will review SPNs and describe the conditions under which an SPN can represent the con-
ditional partition function. We then provide a training algorithm, demonstrate how to compute the
gradient of the conditional log-likelihood of an SPN using backpropagation, and explore variations
of inference. Finally, we show state-of-the-art results where a discriminatively-trained SPN achieves
higher accuracy than SVMs and deep models on image classiﬁcation tasks.

2 Sum-Product Networks

SPNs were introduced with the aim of identifying the most expressive tractable representation pos-
sible. The foundation for their work lies in Darwiche’s network polynomial [14]. We deﬁne an un-
normalized probability distribution Φ(x) ≥ 0 over a vector of Boolean variables X. The indicator
function [.] is one when its argument is true and zero otherwise; we abbreviate [Xi ] and [ ¯Xi ] as xi and
The network polynomial of Φ(x) is deﬁned as (cid:80)
x Φ(x) (cid:81)(x), where (cid:81)(x) is the product of indica-
¯xi . To distinguish random variables from indicator variables, we use roman font for the former and
italic for the latter. Vectors of variables are denoted by bold roman and bold italic font, respectively.
tors that are one in state x. For example, the network polynomial of the Bayesian network X1 → X2
is P (x1 )P (x2 |x1 )x1x2 + P (x1 )P (¯x2 |x1 )x1 ¯x2 + P (¯x1 )P (x2 |¯x1 ) ¯x1x2 + P (¯x1 )P (¯x2 |¯x1 ) ¯x1 ¯x2 . To
compute P (X1 = true, X2 = false), we access the corresponding term of the network polynomial
by setting indicators x1 and ¯x2 to one and the rest to zero. To ﬁnd P (X2 = true), we ﬁx evidence on
X2 by setting x2 to one and ¯x2 to zero and marginalize X1 by setting both x1 and ¯x1 to one. Notice
that there are two reasons we might set an indicator xi = 1: (1) evidence {Xi = true}, in which
case we set ¯xi = 0 and (2) marginalization of Xi , where ¯xi = 1 as well. In general the role of an
indicator xi is to determine whether terms compatible with variable state Xi = true are included in
the summation, and similarly for ¯xi . With this notation, the partition function Z can be computed
by setting all indicators of all variables to one.
The network polynomial has size exponential in the number of variables, but in many cases it can
be represented more compactly using a sum-product network [23, 14].
Deﬁnition 1. (Poon & Domingos, 2011) A sum-product network (SPN) over variables X1 , . . . , Xd
is a rooted directed acyclic graph whose leaves are the indicators x1 , . . . , xd and ¯x1 , . . . , ¯xd and
The value of a sum node is (cid:80)
whose internal nodes are sums and products. Each edge (i, j ) emanating from a sum node i has a
non-negative weight wij . The value of a product node is the product of the values of its children.
j∈C h(i) wij vj , where C h(i) are the children of i and vj is the value of
node j . The value of an SPN S [x1 , ¯x1 , . . . , xd , ¯xd ] is the value of its root.

If we could replace the exponential sum over
variable states in the partition function with
the linear evaluation of the network, inference
would be tractable. For example,
the SPN
in Figure 1 represents the joint probability of
three Boolean variables P (X1 , X2 , X3 ) in the
Bayesian network X2 ← X1 → X3 using six
indicators S [x1 , ¯x1 , x2 , ¯x2 , x3 , ¯x3 ]. To com-
pute P (X1 = true), we could sum over the
joint states of X2 and X3 , evaluating the net-
work a total of four times S [1, 0, 0, 1, 0, 1]+ . . .+
S [1, 0, 1, 0, 1, 0]. Instead, we set the indicators
so that the network sums out both X2 and X3 .
An indicator setting of S[1,0,1,1,1,1] computes

Figure 1: SPN over Boolean variables X1 , X2 , X3

2

+++++++x2x2x3x3x1x10.80.20.30.50.10.70.60.40.50.9the sum over all states compatible with our evidence e = {X1 = true} and requires only one evalua-
tion.
However, not every SPN will have this property. If a linear evaluation of an SPN with indicators
set to represent evidence equals the exponential sum over all variable states consistent with that
evidence, the SPN is valid.
Deﬁnition 2. (Poon & Domingos, 2011) A sum-product network S is valid iff S (e) = ΦS (e) for all
evidence e.

In their paper, Poon and Domingos prove that there are two conditions sufﬁcient for validity: com-
pleteness and consistency.
Deﬁnition 3. (Poon & Domingos, 2011) A sum-product network is complete iff all children of the
same sum node have the same scope.
Deﬁnition 4. (Poon & Domingos, 2011) A sum-product network is consistent iff no variable appears
negated in one child of a product node and non-negated in another.
Theorem 1. (Poon & Domingos, 2011) A sum-product network is valid if it is complete and consis-
tent.

The scope of a node is deﬁned as the set of variables that have indicators among the node’s de-
scendants. To “appear in a child” means to be among that child’s descendants.
If a sum node
is incomplete, the SPN will undercount the true marginals. Since an incomplete sum node has
scope larger than a child, that child will be non-zero for more than one state of the sum (e.g.
if
S [x1 , ¯x1 , x2 , ¯x2 ] = (x1 + x2 ), S [1, 0, 1, 1] < S [1, 0, 1, 0] + S [1, 0, 0, 1]). If a product node is incon-
sistent, the SPN will overcount the marginals as it will incorporate impossible states (e.g. x1 × ¯x1 )
into its computation.
Poon and Domingos show how to generatively train the parameters of an SPN. One method is to
compute the likelihood gradient and optimize with gradient descent (GD). They also show how
to use expectation maximization (EM) by considering each sum node as the marginalization of a
hidden variable [17]. They found that online EM using most probable explanation (MPE or “hard”)
inference worked the best for their image completion task.
Gradient diffusion is a key issue in training deep models. It is commonly observed in neural net-
works that when the gradient is propagated to lower layers it becomes less informative [3]. When
every node in the network takes fractional responsibility for the errors of a top level node, it be-
comes difﬁcult to steer parameters out of local minima. Poon and Domingos also saw this effect
when using gradient descent and EM to train SPNs. They found that online hard EM could provide
a sparse but strong learning signal to synchronize the efforts of upper and lower nodes. Note that
hard training is not exclusive to EM. In the next section we show how to discriminatively train SPNs
with hard gradient descent.

3 Discriminative Learning of SPNs
We deﬁne an SPN S [y , h|x] that takes as input three disjoint sets of variables H, Y , and X (hidden,
query, and given). We denote the setting of all h indicator functions to 1 as S [y , 1|x], where the
bold 1 is a vector. We do not sum over states of given variables X when discriminatively training
SPNs. Given an instance, we treat X as constants. This means that one ignores X variables in the
scope of a node when considering completeness and consistency. Since adding a constant as a child
to a product node cannot make that product inconsistent, a variable x can be the child of any product
node in a valid SPN. To maintain completeness, x can only be the child of a sum node that has scope
outside of Y or H.

Algorithm 1: LearnSPN
Input: Set D of instances over variables X and label variables Y , a valid SPN S with initialized parameters.
Output: An SPN with learned weights
repeat
forall the d ∈ D do
UpdateWeights(S , Inference(S ,xd ,yd ))
until convergence or early stopping condition;

3

The parameters of an SPN can be learned using an online procedure as in Algorithm 1 as proposed
by Poon and Domingos. The three dimensions of the algorithm are generative vs. discriminative,
the inference procedure, and the weight update. Poon and Domingos discussed generative gradient
descent with marginal inference as well as EM with marginal and MPE inference. In this section we
will derive discriminative gradient descent with marginal and MPE inference, where hard gradient
descent can also be used for generative training. EM is not typically used for discriminative training
as it requires modiﬁcation to lower bound the conditional likelihood [25] and there may not be a
closed form for the M-step.

∂
∂w

log

=

−

∂
∂w

3.1 Discriminative Training with Marginal Inference
(cid:88)
(cid:88)
A component of the gradient of the conditional log likelihood takes the form
Φ(Y = y, H = h|x) − ∂
Φ(Y = y(cid:48) , H = h|x)
log P (y|x) =
log
∂w
y(cid:48) ,h
h
∂S [1, 1|x]
∂S [y , 1|x]
1
1
S [1, 1|x]
S [y , 1|x]
∂w
∂w
where the two summations are separate bottom-up evaluations of the SPN with indicators set as
S [y , 1|x] and S [1, 1|x], respectively.
The partial derivatives of the SPN with respect to all weights can be computed with backpropagation,
detailed in Algorithm 2. After performing a bottom-up evaluation of the SPN, partial derivatives are
passed from parent to child as follows from the chain rule and described in [15]. The form of
backpropagation presented takes time linear in the number of nodes in the SPN if product nodes
have a bounded number of children.
Our gradient descent update then follows the direction of the partial derivative of the conditional
∂w log P (y|x). After each gradient step we optionally
log likelihood with learning rate η : ∆w = η ∂
renormalize the weights of a sum node so they sum to one. Empirically we have found this to pro-
duce the best results. The second SPN evaluation that marginalizes H and Y can reuse computation
from the ﬁrst, for example, when Y is modeled by a root sum node. In this case the values of all
non-root nodes are equivalent between the two evaluations. For any architecture, one can memoize
values of nodes that do not have a query variable indicator as a descendant.

Algorithm 2: BackpropSPN
Input: A valid SPN S , where Sn denotes the value of node n after bottom-up evaluation.
Output: Partial derivatives of the SPN with respect to every node ∂S
and weight
∂S
∂Sn
∂wi,j
= 0 except ∂S
Initialize all ∂S
∂S = 1
forall the n ∈ S in top-down order do
∂Sn
if n is a sum node then
forall the j ∈ C h(n) do
← ∂S
∂S
∂S
+ wn,j
∂Sn
∂Sj
∂Sj
← Sj
∂S
∂S
∂wn,j
∂Sn
(cid:81)
forall the j ∈ C h(n) do
← ∂S
+ ∂S
∂S
∂Sj
∂Sj
∂Sn

k∈Ch(n)\{j} Sk

else

3.2 Discriminative Training with MPE Inference

There are several reasons why MPE inference is appealing for discriminatively training SPNs. As
discussed above, hard inference was crucial for overcoming gradient diffusion when generatively
training SPNs. For many applications the goal is to predict the most probable structure, and therefore
it makes sense to use this also during training. Finally, it is common to approximate summations
with maximizations for reasons of speed or tractability. Though summation in SPNs is fast and
exact, MPE inference is still faster. We derive discriminative gradient descent using MPE inference.

4

Figure 2: Positive and negative terms in the hard gradient. The root node sums out the variable Y,
the two sum nodes on the left sum out the hidden variable H1 , the two sum nodes on the right sum
out H2 , and a circled ‘f ’ denotes an input variable Xi . Dashed lines indicate negative elements in
the gradient.
network compactly represents the maximizer polynomial maxx Φ(x) (cid:81)(x), which computes the
We deﬁne a max-product network (MPN) M [y , h|x] based on the max-product semiring. This
MPE [15]. To convert an SPN to an MPN, we replace each sum node by a max node, where weights
on children are retained. The gradient of the conditional log likelihood with MPE inference is then
Φ(Y = y, H = h|x) − ∂
log ˜P (y|x) =
Φ(Y = y(cid:48) , H = h|x)
∂
∂
log max
log max
y(cid:48) ,h
∂w
∂w
∂w
h
where the two maximizations are computed by M [y , 1|x] and M [1, 1|x]. MPE inference also
consists of a bottom-up evaluation followed by a top-down pass. Inference yields a branching path
through the SPN called a complete subcircuit that includes an indicator (and therefore assignment)
for every variable [15]. Analogous to Viterbi decoding, the path starts at the root node and at each
MPN takes the form of a product (cid:81)
max (formerly sum) node it only travels to the max-valued child. At product nodes, the path branches
to all children. We deﬁne W as the multiset of weights traversed by this path1 . The value of the
i , where ci is the number of times wi appears in W .
wi∈W wci
The partial derivatives of the MPN with respect to all nodes and weights is computed by Algorithm
2 modiﬁed to accommodate MPNs: (1) S becomes M , (2) when n is a sum node, the body of the
forall loop is run once for j as the max-valued child.
(cid:81)
The partial derivative of the logarithm of an MPN with respect to a weight takes the form
(cid:81)
ci · wci−1
wj ∈W \{wi } wcj
i
j
wj ∈W wcj
j
The gradient of the conditional log likelihood with MPE inference is therefore ∆ci /wi , where
∆ci = c(cid:48)i − c(cid:48)(cid:48)i
is the difference between the number of times wi is traversed by the two MPE
inference paths in M [y , 1|x] and M [1, 1|x], respectively. The hard gradient update is then ∆wi =
log ˜P (y|x) = η ∆ci
.
η ∂
∂wi
wi
The hard gradient for a training instance (xd , yd ) is illustrated in Figure 2. In the ﬁrst two expres-
sions, the complete subcircuit traveled by each MPE inference is shown in bold. Product nodes do
not have weighted children, so they do not appear in the gradient, depicted in the last expression
We can also easily add regularization to SPN training. An L2 weight penalty takes the familiar
form of −λ||w||2 and partial derivatives −2λwi can be added to the gradient. With an appropriate
optimization method, an L1 penalty could also be used for learning with marginal inference on dense
SPN architectures. However, sparsity is not as important for SPNs as it is for Markov random ﬁelds,
where a non-zero weight can have outsize impact on inference time; with SPNs inference is always
linear with respect to model size.
A summary of the variations of Algorithm 1 is provided in Tables 1 and 2. The generative hard
gradient can be used in place of online EM for datasets where it would be prohibitive to store
inference results from past epoch. For architectures that have high fan-in sum nodes, soft inference
may be able to separate groups of modes faster than hard inference, which can only alter one child
of a sum node at a time.
We observe the similarity between the updates of hard EM and hard gradient descent. In particular,
(cid:48)
if we reparameterize the SPN so that each child of a sum node is weighted by wi = ew
i , the form of
1A consistent SPN allows for MPE inference to reach the same indicator more than once in the same
branching path

∂ log M
∂wi

∂ log M
∂M

∂M
∂wi

ci
wi

=

=

=

1
M

∂M
∂wi

=

5

++++++++++++++++++++++++++++++++++++ffffffffffffffffffffffffffffffffffffSl

∂M
∂Mn

∂M
∂Mn

wkn

∂S
∂Sn

Weight

Product

∂S
∂Sn

∂S
∂wki

Node
Sum

= (cid:80)
(cid:81)
= (cid:80)
(cid:81)
Table 1: Inference procedures
Hard Inference
Soft Inference
(cid:26)wkn
= (cid:80)
= (cid:80)
∂M
∂S
Ml
∂Sk
∂Mk
l∈C h(k)\{n}
k∈P a(n)
k∈P a(n)
l∈C h(k)\{n}
: wkn ∈ W
∂M
∂S
∂Mk
: otherwise
∂Sk
0
k∈P a(n)
k∈P a(n)
∂M
= ∂M
= ∂S
Si
Mi
∂Sk
∂Mk
∂wki
Table 2: Weight updates
(cid:26)1
Hard Inference
Update
Soft Inference
Gen. GD ∆w = η ∂S [x,y ]
∆wi = η ci
∂w
wi
(cid:16)
Gen. EM P (Hk = i|x, y) ∝ wki
P (Hk = i|x, y) =
∂S [x,y ]
(cid:17)
0
∂Sk
∂w − ∆wi = η ∆ci
∂S [y ,1|x]
Disc. GD ∆w = η
1
S [y ,1|x]
wi
∂S [1,1|x]
1
S [1,1|x]
∂w
(cid:81)
the partial derivative of the log MPN becomes
(cid:81)
∈W (cid:48) ecj ·w(cid:48)
ci
w(cid:48)
j
j
∈W (cid:48) ecj ·w(cid:48)
w(cid:48)
j
j
This means that the hard gradient update for weights in logspace is ∆w (cid:48)i = ∆ci , which resembles
structured perceptron [13].

: wki ∈ W
: otherwise

∂ log M
∂w (cid:48)
i

=

1
M

∂M
∂w (cid:48)
i

=

= ci

4 Experiments

We have applied discriminative training of SPNs to image classiﬁcation benchmarks. CIFAR-10
and STL-10 are standard datasets for deep networks and unsupervised feature learning. Both are
10-class small image datasets. We achieve the best results to date on both tasks.
We follow the feature extraction pipeline of Coates et al. [10], which was also used recently to
learn pooling functions [20]. The procedure consists of extracting 4 × 105 6x6 pixel patches from
the training set images, ZCA whitening those patches [19], running k-means for 50 rounds, and
then normalizing the dictionary to have zero mean and unit variance. We then use the dictionary
to extract K features at every 6x6 pixel site in the image (unit stride) with the “triangle” encoding
fk (x) = max{0, ¯z − zk }, where zk = ||x − ck ||2 , ck is the k-th item in the dictionary, and ¯z is the
average zk . For each image of CIFAR-10, for example, this yields a 27 × 27 × K feature vector that
is ﬁnally downsampled by max-pooling to a G × G × K feature vector.
We experiment with a simple architecture that
allows for discriminative learning of
local
structure. This architecture cannot be gener-
atively trained as it violates consistency over
Inspired by the successful star models in
X.
Felzenszwalb et al.
[18], we construct a net-
work with C classes, P parts per class, and T
mixture components per part. A part is a pat-
tern of image patch features that can occur any-
where in the image (e.g. an arrangement of
patches that deﬁnes a curve). Each part ﬁlter
(cid:126)fcpt is of dimension W × W × K and is ini-
tialized to (cid:126)0. The root of the SPN is a sum node
with a child Sc for each class c in the dataset
multiplied by the indicator for that state of the
label variable Y. Sc is a product over P nodes
Scp , where each Scp is a sum node over T nodes

Figure 3: SPN architecture for experiments. Hid-
den variable indicators omitted for legibility.

6

GxGxKMixturexParts+++ClassesLocationWxWxKexij·f111Scpt . The hidden variables H represent the choice of cluster in the mixture over a part and its posi-
tion (Scp and Scpt , respectively). Finally, Scpt sums over positions i, j in the image of the logistic
function e(cid:126)xij · (cid:126)fcpt where the given variable (cid:126)xij is the same dimension as f and parts can overlap.
Notice that the mixture Scp models an additional level of spatial structure on top of the image patch
features learned by k-means. Coates and Ng [12] also learn higher-order structure, but whereas our
method learns structure discriminatively in the context of a parts-based model, their unsupervised
algorithm greedily groups features based on correlation and is unable to learn mixtures. Compared
with the pooling functions in Jia et al. [20] that model independent translation of patch features,
our architecture models how nearby features move together. Other deep probabilistic architectures
should be able to model high-level structure, but considering the difﬁculty in training these models
with approximate inference, it is hard to make full use of their representational power. Unlike the
star model of Felzenswalb et al. [18] that learns ﬁlters over predeﬁned HOG image features, our
SPN learns on top of learned image features that can model color and detailed patterns.
Generative SPN architectures on the same features produce unsatisfactory results as generative train-
ing is led astray by the large number of features, very few of which differentiate labels. In the gen-
erative SPN paper [23], continuous variables are modeled with univariate Gaussians at the leaves
(viewed as a sum node with inﬁnite children but ﬁnite weight sum). With discriminative training, X
can be continuous because we always condition on it, which effectively folds it into the weights.
All networks are learned with stochastic gradient descent regularized by early stopping. We found
that using marginal inference for the root node and MPE inference for the rest of the network worked
best. This allows the SPN to continue learning the difference between classes even when it correctly
classiﬁes a training instance. The fraction of the training set reserved for validation with CIFAR-
10 and STL-10 were 10% and 20%, respectively. Learning rates, P , and T were chosen based on
validation set performance.

4.1 Results on CIFAR-10
CIFAR-10 consists of 32x32 pixel images: 5 × 104 for training and 104 for testing. We ﬁrst compare
discriminative SPNs with other methods as we vary the size of the dictionary K . The results are
seen in Figure 4. To fairly compare with recent work [10, 20] we also set G = 4. In general,
we observe that SPNs can achieve higher performance using half as many features as the next best
approach, the learned pooling function. We hypothesize that this is because the SPN architecture
allows us to discriminatively train large moveable parts, image structure that cannot be captured by
larger dictionaries. In Jia et al. [20] the pooling functions blur individual features (i.e. a 6x6 pixel
dictionary item), from which the classiﬁer may have trouble inferring the coordination of image
parts.
We then experimented with a ﬁner grid and fewer dictionary items (G = 7, K = 400). Pooling
functions destroy information, so it is better if less is done before learning. Finer grids are less
feasible for the method in Jia et al.
[20] as the number of rectangular pooling functions grows
O(G4 ). Our best test accuracy of 83.96% was achieved with W = 3, P = 200, and T = 2, chosen

Figure 4: Impact of dictionary size K with a 4x4 pooling grid (W =3) on CIFAR-10 test accuracy

7

20040080016004000Dictionary Size646872768084AccuracyPerformance on CIFAR-10Discriminative SPNLearned Pooling, Jia et al.K-means (tri.), white, Coates et al.Auto-encoder, raw, Coates et al.RBM, whitened, Coates et al.4000, 4x4 grid

Table 3: Test accuracies on CIFAR-10.
Accuracy
Method
Dictionary
36.0%
Logistic Regression [24]
39.5%
SVM [5]
65.6%
SIFT [5]
mcRBM [24]
68.3%
71.0%
mcRBM-DBN [24]
78.9%
Convolutional RBM [10]
79.6 %
K-means (Triangle) [10]
80.0%
HKDES [4]
1600, 9x9 grid
3-Layer Learned RF [12]
82.0%
83.11%
6000, 4x4 grid
Learned Pooling [20]
400, 7x7 grid
Discriminative SPN
83.96%
Table 4: Comparison of average test accuracies on all folds of STL-10.
Accuracy (±σ )
Method
54.9% (± 0.4%)
1-layer Vector Quantization [11]
59.0% (± 0.8%)
1-layer Sparse Coding [11]
60.1% (± 1.0%)
3-layer Learned Receptive Field [12]
62.3% (± 1.0%)
Discriminative SPN
by validation set performance. This architecture achieves the highest published test accuracy on the
CIFAR-10 dataset, remarkably using one ﬁfth the number of features of the next best approach. We
compare top CIFAR-10 results in Table 3, highlighting the dictionary size of systems that use the
feature extraction from Coates et al. [10].

4.2 Results on STL-10

STL-10 has larger 96x96 pixel images and less labeled data (5,000 training and 8,000 test) than
CIFAR-10 [10]. The training set is mapped to ten predeﬁned folds of 1,000 images. We experi-
mented on the STL-10 dataset in a manner similar to CIFAR-10, ignoring the 105 items of unlabeled
data. Ten models were trained on the pre-speciﬁed folds, and test accuracy is reported as an aver-
age. With K=1600, G=8, W =4, P =10, and T =3 we achieved 62.3% (± 1.0% standard deviation
among folds), the highest published test accuracy as of writing. Notably, this includes approaches
that make use of the unlabeled training images. Like Coates and Ng [12], our architecture learns
local relations among different feature maps. However, the SPN is able to discriminatively learn
latent mixtures, which can encode a more nuanced decision boundary than the linear classiﬁer used
in their work. After we carried out our experiments, Bo et al. [6] reported a higher accuracy with
their unsupervised features and a linear SVM. Just as with the features of Coates et al. [10], we
anticipate that using an SPN instead of the SVM would be beneﬁcial by learning spatial structure
that the SVM cannot model.

5 Conclusion

Sum-product networks are a new class of probabilistic model where inference remains tractable de-
spite high treewidth and many hidden layers. This paper introduced the ﬁrst algorithms for learning
SPNs discriminatively, using a form of backpropagation to compute gradients. Discriminative train-
ing allows for a wider variety of SPN architectures than generative training, because completeness
and consistency do not have to be maintained over evidence variables. We proposed both “soft”
and “hard” gradient algorithms, using marginal inference in the “soft” case and MPE inference in
the “hard” case. The latter successfully combats the diffusion problem, allowing deep networks to
be learned. Experiments on image classiﬁcation benchmarks illustrate the power of discriminative
SPNs.
Future research directions include applying other discriminative learning paradigms to SPNs (e.g.
max-margin methods), automatically learning SPN structure, and applying discriminative SPNs to
a variety of structured prediction problems.
Acknowledgments: This research was partly funded by ARO grant W911NF-08-1-0242, AFRL
contract FA8750-09-C-0181, NSF grant IIS-0803481, and ONR grant N00014-12-1-0312. The
views and conclusions contained in this document are those of the authors and should not be inter-
preted as necessarily representing the ofﬁcial policies, either expressed or implied, of ARO, AFRL,
NSF, ONR, or the United States Government.

8

References
[1] M. Amer and S. Todorovic. Sum-product networks for modeling activities with stochastic structure.
CVPR, 2012.
[2] F. Bach and M.I. Jordan. Thin junction trees. Advances in Neural Information Processing Systems,
14:569–576, 2002.
[3] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127,
2009.
[4] L. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729–1736. IEEE, 2011.
[5] L. Bo, X. Ren, and D. Fox. Kernel descriptors for visual recognition. Advances in Neural Information
Processing Systems, 2010.
[6] L. Bo, X. Ren, and D. Fox. Unsupervised feature learning for RGB-D based object recognition. ISER,
2012.
[7] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-speciﬁc independence in bayesian
networks. In Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial Intelligence, pages 115–
123, 1996.
[8] M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. Artiﬁcial Intelli-
gence, 172(6-7):772–799, 2008.
[9] A. Chechetka and C. Guestrin. Efﬁcient principled learning of thin junction trees. In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press,
Cambridge, MA, 2008.
[10] A. Coates, H. Lee, and A.Y. Ng. An analysis of single-layer networks in unsupervised feature learning.
In aistats11. Society for Artiﬁcial Intelligence and Statistics, 2011.
[11] A. Coates and A.Y. Ng. The importance of encoding versus training with sparse coding and vector
quantization. In International Conference on Machine Learning, volume 8, page 10, 2011.
[12] A. Coates and A.Y. Ng. Selecting receptive ﬁelds in deep networks. NIPS, 2011.
[13] M. Collins. Discriminative training methods for hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–8, Philadelphia, PA, 2002. ACL.
[14] A. Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM, 50:280–
305, 2003.
[15] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press, 2009.
[16] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In Proceedings of the 25th Confer-
ence on Neural Information Processing Systems, 2011.
[17] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.
[18] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part
model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8.
Ieee, 2008.
[19] A. Hyv ¨arinen and E. Oja. Independent component analysis: algorithms and applications. Neural net-
works, 13(4-5):411–430, 2000.
[20] Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image
features. In CVPR, 2012.
[21] A. Kulesza, F. Pereira, et al. Structured learning with approximate inference. Advances in Neural Infor-
mation Processing Systems, 20:785–792, 2007.
[22] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting
and labeling data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages
282–289, Williamstown, MA, 2001. Morgan Kaufmann.
[23] H. Poon and P. Domingos. Sum-product networks: A new deep architecture.
Uncertainty in Artiﬁcial Intelligence, pages 337–346, 2011.
[24] M.A. Ranzato and G.E. Hinton. Modeling pixel means and covariances using factorized third-order
Boltzmann machines. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2551–2558. IEEE, 2010.
[25] J. Saloj ¨arvi, K. Puolam ¨aki, and S. Kaski. Expectation maximization algorithms for conditional likeli-
hoods. In Proceedings of the 22nd international conference on Machine learning, pages 752–759. ACM,
2005.

In Proc. 12th Conf. on

9


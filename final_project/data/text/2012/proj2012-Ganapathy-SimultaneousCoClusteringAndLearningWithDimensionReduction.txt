Simultaneous Co-clustering and Learning with
Dimensionality Reduction
Harish Ganapathy
CS 229 Project Report
E-mail: harishg@utexas.edu

1

Abstract—In this project, we are interested in solving pre-
dictions problems involving dyadic data. The “Netﬂix problem”,
where data may be organized into a matrix with the rows
representing users, the columns representing movies and (some
of) the matrix elements containing ratings,
is an example of
a setting where the data is naturally dyadic. The prediction
problem here is essentially to ﬁll
in the missing entries of
the matrix with predicted ratings. Recently, a technique called
simultaneous co-clustering and learning (SCOAL) was proposed
that was demonstrated to be effective in solving such dyadic
prediction problems. SCOAL is essentially a divide-and-conquer
approach that seeks to form clusters of feature vectors and build
a separate local model for each one of these clusters. Such an
approach inherently suffers from the risk of overﬁtting, especially
when the number of clusters is large. Thus, to alleviate the effects
of over-ﬁtting, while at the same time, beneﬁtting from the use of
multiple models, we propose the use of dimensionality reduction
in conjunction with SCOAL. We investigate two dimensionality
reduction techniques: principle components regression and the
least absolute shrinkage/selection operator (LASSO). We show that
both these techniques provide signiﬁcant reduction in test error
(up to 10% in some settings) when linear predictive models are
employed within each cluster.

I . INTRODUC T ION
In dyadic prediction problems, the training data consists of a
pair of objects called dyads, with associated labels. The goal
is to predict labels for the dyads that have missing labels.
Apart from the popular “Netﬂix problem”, another example of
a dyadic prediction problem arises in marketing where we are
interested in predicting whether a person will purchase an item
given a history of purchases. In such settings, it is of interest
to determine clusters of similar users and/or similar movies.
The technique of co-clustering, which simultaneously clusters
both axes, naturally lends itself to solving such problems.
Co-clustering has been used in other problems such as text
clustering and micro-array data analysis that involve dyadic
data. Once the clusters1 are determined, one may build a
predictive model on a per-cluster basis. Such an approach ﬁnds
its roots in the general wisdom that partitioning the feature
space into multiple homogeneous segments and building more
accurate models for each segment, often works well. It follows
that such a technique would be most effective when the
underlying generative model for the data resembles a mixture
of Gaussians model for instance.
A recent paper by Deodhar and Ghosh [1] proposes a novel
extension to the above sequential approach of co-clustering fol-
lowed by learning. They propose a simultaneous co-clustering
and learning algorithm that essentially recognizes that the
ratings or labels inherently contain information pertaining to

1We use the terms clusters and co-clusters interchangeably in this report.

the similarity amongst the users and must hence be used
in the formation of co-clusters. Since the optimal joint co-
clustering and learning algorithm is of course NP-hard, the
proposed greedy algorithm alternates between (i) learning the
best models given a co-clustering and (ii) learning the best co-
clustering given a set of local models. This algorithm is im-
plicitly made possible by the fact that the model that predicts
a given matrix element, operates on the corresponding row
(user) and column (movie) attributes. Therefore, the divide-
and-conquer-based prediction algorithm proposed by Deodhar
and Ghosh [1] is applicable to settings where such additional
attribute information is indeed available.
While divide-and-conquer is often an effective approach to
improving prediction performance, one has to guard against
over-ﬁtting the training data as the number of co-clusters
increases (equiv. as the cluster size decreases). This is espe-
cially critical for high-dimensional data (attributes) where it
would be quite easy to exactly ﬁt a small number of labels
(located inside a small co-cluster). To address this challenge,
in this paper, we propose to extend the algorithm introduced by
Deodhar and Ghosh [1] to include dimensionality reduction.
This means that when building local models for each co-
cluster or sub-matrix, we include an additional dimensionality
reduction step, which reduces the size or number of param-
eters that constitute the model. We consider two methods of
dimensionality reduction that overlay the SCOAL algorithm:
(i) Principal component regression (PCR) - Given a co-
cluster with a set of user-movie attribute vectors, we retain
only the strongest eigenmodes (through singular value decom-
position) of this set of user-movie attribute vectors and build a
model from this set of eigenmodes to the labels. The number of
eigenmodes that is retained is determined by an upper bound
on the eigenmass, which is an additional input to the algorithm.
Thus, this approach exploits any low-dimensional structure
that may be present in the feature vectors belonging to the
co-cluster.
(ii) LASSO regularization - One may more directly
eliminate attributes that are not useful, i.e., not correlated
to the labels, by using a more standard technique such as
regularization. While the SCOAL framework in general
accommodates any type of model within a co-cluster, LASSO
is applicable speciﬁcally when linear regression/classiﬁcation
is used within each co-cluster.

We demonstrate the gains of dimensionality reduction in
the context of a recommender system application. The main
results in the report are as follows:
• We consider the well-known MovieLens dataset that was
made available by the GroupLens Research Project at the

2

University of Minnesota [3]. We show that as the number
of clusters increases, both LASSO and PCR decrease the
mean-squared test error when overlaid on plain SCOAL.
In particular, PCR provides gains of up to 10% in as the
number of clusters increases.
• We consider the ERIM data set, widely-used in the
marketing community, consisting of household panel data
collected by A.C. Nielsen [5]. We show that SCOAL-
PCR with linear local models does not offer any gains
over plain SCOAL in this setting.
• We study the use of PCR in conjunction with non-linear
local models. In particular, we apply regression trees
within each co-cluster. As with the ERIM data set, we
show that SCOAL-PCR does not offer any gains here.

wij =

I I . PROB LEM D E FIN I T ION
We describe the problem formulation in this section. There
are a total of m customers/users and n items. Matrix Z
contains the ratings that
the users have given the items;
zmn 2 R+ denotes the rating given to item n by user m.
It is also possible that a user has not rated an item. This gives
rise to a natural matrix prediction problem, where given a set
{
of ratings, we are interested in predicting in the missing cells.
Let matrix W with
cell (i, j ) has a missing entry
0,
1, otherwise

(1)
2 Rd1 , i =
encode the locations of the missing ratings. Let pi
1, 2, . . . , m, (resp. qj 2 Rd2 , j = 1, 2, . . . , n,) be a d1 -
dimensional (resp. d2 -dimensional) feature/attribute vector that
]T 2 Rd1+d2+1 . The regression
[
is associated with customer i (resp. item j ). Thus, associated
with each user-item dyad, we can assemble a cumulative
feature vector xij =
i qT
1 pT
j
problem of interest is to ﬁnd a map from the set of features xij
to the positive reals such that the missing ratings are predicted
accurately, in the sense of mean-squared error.
In the next section, we describe the SCOAL algorithm that
breaks up the data into smaller co-clusters or sub-matrices and
builds a model within each co-cluster.

I I I . S IMU LTANEOU S CO -CLU ST ER ING AND LEARN ING
W I TH D IM EN S IONAL I TY R EDUC T ION
We introduce the following notation to facilitate co-
clustering. Let K and L be the total number of row and column
clusters respectively. In particular, let Ck , k = 1, 2, . . . , K
denote the k-th column cluster and Rl , l = 1, 2, . . . , L denote
the l-th row cluster. For convenience, let r : f1, 2, . . . , mg !
f1, 2, . . . , K g and c : f1, 2, . . . , ng ! f1, 2, . . . , Lg denote
mappings from the customer (resp. item) index to its associated
row (resp. column) cluster. K and L are treated as inputs to
the problem in our formulation, making the presented results
applicable to settings where model selection is not feasible.
∑
∑
For a given K and L, the regression problem that SCOAL
seeks to solve can be written as
j=1 wij (zij (cid:0) (cid:12)T
n
m
c(i)r(j )xij )2
min
Ci , Rj , (cid:12) ij , i = 1, . . . , K, j = 1, 2, . . . , L.
i=1
w.r.t
The above problem is NP-hard and hence, SCOAL is essen-
tially a greedy approximation algorithm. Note that while (2)
assumes the use of linear prediction , i.e., (cid:12)T
c(i)r(j )xij , within

(2)

each co-cluster for notational simplicity, we may in general use
any predictive model here. In Algorithm 1 below, we reproduce
the pseudocode for the algorithm from [1]. As motivated in

7:
8:
9:

Algorithm 1 SCOAL
1: Input: convergence error threshold ", Z , W , random co-clustering
Ci ; Rj ; i = 1; : : : ; K; j = 1; 2; : : : ; L.
2: while True do
Build models:
3:
for k = 1; : : : ; K do
4:
for l = 1; : : : ; L do
5:
Train linear regression model (cid:12)kl given data points
6:
(xij ; zij ); i 2 Ck ; j 2 Rl ; wij = 1.
end for
end for
Update row cluster assignment r((cid:1)): Assign each row to cluster
∑
∑
that minimizes row error:
for i = 1; : : : ; m do
s:c(s)=j wis (zis (cid:0)
r(i) = arg mink=1;:::;K
(cid:12)T
kj xis )2
end for
Update column cluster assignment c((cid:1)): Assign each row to
∑
∑
cluster that minimizes row error:
for j = 1; : : : ; n do
t:r(t)=i wj t (zj t (cid:0)
c(j )
arg minl=1;:::;L
=
(cid:12)T
li xj t )2
end for
16:
17: Measure error at iteration n and store in error[n].
if jerror[n] (cid:0) error[n (cid:0) 1]j (cid:20) " then
18:
break while loop.
19:
end if
20:
21: end while

10:
11:

12:
13:

14:
15:

L
j=1

K
i=1

the introduction, the original approach in Algorithm 1 may
tend to overﬁt the data especially as the size of the co-cluster
decreases (equiv. as K and L increase). In order to address
this issue, we propose two variants or overlays to the above
approach.
The ﬁrst variant, called Principle Components Regression
(PCR) [2], essentially seeks to exploits any low-dimensional
structure that may be present in the set of features attached to
the particular co-cluster. The technique projects the features
onto a lower-dimensional space and performs the regression
using these “latent” features. Here, each latent feature is a
linear combination of all the features.
The second variant is a more standard regularization ap-
proach called LASSO [4]. Here, an ℓ1 -penalty term is added
to the objective function in (2) in order to encourage sparsity.
Whereas PCR tries to succinctly summarize all the features in
the form of a few latent features, LASSO attempts to eliminate
features that are not useful.
The two proposed SCOAL overlays are presented below in
more detail.

A. Principal components regression
We describe the PCR algorithm for a single co-cluster.
Consider an arbitrary co-cluster (Ci , Rj ) and let s =
1, 2, . . . , jCi jjRj j denote a serialized index obtained by count-
ing top-down through each column of the co-cluster. Similarly,
given a matrix A 2 Rm×n , let VecfAg 2 Rmn×1 denote
the natural vectorization of this matrix by counting top-down
through each column of the matrix. For convenience, let A(S )
denote a sub-matrix A formed by retaining rows indexed by
set S . Next, let X = x1x2 . . . x|Ci ||Rj | ]T be a matrix that is

(3)

p = min

formed by collecting together all feature vectors belonging to
a co-cluster. Let Skl = fi : wi = 1, i = 1, 2, . . . , jCi jjRj jg
index the set of rated items in co-cluster (k , l). The basic idea
of PCR is to use the SVD to form a set of latent features
in a lower-dimension. To that effect we decompose X (Skl )
as X (Skl ) = U (cid:3)V T where U T U = I , V T V = I and (cid:3)
(cid:21) . . . (cid:21) σ2
(cid:21) σ2
contains the singular values σ2
1
2
d1+d2+1
arranged in descending order. The columns of T = U (cid:3) are
often referred to as the principle components. We then form
a rank-p approximation
X (Skl ) (cid:25) TpV T
p ,
where Tp and Vp retain the ﬁrst p columns of T of V
respectively.
In the context of our algorithm, we are interested in con-
trolling the rank of the approximation through the eigenvalue
mass. In other words, given an upper bound λ 2 (0, 1], we
}
{
∑
∑
choose rank p such that
p
(cid:21) λ
c=1 σ2
c
p
d1+d2+1
σ2
c
c=1
λ represents the fraction of eigenvalue mass that should be
retained. An advantage of working with eigenvalue masses
instead of direct rank values is that this allows the model to
retain a ﬁxed amount of “information” since eigenvalue masses
may be treated as proxies for the amount of information in a
particular direction. With rank p chosen as described above,
we may form the low-rank approximation Tp (λ) to X (Skl )
and then solve the following PCR regression problem
min jjY (Skl ) (cid:0) Tp (λ)T (cid:12) jj2
(4)
w.r.t (cid:12) .
within co-cluster (k , l), where Y = VecfZ g. The closed-form
solution to (4) is well-known and is given by
−1Tp (λ)T Y (Skl ).
∗
(λ) = (Tp (λ)T Tp (λ))
Note that while the SVD does indeed place an added compu-
tational burden, the model in (5) can be computed efﬁciently
since the columns of Tp are orthogonal. Thus, the SCOAL-
PCR variant to SCOAL may be obtained by replacing all
features in Algorithm 1 by the corresponding latent features.
Once the models are constructed, we can now turn our
attention to the prediction problem. In order to predict the
missing entries, i.e., those indexed by f(i, j ) : wij = 0g,
we ﬁrst project
the feature matrix Xtest onto the lower-
dimensional space Ttest (λ) = XtestVp (λ) and then apply the
regression model to the transformed features to obtain the
predictions Vecf ^Z g = Ttest (λ)(cid:12)
∗
(λ). The error within co-
cluster (k , l) is then measured as
1jSkl j jjVecf ^Z g (cid:0) VecfZ gjj2 .
Having described the PCR overlay algorithm, we now move on
to the LASSO approach that we review brieﬂy as it is already
well-studied in the literature.

(5)

′

:

;

(cid:12)

B. LASSO regularization
As mentioned earlier, while PCR seeks to retain (in part)
the information contained in all features, Lasso on the other
hand tried to discard features by promoting sparsity in the

3

∑
∑
linear models. The LASSO regression problem within each
co-cluster can be written as
j=1 wij (zij (cid:0) (cid:12)T xij )2 + γ jj(cid:12) jj1
n
m
min
(6)
i=1
.
w.r.t (cid:12)
for some appropriately chosen γ > 0, where jj (cid:1) jj1 denotes
the ℓ1 -norm. As γ increases, the amount of sparsity increases
at the cost of training accuracy. The SCOAL-LASSO variant
may be obtained by replacing Step 6 in Algorithm 1 by (6).
Note that, in contrast to PCR, LASSO is only applicable when
using linear models inside each co-cluster.
In the next section, we present some preliminary results
that quantify the performance of the proposed dimensionality
reduction techniques.

IV. R E SU LT S : R ECOMM END ER SY ST EM S
We ﬁrst apply the algorithms to the MovieLens dataset.
The dataset consists of 100, 000 ratings from 943 users and
1682 movies. Each user has rated at least 20 movies. In the
interest of expediting the run-time, we reduce the size of the
data set. We choose the top 378 users that have rated the
most number of movies. Following this initial pruning step,
we prune further by choosing the top 673 movies that have
the most number of ratings. This amounts to retaining 40% of
the initial data set. Of the user attributes that are available, we
consider age and occupation. The occupation is represented
as a binary vector on a pre-speciﬁed set of occupations (e.g.,
administrator, artiste, doctor, etc.). The movie attributes that
we use are release date and genre. The genre is again a binary
vector on a pre-speciﬁed set of genres (e.g., action, adventure,
animation, etc.). The feature dimensions are d1 = 21 and
d2 = 20 for this application. In addition, we pre-process each
feature vector by subtracting away the mean and dividing by
the standard deviation.

A. Linear local models
In our ﬁrst experiment, we apply linear regression within
each co-cluster and evaluate the performance with and without
dimensionality reduction through PCR. The experiment
is
conducted as follows:
• Step 1: We choose some (K, L).
• Step 2: We partition the data into training and test data.
This is accomplished by designating 80% of the available
user-movie pairs as training and the remaining available
data as test. Denote this partition by P1 .
• Step 3: We vary the eigenvalue mass λ across set
f0.1, 0.2, 0.3, . . . , 1g and for each value, we record the
training error εtrain (λ(P1 ), P1 ) under SCOAL-PCR.
(P1 ) with the minimum
∗
• Step 4: We choose the value λ
training error.
• Step 5: Once chosen, we apply SCOAL-PCR with eigen-
∗ to the test data and record the test error
value mass λ
(P1 ), P1 ).
∗
e(λ
• Step 6: We then repeat Steps 2 - 5 and record
(Pi ), Pi ) under different train/test partitions Pi , i =
∗
∑
e(λ
1, 2, . . . , 10.
• Step 7: We ﬁnally compute the average test error
(Pk ), Pk ) and the associ-
∗
10
εtest (K, L) = 1
k=1 e(λ
10
ated standard deviation.

4

• Step 8: We repeat the above experiment under SCOAL-
LASSO and record the corresponding average test error.
• Step 9: Finally, we repeat the above experiment with plain
SCOAL in Algorithm 1 and record the corresponding
average test error.
In Figs.1(a)-1(f), we compare the test error εtest (K, L) under
SCOAL, SCOAL-PCR and SCOAL-PCR for different conﬁg-
urations (K, L) 2 f2, 4, 6, 8, 10, 12g (cid:2) f2, 4, 6, 8, 10g. We can
clearly make the following observations from the plots.
For the SCOAL algorithm, for ﬁxed/small values of K such
as K 2 f2, 4g in Figs.1(a) and 1(b), as L increases, we ﬁrst
see a decrease in test error due to a better model ﬁt. However,
as we approach large L, i.e., employ a large number of local
models, we see the effects of over-ﬁtting. For larger values of
K such as K 2 f10, 12g, since we are already in the regime
of over-ﬁtting, as L increases, we only see further over-ﬁtting
and an increase in error as shown in Figs.1(e) and 1(f).
The story is quite different with the SCOAL-PCR algo-
rithm however. From Figs.1(a)-1(f), we see that SCOAL-
PCR effectively combats over-ﬁtting. In particular, SCOAL-
PCR signiﬁcantly outperforms SCOAL as L increases. The
performance gain are quite pronounced as (K, L) increases
and is plotted in Fig.2 at L = 10. We see that for large K ,
the decrease in test error is roughly 10%. Thus, by using the
PCR overlap, we are able to derive the dual beneﬁts of using
multiple local models while guarding against over-ﬁtting.

(a) K = 2

(b) K = 4

(c) K = 6

(d) K = 8

(e) K = 10

(f) K = 12

Fig. 1. Average test error versus number of column clusters L for the three
algorithms on the MovieLens data set.

Finally, we have also plotted the test error due to the LASSO
algorithm in Figs.1(a)-1(f). We see that as (K, L) increases, its

Fig. 2. Decrease in test error between SCOAL-PCR and SCOAL as a function
of K at L = 10.

performance lies between that of SCOAL and SCOAL-PCR.
Note that the LASSO solution is computationally less demand-
ing than SCOAL-PCR since the latter involves calculating the
SVD of the feature matrix. Nevertheless, while, some of the
gains may be attributed to the additional computation, the
remainder is due to the fact that PCR summarizes features
and retains as much information as possible while LASSO
eliminates features. Before we move on to the next section,

Fig. 3. Training error versus number of rows clusters L with K = 2.

we plot the training error under SCOAL, SCOAL-LASSO and
SCOAL-PCR for the cases L = 2 and K 2 f2, 4, 6, 8, 10, 12g.
The training error under SCOAL-PCR is seen to be the highest
while the test error in Figs.1(a)-1(f) at L = 2 is also seen to be
the largest. This could indicate high bias due to the fact that
compressing the feature dimension simpliﬁes the model. That
said, more investigation is needed here. An analysis of the
eigenvalue mass that is retained in each (K, L) conﬁguration
would shed light on the precise amount of compression that
is being applied at each stage.

B. Results on ERIM dataset
Next, we compare the performance of SCOAL vs. SCOAL-
PCR (i.e., Step 1-Step 8) on the ERIM data set, consisting
of household panel data collected by A.C. Nielsen [5]. In
particular, the data set contains past information about 1714
products that were purchased by a set of 121 households (if
one were to ignore households that have made less than two
purchases). There are three product attributes - market share,
price and the number of times the product was advertised. The
household attributes are income, number of residents, male
head employed, female head employed, total visits and total
expense. Thus, the feature dimensions are d1 = 6 and d2 = 3
for this application. As was done with MovieLens, we pre-
process each feature vector by subtracting away the mean and
dividing by the standard deviation. Element (i, j ) of the data
matrix contains the number of units of product j that was

0246810120.880.890.90.910.920.930.940.950.960.970.98Lεtest(K,L)  K = 2, SCOAL−PCRK = 2, SCOAL0246810120.860.870.880.890.90.910.920.93Lεtest  K = 4, LASSOK = 4, SCOAL−PCRK = 4, SCOAL0246810120.840.850.860.870.880.890.90.910.920.93Lεtest(K,L)  K = 6, SCOAL−PCRK = 6, SCOALK = 6, LASSO0246810120.840.850.860.870.880.890.90.910.920.930.94Lεtest  K = 8, LASSOK = 8, SCOAL−PCRK = 8, SCOAL0246810120.820.840.860.880.90.920.940.96Lεtest(K,L)  K = 10, SCOAL−PCRK = 10, SCOALK = 10, LASSO0246810120.820.840.860.880.90.920.940.96Lεtest  K = 12, LASSOK = 12, SCOALK = 12, SCOAL−PCR24681012024681012KPercentage gain (%)0246810120.820.840.860.880.90.920.940.96Kεtrain(K,L)  SCOAL−PCRLASSOSCOALpurchased by household i. The data matrix is sparse with
around 75% of the values being zero. The data matrix also
contains outliers - while 99.12% of the values are below 20,
some values are very large and range up to 200.
The results are provided in Figs. 4(a)-4(d). We see that
the PCR overlay is not as effective in this setting. More
speciﬁcally, for lower values of K such as K 2 f2, 4g,
SCOAL-PCR underperforms SCOAL while for larger values
K 2 f6, 8g, the test error is roughly the same. There might be
multiple reasons for this trend. Firstly, the feature dimension
is already quite small (nine features) in the case of this data
set and furthermore, the sparsity in data might lead to poor
PCR models in the latent (compressed) feature space. Further
investigation is needed here to determine the root cause for
the lukewarm performance of PCR.

(a) K = 2

(b) K = 4

(c) K = 6

(d) K = 8

Fig. 4. Average test error versus number of column clusters L for the three
algorithms on the ERIM data set.

In the next section, we revert back to the MovieLens data
set but this time however, we brief study the use of non-linear
local models.

C. Non-linear local models
While the SCOAL algorithm as presented in Algorithm 1
uses linear regression as the predictive model within each co-
cluster, in its most general form [1], it is a framework that
accommodates any model within each co-cluster.
We brieﬂy study the impact of employing PCR dimension
reduction in conjunction with regression trees inside each co-
cluster. In other words, we ﬁrst reduce the dimension of the
feature matrix X as per (3) and then construct a regression
tree. The prediction that is made at any given leaf consists of
the centroid of all the training feature vectors that percolate
down to that leaf. We control the size of the tree (and hence
the tendency to overﬁt) by enforcing a “split” criterion - a
node is allowed to split and add another level to the tree only
if the number of training feature vectors that percolate down
to that node exceeds 50% of the total size of the data in the
co-cluster (the number of rows of feature matrix X ).

5

We repeat Steps 1-Step 8 for an assortment of cluster sizes.
The results are presented in Table I. As with the earlier results
with linear models on the ERIM dataset, we do not see any
signiﬁcant gains due to PCR. A more comprehensive analysis
is necessary at this point but was not possible due to lack of
time.

TABLE I
SCOAL -PCR V ER SU S SCOAL W I TH R EGR E S S ION TR EE S .

Algorithm
SCOAL
SCOAL
SCOAL
SCOAL
SCOAL-PCR
SCOAL-PCR
SCOAL-PCR
SCOAL-PCR
SCOAL-PCR
SCOAL-PCR

(K; L) conﬁguration
(2,2)
(4,4)
(6,2)
(6,4)
(2,2)
(4,4)
(6,6)
(6,8)
(8,6)
(8,8)

Average test error
0.9620
0.8783
0.9220
0.8629
0.9664
0.8878
0.8797
0.8815
0.8765
0.8868

V. CONC LUD ING R EMARK S
In this project, we have shown that dimensionality reduction
may be used effectively in conjunction with the SCOAL
divide-and-conquer approach. In particular, PCR-based dimen-
sionality reduction performs increasingly better as the num-
ber of co-clusters increases. SCOAL-PCR outperforms plain
SCOAL by almost 10% as we approach 100 co-clusters when
using linear models inside each co-cluster. Thus, in large-data
applications where it might be too expensive to perform model
selection and select a suitable (K, L) conﬁguration, it might be
a safe bet to employ the SCOAL-PCR algorithm with a large
number of co-clusters. LASSO on the other hand provides
a low-complexity alternative to PCR while paying a price
in terms of test error. LASSO-based feature selection does
still outperform plain SCOAL as the number of co-clusters
increases.
Further research is needed to systematically determine and
quantify the reasons for the gains, or lack thereof, due to PCR
and/or LASSO. As mentioned earlier, a detailed analysis of the
eigenvalue mass that is retained for each (K, L) conﬁguration
would give us some more insight. Finally, it is worth con-
sidering alternate dimensionality reduction techniques such as
partial least squares, which projects the features onto a basis
that captures most variance across both the features and the
labels. Principle components regression ignores the labels and
ﬁnds directions that maximize the variance amongst only the
feature vectors.

R E F ER ENC E S
[1] M. Deodhar and J. Ghosh, “Simultaneous Co-clustering and Learning
from Complex Data”, Proc. 13th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, August 2007.
[2] I.T. Jolliffe, “A note on the Use of Principal Components in Regression,”
Journal of the Royal Statistical Society, Series C (Applied Statistics), vol.
31, pp. 300-303, 1982.
[3] MovieLens data set, http://www.grouplens.org/system/ﬁles/ml- data.tar
0.gz, University of Minnesota.
[4] R. Tibshirani, “Regression shrinkage and selection via the lasso”, J. Roy.
Statist. Soc. Ser. B, pp. 267-288, 1996.
[5] Kilts Center
for Marketing, ERIM Database,
chicagobooth.edu/marketing/databases/erim/.

http://research.

0246810120.330.340.350.360.370.380.390.4Lεtest  K = 2, SCOAL−PCRK = 2, SCOAL0246810120.330.340.350.360.370.380.390.4Lεtest  K = 4, SCOAL−PCRK = 4, SCOAL0246810120.330.340.350.360.370.380.390.4Lεtest  K = 6, SCOAL−PCRK = 6, SCOAL0246810120.330.340.350.360.370.380.390.4Lεtest  K = 8, SCOAL−PCRK = 8, SCOAL
Breast Cancer Prognosis

Catherine Lu and JJ Liu
{cglu, jsquared}@stanford.edu
December 14, 2012

1

Introduction

2 Objective

An accurate breast cancer prognosis, or breast cancer sur-
vivability prediction, is important as it often guides the
treatment course of action, ability to claim additional ﬁ-
nancial support from the government, actions of the pa-
tient and family, and more [1]. Predicting breast cancer
survivability is commonly done using clinical features.
TNM staging, the globally accepted standard used to de-
scribe cancer, was devised more than 60 years ago and
only looks at three features: size of the tumor, number of
regional lymph nodes with cancer, and the spread of can-
cer to other parts of the body. With the advent of afford-
able genomic sequencing and acceleration of ﬁndings in
molecular biology in the past decade, molecular features
may be practical to improve breast cancer prognosis.
Molecular diagnostics for cancer therapy decision-
making have shown initial promising clinical results.
This has lead to a ﬂood of published reports of signa-
tures predictive of breast cancer phenotypes, and several
molecular diagnostic tests for cancer therapy decision-
making have gained regulatory approval in recent years
[2, 3]. However, there is no consensus for the most accu-
rate computational methods and models to predict breast
cancer survivability. In addition, it is unclear that incor-
porating molecular data as a complement or replacement
for traditional clinical diagnostic tools adds any value [4].
Therefore, it is necessary to objectively assess whether
genomic data currently provides value beyond traditional
clinical diagnosis tools.
To aid in efforts to solve this problem, we predicted
breast cancer survivability with machine learning tech-
niques as part of the DREAM Breast Cancer Prognosis
Challenge. The ultimate goal of the challenge is to objec-
tively compare many computational algorithms through
providing a common training dataset in an effort to ﬁnd
the best features for breast cancer prognosis. The dataset
provided contains standard clinical measurements in ad-
dition to genomic information, thus allowing genomic in-
formation to be compared with standard clinical features.

Our goal was to predict survival for each individual. We
had two approaches: predicting a discrete survival status
based on time since diagnosis and other features, and pre-
dicting a continuous survival time based on all features.

3 Data

Breast cancer sample data is made available through the
DREAM challenge from the METABRIC data of 1,000
breast tumor samples used in a previous study [2], where
data origin and preprocessing is explained in detail. We
further process the data by discarding samples with miss-
ing values, and are thus left with 931 samples.

3.1 Survival

There are two indicators of survival:
time from breast
cancer diagnosis to last follow-up and status of the pa-
tient (alive or dead) at last follow-up time. Survival data
is right-censored, since patients may be alive at the end
of the study or lost to follow-up.

3.2 Gene Expression

Gene expression is generated using molecular proﬁling
platforms, described in full detail in another study [2].
The genes used as training features are narrowed to a list
of 9 suggested by the DREAM challenge and previous
literature. We used two estrogen pathway genes (ER and
PR), two human epidermal growth factor 2 receptor am-
plicon genes (HER2 SNP6 and GII), and ﬁve immune
response genes (CXCL10, STAT1, GBP1, GZMA, and
CD19).

3.3 Clinical Annotations

In addition, we have the following clinical annotations,
the classic features used for breast cancer prognosis:

1

5 DEVELOPMENT AND RESULTS

2

Feature
Age

Treatment

Lymph nodes positive*

Size*

Grade

Metric
Years
NONE, HT, RT,
CT, HT/RT, HT/CT,
RT/CT, CT/HT/RT

0, 1, 2, 3

0, 1, 2, 3

0, 1, 2

Description
Age of patient at diagnosis
HT: hormone therapy
RT: radiation therapy
CT: chemotherapy
Number of lymph nodes found with cancer
0: no nodes
1: 1-3 nodes
2: 4-9 nodes
3: over 9 nodes
0: 0-20 mm
1: 21-50 mm
2: over 50 mm
3: Direct extension to chest wall or skin
0: Nottingham score 3-5
1: Nottingham score 6-7
2: Nottingham score 8-9
The score is a semi-quantitative measure of
three histopathological characteristics seen
under a microscope by a pathologist.
Presence of ER from IHC protocol

Estrogen Receptor Immunohistochemistry (ER IHC)
*Used in standard TNM classiﬁcation of breast cancer

+,-

4 Measuring Performance

4.1 Predicting Survival Status

We initially build machine learning models that predict
the patient’s status (dead or alive) based on all other fea-
tures. We measure performance using 3-fold cross val-
idation accuracy in addition to a data set accuracy for
training and predicting on the same, entire data set.

4.2 Predicting Survival Time

Next, we predict survival time of the patient. However,
we do not have survival time for all patients; the data is
highly skewed and right-censored. Patients may drop out
of the study at any point or still be alive by the end of the
study.
With a data set of only 931, it is extremely important
to still use all of the training data. Two patients’ survival
times can be ranked not only if both have uncensored
survival times but also if the uncensored time or one is
smaller than the censored survival time of the other. One
of the most commonly used performance measures for
survival models is the concordance index (CI) [5]. CI
is the fraction of all pairs of subjects whose predicted
survival times are ordered correctly across all patients. A
CI of 1 indicates perfection prediction accuracy, while a
CI of 0.5 is as good as a random predictor.
Hence, we measure performance using 3-fold cross
validation (3-fold CV) for CI in addition to CI for train-

ing and predicting on the same, entire data set.

5 Development and Results

5.1 Predicting Survival Status

We used patient status as the target variable and all other
features as the input features. We used the R Caret pack-
age, which provides a library for a number of machine
learning models, to write and run different algorithms.

5.1.1 Results

First, we used the K-Nearest Neighbor algorithm to clas-
sify our data based on the closest feature training sam-
ples. We use a k-value of 2, to see if there were any un-
derlying relationships among features for patients based
on status. However, our 3-fold CV accuracy was low
(0.519).
We then tried 5 supervised learning models. None of
them performed better than 0.556 for 3-fold CV, though
running and predicting on the entire data set gave values
ranging from 0.693 to 0.716. The models were overﬁt-
ting the data and were not representing the relationships
between the features accurately.
In particular, the Gradient Boosting Model (GBM), an
ensemble learning method which uses multiple weak pre-
diction models to form a single model in a stage-wise
fashion, resulted in the most overﬁtted model.

5 DEVELOPMENT AND RESULTS

3

Algorithm
K-Nearest Neighbor
Multinomial
Linear Discriminant Analysis
Generalized Linear Models
Linear Support Vector Machines
Generalized Boosted Model
Cox Proportional-Hazard Regression*
Random Survival Forest*
*Concordance Index

3-Fold CV
0.519
0.549
0.542
0.545
0.556
0.541
0.702
0.813

Data Set
0.597
0.694
0.693
0.694
0.697
0.716
0.706
0.812

Out of the standard machine learning approaches, lin-
ear SVM performed slightly better than the rest, possibly
because it did not overﬁt the data as much as other mod-
els.
It is interesting to note that Linear Discriminant Anal-
ysis (LDA) performed approximately the same as Gen-
eralized Linear Models (GLM), even though LDA is a
more simple model than GLM. LDA ﬁnds a linear com-
bination of our clinical features which characterizes the
patient survival status. We also used GLM, a generaliza-
tion of ordinary linear regression models that allow for
response variables that do not follow a normal distribu-
tion, because our response variables do not necessarily
follow a normal distribution, but instead could follow a
distribution more similar to a log-odds model due to our
prediction of status as a Bernoulli variable.

5.2 Predicting Survival time

We then predicted survival time using all features as input
and the CI as the measurement of model performance.
The outputted survival models compute the time it takes
for death to occur according to the features.

5.2.1 Cox Proportional-Hazard Regression with
Akaike Information Criterion

Proportional hazard (PH) models are the standard for
studying the effects of features on survival time distribu-
tions. A hazard function λ(t ) measures the instantaneous
rate of death at time t .
The PH model assumes there is a multiplicative effect
of the features on the hazard function:

λ(t |x) = λ0 (t )e(wT x)
(1)
where λ(t |x) is the hazard function with features x,
λ0 (t ) is the baseline hazard function when x = 0, w is
the vector of unknown parameters, and ewT x is the rela-
tive hazard function.

The Cox Proportional-Hazard [6] approach estimates
weight w by leaving the baseline hazard function unspe-
ciﬁc and maximizing the likelihood:

(2)

ewT xi
L(w) = ∏
∑T j ≥Ti ewT x j
Ti uncensored
where Ti is survival time of patient i.
After this estimation, we trained using weighted linear
regression. In order to avoid overﬁtting, we use Akaike
Information Criterion (AIC) on the features passed to the
Cox model. The AIC is a measure of the relative good-
ness of ﬁt of a statistical model, often described as a
tradeoff between bias and variance or between model ac-
curacy and complexity. We ﬁrst ﬁnd the corresponding
AIC values, and selected the model that minimizes infor-
mation loss.
We obtained a 3-fold CV CI of 0.702, comparable to
the CI of 0.812 for training and predicting over the entire
data set.

5.2.2 Random Survival Forest

The Random Survival Forest (RSF) algorithm [7] is an
ensemble tree method for the analysis of right censored
survival data. More speciﬁcally, the algorithm performs
the following:

1. Draw B bootstrap samples from the original data,
where each bootstrap sample excludes on average
37% of the data, called out-of-bag data (OOB data).

2. Grow a survival tree for each bootstrap sample. At
each node, randomly select p variables. Then, split
the node with the candidate variable which maxi-
mizes survival difference between daughter nodes.

3. Grow the tree to full size.

4. Calculate a hazard function (HF) for each tree, and
average to obtain the ensemble HF.

6 RSF ANALYSIS

4

Based on the size of our data, we ran a RSF algorithm
with the number of trees to grow to 1000. We use the
logrank splitting rule, which splits tree nodes by maxi-
mization of the log-rank test statistic.
We obtained a 3-fold CI of 0.813, which is also com-
parable to the CI of 0.812 for training and predicting over
the entire data set.

6 RSF Analysis

We chose the RSF model, the best performing model, to
gain insights into relationships among features.

6.1 Feature Selection

We determined which features contributed most to the
learning using backward search feature selection.

Features ommitted (cumulative)
None ommitted
ER IHC status
ER expression
Grade
HER2 SNP6 state
GBP1 expression
CD19 expression
Treatment
CXCL10 expression
GZMA expression
PR expression
Size
GII
Lymph nodes posititve
Age

3-fold CV CI
0.812
0.815
0.815
0.812
0.810
0.802
0.789
0.783
0.775
0.769
0.753
0.717
0.689
0.642
N/A (all omitted)

The best 3-fold CV CI was achieved by taking all
features except for EHR IHC status and ER expression.
EHR IHC status appears to lower the CI and ER expres-
sion does not add any value.

6.2 Ensemble Analysis

The following ﬁgure shows the ensemble survival func-
tion for each patient. The thick red line is overall en-
semble survival, and the thick green line is Nelson-Aalen
estimator. The Nelson-Aelen, often used to give an idea
of the survival rate shape, is given by the equation:

H (t ) = ∑
ti≤t

di
ni

(3)

where di is the number of deaths at ti and ni is the total
number of patients alive at ti .

Note that the overall ensemble survival begins to devi-
ate from the Nelson-Aelen estimator at later times.
The second ﬁgure below shows the same relationship,
where it is shown that RSF tends to predict higher sur-
vival probabilities when survival proportions in the data
set are low.

5

References

[1] R. Henderson, M. Jones, J. Stare, ”Accuracy of
Point Predictions in Survival Analysis,” Statistics in
Medicine, 2001.

[2] L. J. vant Veer, H. Dai, M. J. van de Vijver, Y.
D. He, A. A. M. Hart, M. Mao, H. L. Peterse, K.
van der Kooy, M. J. Marton, A. T. Witteveen, G. J.
Schreiber, R. M. Kerkhoven, C. Roberts, P. S. Lins-
ley, R. Bernards, and S. H. Friend, Gene expression
proﬁling predicts clinical outcome of breast cancer,
Nature, vol. 415, no. 6871, pp. 530536, Jan. 2002.

[3] S. Paik, S. Shak, G. Tang, C. Kim, J. Baker, M.
Cronin, F. L. Baehner, M. G. Walker, D. Watson, T.
Park, W. Hiller, E. R. Fisher, D. L. Wickerham, J.
Bryant, and N. Wolmark, A multigene assay to pre-
dict recurrence of tamoxifen-treated, node-negative
breast cancer, N. Engl. J. Med., vol. 351, no. 27, pp.
28172826, Dec. 2004.

[4] C. Curtis, S. P. Shah, S.-F. Chin, G. Turashvili, O.
M. Rueda, M. J. Dunning, D. Speed, A. G. Lynch,
S. Samarajiwa, Y. Yuan, S. GrŁf, G. Ha, G. Haffari,
A. Bashashati, R. Russell, S. McKinney, M. Group,
A. Langerd, A. Green, E. Provenzano, G. Wishart, S.
Pinder, P. Watson, F. Markowetz, L. Murphy, I. Ellis,
A. Purushotham, A.-L. Brresen-Dale, J. D. Brenton,
S. Tavar, C. Caldas, and S. Aparicio, The genomic
and transcriptomic architecture of 2,000 breast tu-
mours reveals novel subgroups, Nature, 2012.

[5] V. C. Raykar, H. Steck, and B. Krishnapuram, ”On
Ranking in Survival Analysis: Bounds on the Con-
cordance Index,” NIPS, 2007.

[6] J. Fox, ”Cox Proportional-Hazards Regression for
Survival Data”, Appendix to ”An R and S-PLUS
Companion to Applied Regression”, 2002.

[7] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and
M. S. Lauer, ”Random Survival Forests,” The Annals
of Applied Statistics, 2008.

REFERENCES

7 Discussion

Breast cancer prognosis presents an important challenge
with many real life implications.
In this paper, we
have described our use of various machine learning ap-
proaches to the complex problem of predicting breast
cancer survivability rate, with the data provided through
the DREAM Breast Cancer Prognosis Challenge.
Our results indicate that it is difﬁcult to create accurate
standard machine learning models for predicting patient
survival status. Survival data has many unique proper-
ties. The standard machine learning models did not have
any notion of a hazard function for determining patient
survival status. Instead, it found unreal relationships that
solely existed in the unique data set, which was seen
from the large difference in accuracy between 3-fold CV
and accuracy from training and predicting on the data set
(which were also quite low).
On the other hand, the two models that predicted haz-
ard functions seemed to do quite well, though it is difﬁ-
cult to compare due to the different model performance
measurements.
It appears that both the Cox and RSF
models capture the relationship among features and sur-
vival outcome, as seen in almost identical values between
the 3-fold CV CI and CI from training and predicting on
the data set.
From feature analysis, we learned that at least for the
RSF model, age at diagnosis was the best feature predic-
tor. In addition, eliminating two features (estrogen recep-
tor copy number and estrogen receptor gene expression)
in the model lead to a slightly higher 3-fold cross valida-
tion score than with all features.
From RSF ensemble analysis, we saw that RSF
seemed to perform better at predicting either patients
with less time since diagnosis or when there is higher
probability of survival, or both. Therefore, RSF com-
bined with another algorithm that performs well in these
conditions may produce even better results.
This work has limitations and could be improved in
three major ways. First, we should examine all genes
available in the data set and, using feature selection, ﬁnd
the most predictive genes. Second, we should modify our
regular machine learning models to predict the Cox haz-
ard function to give each model the right-censored data
relationship that exists. It is not necessarily that RSF is
the best predictor of survival out of the algorithms we
have used. Third, we should run our algorithms on more
data. To do so, we should modify our algorithms to im-
pute or skip missing features without discarding the en-
tire training example and use publicly available data sets.


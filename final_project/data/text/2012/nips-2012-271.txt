Recovery of Sparse Probability Measures via Convex
Programming

Mert Pilanci and Laurent El Ghaoui
Electrical Engineering and Computer Science
University of California Berkeley
Berkeley, CA 94720
{mert,elghaoui}@eecs.berkeley.edu

Venkat Chandrasekaran
Department of Computing and Mathematical Sciences
California Institute of Technology
Pasadena, CA 91125
venkatc@caltech.edu

Abstract

We consider the problem of cardinality penalized optimization of a convex func-
tion over the probability simplex with additional convex constraints. The classical
(cid:96)1 regularizer fails to promote sparsity on the probability simplex since (cid:96)1 norm
on the probability simplex is trivially constant. We propose a direct relaxation of
the minimum cardinality problem and show that it can be efﬁciently solved using
convex programming. As a ﬁrst application we consider recovering a sparse prob-
ability measure given moment constraints, in which our formulation becomes lin-
ear programming, hence can be solved very efﬁciently. A sufﬁcient condition for
exact recovery of the minimum cardinality solution is derived for arbitrary afﬁne
constraints. We then develop a penalized version for the noisy setting which can
be solved using second order cone programs. The proposed method outperforms
known rescaling heuristics based on (cid:96)1 norm. As a second application we consider
convex clustering using a sparse Gaussian mixture and compare our results with
the well known soft k-means algorithm.

1

Introduction

We consider optimization problems of the following form,
p∗ =

min
x∈C, 1T x=1, x≥0

f (x) + λcard(x)

where f is a convex function, C is a convex set, card(x) denotes the number of nonzero elements of
x and λ ≥ 0 is a given tradeoff parameter for adjusting desired sparsity. Since the cardinality penalty
is inherently of combinatorial nature, these problems are in general not solvable in polynomial-time.
In recent years (cid:96)1 norm penalization as a proxy for penalizing cardinality has attracted a great deal
of attention in machine learning, statistics, engineering and applied mathematics [1], [2], [3], [4].
However the aforementioned types of sparse probability optimization problems are not amenable to
the (cid:96)1 heuristic since (cid:107)x(cid:107)1 = 1T x = 1 is constant on the probability simplex. Numerous prob-
lems in machine learning, statistics, ﬁnance and signal processing fall into this category however to
the authors’ knowledge there is no known general convex optimization strategy for such problems
constrained on the probability simplex. The aim of this paper is to claim that the reciprocal of the

(a) Level sets of the regularization function
on the probability simplex

1
maxi xi

(b) The sparsest probability distribution on the set C
is x∗ (green) which also minimizes
on the
1
maxi xi
intersection (red)

Figure 1: Probability simplex and the reciprocal of the inﬁnity norm

inﬁnity-norm, i.e.,
can be used as a convex heuristic for penalizing cardinality on the prob-
1
maxi xi
ability simplex and the resulting relaxations can be solved via convex optimization. Figure 1(a) and
1(b) depict the level sets and an example of a sparse probability measure which has maximal inﬁnity
norm. In the following sections we expand our discussion by exploring two speciﬁc problems: re-
covering a measure from given moments where f = 0 and C is afﬁne, and convex clustering where
f is a log-likelihood and C = R. For the former case we give a sufﬁcient condition for this convex
relaxation to exactly recover the minimal cardinality solution of p∗ . We then present numerical sim-
ulations for the both problems which suggest that the proposed scheme offers a very efﬁcient convex
relaxation for penalizing cardinality on the probability simplex.

2 Optimizing over sparse probability measures
We begin the discussion by ﬁrst taking an alternative approach to the cardinality penalized optimiza-
n(cid:88)
tion by directly lower-bounding the original hard problem using the following relation
|xi | ≤ card(x) (cid:107)x(cid:107)∞
|xi | ≤ card(x) max
(cid:107)x(cid:107)1 =
i
i=1
which is essentially one of the core motivations of using (cid:96)1 penalty as a proxy for cardinality.
When constrained to the probability simplex, the lower-bound for the cardinality simply becomes
≤ card(x). Using this bound on the cardinality, we immediately have a lower-bound on our
1
maxi xi
original NP-hard problem which we denote by p∗
∞ :
p∗ ≥ p∗
∞ :=

f (x) + λ

(1)

min
x∈C, 1T x=1, x≥0

1
maxi xi

The function
is concave and hence the above lower-bounding problem is not a convex
1
maxi xi
optimization problem. However below we show that the above problem can be exactly solved using
convex programming.
Proposition 2.1. The lower-bounding problem deﬁned by p∗
(cid:26)
(cid:27)
∞ can be globally solved using the
following n convex programs in n + 1 dimensions:
p∗ ≥ p∗
∞ = min
min
x∈C, 1T x=1, x≥0, t≥0
i=1,...,n
Note that the constraint xi ≥ λ/t is jointly convex since 1/t is convex in t ∈ R+ , and they can be
handled in most of the general purpose convex optimizers, e.g. cvx, using either the positive inverse
function or rotated cone constraints.

: xi ≥ λ/t

f (x) + t

(2)

.

C x * Proof.

p∗
∞ =

min
x∈C, 1T x=1, x≥0

f (x) + min
i

= min
i

min
x∈C, 1T x=1, x≥0

f (x) +

λ
xi
λ
xi

= min
i

min
x∈C, 1T x=1, x≥0,t≥0

f (x) + t s.t.

≤ t

λ
xi

(3)

(4)

(5)

The above formulation can be used to efﬁciently approximate the original cardinality constrained
problem by lower-bounding for arbitrary convex f and C .
In the next section we show how to
compute the quality of approximation.

2.1 Computing a bound on the quality of approximation

By the virtue of being a relaxation to the original cardinality problem, we have the following remark-
able property. Let ˆx be an optimal solution to the convex program p∗
∞ , then we have the following
relation
f ( ˆx) + λcard( ˆx) ≥ p∗ ≥ p∗
(6)
∞
Since the left-hand side and right-hand side of the above bound are readily available when p∗
∞
deﬁned in (2) is solved, we immediately have a bound on the quality of relaxation. More speciﬁcally
the relaxation is exact, i.e., we ﬁnd a solution for the original cardinality penalized problem, if the
following holds:
f ( ˆx) + λcard( ˆx) = p∗
∞
It should be noted that for general cardinality penalized problems, using (cid:96)1 heuristic does not yield
such a quality bound, since it is not a lower or upper bound in general. Moreover most of the known
equivalence conditions for (cid:96)1 heuristics such as Restricted Isometry Property and variants are NP-
hard to check. Therefore a remarkable property of the proposed scheme is that it comes with a
simple computable bound on the quality of approximation.

3 Recovering a Sparse Measure

Suppose that µ is a discrete probability measure and we would like to know the sparsest measure
satisfying some arbitrary moment constraints:
p∗ = min
: Eµ [Xi ] = bi , i = 1, . . . , m
card(µ)
µ
where Xi ’s are random variables and Eµ denotes expectation with respect to the measure µ. One
motivation for the above problem is the fact that it upper-bounds the minimum entropy power prob-
lem:
where H (µ) := − (cid:80)
p∗ ≥ min
: Eµ [Xi ] = bi , i = 1, . . . , m
µ
i µi log µi is the Shannon entropy. Both of the above problems are non-convex
and in general very hard to solve.
When viewed as a ﬁnite dimensional optimization problem the minimum cardinality problem can
be cast as a linear sparse recovery problem:
p∗ = min
1T x=1, x≥0
As noted previously, applying the (cid:96)1 heuristic doesn’t work and it does not even yield a unique
solution when the problem is underdetermined since it simply solves a feasibility problem:
(cid:107)x(cid:107)1
p∗
1 =
1 : Ax = b

card(x)

: Ax = b

: Ax = b

(7)

(8)

(9)

exp H (µ)

min
1T x=1, x≥0
min
1T x=1, x≥0

=

and recovers the true minimum cardinality solution if and only if the set 1T x = 1, x ≥ 0, Ax = b is
a singleton. This condition may hold in some cases, i.e. when the ﬁrst 2k − 1 moments are available,
i.e., A is a Vandermonde matrix where k = card(x) [6]. However in general this set is a polyhedron
containing dense vectors. Below we show how the proposed scheme applies to this problem.
(cid:27)
(cid:26)
Using general form in (2), the proposed relaxation is given by the following,
(p∗ )−1 ≤ (p∗
∞ )−1 = max
: Ax = b
max
1T x=1, x≥0
i=1,...,n
which can be solved very efﬁciently by solving n linear programs in n variables. The total complex-
ity is at most O(n4 ) using a primal-dual LP solver.
(cid:26)
(cid:27)
It’s easy to check that strong duality holds and the dual problems are given by the following:
wT b + λ : AT w + λ1 ≥ ei
∞ )−1 = max
(p∗
min
i=1,...,n
w, λ
where 1 is the all ones vector and ei is all zeros with a one in only i’th coordinate.

(10)

(11)

xi

.

.

3.1 An alternative minimal cardinality selection scheme

When the desired criteria is to ﬁnd a minimum cardinality probability vector satisfying Ax = b, the
following alternative selection scheme offers a further reﬁnement, by picking the lowest cardinality
solution among the n linear programming solutions. Deﬁne

xi

ˆxi : = arg max
1T x=1, x≥0
card( ˆxi )
ˆxmin : = arg min
i=1,...,n
The following theorem gives a sufﬁcient condition for the recovery of a sparse measure using the
above method.
Theorem 3.1. Assume that the solution to p∗ in (7) is unique and given by x∗ . If the following
condition holds

: Ax = b

(13)

(12)

min
xi s.t. AS x = AS c y > 0
1T x=1, y≥0, 1T y=1
where b = Ax∗ and AS is the submatrix containing columns of A corresponding to non-zero ele-
ments of x∗ and AS c is the submatrix of remaining columns, then the convex linear program
: Ax = b
xi
max
1T x=1, x≥0

has a unique solution given by x∗ .
Let C onv(a1 , . . . , am ) denote the convex hull of the m vectors {a1 , . . . , am }. The following corol-
lary depicts a geometric condition for recovery.
Corollary 3.2. If C onv(AS c ) does not intersect an extreme point of C onv(AS ) then ˆxmin = x∗ ,
i.e. we recover the minimum cardinality solution using n linear programs.

Proof Outline:
Consider k’th inner linear program deﬁned in the problem p∗
∞ . Using the optimality conditions of
the primal-dual linear program pairs in (10) and (11), it can be shown that the existence of a pair
(w, λ) satisfying

(14)
AT
S w + λ1 = ek
(15)
AT
S c w + λ1 > 0
implies that the support of solution of the linear program is exactly equal to the support of x∗ , and
in particular they have the same cardinality. Since the solution of p∗ is unique and has minimum
cardinality, we conclude that x∗ is indeed the unique solution to the k’th linear program. Applying
Farkas’ lemma and duality theory we arrive at the conditions deﬁned in Theorem 3.1. The corollary
follows by ﬁrst observing that the condition of Theorem 3.1 is satisﬁed if C onv(AS c ) does not
intersect an extreme point of C onv(AS ). Finally observe that if any of the n linear programs recover
the minimal cardinality solution then ˆxmin = x∗ , since card( ˆxmin ) ≤ card( ˆxk ), ∀k .

3.2 Noisy measure recovery
When the data contains noise and inaccuracies, such as the case when using empirical moments
(cid:27)
(cid:26)
instead of exact moments, we propose the following noise-aware robust version, which follows
from the general recipe given in the ﬁrst section:
: xi ≥ λ/t
(cid:107)Ax − b(cid:107)2
(16)
.
min
min
2 + t
1T x=1, x≥0,t≥0
i=1,...,n
where λ ≥ 0 is a penalty parameter for encouraging sparsity. The above problem can be solved
using n second-order cone programs in n + 1 variables, hence has O(n4 ) worst case complexity.
The proposed measure recovery algorithms are investigated and compared with a known suboptimal
heuristic in Section 6.

4 Convex Clustering

1
n

1
n

L =

log

L :=

log

In this section we base our discussion on the exemplar based convex clustering framework of [8].
Given a set of data points {z1 , . . . , zn} of d-dimensional vectors, the task of clustering is to ﬁt a

 k(cid:88)
n(cid:88)
mixture probability model to maximize the log likelihood function
xj f (zi ; mj )
j=1
i=1
where f (z ; m) is an exponential family distribution on Z with parameter m, and x is a k-dimensional
vector on the probability simplex denoting the mixture weights. For the standard multivariate Nor-
mal distribution we have f (zi ; mj ) = e−β(cid:107)zi−mj (cid:107)2
2 for some parameter β > 0. As in [8] we’ll
further assume that the mean parameter mj is one of the examples zi which is unknown a-priori.

 k(cid:88)
This assumption helps to simply the log-likelihood whose data dependence is now only through a
n(cid:88)
kernel matrix Kij := e−β(cid:107)zi−zj (cid:107)2
2 as follows
xj e−β(cid:107)zi−zj (cid:107)2
 k(cid:88)

2
n(cid:88)
i=1
j=1
1
n
i=1
j=1
Partitioning the data {z1 , . . . , zn} into few clusters is equivalent to have a sparse mixture x, i.e.,
 − λcardx
 k(cid:88)
each example is assigned to few centers (which are some other examples). Therefore to cluster the
n(cid:88)
data we propose to approximate the following cardinality penalized problem,
p∗
log
xj Kij
c := max
1T x=1, x≥0
i=1
j=1
 − λ exp H (x)
 k(cid:88)
As hinted previously, the above problem can be seen as a lower-bound for the entropy penalized
n(cid:88)
problem
xj Kij
i=1
j=1
where H (x) is the Shannon entropy of the mixture probability vector.
 k(cid:88)
 −
Applying our convexiﬁcation strategy, we arrive at another upper-bound which can be computed via
n(cid:88)
convex optimization
c ≤ p∗
p∗
∞ := max
1T x=1, x≥0
j=1
i=1
We investigate the above approach in a numerical example in Section 6 and compare with the well-
known soft k-means algorithm.

c ≤ max
p∗
1T x=1, x≥0

λ
maxi xi

log

xj Kij

log

xj Kij

(17)

(18)

log

=

(19)

(20)

(21)

5 Algorithms
5.1 Exponentiated Gradient
employs the Kullback-Leibler divergence D(x, y) = (cid:80)
Exponentiated gradient [7] is a proximal algorithm to optimize over the probability simplex which
between two probability distri-
i xi log xi
yi
butions. For minimizing a convex function ψ the exponentiated gradient updates are given by the
following:
ψ(xk ) + ∇ψ(xk )T (x − xk ) +
1
xk+1 = arg min
D(x, xk )
α

(cid:88)
x
When applied to the general form of 2 it yields the following updates to solve the i’th problem of
p∗
∞
xk+1
i = rk
i xk
rk
j xk
i /
j
i = exp (cid:0)α(∇i f (xk ) − λ/x2
i )(cid:1)
j
where the weights ri are exponentiated gradients:
rk
We also note that the above updates can be done in parallel for the n convex programs, and they are
guaranteed to converge to the optimum.

6 Numerical Results

6.1 Recovering a Measure from Gaussian Measurements

: Ax = b

Here we show that the proposed recovery scheme is able to recover a sparse measure exactly with
overwhelming probability, when the matrix A ∈ Rm×n is chosen from the independent Gaussian
ensemble, i.e, Ai,j ∼ N (0, 1) i.i.d.
As an alternative method we consider a commonly employed simple heuristic to optimize over
a probability measure which ﬁrst drops the constraint 1T x = 1 and solves the corresponding (cid:96)1
penalized problem. And ﬁnally rescales the optimal x such that 1T x = 1. In the worst case, this
procedure recovers the true solution whenever minimizing (cid:96)1 -norm recovers the solution, i.e., when
there is only one feasible vector satistfying Ax = b and x ≥ 0, 1T x = 1. This is clearly a
suboptimal approach and we will refer it as the rescaling heuristic. We set n = 50 and randomly
pick a probability vector x∗ which is k sparse, let b = Ax∗ be m noiseless measurements, then
(cid:27)
(cid:26)
check the probability of recovery, i.e. ˆx = x∗ where ˆx is the solution to,
max
max
1T x=1, x≥0
i=1,...,n
Figure 2(a) shows the probability of exact recovery as a function of m, the number of measurements,
in 100 independent realizations of A for the proposed LP formulation and the rescaling heuristic.
As it can be seen in Figure 2(a), the proposed method recovers the correct measure with probability
almost 1 when m ≥ 5. Quite interestingly the rescaling heuristic doesn’t succeed to recover the true
measure with high probability even for a cardinality 2 vector.
(cid:27)
(cid:26)
We then add normal distributed noise with standard deviation 0.1 on the observations and solve,
: xi ≥ λ/t
(cid:107)Ax − b(cid:107)2
min
min
2 + t
1T x=1, x≥0,t≥0
i=1,...,n
We compare the above approach by the corresponding rescaling heuristic, which ﬁrst solves a non-
negative Lasso,
(cid:107)Ax − b(cid:107)2
2 + λ (cid:107)x(cid:107)1
min
x≥0
then rescales x such that 1T x = 1. For each realization of A and measurement noise we run both
methods using a primal-dual interior point solver for 30 equally spaced values of λ ∈ [0, 10] and
record the minimum error (cid:107) ˆx − x∗ (cid:107)1 . The average error over 100 realizations are shown in Figure
2(b). Is it can be seen in the ﬁgure the proposed scheme clearly outperforms the rescaling heuristic
since it can utilize the fact that x is on the probability simplex, without trivializing it’s complexity
regularizer.

(22)

(23)

(24)

xi

.

.

(a) Probability of exact recovery as a function of m

(b) Average error for noisy recovery as a function of m

Figure 2: A comparison of the exact recovery probability in the noiseless setting (top) and estimation
error in the noisy setting (bottom) of the proposed approach and the rescaled (cid:96)1 heuristic

6.2 Convex Clustering

We generate synthetic data using a Gaussian mixture of 10 components with identity covariances
and cluster the data using the proposed method, the resulting clusters given by the mixture density is
presented in Figure 3. The centers of the circles represent the means of the mixture components and
the radii are proportional to the respective mixture weights. We then repeat the clustering procedure
using the well known soft k-means algorithm and present the results in Figure 4.
As it can be seen from the ﬁgures the proposed convex relaxation is able to penalize the cardinality
on the mixture probability vector and produce clusters signiﬁcantly better than soft k-means algo-
rithm. Note that soft k-means is a non-convex procedure whose performance depends heavily on
the initialization. The proposed approach is convex hence insensitive to the initializations. Note that
in [8] the number of clusters are adjusted indirectly by varying the β parameter of the distribution.
In contrast our approach tries to implicitly optimizes the likelihood/cardinality tradeoff by varying
λ. Hence when the number of clusters is unknown, choosing a value of λ is usually easier than
speciﬁcying a value of k for the k-means algorithms.

7 Conclusions and Future Directions

We presented a convex cardinality penalization scheme for problems constrained on the probability
simplex. We then derived a sufﬁcient condition for recovering the sparsest probability measure in
an afﬁne space using the proposed method. The geometric interpretation suggests that it holds for a
large class of matrices. An open theoretical question is to analyze the probability of exact recovery
for a normally distributed A. Another interesting direction is to extend the recovery analysis to the
noisy setting and arbitrary functions such as the log-likelihood in the clustering example. There
might also be other problems where proposed approach could be practically useful such as portfolio
optimization, where a sparse convex combination of assets is sought or sparse multiple kernel learn-

12345678900.10.20.30.40.50.60.70.80.91m − number of measurements (moment constraints)Probability of Exact Recovery in 100 independent trials of A  Rescaling L1 HeuristicProposed relaxation12345678900.20.40.60.811.21.41.61.82m − number of measurements (moment constraints)Averaged error of estimating the true measure : ||x−xt||1  Rescaling L1 HeuristicProposed relaxation(a) λ = 1000

(b) λ = 300

(c) λ = 100

(d) λ = 45

Figure 3: Proposed convex clustering scheme

(a) k = 3

(b) k = 4

(c) k = 8

(d) k = 10

Figure 4: Soft k-means algorithm

ing.
Acknowledgements This work is partially supported by the National Science Foundation under
Grants No. CMMI-0969923, FRG-1160319, and SES-0835531, as well as by a University of Cali-
fornia CITRIS seed grant, and a NASA grant No. NAS2-03144. The authors would like to thank the
Area Editor and the reviewers for their careful review of our submission.

−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5−1.5−1−0.500.511.52−3−2.5−2−1.5−1−0.500.511.5References
[1] E.J. Cand ´es, T. Tao, ”Decoding by linear programming”. IEEE Trans. Inform. Theory 51
(2005), 4203-4215.
[2] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit” SIAM Review,
43(1):129-159, 2001.
[3] A. Bruckstein, D. Donoho, and M. Elad. ”From sparse solutions of systems of equations to
sparse modeling of signals and images”. SIAM Review, 2007.
[4] V. Chandrasekaran, B. Recht, P.A. Parrilo, and A.S. Willsky. ”The convex algebraic geometry
of linear inverse problems”. In Communication, Control, and Computing (Allerton), 2010 48th
Annual Allerton Conference on, pages 699-703, 2010.
[5] S. Boyd and L. Vandenberghe, ”Convex Optimization”. Cambridge, U.K.: Cambridge Univ.
Press, 2003.
[6] A. Cohen and A. Yeredor, ”On the use of sparsity for recovering discrete probability distribu-
tions from their moments”. Statistical Signal Processing Workshop (SSP), 2011 IEEE
[7] J. Kivinen and M. Warmuth. ”Exponentiated gradient versus gradient descent for linear predic-
tors”. Information and Computation, 132(1):1-63, 1997.
[8] D. Lashkari and P. Golland, ”Convex clustering with exemplar-based models”, in NIPS, 2008.


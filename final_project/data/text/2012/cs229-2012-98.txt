Interpolating images between video frames
using non-linear dimensionality reduction

S´ebastien ROBASZKIEWICZ
Sammy EL GHAZZAL

robinio@stanford.edu

selghazz@stanford.edu

Abstract

2. Method

We present a new method for interpolating
images between frames of a video. By apply-
ing Isomap, we map each frame of the video
to a low dimension feature space, which ex-
tracts meaningful features from the sequence
of images. To insert new images between 2
frames, we select images from the original
dataset that are mapped in the same area
as those 2 frames. We assess the perfor-
mance of our algorithm by confronting it to
2 other interpolation algorithms on 2 diﬀer-
ent datasets. We delete every other frame
from the original datasets, and reinterpolate
the deleted images with 3 diﬀerent methods.
By comparing the similarity of the interpo-
lated image and the deleted image with mea-
sures based on SIFT and the Hough Trans-
form, we determine which algorithm leads to
the lowest error. Our results suggests that
the method based on Isomap performs bet-
ter than the two other algorithms when the
video contains repetitive motion.

1. Introduction

Video compression has always been an important chal-
lenge, and it is now more crucial than ever due to
the wide use of streaming services. A way of achiev-
ing compression is to delete images, and interpolate
them back during the decompression process. In other
words, the compression reduces the frame rate of the
video and the decompression is aimed at bringing the
frame rate back to its original value. As suggested
by (Pless, 2003), we investigate whether or not dimen-
sionality reduction can be used to solve this problem.

Proceedings of the 30 th International Conference on Ma-
chine Learning, Atlanta, Georgia, USA, 2013.
JMLR:
W&CP volume 28. Copyright 2013 by the author(s).

2.1. Insuﬃsance of a linear approach

In this pro ject, we intend to take advantage of the
underlying pattern in a video sequence to interpo-
late images between two consecutive frames.
In or-
der to ﬁnd the dimension of the underlying struc-
ture in a sequence of images, we confronted two al-
gorithms: Principal Component Analysis (PCA) and
Isomap (Tenenbaum et al., 2000), a nonlinear dimen-
sionality reduction algorithm. The main diﬀerence in
principle between these two algorithms is that while
PCA uses Euclidean distances, Isomap computes a
good approximation of pairwise geodesic distances
uses them in a spectral method to compute a low-
dimensional embedding.

We found out that PCA needed 59 principal compo-
nents to capture 95 % of the variance in the dataset
(which consisted in a teapot revolving around the z -
axis), whereas Isomap reaches a minimal residual vari-
ance of 4 % for a two dimensional embedding, therefore
suggesting that the data lies in the neighborhood of a
two dimensional manifold (cf. Figure 1).

2.2. Using Isomap

Given a point in Rd , we are not able a priori to ﬁnd
its pre-image(s)1 . There is no guarantee either that
the pre-image will be unique (injectivity of φ) or that
such a pre-image even exists (surjectivity of φ).

Knowing the mapping function φ would allow us to
ﬁnd an approximation of the pre-image by using a re-
gression technique. We would compute x∗ such that:

x∗ = arg min
x∈X

kφ(x) − yk

where y is the point for which we are trying to ﬁnd
a pre-image.
Initializing x to some value, we would

1We deﬁne the pre-image of a point y ∈ Rd in the fea-
ture space to be a point x such that y = φ(x) where φ
denotes the mapping function between the input space and
the low-dimensional space.

Final pro ject CS229

Result of an embedding in a two-dimensional space using PCA

Two-dimensional Isomap embedding (with neighborhood graph).

300

200

100

0

-100

-200

-300
-300

-200

-100

0

100

200

300

15

10

5

0

-5

-10

-15
-15

-10

-5

0

5

10

15

20

(a) Isomap

(b) PCA

Figure 1. Comparison between the two dimensional embeddings of PCA and Isomap on the teapot data set.

apply gradient descent (for instance) to minimize the
ob jective. The point x∗ reached at convergence would
be considered to be a good approximation of a pre-
image of y .

However, in practice, the map φ between the input
space and the feature space is a priori not known.
Consequently, given a new training point x, it is im-
possible to compute either its embedding φ(x) or the
gradient of a function involving φ(x). These remarks
motivate the need for an interpolation method not re-
lying on the mapping function φ, which we detail in
the next section.

2.3. The method

For an illustrated explanation of the method, please
refer to Figure 2.

2.3.1. Mapping the dataset to a

low-dimensional space

We use Isomap to pro ject each image from the origi-
nal video to the low-dimensional space, which is also
called feature space.
In that space, the data points
(cid:0)φ(x(i) )(cid:1)k∈J1;nK are represented by their coordinates
and by their timestamp in the video (which corre-
sponds to the index i).

2.3.2. Interpolating a curve between the

data points in the feature space

We interpolate a smooth curve between the points
in the low-dimensional space. The curve equation is
given by t 7→ α(t) and is constrained to go through
the m data points, i.e. ∃(t1 , . . . , tm ) ∈ Rm s.t. ∀i ∈
J1; mK, α(ti ) = φ(x(i) ), while minimizing the curva-
ture of the curve. This optimization problem can be

solved by a regression (Dam et al., 1998).

2.3.3. Location of the interpolated images in

the feature space

Given 2 images x(i) and x(i+1) , we want to insert n
images (cid:0)p(i,k) (cid:1)k∈J1;nK between them. Let us denote by
(cid:0)d(i,k) (cid:1)k∈J1;nK the n points in the feature space evenly
spaced on the curve linking the two consecutive images
φ(x(i) ) and φ(x(i+1) ). More formally, if α(i) is the trun-
cation of α between φ(x(i) ) and φ(x(i+1) ) such that
α(i) (0) = φ(x(i) ) and α(i) (1) = φ(x(i+1) ), we have, for
any k ∈ J1; nK, α(i) (cid:16) k
n+1 (cid:17) = d(i,k) . We consider the
(d(i,k) )k∈J1;nK to be the embedding of the interpolated
images. As we discussed in section 2.2, it is not pos-
sible to ﬁnd preimage of those points. As a result, we
next detail an alternative method to ﬁnd a reasonable
interpolation in the input space.

2.3.4. Inserting new images

To address the problem of the preimage approxima-
tion, we choose the image corresponding to d(i,k)
to be its nearest neighbor in the low-dimensional
space. Speciﬁcally, for each d(i,k) , we seek for the
data point φ(x(j) ) that minimizes the Euclidean dis-
tance between the two of them, and we then set
p(i,k) = x(j) .
In other words, if we have u(i,k) :=
arg min1≤j≤m (cid:13)(cid:13)d(i,k) − φ(x(j) )(cid:13)(cid:13)2 , then we set p(i,k) :=
x(u(i,k) ) .

2.3.5. Final result

Each pair of consecutive original images (x(i) , x(i+1) )
is now augmented with other images from the dataset
as follows: (x(i) , x(u(i,1) ) , . . . , x(u(i,n) ) , x(i+1) ).

a

b

c

d

Final pro ject CS229

(x(i))

(x(i+4))

(x(i+3))

(x(i+2))

(x(i))

(x(i+4))

(x(i+3))

(x(i+2))

(x(i))

(x(i))

(x(i-1))

(x(i+1))

(x(i-1))

(x(i+1))

d(i, 1) 

d(i, 2) 

d(i, 3) 

d(i, 4) 

(x(i+1))

d(i, 1) 

d(i, 2) 

d(i, 3) 

d(i, 4) 

(x(i+1))

Figure 2. Our interpolation method: (a) We pro ject the images from the video on the low-dimensional space (LDS) with
Isomap. (b) In the LDS, we interpolate a smooth curve between the embedded data points. (c) In the LDS, we estimate
the location of the n images we want to insert (d(i,k) , . . . , d(i,n) ) by placing them evenly on the curve between φ(x(i) ) and
φ(x(i+1) ). (d) We set each of the interpolated images to be equal to the closest point from the original data.

Figure 3. Comparison between two interpolation methods: (a): the method we discussed in section 2.3.4. (b): weighted
average interpolation between the two images. The red circles (two extremal images) are the images between which we
want to insert new images. The blue triangles are the inserted images.

3. Experiment

3.1. Protocol

To test the validity of our method, we used the follow-
ing protocol:

1. From an original video, create a compressed ver-
sion of the video by removing every other image.

2. Interpolate frames between each pair of consecu-
tive images in the compressed video.

3. Compare our interpolated frames with the frames
deleted in step 1.

3.2. Measuring the performance of our method

3.2.1. Two other interpolation methods

In order to measure the performance of our method,
we confronted it with 2 other interpolation methods:

• averaging the pixel values of the adjacent images

• Motion Estimation - Motion Compensation
(MEMC), a common interpolation method used
for instance in the MPEG4/H.263 standard. More
speciﬁcally, we used the toolbox discussed in
(Barjatya, 2004), which consists in two steps:

1. Motion estimation: this task is performed by
a block-matching algorithm (we mainly used
extensive search). The latter produces one
motion vector for each block in the image (a
block being a square of pixels).
2. Motion compensation: using the motion vec-
tors found in the previous step, the algorithm
interpolates an intermediary image by trans-
lating blocks of pixels according to the mo-
tion vector

3.2.2. Assessing the performance of each

interpolation method

To compare the similarity between the image that was
deleted during compression and the interpolated im-
age, we implemented two comparison procedures.

Procedure 1, based on Hough Transform
We ﬁrst derived a comparison method from the Hough
transform (Duda & Hart, 1972). Speciﬁcally, given
a discretization (ρk , θl )(k,l) ∈ ∆ of the Hough space,
we computed the simple Hough transform of both the
original H T (d) and the interpolated images H T (i) . We
then deﬁned the Hough error ǫ to be:
(cid:16)H T (d)(ρk , θl ) − H T (i)(ρk , θl )(cid:17)2

.

ǫ = X
(k,l)∈∆

Final pro ject CS229

Overall, this method consists in comparing the set of
edges of the two images.

Procedure 2, based on Scale-Invariant Feature
Transform
Scale-Invariant Feature Transform (SIFT)
(Lowe,
2004) is usually used to detect the presence of ob-
jects in images. It computes keypoints which are rep-
resented by descriptors and is able to match key points
from an image to those of another image (see Figure 4).
To come with a measure of similarity using this algo-
rithm, we combined two criteria:

• The number of matches between the descriptors
of the two images2 . This measures how similar
the two ob jects are.

• The diﬀerence in position between the matching
descriptors of either image. This allows us to
know if the ob ject is in the right position (loca-
tion and orientation) and gives an idea of how the
interpolated image respects the ﬂow of the video.

Figure 4. Matching keypoints between the interpolated im-
age and the deleted image using SIFT.

3.3. The datasets

Finally, we decided to compare the performance of the
3 interpolation methods on 2 diﬀerent datasets:

• For the ﬁrst one, we built a video by rotating a
3D shell. The frame rate and the revolving speed
are slightly desynchronized so that each rotation
is diﬀerent from the previous one. This video is
an example of a repetitive footage where similar
actions are seen at diﬀerent points in the video.

2Descriptors
128-
a
are usually represented as
dimensional features, which ensures that two matching key-
points are actually referring to the same element with good
probability.

• For the second one, we used a CIPR sequence rep-
resenting a toy train going from right to left, a
board with an upwards movement, and 2 spinning
atoms. This video is therefore much more com-
plex that the previous one. In particular there is
no repetitive movement.

4. Results

4.1. Shell dataset (video with repetitive
motion)

• The Hough error estimation indicates that our
method using Isomap performs signiﬁcantly bet-
ter than the two others on the shell dataset (Fig-
ure 5-(a)).

• The SIFT error estimation was not exploitable
due to the number of outliers and too small diﬀer-
ences in descriptors positions, probably because
some images do not have enough key points.

• For each of the 300 interpolated images, we found
that the minimum error was given by Isomap.

4.2. Caltrain dataset (video without repetitive
motion)

• The Hough error estimation indicates that the av-
erage interpolation performs signiﬁcantly better
than the two others. The method using Isomap
has no signiﬁcant diﬀerence with the MEMC in-
terpolation method.

• The SIFT error estimation indicates that the
method with Isomap has signiﬁcantly more de-
scriptor matches than the two other methods (see
Figure 5-(b)). This shows that the Isomap inter-
polation method is better at preserving the origi-
nal shape of the ob ject. Isomap and MEMC also
perform signiﬁcantly better than the average in-
terpolation in the position change of the matching
descriptors (see ﬁgure 5-(c)). However, there is no
signiﬁcant diﬀerence between Isomap and MEMC
on that aspect.

4.3. Videos

You can see the videos at
the following URL:
http://stanford.edu/~robinio/cs229project/.
On the shell dataset, see how the interpolation with
Isomap yields a video which is very close to the
original video.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Average

Isomap

MEMC

(a) Hough error - Shell dataset

SIFT # descriptor matches

s
e
h
c
t
a
m
 
r
o
t
p
i
r
c
s
e
d
 
#

1300

1200

1100

1000

900

800

700

600

Average

Isomap

MEMC

(b) SIFT number of matches - Caltrain
dataset

SIFT change of position of matching descriptors

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Average

Isomap

MEMC

(c) SIFT change in position of key
points - Caltrain dataset

Figure 5. Comparison of the results of 3 interpolation
methods using the procedures described in Section 3.2.2.
On each box, the central mark is the median, the edges
of the box are the 25th and 75th percentiles, the whiskers
extend to the most extreme data points not considered
outliers, and outliers are plotted individually.

5. Conclusion

Our results suggest that our method is particularly ef-
ﬁcient on footages with repetitive motion (for instance,
chen the camera is rotating around an ob ject, or when
a person is waving his hand at someone, or if the cam-
era is shooting a closeup of a ﬂying bird). It has several
advantages:

• The interpolated images have the same quality as
the rest of the video, which often results in a much

Final pro ject CS229

Hough L2 error

sharper video

• In the worst case scenario, the eﬀective frame rate
of the decoded video is the same as the com-
pressed video (the interpolated frames between
x(i) and x(i+1) will be equal to x(i) or x(i+1) ).

6. Limitations & further work

The main limitation of our method is that we only
use images from the data set. As a consequence, the
method works only for videos that contain a repetitive
motion. An interesting improvement of the method
would be either to ﬁnd a way of computing preim-
ages with Isomap or to use a nonlinear dimensionality
reduction algorithm that is invertible (or pseudo in-
vertible).
Indeed, we would then be able to create
images from scratch, which would probably result in
a systematic increase the eﬀective frame rate of the
video.

Another direction of research would be to ﬁnd a way
to combine our method with another interpolation
method such as multi-reference MEMC. The princi-
pal challenge would then be to ﬁnd a systematic way
of deciding which algorithm generates which images.

References

Barjatya, Aroh. Block matching algorithms for motion
estimation. IEEE, 2004.

Dam, Erik B., Koch, Martin, and Lillholm, Martin.
Quaternions, interpolation and animation. Techni-
cal report, 1998.

Duda, Richard O. and Hart, Peter E. Use of the hough
transformation to detect lines and curves in pictures.
Commun. ACM, 15(1):11–15, January 1972. ISSN
0001-0782. doi: 10.1145/361237.361242.

Lowe, David G. Distinctive image features from scale-
invariant keypoints. Int. J. Comput. Vision, 60(2):
91–110, November 2004. ISSN 0920-5691. doi: 10.
1023/B:VISI.0000029664.99615.94.

Pless, Robert.
Image spaces and video tra jectories:
Using isomap to explore video sequences. In Proceed-
ings of the Ninth IEEE International Conference on
Computer Vision - Volume 2, ICCV ’03, pp. 1433–,
Washington, DC, USA, 2003. IEEE Computer Soci-
ety. ISBN 0-7695-1950-4.

Tenenbaum, Joshua B., de Silva, Vin, and Langford,
John C. A Global Geometric Framework for Nonlin-
ear Dimensionality Reduction. Science, 290(5500):
2319–2323, December 2000.


Fast and Low-power OCR for the Blind

Frank Liu

CS229 - Machine Learning
Stanford University

1. Introduction

Optical character recognition, or OCR, is the pro-
cess of extracting text from scanned documents. Al-
though OCR is a well-studied topic, text embedded in
natural scenes also carries extremely useful informa-
tion. As such, it is often necessary for computers or
mobile devices to automatically recognize them.
Among the various subtopics of scene text classi-
ﬁcation, character classiﬁcation (i.e.
the process of
recognizing various letters and digits) is perhaps the
most important. However, this task is rather difﬁcult,
since recognizable characters often appear in a variety
of different scenes with a variety of different designs.
Additionally, they are difﬁcult to accurately classify
due to the variant properties of scene text.
A common way to learn images of characters is to
apply a scale and viewpoint-invariant feature extrac-
tion algorithm with a classiﬁcation algorithm. For the
English alphabet, this classiﬁcation algorithm would
categorize feature sets from each image into 36 cat-
egories (26 letters and 10 numbers, excluding sym-
bols such as ”!” and ”-”). Many classiﬁcation algo-
rithms are viable, but each produce different results
when paired with a different set of features.
While OCR has many applications, blind men and
women have perhaps the greatest use for letter and
word recognition. The vOICe project [10] is an ex-
cellent example of this. vOICe kits attempt to help the
blind recognize everyday objects, such as a pedestrian
walkway or a parking lot. The package also comes
with an OCR algorithm, which allows users to listen
to, for example, the time displayed on a clock or text
written in a book. However, one of the downsides of

the vOICe project is that its OCR algorithm can take a
couple of seconds to process text, and the accuracy is
somewhat questionable given the low-resolution cam-
era input.
Despite the potential for OCR as seen in the
vOICe project, the vast majority of recent OCR re-
search is focused towards improving classiﬁcation ac-
curacy. Batuwita and Bandara, for example, present a
constrained OCR algorithm for fuzzy, low-resolution
scanned letters and numbers. While their work
presents a high classiﬁcation accuracy ( 98%), it takes
over half a second to classify each character.
[2]
Similarly, Si et al. [7] and Namane et al. [1] both
achieve a high-accuracy (approximately 96%) classiﬁ-
cation method for scanned text, but explicitly mentions
the immense computational complexity of their algo-
rithms. For many recent OCR-related algorithms, run-
time will often double or triple for a just few percent
increase in accuracy. Although there are certain advan-
tages to having extremely accurate algorithms, OCRs
usefulness to the blind only manifests itself when let-
ters and characters can be quickly recognized and out-
put to the user through audio or some other form of
sensory communication.
This paper will detail the design and implement a
novel method for fast, low-power word recognition,
creating an algorithm designed to use small amounts
of computing power while still maintaining a reason-
ably high level of accuracy on everyday recognizable
characters. Section 2 explores several feature extrac-
tion and classiﬁcation algorithm combinations. Some
of these algorithms are tweaked some portions of the
algorithms to obtain better classiﬁcation accuracy or
a lower runtime. Section 2 also presents a qualita-

1

tive formula that determines the quality of a particu-
lar combination of feature extraction + classiﬁcation
algorithms. Finally, Section 3 presents optimized re-
sults compiled using the best selected combination of
feature extraction + classiﬁcation algorithms. Testing
will occur on the OCR dataset collected by Rob Kassel
at the MIT Spoken Language Systems group (found
here: www.seas.upenn.edu/ taskar/ocr/letter.data.gz).

2. Methodology
A power-efﬁcient OCR algorithm attempts to mini-
mize the computational steps used in the OCR process
while still achieving a high classiﬁcation accuracy. For
mobile and low-power applications, an extremely ac-
curate but computationally expensive algorithm is un-
desirable; on the other hand, a fast and power-efﬁcient
algorithm with a relatively low classiﬁcation accuracy
is also inadequate.
To better quantitatively measure this balance, the
OCR method employs a weighted sum over the run-
time and test set classiﬁcation accuracy to determine
which combination is the best. This equation is based
on user preferences generated by the vOICe project:

+ a

(1)

B (r, a) =

r
n
where r is the overall runtime over the test set, a
is the classiﬁcation accuracy, and n is the number of
points in the character dataset. This formula reﬂects
the desire to have an algorithm as close to 100% as
possible, while still maintaining a strong relative run-
time.
To achieve maximum balance between computing
efﬁciency and recognition accuracy, several potential
algorithms learning algorithms were analyzed to see
which produced the best results:

(1) K-nearest neighbor, K = 5

(2) K-nearest neighbor, K = 20
(3) Naive Bayes, n = 0.01 ∗ N
62
(4) Support Vector Machines (SVM)

For kNN, K refers to the number of closest neigh-
bors analyzed. For Naive Bayes, the parameter n
refers to the Laplace smoothing factor, conveniently
deﬁned as 1% of the test set’s element count divided

2

by 62 (the total number of lowercase and uppercase
alphanumeric characters in the English alphabet).
To allow the OCR method to learn the characters,
each learning algorithm was paired with a feature ex-
traction algorithm, designed to take keypoint vectors
at each pixel in the image. These features consisted
of some variable number f of feature vectors, each of
which was 64-bits wide. Feature vectors which had
less than 64 bits were simply zero-padded until the
vector had length of 64. The following image shows a
selected set of SIFT interest keypoints extracted from
an image of Huang Engineering Center:

Figure 1: Selected SIFT features taken from an image of
Huang Engineering Center.

Below is a list of all of the feature extraction algo-
rithms tested for this project:

(1) Harris Corners [5]

(2) Histogram of Gradients

(3) Scale-Invariant Feature Transform (SIFT) [8]

(4) Speeded-up Robust Features (SURF) [4]

To keep runtime calculations consistent, only MAT-
LAB libraries and scripts were used to code the feature
recognition and classiﬁcation algorithms.
As per Equation 1, each combination of algorithms
was analyzed for both runtime and test-set accuracy.
The table below shows a summary of the results. The
maximum output value from Equation 1 was normal-
ized to 100.
In the experiments, the Gaussian kernel was used
to train the SVM classiﬁer. Surprisingly, SIFT per-
formed considerably better than HoG, while SURF

using libraries provided by Rob Hess from Oregon
State University. [6] This provided an almost three-
fold improvement in runtime, with no change to the
classiﬁcation accuracy.

3.2. Image Downsampling and Smoothing
In order to improve the efﬁciency of the algorithm,
the original image is downsampled to a 16 × 16 square
image via bicubic interpolation. Furthermore, to com-
pensate for spurious high frequency information in the
downsampled image, the method employs a Gaussian
lowpass ﬁlter to smooth out any potential sharp cor-
ners. This increases the reliability of the SURF de-
scriptor.
Using LIBSVM [3] to ﬁne tune the classiﬁcation
parameters, two distinct patters were created for fea-
ture extraction and training, both designed to compen-
sate (in one way or another) for the system’s lack of
computing power.
(1) 12 × 12 window size, 128 descriptor elements per
feature. Although the descriptor for each pixel
contains quite a bit of information, the number of
pixels we consider is rather small.
(2) 6 × 6 window size, 72 descriptor elements per
feature. This pattern reduces the number of ele-
ments per descriptor, but increases the number of
pixels in the interest region.

For pattern 1 (see Figure 3), a search range
log2 (C ) = [3, 5, ..., 13, 15] and log2 (γ ) =
of
[−15, −13, ..., −5, −3]. A region was then identiﬁed
around (C, γ ) = (8, 0.03125) which has a high cross-
validation accuracy of 75.2%.
Immediately follow-
ing this, the grid search was rerun using log2 (C ) =
[1.5, 2.5, 3.5] and log2 (γ ) = [−6.5, −5.5, −4.5].
This produced a cross-validation accuracy of 75.4%
and an optimized pair of parameters - (C, γ ) =
(5.6569, 0.0221) Using these parameters, a test set ac-
curacy of 3545/5198 = 68.1993% was achieved.
2
search
(see Figure
pattern
For
a
4),
range
of
and
log2 (C )
=
[1, 3, ..., 11, 13]
[−15, −13, ..., −5, −3] was used
log2 (γ )
=
determining
before
the
ranges
for ﬁne
grid
search
and
be
to
[2.5, 3, 3.5]
=
log2 (C )
log2 (γ ) = [−7.5, −7, −6.5]. The best parameter

Figure 2: Flow chart for the algorithm.

kNN5
61.03
63.28
71.54
65.93

kNN20
65.72
73.80
81.44
69.52

NB
26.40
50.08
86.62
75.69

SVM
48.95
83.62
100
88.16

HC
HoG
SURF
SIFT

Table 1: Normalized values (100 = best) for B(r, a) for
different algorithm combinations.

outperformed SIFT by a relatively small amount de-
spite the reported 4x speedup over SIFT.

3. Optimizations
As shown in Section 2, SURF with SVM has shown
better results than all of other algorithm combinations.
To further improve the OCR method, several tech-
niques were used to improve both the classiﬁcation ac-
curacy and runtime of the algorithm.
3.1. Code Conversion to C
To improve runtime performance and decrease
library and platform-related overhead,
the original
SURF + SVM code was ported from MATLAB to C

3

4. Results
Preliminary ”ﬁeld tests” were generated using an
ﬁeld-programmable gate array (FPGA) running a
MIPS-like processor at a clock frequency of 150MHz.
Due to time constraints, the updated x86-based OCR
method was not ported directly to the MIPS assem-
bly. Instead, Valgrind [9] was used to determine the
number of instances of each assembly instruction that
occurred when running the OCR method over the test
set. These instructions and their corresponding num-
ber of occupancies N in the code were then copied to
the FPGA, which then ran each instruction N times.
Over the entire test set, the FPGA-based system ob-
served a 5.07ms classiﬁcation runtime per character.
This means that the system can correctly classify ap-
proximately 70 out of 100 characters every half second
on a 150MHz clock speed.

5. Further Work
Low-power OCR has huge potential as a blindness
aid, especially as processing power becomes cheaper
and cheaper. Although the OCR system shown here
has shown a good set of preliminary results, there is
still plenty of room for future work.

(1) Comprehensive ﬁeld tests. In this project, no real
ﬁeld tests (with an actual live webcam feed and
user) were performed. Doing so would be a huge
step forward in the project, and could allow for
more optimizations tailored towards actual user-
reported problems.

(2) Addition of text-to-speech capabilities. At the
moment, there is no way for a blind user to assess
the output of the OCR method. Adding text-to-
speech software and recompiling runtime results
would allow for a better understanding of the vi-
ability of the system in a real-world setting.

(3) Improved accuracy for OCR algorithm. While a
low runtime is critical for the application to per-
form well, a reasonably high accuracy just as im-
portant; 70% classiﬁcation accuracy may be a bit
too inaccurate for a user to perform tasks on a
daily basis. To increase classiﬁcations accuracy
or decrease runtime, more learning and feature
extraction algorithms could be analyzed. Image

Figure 3: Grid search for pattern 1. Top graph corresponds
to a coarse search, whereas the bottom graph corresponds
to a ﬁne search.

Figure 4: Grid search for pattern 2. Top graph corresponds
to a coarse search, whereas the bottom graph corresponds
to a ﬁne search.

pair (C, γ ) turned out to be (5.6569, 0.0078), turning
in a test set accuracy of 3607/5198 = 69.3921%.

Given the different possible current OCR algo-
rithms, 70% likely is very close to the maximum pos-
sible accuracy achievable while still maintaining a low
runtime. Both patterns were able to achieve approxi-
mately 0.4ms classiﬁcation runtime per character.

4

modiﬁcation for more robust feature extraction
is also another possibility to improve the OCR
method.

References
[1] E. H. Soubari P. Meyrueis A. Namane,
M. Maamoun.
Csm-based feature extrac-
tion for degraded machine printed character
recognition. In proceedings of ICMLC’08, 2010.
1

[2] K. Batuwita and G. Bandara. Fuzzy recognition
of ofﬂine handwritten numeric characters. In pro-
ceedings of CIS’10, 2010. 1

and
Chang
[3] C.
http://www.csie.ntu.edu.tw/
3

Lin.
C.
cjlin/libsvm/.

Surf:
[4] T. Tuytelaars H. Bay and L. V. Gool.
In proceedings of
Speeded-up robust features.
ECCV’06, 2006. 2

[5] C. Harris and M. Stephens. A combined corner
In proceedings of AVC’88,
and edge detector.
1988. 2

[6] Rob Hess. http://eecs.oregonstate.edu/ hess. 3

[7] X. Tian J. Si, F. Yang. A new algorithm of mixed
chinese-english character segmentation based on
irregularty degree. In proceedings of ICMLC’08,
2010. 1

[8] D. G. Lowe. Distinctive image features from
scale-invariant keypoints. In IJCV’04, 2004. 2

[9] Valgrind. http://valgrind.org/. 4

[10] The
project.
vOICe
http://www.seeingwithsound.com/ocr.htm.
1

5


Learning Static Parameters in Stochastic Processes

Bharath Ramsundar

December 14, 2012

1

Introduction

Consider a Markovian stochastic process XT evolving (perhaps nonlinearly) over time variable T .
We may observe this process only through observations YT . This process is parameterized by vector
of parameters Θ, which is typically unknown and must be learned from the sequence {YT }. Learning
the parameter Θ is critical to gaining understanding of this class of stochastic processes.
Nonlinear stochastic processes arise in a variety of situations. For example, understanding the
time evolution of stock value through a standard stochastic volatility model requires the ability to
track the nonlinear evolution of price XT . Such problems are typically solved by Sequential Monte
Carlo (SMC) Methods, such as particle ﬁltering, but only when parameter vector Θ is provided
to the Monte Carlo Algorithm. Standard algorithms such as particle ﬁltering encounter diﬃculties
when Θ is unknown.
Consequently, a large literature has developed focusing on algorithms to learn Θ in this setting.
For example, researchers from statistics and econometrics have introduced the Particle MCMC
Algorithm [1] which mixes Markov Chain Monte Carlo (MCMC) methods with Sequential Monte
Carlo algorithms in order to learn parameters Θ. Unfortunately, such algorithms tend to be slow,
and often require at least quadratic time complexity [3].
In this pro ject, we introduce a new algorithm that learns parameter Θ while performing inference.
Namely, we modify the Decayed MCMC Filtering algorithm [4] to learn static parameters. We then
perform empirical analysis showing the robustness of this algorithm in handling static parameters
and also prove preliminary correctness results.
Direction and guidance for this research were provided by Professor Stuart Russell of UC Berke-

ley.

2 Nonlinear State Space Model

The following toy nonlinear stochastic problem [2] is routinely used to evaluate algorithms for their
ability to handle nonlinear time evolution.

Xn =

+ 25

Xn−1
2
X 2
n
+ Wn
Yn =
20
X1 ∼ N (0, 5), the Vn are IID drawn from N (0, σ 2
v ), and the Wn are IID drawn from N (0, σ 2
w )
(N (m, σ 2 ) denotes the Gaussian distribution of mean m and variance σ 2 and IID means independent

Xn−1
1 + X 2
n−1

+ 8 cos(1.2n) + Vn

1

and identically distributed). We set parameter vecter Θ = (σv , σw ). Henceforth, we maintain the
convention in ﬁgures that blue lines are observations, green lines are true states, and red lines are
the products of inference.

Figure 1: Nonlinear State Space Model, σw = σv = 3

3 Prior Literature on Static Parameter Learning

In this section we review various algorithms created for Static Parameter Learning Problems.

3.1 Particle Filtering

The Particle Filter does not learn Static Parameters, but eﬀectively draws samples from nonlinear
processes given such parameters. We implemented a simple particle ﬁlter to draw samples from the
nonlinear state space model. Figure 2 shows that the particle ﬁlter can almost infer the true state.

3.2 Particle Markov Chain Monte Carlo

The PMCMC algorithm [1] is an extension of the Markov Chain Monte Carlo (MCMC) framework
to handle nonlinearities. The PMCMC extends the reach of this framework by using a particle
ﬁlter to sample from nonlinear distributions. We implemented PMCMC and used it to calculate
distributions for parameters σv , σw . See Figure 3 for details.

Figure 2: Nonlinear State Space Model Par-
ticle Filtering, 50 particles, 100 time steps

Figure 3: PMCMC Parameter Estimation
Histogram

2

Algorithm 1: DMCMC: Decayed MCMC Filtering
Output: Approximate Filtering Distributions (cid:101)Ds
Input: g(t): Decay Function; S : Total Number of Time Steps;
K : Gibbs Moves Per Time Step;
for s = 1 to S do
for i = 1 to K do
Choose t from g(s);
increment count for Xs in (cid:101)Ds ;
sample Xt from P (Xt | Xt−1 , Xt+1 , Yt );
return (cid:101)D1 , . . . , (cid:101)DT ;

5

6

1

2

3

4

4 SDMCMC: Decayed MCMC with Static Parameters

In this section we deﬁne a variant of the Decayed MCMC Filtering algorithm (introduced in [4])
for the static parameter estimation problem.

4.1 Decayed MCMC Algorithm

We start by giving a brief overview of Decayed MCMC Filtering. Assume as before hidden state
variables X1 , . . . , XT and evidence variables Y1 , . . . , YT . Decayed MCMC creates a markov chain to
target the distribution XT | Y1 , . . . YT through Gibbs updates. To save time, the algorithm spends
progressively less time updating past elements and focuses on recent history. The rate of this decay
is given by function g(t) on window [0, T ] which controls sampling. For example g(T ) = 1
T gives
the standard Gibbs Sampling Methodology. The result achieved in [4] is that for
gα,δ (t) = α(T − t + 1)−(1+δ)
Algorithm 1 converges to the true ﬁltering distribution for XT in time not dependent on T .

4.2 Decayed MCMC for Static Parameters

MCMC techniques have long been used in the statistical literature to estimate static parameters.
We consequently propose Algorithm (2) as a method to dynamically learn static parameters while
ﬁltering.

4.2.1 Empirical Results

Empirical results on the nonlinear state space model shows that Algorithm 2 works well in practice.
However, the eﬀectiveness of Algorithm 2 becomes most clear when we emphasize the fact that
this inference is performed without prior knowledge of Θ = (σv , σw ). For comparison, we perform
inference in the Particle Filter with Θ initialized according to PΘ . From Figures 4 and 5, we see
that the particle ﬁlter noticeably diverges from the true state, while SDMCMC achieves near perfect
accuracy.
In fact, with growing sequence size S , Algorithm 2 does not lose accuracy, while the particle
ﬁlter does. Figure 6 compares the L2 distances of the inferred and true solutions for SDMCMC and

3

1

2

3

4

5

6

7

Algorithm 2: SDMCMC: Decayed MCMC Filtering for Static Parameters
Output: Approximate Filtering Distributions (cid:101)Ds , parameter Θ
Input: g(t): Decay Function; S : Total Number of Time Steps;
K : Gibbs Moves Per Time Step;
Sample Θ from prior PΘ ;
for s = 1 to S do
for i = 1 to K do
Choose t from g(s);
Sample u from U ([0, 1]);
if u < 1
S then
Resample Θ from P (Θ | X1 , . . . , Xs );
else
increment count for Xs in (cid:101)Ds ;
sample Xt from P (Xt | Xt−1 , Xt+1 , Yt , Θ);
return (cid:101)D1 , . . . , (cid:101)DT , Θ;
Sample Xs+1 from P (Xs+1 | Xs , Ys+1 , Θ);

8

9

10

11

12

Figure 4: Decayed MCMC, K = 3

Figure 5: Particle Filter, 50 particles

SMC (Particle Filter) methods for S = 200 timesteps. This ﬁgure indicates that the L2 distance is
divergent for growing T with SMC methods but is convergent for SDMCMC.
Finally, we consider the parameter learning capabilities of SDMCMC. Figures 7 and 8 show the
histograms of σv , σw considered in a run of Algorithm 2. Although the distribution does not center
around the true parameters σv = σw = 3, it is close. The diagrams suggest that there might be
some bias. Further analysis is required to clarify this point.

4.2.2 Preliminary Mathematical Correctness Analysis

We present some preliminary mathematical analysis of Algorithm 2.

Theorem 4.1. For al l s < S in the outer for loop of Algorithm 2, the inner for loop deﬁnes a
Markov process with stationary distribution X1 , . . . , Xs , Θ | Y1 , . . . , Ys .
Proof. It suﬃces to show that the distribution X1 , . . . , Xs , Θ | Y1 , . . . , Ys is invariant under an
action of the inner for loop. To do so, we will consider the two cases of the if condition separately.
Suppose u < 1/S . Note that the marginal distribution (summing out Θ) on X1 , . . . Xs is exactly

4

Figure 6: Log-Log Compari-
Figure 7: σv Histogram, True
Figure 8: σw Histogram, True
son of L2 distance for SDM-
σv = 3
σw = 3
CMC and SMC
the conditional X1 , . . . , Xs | Y1 , . . . , Ys . Algorithm 2 samples Θ ∼ P (Θ | X1 , . . . , Xs ). It follows
that the joint distribution after the sample is

P (Θ | X1 , . . . , Xs )P (X1 , . . . , Xs | Y1 , . . . , Ys ) = P (Θ | X1 , . . . , Xs , Y1 , . . . , Ys )P (X1 , . . . , Xs | Y1 , . . . , Ys )
= P (Θ, X1 , . . . , Xs | Y1 , . . . , Ys )
Now suppose that u ≥ 1/S . Suppose that i is sampled according to decay g(s). Then the
marginal distribution (summing out Xi ) is

Θ, X1 , . . . , ˆXi , . . . , Xs ∼ P (Θ, X1 , . . . , ˆXi , . . . , Xs | Y1 , . . . , Ys )
The hat signiﬁes exclusion. Conditional independence shows that the joint distribution after
the Gibbs sample is then

P (Xi | Θ, Xi−1 , Xi+1 , Yi )P (Θ, X1 , . . . , ˆXi , . . . Xs | Y1 , . . . , Ys )
= P (Xi | Θ, X1 , . . . , ˆXi , . . . , Xs , Y1 , . . . , Ys )P (Θ, X1 , . . . , ˆXi , . . . , Xs | Y1 , . . . , Ys )
= P (Θ, X1 , . . . , Xs | Y1 , . . . , Ys )

References

[1] C. Andrieu, A. Doucet, and R. Holenstein. Particle markov chain monte carlo methods. Journal
of the Royal Statistical Society, 2010.

[2] N.J. Gordon, D.J. Salmond, and A.F.M. Smith. Novel approach to nonlinear/non-gaussian
bayesian state estimation.
In Radar and Signal Processing, IEE Proceedings F, volume 140,
pages 107–113. IET, 1993.

[3] N. Kantas. An overview of sequential monte carlo methods for parameter estimation in general
state-space models. In Proc. IFAC Symposium on System Identiﬁcation (SYSID), 2009.

[4] B. Marthi, H. Pasula, S. Russell, and Y. Peres. Decayed mcmc ﬁltering. In Proceedings of the
Eighteenth conference on Uncertainty in artiﬁcial intel ligence, pages 319–326. Morgan Kauf-
mann Publishers Inc., 2002.

5


Nonconvex Penalization Using Laplace Exponents
and Concave Conjugates

Zhihua Zhang and Bojun Tu
College of Computer Science & Technology
Zhejiang University
Hangzhou, China 310027
{zhzhang, tubojun}@zju.edu.cn

Abstract

In this paper we study sparsity-inducing nonconvex penalty functions using L ´evy
processes. We deﬁne such a penalty as the Laplace exponent of a subordina-
tor. Accordingly, we propose a novel approach for the construction of sparsity-
inducing nonconvex penalties. Particularly, we show that the nonconvex logarith-
mic (LOG) and exponential (EXP) penalty functions are the Laplace exponents
of Gamma and compound Poisson subordinators, respectively. Additionally, we
explore the concave conjugate of nonconvex penalties. We ﬁnd that the LOG and
EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) dis-
tance functions. Furthermore, the relationship between these two penalties is due
to asymmetricity of the KL distance.

1 Introduction

Variable selection plays a fundamental role in statistical modeling for high-dimensional data sets,
especially when the underlying model has a sparse representation. The approach based on penalty
theory has been widely used for variable selection in the literature. A principled approach is to
due the lasso of [17], which uses the ℓ1 -norm penalty. Recently, some nonconvex alternatives,
such as the bridge penalty, the nonconvex exponential penalty (EXP) [3, 8], the logarithmic penalty
(LOG) [19, 13], the smoothly clipped absolute deviation (SCAD) penalty [6] and the minimax con-
cave plus (MCP) penalty [20], have been demonstrated to have attractive properties theoretically and
practically.
There has also been work on nonconvex penalties within a Bayesian framework. Zou and Li [23]
derived their local linear approximation (LLA) algorithm by combining the EM algorithm with an
inverse Laplace transformation. In particular, they showed that the bridge penalty can be obtained
by mixing the Laplace distribution with a stable distribution. However, Zou and Li [23] proved that
both MCP and SCAD can not be cast into this framework. Other authors have shown that the prior
induced from the LOG penalty has an interpretation as a scale mixture of Laplace distributions with
an inverse gamma density [5, 9, 12, 2]. Recently, Zhang et al. [22] extended this class of Laplace
variance mixtures by using a generalized inverse Gaussian density. Additionally, Grifﬁn and Brown
[11] devised a family of normal-exponential-gamma priors.
Our work is motivated by recent developments of Bayesian nonparametric methods in feature se-
lection [10, 18, 4, 15]. Especially, Polson and Scott [15] proposed a nonparametric approach for
normal variance mixtures using L ´evy processes, which embeds ﬁnite dimensional normal variance
mixtures in inﬁnite ones. We develop a Bayesian nonparametric approach for the construction
of sparsity-inducing nonconvex penalties. Particularly, we show that Laplace transformations of
L ´evy processes can be viewed as pseudo-priors and the corresponding Laplace exponents then form

1

sparsity-inducing nonconvex penalties. Moreover, we exemplify that the LOG and EXP penalties
can be respectively regarded as Laplace exponents of Gamma and compound Poisson subordinators.
In addition, we show that both LOG and EXP can be constructed via the Kullback-Leibler distance.
This construction recovers an inherent connection between LOG and EXP. Moreover, it provides us
with an approach for adaptively updating tuning hyperparameters, which is a very important com-
putational issue in nonconvex sparse penalization. Typically, the multi-stage LLA and SparseNet
algorithms with nonconvex penalties [21, 13] implement a two-dimensional grid research, so they
take more computational costs. However, we do not claim that our method will always be optimal
for generalization performance.

2 L ´evy Processes for Nonconvex Penalty Functions
∑
∑
Suppose we are given a set of training data {(xi , yi ) : i = 1, . . . , n}, where the xi ∈ Rp are the
n
input vectors and the yi are the corresponding outputs. Moreover, we assume that
i=1 xi = 0 and
n
i=1 yi = 0. We now consider the following linear regression model:
y = Xb + ",
where y = (y1 , . . . , yn )T is the n×1 output vector, X = [x1 , . . . , xn ]T is the n×p input matrix,
and " is a Gaussian error vector N ("|0, σIn ). We aim to ﬁnd a sparse estimate of regression vector
b = (b1 , . . . , bp )T under the MAP framework.
We particular study the use of Laplace variance mixtures in sparsity modeling. For this purpose, we
deﬁne a hierarchical model:

−1 ),

[bj |ηj , σ ] ind∼ L(bj |0, σ(2ηj )
[ηj ] iid∼ p(ηj ),
p(σ) = \C onstant",
(
)
where the ηj s are known as the local shrinkage parameters and L(b|u, η) denotes a Laplace distri-
bution of the density
L(b|u, η) =
|b − u|
− 1
2η

.

1
4η

exp

The classical regularization framework is based on a penalty function induced from the margin prior
∫ ∞
p(bj |σ). Let
ψ(|b|) = − log p(b|σ),
}
{
p∑
where p(b|σ) =
0 L(b|0, ση
−1 )p(η)dη . Then the penalized regression problem is
ψ(|bj |)
∥y−Xb∥2
F (b) , 1
.
2 + λ
min
2
b
j=1
Using some direct calculations, we can obtain that d (|b|)
d|b| > 0 and d2 (|b|)
d|b|2 < 0. This implies
that ψ(|b|) is nondecreasing and concave in |b|. In other words, ψ(|b|) forms a class of nonconvex
penalty functions for b.
Motivated by use of Bayesian nonparametrics in sparsity modeling, we now explore Laplace scale
mixtures by relating η with a subordinator. We thus have a Bayesian nonparametric formulation for
the construction of joint priors of the bj ’s.

2.1 Subordinators and Laplace Exponents

Before we go into the presentation, we give some notions and lemmas that will be uses later. Let
(0, ∞) with f ≥ 0. We say f is completely monotone if (−1)n f (n) ≥ 0 for all n ∈ N and
f ∈ C
∞
∫ ∞
a Bernstein function if (−1)n f (n) ≤ 0 for all n ∈ N. The following lemma will be useful.
0 min(u, 1)ν (du) < ∞.
Lemma 1 Let ν be the L ´evy measure such that

2

(1) f is a Bernstein function if and only if the mapping s 7→ exp(−tf (s)) is completely mono-
tone for all t ≥ 0.
∫ ∞
[
]
(2) f is a Bernstein function if and only if it has the representation
1 − exp(−su)
ν (du) for all s > 0,
0

f (s) = α + β s +
where α, β ≥ 0.
Our work is based on the notion of subordinators. Roughly speaking, a subordinator is an one-
dimensional L ´evy process that is non-decreasing (a.s.) [16]. An important property for subordinators
is given in the following lemma.
∫ ∞
Lemma 2 If T = (T (t) : t ≥ 0) is a subordinator, then the Laplace transformation of its density
)
(
takes the form
−sT (t)p(T (t))dT (t) = e
−t (s) ,
∫ ∞
E
]
[
e
0
1 − e
−su
ν (du)
ψ(s) = β s +
0
Here β ≥ 0 and ν is the L ´evy measure deﬁned in Lemma 1.
Conversely, if ψ is an arbitrary mapping from (0, ∞) → (0, ∞) of the form (2), then e
−t (s) is the
Laplace transformation of the density of a subordinator.

for s > 0.

−sT (t)
e

=

(1)

(2)

where

Lemmas 1 and 2 can be found in [1, 16]. The function ψ in (2) is usually called the Laplace
exponent of the subordinator and it satisﬁes ψ(0) = 0. Lemma 1 implies that the Laplace exponent
ψ is a Bernstein function and the corresponding Laplace transformation exp(−tψ(s)) is completely
monotone.
Recall that the Laplace exponent ψ(s) is nonnegative, nondecreasing and concave on (0, ∞). Thus,
if we let s = |b|, then ψ(|b|) deﬁnes a nonconvex penalty function of b on (−∞, ∞). Moreover,
such ψ(|b|) is nondifferentiable at the origin because ψ
′
′
−
) < 0. Thus, it is able
(0+ ) > 0 and ψ
(0
to induce sparsity. In this regard, exp(−tψ(|b|)) forms a pseudo-prior for b1 . Lemma 2 shows that
the prior can be deﬁned by a Laplace transformation. In summary, we have the following theorem.
Theorem 1 Let ψ(s) be a nonzero Bernstein function of s on (0, ∞). If ψ(s) = 0, then ψ(|b|) is a
∫ ∞
nondifferentiable and nonconvex function of b on (−∞, ∞). Furthermore,
exp(−tψ(|b|)) =
exp(−|b|T (t))p(T (t))dT (t), t ≥ 0,
0
where (T (t) : t ≥ 0) is some subordinator.

The subordinator T (t) plays the same role as the local shrinkage parameter η , which is also called a
latent variable. Moreover, we will see that t plays the role of a tuning hyperparameter. Theorem 1
∫ ∞
shows an explicit relationship between the local shrinkage parameter and the corresponding tuning
hyperparameter; i.e., the former is a stochastic process of the later. It is also worth noting that
∫ ∞
exp(−tψ(|b|)) = 2
L(b|0, (2T (t))
−1 )T (t)
−1 p(T (t))dT (t).
0
∫ ∞
−1 p(T (t))dT (t) = 1/C < ∞, p
−1 p(T (t)) deﬁnes a new proper
∗
(T (t)) , C T (t)
Thus, if
0 T (t)
density for T (t). In this case, the proper prior C exp(−tψ(|b|)) is a Laplace scale mixture, i.e., the
mixture of L(b|0, (2T (t))
−1p(T (t))dT (t) = ∞, then p
−1 ) with p
∗
∗
(T (t)) ,
(T (t)). If
0 T (t)
−1 p(T (t)) deﬁnes an improper density for T (t). Thus, the improper prior exp(−tψ(|b|)) is a
∫ 1
T (t)
mixture of L(b|0, (2T (t))
−1 ) with p
∗
(T (t)).
0 exp((cid:0)t (s))ds is inﬁnite, exp((cid:0)t (jbj)) is an improper density w.r.t. Lebesgue measure. Other-
1 If
wise, it can forms a proper density. In any case, we use the terminology of pseudo-priors for exp((cid:0)t (jbj)).

3

2.2 The MAP Estimation

Based on the subordinator given in the previous subsection, we rewrite the hierarchical representa-
tion for joint prior of the bj under the regression framework. That is,
ind∼ L(bj |0, σ(2ηj )
[bj |ηj , σ ]
−1 ),
(ηj ) ∝ ση
−1
∗
(
)
p
j p(ηj ),

which is equivalent to

ind∝ exp
[bj , ηj |σ ]
|bj |
− ηj
p(ηj ).
∫ ∞
σ
(
)
p∏
p∏
Here T (tj ) = ηj . The joint marginal pseudo-prior of the bj ’s is
|bj |
(b|σ) =
− ηj
σ
0
j=1
j=1

∗
p

(

P (ηj )dηj =
exp
exp
{
}
p∑
Thus, the MAP estimate of b is based on the following optimization problem
tj ψ(|bj |/σ)
∥y − Xb∥2
2 + σ
j=1
Clearly, the tj ’s are tuning hyperparameters and the ηj ’s are latent variables. Moreover, it is inter-
esting that ηj (T (tj )) is deﬁned as a subordinator w.r.t. tj .

min
b

1
2

.

.

))

( |bj |
σ

− tj ψ

3 Gamma and Compound Poisson Subordinators

In [15], the authors discussed the use of α-stable subordinators and inverted-beta subordinators. In
this section we study applications of Gamma and Compound Poisson subordinators in constructing
nonconvex penalty functions. We establish an interesting connection of these two subordinators with
nonconvex logarithmic (LOG) and exponential (EXP) penalties. Particularly, these two penalties are
the Laplace exponents of the two subordinators, respectively.

3.1 The LOG penalty and Gamma Subordinator
(
)
The log-penalty function is deﬁned by
ψ(|b|) =
α|b|+1
1
(3)
log
, α, γ > 0.
γ
Clearly, ψ(|b|) is a Bernstein function of |b| on (0, ∞). Thus, it is the Laplace exponent of a subor-
dinator. In particular, we have the following theorem.
∫ ∞
)
(
[
]
Theorem 2 Let ψ(s) be deﬁned by (3) with s = |b|. Then,
1 − exp(−su)
1
αs+1
ν (du),
γ
0
where the L ´evy measure ν is

log

=

1
γu

ν (du) =

Furthermore,

exp(−u/α)du.
∫ ∞
exp(−sT (t))p(T (t))dT (t),
exp(−tψ(s)) = (αs+1)
−t=(cid:13) =
0
where {T (t) : t ≥ 0} is a Gamma subordination and each T (t) has density
− t
−1 exp(−α
−1 η).
(cid:13)

p(T (t) = η) =

η

t
(cid:13)

α
(cid:0)(t/γ )

4

As we see, T (t) follows Gamma distribution Ga(T (t)|t/γ , α). Thus, the {T (t) : t ≥ 0} is called
∫ ∞
the Gamma subordinator.
(
)−t=(cid:13) ∝
We also note that the corresponding pseudo-prior is
L(b|0, T (t)
α|b|+1
exp(−tψ(|b|)) =
−1 )T (t)
0
Furthermore, if t > γ , we can form the pseudo-prior as a proper distribution, which is the mixture
−1 ) with Gamma distribution Ga(T (t)|γ
of L(b|0, T (t)
−1 t−1, α).

−1 p(T (t))dT (t).

3.2 The EXP Penalty and Compound Poisson Subordinator
We call {K (t), t ≥ 0} a Poisson process of intensity λ > 0 if K takes values in N ∪ {0} and each
K (t) ∼ Po(K (t)|λt), namely,

−(cid:21)t , for k = 0, 1, 2, . . .
(λt)k
e
P (K (t) = k) =
k !
Let {Z (k) : k ∈ N} be a sequence of i.i.d. random real variables from common law µZ and let K be
a Poisson process of intensity λ that is independent of all the Z (k). Then T (t) , Z (K (1)) + · · · +
Z (K (t)) for t ≥ 0 follows a compound Poisson distribution (denoted T (t) ∼ Po(T (t)|λt, µZ )). We
then call {T (t) : t ≥ 0} the compound Poisson process. It is well known that Poisson processes are
subordinators. A compound Poisson process is a subordinator if and only if the Z (k) are nonnegative
random variables [16].
In this section we employ the compound Poisson process to explore the EXP penalty, which is
ψ(|b|) =
(1 − exp(−α|b|)), α, γ > 0.
1
γ
It is easily seen that ψ(|b|) is a Bernstein function of |b| on (0, ∞). Moreover, we have
∫ ∞
Theorem 3 Let ψ(s) be deﬁned by (4) where |b| = s. Then
[1 − exp(−su)]ν (du)
ψ(s) =
∫ ∞
0
−1 δ(cid:11) (u)du. Furthermore,
with the L ´evy measure ν (du) = γ
exp(−sT (t))P (T (t))dT (t),
exp(−tψ(s)) =
0
where {T (t) : t ≥ 0} is a compound Poisson subordinator, each T (t) ∼ Po(T (t)|t/γ , δ(cid:11) (·)), and
∫
δu (·) is the Dirac Delta measure.
−1 (1 − exp(−α|b|)) is an improper prior of b.
R (1− exp(−α|b|))db = ∞, so γ
Note that
As we see, there are two parameters α and γ in both LOG and EXP penalties. Usually, for the LOG
penalty ones set γ = log(1 + α), because the corresponding ψ(|b|) goes from ∥b∥1 to ∥b∥0 , as α
varying from 0 to ∞. In the same reason, ones set γ = 1− exp(−α) for the EXP penalty. Thus, α
(or γ ) measures the sparseness. It makes sense to set α as α = p (i.e., the dimension of the input
vector) in the following experiments. Interestingly, the following theorem shows a limiting property
of the subordinators.

(4)

Theorem 4 Assume that α > 0 and γ > 0.
(1) If γ = log(1 + α), then lim(cid:11)→0 Ga(T (t)|t/γ , α) d→ δt (T (t)).
(2) If γ = 1 − e
−(cid:11) , then lim(cid:11)→0 Po(T (t)|t/γ , δ(cid:11) (·)) d→ δt (T (t)).
In this section we have an interesting connection between the LOG and EXP penalties based on
the relationship between the Gamma and compound Poisson subordinators. Subordinators help

5

us establish a direct connection between the tuning hyperparameters tj and the latent variables ηj
(T (tj )). However, when we implement the MAP estimation, it is challenging how to select these
tuning hyperparameters. Recently, Palmer et al. [14] considered the application of concave conju-
gates in developing variational EM algorithms for non-Gaussian latent variable models. In the next
section we rederive the nonconvex LOG and EXP penalties via concave conjugate. This derivation
is able to deal with the challenge.

4 A View of Concave Conjugate

log

= min
w≥0

aj
α

}

1
α

KL(w, a)

[1 − exp(−α|bj |)] = min
w≥0

Our derivation for the LOG and EXP penalties is based on the Kullback-Leibler (KL) distance.
p∑
Given two nonnegative vectors a = (a1 , . . . , ap )T and s = (s1 , . . . , sp )T , the KL distance between
them is
−aj +sj ,
aj
KL(a, s) =
aj log
sj
j=1
0 = 0. It is well known that KL(a, s) ≥ 0 and KL(a, s) = 0 if and only if a = s, but
where 0 log 0
typically KL(a, s) ̸= KL(s, a).
}
{
p∑
p∑
(
)
Theorem 5 Let a = (a1 , . . . , ap )T be a nonnegative vector and |b| = (|b1 |, . . . , |bp |)T . Then,
aj ψ(|bj |) ,
α|bj |+1
wT |b| +
1
aj
KL(a, w)
α
α
j=1
j=1
{
p∑
p∑
when wj = aj /(1 + α|bj |), and
aj ψ(|bj |) ,
wT |b| +
j=1
j=1
when wj = aj exp(−α|bj |).
When setting aj = (cid:11)
(cid:13) tj , we readily see the LOG and EXP penalties. Thus, Theorem 5 illustrates
a very interesting connection between the LOG and EXP penalties. Since KL(a, w) is strictly
convex in either w or a, the LOG and EXP penalties are respectively the concave conjugates of
−α
−1KL(a, w) and −α
−1KL(w, a).
The construction method for the nonconvex penalties provides us with a new approach for solv-
{
}
ing the corresponding penalized regression model. In particular, to solve the nonconvex penalized
p∑
regression problem:
J (b, a) , 1
,
2
}}
{ 1
j=1
∥y − Xb∥2
2 + wT |b| +
1
min
D(w, a)
.
min
w≥0
2
α
b
Here D(w, a) is either KL(a, w) or KL(w, a). Moreover, we are also interested in adaptive esti-
mation of a in solving the problem (6). Accordingly, we develop a new training algorithm, which
consists of two steps.
We are given initial values w(0) , e.g., w(0) = (1, . . . , 1)T . After the k th estimates (b(k) , a(k) ) of
(b, a) are obtained, the (k+1)th iteration of the algorithm is deﬁned as follows.
}
{ p∑
D(w, a(k) )
w(k) = argmin
.
w>0
j=1
j /(1 + α|b(k)
|) in LOG, while w(k)
Particular, w(k)
j = a(k)
j = a(k)
j
j

min
b
{
we equivalently formulate it as

The ﬁrst step calculates w(k) via

exp(−α|b(k)
j

|) in EXP.

wj |b(k)
j

| +

1
α

∥y − Xb∥2
2 +

aj ψ(|bj |)

(5)

(6)

6

}
.

(b(k+1) , a(k+1) ) = argmin
b; a

{
The second step then calculates (b(k+1) , a(k+1) ) via
∥y − Xb∥2
2 + |b|T w(k) +
1
D(w(k) , a)
2
Note that given w(k) , b and a are independent. Thus, this step can be partitioned into two parts.
}
{
p∑
Namely, a(k+1) = w(k) and
∥y − Xb∥2
1
w(k)
b(k+1) = argmin
.
2 +
j
2
b
j=1
Recall that the LOG and EXP penalties are differentiable and strictly concave in |b| on [0, ∞). Thus,
the above algorithm enjoys the same convergence property of the LLA was studied by Zou and Li
[23] (see Theorem 1 and Proposition 1 therein).

|bj |

1
α

5 Experimental Analysis

We conduct experimental analysis of our algorithms with LOG and EXP given in the previous sec-
tion. We also implement the Lasso, adaptive Lasso (adLasso) and MCP-based methods. All these
methods are solved by the coordinate descent algorithm. For LOD and EXP algorithms, we ﬁx
α = p (the dimension of the input vector), and set w(0) = ω1 where ω is selected by using cross-
validation and 1 is the vector of ones. For Lasso, AdLasso and MCP, we use cross-validation to
select the tunning parameters (λ in Lasso, λ and γ in AdLasso and MCP).
In this simulation example, we use a data model as follow

y = xT b + σϵ
where ϵ ∼ N (0, 1), and b is a 200-dimension vector with only 10 non-zeros such that bi = b100+i =
0.2i, i = 1, . . . , 5. Each data point x is sampled from a multivariate normal distribution with zero
mean and covariance matrix (cid:6) = {0.7
|i−j | }1≤i;j≤200 . We choose σ such that the Signal-to-Noise
√
Ratio (SNR), which is deﬁned as

,

^b)2

SPE =

bT (cid:6)b
SNR =
σ
is a speciﬁed value. Our experiment is performed on n = 100 and two different SNR values. We
∑
generate N = 1000 test data for each test. Let ^b denote the solution given by each algorithm. The
Standardized Prediction Error (SPE) is deﬁned as
i=1 (yi − xT
N
i
N σ2
and the Feature Selection Error (FSE) is proportion of coefﬁcients in ^b which is correctly set to zero
or non-zero based on true b.
Figure 1 reports the average results over 20 repeats. From the ﬁgure, we see that both the LOG and
EXP outperform the other methods in prediction accuracy and sparseness in most cases. Our meth-
ods usually takes about 10 iterations to get convergence. Thus, our methods are computationally
more efﬁcient than the AdLasso and MCP.
In the second experiment, we apply our methods to regression problems on four datasets from UCI
Machine Learning Repository and the cookie (Near-Infrared (NIR) Spectroscopy of Biscuit Doughs)
dataset [7]. For the four UCI datasets, we randomly select 70% of the data for training and the rest
for test, and repeat this process for 20 times. We report the mean and standard deviation of the Root
Mean Square Error (RMSE) and the model sparsity (proportion of zero coefﬁcients in the model)
in Tables 1 and 2. For the NIR dataset, we follow the steup for the original dataset: 40 instances
for training and 32 instances for test. We form four different datasets for the four responses (“fat”,
“sucrose”, “dry ﬂour” and “water”) in the experiment, and report the RMSE on the test set and
the model sparsity in Table 3. We can see that all the methods are competitive in both prediction
accuracy. But the nonconvex LOG, EXP and MCP have strong ability in feature selection.

7

SPE

SNR = 3.0

“FSE”

SPE

“FSE”

SNR = 10.0

Figure 1: Box-and-whisker plots of SPE and FSE results. Here (a), (b), (c), (d), (e) are for LOG,
EXP, Lasso, AdLasso, and MCP, respectively

Table 1: Root Mean Square Error on Real datasets
Pyrim
Housing
Abalone
0.138(±0.032)
4.880(±0.405)
2.207(±0.077)
2.208(±0.077)
4.883(±0.405)
0.130(±0.033)
0.118(±0.035)
4.886(±0.414)
2.208(±0.078)
2.208(±0.078)
4.887(±0.413)
0.127(±0.028)
2.209(±0.078)
4.889(±0.412)
0.122(±0.036)

Triazines
0.156(±0.018)
0.153(±0.020)
0.146(±0.017)
0.146(±0.017)
0.148(±0.017)

LOG
EXP

Lasso
AdLasso
MCP

Table 2: Sparsity on Real datasets
Pyrim
Housing
11.54(±5.70)
57.22(±35.32)
88.15(±5.69)
8.08(±5.15)
3.08(±5.10)
36.48(±24.52)
8.07(±7.08)
34.62(±28.81)
41.48(±23.88)
11.54(±6.66)

Abalone
12.50(±0.00)
10.63(±4.46)
1.88(±4.46)
8.75(±5.73)
12.50(±0.00)

Triazines
68.17(±31.19)
76.25(±21.84)
62.08(±14.65)
63.58(±15.18)
73.00(±18.77)

LOG
EXP

Lasso
AdLasso
MCP

Table 3: Root Mean Square Error and Sparsity on Real datasets NIR
NIR(fat)
NIR(sucrose)
NIR(dry ﬂour)
NIR(water)
RMSE Sparsity RMSE Sparsity RMSE Sparsity RMSE Sparsity

LOG
EXP

Lasso
AdLasso
MCP

0.334
0.307

0.437
0.835
0.943

99.14
97.29

68.86
88.14
94.14

1.45
1.47

2.54
2.22
2.07

98.71
97.71

53.43
86.14
95.43

0.992
0.908

0.785
0.862
0.839

99.71
98.86

92.29
99.14
99.71

0.400
0.484

0.378
0.407
0.504

98.14
94.14

65.57
85.86
96.29

6 Conclusion

In this paper we have introduced subordinators of L ´evy processes into the deﬁnition of nonconvex
penalties. This leads us to a Bayesian nonparametric approach for constructing sparsity-inducing
penalties. In particular, we have illustrated the construction of the LOG and EXP penalties. Along
this line, it would be interesting to investigate other penalty functions via subordinators and compare
the performance of these penalties. We will conduct a comprehensive study in the future work.

Acknowledgments

This work has been supported in part by the Natural Science Foundations of China (No. 61070239).

8

References
[1] D. Applebaum. L ´evy Processes and Stochastic Calculus. Cambridge University Press, Cambridge, UK,
2004.
[2] A. Armagan, D. Dunson, and J. Lee. Generalized double Pareto shrinkage. Technical report, Duke
University Department of Statistical Science, February 2011.
[3] P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector
machines. In The 26th International Conference on Machine Learning, pages 82–90. Morgan Kaufmann
Publishers, San Francisco, California, 1998.
[4] F. Caron and A. Doucet. Sparse bayesian nonparametric regression. In Proceedings of the 25th interna-
tional conference on Machine learning, page 88, 2008.
[5] V. Cevher. Learning with compressible priors. In Advances in Neural Information Processing Systems
22, pages 261–269, 2009.
[6] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its Oracle properties. Journal
of the American Statistical Association, 96:1348–1361, 2001.
[7] Osborne B. G., Fearn T., Miller A. R., and Douglas S. Application of near-infrared reﬂectance spec-
troscopy to compositional analysis of biscuits and biscuit dough. Journal of the Science of Food and
Agriculture, 35(1):99–105, 1984.
[8] C. Gao, N. Wang, Q. Yu, and Z. Zhang. A feasible nonconvex relaxation approach to feature selection. In
Proceedings of the Twenty-Fifth National Conference on Artiﬁcial Intelligence (AAAI’11), 2011.
[9] P. J. Garrigues and B. A. Olshausen. Group sparse coding with a Laplacian scale mixture prior.
Advances in Neural Information Processing Systems 22, 2010.
[10] Z. Ghahramani, T. Grifﬁths, and P. Sollich. Bayesian nonparametric latent feature models.
meeting on Bayesian Statistics, 2006.
[11] J. E. Grifﬁn and P. J. Brown. Bayesian adaptive Lassos with non-convex penalization. Technical report,
University of Kent, 2010.
[12] A. Lee, F. Caron, A. Doucet, and C. Holmes. A hierarchical Bayesian framework for constructing sparsity-
inducing priors. Technical report, University of Oxford, UK, 2010.
[13] R. Mazumder, J. Friedman, and T. Hastie. SparseNet: Coordinate descent with nonconvex penalties.
Journal of the American Statistical Association, 106(495):1125–1138, 2011.
[14] J. A. Palmer, D. P. Wipf, K. Kreutz-Delgado, and B. D. Rao. Variational EM algorithms for non-Gaussian
latent variable models. In Advances in Neural Information Processing Systems 18, 2006.
[15] N. G. Polson and J. G. Scott. Local shrinkage rules, l ´evy processes, and regularized regression. Journal
of the Royal Statistical Society (Series B), 74(2):287–311, 2012.
[16] S.-I. P. Sato. L ´evy Processes and inﬁnitely Divisible Distributions. Cambridge University Press, Cam-
bridge, UK, 1999.
[17] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
Series B, 58:267–288, 1996.
[18] M. K. Titsias. The inﬁnite gamma-poisson feature models. In Advances in Neural Information Processing
Systems 20, 2007.
[19] J. Weston, A. Elisseeff, B. Sch ¨olkopf, and M. Tipping. Use of the zero-norm with linear models and
kernel methods. Journal of Machine Learning Research, 3:1439–1461, 2003.
[20] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics,
38:894–942, 2010.
[21] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine Learn-
ing Research, 11:1081–1107, 2010.
[22] Z. Zhang, S. Wang, D. Liu, and M. I. Jordan. EP-GIG priors and applications in Bayesian sparse learning.
Journal of Machine Learning Research, 13:2031–2061, 2012.
[23] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. The Annals of
Statistics, 36(4):1509–1533, 2008.

In World

In

9


Confusion-Based Online Learning and a
Passive-Aggressive Scheme

Liva Ralaivola
QARMA, Laboratoire d’Informatique Fondamentale de Marseille
Aix-Marseille University, France
liva.ralaivola@lif.univ-mrs.fr

Abstract

This paper provides the ﬁrst —to the best of our knowledge— analysis of online
learning algorithms for multiclass problems when the confusion matrix is taken
as a performance measure. The work builds upon recent and elegant results on
noncommutative concentration inequalities, i.e. concentration inequalities that
apply to matrices, and, more precisely, to matrix martingales. We do establish
generalization bounds for online learning algorithms and show how the theoretical
study motivates the proposition of a new confusion-friendly learning procedure.
This learning algorithm, called COPA (for COnfusion Passive-Aggressive) is a
passive-aggressive learning algorithm; it is shown that the update equations for
COPA can be computed analytically and, henceforth, there is no need to recourse
to any optimization package to implement it.

1

Introduction

This paper aims at promoting an infrequent way to tackle multiclass prediction problems: we ad-
vocate for the use of the confusion matrix —the matrix which reports the probability of predicting
class q for an instance of class p for all potential label pair (p, q)— as the objective ‘function’ to
be optimized. This way, we step aside the more widespread viewpoint of relying on the misclassi-
ﬁcation rate —the probability of misclassifying a point— as a performance measure for multiclass
predictors. There are obvious reasons for taking this perspective, among which we may name the fol-
lowing. First, the confusion information is a ﬁner-grain information than the misclassiﬁcation rate,
as it allows one to precisely identify the types of error made by a classiﬁer. Second, the confusion
matrix is independent of the class distributions, since it reports conditional probability distributions:
a consequence is that a predictor learned to achieve a ‘small’ confusion matrix will probably be in-
sensitive to class imbalance and it will also be robust to changes in class prior distributions between
train and test data. Finally, there are many application domains such as medicine, bioinformatics,
information retrieval, where the confusion matrix (or an estimate thereof) is precisely the object of
interest for an expert who wants to assess the relevance of an automatic prediction procedure.

Contributions. We essentially provide two contributions. On the one hand, we provide a statistical
analysis for the generalization ability of online learning algorithms producing predictors that aim at
optimizing the confusion. This requires us to introduce relevant statistical quantities that are taken
advantage of via a concentration inequality for matrix martingales proposed by [8]. Motivated by our
statistical analysis, we propose an online learning algorithm from the family of passive aggressive
learning algorithms [2]: this algorithm is inherently designed to optimize the confusion matrix and
numerical simulations are provided that show it reaches its goal.

Outline of the paper. Section 2 formalizes our pursued objective of targetting a small confu-
sion error. Section 3 provides our results regarding the ability of online confusion-aware learning

1

procedures to achieve a small confusion together with the update equations for COPA, a new passive-
aggressive learning procedure designed to control the confusion risk. Section 4 reports numerical
results that should be viewed as a proof of concept for the effectiveness of our algorithm.

2 Problem and Motivation

2.1 Notation
We focus on the problem of multiclass classiﬁcation: the input space is denoted by X and the target
space is Y = {1, . . . , Q}. The training sequence Z = {Zt = (Xt , Yt )}T
t=1 is made of T identically
and independently random pairs Zt = (Xt , Yt ) distributed according to some unknown distribution
D over X × Y . The sequence of input data will be referred to as X = {Xt}T
t=1 and the sequence
of corresponding labels Y = {Yt}T
t=1 . We may write that Z is distributed according to DT =
t=1D . Z 1:t denotes the subsequence Z 1:t = {(Xτ , Yτ )}t
⊗T
τ =1 . We use DX |y for the conditional
distribution on X given that Y = y ; therefore, for a given sequence y = (y1 , . . . , yT ) ∈ Y T ,
DX |y = ⊗T
t=1DX |yt is the distribution of the random sample X = {X1 , . . . , XT } over X T such
that Xt is distributed according to DX |yt . E[·] and EX |y respectively denote the expectation with
respect to D and DX |y .
For a sequence y of labels, T (y) = [T1 (y) · · · TQ (y)] ∈ NQ is such that Tq (y) is the number of
times label q appears in y . Often, we will drop the dependence upon y for T (y). Throughout, we
make the simplifying assumption that Tq > 1 for all q —otherwise, our analysis still holds but extra
care and notation must be taken for handling classes absent from Z .
The space of hypotheses we consider is H (e.g. H ⊆ {f : f : X → R}Q ), and A designates an
online learning algorithm that produces hypothesis ht ∈ H when it encounters a new example Zt .
Finally, (cid:96) = ((cid:96)q |p )1≤p,q≤Q is a family of class-dependent loss functionals (cid:96)q |p : H × X → R+ . For
a point x ∈ X of class y ∈ Y , (cid:96)q |y (h, x) is the cost of h’s favoring class q over y for x.
Example 1 (Misclassiﬁcation Loss). The family of (cost-sensitive) misclassiﬁcation losses (cid:96)misclass
is deﬁned as
.
(cid:96)misclass
(1)
= χ[h(x)=q ]Cyq ,
(h, x)
q |y
where Cpq ∈ R+ , ∀p, q ∈ Y and χ[E ] = 1 if E is true and 0 otherwise.
{w1 , . . . , wQ} ∈ X Q with (cid:80) wq = 0 and hypothesis hW such that hW (x) = [(cid:104)w1 , x(cid:105) · · · (cid:104)wQ , x(cid:105)]
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq , x(cid:105) +
(cid:12)(cid:12)(cid:12)(cid:12)+
Example 2 (Hinge Loss). The family of multiclass hinge losses (cid:96)hinge is such that, given W =
, where | · |+ = max(0, ·).
1
.
(cid:96)hinge
(2)
Q − 1
q |y (hW , x)
=
2.2 From the Confusion Matrix to the Confusion Risk, and their Minimization

Confusion matrix.
In many situations, e.g. class-imbalanced datasets, it is important not to mea-
sure the quality of a predictor h based upon its classiﬁcation rate
= PX Y (h(X ) (cid:54)= Y )
.
(3)
R(h)
only; this may lead to erroneous conclusions regarding the quality of h. Indeed, if, for instance,
some class q is predominantly present in the data at hand, say P(Y = q) = 1 − ε, for some small
ε > 0, then the predictor hmaj that always outputs hmaj (x) = q regardless of x has a classiﬁcation
error lower that is at most ε, whereas it never correctly predicts the class of data from classes p (cid:54)= q .
A more informative object is the confusion matrix C (h) ∈ RQ×Q of h, which is deﬁned as:
C (h)
= (P(h(X ) = q)|Y = p)1≤p,q≤Q .
.
(4)
The nondiagonal entries of C (h) provides the information as to the types of confusion, and their
prevalence, h makes when predicting the class of some x. Let us now abuse the notation and denote
C (h) the confusion matrix where the diagonal entries are zeroed. It is straightforward to see, that, if
π = [P(Y = 1) · · · P(Y = Q)] is the vector of class prior distributions then, the misclassiﬁcation
rate R(h) (cf. (3)) can be retrieved as
R(h) = (cid:107)πC (h)(cid:107)1 ,

2

where (cid:107) · (cid:107)1 denotes the 1-norm. This says that, with little additional information, the misclassiﬁca-
tion rate might be obtained from the confusion matrix, while the converse is not true.
Is is clear that having the confusion matrix C (h) be zero means that h is a perfect predictor. When
such situation is not possible (if the data are corrupted by noise, for instance), a valuable objective
might be to look for a classiﬁer h having a confusion matrix as close to zero as possible. Here, we
choose to measure the closeness to zero of matrices through the operator norm (cid:107) · (cid:107), deﬁned as:
(cid:107)M v(cid:107)2
(cid:107)M (cid:107) .
(5)
(cid:107)v(cid:107)2
= max
,
v (cid:54)=0
where (cid:107) · (cid:107)2 is the standard Euclidean norm —(cid:107)M (cid:107) is merely the largest singular value of M . In
addition to being a valid norm, the operator norm has the following nice property, that will be of
help to us. Given two matrices A = (aij ) and B = (bij ) of the same dimensions
0 ≤ aij ≤ bij , ∀i, j ⇒ (cid:107)A(cid:107) ≤ (cid:107)B (cid:107).
R(h) ≤ (cid:112)Q(cid:107)C (h)(cid:107),
Given the equivalence between norms and the deﬁnition of the operator norm, it is easy to see that
and targetting a small confusion matrix for h may have the consequence of implying a small mis-
classiﬁcation risk R(h).
The discussion conducted so far brings us to a natural goal of multiclass learning, that of minimizing
the norm of the confusion matrix, i.e. that of solving the following optimization problem
h∈H (cid:107)C (h)(cid:107).
min
However, addressing this question directly poses a number of problems, both from the statistical and
algorithmic sides: a) it is not possible to compute C (h) as it depends on the unknown distribution D
and b) relying on empirical estimates of C (h) as is folk in statistical learning, requires to deal with
the indicator function χ[] that appears in (1), which is not optimization-friendly.

(6)

1≤p,q≤Q

(7)

if p (cid:54)= q ,
otherwise.

Confusion Risk.
In order to deal with the latter problem and prepare the ground for tackling the
former one from a theoretical point of view, we now introduce and discuss the confusion risk, which
is parameterized by a family of loss functions that may be surrogate for the indicator loss χ[] .
(cid:26) EX |p (cid:96)q |p (h, X )
Deﬁnition 1 (Confusion Risk). The confusion risk C(cid:96) (h) of h is deﬁned as
(cid:1)
C(cid:96) (h) = (cid:0)c(cid:96)
∈ RQ×Q , with c(cid:96)
.
=
pq
pq
0
Observe that if the family (cid:96)misclass of losses from Example 1 is retained, and Cpq = 1 for all p, q then
C(cid:96)misclass (h) is precisely the confusion matrix discussed above (with the diagonal set to zero).
= (cid:107)πC(cid:96) (h)(cid:107)1 and R(cid:96) (h) ≤ √
Q(cid:107)C(cid:96)(h) (cid:107).
.
Similarly as before, the (cid:96)-risk R(cid:96) (h) is deﬁned as R(cid:96) (h)
The following lemma directly comes from Equation (6).
Lemma 1. Let h ∈ H. If 0 ≤ χ[h(x)(cid:54)=p] ≤ (cid:96)q |p (h, x) , ∀x ∈ X , ∀p, q ∈ Y , then (cid:107)C (h)(cid:107) ≤ (cid:107)C(cid:96) (h)(cid:107).
This says that if we recourse to surrogate (cid:96) that upper bounds the 0-1 indicator function then the
norm of the confusion risk is always larger than the norm of the confusion matrix.
Armed with the confusion risk and the last lemma, we may now turn towards the legitimate objective
h∈H (cid:107)C(cid:96) (h)(cid:107),
min
a small value of (cid:107)C(cid:96) (h)(cid:107) implying a small value of (cid:107)C (h)(cid:107) (which was our primary goal).
It is still impossible to solve this problem because of the unknown distribution D according to which
expectations are computed. However, as already evoked, it is possible to derive learning strategies
based on empirical estimates of the confusion risk, that ensures (cid:107)C(cid:96) (h)(cid:107) will be small. The next
section is devoted to show how this could be done.

3

3 Bounds on the Confusion Risk via Online Learning and COPA

(8)

3.1
(Empirical) Online Confusion Risk
Assume an online learning algorithm A that outputs hypotheses from a family H: run on the traning
Deﬁnition 2 ( (cid:98)L|p (·, ·) and L|p (·) matrices). Let (cid:96) = ((cid:96)q |p ) be a family of losses and let p ∈ Y .
sequence Z ∼ DT , A outputs hypotheses h = {h0 , . . . , hT }, where h0 is an arbitrary hypothesis.
(cid:26) (cid:96)v |u (h, x)
For h ∈ H and x ∈ X , we deﬁne
(cid:98)L|p (h, x) = ((cid:98)luv )1≤u,v≤Q ∈ RQ×Q , with (cid:98)luv
if u = p and v (cid:54)= u,
.
=
otherwise.
0
The matrix L|p (h) ∈ RQ×Q is naturally given by
L|p (h) = EX |p (cid:98)L|p (h, X ).
Remark 1. Note that the only row that may be nonzero in (cid:98)L|p (h, x) and L|p (h) is the pth row.
Deﬁnition 3 ((Conditional) Online Confusion Risk). Let y ∈ Y T be a ﬁxed sequence of labels and
Y a random sequence of T labels. Let h = {h0 , . . . , hT −1} be a sequence of T hypotheses.
The conditional and non-conditional confusion risks C(cid:96),y (h) and C(cid:96),Y (h) are deﬁned as
T(cid:88)
T(cid:88)
L|yt (ht−1 ),
L|Yt (ht−1 ).
1
1
TYt
Tyt
t=1
t=1
Remark 2. C(cid:96),Y (h) is a random variable. In order to provide our generalization bounds, it will be
more convenient for us to work with the conditional confusion C(cid:96),y (h). A simple argument will
then allow us to provide generalization bounds for C(cid:96),Y (h).
Deﬁnition 4. Let y ∈ Y T be a ﬁxed sequence of labels. Let h = {h0 , . . . , hT −1 } be the hypotheses
output by A when run on Z y = {(Xt , yt )}T
The (non-)conditional empirical online confusion risks (cid:98)C(cid:96),y (h, X ) and (cid:98)C(cid:96),Y (h, X ) are
t=1 , such that Xt is distributed according to DX |yt .
T(cid:88)
T(cid:88)
(cid:98)C(cid:96),Y (h, X )
(cid:98)C(cid:96),y (h, X )
(cid:98)L|Yt (ht−1 , Xt ).
(cid:98)L|yt (ht−1 , Xt ),
1
1
.
.
and,
=
=
Tyt
TYt
t=1
t=1

and, C(cid:96),Y (h)

C(cid:96),y (h)

(11)

(10)

(9)

.
=

.
=

We now are almost ready to provide our results. We just need to introduce a pivotal result that is a
corollary of Theorem 7.1 from [8], a proof of which can be found in the appendix.
Corollary 1 (Rectangular Matrix Azuma). Consider a sequence {Uk } of d1 × d2 random matrices,
and a ﬁxed sequence of scalars {Mk } such that
EUk |U1 ...Uk−1 Uk = 0, and (cid:107)Uk (cid:107) ≤ Mk almost surely.
(cid:26)
(cid:27)
(cid:110)(cid:13)(cid:13)(cid:13)(cid:88)
(cid:13)(cid:13)(cid:13) ≥ t
(cid:111) ≤ (d1 + d2) exp
Then, for all t > 0,
k
3.2 New Results

, with σ2 .
=

(cid:88)
k

− t2
2σ2

M 2
k .

Uk

P

This subsection reports our main results.
Theorem 1. Suppose that the losses in (cid:96) are such that 0 ≤ (cid:96)q |p ≤ M for some M > 0. Fix y ∈ Y T .
(cid:115)
For any δ ∈ (0; 1], it holds with probability 1 − δ over the draw of X ∼ DX |y that
(cid:13)(cid:13)(cid:13)C(cid:96),y (h) − (cid:98)C(cid:96),y (h, X )
(cid:13)(cid:13)(cid:13) ≤ M
(cid:88)
Q
1
δ
Tp
p
where h = {h0 , . . . , hT −1} is the set of hypotheses output by A when provided with {(Xt , yt )}T
t=1 .

(12)

2Q

log

4

1
Tyt

P

(13)

∆t,q vq

1
Tyt

.
=

Ut

log

Q
δ

.

sup
v :(cid:107)v(cid:107)≤1

(cid:107)Utv(cid:107)2 =

t,q ≤ M
∆2
Tyt

we observe that
(cid:107)Ut(cid:107) = sup
v :(cid:107)v(cid:107)≤1

Therefore, with probability at least 1 − δ
(cid:107)C(cid:96),y (h)(cid:107) ≤ (cid:13)(cid:13)(cid:13) (cid:98)C(cid:96),y (h, X )
(cid:13)(cid:13)(cid:13) + M
(cid:88)
1
Tp
p
(cid:98)L|yt (ht−1 , Xt ).
Proof. The proof is straightforward using Corollary 1. Indeed, consider the random variable
L|yt (ht−1 ) − 1
Tyt

1
Tyt
On the one hand, we observe that:
since EXt |X 1:t−1 y1:t (cid:98)L|yt (ht−1 , Xt ) = L|yt (ht−1 ). On the other hand, introducing
EXt |X 1:t−1 ,y Ut = EXt |X 1:t−1 ,y1:t
Ut = 0,
= EXt |yt (cid:96)q |yt (ht−1 , Xt ) − (cid:96)q |yt (ht−1 , Xt ),
.
∆t,q
(cid:115) (cid:88)
(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:88)
(cid:112)Q.
q (cid:54)=yt
q (cid:54)=yt
where we used that the only row of Ut not to be zero is its yt th row (see Remark 1), the fact that
supv :(cid:107)v(cid:107)≤1 v · u = (cid:107)u(cid:107)2 and the assumption 0 ≤ (cid:96)q |p ≤ M , which gives that |∆t,q | ≤ M .
√
(cid:40)
(cid:41)
Using Corollary 1 on the matrix martingale {Ut}, where (cid:107)Ut(cid:107) ≤ M
(cid:13)(cid:13)(cid:13) ≥ ε
(cid:110)(cid:13)(cid:13)(cid:13)(cid:88)
(cid:111) ≤ 2Q exp
Q/Tyt almost surely, gives
2M 2Q (cid:80)
ε2
−
Ut
1
t
t
T 2
yt
(cid:115)
Setting the right hand side to δ gives that, with probability at least 1 − δ
(cid:13)(cid:13)(cid:13) ≤ M
(cid:13)(cid:13)(cid:13)(cid:88)
(cid:88)
2Q
1
= (cid:80)
(cid:80)
2Q
T 2
δ
t
t
yt
gives (12). The triangle inequality |(cid:107)A(cid:107) − (cid:107)B (cid:107)| ≤ (cid:107)A − B (cid:107) gives (13).
p T −1
t T −2
yt
p
If one takes a little step back to fully understand Theorem 1, it may not be as rejoicing as expected.
Indeed, it provides a bound on the norm of the average confusion risks of hypotheses h0 , . . . , hT −1 ,
which, from a practical point of view, does not say much about the norm of the confusion risk
of a speciﬁc hypothesis.
In fact, as is usual in online learning [1], it provides a bound on the
confusion risk of the Gibbs classiﬁer, which uniformly samples over h0 , . . . , hT −1 before outputting
a prediction. Just as emphasized by [1], things may turn a little bit more enticing when the loss
functions (cid:96) are convex with respect to their ﬁrst argument, i.e.
∀h, h(cid:48) ∈ H, ∀p, q ∈ Y , ∀λ ∈ [0, 1], (cid:96)q |p (λh + (1 − λ)h(cid:48) , x) ≤ λ(cid:96)q |p (h) + (1 − λ)(cid:96)q |p (h(cid:48) , x). (14)
In that case, we may show the following theorem, that is much more compatible with the stated goal
of trying to ﬁnd a hypothesis that has small (or at least, controlled) confusion risk.
Theorem 2. In addition to the assumptions of Theorem 1 we now assume that (cid:96) is made of convex
losses (as deﬁned in (14)).
(cid:115)
For any δ ∈ (0; 1], it holds with probability 1 − δ over the draw of X ∼ DX |y that
(cid:107)C(cid:96) (h)(cid:107) ≤ (cid:13)(cid:13)(cid:13) (cid:98)C(cid:96),y (h, X )
(cid:13)(cid:13)(cid:13) + M
(cid:88)
T(cid:88)
2Q
t=1
Proof. It is a direct consequence of the convexity of (cid:96) combined with Equation (6).

1
Tp

p

log

, with h

.
=

1
T

ht−1 .

(15)

Ut

ln

.

(cid:115)

2Q

Q
δ

5

It is now time to give the argument allowing us to state results for the non-conditional online
(cid:80)
PA|B=b (E (A, b))PB (B = b) ≥ (cid:80)
If a random event E (A, B ) deﬁned with respect to random variables A and
confusion risks.
B is such that PA|B=b (E (A, b)) ≥ 1 − δ for all possible values of b then PAB (E (A, B )) =
b (1 − δ)PB (B = b) = 1 − δ . The results of Theorem 1
b
and Theorem 2 may be therefore stated in terms of Y instead of y .
In light of the generic results of this subsection, we are now ready to motivate and derive a new
online learning algorithm that aims at a small confusion risk.

3.3 Online Learning with COPA

This subsection presents a new online algorithm, COPA (for COnfusion Passive-Aggressive learn-
ing). Before giving the full detail of our algorithm, we further discuss implications of Theorem 2.
is bounded by (cid:107) (cid:98)C(cid:96),y (h, X )(cid:107), plus some quantity directly related to the number of training data en-
A ﬁrst message from Theorem 2 is that, provided the functions (cid:96) considered are convex, it is relevant
to use the average hypothesis h as a predictor. We indeed know by (15) that the confusion risk of h
countered. The second message from the theorem is that the focus naturally comes to (cid:107) (cid:98)C(cid:96),y (h, X )(cid:107)
According to Deﬁnition 4, (cid:98)C(cid:96),y (h, X ) is the sum of
and the question as to how minimize this quantity.
(cid:98)L|yt (ht−1 , Xt )/Tyt .
instantaneous confusion matrices
(cid:98)L|yt (h, Xt )/Tyt with respect to h to get ht , with the hope that the instantaneous risk of ht on
In light of (6),
it does make sense to try to minimize each entry of
Xt+1 will be small: one may want to minimize the norm of (cid:98)L|yt (h, Xt )/Tyt and pose a problem
(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13) .
(cid:98)L|yt (h, Xt )
like the following:
However, as remarked before, (cid:98)L|yt has only one nonzero row, its yt th. Therefore, the operator norm
min
Tyt
h
of (cid:98)L|yt (h, Xt )/Tyt is simply the Euclidean norm of its yt th row. Trying to minimize the square
(cid:88)
Euclidean norm of this row might be written as
1
T 2
q (cid:54)=yt
yt
W = {w1 , . . . , wQ} with (cid:80)
This last equation is the starting point of COPA. To see how the connection is made, we make some
instantiations. The hypothesis space H is made of linear classiﬁers, so that a sequence of vectors
(cid:12)(cid:12)(cid:12)(cid:12)+
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq , x(cid:105) +
q wq = 0 deﬁnes a hypothesis hW . The family (cid:96) COPA builds upon is
, ∀p, q ∈ Y .
1
(cid:96)q |p (hW , x) =
Q − 1
In other words, COPA is an instance of Example 2. We focus on this setting because it is known that,
in the batch scenario, it provides Bayes consistent classiﬁers [3, 4]. Given a current set of vectors
Q}, using (16), and implementing a passive-aggressive learning strategy [2], the
Wt = {wt
1 , . . . , wt
(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq , x(cid:105) +
(cid:88)
Q(cid:88)
(cid:13)(cid:13)2
(cid:13)(cid:13)wq − wt
new set of vectors Wt+1 is searched as the solution of
W,(cid:80)
min
q
2
q (cid:54)=y
q wq =0
+
q=1
It turns out that it is possible to ﬁnd the solution of this optimization problem without having to
recourse to any involved optimization procedure. This is akin to a result obtained by [6], which
applies for another family of loss functions. We indeed prove the following theorem (proof in
supplementary material).
(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq , x(cid:105) +
(cid:88)
Q(cid:88)
(cid:13)(cid:13)2
(cid:13)(cid:13)wq − wt
Theorem 3 (COPA update). Suppose we want to solve
W,(cid:80)
min
q
2
q (cid:54)=y
q wq =0
+
q=1

1
Q − 1

1
Q − 1

(cid:96)2
q |yt

(h, Xt ).

+

C
2

+

C
2T 2
y

.

(17)

.

(18)

1
2

1
2

min
h

(16)

6

Algorithm 1 COPA
Input: z = {(xt , yt )}T
t=1 training set (realization of Z ), R number of epochs over z , C cost
Output: W = {w1 , . . . , wQ}, the classiﬁcation vectors
τ = 0
w0
1 = . . . = w0
Q
for r=1 to R do
for t=1 to T do
q − (cid:16)
receive (xt , yt )
compute α∗ according to (20)
∀q , perform the update: wτ +1
q ← wτ
τ ← τ + 1
(cid:80)τ
end for
end for
∀q , wq ← 1
k=1 wk
q
τ

(cid:80)I ∗
q=1 α∗
q

q − 1
α∗
Q

(cid:17)

xt

Let (cid:96)t
q be deﬁned as

(cid:96)t
σ(I ) +

= (cid:104)wq , x(cid:105) + 1/(Q − 1).
.
(cid:96)t
q
Let σ be a permutation deﬁned over {1, . . . , Q − 1} taking values in Y \{y} such that
σ(1) ≥ . . . ≥ (cid:96)t
(cid:96)t
σ(Q−1) .
Let I ∗ be the largest index I ∈ {1, . . . , Q − 1} such that
I−1(cid:88)
(cid:107)x(cid:107)2
1
σ(q) > 0, with κ
(cid:96)t
κQ − (I − 1)(cid:107)x(cid:107)2
C
q=1
 1
(cid:19)
(cid:18)
If I ∗ is set to I ∗ .
= {σ(1), . . . , σ(I ∗ )}, then we may deﬁne α∗ = [α∗
1 · · · α∗
Q ] as
(cid:107)x(cid:107)2
sα (I ∗ )
if q ∈ I ∗
, where sα (I ∗ )
κ
Q
otherwise
0
(cid:33)
(cid:32)
I ∗(cid:88)
and the vectors
q − 1
α∗
Q
q=1
are the solution of optimization problem (18). These equations provide COPA’s update procedure.

Q
κQ − I ∗ (cid:107)x(cid:107)2

(cid:88)
q∈I ∗

x, q = 1, . . . , Q

+ (cid:107)x(cid:107)2

w∗
q

q −
.
= wt

α∗
q

α∗
q

.
=

(cid:96)t
q +

.
=

.
=

(cid:96)t
q .

(20)

(19)

(21)

The full COPA algorithm is depicted in Algorithm 1.

4 Numerical Simulations

We here report results of preliminary simulations of COPA on a toy dataset. We generate a set of
5000 samples according to three Gaussian distributions each of variance σ2 I with σ = 0.5. One
of the Gaussian is centered at (0, 1), the other at (1, 0) and the last one at (−1, 0). The respective
weights of the Gaussian distributions are 0.9, 0.05 and 0.05. The ﬁrst generated sample is used
to choose the parameter C of COPA with a half split of the data between train and test; 10 other
samples of size 5000 are generated and split as 2500 for learning and 2500 for testing and the results
are averaged on the 10 samples. We compare the results of COPA to that of a simple multiclass
perceptron procedure (the number of epochs for each algorithm is set to 5). As recommended by the
theory we average the classiﬁcation vector to get our predictors (both for COPA and the perceptron).
The essential ﬁnding of these simulations is that COPA and the perceptron achieve similar rate of
classiﬁcation accuracy, 0.85 and 0.86, respectively but the norm of the confusion matrix of COPA is
0.10 and that of the Perceptron is 0.18. This means COPA indeed does its job in trying to minimize
the confusion risk.

7

5 Conclusion

In this paper, we have provided new bounds for online learning algorithms aiming at controlling
their confusion risk. To the best of our knowledge, these results are the ﬁrst of this kind. Motivated
by the theoretical results, we have proposed a passive-aggressive learning algorithm, COPA, that
has the seducing property that its updates can be computed easily, without having to resort to any
optimization package. Preliminary numerical simulations tend to support the effectiveness of COPA.
In addition to complementary numerical simulations, we want to pursue our work in several direc-
tions. First, we would like to know whether efﬁcient update equation can be derived if a simple
hinge, instead of a square hinge is used. Second, we would like to see if a full regret-style analysis
can be made to study the properties of COPA. Also, we are interested in comparing the merits of our
theoretical results with those recently proposed in [5] and [7], which propose to address learning
with the confusion matrix from the algorithmic stability and the PAC-Bayesian viewpoints. Finally,
we would like to see how the proposed material can be of some use in the realm of structure predic-
tion and by extension, in the case where the confusion matrix to consider is inﬁnite-dimensional.
Acknowledgments. Work partially supported by Pascal 2 NoE ICT-216886-NOE, French ANR
Projects ASAP (ANR-09-DEFIS–001) and GRETA (ANR-12-BS02-0004).

P

,

I,

Xk

X 2
k

Appendix
Theorem 4 (Matrix Azuma-Hoeffding, [8]). Consider a ﬁnite sequence {Xk } of self-adjoint ma-
trices in dimension d, and a ﬁxed sequence {Ak } of self-adjoint matrices that satisfy Ek−1Xk = 0
and
(cid:52) A2
k , and AkXk = XkAk , almost surely.
(cid:111) ≤ d · e−t2 /2σ2
(cid:17) ≥ t
(cid:16)(cid:88)
(cid:110)
λmax
k

Then, for all t ≥ 0,
with σ2 = (cid:13)(cid:13)(cid:80)
(cid:13)(cid:13) .
k A2
k
Proof of Corollary 1. To prove the result, it sufﬁces to make use of the dilation technique and apply
(cid:20) 0 U
(cid:21)
Theorem 4. The self-adjoint dilation D(U ) of a matrix U ∈ Rd1×d2 is the self-adjoint matrix D(U )
of order d1 + d2 deﬁned by
D(U ) =
U ∗
0
where U ∗ is the adjoint of U (as U has only real coefﬁcient here, U ∗ is the transpose of U ).
As recalled in [8], (cid:107)D(U )(cid:107) = (cid:107)U (cid:107) and, therefore, the largest eigenvalue λmax of D2 (U ) is equal to
(cid:107)U (cid:107)2 and D2 (U ) (cid:52) (cid:107)U (cid:107)2 I.
Considering Uk , we get that, almost surely:
D2 (Uk ) (cid:52) M 2
k
and since dilation is a linear operator, we have that
EUk |U1 ···Uk−1 D(Uk ) = 0.
The sequence of matrices {D(Uk )} is therefore a matrix martingale that veriﬁes the assumption of
(cid:26)
(cid:27)
(cid:111) ≤ (d1 + d2 ) exp
(cid:17) ≥ t
(cid:16)(cid:88)
(cid:110)
Theorem 4, the application of which gives
− t2
D(Uk )
with σ2 = (cid:80)
P
λmax
,
2σ2
k
(cid:13)(cid:13)(cid:13) ,
(cid:13)(cid:13)(cid:13)(cid:88)
(cid:17)(cid:17)
(cid:16)D (cid:16)(cid:88)
(cid:17)
(cid:16)(cid:88)
k . Thanks to the linearity of D ,
k M 2
D(Uk )
= λmax
λmax
k

Uk

k

=

Uk

k

which closes the proof.

8

References
[1] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of online learning
algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057, 2004.
[2] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585, 2006.
[3] Y. Lee. Multicategory support vector machines, theory, and application to the classiﬁcation of
microarray data and satellite radiance data. Technical report, University of Wisconsin, 2002.
[4] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines. Journal of the American
Statistical Association, 99:67–81, march 2004.
[5] P. Machart and L. Ralaivola. Confusion matrix stability bounds for multiclass classiﬁcation.
Technical report, Aix-Marseille University, 2012.
[6] S. Matsushima, N. Shimizu, K. Yoshida, T. Ninomiya, and H. Nakagawa. Exact passive-
aggressive algorithm for multiclass classiﬁcation using support class. In SDM 10, pages 303–
314, 2010.
[7] E. Morvant, S. Koc¸ o, and L. Ralaivola. PAC-Bayesian Generalization Bound on Confusion
Matrix for Multi-Class Classiﬁcation. In John Langford and Joelle Pineau, editors, International
Conference on Machine Learning, pages 815–822, Edinburgh, United Kingdom, 2012.
[8] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-
tional Mathematics, pages 1–46, 2011.

9


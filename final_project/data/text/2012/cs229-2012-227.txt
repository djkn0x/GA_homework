Using Twitter Data to Predict Box Ofﬁce Revenues
P. Thomas Barthelemy (bartho@stanford.edu)
Department of Computer Science, 353 Serra Mall
Stanford, CA 94305

Devin Guillory (deving09@stanford.edu)
Department of Computer Science, 353 Serra Mall
Stanford, CA 94305

Chip Mandal (cmandal@stanford.edu)
Department of Computer Science, 353 Serra Mall
Stanford, CA 94305

Abstract

We summarize an effort to predict box ofﬁce revenues using
Twitter data. It is hypothesized that an increased number of
tweets about a movie before its release will result in increased
box ofﬁce revenue. Our task can be decomposed into two sub-
tasks: the estimation of the frequency of tweets about particu-
lar movies and revenue prediction given this frequency.
Keywords: Tweet classiﬁcation, Na¨ıve Bayes.

Overview and Motivation
The strategy was to ﬁrst identify the number of tweets about
the movie prior to movie opening, and then to use regression
to create a model for predicting box ofﬁce revenue. The for-
mer was the more challenging task, and we approached it in
three different ways. The most basic way was to count the oc-
currence of the title in tweets, although there are clear cases
in which this is not expected to perform well. Next, we at-
tempted a variant of Na¨ıve Bayes. Finally, we utilized a bag
of words model to estimate the frequency of tweets which are
about the movie.
We did not use hashtags during classiﬁcation, as our Twit-
ter dataset is from 2009, before hashtags were commonly
used.

Data and Processing
We used two separate data sources: Twitter data and movie
reviews from IMDB.
Twitter Data
The Twitter data included a sampling of approximately half
a billion tweets over the last 6 months of 2009. Because we
wanted to predict revenue for 80 movies and tweets about
movies occurred at a rate of 1/100 at best, and usually far less
than that, it was not feasible to label tweets for each movie.
We manually examined 10k tweets to label those that were
about movies in general—that is, about any movie. This was
used to identify the prior probability of a tweet being about
any movie, a value used in the Na¨ıve Bayes analysis.
On occasion, we used a search-labeled set of tweets by
searching for movies for which it was unlikely to mistake the
title or keyword for a non-movie reference, and we assumed
that this correctly labeled the tweets. For instance, we used

“transformers” as an indicator for Transformers: Revenge of
the Fallen and “inglourious” for Inglorious Basterds. Such
a classiﬁcation method was expected to bias our probabilities
of movie-speciﬁc words—that is, we would expect an overes-
timated probability of the movie title—and thus was not used
for such purpose. Rather, the approximately labeled tweets
were used for identifying movie general words (e.g. “movie”,
“watch”, “tonight”) or for validating classiﬁcation.
Initial word counts were performed using grep. The perfor-
mance of grep was slow, especially since some of our algo-
rithms required searching multiple keywords in a given ﬁle.
For better performance, we indexed the tweets using Apache
Lucene. Direct frequency calculation was performed using
this index. Inference was implemented using tweet-by-tweet
classiﬁcation.
IMDB Review Data
We used IMDB for two goals: to identify general attributes
for each movie (e.g. opening day, box ofﬁce revenue) and to
observe the probability of generating a particular word in ref-
erence to a movie. Concerning this latter point, we assumed
that the probability of generating a word in an IMDB review
about a given movie was the same as the probability of gen-
erating the same word in a tweet about the same movie. 30
reviews per movie were taken from IMDB. Each set including
about 10,000 words in total.
Models
We used multiple methods to estimate the frequency of tweets
as input to our regression model. The two initial strategies at-
tempt to classify these tweets individually, and the remaining
strategies consider instead a particular day as simply a mix
of a non-movie-speciﬁc bag of words and a movie-speciﬁc
tweets bag of words.
Title Search
To provide baseline performance, we ﬁt linear regression
model using a keyword search, as shown in Figure 1. That
is, we simply searched the occurrence of the title (case insen-
sitive) in all of the tweets in the week before their respective
opening days.

Figure 1: Count of tweets having title words versus movie
revenue.

Searching for the movie title is not always a good indica-
tor that the tweet is about a movie. There are two common
cases in which this posed a problem. First is the case in which
the movie title is long and infrequently mentioned in its en-
tirety. For instance, The Lord of the Rings: The Fellowship of
the Ring is often referred to as “Lord of the Rings” or even
“LOTR”. Second is the case in which the title is very short
and likely to be contained in tweets that do not refer to the
movie. One example of this is Shorts, a movie released in
August 2009.
Na¨ıve Bayes
We could not use conventional Na¨ıve Bayes for tweet clas-
siﬁcation because we did not know the prior probability of a
tweet being about a speciﬁc movie. (If we did, this part of
our project would be trivial!) We considered circumventing
this problem by decomposing the causal model into one for
which the probabilities could be estimated.
Let ma be the variable representing the tweet being about
any movie (Ma ) or not about any movie (¬Ma ), let ms be the
variable representing a tweet being about a movie (Ms ) or not
about a movie (¬Ms ), and let W represent the generation of
a particular word. Given the graphical model in Figure 2,
and using the simplifying assumption that p(W |Ma , Ms ) =
p(W |Ma ) p(W |Ms ) and the fact that p(Ms |¬Ma ) = 0, we can
represent the probability of a tweet being about a speciﬁc
movie given a word.

p(Ms |W ) =

=

=

p(Ms ,W )
p(W )
∑ma p(W |Ms , ma ) p(Ms |ma ) p(ma )
∑ma ∑ms p(W |ms , ma ) p(ms |ma ) p(ma )
p(W |Ms ) p(W |Ma ) p(Ms |Ma ) p(Ma )
∑ma ∑ms p(W |ms ) p(W |ma ) p(ms |ma ) p(ma )

Figure 2: The graphical model representing the probability
of a tweet containing a particular word conditional upon it
being about any movie and conditional upon it being about a
particular movie.

There is a similar derivation for p(¬Ms |W ), though the
summand in the numerator remains. This gives us many more
probabilities that we have to estimate. However, there are
ways to approximate them:
• p(ma ) was calculated using the 10k hand labeled tweets. It
was observed that the prior probability of the tweet being
about a movie is roughly 1/200.
• p(W |ma ) was calculated using search-labeled data. Be-
cause the search-labeled data was selected on the basis of
the movie title, the sampling method was not expected to
adversely effect the probabilities of movie-general words.
Words with high p(W |Ma ) included “movie”, “watch”, and
“tonight”.
• p(W |ms ) was approximated using IMDB data. That is, we
assumed that the distribution of words in IMDB movie re-
views matched the distribution of words in tweets about the
same movie. For the movie Transformers: Revenge of the
Fallen, words with high p(W |Ms ) included “transformers”,
“bumblebee”, and “optimus”.
• p(ms |ma ) could not be measured. Our strategy was to as-
sume that it would be ﬁxed and calculate it by optimizing
the F1 score using a few movies.

Frequent Itemset Analysis As an optimization, we lim-
ited the set of movie-general words using frequent itemset
analysis. That is, we identiﬁed a set of movie-general words
having the highest interest before calculating the probability
p(W |Ma ) for each. “Interest” is deﬁned as:
I nt erest (W ) = p(Ms ,W ) − p(W )

Here, we estimate p(Ms ,W ) by identifying the frequency of
word W as it appears in the result of an exact title search. The
other probability p(W ) is simply the frequency of word W in
all tweets.

Letting Sn be the n highest interest words for a particular
movie, we calculate how common the word W is using:

CW =

1
N

1{W ∈ Sn}

N
∑
n=0
For words common to many movie-speciﬁc tweets (e.g.
“movie”), this value is larger than 1/2. For words speciﬁc
to only one movie, it is close to 1/N .
Estimating p(Ms |Ma ) We selected a p(Ms |Ma ) to optimiz-
ing the F1 score for particular test movies. It was expected
that optimizing over various movies would allow us to select
an average value which we could use for Na¨ıve Bayes.
We computed the F1 score by measuring both precision
and recall. We used our search-labeled dataset to measure
both values, though this allows one to obtain a recall value
greater than one. For instance, when comparing classiﬁca-
tion of tweets about Transformers: Revenge of the Fallen for
which the label is deﬁned based on the presence of the word
“transformers”, our classiﬁcation algorithm identiﬁed some
tweets that were about the movie but did not contain the word
“transformers”. For instance, the algorithm identiﬁed tweets
referring to the character Bumblebee. Nevertheless, the goal
of this exercise was to optimize the F1 score, which could be
performed regardless of the denominator used in recall.
The ultimate concern with the Na¨ıve Bayes model is ap-
parent in Table 1, which shows the optimal conditional prob-
ability for a set of movies. Note that the optimal value of
p(Ms |Ma ) varies greatly from movie to movie; it varies from
1/500 to 9/10. Correspondingly, the precision and recall was
greatly reduced when using p(Ms |Ma ) far away from the op-
timal value for that movie. Thus, the probability of a tweet
being about a speciﬁc movie given that that tweet is about any
movie cannot be well approximated by a ﬁxed value. Thus,
our Na¨ıve Bayes model would not be useful by itself to pre-
dict the number of tweets about a movie. It is for this reason
that we did not use it to predict movie revenue.

Table 1: Optimal p(Ms |Ma ) values.
p(Ms |Ma )
0.002
0.01
0.9

Movie Title
Law Abiding Citizen
District 9
Transformers ...

Performance
In Table 2, we compare performance of
Na¨ıve Bayes to the direct title search. To do so, we searched
over a subset of tweets on opening day of the particular
movie. Determining precision is straight-forward: we can
simply hand classify the positively-labeled tweets. Recall is
more challenging: because it is not feasible to label enough
tweets for a usable sample, we must presume a number of true
positives. However, in our case, we provide the F1 values pri-
marily to compare two classiﬁcation strategies, and thus the

number of true positives is arbitrary as long as we keep it con-
sistent. For each movie, we use as the number of true posi-
tives the maximum value of correctly identiﬁed tweets over
the two classiﬁcation techniques.

Table 2: Comparison of Title Search and Na¨ıve Bayes perfor-
mance using F1 score.

Movie Title
Law Abiding Citizen
Fame
Zombieland
Transformers ...

Title Search Na¨ıve Bayes
0.73
0.64
0.28
0.29
0.35
1.00
0.04
0.84

Notably, there is a large performance improvement when
searching for movies with long names like Transformers: Re-
venge of the Fallen, which was expected. However, there is
not a signiﬁcant gain in the identiﬁcation of tweets relavent to
a movie titled with a short, commonly used word like Fame.
Further, the title search performs much better when looking
for movies titled with short, uncommonly used words like
Zombieland.
In general, the Na¨ıve Bayes model provides
more consistent F1 score, which suggests that it would re-
sult in better revenue prediction were it not for the fact that
we cannot hold ﬁxed p(Ms |Ma ).
There were observable differences in the word frequencies
of IMDB data and Twitter data. The primary difference is
that reviews about a particular movie infrequently reference
the movie title, as this context is understood by the audience.
Conversely, such a context is not understood in the Twitter-
sphere; the audience would not know that a tweet is about a
movie unless it contained a movie title or an obvious refer-
ence. Thus, we augmented our IMDB data by adding the title
to the IMDB reviews at a frequency of one out of every 20
words, which corresponds to the assumption that each tweet
about a movie contains roughly one mention of the title. This
ensured that the most indicative word for the movie was gen-
erally a word in the movie title itself. However, there were
cases in which other words were still more indicative, as in
“mj” (for Michael Jackson) for the movie This is It.
Additionally,
there was noticeable difference between
IMDB vernacular and Twitter vernacular. For instance, it
was observed that the IMDB reviews about This is It used the
word “mj” less frequently than tweets about the same movie.
This keyword appeared with a frequency of approximately
one mention every thousand words in IMDB reviews. How-
ever, when simply searching for tweets with the phrase “this
is it”, the incidence of “mj” was approximately one order of
magnitude higher (≈ 1/100). We consider this to be a con-
servative estimate because this set of tweets included some
tweets that were not about the movie (that is, searching for the
movie title was not completely precise). Thus, the incidence
of “mj” in tweets about This is It is certainly over 1/100.

Frequency Estimation
If we divide the tweets on opening day into two groups,
tweets about the speciﬁc movie and all remaining tweets, we
can consider both separately as bags of words. Next, if we as-
sume that on some arbitrary day far from the movie opening,
the bag is entirely not about the movie, then we can estimate
the “mix” of the bag on opening day. We estimate the follow-
ing values:
• p(W |¬Ms ) can be approximated as the frequency of word
W on an arbitrary day far away from opening day.
• p(W |Ms ) can be approximated as the frequency of word
W in IMDB reviews in the same manner as deﬁned in the
Na¨ıve Bayes analysis.
• p(W ) is essentially a mixed bag observed near or on open-
ing day. So, this probability is equal to the proportionate
contribution of each bag.
p(W ) = p(W |Ms ) p(Ms ) + p(W |¬Ms )(1 − p(Ms ))

p(Ms ) =

Ultimately, we can solve for the prior probability of the
movie, which provides the equation below. Assuming that
the length of each tweet is constant, then p(Ms ) gives us the
fraction of tweets about our movie.
p(W ) − p(W |¬Ms )
p(W |Ms ) − p(W |¬Ms )
Theoretically,
this equation should hold for any word.
However, this equation is sensitive to errors when the denom-
inator is close to zero. Our strategy for avoiding this circum-
stance is to use the word with the highest ratio of p(W |Ms )
to p(W |¬Ms ), which tended to prefer words in the movie title
like “pelham” from The Taking of Pelham 1 2 3.
Performance Results of this process are shown in Figure 3.
Some outliers are noticeable: it is impossible for a frequency
to be negative, and it is unlikely that nearly all tweets on one
day were about one movie. The data point having frequency
0.93 represents This is It, for which the frequency is grossly
overestimated because we underestimated the frequency of
“mj” in tweets about the movie. Ignoring movies with pre-
dicted frequencies less than zero and greater than 0.1, we have
Figure 4.
Table 3 shows a few comparisons between observed and
predicted frequencies, where the observed tweets were iden-
tiﬁed using a keyword search. Assuming that a majority of
the tweets about the test movies contain the keyword speci-
ﬁed in the table, then the observed frequency should be within
a factor of two of the actual frequency. Notice, however, that
the predicted frequency is one to two orders of magnitude too
large.
This is could be caused by an underestimation of p(W |Ms ).
The scope of discussion on IMDB movie reviews is much
more rich than what is generally included in movie tweets and
is likely to contain a much more varied set of movie-speciﬁc

Figure 3: Estimated frequency of movie-speciﬁc tweet versus
movie revenue, uncorrected.

Table 3: Observed and predicted frequencies of tweets about
a movie.

Movie Title
Ice Age 2
Sherlock Holmes
Transformers ...

Keyword
“ice age”
“sherlock”
“transformers”

Obs.
0.0004
0.0005
0.007

Pred.
0.003
0.01
0.02

words. That is, since a tweet about a movie must make obvi-
ous to its audience that it is about a particular movie within
only a few dozen words, it is unlikely that the tweet will men-
tion an obscure (or, improbable) word related to the movie.
IMDB reviews, on the other hand, are free to discuss more
nuanced topics at length and in detail.
In short, the distri-
bution of words for tweets is skewed towards a smaller set
of words. Thus, when selecting the word with the maximum
frequency, we underestimate p(W |Ms ), which would in turn
make our predicted p(Ms ) too high.
Nevertheless, the source of this error affects the data in a
sufﬁciently regular fashion that the model offers slightly im-
proved prediction over the title search model, which is dis-
cussed in the Model Comparison section.
Frequency Estimate Variant As a slight tweak, if we take
the words with the highest p(W |Ms )/ p(W |¬Ms ) and scale the
numerator of the p(W ) equation above, we are left with an
equation that biases movies for which the indicative words
have large changes from the control day and for which the
word strongly indicates the movie. Roughly, our score is as
follows:
keyword score = ( p(W ) − p(W |¬Ms ))

p(W |Ms )
p(W |¬Ms )
This score further improves our ability to predict box ofﬁce
movie revenue.

Figure 4: Estimated frequency of movie-speciﬁc tweet versus
movie revenue, with outliers removed.

Figure 6: Performance of the various models. Average RMS
error from LOOCV decreases with each subsequent model.

though there remains room for improvement.
We demonstrated the potential to use labeled data from an
alternative source when labeled data from the target source
is absent. However, the result is strictly an approximation
and ignores the differing contexts and colloquialisms idiosyn-
cratic to a particular medium.
The revenue of a movie may also be determined by other
factors, like revenue of the lead actor, budget of the movie,
etc. Adding additional features like this to suppliment tweet
frequency could provide a better model for revenue predic-
tion.
Future work could also combine the frequency estimation
with classiﬁcation. That is, if one can estimate the prior prob-
ability of tweets about a movie, then one could apply our vari-
ant of Na¨ıve Bayes. This could allow one to apply more so-
phisticated text analysis, like sentiment analysis.

Figure 5: Keyword score and movie revenue.

Model Comparison
For each model, we predict movie revenue using linear
regression. Next, we use leave one out cross validation
(LOOCV) to compare the performance of our models.
As shown in Figure 6, the average RMS error is improved
from $14M to $10M, which one could compare to the av-
erage revenue of movies in our set, $14M. Further, we tried
ﬁtting to higher order polynomials, but we observed marginal
improvement at second order and overﬁtting at higher order
polynomials. In short, the accuracy of our models’ predic-
tions leaves room for future work.
Conclusion and Future Work
We demonstrated various techniques for estimating tweet fre-
quencies and attempted to use this to predict movie revenue.
Prediction accuracy was improved over a simple title search,


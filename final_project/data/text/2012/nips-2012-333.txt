Fusion with Diffusion for Robust Visual Tracking

Yu Zhou1 ∗, Xiang Bai1 , Wenyu Liu1 , Longin Jan Latecki2
1 Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, P. R. China
2 Dept. of Computer and Information Sciences, Temple Univ., Philadelphia, USA
{zhouyu.hust,xiang.bai}@gmail.com,liuwy@hust.edu.cn,latecki@temple.edu

Abstract

A weighted graph is used as an underlying structure of many algorithms like semi-
supervised learning and spectral clustering. If the edge weights are determined by
a single similarity measure, then it hard if not impossible to capture all relevant
aspects of similarity when using a single similarity measure.
In particular, in
the case of visual object matching it is beneﬁcial to integrate different similarity
measures that focus on different visual representations.
In this paper, a novel approach to integrate multiple similarity measures is pro-
posed. First pairs of similarity measures are combined with a diffusion process on
their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-
jects becomes a function of joint diffusion of the two original similarities, which
in turn depends on the neighborhood structure of the TPG. We call this process
Fusion with Diffusion (FD). However, a higher order graph like the TPG usually
means signiﬁcant increase in time complexity. This is not the case in the proposed
approach. A key feature of our approach is that the time complexity of the dif-
fusion on the TPG is the same as the diffusion process on each of the original
graphs. Moreover, it is not necessary to explicitly construct the TPG in our frame-
work. Finally all diffused pairs of similarity measures are combined as a weighted
sum. We demonstrate the advantages of the proposed approach on the task of
visual tracking, where different aspects of the appearance similarity between the
target object in frame t − 1 and target object candidates in frame t are integrat-
ed. The obtained method is tested on several challenge video sequences and the
experimental results show that it outperforms state-of-the-art tracking methods.

Introduction
1
The considered problem has a simple formulation: Given are multiple similarities between the same
set of n data points, each similarity can be represented as a weighted graph. The goal is to combine
them to a single similarity measure that best reﬂects the underlying data manifold. Since the set of
nodes is the same, it is easy to combine the graphs into a single weighted multigraph, where there
are multiple edges between the same pair of vertices representing different similarities. Then our
task can be stated as ﬁnding a mapping from the multigraph to a weighted simple graph whose edge
weights best represent the similarity of the data points. Of course, this formulation is not precise,
since generally the data manifold is unknown, and hence it is hard to quantify the ’best’. However,
it is possible to evaluate the quality of the combination experimentally in many applications, e.g.,
the tracking performance considered in this paper.
There are many possible solutions to the considered problem. One of the most obvious ones is a
weighted linear combination of the similarities. However, this solution does not consider the simi-
larity dependencies of different data points. The proposed approach aims to utilize the neighborhood
structure of the multigraph in the mapping to the weighted simple graph.
∗Part of this work was done while the author was visiting Temple University

1

Given two different similarity measures, we ﬁrst construct their Tensor Product Graph (TPG). Then
we jointly diffuse both similarities with a diffusion process on TPG. However, while the original
graphs representing the two measures have n nodes, their TPG has n2 nodes, which signiﬁcantly
increases the time complexity of the diffusion on TPG. To address this problem, we introduce an
iterative algorithm that operates on the original graphs and prove that it is equivalent to the diffusion
on TPG. We call this process Fusion with Diffusion (FD). FD is a generalization of the approached
in [26], where only a single similarity measure is considered. While the diffusion process on TPG in
[26] is used to enhances a single similarity measure, our approach aims at combining two different
similarity measures so that they enhance and constrain each others.
Although algorithmically very different, our motivation is similar to co-training style algorithms in
[5, 23, 24] where multiple cues are fused in an iterative learning process. The proposed approach is
also related to the semi-supervised learning in [6, 7, 21, 28, 29]. For online tracking task, we only
have the label information from the current frame, which can be regarded as the labeled data, and
the label information in the next frame is unavailable, which can be regarded as unlabeled data. In
this context, FD jointly propagates two similarities of the unlabeled data to the labeled data. The
obtained new diffused similarity, can be then interpreted as the label probability over the unlabeled
data. Hence from the point of view of visual tracking, but in the spirit of semi-supervised learning,
our approach utilizes the unlabeled data from the next frame for improved visual similarity to the
labeled data representing the tracked objets.
Visual tracking is an important issue in computer vision and has many practical applications. The
challenges in designing a tracking system are often caused by shape deformation, occlusion, view-
points variances, and background clutter. Different strategies have been proposed to obtain robust
tracking systems. In [8, 12, 14, 16, 25, 27], matching based strategy is utilized. Discriminate ap-
pearance model of the target is extracted from the current frame, then the optimal target is estimated
based on the distance/similatity between the appearance model and the candidate in the hypothesis
set. Classiﬁcation based strategies are introduced in [1, 2, 3, 4, 10, 11]. Tracking task is transformed
into foreground and background binary classiﬁcation problem in this framework. [15, 20] try to
combine both of those two frameworks. In this paper, we focus on improving the distance/similarity
measure to improve the matching based tracking strategy. Our motivation is similar to [12], where
metric learning is proposed to improve the distance measure. However, different from [12], multiple
cues are fused to improve the similarity in our approach. Moreover, the information from the forth-
coming frame is also used to improve the similarity. This leads to more stable tracking performance
than in [12].
Multiple cues fusion seem to be an effective way to improve the tracking performance. In [13],
multiple feature fusion is implemented based on sampling the state space. In [20], the tracking task
is formulated as the combination of different trackers, three different trackers are combined into a
cascade. Different from those methods, we combine different similarities into a single similarity
measure, which makes our method a more general for integrating various appearance models.
In summary, we propose a novel framework for integration of multiple similarity measures into a
single consistent similarity measure, where the similarity of each pair of data points depends on their
similarity to other data points. We demonstrate its superior performance on a challenging task of
tracking by visual matching.

2 Problem Formulation

The problem of matching based visual tracking boils down to the following simple formulation.
Given the target in frame It−1 which can be represented as image patch I1 enclosing the target, and
the set of candidate target patches in frame It , C = {In | n = 2, ..., N }, the goal is to determine
which patch in C corresponds to the target in frame It−1 . Of course, one can make this setting more
complicated, e.g., by considering more frames, but we consider this simple formulation in this paper.
The candidate set C is determined by the motion model, which is particularly simple in our setting.
The size of all the image patches is ﬁxed and the candidate set is composed of patches in frame It
inside a search radius r , i.e. ||c(In ) − c(I1 )|| < r , where c is the 2-D coordinate of center position
of the image patch.

2

Let S be a similarity measure deﬁned on the set of the image patches V = {I1 } ∪ C , i.e., S is a
function from V × V into positive real numbers. Then our tracking goal can be formally stated as
ˆI = arg max
X ∈C S (I1 , X )
(1)
meaning that the patch in C with most similar appearance to patch I1 is selected as the target location
in frame t.
Since the appearance of the target object changes, e.g., due to motion and lighting changes, single
similarity measure is often not sufﬁcient to identify the target in the next frame. Therefore, we con-
sider a set of similarity measures S = {S1 , . . . SQ}, each Sα deﬁned on V × V for α = 1, . . . , Q.
For example, in our experimental results, each image patch is represented with three histograms
based on three different features, HOG[9], LBP[18], Haar-like feature[4], which lead to three differ-
ent similarity measures. In other words, each pair of patches can be compared with respect to three
different appearance features.
We can interpret each similarity measure Sα as the afﬁnity matrix of a graph Gα whose vertex set
is V , i.e., Sα a N × N matrix with positive entries, where N is the cardinality of V . Then we
can combine the graphs Gα into a single multigraph whose edge weights corresponds to different
similarity measures Sα .
However, in order to solve Eq. (1), we need a single similarity measure S . Hence we face a question
how to combine the measures in S into a single similarity measure. We propose a two stage approach
to answer this question. First, we combine pairs of similarity measures Sα and Sβ into a single
α,β , which is a matrix of size N × N . P∗
measure P∗
α,β is deﬁned in Section 3 and it is obtained with
the proposed process called fusion with diffusion.
(cid:88)
In the second stage we combine all P∗
α,β for α, β = 1, . . . Q into a single similarity measure S
deﬁned as a weighted matrix sum
ωαωβ P∗
α,β
α,β
where ωα and ωβ are positive weights associated with measures Sα and Sβ deﬁned in Section 5.
We also observe that in contrast to many tracking by matching methods, the combined measure S is
not only a function of similarities between I1 and the candidate patches in C , but also of similarities
of patches in C to each other.
3 Fusion with Diffusion
3.1 Single Graph on Consecutive Frames
Given a single graph Gα = (V , Sα ), a reversible Markov chain on V can be constructed with the
where Di = (cid:80)N
transition probability deﬁned as
(3)
Pα (i, j ) = Sα (i, j )/Di
inherits the positivity-preserving property (cid:80)N
j=1 Sα (i, j ) is the degree of each vertex. Then the transition probability Pα (i, j )
j=1 Pα (i, j ) = 1, i = 1, ..., N .
(cid:26) Pα (i, j )
The graph Gα is fully connected graph in many applications. To reduce the inﬂuence of noisy points,
i.e., cluttered background patches in tracking, a local transition probability is used:
j ∈ kNN(i)
otherwise
(cid:80)n
0
Hence the number of non-zero elements in each row is not
larger than k , which implies
j=1 (Pk,α )(i, j ) < 1. This inequality is important in our framework, since it guarantees the
converge of the diffusion process on the tensor product graph presented in the next section.
3.2 Tensor Product Graph of Two Similarities
Given are two graphs Gα = (V , Pk,α ) and Gβ = (V , Pk,β ) deﬁned in Sec. 3.1, we can deﬁne their
Tensor Product Graph (TPG) as

(Pk,α )(i, j ) =

S =

(2)

(4)

(5)

Gα ⊗ Gβ = (V × V , P),

3

(6)

where P = Pk,α ⊗ Pk,β is the Kronecker product of matrices deﬁned as P(a, b, i, j ) =
Pk,α (a, b) Pk,β (i, j ). Thus, each entry of P relates four image patches. When Pk,α and Pk,β are
two N × N matrices, then P is a N 2 × N 2 matrix. However, as we will see in the next subsection,
we actually never compute P explicitly.
3.3 Diffusion Process on Tensor Product Graph
We utilize a diffusion process on TPG to combine the two similarity measures Pk,α and Pk,β . We
begin with some notations. The vec operator creates a column vector from a matrix M by stacking
the column vectors of M below one another. More formally vec : RN ×N → RN 2 is deﬁned as
vec(M )g = (M )ij , where i = (cid:98)(g − 1)/N (cid:99) + 1 and j = g mod N . The inverse operator vec−1
(cid:26) 1
that maps a vector into a matrix is often called the reshape operator. We deﬁne a diagonal N × N
matrix as
i = 1
∆(i, i) =
otherwise,
0
Only the entry representing the patch I1 is set to one and all other entries are set to zero in ∆.
We observe that P is the adjacency matrix of TPG Gα ⊗ Gβ . We deﬁne a q -th iteration of the
q(cid:88)
diffusion process on this graph as
(P)e vec(∆).
e=0
q(cid:88)
As shown in [26], this iterative process is guaranteed to converge to a nontrivial solution given by
(P)e vec(∆) = (I − P)−1 vec(∆),
e=0
where I is a identity matrix. Following [26], we deﬁne
α,β = P∗ = vec−1 ((I − P)−1 vec(∆))
P∗
We observe that our solution P∗ is a N × N matrix.
We call the diffusion process to compute P∗ a Fusion with Diffusion (FD) process, since diffusion
on TPG Gα ⊗ Gβ is used to fuse two similarity measures Sα and Sβ .
Since P is a N 2 × N 2 matrix, FD process on TPG as deﬁned in Eq. (7) may be computationally too
demanding. To compute P∗ effectively, instead of diffusing on TPG directly, we show in Section 3.4
that FD process on TPG is equivalent to an iterative process on N × N matrices only. Consequently,
instead of an O(n6 ) time complexity, we obtain an O(n3 ) complexity. Then in Section 4 we further
reduce it to an efﬁcient algorithm with time complexity O(n2 ), which can be used in real time
tracking algorithms.
Iterative Algorithm for Computing P∗
3.4
(k,β ) and
We deﬁne P1 = P(k,α)P T
Pq+1 = Pk,α (Pk,α )q (P T
k,β )q P T
k,β + ∆.

lim
q→∞

(10)

(7)

(8)

(9)

We iterate Eq.(10) until convergence, and as we prove in Proposition 1, we obtain
q→∞ Pq=vec−1 ((I − P)−1 vec(∆))
P∗= lim
The iterative process in Eq.(10) is a generalization of the process introduced in [26]. Consequently,
the following properties are simple extensions of the properties derived in [26]. However, we state
them explicitly, since we combine two different afﬁnity matrices, while [26] considers only a single
matrix. In other words, we consider diffusion on TPG of two different graphs, while diffusion on
TPG of a single graph with itself is considered in [26].
(cid:19)
(cid:18)
q−1(cid:88)
Proposition 1
e=0

Pe vec(∆) = (I − P)−1 vec(∆).

q→∞ P(q+1)
lim

= lim
q→∞

(11)

(12)

vec

4

Proof: Eq.(10) can be rewritten as
P(q+1) = Pk,α (Pk,α )q (P T
k,β )q P T
k,β + ∆
k,β )(q−1) P T
= Pk,α [Pk,α (Pk,α )(q−1) (P T
k,β + ∆]P T
k,β + ∆
= (Pk,α )2 (Pk,α )(q−1) (P T
k,β )(q−1) (P T
k,β )2 + Pk,α ∆ Pk,β + ∆
= · · ·
k,β )q−1 + · · · + ∆
k,β )q + (Pk,α )q−1 ∆ (P T
q−1(cid:88)
k,β (P T
= (Pk,α )q Pk,αP T
(Pk,α )e ∆ (P T
k,β )e
e=0

k,β (P T
= (Pk,α )q Pk,αP T
k,β )q +

(13)

Lemma 1 limq→∞ (Pk,α )q Pk,αP T
k,β (P T
k,β )q = 0
k,β )q go to 0, when q → ∞. This is true if and only
Proof: It sufﬁces to show that (Pk,α )q and (P T
(cid:80)N
b=1 (Pk,α )a,b < 1 and (cid:80)N
if every eigenvalue of Pk,α and Pk,β is less than one in absolute value. Since Pk,α and Pk,β has
nonnegative entries, this holds if its row sums are all less than one. As described in Sec.3.1, we have
j=1 (Pk,β )i,j < 1.
q−1(cid:88)
Lemma 1 shows that the ﬁrst summand in Eq.(13) converges to zero, and consequently we have
q→∞ P(q+1) = lim
(Pk,α )e ∆ (P T
k,β )e .
(cid:16)
k,β )e(cid:17)
lim
q→∞
e=0
(cid:16)
k,β )l(cid:17)
Lemma 2 vec
= (P)e vec(∆) for e = 1, 2, . . . .
(Pk,α )e ∆ (P T
Proof: Our proof is by induction. Suppose (P)l vec(∆)=vec
(P)l+1 vec(∆) = P Pl vec(∆) = vec (cid:0)Pk,α vec−1 (Pl vec(∆)) P T
(Pk,α )l ∆ (P T
then for e = l + 1 we have
= vec (cid:0)Pk,α ((Pk,α )l ∆ (P T
(cid:1)
= vec (cid:0)(Pk,α )l+1 ∆ (P T
k,β )l+1 (cid:1)
k,β
k,β )l ) P T
k,β
and the proof of Lemma 2 is complete.
(cid:33)
(cid:32)q−1(cid:88)
q−1(cid:88)
By Lemma 1 and Lemma 2, we obtain that
e=0
e=0

(cid:1)
is true for e = l,

(Pk,α )e ∆ (P T
k,β )e

(P)e vec(∆).

(14)

(15)

vec

=

The following useful identity holds for the Kronecker Product [22]:
k,α ) = (Pk,α ⊗ Pk,β )vec(∆) = (P)vec(∆)
vec(Pk,β ∆P T
(cid:32)
(cid:33)
(cid:18)
(cid:19)
q−1(cid:88)
Putting together (14), (15), (16), we obtain
q→∞ P(q+1)
lim
lim
q−1(cid:88)
q→∞
e=0
Pe vec(∆) = (I − P)−1 vec(∆)=vec(P∗ ).
e=0

(Pk,α )e ∆ (P T
k,β )e

= lim
q→∞

= vec

vec

(16)

(17)

(18)

This proves Proposition 1.
We now show how FD could improve the original similarity measures. Suppose we have two simi-
larity measures Sα and Sβ . I1 denotes the image patch enclosing the target in frame t−1. According
to Sα , there are many patches in frame t that have nearly equal similarity to I1 with patch In be-
ing most similar to I1 , while according to Sβ , I1 is clearly more similar to Im in frame t. Then
the proposed diffusion will enhance the similarity Sβ (I1 , Im ), since it will propagate faster the Sβ
similarity of I1 to Im than to the other patches. In contrast, the Sα similarities will propagate with
similar speed. Consequently, the ﬁnal joint similarity P∗ will have Im as the most similar to I1 .

5

Algorithm 1: Iterative Fusion with Diffusion Process
Input: Two matrices Pk,α , Pk,β ∈ RN ×N
Output: Diffusion result P∗ ∈ RN ×N
1 Compute P∗ = ∆.
2 Compute uα = ﬁrst column of Pk,α , uβ = ﬁrst column of Pk,β
3 Compute P∗ ← P∗ + uαuT
β .
4 for i = 2, 3, . . . do
Compute uα ← Pk,αuα
5
Compute uβ ← Pk,β uβ
6
Compute P∗ ← P∗ + uαuT
7
β
8 end

4 FD Algorithm
To effectively compute P∗ , we propose an iterative algorithm that takes the advantage of the structure
of matrix ∆. Let uα be a N × 1 vector containing the ﬁrst column of Pk,α . We write Pk,α = [uα |R]
and Pk,α∆ = [uα |0].
It follows then that Pk,α ∆ P T
β . Furthermore, if we denote
k,β = uαuT
β ,j , with uα,j being N × 1, and uT
β ,j being 1 × N , it follows that
(Pk,α )j ∆ (P T
k,β )j = uα,j uT
P j+1
k,β )j+1 = Pk,α (P j
k,α ∆ (P T
k,α ∆ (P T
k,β )j )P T
k,β = Pk,αuα,j uT
β ,j P T
k,β
= (Pk,αuα,j )(Pk,β uβ ,j )T = uα,j+1uT
β ,j+1 .
Hence, we replaced one of the two N × N matrix products with one matrix product between an
N × N matrix and N × 1 vector, and the other with a product of an N × 1 by an 1 × N vector. This
reduces the complexity of our algorithm from O(n3 ) to O(n2 ).
The ﬁnal algorithm is shown in Alg. 1.

1
H

(19)

ωα =

5 Weight Estimation
The weight ωα of measure Sα is proportional to how well Sα is able to distinguish the target I1
in frame It−1 from the background surrounding the target. Let {Bh | h = 1, ..., H } be a set of
background patches surrounding the target I1 in frame It−1 . The weight of Sα is deﬁned as
(cid:80)H
1
h=1 Sα (I1 , Bh )
are normalized so that (cid:80)Q
Thus, the smaller the values of Sα , the larger is the weight ωα . The weights of all similarity measures
α=1 ωα = 1. The weights are computed for every frame in order to
accommodate appearance changes of the tracked object.
6 Experimental Results
We validate our tracking algorithm on eight challenging videos from [4] and [17]: Sylvester, Coke
Can, Tiger1, Cliff Bar, Coupon Book, Surfer, and Tiger2, PETS01D1. We compare our method with
six famous state-of-the-art tracking algorithms including Multiple Instance Learning tracker (MIL)
[4], Fragment tracker(Frag) [1], IVT [19], Online Adaboost tracker (OAB) [10], SemiBoost tracker
(Semi) [11], Mean-Shift (MS) tracker, and a simple weighted linear sum of multiple cues (Linear).
For the comparison methods, we run source code of Semi, Frag, MIL, IVT and OAB supplied by the
authors on the testing videos and use the parameters mentioned in their papers directly. For MS, we
implement it based on OpenCV. For Linear, we use three kinds of image features to get the afﬁnity
and then simply calculate the average afﬁnity and use the diffusion process mentioned in [26]. Note
that all the parameters in our algorithm were ﬁxed for all the experiments.
In our experiments, HOG[9], LBP[18] and Haar-like[4] features are used to represent the image
patches. Hence each pair of patches is compared with three different similarities based on histograms

6

Figure 1: Center Location Error (CLE) versus frame number

of HOG, LBP, and Haar-like feature. For the experimental parameters, we set r = 15 pixels,
H = 300, k = 12 and the iteration number in Alg. 1 is set to 200.
To impartially and comprehensively compare our algorithm with other state-of-the-art trackers, we
used two kinds of quantitative comparisons Average Center Location Error (ACLE) and Precision
Score [4]. The results are shown in Table 1 and Table 2, respectively. Two kinds of curve evaluation
methodologies are also used Center Location Error (CLE) curve and Precision Plots curve1 . The
results are shown in Fig.1 and Fig.2, respectively.

Table 1: Average Center Location Error (ACLE measured in pixels). Red color indicates best
performance, Blue color indicates second best, Green color indicates the third best
Frag3 MIL Linear
Frag2
Frag1
Semi
IVT
MS OAB
Video
Coke Can
43.7
25.0
37.3
40.5
69.1
69.0
34.1
31.9
16.8
Cliff Bar
15.0
14.2
44.8
34.0
34.7
57.2
47.1
34.6
43.8
Tiger 1
23.8
7.6
31.1
26.7
39.7
20.9
50.2
39.8
45.5
Tiger2
6.5
20.6
51.9
38.8
38.6
39.3
98.5
13.2
47.6
Coup. Book
13.6
19.8
67.0
56.1
55.9
65.1
32.2
17.7
20.0
Sylvester
20.0
35.0
96.1
21.0
23.0
12.2
10.1
11.4
10.5
Surfer
6.5
7.7
138.6
139.8
140.1
9.3
19.0
13.4
17.0
PETS01D1
18.1
7.1
241.8
158.9
6.7
7.2
9.5
11.7
245.4

our
15.4
6.1
6.9
5.7
6.5
9.3
5.5
6.0

Table 2: Precision Score (precision at the ﬁxed threshold of 15). Red color indicates best perfor-
mance, Blue color indicates second best, Green color indicates the third best.
Frag3 MIL Linear
Frag2
Frag1
Semi
MS OAB IVT
Video
Coke Can
0.11
0.21
0.15
0.18
0.09
0.09
0.17
0.24
0.36
Cliff Bar
0.52
0.79
0.12
0.23
0.20
0.34
0.19
0.21
0.08
Tiger 1
0.54
0.90
0.38
0.38
0.21
0.52
0.03
0.17
0.05
Tiger 2
0.89
0.66
0.12
0.09
0.09
0.44
0.01
0.65
0.06
Coupon Book
0.53
0.23
0.39
0.39
0.39
0.41
0.21
0.18
0.16
Sylvester
0.46
0.30
0.06
0.53
0.72
0.78
0.81
0.76
0.86
Surfer
1.00
0.93
0.23
0.21
0.19
0.89
0.40
0.61
0.59
PETS01D1
0.38
1.00
0.01
0.29
0.99
0.97
0.95
0.80
0.02

our
0.46
0.95
0.91
0.95
1.00
0.90
1.00
1.00

Comparison to matching based methods: MS, IVT, Frag and Linear are all matching based
tracking algorithms. In MS, famous Bhattacharyya coefﬁcient is used to measure the distance be-
tween histogram distributions; for Frag, we test it under three different measurement strategies: the

1More details about the meaning of Precision Plots can be found in [4]

7

050100150200250300020406080100120140Coke CanFrame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur050100150200250300350050100150Cliff BarFrame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur050100150200250300350020406080100120140160180Coupon BookFrame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0200400600800100012001400050100150200250SylvesterFrame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur050100150200250300350400050100150200250300350400450SurferFrame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur050100150200250300350400020406080100120140Tiger1Frame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur050100150200250300350400020406080100120140160180Tiger2Frame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0501001502002503003504004500100200300400500600PETS01D1Frame #Center Location Error (pixel)  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOurFigure 2: Precision Plots. The threshold is set to 15 in our experiments.

Kolmogorov-Smirnov statistic, EMD, and Chi-Square distance, represented as Frag1, Frag2, Frag3
in Table 1 and Table 2, respectively. For Linear Combination, the average similarity is used and
the diffusion process in [26] is used to improve the similarity measure. Our FD approach clearly
outperforms the other approaches, as shown in Table1 and Table2. Our tracking results achieve the
best performance in all the testing videos, especially for the Precision Plots shown in Table 2. Even
though we set the threshold to 15, which is more challenging for all the trackers, we still get three
1.00 scores. In some videos like sylvester and PETS01D1, Frag achieves comparable results with
our method, but it works badly in other videos which means that speciﬁc distance measure can only
work on some special cases but our fusion framework is robust for all the challenges that appear in
the videos. Our method is always batter than Linear Combination, which means that the fusion with
diffusion can really improve the tracking performance. The stability of our method can be clearly
seen in the plots of location error as the function of frame number in Fig.1. Our tracking results
are always stable, which means that we do not lose the target in the whole tracking process. This is
also reﬂected in the fact that our Precision is always batter than all the other methods under different
thresholds as shown in Fig.2.
Comparison to classiﬁcation based methods: MIL and OAB are both classiﬁcation based tracking
algorithms. For OAB, on-line Adaboost is used to train the classiﬁer for the foreground and back-
ground classiﬁcation. MIL combines multiple instance learning with on-line Adaboost. Haar-like
features are used in both methods. Again our method outperforms those two methods as can be seen
in Table1 and Table 2.
Comparison to semi-supervised learning based methods: SemiBoost combines semi-supervised
learning with on-line Adaboost. Our method is also similar to semi-supervised learning for we build
the graph model on consecutive frames, which means that both of our method and SemiBoost use
the information from the forthcoming frame. Our method is always better than SemiBoost as shown
in Table 1 and Table 2.

7 Conclusions
In this paper, a novel Fusion with Diffusion process is proposed for robust visual tracking. Pairs
of similarity measures are fused into a single similarity measure with a diffusion process on the
tensor product of two graphs determined by the two similarity measures. The proposed method has
time complexity of O(n2 ), which makes it suitable for real time tracking. It is evaluated on sev-
eral challenging videos, and it signiﬁcantly outperforms a large number of state-of-the-art tracking
algorithms.

Acknowledgments
We would like to thank all the authors for releasing their source codes and testing videos, since they
made our experimental evaluation possible. This work was supported by NSF Grants IIS-0812118,
BCS-0924164, OIA-1027897, and by the National Natural Science Foundation of China (NSFC)
Grants 60903096, 61222308 and 61173120.

8

0510152025303540455000.10.20.30.40.50.60.70.80.91Cliff BarThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91Coke CanThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91Coupon BookThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91SylvesterThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91SurferThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91PETS01D1ThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91Tiger1ThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOur0510152025303540455000.10.20.30.40.50.60.70.80.91Tiger2ThresholdPrecision  MSFrag(KS)Frag(EMD)Frag(Chi)IVTLinearOurIn IEEE Computer Society Conference on

IEEE Transactions on Pattern Analysis and Machine Intelligence,

References
[1] A. Adam, E. Rivlin, and I. Shimshoni. Robust fragment-based tracking using the integral histogram. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR), pages 798–805,
2006.
[2] S. Avidan. Support vector tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence,
26(8):1064–1072, 2004.
[3] S. Avidan. Ensemble tracking.
29(2):261–271, 2007.
[4] B. Babenko, M. Yang, and S. Belongie. Robust object tracking with online multiple instance learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1619–1632, 2011.
[5] X. Bai, B. Wang, C. Yao, W. Liu, and Z. Tu. Co-transduction for shape retrieval. IEEE Transactions on
Image Processing, 21(5):2747–2757, 2012.
[6] X. Bai, X. Yang, L. J. Latecki, W. Liu, and Z. Tu. Learning context sensitive shape similarity by graph
transduction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(5):861–874, 2010.
[7] M. Belkin and P. Niyogi. Semi-supervised learning on riemannian manifolds. Machine Learning, 56(spe-
cial Issue on clustering):209–239, 2004.
[8] D. Comaniciu, V. R. Member, and P. Meer. Kernel-based object tracking. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 25(5):564–575, 2003.
[9] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In IEEE Computer Society
Conference on Computer Vision and Pattern Recognition(CVPR), pages 886–893, 2005.
[10] H. Grabner, M. Grabner, and H. Bischof. Real-time tracking via on-line boosting. In British Machine
Vision Conference(BMVC), pages 47–56, 2006.
[11] H. Grabner, C. Leistner, and H. Bischof. Semi-supervised on-line boosting for robust tracking. In Euro-
pean Conference on Computer Vision(ECCV), pages 234–247, 2008.
[12] N. Jiang, W. Liu, and Y. Wu. Learning adaptive metric for robust visual tracking. IEEE Transactions on
Image Processing, 20(8):2288–2300, 2011.
[13] J. Kwon and K. M. Lee. Visual tracking decomposition.
Computer Vision and Pattern Recognition(CVPR), 2010.
[14] J. Lim, D. Ross, R.-S. Lin, and M.-H. Yang. Incremental learning for visual tracking. In Advances in
Neural Information Processing Systems (NIPS), 2005.
[15] R. Liu, J. Cheng, and H. Lu. A robust boosting tracker with minimum error bound in a co-training
framework. In IEEE Interestial Conference on Computer Vision(ICCV), 2009.
[16] X. Mei and H. Ling. Robust visual tracking and vehicle classiﬁcation via sparse representation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 33(11):2259–2272, 2011.
[17] X. Mei, H. Ling, Y. Wu, E. Blasch, and L. Bai. Minimum error bounded efﬁcient l1 tracker with occlusion
detection. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2011.
[18] T. Ojala, M. Pietik ¨ainen, and T. M ¨aenp ¨a ¨a. Multiresolution gray-scale and rotation invariant texture clas-
siﬁcation with local binary patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence,
24(7):971–987, 2002.
[19] D. Ross, J. Kim, R.-S. Lin, and M.-H. Yang. Incremental learning for robust visual tracking. International
Journal of Computer Vision, 77(1):125–141, 2008.
[20] J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof. Prost: Parallel robust online simple tracking.
In IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR), 2010.
In Advances in
[21] K. Sinha and M.Belkin. Semi-supervised learning using sparse eigenfunction bases.
Neural Information Processing Systems(NIPS), 2009.
[22] S. Vishwanathan, N. Schraudolph, R. Kondor, and K. Borgwardt. Graph kernels. Journal of Machine
Learning Research, 11(4):1201–1242, 2010.
[23] B. Wang, J. Jiang, W. Wang, Z.-H. Zhou, and Z. Tu. Unsupervised metric fusion by cross diffusion. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR), 2012.
In Internal Conference on Machine Learn-
[24] W. Wang and Z. Zhou. A new analysis of co-training.
ing(ICML), 2010.
[25] Y. Wu and J. Fan. Contextual ﬂow.
Pattern Recognition(CVPR), 2009.
[26] X. Yang and L. J. Latecki. Afﬁnity learning on a tensor product graph with applications to shape and image
retrieval. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR),
2011.
[27] W. Zhong, H. Lu, and M.-H. Yang. Robust object tracking via sparsity-based collaborative model. In
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
[28] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency.
In Advances in Neural Information Processing Systems (NIPS), 2004.
[29] X. Zhu. Semi-supervised learning literature survey. In Technical Report 1530, Department of Computer
Sciences, University of Wisconsin, Madison, 2005.

In IEEE Computer Society Conference on Computer Vision and

9


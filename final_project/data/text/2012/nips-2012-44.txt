Optimal Regularized Dual Averaging Methods for
Stochastic Optimization

Xi Chen
Machine Learning Department
Carnegie Mellon University
xichen@cs.cmu.edu

Javier Pe ˜na
Qihang Lin
Tepper School of Business
Carnegie Mellon University
{qihangl,jfp}@andrew.cmu.edu

Abstract

This paper considers a wide spectrum of regularized stochastic optimization prob-
lems where both the loss function and regularizer can be non-smooth. We develop
a novel algorithm based on the regularized dual averaging (RDA) method, that
can simultaneously achieve the optimal convergence rates for both convex and
strongly convex loss. In particular, for strongly convex loss, it achieves the opti-
N 2 ) for N iterations, which improves the rate O( log N
mal rate of O( 1
N ) for pre-
N + 1
vious regularized dual averaging algorithms. In addition, our method constructs
the ﬁnal solution directly from the proximal mapping instead of averaging of all
previous iterates. For widely used sparsity-inducing regularizers (e.g., (cid:96)1 -norm),
it has the advantage of encouraging sparser solutions. We further develop a multi-
stage extension using the proposed algorithm as a subroutine, which achieves the
N + exp{−N }) for strongly convex loss.
uniformly-optimal rate O( 1

1

Introduction

Many risk minimization problems in machine learning can be formulated into a regularized stochas-
tic optimization problem of the following form:
minx∈X {φ(x) := f (x) + h(x)}.
(1)
Here, the set of feasible solutions X is a convex set in Rn , which is endowed with a norm (cid:107) · (cid:107) and
function f (x) takes the form: f (x) := Eξ (F (x, ξ )) = (cid:82) F (x, ξ )dP (ξ ), where ξ is a random vector
the dual norm (cid:107) · (cid:107)∗ . The regularizer h(x) is assumed to be convex, but could be non-differentiable.
Popular examples of h(x) include (cid:96)1 -norm and related sparsity-inducing regularizers. The loss
with the distribution P . In typical regression or classiﬁcation tasks, ξ is the input and response (or
class label) pair. We assume that for every random vector ξ , F (x, ξ ) is a convex and continuous
constants L ≥ 0, M ≥ 0 and (cid:101)µ ≥ 0 such that
function in x ∈ X . Therefore, f (x) is also convex. Furthermore, we assume that there exist
(cid:101)µ
∀x, y ∈ X ,
(cid:107)x − y(cid:107)2 + M (cid:107)x − y(cid:107),
(cid:107)x − y(cid:107)2 ≤ f (y) − f (x) − (cid:104)y − x, f (cid:48) (x)(cid:105) ≤ L
(2)
2
2
where f (cid:48) (x) ∈ ∂ f (x), the subdifferential of f . We note that this assumption allows us to adopt a
wide class of loss functions. For example, if f (x) is smooth and its gradient f (cid:48) (x) = ∇f (x) is
smooth but Lipschitz continuous, we have L = 0 and M > 0 (e.g., hinge loss). If (cid:101)µ > 0, f (x) is
Lipschitz continuous, we have L > 0 and M = 0 (e.g., squared or logistic loss). If f (x) is non-
strongly convex and (cid:101)µ is the so-called strong convexity parameter.
In general, the optimization problem in Eq.(1) is challenging since the integration in f (x) is compu-
tationally intractable for high-dimensional P . In many learning problems, we do not even know the
underlying distribution P but can only generate i.i.d. samples ξ from P . A traditional approach is to

1

(cid:80)m
consider empirical loss minimization problem where the expectation in f (x) is replaced by its em-
pirical average on a set of training samples {ξ1 , . . . , ξm }: femp (x) := 1
i=1 F (x, ξi ). However,
m
for modern data-intensive applications, minimization of empirical loss with an off-line optimization
solver could suffer from very poor scalability.
In the past few years, many stochastic (sub)gradient methods [6, 5, 8, 12, 14, 10, 9, 11, 7, 18] have
been developed to directly solve the stochastic optimization problem in Eq.(1), which enjoy low per-
iteration complexity and the capability of scaling up to very large data sets. In particular, at the t-th
iteration with the current iterate xt , these methods randomly draw a sample ξt from P ; then com-
pute the so-called “stochastic subgradient” G(xt , ξt ) ∈ ∂xF (xt , ξt ) where ∂xF (xt , ξt ) denotes the
subdifferential of F (x, ξt ) with respect to x at xt ; and update xt using G(xt , ξt ). These algorithms
fall into the class of stochastic approximation methods. Recently, Xiao [21] proposed the regu-
larized dual averaging (RDA) method and its accelerated version (AC-RDA) based on Nesterov’s
primal-dual method [17]. Instead of only utilizing a single stochastic subgradient G(xt , ξt ) of the
current iteration, it updates the parameter vector using the average of all past stochastic subgradients
{G(xi , ξi )}t
i=1 and hence leads to improved empirical performances.
which achieves the optimal expected convergence rate of E[φ((cid:98)x) − φ(x∗ )], where (cid:98)x is the solution
In this paper, we propose a novel regularized dual averaging method, called optimal RDA or ORDA,
from ORDA and x∗ is the optimal solution of Eq.(1). As compared to previous dual averaging
methods, it has three main advantages:
(cid:16) σ2+M 2
(cid:17) ≈
1. For strongly convex f (x), ORDA improves the convergence rate of stochastic dual aver-
(cid:16) 1(cid:101)µN
(cid:17)
aging methods O( σ2 log N(cid:101)µN ) ≈ O( log N(cid:101)µN ) [17, 21] to an optimal rate O
(cid:101)µN + L
N 2
tions, and the parameters (cid:101)µ, M and L of f (x) are deﬁned in Eq.(2).
, where σ2 is the variance of the stochastic subgradient, N is the number of itera-
O
vex f (x) with the strong convexity parameter (cid:101)µ as an input. When (cid:101)µ = 0, ORDA reduces
2. ORDA is a self-adaptive and optimal algorithm for solving both convex and strongly con-
to a variant of AC-RDA in [21] with the optimal rate for solving convex f (x). Further-
of f (x). For strongly convex f (x) with (cid:101)µ > 0, our algorithm achieves the optimal rate of
(cid:17)
(cid:16) 1(cid:101)µN
more, our analysis allows f (x) to be non-smooth while AC-RDA requires the smoothness
while AC-RDA does not utilize the advantage of strong convexity.
can only show the convergence rate for the averaged iterates: ¯xN = (cid:80)N
t=1 txt/ (cid:80)N
3. Existing RDA methods [21] and many other stochastic gradient methods (e.g., [14, 10])
t=1 t ,
where the {t} are nonnegative weights. However, in general, the average iterates ¯xN
cannot keep the structure that the regularizer tends to enforce (e.g., sparsity, low-rank,
etc). For example, when h(x) is a sparsity-inducing regularizer ((cid:96)1 -norm), although xt
computed from proximal mapping will be sparse as t goes large, the averaged solution
could be non-sparse. In contrast, our method directly generates the ﬁnal solution from the
proximal mapping, which leads to sparser solutions.

In addition to the rate of convergence, we also provide high probability bounds on the error of
objective values. Utilizing a technical lemma from [3], we could show the same high probability
(cid:101)µN + exp{−(cid:112)(cid:101)µ/LN }(cid:17)
(cid:16) σ2+M 2
bound as in RDA [21] but under a weaker assumption.
Furthermore, using ORDA as a subroutine, we develop the multi-stage ORDA which obtains the
(cid:17)
(cid:16) σ2+M 2
for strongly convex f (x). Recall that ORDA
convergence rate of O
(cid:16)
exp{−(cid:112)(cid:101)µ/LN }(cid:17)
the second term in the rate of ORDA from O (cid:0) L
(cid:1) to O
(cid:101)µN + L
for strongly convex f (x). The rate of muli-stage ORDA improves
has the rate O
N 2
and achieves the so-
N 2
called “uniformly-optimal ” rate [15]. Although the improvement is on the non-dominating term,
multi-stage ORDA is an optimal algorithm for both stochastic and deterministic optimization. In
particular, for deterministic strongly convex and smooth f (x) (M = 0), one can use the same al-
gorithm but only replaces the stochastic subgradient G(x, ξ ) by the deterministic gradient ∇f (x).
(cid:101)µN
Then, the variance of the stochastic subgradient σ = 0. Now the term σ2+M 2
in the rate equals
to 0 and multi-stage ORDA becomes an optimal deterministic solver with the exponential rate

2

Algorithm 1 Optimal Regularized Dual Averaging Method: ORDA(x0 , N , Γ, c)
Parameters for f (x): Constants L, M and (cid:101)µ for f (x) in Eq. (2) and set µ = (cid:101)µ/τ .
Input Parameters: Starting point x0 ∈ X , the number of iterations N , constants Γ ≥ L and c ≥ 0.
Initialization: Set θt = 2
t+2 ; νt = 2
t+1 ; γt = c(t + 1)3/2 + τ Γ; z0 = x0 .
Iterate for t = 0, 1, 2, . . . , N :
t )µ xt + (1−θt )θt µ+θ3
1. yt = (1−θt )(µ+θ2
t γt
t γt )
t )µ zt
t γt+(1−θ2
t γt+(1−θ2
(cid:16)(cid:80)t
(cid:17)
θ2
θ2
2. Sample ξt from the distribution P (ξ ) and compute the stochastic subgradient G(yt , ξt ).
(cid:16)(cid:80)t
(cid:111)
(cid:17)
(cid:110)(cid:104)x, gt (cid:105) + h(x) + θt νt
G(yi ,ξi )
3. gt = θt νt
i=0
νi
(cid:110)(cid:104)x, G(yt , ξt )(cid:105) + h(x) +
(cid:16) µ
(cid:111)
4. zt+1 = arg minx∈X
+ θt νtγt+1V (x, x0 )
i=0
V (x, yt )
τ θ2
t

5. xt+1 = arg minx∈X
Output: xN +1
exp{−(cid:112)(cid:101)µ/LN }(cid:17)
(cid:16)
. This is the reason why such a rate is “uniformly-optimal”, i.e., optimal
O
with respect to both stochastic and deterministic optimization.
2 Preliminary and Notations

(cid:17)
µV (x,yi )
νi

+ γt
τ

In the framework of ﬁrst-order stochastic optimization, the only available information of f (x) is the
stochastic subgradient. Formally speaking, stochastic subgradient of f (x) at x, G(x, ξ ), is a vector-
valued function such that EξG(x, ξ ) = f (cid:48) (x) ∈ ∂ f (x). Following the existing literature, a standard
assumption on G(x, ξ ) is made throughout the paper : there exists a constant σ such that ∀x ∈ X ,
Eξ ((cid:107)G(x, ξ ) − f (cid:48) (x)(cid:107)2∗ ) ≤ σ2 .
(3)
A key updating step in dual averaging methods, the proximal mapping, utilizes the Bregman diver-
gence. Let ω(x) : X → R be a strongly convex and differentiable function, the Bregman divergence
associated with ω(x) is deﬁned as:
V (x, y) := ω(x) − ω(y) − (cid:104)∇ω(y), x − y(cid:105).
(4)
2 (cid:107)x(cid:107)2
2 (cid:107)x − y(cid:107)2
One typical and simple example is ω(x) = 1
2 together with V (x, y) = 1
2 . One may
refer to [21] for more examples. We can always scale ω(x) so that V (x, y) ≥ 1
2 (cid:107)x − y(cid:107)2 for all
x, y ∈ X . Following the assumption in [10]: we assume that V (x, y) grows quadratically with the
2 (cid:107)x − y(cid:107)2 with τ > 1 for all x, y ∈ X . In fact, we could simply
parameter τ > 1, i.e., V (x, y) ≤ τ
choose ω(x) with a τ -Lipschitz continuous gradient so that the quadratic growth assumption will be
automatically satisﬁed.

3 Optimal Regularized Dual Averaging Method
(cid:110)(cid:104)gt , x(cid:105) + h(x) + βt
(cid:111)
(cid:80)t
In dual averaging methods [17, 21], the key proximal mapping step utilizes the average of all past
In particular, it takes the form: zt+1 =
stochastic subgradients to update the parameter vector.
i=0 G(zi , ξi ).
, where βt is the step-size and gt = 1
arg minx∈X
t V (x, x0 )
t+1
For strongly convex f (x), the current dual averaging methods achieve a rate of O( σ2 log N(cid:101)µN ), which
strongly and non-strongly convex f (x) via the strong convexity parameter (cid:101)µ and achieves optimal
is suboptimal. In this section, we propose a new dual averaging algorithm which adapts to both
(cid:80)N
the ﬁnal solution takes the form: (cid:98)x = 1
rates in both cases. In addition, for previous dual averaging methods, to guarantee the convergence,
t=0 zt and hence is not sparse in nature for sparsity-
N +1
inducing regularizers. Instead of taking the average, we introduce another proximal mapping and
generate the ﬁnal solution directly from the second proximal mapping. This strategy will provide us
sparser solutions in practice. It is worthy to note that in RDA, zN has been proved to achieve the de-
sirable sparsity pattern (i.e., manifold identiﬁcation property) [13]. However, according to [13], the

3

convergence of φ(zN ) to the optimal φ(x∗ ) is established only under a more restrictive assumption
that x∗ is a strong local minimizer of φ relative to the optimal manifold and the convergence rate is
quite slow. Without this assumption, the convergence of φ(zN ) is still unknown.
we deﬁne the parameter µ = (cid:101)µ/τ , which scales the strong convexity parameter (cid:101)µ by 1
The proposed optimal RDA (ORDA) method is presented in Algorithm 1. To simplify our notations,
τ , where τ is
the quadratic growth constant. In general, the constant Γ which deﬁnes the step-size parameter γt
is set to L. However, we allow Γ to be an arbitrary constant greater than or equal to L to facilitate
optimal rates for both convex and strongly convex loss. When µ > 0 (or equivalently, (cid:101)µ > 0), c is
the introduction of the multi-stage ORDA in the later section. The parameter c is set to achieve the
√
√
set to 0 so that γt ≡ τ Γ ≥ τ L; while for µ = 0, c =
. Since x∗ is unknown in practice,
τ (σ+M )
V (x∗ ,x0 )
2
one might replace V (x∗ , x0 ) in c by a tuning parameter.
Here, we make a few more explanations of Algorithm 1. In Step 1, the intermediate point yt is
a convex combination of xt and zt and when µ = 0, yt = (1 − θt )xt + θt zt . The choice of the
(cid:80)t
combination weights is inspired by [10]. Second, with our choice of θt and νt , it is easy to prove that
. Therefore, gt in Step 3 is a convex combination of {G(yi , ξi )}t
i=0 . As compared
= 1
1
i=0
νi
θt νt
to RDA which uses the average of past subgradients, gt in ORDA is a weighted average of all
(cid:17)
(cid:16) gt−1
past stochastic subgradients and the subgradient from the larger iteration has a larger weight (i.e.,
2(i+1)
G(yi , ξi ) has the weight
(t+1)(t+2) ). In practice, instead of storing all past stochastic subgradients,
+ G(yt ,ξt )
gt could be simply updated based on gt−1 : gt = θt νt
. We also note that
θt−1 νt−1
νt
since the error in the stochastic subgradient G(yt , ξt ) will affect the sparsity of xt+1 via the second
proximal mapping, to obtain stable sparsity recovery performances, it would be better to construct
the stochastic subgradient with a small batch of samples [21, 1]. This could help to reduce the noise
of the stochastic subgradient.

3.1 Convergence Rate

We present the convergence rate for ORDA. We start by presenting a general theorem without plug-
ging the values of the parameters. To simplify our notations, we deﬁne ∆t := G(yt , ξt ) − f (cid:48) (yt ).
Theorem 1 For ORDA, if we require c > 0 when (cid:101)µ = 0, then for any t ≥ 0:
t(cid:88)
t(cid:88)
(cid:104)x∗ − (cid:98)zi , ∆i (cid:105)
(cid:16) µ
(cid:17)
((cid:107)∆i (cid:107)∗ + M )2
) ≤ θt νt γt+1V (x
φ(xt+1 ) − φ(x
∗
∗
θt νt
, (5)
+ θt νt
, x0 ) +
τ − θiL
2
νi
+ θi γi
νi
zt , is a convex combination of yt and zt ; and (cid:98)zt = zt when
where (cid:98)zt = θt µ
i=0
i=0
τ θi
yt + (1−θt )µ+γt θ2
t
µ+γt θ2
µ+γt θ2
t
t
t(cid:88)
µ = 0. Taking the expectation on both sides of Eq.(5):
(cid:16) µ
(cid:17)
Eφ(xt+1 ) − φ(x
) ≤ θt νt γt+1V (x
1
, x0 ) + (σ2 + M 2 )θt νt
τ − θiL
+ θi γi
i=0
τ θi
convergence in expectation for ORDA by choosing different values for c based on (cid:101)µ.
The proof of Theorem 1 is given in Appendix. In the next two corollaries, we establish the rates of
Corollary 1 For convex f (x) with (cid:101)µ = 0 , by setting c =
√
√
8(σ + M )(cid:112)τ V (x∗ , x0 )
τ (σ+M )
and Γ = L, we obtain:
V (x∗ ,x0 )
2
Eφ(xN +1 ) − φ(x∗ ) ≤ 4τ LV (x∗ , x0 )
√
+
N 2
N

(7)

(6)

.

νi

∗

∗

.

Based on Eq.(6), the proof of Corollary 1 is straightforward with the details in Appendix. Since x∗
By doing so, Eq.(7) remains valid after replacing all V (x∗ , x0 ) by D∗ . For convex f (x) with (cid:101)µ = 0,
is unknown in practice, one could set c by replacing V (x∗ , x0 ) in c with any value D∗ ≥ V (x∗ , x0 ).
the rate in Eq.(7) has achieved the uniformly-optimal rate according to [15]. In fact, if f (x) is a
deterministic and smooth function with σ = M = 0 (e.g., smooth empirical loss), one only needs

4

to change the stochastic subgradient G(yt , ξt ) to ∇f (yt ). The resulting algorithm, which reduces to
Algorithm 3 in [20], is an optimal deterministic ﬁrst-order method with the rate O( LV (x∗ ,x0 )
).
N 2
(cid:16) µ
(cid:110)(cid:104)x, G(yt , ξt )(cid:105) + h(x) +
(cid:17) (cid:107)x − yt(cid:107)2(cid:111)
We note that the quadratic growth assumption of V (x, y) is not necessary for convex f (x).
If one does not assume this assumption and replaces the last step in ORDA by xt+1 =
+ γt
arg minx∈X
, we can achieve the same rate as in
2θ2
2
t
Eq.(7) but just removing all τ from the right hand side. But the quadratic growth assumption is
Corollary 2 For strongly convex f (x) with (cid:101)µ > 0, we set c = 0 and Γ = L and obtain that:
indeed required for showing the convergence for strongly convex f (x) as in the next corollary.
Eφ(xN +1 ) − φ(x∗ ) ≤ 4τ LV (x∗ , x0 )
4τ (σ2 + M 2 )
(cid:17)
(cid:16) log N
(cid:17)
(cid:16) 1
+
.
N 2
µN
µN + exp(−(cid:112) µ
rate for previous
, is optimal and better than the O
The dominating term in Eq.(8), O
µN
µN
dual averaging methods. However, ORDA has not achieved the uniformly-optimal rate, which takes
optimal deterministic rate should be O (cid:0)exp(−(cid:112) µ
L N )(cid:1) [16]. Inspired by the multi-restart technique
the form of O( σ2+M 2
L N )). In particular, for deterministic smooth and strongly convex
f (x) (i.e., empirical loss with σ = M = 0), ORDA only achieves the rate of O( L
N 2 ) while the
in [7, 11], we present a multi-stage extension of ORDA in Section 4 which achieves the uniformly-
optimal convergence rate.

(8)

3.2 High Probability Bounds

For stochastic optimization problems, another important evaluation criterion is the conﬁdence
level of the objective value.
In particular, it is of great interest to ﬁnd (N , δ) as a mono-
tonically decreasing function in both N and δ ∈ (0, 1) such that the solution xN +1 satisﬁes
Pr (φ(xN +1 ) − φ(x∗ ) ≥ (N , δ)) ≤ δ .
In other words, we want to show that with probability
at least 1 − δ , φ(xN +1 ) − φ(x∗ ) < (N , δ). According to Markov inequality, for any  > 0,
Eφ(xN +1 )−φ(x∗ )
Pr(φ(xN +1 ) − φ(x∗ ) ≥ ) ≤ E(φ(xN +1 )−φ(x∗ ))
(cid:16) σ2+M 2
(cid:16) (σ+M )
(cid:17)
(cid:17)
. Therefore, we have (N , δ) =
.
Under the basic assumption in Eq.(3), namely Eξ ((cid:107)G(x, ξ ) − f (cid:48) (x)(cid:107)2∗ ) ≤ σ2 , and according to

δ
√
V (x∗ ,x0 )
√
Corollary 1 and 2, (N , δ) = O
for convex f (x), and (N , δ) = O
µN δ
N δ
for strongly convex f (x).
we assume that E (cid:0)exp (cid:8)(cid:107)G(x, ξ ) − f (cid:48) (x)(cid:107)2∗ /σ2(cid:9)(cid:1) ≤ exp{1}, ∀x ∈ X . By further making the
However, the above bounds are quite loose. To obtain tighter bounds, we strengthen the basic
assumption of the stochastic subgradient in Eq. (3) to the “light-tail” assumption [14]. In particular,
boundedness assumption ((cid:107)x∗ − (cid:98)zt(cid:107) ≤ D) and utilizing a technical lemma from [3], we obtain a
(cid:16) √
(cid:17)
√
ln(1/δ)Dσ
much tighter high probability bound with (N , δ) = O
for both convex and strongly
N
convex f (x). The details are presented in Appendix.

4 Multi-stage ORDA for Stochastic Strongly Convex Optimization

As we show in Section 3.1, for convex f (x), ORDA achieves the uniformly-optimal rate. How-
ever, for strongly convex f (x), although the dominating term of the convergence rate in Eq.(8) is
optimal, the overall rate is not uniformly-optimal. Inspired by the multi-stage stochastic approx-
imation methods [7, 9, 11], we propose the multi-stage extension of ORDA in Algorithm 2 for
stochastic strongly convex optimization. For each stage 1 ≤ k ≤ K , we run ORDA in Algorithm
1 as a sub-routine for Nk iterations with the parameter γt = c(t + 1)3/2 + τ Γ with c = 0 and
Γ = Λk + L. Roughly speaking, we set Nk = 2Nk−1 and Λk = 4Λk−1 . In other words, we double
the number of iterations for the next stage but reduce the step-size. The multi-stage ORDA has
achieved uniformly-optimal convergence rate as shown in Theorem 2 with the proof in Appendix.
The proof technique follows the one in [11]. Due this specialized proof technique, instead of show-
ing E(φ(xN )) − φ(x∗ ) ≤ (N ) as in ORDA, we show the number of iterations N () to achieve the
-accurate solution: E(φ(xN () )) − φ(x∗ ) ≤ . But the two convergence rates are equivalent.

5

Algorithm 2 Multi-stage ORDA for Stochastic Strongly Convex Optimization
Initialization: x0 ∈ X , a constant V0 ≥ φ(x0 ) − φ(x∗ ) and the number of stages K .
(cid:110)
(cid:111)
(cid:113) τ L
Iterate for k = 1, 2, . . . , K :
(cid:113) 2k−1 µ(σ2+M 2 )
µ , 2k+9 τ (σ2+M 2 )
1. Set Nk = max
4
µV0
3. Generate (cid:101)xk by calling the sub-routine ORDA((cid:101)xk−1 , Nk , Γ = Λk + L, c = 0)
2. Set Λk = N 3/2
τ V0
k
Output: (cid:101)xK
(cid:0) V0
(cid:1) for any given , we have
E(φ((cid:101)xK )) − φ(x∗ ) ≤  and the total number of iterations is upper bounded by:
Theorem 2 If we run multi-stage ORDA for K stages with K = log2
(cid:115)
(cid:18) V0
(cid:19)

K(cid:88)
1024τ (σ2 + M 2 )
Nk ≤ 4
τ L
.
+
log2
N =
µ

µ
k=1

(9)

5 Related Works

In the last few years, a number of stochastic gradient methods [6, 5, 8, 12, 14, 21, 10, 11, 7, 4, 3] have
been developed to solve Eq.(1), especially for a sparsity-inducing h(x). In Table 1, we compare the
proposed ORDA and its multi-stage extension with some widely used stochastic gradient methods
using the following metrics. For the ease of comparison, we assume f (x) is smooth with M = 0.
1. The convergence rate for solving (non-strongly) convex f (x) and whether this rate has
(cid:17)
(cid:16) σ2(cid:101)µN
achieved the uniformly-optimal (Uni-opt) rate.
2. The convergence rate for solving strongly convex f (x) and whether (1) the dominating
3. Whether the ﬁnal solution (cid:98)x, on which the results of convergence are built, is generated
and (2) the overall rate is uniformly-optimal.
term of rate is optimal, i.e., O
from the weighted average of previous iterates (Avg) or from the proximal mapping (Prox).
For sparsity-inducing regularizers, the solution directly from the proximal mapping is often
sparser than the averaged solution.
4. Whether an algorithm allows to use a general Bregman divergence in proximal mapping or
2 (cid:107)x − y(cid:107)2
it only allows the Euclidean distance V (x, y) = 1
2 .
In Table 1, the algorithms in the ﬁrst 7 rows are stochastic approximation algorithms where only
the current stochastic gradient is used at each iteration. The last 4 rows are dual averaging methods
where all past subgradients are used. Some algorithms in Table 1 make a more restrictive assumption
on the stochastic gradient: ∃G > 0, E(cid:107)G(x, ξ )(cid:107)2∗ ≤ G2 , ∀x ∈ X . It is easy to verify that this
assumption implies our basic assumption in Eq.(3) by Jensen’s inequality.
As we can see from Table 1, the proposed ORDA possesses all good properties except that the
(cid:1) ≤ D for all t ≥ 0
since the parameter D in the convergence rate is chosen such that E (cid:0)(cid:107)xt − x∗ (cid:107)2
convergence rate for strongly convex f (x) is not uniformly-optimal. Multi-stage ORDA further
improves this rate to be uniformly-optimal. In particular, SAGE [8] achieves a nearly optimal rate
and it could be much larger than V ≡ V (x∗ , x0 ). In addition, SAGE requires the boundedness of the
2
domain X , the smoothness of f (x), and only allows the Euclidean distance in proximal mapping.
As compared to AC-SA [10] and multi-stage AC-SA [11], our methods do not require the ﬁnal
averaging step; and as shown in our experiments, ORDA has better empirical performances due
to the usage of all past stochastic subgradients. Furthermore, we improve the rates of RDA and
extend AC-RDA to an optimal algorithm for both convex and strongly convex f (x). Another highly
relevant work is [9]. Juditsky et al. [9] proposed multi-stage algorithms to achieve the optimal
strongly convex rate based on non-accelerated dual averaging methods. However, the algorithms in
[9] assume that φ(x) is a Lipschitz continuous function, i.e., the subgradient of φ(x) is bounded.
Therefore, when the domain X is unbounded, the algorithms in [9] cannot be directly applied.

6

Final (cid:98)x Bregman
Prox NO

YES

SAGE [8]

O

O

YES NO

YES YES

NO

O

(cid:17)
(cid:17)

(cid:17)
(cid:17)

RDA [21]

O

NO NO

YES NO

O

O

O

O

NA

NA

NO

FOBOS [6]

Prox NO

COMID [5]

NEARLY O

Prox YES

AC-SA [10] O

(cid:16) G
Rate
(cid:16) G
√
V√
O
(cid:16) σ
√
N
V√
(cid:16) σ
√
N
D√
√
N
V√
N
M-AC-SA [11] NA
(cid:16) G
Epoch-GD [7] NA
(cid:16) σ
√
V√
(cid:16) σ
√
N
V√
√
N
V√
N
NA

(cid:17)
(cid:16) G2 log N(cid:101)µN
(cid:17)
Convex f (x)
Strongly Convex f (x)
Opt Uni-opt
Uni-opt Rate
(cid:16) G2 log N(cid:101)µN
(cid:17)
(cid:17)
NO
NO NO
O
(cid:16) σ2(cid:101)µN + LD
(cid:17)
(cid:17)
(cid:16) σ2(cid:101)µN + LV
+ LD
(cid:18)
σ2(cid:101)µN + exp{−(cid:113) (cid:101)µ
N 2
N 2
+ LV
N 2
N 2
(cid:17)
(cid:16) G2(cid:101)µN
L N }
(cid:16) G2 log N(cid:101)µN
(cid:17)
(cid:17)
(cid:17)
(cid:16) σ2(cid:101)µN + LV
NA
+ LV
(cid:18)
σ2(cid:101)µN + exp{−(cid:113) (cid:101)µ
N 2
+ LV
N 2
N 2
L N }
Table 1: Summary for different stochastic gradient algorithms. V is short for V (x∗ , x0 ); AC for “accelerated”;
M for “multi-stage” and NA stands for either “not applicable” or “no analysis of the rate”.
Recently, the paper [18] develops another stochastic gradient method which achieves the rate O( G2(cid:101)µN )
for strongly convex f (x). However, for non-smooth f (x), it requires the averaging of the last a few
iterates and this rate is not uniformly-optimal.

AC-RDA [21] O
ORDA

O

Prox YES

Prox YES

Avg

Avg

Avg

YES

YES

YES

YES

Avg

Avg

YES

YES

NO

YES NO

NO NO

NA NA

YES NO

(cid:19)

(cid:19)

O

O

NA

M-ORDA

YES YES

6 Simulated Experiments

In this section, we conduct simulated experiments to demonstrate the performance of ORDA and
its multi-stage extension (M ORDA). We compare our ORDA and M ORDA (only for strongly
convex loss) with several state-of-the-art stochastic gradient methods, including RDA and AC-RDA
[21], AC-SA [10], FOBOS [6] and SAGE [8]. For a fair comparison, we compare all different
methods using solutions which have expected convergence guarantees. For all algorithms, we tune
the parameter related to step-size (e.g., c in ORDA for convex loss) within an appropriate range and
choose the one that leads to the minimum objective value.
In this experiment, we solve a sparse linear regression problem: minx∈Rn f (x)+h(x) where f (x) =
Ea,b ((aT x − b)2 ) + ρ
2 (cid:107)x(cid:107)2
2 and h(x) = λ(cid:107)x(cid:107)1 . The input vector a is generated from N (0, In×n )
1
i = 1 for 1 ≤ i ≤ n/2 and 0 otherwise and the noise
2
and the response b = aT x∗ + , where x∗
 ∼ N (0, 1). When ρ = 0, th problem is the well known Lasso [19] and when ρ > 0, it is known
as Elastic-net [22]. The regularization parameter λ is tuned so that a deterministic solver on all the
samples can correctly recover the underlying sparsity pattern. We set n = 100 and create a large
pool of samples for generating stochastic gradients and evaluating objective values. The number
of iterations N is set to 500. Since we focus on stochastic optimization instead of online learning,
we could randomly draw samples from an underlying distribution. So we construct the stochastic
precision+recall where precision = (cid:80)p
i =1} / (cid:80)p
gradient using the mini-batch strategy [2, 1] with the batch size 50. We run each algorithm for 100
recall = (cid:80)p
i =1} / (cid:80)p
times and report the mean of the objective value and the F1-score for sparsity recovery performance.
i=1 1{(cid:98)xi=1,x∗
i=1 1{(cid:98)xi=1} and
F1-score is deﬁned as 2 precision·recall
i=1 1{(cid:98)xi=1,x∗
i =1} . The higher the F1-score is, the better the recovery
i=1 1{x∗
ability of the sparsity pattern. The standard deviations for both objective value and the F1-score in
100 runs are very small and thus omitted here due to space limitations.
We ﬁrst set ρ = 0 to test algorithms for (non-strongly) convex f (x). The result is presented in
Table 2 (the ﬁrst two columns). We also plot the decrease of the objective values for the ﬁrst 200
iterations in Figure 1. From Table 2, ORDA performs the best in both objective value and recovery
ability of sparsity pattern. For those optimal algorithms (e.g., AC-RDA, AC-SA, SAGE, ORDA),
they achieve lower ﬁnal objective values and the rates of the decrease are also faster. We note that
for dual averaging methods, the solution generated from the (ﬁrst) proximal mapping (e.g., zt in

7

ρ = 1
ρ = 0
Obj
Obj
F1
F1
RDA
20.87 0.67 21.57 0.67
AC-RDA 20.67 0.67 21.12 0.67
20.66 0.67 21.01 0.67
AC-SA
20.98 0.83 21.19 0.84
FOBOS
20.65 0.82 21.09 0.73
SAGE
20.56 0.92 20.97 0.87
ORDA
M ORDA N.A. N.A. 20.98 0.88

Table 2: Comparisons in objective
value and F1-score.

Figure 1: Obj for Lasso.

Figure 2: Obj for Elastic-Net.

ORDA) has almost perfect sparsity recovery performance. However, since here is no convergence
guarantee for that solution, we do not report results here.
Then we set ρ = 1 to test algorithms for solving
strongly convex f (x). The results are presented
in Table 2 (the last two columns) and Figure
2 and 3. As we can see from Table 2, ORDA
and M ORDA perform the best. Although
M ORDA achieves the theoretical uniformly-
optimal convergence rate, the empirical per-
formance of M ORDA is almost identical to
that of ORDA. This observation is consistent
with our theoretical analysis since the improve-
ment of the convergence rate only appears on
the non-dominating term. In addition, ORDA,
M ORDA, AC-SA and SAGE with the conver-
gence rate O( 1(cid:101)µN ) achieve lower objective val-
ues as compared to other algorithms with the
rate O( log N(cid:101)µN ) . For better visualization, we do
not include the comparison between M ORA and ORDA in Figure 2. Instead, we present the com-
parison separately in Figure 3. From Figure 3, the ﬁnal objective values of both algorithms are very
close. An interesting observation is that, for M ORDA, each time when a new stage starts, it leads
to a sharp increase in the objective value following by a quick drop.

Figure 3: ORDA v.s. M ORDA.

7 Conclusions and Future Works

In this paper, we propose a new dual averaging method which achieves the optimal rates for solving
stochastic regularized problems with both convex and strongly convex loss functions. We further
propose a multi-stage extension to achieve the uniformly-optimal convergence rate for strongly con-
vex loss.
Although we study stochastic optimization problems in this paper, our algorithms can be easily
(cid:0)(F (xt , ξt ) + h(xt )) − (F (x∗ , ξt ) + h(x∗ ))(cid:1). Given
converted into online optimization approaches, where a sequence of decisions {xt}N
so-called regret, deﬁned as RN (x∗ ) = (cid:80)N
t=1 are generated
according to Algorithm 1 or 2. We often measure the quality of an online learning algorithm via the
t=1 (E(φ(xt )) − φ(x∗ )) ≤ (cid:80)N
example, for strongly convex f (x): ERN (x∗ ) ≤ (cid:80)N
t=1
the expected convergence rate in Corollary 1 and 2, the expected regret can be easily derived. For
t=1 O( 1
t ) =
O(ln N ). However, it would be a challenging future work to derive the regret bound for ORDA
instead of the expected regret. It would also be interesting to develop the parallel extensions of
ORDA (e.g., combining the distributed mini-batch strategy in [21] with ORDA) and apply them to
some large-scale real problems.

8

50100150200202122232425262728IterationObjective  RDAAC−RDAAC−SAFOBOSSAGEORDA50100150200232425262728293031IterationObjective  RDAAC−RDAAC−SAFOBOSSAGEORDA100200300400500202224262830IterationObjective  ORDAM_ORDAReferences
[1] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated
gradient methods. In Advances in Neural Information Processing Systems (NIPS), 2011.
[2] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction
using mini-batches. Technical report, Microsoft Research, 2011.
[3] J. Duchi, P. L. Bartlett, and M. Wainwright. Randomized smoothing for stochastic optimiza-
tion. arXiv:1103.4296v1, 2011.
[4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Conference on Learning Theory (COLT), 2010.
[5] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In
Conference on Learning Theory (COLT), 2010.
[6] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward-backward splitting.
Journal of Machine Learning Research, 10:2873–2898, 2009.
[7] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. In Conference on Learning Theory (COLT), 2011.
[8] C. Hu, J. T. Kwok, and W. Pan. Accelerated gradient methods for stochastic optimization and
online learning. In Advances in Neural Information Processing Systems (NIPS), 2009.
[9] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly con-
vex functions. August 2010.
[10] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, part i: a generic algorithmic framework. Technical report,
University of Florida, 2010.
[11] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, part ii: shrinking procedures and optimal algorithms. Tech-
nical report, University of Florida, 2010.
[12] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of
Machine Learning Research, 10:777–801, 2009.
[13] S. Lee and S. J. Wright. Manifold identiﬁcation of dual averaging methods for regularized
stochastic online learning. In International Conference on Machine Learning (ICML), 2011.
[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[15] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. John
Wiley New York, 1983.
[16] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic
Pub, 2003.
[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Program-
ming, 120:221–259, 2009.
[18] A. Rakhlin, O. Shamir, and K. Sridharan. To average or not to average? making stochastic gra-
dient descent optimal for strongly convex problems. In International Conference on Machine
Learning (ICML), 2012.
[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J.R.Statist.Soc.B, 58:267–288,
1996.
[20] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. SIAM
Journal on Optimization (Submitted), 2008.
[21] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11:2543–2596, 2010.
[22] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist.
Soc. B, 67(2):301–320, 2005.

9


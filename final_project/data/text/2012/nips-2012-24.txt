Supervised Learning with Similarity Functions

Purushottam Kar
Indian Institute of Technology
Kanpur, INDIA
purushot@cse.iitk.ac.in

Prateek Jain
Microsoft Research Lab
Bangalore, INDIA
prajain@microsoft.com

Abstract

We address the problem of general supervised learning when data can only be ac-
cessed through an (indeﬁnite) similarity function between data points. Existing
work on learning with indeﬁnite kernels has concentrated solely on binary/multi-
class classiﬁcation problems. We propose a model that is generic enough to handle
any supervised learning task and also subsumes the model previously proposed for
classiﬁcation. We give a “goodness” criterion for similarity functions w.r.t. a given
supervised learning task and then adapt a well-known landmarking technique to
provide efﬁcient algorithms for supervised learning using “good” similarity func-
tions. We demonstrate the effectiveness of our model on three important super-
vised learning problems: a) real-valued regression, b) ordinal regression and c)
ranking where we show that our method guarantees bounded generalization error.
Furthermore, for the case of real-valued regression, we give a natural goodness
deﬁnition that, when used in conjunction with a recent result in sparse vector re-
covery, guarantees a sparse predictor with bounded generalization error. Finally,
we report results of our learning algorithms on regression and ordinal regression
tasks using non-PSD similarity functions and demonstrate the effectiveness of
our algorithms, especially that of the sparse landmark selection algorithm that
achieves signiﬁcantly higher accuracies than the baseline methods while offering
reduced computational costs.

1

Introduction

The goal of this paper is to develop an extended framework for supervised learning with similarity
functions. Kernel learning algorithms [1] have become the mainstay of discriminative learning with
an incredible amount of effort having been put in, both from the theoretician’s as well as the prac-
titioner’s side. However, these algorithms typically require the similarity function to be a positive
semi-deﬁnite (PSD) function, which can be a limiting factor for several applications. Reasons being:
1) the Mercer’s condition is a formal statement that is hard to verify, 2) several natural notions of
similarity that arise in practical scenarios are not PSD, and 3) it is not clear as to why an artiﬁcial
constraint like PSD-ness should limit the usability of a kernel.
Several recent papers have demonstrated that indeﬁnite similarity functions can indeed be success-
fully used for learning [2, 3, 4, 5]. However, most of the existing work focuses on classiﬁcation tasks
and provides specialized techniques for the same, albeit with little or no theoretical guarantees. A
notable exception is the line of work by [6, 7, 8] that deﬁnes a goodness criterion for a similarity
function and then provides an algorithm that can exploit this goodness criterion to obtain provably
accurate classiﬁers. However, their deﬁnitions are yet again restricted to the problem of classiﬁ-
cation as they take a “margin” based view of the problem that requires positive points to be more
similar to positive points than to negative points by at least a constant margin.
In this work, we instead take a “target-value” point of view and require that target values of similar
points be similar. Using this view, we propose a generic goodness deﬁnition that also admits the

1

goodness deﬁnition of [6] for classiﬁcation as a special case. Furthermore, our deﬁnition can be seen
as imposing the existence of a smooth function over a generic space deﬁned by similarity functions,
rather than over a Hilbert space as required by typical goodness deﬁnitions of PSD kernels.
We then adapt the landmarking technique of [6] to provide an efﬁcient algorithm that reduces learn-
ing tasks to corresponding learning problems over a linear space. The main technical challenge at
this stage is to show that such reductions are able to provide good generalization error bounds for
the learning tasks at hand. To this end, we consider three speciﬁc problems: a) regression, b) ordinal
regression, and c) ranking. For each problem, we deﬁne appropriate surrogate loss functions, and
show that our algorithm is able to, for each speciﬁc learning task, guarantee bounded generalization
error with polynomial sample complexity. Moreover, by adapting a general framework given by
[9], we show that these guarantees do not require the goodness deﬁnition to be overly restrictive by
showing that our deﬁnitions admit all good PSD kernels as well.
For the problem of real-valued regression, we additionally provide a goodness deﬁnition that cap-
tures the intuition that usually, only a small number of landmarks are inﬂuential w.r.t. the learning
task. However, to recover these landmarks, the uniform sampling technique would require sampling
a large number of landmarks thus increasing the training/test time of the predictor. We address this
issue by applying a sparse vector recovery algorithm given by [10] and show that the resulting sparse
predictor still has bounded generalization error.
We also address an important issue faced by algorithms that use landmarking as a feature construc-
tions step viz [6, 7, 8], namely that they typically assume separate landmark and training sets for ease
of analysis. In practice however, one usually tries to overcome paucity of training data by reusing
training data as landmark points as well. We use an argument outlined in [11] to theoretically justify
such “double dipping” in our case. The details of the argument are given in Appendix B.
We perform several experiments on benchmark datasets that demonstrate signiﬁcant performance
gains for our methods over the baseline of kernel regression. Our sparse landmark selection tech-
nique provides signiﬁcantly better predictors that are also more efﬁcient at test time.
Related Work: Existing approaches to extend kernel learning algorithms to indeﬁnite kernels can
be classiﬁed into three broad categories: a) those that use indeﬁnite kernels directly with existing
kernel learning algorithms, resulting in non-convex formulations [2, 3]. b) those that convert a given
indeﬁnite kernel into a PSD one by either projecting onto the PSD-cone [4, 5] or performing other
spectral operations [12]. The second approach is usually expensive due to the spectral operations
involved apart from making the method inherently transductive. Moreover, any domain knowledge
stored in the original kernel is lost due to these task oblivious operations and consequently, no
generalization guarantees can be given. c) those that use notions of “task-kernel alignment” or
equivalently, notions of “goodness” of a kernel, to give learning algorithms [6, 7, 8]. This approach
enjoys several advantages over the other approaches listed above. These models are able to use
the indeﬁnite kernel directly with existing PSD kernel learning techniques; all the while retaining
the ability to give generalization bounds that quantitatively parallel those of PSD kernel learning
models. In this paper, we adopt the third approach for general supervised learning problem.

2 Problem formulation and Preliminaries

The goal in similarity-based supervised learning is to closely approximate a target predictor y :
X → Y over some domain X using a hypothesis ˆf ( · ; K ) : X → Y that restricts its interaction
with data points to computing similarity values given by K . Now, if the similarity function K is
not discriminative enough for the given task then we cannot hope to construct a predictor out of it
that enjoys good generalization properties. Hence, it is natural to deﬁne the “goodness” of a given
similarity function with respect to the learning task at hand.
Deﬁnition 1 (Good similarity function: preliminary). Given a learning task y : X → Y over some
distribution D , a similarity function K : X × X → R is said to be (0 , B )-good with respect to
this task if there exists some bounded weighing function w : X → [−B , B ] such that for at least a
x(cid:48)∼D (cid:74)w(x(cid:48) )y(x(cid:48) )K (x, x(cid:48) )(cid:75) .
(1 − 0 ) D-fraction of the domain, we have y(x) = E
The above deﬁnition is inspired by the deﬁnition of a “good” similarity function with respect to
classiﬁcation tasks given in [6]. However, their deﬁnition is tied to class labels and thus applies only

2

Algorithm 1 Supervised learning with Similarity functions
training points sampled from D : T = (cid:8)(xt
n , yn )(cid:9), loss function (cid:96)S : R × Y → R+ .
Input: A target predictor y : X → Y over a distribution D , an (0 , B )-good similarity function K , labeled
1: Sample d unlabeled landmarks from D : L = (cid:8)xl
(cid:9)
1 , y1 ), . . . , (xt
Output: A predictor ˆf : X → R with bounded true loss over D
d (cid:0)K (x, xl
d )(cid:1) ∈ Rd
1 , . . . , xl
// Else subsample d landmarks from T (see Appendix B for details)
(cid:0)(cid:10)w, ΨL (xt
i )(cid:11) , yi
(cid:1)
(cid:80)n
d
√
2: ΨL : x (cid:55)→ 1/
1 ), . . . , K (x, xl
3: ˆw = arg min
i (cid:96)S
w∈Rd :(cid:107)w(cid:107)2≤B
4: return ˆf : x (cid:55)→ (cid:104) ˆw, ΨL (x)(cid:105)

to classiﬁcation tasks. Similar to [6], the above deﬁnition calls a similarity function K “good” if the
target value y(x) of a given point x can be approximated in terms of (a weighted combination of)
the target values of the K -“neighbors” of x. Also, note that this deﬁnition automatically enforces a
smoothness prior on the framework.
However the above deﬁnition is too rigid. Moreover, it deﬁnes goodness in terms of violations, a
non-convex loss function. To remedy this, we propose an alternative deﬁnition that incorporates an
arbitrary (but in practice always convex) loss function.
Deﬁnition 2 (Good similarity function: ﬁnal). Given a learning task y : X → Y over some
distribution D , a similarity function K is said to be (0 , B )-good with respect to a loss function
(cid:96)S : R × Y → R if there exists some bounded weighing function w : X → [−B , B ] such that if we
x(cid:48)∼D (cid:74)w(x(cid:48) )K (x, x(cid:48) )(cid:75), then we have E
x∼D (cid:74)(cid:96)S (f (x), y(x))(cid:75) ≤ 0 .
deﬁne a predictor as f (x) := E
Note that Deﬁnition 2 reduces to Deﬁnition 1 for (cid:96)S (a, b) = 1{a(cid:54)=b} . Moreover, for the case of
binary classiﬁcation where y ∈ {−1, +1}, if we take (cid:96)S (a, b) = 1{ab≤Bγ } , then we recover the
(0 , γ )-goodness deﬁnition of a similarity function, given in Deﬁnition 3 of [6]. Also note that,
{|y(x)|} < ∞ we can w.l.o.g. merge w(x(cid:48) )y(x(cid:48) ) into a single term w(x(cid:48) ).
assuming sup
x∈X
Having given this deﬁnition we must make sure that “good” similarity functions allow the construc-
tion of effective predictors (Utility property). Moreover, we must make sure that the deﬁnition does
not exclude commonly used PSD kernels (Admissibility property). Below, we formally deﬁne these
two properties and in later sections, show that for each of the learning tasks considered, our goodness
deﬁnition satisﬁes these two properties.

2.1 Utility
Deﬁnition 3 (Utility). A similarity function K is said to be 0 -useful w.r.t. a loss function (cid:96)actual (·, ·)
(cid:16) ˆf (x), y(x)
(cid:17)(cid:122) ≤ 0 + 1 .
if the following holds: there exists a learning algorithm A that, for any 1 , δ > 0, when given
(cid:114)(cid:96)actual
poly(1/1 , log(1/δ)) “labeled” and “unlabeled” samples from the input distribution D , with prob-
ability at least 1 − δ , generates a hypothesis ˆf (x; K ) s.t. E
x∼D
Note that ˆf (x; K ) is restricted to access the data solely through K .

Here, the 0 term captures the misﬁt or the bias of the similarity function with respect to the learning
problem. Notice that the above utility deﬁnition allows for learning from unlabeled data points and
thus puts our approach in the semi-supervised learning framework.
All our utility guarantees proceed by ﬁrst using unlabeled samples as landmarks to construct a land-
marked space. Next, using the goodness deﬁnition, we show the existence of a good linear predictor
in the landmarked space. This guarantee is obtained in two steps as outlined in Algorithm 1: ﬁrst of
all we choose d unlabeled landmark points and construct a map Ψ : X → Rd (see Step 1 of Algo-
rithm 1) and show that there exists a linear predictor over Rd that closely approximates the predictor
f used in Deﬁnition 2 (see Lemma 15 in Appendix A). In the second step, we learn a predictor (over
the landmarked space) using ERM over a fresh labeled training set (see Step 3 of Algorithm 1). We
then use individual task-speciﬁc arguments and Rademacher average-based generalization bounds
[13] thus proving the utility of the similarity function.

3

2.2 Admissibility

In order to show that our models are not too rigid, we would prove that they admit good PSD
kernels. The notion of a good PSD kernel for us will be one that corresponds to a prevalent large
margin technique for the given problem. In general, most notions correspond to the existence of a
linear operator in the RKHS of the kernel that has small loss at large margin. More formally,
Deﬁnition 4 (Good PSD Kernel). Given a learning task y : X → Y over some distribution D , a
PSD kernel K : X × X → R with associated RKHS HK and canonical feature map ΦK : X → HK
is said to be (0 , γ )-good with respect to a loss function (cid:96)K : R × Y → R if there exists W∗ ∈ HK
(cid:18) (cid:104)W∗ , ΦK (x)(cid:105)
(cid:19)(cid:123) < 0
(cid:115)(cid:96)K
such that (cid:107)W∗ (cid:107) = 1 and
γ
We will show, for all the learning tasks considered, that every (0 , γ )-good PSD kernel, when treated
as simply a similarity function with no consideration of its RKHS, is also ( + 1 , B )-good for
arbitrarily small 1 with B = h(γ , 1 ) for some function h. To prove these results we will adapt
techniques introduced in [9] with certain modiﬁcations and task-dependent arguments.

E
x∼D

, y(x)

3 Applications

We will now instantiate the general learning model described above to real-valued regression, ordinal
regression and ranking by providing utility and admissibility guarantees. Due to lack of space, we
relegate all proofs as well as the discussion on ranking to the supplementary material (Appendix F).

3.1 Real-valued Regression

Real-valued regression is a quintessential learning problem [1] that has received a lot of attention
in the learning literature. In the following we shall present algorithms for performing real-valued
regression using non-PSD similarity measures. We consider the problem with (cid:96)actual (a, b) = |a − b|
(cid:26) 0,
as the true loss function. For the surrogates (cid:96)S and (cid:96)K , we choose the -insensitive loss function [1]
deﬁned as follows:
if |a − b| < ,
(cid:96) (a, b) = (cid:96) (a − b) =
|a − b| − ,
otherwise.
The above loss function automatically gives us notions of good kernels and similarity functions by
appealing to Deﬁnitions 4 and 2 respectively. It is easy to transfer error bounds in terms of absolute
error to those in terms of mean squared error (MSE), a commonly used performance measure for
real-valued regression. See Appendix D for further discussion on the choice of the loss function.
(cid:80)n
Using the landmarking strategy described in Section 2.1, we can reduce the problem of real regres-
sion to that of a linear regression problem in the landmarked space. More speciﬁcally, the ERM step
i (cid:96) ((cid:104)w, ΨL (xi )(cid:105) − yi ).
in Algorithm 1 becomes the following:
arg min
w∈Rd :(cid:107)w(cid:107)2≤B
There exist solvers (for instance [14]) to efﬁciently solve the above problem on linear spaces. Using
proof techniques sketched in Section 2.1 along with speciﬁc arguments for the -insensitive loss, we
can prove generalization guarantees and hence utility guarantees for the similarity function.
Theorem 5. Every similarity function that is (0 , B )-good for a regression problem with respect
landmarked space as well as the labeled sample complexity can be bounded by O (cid:16) B 2
(cid:17)
to the insensitive loss function (cid:96) (·, ·) is (0 + )-useful with respect to absolute loss as well as
(B 0 + B )-useful with respect to mean squared error. Moreover, both the dimensionality of the
.
log 1
2
δ
1
0 + 1 , O (cid:16) 1
(cid:16)
(cid:17)(cid:17)
We are also able to prove the following (tight) admissibility result:
Theorem 6. Every PSD kernel that is (0 , γ )-good for a regression problem is, for any 1 > 0,
(cid:17)
(cid:16) 1
-good as a similarity function as well. Moreover, for any 1 < 1/2 and any
1 γ 2
γ < 1, there exists a regression instance and a corresponding kernel that is (0, γ )-good for the
.
regression problem but only (1 , B )-good as a similarity function for B = Ω
1 γ 2

4

3.2 Sparse regression models

An artifact of a random choice of landmarks is that very few of them might turn out to be “informa-
tive” with respect to the prediction problem at hand. For instance, in a network, there might exist
hubs or authoritative nodes that yield rich information about the learning problem. If the relative
abundance of such nodes is low then random selection would compel us to choose a large number
of landmarks before enough “informative” ones have been collected.
However this greatly increases training and testing times due to the increased costs of constructing
the landmarked space. Thus, the ability to prune away irrelevant landmarks would speed up training
and test routines. We note that this issue has been addressed before in literature [8, 12] by way
of landmark selection heuristics. In contrast, we guarantee that our predictor will select a small
number of landmarks while incurring bounded generalization error. However this requires a careful
restructuring of the learning model to incorporate the “informativeness” of landmarks.
Deﬁnition 7. A similarity function K is said to be (0 , B , τ )-good for a real-valued regression
problem y : X → R if for some bounded weight function w : X → [−B , B ] and choice function
x∼D (cid:74)R(x)(cid:75) = τ , the predictor f : x (cid:55)→ E
x(cid:48)∼D (cid:74)w(x(cid:48) )K (x, x(cid:48) )|R(x(cid:48) )(cid:75) has
R : X → {0, 1} with E
x∼D (cid:74)(cid:96) (f (x), y(x))(cid:75) < 0 .
bounded -insensitive loss i.e. E
The role of the choice function is to single out informative landmarks, while τ speciﬁes the relative
density of informative landmarks. Note that the above deﬁnition is similar in spirit to the goodness
deﬁnition presented in [15]. While the motivation behind [15] was to give an improved admissi-
bility result for binary classiﬁcation, we squarely focus on the utility guarantees; with the aim of
accelerating our learning algorithms via landmark pruning.
We prove the utility guarantee in three steps as outlined in Appendix D. First, we use the usual
a randomized map Ψ : X → Rd for d = O (cid:16) B 2
(cid:17)
landmarking step to project the problem onto a linear space. This step guarantees the following:
Theorem 8. Given a similarity function that is (0 , B , τ )-good for a regression problem, there exists
such that with probability at least 1 − δ ,
log 1
τ 2
δ
there exists a linear operator ˜f : x (cid:55)→ (cid:104)w, x(cid:105) over Rd such that (cid:107)w(cid:107)1 ≤ B with -insensitive loss
1
bounded by 0 + 1 . Moreover, with the same conﬁdence we have (cid:107)w(cid:107)0 ≤ 3dτ
2 .
Our proof follows that of [15], however we additionally prove sparsity of w as well. The number of
landmarks required here is a Ω (1/τ ) fraction greater than that required by Theorem 5. This formally
captures the intuition presented earlier of a small fraction of dimensions (read landmarks) being ac-
tually relevant to the learning problem. So, in the second step, we use the Forward Greedy Selection
algorithm given in [10] to learn a sparse predictor. The use of this learning algorithm necessitates
the use of a different generalization bound in the ﬁnal step to complete the utility guarantee given
below. We refer the reader to Appendix D for the details of the algorithm and its utility analysis.
dimensionality of the landmarked space being bounded by O (cid:16) B 2
(cid:17)
Theorem 9. Every similarity function that is (0 , B , τ )-good for a regression problem with respect
to the insensitive loss function (cid:96) (·, ·) is (0 + )-useful with respect to absolute loss as well; with the
complexity being bounded by O (cid:16) B 2
(cid:17)
and the labeled sampled
log 1
τ 2
δ
1
. Moreover, this utility can be achieved by an O (τ )-
log B
2
1 δ
1
sparse predictor on the landmarked space.
We note that the improvements obtained here by using the sparse learning methods of [10] provide
Ω (τ ) increase in sparsity. We now prove admissibility results for this sparse learning model. We
do this by showing that the dense model analyzed in Theorem 5 and that given in Deﬁnition 7 are
(cid:1)-good where ¯w =
Theorem 10. Every (0 , B )-good similarity function K is also (cid:0)0 , B , ¯w
interpretable in each other for an appropriate selection of parameters. The guarantees in Theorem 6
can then be invoked to conclude the admissibility proof.
x∼D (cid:74)|w(x)|(cid:75). Moreover, every (0 , B , τ )-good similarity function K is also (0 , B/τ )-good.
B
E
0 + 1 , O (cid:16) 1
(cid:16)
(cid:17)
(cid:17)
Using Theorem 6, we immediately have the following corollary:
Corollary 11. Every PSD kernel that is (0 , γ )-good for a regression problem is, for any 1 > 0,
-good as a similarity function as well.
, 1
1 γ 2

5

3.3 Ordinal Regression

The problem of ordinal regression requires an accurate prediction of (discrete) labels coming from
a ﬁnite ordered set [r] = {1, 2, . . . , r}. The problem is similar to both classiﬁcation and regression,
but has some distinct features due to which it has received independent attention [16, 17] in domains
such as product ratings etc. The most popular performance measure for this problem is the absolute
loss which is the absolute difference between the predicted and the true labels.
A natural and rather tempting way to solve this problem is to relax the problem to real-valued
regression and threshold the output of the learned real-valued predictor using predeﬁned thresholds
b1 , . . . , br to get discrete labels. Although this approach has been prevalent in literature [17], as the
discussion in the supplementary material shows, this leads to poor generalization guarantees in our
model. More speciﬁcally, a goodness deﬁnition constructed around such a direct reduction is only
able to ensure (0 + 1)-utility i.e. the absolute error rate is always greater than 1.
One of the reasons for this is the presence of the thresholding operation that makes it impossible to
distinguish between instances that would not be affected by small perturbations to the underlying
real-valued predictor and those that would. To remedy this, we enforce a (soft) margin with respect
to thresholding that makes the formulation more robust to noise. More formally, we expect that if
a point belongs to the label i, then in addition to being sandwiched between the thresholds bi and
bi+1 , it should be separated from these by a margin as well i.e. bi + γ ≤ f (x) ≤ bi+1 − γ .
This is a direct generalization of the margin principle in classiﬁcation where we expect w(cid:62)x > b+ γ
for positively labeled points and w(cid:62)x < b − γ for negatively labeled points. Of course, wherein
classiﬁcation requires a single threshold, we require several, depending upon the number of labels.
For any x ∈ R, let [x]+ = max {x, 0}. Thus, if we deﬁne the γ -margin loss function to be [x]γ :=
[γ − x]+ (note that this is simply the well known hinge loss function scaled by a factor of γ ), we
can deﬁne our goodness criterion as follows:
Deﬁnition 12. A similarity function K is said to be (0 , B )-good for an ordinal regression problem
y : X → [r ] if for some bounded weight function w : X → [−B , B ] and some (unknown but ﬁxed)
x(cid:48)∼D (cid:74)w(x(cid:48) )K (x, x(cid:48) )(cid:75) satisﬁes
γ + (cid:2)by(x)+1 − f (x)(cid:3)
(cid:114)(cid:2)f (x) − by(x)
(cid:3)
(cid:122) < 0 .
set of thresholds {bi }r
i=1 with b1 = −∞, the predictor f : x (cid:55)→ E
E
x∼D
γ
We now give utility guarantees for our learning model. We shall give guarantees on both the mis-
classiﬁcation error as well as the absolute error of our learned predictor. We say that a set of points
{|xi − xj |} ≥ ∆. Deﬁne the function ψ∆ (x) = x+∆−1
x1 , . . . , xi . . . is ∆-spaced if min
∆ .
i (cid:54)=j
(cid:16) 0
(cid:16) 0
(cid:17)
(cid:17)
Theorem 13. Let K be a similarity function that is (0 , B )-good for an ordinal regression prob-
lem with respect to ∆-spaced thresholds and γ -margin loss. Let ¯γ = max {γ , 1}. Then K is
-useful with respect to ordinal regression error (absolute loss). Moreover, K is
-
ψ(∆/¯γ )
¯γ
¯γ
useful with respect to the zero-one mislabeling error as well.
by O (cid:16) B 2
(cid:17)
We can bound, both dimensionality of the landmarked space as well as labeled sampled complexity,
. Notice that for 0 < 1 and large enough d, n, we can ensure that the ordinal
log 1
2
δ
1
regression error rate is also bounded above by 1 since
(ψ∆ (x)) = 1. This is in contrast
sup
x∈[0,1],∆>0
with the direct reduction to real valued regression which has ordinal regression error rate bounded
below by 1. This indicates the advantage of the present model over a naive reduction to regression.
We can show that our deﬁnition of a good similarity function admits all good PSD kernels as well.
The kernel goodness criterion we adopt corresponds to the large margin framework proposed by
(cid:17)(cid:17)
γ1 0 + 1 , O (cid:16) γ 2
(cid:16)
[16]. We refer the reader to Appendix E.3 for the deﬁnition and give the admissibility result below.
Theorem 14. Every PSD kernel that is (0 , γ )-good for an ordinal regression problem is also
-good as a similarity function with respect to the γ1 -margin loss for any
1
1 γ 2
(cid:17)
(cid:16) γ 2
γ1 , 1 > 0. Moreover, for any 1 < γ1/2, there exists an ordinal regression instance and a corre-
sponding kernel that is (0, γ )-good for the ordinal regression problem but only (1 , B )-good as a
.
similarity function with respect to the γ1 -margin loss function for B = Ω
1
1 γ 2

6

(a) Mean squared error for landmarking (RegLand), sparse landmarking (RegLand-Sp) and kernel regression (KR)

(b) Avg. absolute error for landmarking (ORLand) and kernel regression (KR) on ordinal regression datasets

Figure 1: Performance of landmarking algorithms with increasing number of landmarks on real-
valued regression (Figure 1a) and ordinal regression (Figure 1b) datasets.

Datasets

Sigmoid kernel
Land-Sp

KR

Manhattan kernel
KR
Land-Sp

Datasets

Sigmoid kernel
ORLand

KR

Manhattan kernel
KR
ORLand

6.2e-01
(2.0e-02)

2.9e+0
(6.2e-02)

4.5e-01
(3.2e-02)

4.9e-01
(1.5e-02)

6.7e-01
(3.0e-02)

6.2e-01
(2.0e-02)

4.2e-01
(3.8e-02)

8.9e-01
(8.5e-01)

3.9e-04
(2.2e-05)

5.8e-02
(1.9e-04)

6.0e-03
(3.7e-04)

3.5e-05
(1.3e-05)

1.6e-02
(6.2e-04)

1.4e-03
(1.7e-04)

5.9e-02
(2.3e-04)

4.1e-02
(1.6e-03)

6.2e-03
(8.4e-04)

9.5e-05
(1.3e-04)

2.1e-02
(8.3e-04)

4.6e-04
(6.5e-05)

1.7e-02
(7.1e-04)

6.8e-01
(2.8e-02)

Abalone [18]
Wine-Red [18]
N = 4177
N = 1599
d = 8
d = 11
Bodyfat [19]
Wine-White [18]
N = 252
N = 4898
d = 14
d = 11
CAHousing [19]
Bank-8 [20]
N = 20640
N = 8192
d = 8
d = 8
CPUData [20]
Bank-32 [20]
N = 8192
N = 8192
d = 12
d = 32
PumaDyn-8 [20]
House-8 [20]
N = 8192
N = 22784
d = 8
d = 8
PumaDyn-32 [20]
House-16 [20]
N = 8192
N = 22784
d = 32
d = 16
(a) Mean squared error for real regression
(b) Mean absolute error for ordinal regression
Table 1: Performance of landmarking-based algorithms (with 50 landmarks) vs. baseline kernel
regression (KR). Values in parentheses indicate standard deviation values. Values in the ﬁrst columns
indicate dataset source (in parentheses), size (N) and dimensionality (d).

2.3e-01
(4.6e-03)

1.8e-01
(3.6e-03)

1.4e-02
(4.5e-04)

1.4e-02
(3.7e-04)

1.5e-02
(1.4e-04)

1.2e-03
(3.2e-05)

4.3e-02
(1.6e-03)

2.3e-01
(4.5e-03)

1.4e-02
(4.8e-04)

1.4e-02
(3.1e-04)

6.1e-01
(4.4e-02)

1.6e+0
(2.3e-02)

2.7e+0
(6.6e-02)

2.6e+0
(8.1e-02)

6.3e-01
(1.7e-02)

1.6e+0
(9.4e-02)

2.7e+0
(1.2e-01)

2.8e+0
(9.3e-03)

1.5e+0
(2.0e-02)

1.5e+0
(1.0e-02)

2.7e+0
(1.0e-02)

2.8e+0
(2.0e-02)

1.4e+0
(1.2e-02)

1.4e+0
(2.3e-02)

1.8e-01
(3.6e-03)

2.7e+0
(2.0e-02)

Due to lack of space we refer the reader to Appendix F for a discussion on ranking models that
includes utility and admissibility guarantees with respect to the popular NDCG loss.

4 Experimental Results

In this section we present an empirical evaluation of our learning models for the problems of real-
valued regression and ordinal regression on benchmark datasets taken from a variety of sources
(cid:80)
[18, 19, 20]. In all cases, we compare our algorithms against kernel regression (KR), a well known
technique [21] for non-linear regression, whose predictor is of the form:
(cid:80)
xi∈T y(xi )K (x, xi )
f : x (cid:55)→
.
xi∈T K (x, xi )
where T is the training set. We selected KR as the baseline as it is a popular regression method that
does not require similarity functions to be PSD. For ordinal regression problems, we rounded off the
result of the KR predictor to get a discrete label. We implemented all our algorithms as well as the

7

baseline KR method in Matlab. In all our experiments we report results across 5 random splits on
the (indeﬁnite) Sigmoid: K (x, y) = tanh(a (cid:104)x, y(cid:105) + r) and Manhattan: K (x, y) = − (cid:107)x − y(cid:107)1
kernels. Following standard practice, we ﬁxed r = −1 and a = 1/dorig for the Sigmoid kernel
where dorig is the dimensionality of the dataset.
Real valued regression: For this experiment, we compare our methods (RegLand and RegLand-Sp)
with the KR method. For RegLand, we constructed the landmarked space as speciﬁed in Algorithm 1
and learned a linear predictor using the LIBLINEAR package [14] that minimizes -insensitive
loss. In the second algorithm (RegLand-Sp), we used the sparse learning algorithm of [10] on the
landmarked space to learn the best predictor for a given sparsity level. Due to its simplicity and
good convergence properties, we implemented the Fully Corrective version of the Forward Greedy
Selection algorithm with squared loss as the surrogate.
We evaluated all methods using Mean Squared Error (MSE) on the test set. Figure 1a shows the MSE
incurred by our methods along with reference values of accuracies obtained by KR as landmark sizes
increase. The plots clearly show that our methods incur signiﬁcantly lesser error than KR. Moreover,
RegLand-Sp learns more accurate predictors using the same number of landmarks. For instance,
when learning using the Sigmoid kernel on the CPUData dataset, at 20 landmarks, RegLand is able
to guarantee an MSE of 0.016 whereas RegLand-Sp offers an MSE of less than 0.02 ; MLKR is
only able to guarantee an MSE rate of 0.04 for this dataset. In Table 1a, we compare accuracies of
the two algorithms when given 50 landmark points with those of KR for the Sigmoid and Manhattan
kernels. We ﬁnd that in all cases, RegLand-Sp gives superior accuracies than KR. Moreover, the
Manhattan kernel seems to match or outperform the Sigmoid kernel on all the datasets.
Ordinal Regression: Here, we compare our method with the baseline KR method on benchmark
datasets. As mentioned in Section 3.3, our method uses the EXC formulation of [16] along with
landmarking scheme given in Algorithm 1. We implemented a gradient descent-based solver (OR-
Land) to solve the primal formulation of EXC and used ﬁxed equi-spaced thresholds instead of
learning them as suggested by [16]. Of the six datasets considered here, the two Wine datasets are
ordinal regression datasets where the quality of the wine is to be predicted on a scale from 1 to 10.
The remaining four datasets are regression datasets whose labels were subjected to equi-frequency
binning to obtain ordinal regression datasets [16]. We measured the average absolute error (AAE)
for each method. Figure 1b compares ORLand with KR as the number of landmarks increases. Ta-
ble 1b compares accuracies of ORLand for 50 landmark points with those of KR for Sigmoid and
Manhattan kernels. In almost all cases, ORLand gives a much better performance than KR. The
Sigmoid kernel seems to outperform the Manhattan kernel on a couple of datasets.
We refer the reader to Appendix G for additional experimental results.

5 Conclusion

In this work we considered the general problem of supervised learning using non-PSD similarity
functions. We provided a goodness criterion for similarity functions w.r.t. various learning tasks.
This allowed us to construct efﬁcient learning algorithms with provable generalization error bounds.
At the same time, we were able to show, for each learning task, that our criterion is not too restrictive
in that it admits all good PSD kernels. We then focused on the problem of identifying inﬂuential
landmarks with the aim of learning sparse predictors. We presented a model that formalized the
intuition that typically only a small fraction of landmarks is inﬂuential for a given learning problem.
We adapted existing sparse vector recovery algorithms within our model to learn provably sparse
predictors with bounded generalization error. Finally, we empirically evaluated our learning algo-
rithms on benchmark regression and ordinal regression tasks. In all cases, our learning methods,
especially the sparse recovery algorithm, consistently outperformed the kernel regression baseline.
An interesting direction for future research would be learning good similarity functions ´a la metric
learning or kernel learning. It would also be interesting to conduct large scale experiments on real-
world data such as social networks that naturally capture the notion of similarity amongst nodes.

Acknowledgments

P. K. is supported by a Microsoft Research India Ph.D. fellowship award. Part of this work was done
while P. K. was an intern at Microsoft Research Labs India, Bangalore.

8

In 23rd

In 24th Annual

References
[1] Bernhard Sch ¨olkopf and Alex J. Smola. Learning with Kernels : Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, 2002.
[2] Bernard Haasdonk. Feature Space Interpretation of SVMs with Indeﬁnite Kernels. IEEE Transactions on
Pattern Analysis and Machince Intelligence, 27(4):482–492, 2005.
[3] Cheng Soon Ong, Xavier Mary, St ´ephane Canu, and Alexander J. Smola. Learning with non-positive
Kernels. In 21st Annual International Conference on Machine Learning, 2004.
[4] Yihua Chen, Maya R. Gupta, and Benjamin Recht. Learning Kernels from Indeﬁnite Similarities. In 26th
Annual International Conference on Machine Learning, pages 145–152, 2009.
[5] Ronny Luss and Alexandre d’Aspremont. Support Vector Machine Classiﬁcation with Indeﬁnite Kernels.
In 21st Annual Conference on Neural Information Processing Systems, 2007.
[6] Maria-Florina Balcan and Avrim Blum. On a Theory of Learning with Similarity Functions.
Annual International Conference on Machine Learning, pages 73–80, 2006.
[7] Liwei Wang, Cheng Yang, and Jufu Feng. On Learning with Dissimilarity Functions.
International Conference on Machine Learning, pages 991–998, 2007.
[8] Purushottam Kar and Prateek Jain. Similarity-based Learning via Data Driven Embeddings. In 25th Annual
Conference on Neural Information Processing Systems, 2011.
[9] Nathan Srebro. How Good Is a Kernel When Used as a Similarity Measure? In 20th Annual Conference
on Computational Learning Theory, pages 323–335, 2007.
[10] Shai Shalev-Shwartz, Nathan Srebro, and Tong Zhang. Trading Accuracy for Sparsity in Optimization
Problems with Sparsity Constraints. SIAM Journal on Optimization, 20(6):2807–2832, 2010.
[11] Nathan Srebro Shai Ben-David, Ali Rahimi. Generalization Bounds for Indeﬁnite Kernel Machines. In
NIPS 2008 Workshop: New Challenges in Theoretical Machine Learning, 2008.
[12] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Cazzanti. Similarity-based Classiﬁ-
cation: Concepts and Algorithms. Journal of Machine Learning Research, 10:747–776, 2009.
[13] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the Complexity of Linear Prediction :
In 22nd Annual Conference on Neural Information
Risk Bounds, Margin Bounds, and Regularization.
Processing Systems, 2008.
[14] Chia-Hua Ho and Chih-Jen Lin. Large-scale Linear Support Vector Regression. http://www.csie.
ntu.edu.tw/˜cjlin/papers/linear-svr.pdf, retrieved on May 18, 2012, 2012.
[15] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. Improved Guarantees for Learning via Similarity
Functions. In 21st Annual Conference on Computational Learning Theory, pages 287–298, 2008.
[16] Wei Chu and S. Sathiya Keerthi. Support Vector Ordinal Regression. Neural Computation, 19(3):792–
815, 2007.
[17] Shivani Agarwal. Generalization Bounds for Some Ordinal Regression Algorithms. In 19th International
Conference on Algorithmic Learning Theory, pages 7–21, 2008.
[18] A. Frank and Arthur Asuncion. UCI Machine Learning Repository. http://archive.ics.uci.
edu/ml, 2010. University of California, Irvine, School of Information and Computer Sciences.
[19] StatLib Dataset Repository. http://lib.stat.cmu.edu/datasets/. Carnegie Mellon Univer-
sity.
[20] Delve Dataset Repository. http://www.cs.toronto.edu/˜delve/data/datasets.html.
University of Toronto.
[21] Kilian Q. Weinberger and Gerald Tesauro. Metric Learning for Kernel Regression. In 11th International
Conference on Artiﬁcial Intelligence and Statistics, pages 612–619, 2007.

9


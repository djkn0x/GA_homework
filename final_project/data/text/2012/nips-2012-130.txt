The Lov ´asz ϑ function, SVMs and ﬁnding large dense
subgraphs

Vinay Jethava ∗
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
jethava@chalmers.se

Anders Martinsson
Department of Mathematics,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
andemar@student.chalmers.se

Chiranjib Bhattacharyya
Department of CSA,
Indian Institute of Science
Bangalore, 560012, INDIA
chiru@csa.iisc.ernet.in

Devdatt Dubhashi
Computer Science & Engineering Department,
Chalmers University of Technology
412 96, Goteborg, SWEDEN
dubhashi@chalmers.se

Abstract
The Lov ´asz ϑ function of a graph, a fundamental tool in combinatorial optimiza-
tion and approximation algorithms, is computed by solving a SDP. In this paper
we establish that the Lov ´asz ϑ function is equivalent to a kernel learning problem
related to one class SVM. This interesting connection opens up many opportuni-
ties bridging graph theoretic algorithms and machine learning. We show that there
exist graphs, which we call SVM − ϑ graphs, on which the Lov ´asz ϑ function
can be approximated well by a one-class SVM. This leads to novel use of SVM
√
techniques for solving algorithmic problems in large graphs e.g.
identifying a
planted clique of size Θ(
n) in a random graph G(n, 1
2 ). A classic approach for
this problem involves computing the ϑ function, however it is not scalable due
to SDP computation. We show that the random graph with a planted clique is an
example of SVM − ϑ graph. As a consequence a SVM based approach easily
identiﬁes the clique in large graphs and is competitive with the state-of-the-art.
We introduce the notion of common orthogonal labelling and show that it can be
computed by solving a Multiple Kernel learning problem. It is further shown that
such a labelling is extremely useful in identifying a large common dense subgraph
in multiple graphs, which is known to be a computationally difﬁcult problem. The
proposed algorithm achieves an order of magnitude scalability compared to state
of the art methods.

Introduction
1
The Lov ´asz ϑ function [19] plays a fundamental role in modern combinatorial optimization and
in various approximation algorithms on graphs, indeed Goemans was led to say It seems all
roads lead to ϑ [10]. The function is an instance of semideﬁnite programming(SDP) and
hence computing it is an extremely demanding task even for moderately sized graphs. In this paper
we establish that the ϑ function is equivalent to solving a kernel learning problem in the one-class
SVM setting. This surprising connection opens up many opportunities which can beneﬁt both graph
theory and machine learning. In this paper we exploit this novel connection to show an interesting
application of the SVM setup for identfying large dense subgraphs. More speciﬁcally we make the
following contributions.
http://www.cse.chalmers.se/ (cid:101)
∗Relevant
datasets
code
and
jethava/svm-theta.html

found

on

can

be

1

1.1 Contributions:

1.We give a new SDP characterization of Lov ´asz ϑ function,

min
K∈K(G)

ω(K) = ϑ(G)

where ω(K) is computed by solving an one-class SVM. The matrix K is a kernel matrix, associated
with any orthogonal labelling of G. This is discussed in Section 2.
2. Using an easy to compute orthogonal labelling we show that there exist graphs, which we call
SVM − ϑ graphs, on which Lov ´asz ϑ function can be well approximated by solving an one-class
SVM. This is discussed in Section 3.
3. The problem of ﬁnding a large common dense subgraph in multiple graphs arises in a variety
of domains including Biology, Internet, Social Sciences [18]. Existing state-of-the-art methods
[14] are enumerative in nature and has complexity exponential in the size of the subgraph. We
introduce the notion of common orthogonal labelling which can be used to develop a formulation
which is close in spirit to a Multiple Kernel Learning based formulation. Our results on the well
known DIMACS benchmark dataset show that it can identify large common dense subgraphs in
wide variety of settings, beyond the realm of state-of-the-art methods. This is discussed in Section
4.
4. Lastly, in Section 5, we show that the famous planted clique problem, can be easily solved for
large graphs by solving an one-class SVM. Many problems of interest in the area of machine learning
can be reduced to the problem of detecting planted clique, e.g detecting correlations [1, section 4.6],
correlation clustering [21] etc. The planted clique problem consists of identifying a large clique in
a random graph. There is an elegant approach for identifying the planted clique by computing the
Lov ´asz ϑ function [8], however it is not practical for large graphs as it requires solving an SDP.
We show that the graph associated with the planted clique problem is a SVM − ϑ graph, paving
the way for identifying the clique by solving an one-class SVM. Apart from the method based on
computing the ϑ function, there are other methods for planted clique identiﬁcation, which do not
require solving an SDP [2, 7, 24]. Our result is also competitive with the state-of-the-art non-SDP
based approaches [24].
Notation We denote the Euclidean norm by (cid:107) · (cid:107) and the inﬁnity norm by (cid:107) · (cid:107)∞ . Let S d−1 =
{u ∈ Rd | (cid:107)u(cid:107) = 1} denote a d dimensional sphere. Let Sn denote the set of n × n square symmetric
n denote n × n square symmetric positive semideﬁnite matrices. For any A ∈ Sn
matrices and S+
we denote the eigenvalues λ1 (A) ≥ . . . ≥ λn (A). diag(r) will denote a diagonal matrix with
(cid:33)
(cid:32)
αi − n(cid:88)
n(cid:88)
diagonal entries deﬁned by components of r . We denote the one-class SVM objective function by
(cid:124)
(cid:123)(cid:122)
(cid:125)
2
i=1
i=1
f (α;K)
where K ∈ S+
n . Let G = (V , E ) be a graph on vertices V = {1, . . . , n} and edge set E . Let
A ∈ Sn denote the adjacency matrix of G where Aij = 1 if edge (i, j ) ∈ E , and 0 otherwise. An
eigenvalue of graph G would mean the eigenvalue of the adjacency matrix of G. Let ¯G denote the
complement graph of G. The adjacency matrix of ¯G is ¯A = ee(cid:62) − I − A, where e = [1, 1, . . . , 1](cid:62)
the subgraph induced by S ⊆ V in graph G; having density γ (GS ) is given by γ (GS ) = |ES |/(cid:0)|S |
(cid:1).
is a vector of length n containing all 1’s, and I denotes the identity matrix. Let GS = (S, ES ) denote
Let Ni (G) = {j ∈ V : (i, j ) ∈ E } denote the set of neighbours of vertex i in graph G, and degree
2
of node i to be di (G) = |Ni (G)|. An independent set in G (a clique in ¯G is a subset of vertices
S ⊆ V for which no (every) pair of vertices has an edge in G (in ¯G). The notation is standard e.g.
see [3].

max
αi≥0,i=1,...,n

ω(K) =

αiαj Kij

(1)

2 Lov ´asz ϑ function and Kernel learning
Consider the problem of embedding a graph G = (V , E ) on a d dimensional unit sphere S d−1 . The
study of this problem was initiated in [19] which introduced the idea of orthogonal labelling: An

2

orthogonal labelling of graph G = (V , E ) with |V | = n, is a matrix U = [u1 , . . . , un ] ∈ Rd×n such
i uj = 0 whenever (i, j ) (cid:54)∈ E and ui ∈ S d−1 ∀ i = 1, . . . , n.
that u(cid:62)
An orthogonal labelling deﬁnes an embedding of a graph on a d dimensional unit sphere: for every
vertex i there is a vector ui on the unit sphere and for every (i, j ) (cid:54)∈ E ui and uj are orthogonal.
Using the notion of orthogonal labellings, [19] deﬁned a function, famously known as Lov ´asz ϑ
function, which upper bounds the size of maximum independent set. More speciﬁcally
for any graph G : ALPHA(G) ≤ ϑ(G),
where ALPHA(G) is the size of the largest independent set. Finding large independent sets is
a fundamental problem in algorithm design and analysis and computing ALPHA(G) is a classic
NP-hard problem which is even very hard even to approximate [11]. However, the Lov ´asz function
ϑ(G) gives a tractable upper-bound and since then Lov ´asz ϑ function has been extensively used
in solving a variety of algorithmic problems e.g. [6]. It maybe useful to recall the deﬁnition of
Lov ´asz ϑ function. Denote the set of all possible orthogonal labellings of G by Lab(G) = {U =
i uj = 0 ∀(i, j ) (cid:54)∈ E }.
[u1 , . . . , un ]|ui ∈ S d−1 , u(cid:62)

(2)

max
i

min
c∈S d−1

ϑ(G) = min
U∈Lab(G)

1
(c(cid:62)ui )2
There exist several other equivalent deﬁnitions of ϑ, for a comprehensive discussion see [16].
However computation of Lov ´asz ϑ function is not practical even for moderately sized graphs as it
requires solving a semideﬁnite program on a matrix which is of the size of the graph. In the following
theorem, we show that there exist connections between the ϑ function and the SVM formulation.
n | Kii =
Theorem 2.1. For a undirected graph G = (V , E ), with |V | = n, let K(G) := {K ∈ S+
1, i ∈ [n], Kij = 0, (i, j ) (cid:54)∈ E } Then, ϑ(G) = minK∈K(G) ω(K)
Proof. We begin by noting that any K ∈ K(G) is positive semideﬁnite and hence there exists
U ∈ Rd×n such that K = U(cid:62)U. Note that Kij = u(cid:62)
i uj where ui is a column of U. Hence by
inspection it is clear that the columns of U deﬁnes an orthogonal labelling on G, i.e U ∈ Lab(G).
Using a similar argument we can show that for any U ∈ Lab(G), the matrix K = U(cid:62)U, is an
element of K(G). The set of valid kernel matrices K(G) is thus equivalent to Lab(G). Note that if
U is a labelling then U = Udiag() is also an orthogonal labelling for any (cid:62) = [1 , . . . , n ], i =
±1 i = 1, . . . , n. It thus sufﬁces to consider only those labellings for which c(cid:62)ui ≥ 0 ∀ i =
≤ t. This is
(c(cid:62)ui )2 = mint t2 subject to
1, . . . , n holds. For a ﬁxed c one can write maxi
1
1
c(cid:62)ui
true because the minimum over t is attained at maxi
. Setting w = 2tc yields the following
1
c(cid:62)ui
(cid:107)w(cid:107)2
4 with constraints w(cid:62)ui ≥ 2. This establishes
relation minc∈S d−1 maxi
1
(c(cid:62)ui )2 = minw∈Rd
that for a labelling, U, the optimal c is obtained by solving an one-class SVM. Application of
(c(cid:62)ui )2 = ω(K) where K = U(cid:62)U
strong duality immediately leads to the claim minc∈S d−1 maxi
1
and ω(K) is deﬁned in (1). As there is a correspondence between each element of Lab(G) and K
minimization of ω(K) over K is equivalent to computing the ϑ(G) function.

This is a signiﬁcant result which establishes connections between two well studied formulations,
namely ϑ function and the SVM formulation. An important consequence of Theorem 2.1 is an
easily computable upperbound on ϑ(G) namely that for any graph G
ALPHA(G) ≤ ϑ(G) ≤ ω(K) ∀K ∈ K(G)
Since solving ω(K) is a convex quadratic program, it is indeed a computationally efﬁcient alternative
to the ϑ function. In fact we will show that there exist families of graphs for which ϑ(G) can be
approximated to within a constant factor by ω(K) for suitable K. Theorem 2.1 is closely related to
the following result proved in [20].
Theorem 2.2. [20] For a graph G = (V , E ) with |V | = n let C ∈ Sn matrix with Cij = 0
(cid:19)
(cid:19)
2x(cid:62) e − x(cid:62) (cid:18) C
(cid:18)
whenever (i, j ) (cid:54)∈ E . Then,
−λn (C)
= max
x≥0

ϑ(G) = minC v(G, C)

+ I

(3)

x

3

Proof. See [20]
See that for any feasible C the matrix I + C−λn (C) ∈ K(G). Theorem 2.1 is a restatement of
Theorem 2.2, but has the additional advantage that the stated optimization problem can be solved
as an SDP. The optimization problem minC v(G, C) with constraints on C is not an SDP. If we
ﬁx C = A, the adjacency matrix, we obtain a very interesting orthogonal labelling, which we will
refer to as LS labelling, introduced in [20]. Indeed there exists family of graphs, called Q graphs
for which LS labelling yields the interesting result ALPHA(G) = v(G, A), see [20]. Indeed
on a Q graph one does not need to compute a SDP, but can solve an one-class SVM, which has
obvious computational beneﬁts. Inspired by this result, in the remaining part of the paper, we study
this labelling more closely. As a labelling is completely deﬁned by the associated kernel matrix, we
refer to the following kernel as the LS labelling,
+ I where ρ ≥ −λn (A).

A
ρ
3 SVM − ϑ graphs: Graphs where ϑ function can be approximated by SVM

K =

(4)

We now introduce a class of graphs on which ϑ function can be well approximated by ω(K) for K
deﬁned by (4). In the spirit of approximation algorithms we deﬁne:
Deﬁnition 3.1. A graph G is a SVM − ϑ graph if ω(K) ≤ (1 + O(1))ϑ(G) where K is a LS la-
belling.

Such classes of graphs are interesting because on them, one can approximate the Lov ´asz ϑ function
by solving an SVM, instead of an SDP, which in turn can be extremely useful in the design and anal-
ysis of approximation algorithms. We will demosntrate two examples of SVM − ϑ graphs namely
(a.) the Erd ¨os–Renyi random graph G(n, 1/2) and (b.) a planted variation. Here the relaxation
ω(K) could be used in place of ϑ(G), resulting in algorithms with the same quality guarantees but
with faster running time – in particular, this will allow the algorithms to be scaled to large graphs.
The classical Erd ¨os-Renyi random graph G(n, 1/2) has n vertices and each edge (i, j ) is present
independently with probability 1/2. We list a few facts about G(n, 1/2) that will be used repeatedly.
Fact 3.1. For G(n, 1/2),
• With probability 1 − O(1/n), the degree of each vertex is in the range n/2 ± O(
n log n).
• With probability 1 − e−nc for some c > 0, the maximum eigenvalue is n/2 ± o(n) and the
minimum eigenvalue is in the range [−√
√
n] [9].
n,
√
2 − 1. For G = G(n, 1/2) , with probability 1 − O(1/n), ω(K) ≤
√
Theorem 3.1. Let  >
(1 + )ϑ(G) where K is deﬁned in (4) with ρ = 1+√
n.
2

√

√
Proof. We begin by considering the case for ρ = (1 + δ
n. By Fact 3.1 for all choices of δ > 0,
2 )
ρ A + I is, almost surely, greater than 0 which implies that f (α, K)
the minimum eigenvalue of 1
(see (1)) is strongly concave. For such functions KKT conditions are neccessary and sufﬁcient for
(cid:88)
optimality. The KKT conditions for a G(n, 1
2 ) are given by the following equation
Ai,j αj = 1 + µi , µiαi = 0, µi ≥ 0
(i,j )∈E
2 (ee(cid:62) − I ), be
As A is random we begin by analyzing the case for expectation of A. Let E(A) = 1
E(A)
the expectation of A. For the given choice of ρ, the matrix ˜K =
ρ + I is positive deﬁnite. More
importantly f (α, ˜K) is again strongly concave and attains maximum at a KKT point. By direct
veriﬁcation ˆα = ˆβ e where ˆβ = 2ρ
n−1+2ρ satisﬁes
1
ρ

E(A)α = e.

αi +

α +

(5)

(6)

1
ρ

4

(7)

(8)

(9)

1
ρ

(cid:19)

¯f = max
α≥0

f (α, ˜K) =

where a(cid:62)
i

Thus ˆα is the KKT point for the problem,

ˆα − ˆα(cid:62) (cid:18) E(A)
n(cid:88)
ˆα = n ˆβ
+ I
ρ
i=1
√
with the optimal objective function value ¯f . By choice of ρ = (1 + δ
n we can write ˆβ =
2 )
+ ∆i with |∆i | ≤ (cid:112)n log n
2ρ/n + O(1/n). Using the fact about degrees of vertices in G(n, 1/2), we know that
n − 1
a(cid:62)
i e =
2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆαi +
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:88)
is the ith row of the adjacency matrix A. As a consequence we note that
ˆβ
Aij ˆαj − 1
∆i
ρ
j
(cid:112)n log n
Recalling the deﬁnition of f and using the above equation along with (8) gives
ˆβ 2
|f ( ˆα; K) − ¯f | ≤ n
ρ
As noted before the function f (α; K) is strongly concave with ∇2
α f (α; K) (cid:22) − δ
2+δ I for all feasible
(cid:19)
(cid:18)
α. Recalling a useful result from convex optimization, see Lemma 3.1, we obtain
ω(K) − f ( ˆα; K) ≤
(cid:107)∇f ( ˆα; K)(cid:107)2
1
(11)
1 +
δ
Observing that ∇f (α; K) = 2(e − α − A
ρ α) and using the relation between (cid:107) · (cid:107)∞ and 2 norm along
√
with (9) and (8) gives (cid:107)∇f ( ˆα; K)(cid:107) ≤ √
n(cid:107)∇f ( ˆα; K)(cid:107)∞ = 2n
ˆβ
log n. Plugging this estimate in
√
ρ
(11) and using equation (10) we obtain ω(K) ≤ ˆf + O(log n) = (2 + δ)
√
n + O(log n) The second
√
equality follows by plugging in the value of ˆβ in (7). It is well known [6] that ϑ(G) =
√
n for
2
2 ) with high probability. One concludes that ω(K) ≤ 2+δ√
n) and the theorem
G(n, 1
ϑ(G) + o(
2
follows by choice of δ .

(10)

Discussion: Theorem 3.1 establishes that instead of SDP one can solve an SVM to evaluate
ϑ function on G(n, 1/2). Although it is well known that ALPHA(G(n, 1/2)) = 2 log n whp,
there is no known polynomial time algorithm for computing the maximum independent set. [6]
gives an approximation algorithm that ﬁnds an independent set in G(n, p) which runs in expected
polynomial time, via a computation of ϑ(G(n, p)),which also applies to p = 1/2. The ϑ function
also serves as a guarantee of the approximation which other algorithms a simple Greedy algorithm
cannot give. Theorem 3.1 allows us to obtain similar guarantees but without the computational over-
head of solving an SDP. Apart from ﬁnding independent sets computing ϑ(G(n, p)) is also used as
a subroutine in colorability [6], and here again one can use the SVM based approach to approximate
the ϑ function.
Similar arguments show also that other families of graphs such as the 11 families of pseudo–random
graphs described in [17] are SVM − ϑ graphs.
Lemma 3.1. [4] A function g : C ⊂ Rd → R is said to be strongly concave over S if there
exists t > 0 such that ∇2 g(x) (cid:22) −tI ∀ x ∈ C . For such functions one can show that if p∗ =
maxx∈C g(x) < ∞ then

∀x ∈ C p∗ − g(x) ≤ 1
2t

(cid:107)∇g(x)(cid:107)2

4 Dense common subgraph detection

The problem of ﬁnding a large dense subgraph in multiple graphs has many applications [23, 22,
18]. We introduce the notion of common orthogonal labelling, and show that it is indeed possible
to recover dense regions in large graphs by solving a MKL problem. This constitutes signiﬁcant
progress with respect to state of the art enumerative methods [14].

5

Problem deﬁnition Let G = {G(1) , . . . , G(M ) } be a set of simple, undirected graphs G(m) =
(V , E (m) ) deﬁned on vertex set V = {1, . . . , n}. Find a common subgraph which is dense in all the
graphs.
Most algorithms which attempts the problem of ﬁnding a dense region are enumerative in nature and
(cid:1) possible
dense in G(i) . In the worst case, the algorithm performs depth ﬁrst search over space of (cid:0) n
hence do not scale well to ﬁnding large cliques. [14], ﬁrst studied a related problem of ﬁnding all
possible common subgraphs for a given choice of parameters {γ (1) , . . . , γ (M ) } which is atleast γi
cliques of size nT . This has Θ((cid:0) n
(cid:1)) space and time complexity, which makes it impractical for
nT
nT
moderately large nT . For example, ﬁnding quasicliques of size 60 requires 8 hours (see Section 6).
In the remainder of this section, we focus on ﬁnding a large common sparse subgraph in a given
collection of graphs; with the observation that this is equivalent to ﬁnding a large common dense
subgraph in the set of complement graphs. To this end we introduce the following deﬁnition
Deﬁnition 4.1. Given simple unweighted graphs, G(m) = (V , E (m) ) m = 1, . . . , M on a common
vertex set V with |V | = n, the common orthogonal labelling on all the labellings is given by
ui ∈ S d−1 such that u(cid:62)
i uj = 0 if (i, j ) /∈ E (m) ∀ m = 1, . . . , M }.1
Following the arguments of Section 2 it is immediate that size of the largest common independent
set is upper bounded by minK∈L ω(K) where L = {K ∈ S+
: Kii = 1 ∀i ∈ [n], Kij =
n
0 whenever (i, j ) /∈ E (m) ∀ m = 1, . . . , M }. We wish to exploit this fact in identifying large
common sparse regions in general graphs. Unfortunately this problem is a SDP and will not scale
well to large graphs. Taking cue from MKL literature we pose a restricted version of the problem
namely
m=1 δmK(m) , δm≥0 (cid:80)M
K=(cid:80)M
min
ω(K)
m=1 δm=1
also a common orthogonal labelling. Using the fact that ∀x ∈ RM minpm≥0,(cid:80)M
where K(m) is an orthogonal labelling of G(m) . Direct veriﬁcation shows that any feasible K is
m=1 pm=1 p(cid:62)x =
minm xm = max{t|xm ≥ t ∀ m = 1, . . . , M } one can recast the optimization problem in (12) as
follows
t s.t. f (α; K(m) ) ≥ t ∀ m = 1, . . . , M
(13)
max
t∈R,αi≥0
where K(m) is the LS labelling for G(m) , ∀m = 1, . . . , M . The above optimization can be readily
solved by state of the art MKL solvers. This result allows us to build a parameter free common
sparse subgraph (CSS) algorithm shown in Figure 1 having following advantages:
it provides a
theoretical bound on subgraph density (Claim 4.1 below); and, it requires no parameters from the
user beyond the set of graphs G(1) , . . . , G(M ) .
(cid:80)
i = 1} with
i > 0} and S1 = {i : α∗
Let α∗ be the optimal solution in (13); and SV = {i : α∗
α∗
cardinalities nsv = |SV | and n1 = |S1 | respectively. Let ¯α(m)
j∈Ni (G
(m)
j
denote
)
min,S = mini∈S
S
di (G(m)
S )
the average of the support vector coefﬁcients in the neighbourhood Ni (G(m)
S ) of vertex i in induced
(cid:41)
(cid:40)
S )|. We deﬁne
S ) = |Ni (G(m)
having degree di (G(m)
subgraph G(m)
S
(1 − c)ρ(m)
i ∈ SV : di (G(m)
SV ) <
¯α(m)
min,SV

where c = min
i∈SV

T (m) =

(12)

α∗
i

(14)

Claim 4.1. Let T ⊆ V be computed as in Al-
α∗ = Use MKL solvers to solve eqn. (13)
T = ∩mT (m) {eqn. (14)}
gorithm 1. The subgraph G(m)
induced by T ,
T
in graph G(m) , has density at most γ (m) where
Return T
γ (m) = (1−c)ρ(m)
Proof. (Sketch) At optimality, t = (cid:80)n
¯αmin,SV (nT −1)
Figure 1: Algorithm for ﬁnding common sparse
This allows us to write 0 ≤ (cid:80)
i − (cid:80)
j ) − t as 0 ≤ (cid:80)
i=1 α∗
subgraph: T = C SS (G(1) , . . . , G(M ) )
i .
min,SV ) Dividing by (cid:0)nT
(cid:1) completes the proof.
i∈T (1 − c −
i (2 − α∗
i∈S α∗
ij α∗
j (cid:54)=i K (m)
di (G(m)
T )
¯α(m)
2
ρ(m)
1This is equivalent to deﬁning an orthogonal labelling on the Union graph of G(1) , . . . , G(M )

6

5 Finding Planted Cliques in G(n, 1/2) graphs

Finding large cliques or independent sets is a computationally difﬁcult problem even in random
graphs. While it is known that the size of the largest clique or independent set in G(n, 1/2) is 2 log n
with high probability, there is no known efﬁcient algorithm to ﬁnd a clique of size signiﬁcantly larger
than log n - even a cryptographic application was suggested based on this (see the discussion and
references in the introduction of [8]).

Hidden planted clique A random graph G(n, 1/2) is chosen ﬁrst and then a clique of size k is
introduced in the ﬁrst 1, . . . , k vertices. The problem is to identify the clique.
√
[8] showed that if k = Ω(
n), then the hidden clique can be discovered in polynomial time by com-
puting the Lov ´asz ϑ function. There are other approaches [2, 7, 24] which do not require computing
the ϑ function.
√
We consider the (equivalent) complement model G(n, 1/2, k) where a independent set is planted on
n), ¯G(n, 1/2, k) is a SVM − ϑ graph.
the set of k vertices. We show that in the regime k = Ω(
We will further demonstrate that as a consequence one can identify the hidden independent set with
high probability by solving an SVM. The following is the main result of the section.
√
n for large enough constant t ≥ 1 with K as in
(cid:19)
(cid:18)
√
Theorem 5.1. For G = ¯G(n, 1/2, k) and k = 2t
n + k/2,
(4) and ρ =
√
+ o(1)

1
ω(K) = 2(t + 1)
t
with probability at least 1 − O(1/n).
Proof. The proof is analogous to that of Theorem 3.1. Note that |λn (G)| ≤ √
n + k/2. First we
consider the expected case where all vertices outside the planted part S are adjacent to k/2 vertices
in S and (n − k)/2 vertices outside S . and all verties in the planted part have degree (n − k)/2.
√
√
probability, all vertices in S have degree (n − k)/2 ± (cid:112)(n − k) log(n − k) and those outside S are
n for i (cid:54)∈ S and αi = 2(t + 1)2 /
n for i ∈ S satisfy KKT
√
We check that αi = 2(t + 1)/
k log k vertices in S and to (n − k)/2 ± (cid:112)(n − k) log(n − k) vertices ouside
conditions with an error of O(1/
n). Now apply Chernoff bounds to conclude that with high
adjacent to k/2 ± √
(cid:19)
(cid:18)(cid:113) log n
S . Now we check that the same solution satisﬁes KKT conditions of ¯G(n, 1/2, k) with an error of
. Using the same arguments as in the proof of Theorem 3.1, we conclude that
 = O
√
√
n
ω(K) ≤ 2(t + 1)

n + O(log n) =

n + O(log n). Since ϑ(G) = 2t

n for this case [8], the result follows.

1 +

ϑ(G)

The above theorem suggests that the planted independent set can be recovered by taking the top k
values in the optimal solution. In the experimental section we will discuss the performance of this
recovery algorithm. The runtime of this algorithm is one call to SVM solver, which is considerably
cheaper than the SDP option. Indeed the algorithm due to [8], requires computation of ϑ function.
The current best known algorithm for ϑ computation has an O(n5 log n)[5], run time complexity. In
contrast the proposed approach needs to solve an SVM and hence scales well to large graphs. Our
approach is competitive with the state of the art [24] as it gives the same high probability guarantees
and have the same running time, O(n2 ). Here we have assumed that we are working with a SVM
solver which has a time complexity of O(n2 ) [13].

6 Experimental evaluation
√
Comparison with exhaustive approach [14] We generate synthetic m = 3 random graphs over
n vertices with average density δ = 0.2, and having single (common) quasi-clique of size k = 2
n
with density γ = 0.95 in all the three graphs. This is similar to the synthetic graphs generated
in the original paper [see 14, Section 6.1.2]. We note that both our MKL-based approach and
exhaustive search in [14] recovers the quasi-clique. However, the time requirements are drastically
different. All experiments were conducted on a computer with 16 GB RAM and Intel X 3470 quad-
core processor running at 2.93 GHz. Three values of k namely k = 50, 60 and k = 100 were used.
It is interesting to note that CROCHET [14] took 2 hours and 9 hours for k = 50 and k = 60 sized
cliques and failed to ﬁnd a clique of size of 100. The corresponding numbers for MKL are 47.5,54.8
and 137.6 seconds respectively.

7

Common dense subgraph detection We evaluate our algorithm for ﬁnding large dense regions on
the DIMACS Challenge graphs 2 [15], which is a comprehensive benchmark for testing of clique
ﬁnding and related algorithms. For the families of dense graphs (brock, san, sanr), we focus on
ﬁnding large dense region in the complement of the original graphs.
We run Algorithm 1 using SimpleMkl3 to ﬁnd large common dense subgraph. In order to evalu-
ate the performance of our algorithm, we compute ¯a = maxm a(m) and a = minm a(m) where
a(m) = γ (G(m)
T )/γ (G(m) ) is relative density of induced subgraph (compared to original graph den-
sity); and nT /N is relative size of induced subgraph compared to original graph size. We want
a high value of nT /N ; while a should not be lower than 1. Table 1 shows evaluation of Algo-
rithm 1 on DIMACS dataset. We note that our algorithm ﬁnds a large subgraph (large nT /N )
with higher density compared to original graph in all of DIMACS graph classes making it suit-
able for ﬁnding large dense regions in multiple graphs. In all cases the size of the subgraph, nT
was more than 100. The MKL experiments reported in Table 1 took less than 1 minute (for each
graph family); while the algorithm in [14] aborts after several hours due to memory constraints.

Planted clique recovery We generate 100
random graphs based on planted clique
√
model G(n, 1/2, k) where n = 30000 and
n for each choice
hidden clique size k = 2t
of t. We evaluate the recovery algorithm
discussed in Section 4.2. The SVM prob-
lem is solved using Libsvm4 . For t ≥ 2 we
ﬁnd perfect recovery of the clique on all the
graphs, which is agreement with Theorem
5.1.
It is worth noting that the approach takes 10
minutes to recover the clique in this graph of
30000 vertices which is far beyond the scope
of SDP based procedures.

7 Conclusion

Graph family
c-fat200
c-fat500
brock200‡
brock400‡
brock800‡
p hat300
p hat500
p hat700
p hat1000
p hat1500
san200‡
san400‡
sanr200‡
sanr400‡

N M nT
N
200
3
0.50
0.31
4
500
0.41
4
200
0.50
4
400
0.50
4
800
300
3
0.53
0.48
3
500
0.45
3
700
0.43
3
1000
1500
3
0.38
0.50
5
200
0.42
3
400
0.39
2
200
400
2
0.43

¯a
2.12
3.57
1.36
1.15
1.08
1.53
1.55
1.58
1.60
1.63
1.51
1.19
1.86
1.20

a
0.99
1.01
0.99
1.05
1.01
1.15
1.17
1.18
1.19
1.20
1.08
1.02
1.04
1.02

Table 1: Common dense subgraph recovery on multi-
ple graphs in DIMACS dataset. Here ¯a and a denote
the maximum and minimum relative density of the
induced subgraph (relative to density of the original
graph) and nT /N is the relative size of the induced
subgraph compared to original graph size.

In this paper we have established that the
Lov ´asz ϑ function, well studied in graph the-
ory can be linked to the one-class SVM for-
mulation. This link allows us to design scal-
able algorithms for computationally difﬁcult
problems.
In particular we have demon-
strated that ﬁnding a common dense region
in multiple graphs can be solved by a MKL problem, while ﬁnding a large planted clique can be
solved by an one class SVM.

Acknowledgements

CB is grateful to Department of CSE, Chalmers University of Technology for their hospitality and
was supported by grants from ICT and Transport Areas of Advance, Chalmers University. VJ and
DD were supported by SSF grant Data Driven Secure Business Intelligence.

2ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/clique/
3http://asi.insa-rouen.fr/enseignants/˜arakotom/code/mklindex.html
4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/

8

References
[1] Louigi Addario-berry, Nicolas Broutin, Gbor Lugosi, and Luc Devroye. Combinatorial testing
problems. Annals of Statistics, 38:3063–3092, 2010.
[2] Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a
random graph. Random Structures and Algorithms, pages 457–466, 1998.
[3] B. Bollob ´as. Modern graph theory, volume 184. Springer Verlag, 1998.
[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
New York, NY, USA, 2004.
[5] T.-H. Hubert Chan, Kevin L. Chang, and Rajiv Raman. An sdp primal-dual algorithm for
approximating the lov ´asz-theta function. In ISIT, 2009.
[6] Amin Coja-Oghlan and Anusch Taraz. Exact and approximative algorithms for coloring g(n,
p). Random Struct. Algorithms, 24(3):259–278, 2004.
[7] U. Feige and D. Ron. Finding hidden cliques in linear time. In AofA10, 2010.
[8] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semi-
random graph. Random Struct. Algorithms, 16:195–208, March 2000.
[9] Z. F ¨uredi and J. Koml ´os. The eigenvalues of random symmetric matrices. Combinatorica,
1:233–241, 1981.
[10] Michel X. Goemans. Semideﬁnite programming in combinatorial optimization. Math. Pro-
gram., 79:143–161, 1997.
[11] J. H ˚astad. Clique is hard to approximate within n1−ε . Acta Mathematica, 182(1):105–142,
1999.
[12] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[13] Don R. Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. Qp algorithms with guaranteed
accuracy and run time for support vector machines. Journal of Machine Learning Research,
7:733–769, 2006.
[14] D. Jiang and J. Pei. Mining frequent cross-graph quasi-cliques. ACM Transactions on Knowl-
edge Discovery from Data (TKDD), 2(4):16, 2009.
[15] D.S. Johnson and M.A. Trick. Cliques, coloring, and satisﬁability: second DIMACS imple-
mentation challenge, October 11-13, 1993, volume 26. Amer Mathematical Society, 1996.
[16] Donald Knuth. The sandwich theorem. Electronic Journal of Combinatorics, 1(A1), 1994.
[17] Michael Krivelevich and Benny Sudakov. Pseudo-random graphs. In More Sets, Graphs and
Numbers, volume 15 of Bolyai Society Mathematical Studies, pages 199–262. Springer Berlin
Heidelberg, 2006.
[18] V.E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph
discovery. Managing and Mining Graph Data, pages 303–336, 2010.
[19] L. Lovasz. On the Shannon capacity of a graph. Information Theory, IEEE Transactions on,
25(1):1–7, 1979.
[20] C.J. Luz and A. Schrijver. A convex quadratic characterization of the lov ´asz theta number.
SIAM Journal on Discrete Mathematics, 19(2):382–387, 2006.
[21] Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’10, pages
712–728, Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics.
[22] P. Pardalos and S. Rebennack. Computational challenges with cliques, quasi-cliques and clique
partitions in graphs. Experimental Algorithms, pages 13–22, 2010.
[23] V. Spirin and L.A. Mirny. Protein complexes and functional modules in molecular networks.
Proceedings of the National Academy of Sciences, 100(21):12123, 2003.
[24] Dekel Yael, Gurel-Gurevich Ori, and Peres Yuval. Finding hidden cliques in linear time with
high probability. In ANALCO11, 2011.

9


Online allocation and homogeneous partitioning for
piecewise constant mean-approximation

Odalric Ambrym Maillard
Montanuniversit ¨at Leoben
Franz-Josef Strasse 18
A-8700 Leoben, Austria
odalricambrym.maillard@gmail.com

Alexandra Carpentier
Statistical Laboratory, CMS
Wilberforce Road, Cambridge
CB3 0WB UK
a.carpentier@statslab.cam.ac.uk

Abstract

In the setting of active learning for the multi-armed bandit, where the goal of a
learner is to estimate with equal precision the mean of a ﬁnite number of arms,
recent results show that it is possible to derive strategies based on ﬁnite-time con-
ﬁdence bounds that are competitive with the best possible strategy. We here con-
sider an extension of this problem to the case when the arms are the cells of a
ﬁnite partition P of a continuous sampling space X ⊂ Rd . Our goal is now to
build a piecewise constant approximation of a noisy function (where each piece is
one region of P and P is ﬁxed beforehand) in order to maintain the local quadratic
error of approximation on each cell equally low. Although this extension is not
trivial, we show that a simple algorithm based on upper conﬁdence bounds can
be proved to be adaptive to the function itself in a near-optimal way, when |P | is
chosen to be of minimax-optimal order on the class of α−H ¨older functions.

1 Setting and Previous work
Let us consider some space X ⊂ Rd , and Y ⊂ R. We call X the input space or sampling space, Y
the output space or value space. We consider the problem of estimating with uniform precision the
function f : X ⊂ Rd → Y ⊂ R. We assume that we can query n times the function f , anywhere in
the domain, and observe noisy samples of this function. These samples are collected sequentially,
and our aim is to design an adaptive procedure that selects wisely where on the domain to query the
function, according to the information provided by the previous samples. More formally:
Observed process We consider an unknown Y -valued process deﬁned on X , written ν : X →
M+
1 (Y ), where M+
1 (Y ) refers to the set of all probability measures on Y , such that for all x ∈ X ,
the random variable Y (x) ∼ ν (x) has mean f (x) def= E[Y (x)|x] ∈ R. We write for convenience the
model in the following way
Y (x) = f (x) + noise(x) ,
where noise(x) def= Y (x) − E[Y (x)|x] is the centered random variable corresponding to the noise,
with unknown variance σ2 (x). We assume throughout this paper that f is α-H ¨older.
Partition We consider we can deﬁne a partition P of the input space X , with ﬁnitely many P
regions {Rp }1≤p≤P that are assumed to be convex and not degenerated, i.e. such that the interior
of each region Rp has positive Lebesgue volume vp . Moreover, with each region Rp is associated
a sampling distribution in that region, written µp ∈ M+
1 (Rp ). Thus, when we decide to sample in
region Rp , a new sample X ∈ Rp is generated according to X ∼ µp .
Allocation. We consider that we have a ﬁnite budget of n ∈ N samples that we can use in order
to allocate samples as we wish among the regions {Rp }1≤p≤P . For illustration, let us assume that
we deterministically allocate Tp,n ∈ N samples in region Rp , with the constraint that the alloca-
tion {Tp,n }1≤p≤P must some to n. In region Rp , we thus sample points {Xp,i }1≤p≤P at random

1

ˆfn (x) =

according to the sampling distribution µp , and then get the corresponding values {Yp,i }1≤i≤Tp,n ,
where Yp,i ∼ ν (Xp,i ). In the sequel, the distribution µp is assumed to be the uniform distribution
over region Rp , i.e. the density of µp is dλ(x)1x∈Rp
where λ denotes the Lebesgue measure. Note
λ(Rp )
that this is not restrictive since we are in an active, not passive setting.
Piecewise constant mean-approximation. We use the collected samples in order to build a piece-
wise constant approximation ˆfn of the mean f , and measure the accuracy of approximation on a
region Rp with the expected quadratic norm of the approximation error, namely
λ(Rp ) � = Eµp ,ν �(f (X ) − ˆmp,n )2 � ,
E� �Rp
(f (x) − ˆfn (x))2 λ(dx)
where ˆmp,n is the constant value that takes ˆfn on the region Rp . A natural choice for the estimator
ˆmp,n is to use the empirical mean that is unbiased and asymptotically optimal for this criterion.
Thus we consider the following estimate (histogram)
Tp,n�i=1
P�p=1
ˆmp,n I{x ∈ Rp } where ˆmp,n =
Pseudo loss Note that, since the Tp,n are deterministic, the expected quadratic norm of the approxi-
mation error of this estimator can be written in the following form
Eµp ,ν �(f (X ) − ˆmp,n )2 � = Eµp ,ν �(f (X ) − Eµp [f (X )])2 � + Eµp ,ν �(Eµp [f (X )] − ˆmp,n )2 �
= Vµp �f (X )� + Vµp ,ν � ˆmp,n �
= Vµp �f (X )� +
Vµp ,ν �Y (X )� .
1
Tp,n
Now, using the following immediate decomposition
Vµp ,ν �Y (X )� = Vµp �f (X )� + �Rp
σ2 (x)µp (dx) ,
we deduce that the maximal expected quadratic norm of the approximation error over the regions
{Rp }1≤p≤P , that depends on the choice of the considered allocation strategy A def= {Tp,n }1≤p≤P
is thus given by the following so-called pseudo-loss
Eµp �σ2 (X )��.
1≤ p ≤P � Tp,n + 1
Vµp �f (X )� +
Ln (A) def= max
Tp,n
Our goal is to minimize this pseudo-loss. Note that this is a local measure of performance, as
opposed to a more usual yet less challenging global quadratic error. Eventually, as the number of
cells tends to ∞, this local measure of performance approaches supx∈X Eν ��f (x) − ˆfn (x)�2 �. At
this point, let us also introduce, for convenience, the notation Qp (Tp,n ) that denotes the term inside
the max, in order to emphasize the dependency on the quadratic error with the allocation.
Previous work
There is a huge literature on the topic of functional estimation in batch setting. Since it is a rather
old and well studied question in statistics, many books have been written on this topic, such as Bosq
and Lecoutre [1987], Rosenblatt [1991], Gy ¨orﬁ et al. [2002], where piecewise constant mean-
approximation are also called “partitioning estimate” or “regressogram” (ﬁrst introduced by Tukey
[1947]). The minimax-optimal rate of approximation on the class of α-H ¨older functions is known
to be in O(n− 2α
2α+d ) (see e.g. Ibragimov and Hasminski [1981], Stone [1980], Gy ¨orﬁ et al. [2002]).
In such setting, a dataset {(Xi , Yi )}i≤n is given to the learner, and a typical question is thus to try
to ﬁnd the best possible histogram in order to minimize a approximation error. Thus the dataset is
ﬁxed and we typically resort to techniques such as model selection where each model corresponds
to one histogram (see Arlot [2007] for an extensive study of such).
However, we here ask a very different question, that is how to optimally sample in an online setting
in order to minimize the approximation error of some histogram. Thus we choose the histogram

1
Tp,n

1
Tp,n

Yp,i .

(1)

2

before we see any sample, then it is ﬁxed and we need to decide which cell to sample from at
each time step. Motivation for this setting comes naturally from some recent works in the setting
of active learning for the multi-armed bandit problem Antos et al. [2010], Carpentier et al. [2011].
In these works, the objective is to estimate with equal precision the mean of a ﬁnite number of
distributions (arms), which would correspond to the special case when X = {1, . . . , P } is a ﬁnite
set in our setting. Intuitively, we reduce the problem to such bandit problem with ﬁnite set of arms
(regions), and our setting answers the question whether it is possible to extend those results to the
case when the arms do not correspond to a singleton, but rather to a continuous region. We show
that the answer is positive, yet non trivial. This is non trivial due to the variance estimation in
each region: points x in some region may have different means f(x), so that standard estimators for
the variance are biased, contrary to the point-wise case and thus ﬁnite-arm techniques may yield
disastrous results. (Estimating the variance of the distribution in a continuous region actually needs
to take into account not only the point-wise noise but also the variation of the function f and the
noise level σ2 in that region.) We describe a way, inspired from quasi Monte-Carlo techniques, to
correct this bias so that we can handle the additional error. Also, it is worth mentioning that this
setting can be informally linked to a notion of curiosity-driven learning (see Schmidhuber [2010],
Baranes and Oudeyer [2009]), since we want to decide in which region of the space to sample,
without explicit reward but optimizing the goal to understand the unknown environment.
Outline Section 2 provides more intuition about the pseudo-loss and a result about the optimal or-
acle strategy when the domain is partitioned in a minimax-optimal way on the class of α−H ¨older
functions. Section 3 presents our assumptions, that are basically to have a sub-Gaussian noise and
smooth mean and variance functions, then our estimator of the pseudo-loss together with its con-
centration properties, before introducing our sampling procedure, called OAHPA-pcma. Finally, the
performance of this procedure is provided and discussed in Section 4.
2 The pseudo-loss: study and optimal strategies
2.1 More intuition on each term in the pseudo-loss
It is natural to look at what happens to each of the two terms that appear in equation 1 when one
makes Rp shrink towards a point. More precisely, let xp be the mean of X ∼ µp and let us look at
the limit of Vµp (f (X )) when vp goes to 0. Assuming that f is differentiable, we get
Eµp ��f (X ) − f (xp ) − E[f (X ) − f (xp )]�2 �
Vµp (f (X )) = lim
lim
vp→0
vp→0
Eµp ���X − xp , ∇f (xp )� − E[�X − xp , ∇f (xp )�]�2 �
= lim
vp→0
Eµp ��X − xp , ∇f (xp )�2 �
= lim
vp→0
vp→0 ∇f (xp )T Eµp �(X − xp )(X − xp )T �∇f (xp ) .
= lim
Therefore, if we introduce Σp to be the covariance matrix of the random variable X ∼ µp , then we
vp→0 ||∇f (xp )||2
simply have lim
Σp .
Vµp (f (X )) = lim
vp→0
Example with hyper-cubic regions An important example is when Rp is a hypercube with side
length v1/d
and µp is the uniform distribution over the region Rp . In that case (see Lemma 1), we
p
dx
have µp (dx) =
, and
Σp = ||∇f (xp )||2 v2/d
vp
p
||∇f (xp )||2
.
12
More generally, when f is α−differentiable, i.e. that ∀a ∈ X , ∃∇α f (a, ·) ∈ Sd (0, 1)R such that
f (a+hx)−f (a)
= ∇α f (a, x), then it is not too difﬁcult to show that for such
∀x ∈ Sd (0, 1), limh→0
hα
hyper-cubic regions, we have
S(0,1) |∇α f (xp , u)|2�.
Vµp �f (X )� = O�v
2α
sup
d
p
On the other hand, by direct computation, the second term is such that limvp→0 Eµp �σ2 (X )� =
σ2 (xp ). Thus, while Vµp �f (X )� vanishes, Eµp �σ2 (X )� stays bounded away from 0 (unless ν is
deterministic).
3

= n .

γ def=

2.2 Oracle allocation and homogeneous partitioning for piecewise constant
mean-approximation.
We now assume that we are allowed to choose the partition P depending on n, thus P = Pn ,
amongst all homogeneous partitions of the space, i.e. partitions such that all cells have the same
volume, and come from a regular grid of the space. Thus the only free parameter is the number of
cells Pn of the partition.
An exact yet not explicit oracle algorithm. The minimization of the pseudo-loss (1) does not yield
to a closed-form solution in general. However, we can still derive the order of the optimal loss
(see [Carpentier and Maillard, 2012, Lemma 2] in the full version of the paper for an example of
minimax yet non adaptive oracle algorithm given in closed-form solution):
Lemma 1 In the case when Vµp �f (X )� = Ω�P −α�
σ2 (x)µp (dx) = Ω�P −β �
� and �Rp
�, then an
n
n
n satisﬁes that
optimal allocation and partitioning strategy A�
Vµp �f (X )� + Eµp �σ2 (X )�
1
def=
T �
P �
and
L − Vµp �f (X )�
max(1+α�−β � ,1) )
,
n = Ω(n
p,n
as soon as there exists, for such range of P �
n , a constant L such that
Vµp �f (X )� + Eµp �σ2 (X )�
P �
n�p=1
L − Vµp �f (X )�
n , optimal amongst the allocations strategies that use the
The pseudo-loss of such an algorithm A�
partition Pn in P �
n regions, is then given by
max(1 − β � , 1 − α� )
n ) = Ω�nγ � where
Ln (A�
max(1 + α� − β � , 1) − 1 .
The condition involving the constant L is here to ensure that the partition is not degenerate. It is
morally satisﬁed as soon as the variance of f and the noise are bounded and n is large enough.
This Lemma applies to the important class W 1,2 (R) of functions that admit a weak derivative that
Indeed these functions are H ¨older with coefﬁcient α = 1/2, i.e. we have
belongs to L2 (R).
W 1,2 (R) ⊂ C 0,1/2 (R). The standard Brownian motion is an example of function that is 1/2-H ¨older.
More generally, for k = d
2 + α with α = 1/2 when d is odd and α = 1 when d is even, we have the
inclusion
W k,2 (Rd ) ⊂ C 0,α (Rd ) ,
where W k,2 (Rd ) is the set of functions that admit a k th weak derivative belonging to L2 (Rd ). Thus
the previous Lemma applies to sufﬁciently smooth functions with smoothness linearly increasing
with the dimension d of the input space X .
Important remark Note that this Lemma gives us a choice of the partition that is minimax-optimal,
and an allocation strategy on that partition that is not only minimax-optimal but also adaptive to the
function f itself. Thus it provides a way to decide in a minimax way what is the good number of
regions, and then to provide the best oracle way to allocate the budget.
We can deduce the following immediate corollary on the class of α−H ¨older functions observed in a
non-negligible noise of bounded variance (i.e. in the setting β � = 0 and α� = 2α
d ).
Corollary 1 Consider that f is α−H ¨older and the noise is of bounded variance. Then a minimax-
d
optimal partition satisﬁes P �
d+2α ) and an optimal allocation achieves the rate Ln (A�
n = Ω(n
n ) =
Ω�n −2α
d+2α �. Moreover, the strategy of Lemma 1 is optimal amongst the allocations strategies that
use the partition Pn in P �
n regions.
The rate Ω�n −2α
d+2α � is minimax-optimal on the class of α−H ¨older functions (see Gy ¨orﬁ et al. [2002],
Ibragimov and Hasminski [1981], Stone [1980]), and it is thus interesting to consider an initial num-
d
d+2α ). After having built the partition, if the quantities
n that is of order P �
ber of regions P �
n = Ω(n
�Vµp �f ��p≤P and �Eµp �σ2 ��p≤P are known to the learner, it is optimal, in the aim of minimizing
p,n provided in Lemma 1. Our
the pseudo-loss, to allocate to each region the number of samples T �
objective in this paper is, after having chosen beforehand a minimax-optimal partition, to allocate

4

the samples properly in the regions, without having any access to those quantities. It is then neces-
sary to balance between exploration, i.e. allocating the samples in order to estimate �Vµp �f ��p≤P
and �Eµp �σ2 ��p≤P , and exploitation, i.e. use the estimates to target the optimal allocation.
3 Online algorithms for allocation and homogeneous partitioning for
piecewise constant mean-approximation
In this section, we now turn to the design of algorithms that are fully online, with the goal to be
competitive against the kind of oracle algorithms considered in Section 2.2. We now assume that the
space X = [0, 1]d is divided in Pn hyper-cubic regions of same measure (the Lebesgue measure on
[0, 1]d ) vp = v = 1
. The goal of an algorithm is to minimize the quadratic error of approximation
Pn
of f by a constant over each cell, in expectation, which we write as
λ(Rp ) � = max
λ(Rp ) � ,
E� �Rp
E� �Rp
(f (x) − ˆmp,n )2 λ(dx)
(f (x) − ˆfn (x))2 λ(dx)
max
1≤p≤Pn
1≤p≤Pn
where ˆfn is the histogram estimate of the function f on the partition P and ˆmp,n is the empirical
mean deﬁned on region Rp with the samples (Xi , Yi ) such that Xi ∈ Rp . To do so, an algorithm is
only allowed to specify at each time step t, the next point Xt where to sample, based on all the past
samples {(Xs , Ys )}s<t . The total budget n is known at the beginning as well as Pn and the regions
{Rp }1≤p≤Pn .
We want to compare the strategy of an online learning algorithm to the strategy of an oracle that
perfectly knows the law ν . We however restrict the power of the oracle by forcing it to only sample
uniformly inside a region Rp . Thus the oracle is only allowed to choose at each time step t in which
cell Rp to sample, but is not allowed to decide which point in the cell it can sample. The point Xt
has to be sampled uniformly in Rp .
Now, since a learning algorithm has no access to the true distribution ν , we give slightly more power
to the learning algorithm by allowing it to resort to a reﬁned partition. We allow it to divide each
region Rp for p ∈ {1, . . . , Pn } into K hyper-cubic sub-regions {Rp,k }1≤k≤K of same Lebesgue
def= K Pn of hyper-cubic regions of same measure vp,k =
measure, resulting in a total number P +
n
. Equivalently, this can be seen as letting the player use a reﬁned partition with P +
n cells.
1
KPn
However, instead of sampling one point in Rp,k , the algorithm is only allowed to sample all the K
points in region in the chosen Rp at the same time, one uniformly in each sub-region Rp,k , still
using of course the same total budget of n points (and not nK ). Thus the algorithm is free to choose
K , but once a region Rp is chosen at time t, it can not choose moreover which point to sample inside
that region but only sample a set of points in one shot. The reason to do so is that this will allow
us to estimate the unknown quantities such as the quadratic variation of f on each region, but we
do not want to give the learner too much power. This one shot restriction is also for clarity purpose,
as otherwise one has to consider technical details and perform nasty computations that in the end
only affects second order terms. The effect of the factor K on the performance bound can be seen in
Section 4. For Pn of minimax order, our result shows that K can be chosen to be a (large) constant.
3.1 Assumptions
In order to derive performance bounds for a learning algorithm that does not know the noise and
the local variance of the function, we now need some assumptions on the data. These are here to
ensure that concentration properties apply and that empirical moments are close to true moments
with high probability depending on the number of samples. These add to the two other assumptions
on the structure of the histograms (uniformed grid partitions) and on the active scheme (that is we
can choose a bean but only get a random sample uniformly distributed in that bean).
We assume that ν is exactly sub-Gaussian, meaning that for all x ∈ X , the variance of the noise(x),
written σ2 (x) < ∞ satisﬁes that
λ2σ2 (x)
∀λ ∈ R+ log E exp[λ noise(x)] ≤
,
2
and we further assume that it satisﬁes the following slightly stronger second property (that is for
instance exactly veriﬁed for a Gaussian variable, looking at the moment generating function):
log �1 − 2γσ2 (x)� .
∀λ, γ ∈ R+ log E exp �λnoise(x) + γ noise(x)2 � ≤
λ2σ2 (x)
1
2(1 − 2γσ2 (x)) −
2
5

def=

.

T �
p,n

The function f is assumed to be (L, α)-H ¨older, meaning that it satiﬁes
∀x, x� ∈ X f (x) − f (x� ) ≤ L||x − x� ||α .
Similarly, the function σ2 is assumed to be (M , β )-H ¨older i.e. it satisﬁes
∀x, x� ∈ X σ2 (x) − σ2 (x� ) ≤ M ||x − x� ||β .
We assume that Y is a convex and compact subset of R, thus w.l.g. that it is [0, 1], and that it is
known that ||σ2 ||∞ , which is thus ﬁnite, is bounded by the constant 1.
3.2 Empirical estimation of the quadratic approximation error on each cell
We deﬁne the sampling distribution ˜µp in the region Rp for each p ∈ {1, . . . , Pn } as a quasi-uniform
sampling scheme using the uniform distribution over the sub-regions. More precisely at time t ≤ n,
if we decide to sample in the region Rp according to ˜µp , we sample uniformly in each sub-region
one sample, resulting in a new batch of samples {(Xt,k , Yt,k )}1≤k≤K , where Xt,k ∼ µp,k . Note that
due to this sampling process, the number of points Tp,t sampled in sub-region Rp at time t is always
a multiple of K and that moreover for all k , k � ∈ {1, . . . , K } we have that Tp,k,t = Tp,k� ,t = Tp,t
K .
Now this speciﬁc sampling is used in order to be able to estimate the variances Vµp f and Eµp σ2 ,
p,n can be computed as accurately as possible. Indeed, as explained in
so that the best proportions T �
Lemma 1, we have that
Vµp �f (X )� + Eµp �σ2 (X )�
L − Vµp �f (X )�
Variance estimation We now introduce two estimators. The ﬁrst estimator is written ˆVp,t and is
def=
built in the following way. First,let us introduce the empirical estimate ˆfp,k,t of the mean fp,k
Eµp,k �f (X )� of f in sub-region Rp,k . Similarly, to avoid some cumbersome notations, we introduce
def= Eµp,k �σ2 (X )�
def= Vµp,k �f (X )� for the function f , and then σ2
def= Eµp �f (X )� and vp,k
fp
p,k
for the variance of the noise σ2 . We now deﬁne the empirical variance estimator to be
K�k=1
1
( ˆfp,k,t − ˆmp,t )2 ,
ˆVp,t =
K − 1
that is a biased estimator. Indeed, for a deterministic Tp,t , it is not difﬁcult to show that we have
K�k=1 �Eµp,k �f � − Eµp �f ��2
E� ˆVp,t � =
K�k=1 �Vµp,k �f � + Eµp,k �σ2 �� .
1
1
+
K − 1
Tp,t
The leading term in this decomposition, that is given by the ﬁrst sum, is closed to Vµp �f � since, by
using the assumption that f is (L, α)−H ¨older, we have the following inequality
− Vµp �f (X )����� ≤
����
K�k=1 �Eµp,k �f � − Eµp �f ��2
2L2dα
1
(K Pn )2α/d ,
K
where we also used that the diameter of a sub-region Rp,k is given by diam(Rp,k ) = d1/2
(KPn )1/d .
Then, the second term also contributes to the bias, essentially due to the fact that V[ ˆfp,k,t ] =
def= Eµp �σ2 (X )�).
def= Vµp �f (X )� and σ2
p,k ) and not
k ) (with vp
1
1
(vp,k + σ2
(vk + σ2
p
Tp,k,t
Tp,t
In order to correct this term, we now introduce the second estimator ˆσ2
p,k,t that estimates the variance
of the outputs in a region Rp,k , i.e. Vµp,k ,ν �Y (X )� = Vµp,k �f (X )� + Eµp,k �σ2 �. It is deﬁned as
t�i=1 �Yi −
Yj I{Xj ∈ Rp,k }�2
t�j=1
1
1
def=
ˆσ2
I{Xi ∈ Rp,k } .
p,k,t
Tp,k,t − 1
Tp,k,t
Now, we combine the two previous estimators to form the following estimator
Tp,t � ˆσ2
K�k=1 � 1
1
ˆQp,t = ˆVp,t −
Tp,k,t −
p,k,t .
The following proposition provides a high-probability bound on the difference between ˆQp,t and
the quantity we want to estimate. We report the detailed proof in [Carpentier and Maillard, 2012].

1
K

6

Proposition 1 By the assumption that f is (L, α)-H ¨older, the bias of the estimator ˆQp,t , and for
deterministic Tp,t , is given by
K�k=1 �Eµp,k �f � − Eµp �f ��2
E� ˆQp,t − Qp (Tp,t )� =
− Vµp �f (X )� ≤
2L2dα
1
(K Pn )2α/d .
K
Moreover, it satisﬁes that for all δ ∈ [0, 1], there exists an event of probability higher than 1 − δ such
that on this event, we have
Tp,k,t√K ���� 1
ˆQp,t − E� ˆQp,t � ���� ≤ ���� 8 log(4/δ)
����
+ o�
p,k� .
K�k=1
K�k=1
ˆσ2
1
p,k,t
σ2
T 2
(K − 1)2
K
p,k,t
We also state the following Lemma that we are going to use in the analysis, and that takes into
account randomness of the stopping times Tp,k,t .
Lemma 2 Let {Xp,k,u }p≤P, k≤K, u≤n be samples potentially sampled in region Rp,k . We introduce
qp,u to be the equivalent of Qp (Tp,t ) with explicitly ﬁxed value of Tp,t = u. Let also ˆqp,u be the
estimate of E�qp,u �, that is to say the equivalent of ˆQp,t but computed with the ﬁrst u samples in
each region Rp,k (i.e. Tp,t = u). Let us deﬁne the event
u � log(4nP /δ ) ˆVp,t
ξn,P,K (δ ) = �p≤P �u≤n �ω : ��� ˆqp,u (ω) − E�qp,u � ��� ≤
(K Pn )2α/d �,
2L2dα
AK
+
K − 1
K−1 �K
where ˆVp,t = ˆVp (Tp,t ) = 1
k=1 ˆσ2
p,k,t and where A ≤ 4 is a numerical constant. Then it
holds that
P�ξn,P,K (δ )� ≥ 1 − δ .
Note that, with the notations of this Lemma, Proposition 1 above is thus about ˆqp,u .
3.3 The Online allocation and homogeneous partitioning algorithm for piecewise constant
mean-approximation (OAHPA-pcma)
We are now ready to state the algorithm that we propose for minimizing the quadratic error of ap-
proximation of f . The algorithm is described in Figure 1. Although it looks similar, this algorithm is
quite different from a normal UCB algorithm since ˆQp,t decreases in expectation with Tp,t . Indeed,
its expectation is close to Vµp �f � + 1
k=1 �Vµp,k �f � + Eµp,k �σ2 ��.
K Tp,t �K
Algorithm 1 OAHPA-pcma.
1: Input: A, L, α, Horizon n; Partition {Rp }p≤P , with sub-partitions {Rp,k }k≤K .
2: Initialization: Sample K points in every sub-region {Rp,k }p≤P,k≤K
3: for t = K 2P + 1; t ≤ n; t = t + K do
Compute ∀p, ˆQp,t .
4:
Tp,t � log(4nP /δ) ˆVp,t
+ 2L2 dα
Compute ∀p, Bp,t = ˆQp,t + AK
(KPn )2α/d .
5:
K−1
Select the region pt = argmax1≤p≤Pn Bp,t where to sample.
6:
Sample K samples in region Rpt one per sub-region Rpt ,k according to µpt ,k .
7:
8: end for

4 Performance of the allocation strategy and discussion
Here is the main result of the paper; see the full version [Carpentier and Maillard, 2012] for the
proof. We remind that the objective is to minimize for an algorithm A the pseudo-loss Ln (A).
maxp T �
Theorem 1 (Main result) Let γ =
be the distortion factor of the optimal allocation strat-
p,n
minp T �
p,n
def= n
d
2α+d �2+ d
2α , and of the
egy, and let � > 0. Then with the choice of the number of regions Pn
number of sub-regions K def= C
α , where C def= 8L2α
2d
4α+d �−2− d
Ad1−α then the pseudo-loss of the OAHPA-
pcma algorithm satisﬁes, under the assumptions of Section 3.1 and on an event of probability higher
than 1 − δ ,
Ln (A) ≤ �1 + �γC ��log(1/δ)�Ln (A�
n ) + o�n− 2α
2α+d � ,
for some numerical constant C � not depending on n, where A�
n is the oracle of Lemma 1.

7

Minimax-optimal partitioning and �-adaptive performance Theorem 1 provides a high proba-
bility bound on the performance of the OAHPA-pcma allocation strategy. It shows that this perfor-
mance is competitive with that of an optimal (i.e. adaptive to the function f , see Lemma 1) allocation
d
A� on a partition with a number of cells Pn chosen to be of minimax order n
2α+d for the class of
2α
α-H ¨older functions. In particular, since Ln (A�
d+2α ) on that class, we recover the same
n ) = O(n
minimax order as what is obtained in the batch learning setting, when using for instance wavelets,
or Kernel estimates (see e.g. Stone [1980], Ibragimov and Hasminski [1981]). But moreover, due to
n to the function itself, this procedure is also �-adaptive to the function and not
the adaptivity of A�
only minimax-optimal on the class, on that partition (see Section 2.2). Naturally, the performance of
the method increases, in the same way than for any classical functional estimation method, when the
smoothness of the function increases. Similarly, in agreement with the classical curse of dimension,
the higher the dimension of the domain, the less efﬁcient the method.
Limitations
In this work, we assume that the smoothness α of the function is available to the
learner, which enables her to calibrate Pn properly. Now it makes sense to combine the OAHPA-
pcma procedure with existing methods that enable to estimate this smoothness online (under a
slightly stronger assumption than H ¨older, such as H ¨older functions that attain their exponents,
see Gin ´e and Nickl [2010]). It is thus interesting, when no preliminary knowledge on the smoothness
of f is available, to spend some of the initial budget in order to estimate α.
We have seen that the OAHPA-pcma procedure, although very simple, manages to get minimax
optimal results. Now the downside of the simplicity of the OAHPA-pcma strategy is two-fold.
The ﬁrst limitation is that the factor (1 + �γC ��log(1/δ)) = (1 + O(�)) appearing in the bound
before Ln (A� ) is not 1, but higher than 1. Of course it is generally difﬁcult to get a constant 1 in
the batch setting (see Arlot [2007]), and similarly this is a difﬁcult task in our online setting too: If
� is chosen to be small, then the error with respect to the optimal allocation is small. However, since
Pn is expressed as an increasing function of �, this implies that the minimax bound on the loss for
partition P increases also with �. That said, in the view of the work on active learning multi-armed
bandit that we extend, we would still prefer to get the optimal constant 1.
The second limitation is more problematic: since K is chosen irrespective of the region Rp , this
causes the presence of the factor γ . Thus the algorithm will essentially no longer enjoy near-optimal
performance guarantees when the optimal allocation strategy is highly not homogeneous.
Conclusion and future work In this paper, we considered online regression with histograms in
an active setting (we select in which bean to sample), and when we can choose the histogram in a
class of homogeneous histograms. Since the (unknown) noise is heteroscedastic and we compete
not only with the minimax allocation oracle on α-H ¨older functions but with the adaptive oracle
that uses a minimax optimal histogram and allocates samples adaptively to the target function, this
is an extremely challenging (and very practical) setting. Our contribution can be seen as a non
trivial extension of the setting of active learning for multi-armed bandits to the case when each arm
corresponds to one continuous region of a sampling space, as opposed to a singleton, which can also
be seen as a problem of non parametric function approximation. This new setting offers interesting
challenges: We provided a simple procedure, based on the computation of upper conﬁdence bounds
of the estimation of the local quadratic error of approximation, and provided a performance analysis
that shows that OAHPA-pcma is ﬁrst order �-optimal with respect to the function, for a partition
chosen to be minimax-optimal on the class of α-H ¨older functions. However, this simplicity also
has a drawback if one is interested in building exactly ﬁrst order optimal procedure, and going
beyond these limitations is deﬁnitely not trivial: A more optimal but much more complex algorithm
would indeed need to tune a different factor Kp in each cell in an online way, i.e. deﬁne some Kp,t
that evolves with time, and redeﬁne sub-regions accordingly. Now, the analysis of the OAHPA-pcma
already makes use of powerful tools such as empirical-Bernstein bounds for variance estimation (and
not only for mean estimation), which make it non trivial; in order to handle possibly evolving sub-
regions and deal with the progressive reﬁnement of the regions, we would need even more intricate
analysis, due to the fact that we are online and active. This interesting next step is postponed to
future work.
Acknowledgements This research was partially supported by Nord-Pas-de-Calais Regional Coun-
cil, French ANR EXPLO-RA (ANR-08-COSI-004), the European Communitys Seventh Framework
Programme (FP7/2007-2013) under grant agreement no 270327 (CompLACS) and no 216886 (PAS-
CAL2).

8

References
Andr `as Antos, Varun Grover, and Csaba Szepesv `ari. Active learning in heteroscedastic noise. The-
oretical Computer Science, 411(29-30):2712–2728, 2010.
Sylvain Arlot. R ´e ´echantillonnage et S ´election de mod `eles. PhD thesis, Universit ´e Paris Sud - Paris
XI, 2007.
A. Baranes and P.-Y. Oudeyer. R-IAC: Robust Intrinsically Motivated Exploration and Active Learn-
ing. IEEE Transactions on Autonomous Mental Development, 1(3):155–169, October 2009.
D. Bosq and J.P. Lecoutre. Th ´eorie de l’estimation fonctionnelle, volume 21. Economica, 1987.
Online allocation and homoge-
Alexandra Carpentier and Odalric-Ambrym Maillard.
neous partitioning for piecewise constant mean-approximation.
HAL, 2012.
URL
http://hal.archives-ouvertes.fr/hal-00742893.
Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, Rmi Munos, and Peter Auer.
Upper-conﬁdence-bound algorithms for active learning in multi-armed bandits. In Jyrki Kivinen,
Csaba Szepesv `ari, Esko Ukkonen, and Thomas Zeugmann, editors, Algorithmic Learning Theory,
volume 6925 of Lecture Notes in Computer Science, pages 189–203. Springer Berlin / Heidelberg,
2011.
E. Gin ´e and R. Nickl. Conﬁdence bands in density estimation. The Annals of Statistics, 38(2):
1122–1170, 2010.
L. Gy ¨orﬁ, M. Kohler, A. Krzy ´zak, and Walk H. A distribution-free theory of nonparametric regres-
sion. Springer Verlag, 2002.
I. Ibragimov and R. Hasminski. Statistical estimation: Asymptotic theory. 1981.
M. Rosenblatt. Stochastic curve estimation, volume 3. Inst of Mathematical Statistic, 1991.
J. Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (19902010). Autonomous
Mental Development, IEEE Transactions on, 2(3):230–247, 2010.
C.J. Stone. Optimal rates of convergence for nonparametric estimators. The annals of Statistics,
pages 1348–1360, 1980.
J.W. Tukey. Non-parametric estimation ii. statistically equivalent blocks and tolerance regions–the
continuous case. The Annals of Mathematical Statistics, 18(4):529–539, 1947.

9


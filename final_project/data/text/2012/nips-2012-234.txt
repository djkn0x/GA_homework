Structure estimation for discrete graphical models:
Generalized covariance matrices and their inverses

Po-Ling Loh
Department of Statistics
University of California, Berkeley
Berkeley, CA 94720
ploh@berkeley.edu

Martin J. Wainwright
Departments of Statistics and EECS
University of California, Berkeley
Berkeley, CA 94720
wainwrig@stat.berkeley.edu

Abstract

We investigate a curious relationship between the structure of a discrete graphical
model and the support of the inverse of a generalized covariance matrix. We show
that for certain graph structures, the support of the inverse covariance matrix of
indicator variables on the vertices of a graph reﬂects the conditional independence
structure of the graph. Our work extends results that have previously been es-
tablished only in the context of multivariate Gaussian graphical models, thereby
addressing an open question about the signiﬁcance of the inverse covariance ma-
trix of a non-Gaussian distribution. Based on our population-level results, we
show how the graphical Lasso may be used to recover the edge structure of cer-
tain classes of discrete graphical models, and present simulations to verify our
theoretical results.

1

Introduction

Graphical model inference is now prevalent in many ﬁelds, running the gamut from computer vision
and civil engineering to political science and epidemiology.
In many applications, learning the
edge structure of an underlying graphical model is of great importance—for instance, a graphical
model may be used to represent friendships between people in a social network, or links between
organisms with the propensity to spread an infectious disease [1]. It is well known that zeros in the
inverse covariance matrix of a multivariate Gaussian distribution indicate the absence of an edge
in the corresponding graphical model. This fact, combined with techniques in high-dimensional
statistical inference, has been leveraged by many authors to recover the structure of a Gaussian
graphical model when the edge set is sparse (e.g., see the papers [2, 3, 4, 5] and references therein).
Recently, Liu et al. [6, 7] introduced the notion of a nonparanormal distribution, which generalizes
the Gaussian distribution by allowing for univariate monotonic transformations, and argued that the
same structural properties of the inverse covariance matrix carry over to the nonparanormal.
However, the question of whether a relationship exists between conditional independence and the
structure of the inverse covariance matrix in a general graph remains unresolved. In this paper, we
focus on discrete graphical models and establish a number of interesting links between covariance
matrices and the edge structure of an underlying graph. Instead of only analyzing the standard co-
variance matrix, we show that it is often fruitful to augment the usual covariance matrix with higher-
order interaction terms. Our main result has a striking corollary in the context of tree-structured
graphs: for any discrete graphical model, the inverse of a generalized covariance matrix is always
(block) graph-structured. In particular, for binary variables, the inverse of the usual covariance ma-
trix corresponds exactly to the edge structure of the tree. We also establish several corollaries that
apply to more general discrete graphs. Our methods are capable of handling noisy or missing data
in a seamless manner.

1

Other related work on graphical model selection for discrete graphs includes the classic Chow-
Liu algorithm for trees [8]; nodewise logistic regression for discrete models with pairwise inter-
actions [9, 10]; and techniques based on conditional entropy or mutual information [11, 12]. Our
main contribution is to present a clean and surprising result on a simple link between the inverse
covariance matrix and edge structure of a discrete model, which may be used to derive inference
algorithms applicable even to data with systematic corruptions.
The remainder of the paper is organized as follows: In Section 2, we provide brief background and
notation on graphical models, and describe the classes of augmented covariance matrices we will
consider. In Section 3, we state our main results on the relationship between the support of general-
ized inverse covariance matrices and the edge structure of a discrete graphical model. We relate our
population-level results to concrete algorithms that are guaranteed to recover the edge structure of a
discrete graph with high probability. In Section 4, we report the results of simulations used to verify
our theoretical claims. For detailed proofs, we refer the reader to the technical report [13].

2 Background and problem setup

In this section, we provide background on graphical models and exponential families. We then work
through a simple example that illustrates the phenomena and methodology studied in this paper.

ψC (xC ),

2.1 Graphical models
An undirected graph G = (V , E ) consists of a collection of vertices V = {1, 2, . . . , p} and a
collection of unordered vertex pairs E ⊆ V × V , meaning no distinction is made between edges
(s, t) and (t, s). We associate to each vertex s ∈ V a random variable Xs taking values in some
space X . The random vector X := (X1 , . . . , Xp ) is a Markov random ﬁeld with respect to G if
XA ⊥⊥ XB | XS whenever S is a cutset of A and B , meaning every path from A to B in G must pass
through S . We have used the shorthand XA := {Xs : s ∈ A}. In particular, Xs ⊥⊥ Xt | X\{s,t}
whenever (s, t) /∈ E .
By the Hammersley-Clifford theorem for strictly positive distributions [14], the Markov properties
P(x1 , . . . , xp ) ∝ (cid:89)
imply a factorization of the distribution of X :
C∈C
where C is the set of all cliques (fully-connected subsets of V ) and ψC (xC ) are the corresponding
clique potentials. The factorization (1) may alternatively be represented in terms of an exponential
family associated with the clique structure of G. For each clique C ∈ C , we deﬁne a family of
sufﬁcient statistics {φC ;α : X |C | → R, α ∈ IC } associated with variables in C , where IC indexes
the sufﬁcient statistics corresponding to C . We also introduce a canonical parameter θC ;α ∈ R
associated with each sufﬁcient statistic φC ;α . For a given assignment of canonical parameters θ , we
(cid:88)
may express the clique potentials as
θC ;αφC ;α (xC ) := (cid:104)θC , φC (cid:105),
ψC (xC ) =
α∈IC
Pθ (x1 , . . . , xp ) = exp (cid:8) (cid:88)
(cid:104)θC , φC (cid:105) − A(θ)(cid:9),
so equation (1) may be rewritten as
x∈X p exp (cid:0) (cid:80)
C∈C (cid:104)θC , φC (cid:105)(cid:1) is the (log) partition function.
where A(θ) := log (cid:80)
C∈C
Note that for a graph with only pairwise interactions, we have C = V ∪ E . If we associate the
function φs (xs ) = xs with clique {s} and the function φst (xs , xt ) = xsxt with edge (s, t), the
(cid:88)
Pθ (x1 , . . . , xp ) = exp (cid:8) (cid:88)
θstxsxt − A(θ)(cid:9).
factorization (2) becomes
s∈V
(s,t)∈E

θsxs +

(1)

(2)

(3)

2

When X = {0, 1}, this family of distributions corresponds to the inhomogeneous Ising model.
When X = R (and with certain additional restrictions on the weights), the family (3) corresponds
to a Gauss-Markov random ﬁeld. Both of these models are minimal exponential families, meaning
the sufﬁcient statistics are linearly independent [15].
For a discrete graphical model with X = {0, 1, . . . , m − 1}, it is convenient to make use of sufﬁcient
statistics involving indicator functions. For clique C , deﬁne the subset of conﬁgurations
X |C |
0 = {J = (j1 , . . . , j|C | ) | j(cid:96) (cid:54)= 0 for all (cid:96) = 1, . . . , |C |},
for which no variables take the value 0. Then |X |C |
| = (m − 1)|C | . For any conﬁguration J ∈ X |C |
(cid:26)1
0
0
we deﬁne the indicator function
if xC = J,
φC ;J (xC ) =
otherwise,
0
Pθ (x1 , . . . , xp ) = exp (cid:8) (cid:88)
(cid:104)θC , φC (cid:105) − A(θ)(cid:9), where xj ∈ X = {0, 1, . . . , m − 1},
and consider the family of models
with (cid:104)θC , φC (cid:105) = (cid:80)
C∈C
|C |
θC ;J φC ;J (xC ). Note in particular that when m = 2, X
J ∈X |C |
(cid:89)
0
0
state containing the vector of all ones, and the sufﬁcient statistics are given by
and J = {1}|C | ;
for C ∈ C
s∈C

is a singleton

(4)

,

φC ;J (xC ) =

xs ,

i.e., the indicator functions may simply be expressed as products of variables appearing in the clique.
When the graphical model has only pairwise interactions, elements of C have cardinality at most
two, and the model (4) clearly reduces to the Ising model (3). Finally, as with the equation (3), the
family (4) is a minimal exponential family.

2.2 Covariance matrices and beyond

Consider the usual covariance matrix Σ = cov(X1 , . . . , Xp ). When X is Gaussian, it is a well-
known consequence of the Hammersley-Clifford theorem that the entries of the precision matrix
Γ = Σ−1 correspond to rescaled conditional correlations [14]. The magnitude of Γst is a scalar
multiple of the correlation of Xs and Xt conditioned on X\{s,t} , and encodes the strength of the
edge (s, t). In particular, the sparsity pattern of Γst reﬂects the edge structure of the graph: Γst = 0
if and only if Xs ⊥⊥ Xt | X\{s,t} . For more general distributions, however, Corr(Xs , Xt | X\{s,t} )
is a function of X\{s,t} , and it is not known whether the entries of Γ have any relationship with the
strengths of edges in the graph.
Nonetheless, it is tempting to conjecture that inverse covariance matrices, and more generally, in-
verses of higher-order moment matrices, might be related to graph structure. Let us explore this
possibility by considering a simple example, namely the binary Ising model (3) with X = {0, 1}.
Example 1. Consider a simple chain graph on four nodes, as illustrated in Figure 1(a). In terms
of the factorization (3), let the node potentials be θs = 0.1 for all s ∈ V and the edge potentials
be θst = 2 for all (s, t) ∈ E . For a multivariate Gaussian graphical model deﬁned on G, standard
theory predicts that the inverse covariance matrix Γ = Σ−1 of the distribution is graph-structured:
Γst = 0 if and only if (s, t) /∈ E . Surprisingly, this is also the case for the chain graph with binary
variables: a little computation show that Γ takes the form shown in panel (f). However, this statement
is not true for the single-cycle graph shown in panel (b), with added edge (1, 4). Indeed, as shown
in panel (g), the inverse covariance matrix has no nonzero entries at all. But for a more complicated
graph, say the one in (e), we again observe a graph-structured inverse covariance matrix.
Still focusing on the single-cycle graph in panel (b), suppose that instead of considering the or-
dinary covariance matrix, we compute the covariance matrix of the augmented random vector
(X1 , X2 , X3 , X4 , X1X3 ), where the extra term X1X3 is represented by the dotted edge shown

3

(a) Chain

Γchain =

Γaug = 103 ×

(c) Edge augmented
 .

Γloop =

−3.59
0
0
34.30 −4.77
0
34.30 −3.59
−4.77
−3.59
0
9.80

(b) Single cycle
 9.80
−3.59
0
0

(e) Dino
(d) With 3-cliques
 51.37 −5.37 −0.17 −5.37
 ,
−5.37
51.37 −5.37 −0.17
−0.17 −5.37
51.37 −5.37
−5.37 −0.17 −5.37
51.37
(g)
(f)
Figure 1. (a)–(e) Different examples of graphical models. (f) Inverse covariance for chain-structured
graph in (a). (g) Inverse covariance for single-cycle graph in (b).

 .
in panel (c). The 5 × 5 inverse of this generalized covariance matrix takes the form
1.15 −0.02
1.09 −0.02 −1.14
0.05 −0.02
−0.02
0.01
0
1.09 −0.02
1.14 −0.02 −1.14
−0.02
−0.02
0.01
0.05
0
−1.14
0.01 −1.14
0.01
1.19
This matrix safely separates nodes 1 and 4, but the entry corresponding to the phantom edge (1, 3) is
not equal to zero. Indeed, we would observe a similar phenomenon if we chose to augment the graph
by including the edge (2, 4) rather than (1, 3). Note that the relationship between entries of Γaug and
the edge strength is not direct; although the factorization (3) has no potential corresponding to the
augmented “edge” (1, 3), the (1, 3) entry of Γaug is noticeably larger in magnitude than the entries
corresponding to actual edges with nonzero potentials. This example shows that the usual inverse
covariance matrix is not always graph-structured, but computing generalized covariance matrices
involving higher-order interaction terms may indicate graph structure.
Φ(X ) = (cid:8)X1 , X2 , X3 , X4 , X1X2 , X2X3 , X3X4 , X1X4 , X1X3 , X1X2X3 , X1X3X4
(cid:9) ∈ {0, 1}11 .
Now let us consider a more general graphical model that adds the 3-clique interaction terms shown
in panel (d) to the usual Ising terms. We compute the covariance matrix of the augmented vector
functions Xα = (cid:81)
s∈α Xs and Xβ = (cid:81)
Empirically, we ﬁnd that the 11× 11 inverse of the matrix cov(Φ(X )) continues to respect aspects of
the graph structure: in particular, there are zeros in position (α, β ), corresponding to the associated
s∈β Xβ , whenever α and β do not lie within the same max-
imal clique. (For instance, this applies to the pairs (α, β ) = ({2}, {4}) and (α, β ) = ({2}, {1, 4}).)
The goal of this paper is to understand when certain inverse covariances do (and do not) capture
the structure of a graphical model. The underlying principles behind the behavior demonstrated in
Example 1 will be made concrete in Theorem 1 and its corollaries in the next section.

3 Main results and consequences

We now state our main results on the structure of generalized inverse covariance matrices and graph
structure. We present our results in two parts: one concerning statements at the population level,
and the other concerning statements at the level of statistical consistency based on random samples.

3.1 Population-level results
Our main result concerns a connection between the inverses of generalized inverse covariance ma-
Recall that a triangulation of a graph G = (V , E ) is an augmented graph (cid:101)G = (V , (cid:101)E ) with no
trices associated with the model (4) and the structure of the graph. We begin with some notation.
chordless 4-cycles. (For instance, the single cycle in panel (b) is a chordless 4-cycle, whereas panel

4

(c) shows a triangulated graph. The dinosaur graph in panel (e) is also triangulated.) The edge set (cid:101)E
corresponds to the original edge set E plus the additional edges added to form the triangulation. In
general, G admits many different triangulations; the results we prove below will hold for any ﬁxed
triangulation of G.
We also require some notation for deﬁning generalized covariance matrices. Let S be a collection
Φ(X ; S ) = (cid:8)φS ;J , J ∈ X |C |
, S ∈ S ∩ C (cid:9),
of subsets of vertices, and deﬁne the random vector
(5)
0
consisting of all sufﬁcient statistics over cliques in S . We will often be interested in situations where
S contains all subsets of a given set. For a subset A ⊆ V , we let pow(A) denote the set of all
non-empty subsets of A. (For instance, pow({1, 2}) = {1, 2, (1, 2)}.) Furthermore, for a collection
of subsets S , we let pow(S ) be the set of all subsets {pow(S ), S ∈ S }, discarding any duplicates
that arise. We are now ready to state our main theorem regarding the support of a certain type of
generalized inverse covariance matrix.
Theorem 1. [Triangulation and block graph-structure.] Consider an arbitrary discrete graphical
model of the form (4), and let T be the set of maximal cliques in any triangulation of G. Then the
inverse Γ of the augmented covariance matrix cov(Φ(X ; pow(T ))) is block graph-structured in the
following sense:

(a) For any two subsets A and B which are not subsets of the same maximal clique, the block
Γ(pow(A), pow(B )) is zero.

(b) For almost all parameters θ , the entire block Γ(pow(A), pow(B )) is nonzero whenever A
and B belong to a common maximal clique.

The proof of this result relies on convex analysis and the geometry of exponential families [15, 16].
In particular, in any minimal exponential family, there is a one-to-one correspondence between
exponential parameters (θα in our notation) and mean parameters (µα = E[φα (X )]). This corre-
spondence is induced by the Fenchel-Legendre duality between the log partition function A and its
dual A∗ , and allows us to relate Γ to the graph structure.
Note that when the original graph G is a tree, the graph is already triangulated and the set T in
Theorem 1 is equal to the edge set E . Hence, Theorem 1 implies that the inverse Γ of the augmented
covariance matrix with sufﬁcient statistics for all vertices and edges is graph-structured, and blocks
of nonzeros in Γ correspond to edges in the graph. In particular, the (m − 1)p × (m − 1)p submatrix
ΓV ,V corresponding to sufﬁcient statistics of vertices is block graph-structured; in the case when
m = 2, the submatrix ΓV ,V is simply the p × p block corresponding to the vector (X1 , . . . , Xp ).
When G is not triangulated, however, we may need to invert a larger augmented covariance matrix
and include sufﬁcient statistics over pairs (s, t) /∈ E , as well.
In fact, it is not necessary to take the set of sufﬁcient statistics over all maximal cliques, and we
may consider a slightly smaller augmented covariance matrix. Recall that any triangulation T gives
rise to a junction tree representation of G, where nodes of the junction tree are subsets of V cor-
responding to maximal cliques in T , and the edges are intersections of adjacent cliques known as
separator sets [15]. The following corollary involves the generalized covariance matrix containing
only sufﬁcient statistics for nodes and separator sets of T :
cov(Φ(X ; V ∪ pow(S ))). Then ΓV ,V is block graph-structured: Γs,t = 0 whenever (s, t) /∈ (cid:101)E .
Corollary 1. Let S be the set of separator sets in any triangulation of G, and let Γ be the inverse of
The proof of this corollary is based on applying the block matrix inversion formula [17] to express
ΓV ,V in terms of the matrix Γ from Theorem 1. Panel (c) of Example 1 and the associated matrix
Γaug provides a concrete instance of this corollary in action. In panel (c), the single separator set in
the triangulation is {1, 3}, so augmenting the usual covariance matrix with the additional sufﬁcient
does not belong to (cid:101)E , and as predicted by Corollary 1, we observe that Γaug (2, 4) = 0.
statistic X1X3 and taking the inverse should yield a graph-structured matrix. Indeed, edge (2, 4)
Note that V ∪ pow(S ) ⊆ pow(T ), and the set of sufﬁcient statistics considered in Corollary 1 is
generally much smaller than the set of sufﬁcient statistics considered in Theorem 1. Hence, the gen-
eralized covariance matrix of Corollary 1 has a smaller dimension than the generalized covariance
matrix of Theorem 1, and is much more tractable for estimation.

5

Although Theorem 1 and Corollary 1 are clean results at the population level, however, forming the
proper augmented covariance matrix requires some prior knowledge of the graph—namely, which
edges are involved in a suitable triangulation. In the case of a graph with only singleton separator
sets, Corollary 1 specializes to the following useful corollary, which only involves the covariance
matrix over indicators of vertices of G:
Corollary 2. For any graph with singleton separator sets, the inverse matrix Γ of the ordinary
covariance matrix cov(Φ(X ; V )) is graph-structured. (This class includes trees as a special case.)

Again, we may relate this corollary to Example 1—the inverse covariance matrices for the tree graph
in panel (a) and the dinosaur graph in panel (e) are exactly graph-structured. Indeed, although the
dinosaur graph is not a tree, it possesses the nice property that the only separator sets in its junction
tree are singletons.
Corollary 1 also guarantees that inverse covariances may be partially graph-structured, in the sense
that (ΓV ,V )st = 0 for any pair of vertices (s, t) separable by a singleton separator set. This is
because for any such pair (s, t), we form a junction tree with two nodes, one containing s and one
containing t, and apply Corollary 1 to conclude that (ΓV ,V )st = 0. Indeed, the matrix ΓV ,V over
singleton vertices is agnostic to which triangulation we choose for the graph.
In settings where there exists a junction tree representation of the graph with only singleton separator
sets, Corollary 2 has a number of useful implications for the consistency of methods that have
traditionally only been applied for edge recovery in Gaussian graphical models. In such settings,
Corollary 2 implies that it sufﬁces to estimate the support of ΓV ,V from the data.

3.2 Consequences for graphical Lasso for trees

Moving beyond the population level, we now establish results concerning the statistical consistency
of methods for graph selection in discrete graphical models, based on i.i.d. draws from a discrete
graph. We describe how a combination of our population-level results and some concentration
inequalities may be leveraged to analyze the statistical behavior of log-determinant methods for dis-
crete tree-structured graphical models, and suggest extensions of these methods when observations
are systematically corrupted by noise or missing data.
(cid:88)
(cid:98)Θ ∈ arg min
{trace( (cid:98)ΣΘ) − log det(Θ) + λn
Given p-dimensional random variables (X1 , . . . , Xp ) with covariance Σ∗ , consider the estimator
|Θst |},
(6)
where (cid:98)Σ is an estimator for Σ∗ . For multivariate Gaussian data, this program is an (cid:96)1 -regularized
Θ(cid:23)0
s(cid:54)=t
maximum likelihood estimate known as the graphical Lasso, and is a well-studied method for re-
covering the edge structure in a Gaussian graphical model [18, 19, 20]. Although the program (6)
has no relation to the MLE in the case of a discrete graphical model, it is still useful for estimating
covering the structure of any tree-structured Ising model. We consider a general estimate (cid:98)Σ of the
Θ∗ := (Σ∗ )−1 , and our analysis shows the surprising result that the program is consistent for re-
(cid:114)
P(cid:2)(cid:107) (cid:98)Σ − Σ∗ (cid:107)max ≥ ϕ(Σ∗ )
(cid:3) ≤ c exp(−ψ(n, p))
covariance matrix Σ such that
log p
(cid:80)n
n
observed i.i.d. data with sub-Gaussian parameter σ2 , where (cid:98)Σ = 1
for functions ϕ and ψ , where (cid:107) · (cid:107)max denotes the elementwise (cid:96)∞ -norm.
In the case of fully-
i − xxT is the usual
i=1 xixT
n
sample covariance, this bound holds with ϕ(Σ∗ ) = σ2 and ψ(n, p) = c(cid:48) log p.
In addition, we require a certain mutual incoherence condition on the true covariance matrix Σ∗ to
control the correlation of non-edge variables with edge variables in the graph. Let Γ∗ = Σ∗ ⊗ Σ∗ ,
where ⊗ denotes the Kronecker product. Then Γ∗ is a p2 × p2 matrix indexed by vertex pairs. The
incoherence condition is given by
(cid:107)Γ∗
SS )−1(cid:107)1 ≤ 1 − α,
α ∈ (0, 1],
eS (Γ∗
(8)
max
e∈S c
where S := {(s, t) : Θ∗
st (cid:54)= 0} is the set of vertex pairs corresponding to nonzero elements of
the precision matrix Θ∗—equivalently, the edge set of the graph, by our theory on tree-structured
discrete graphs. For more intuition on the mutual incoherence condition, see Ravikumar et al. [4].

(7)

6

Our global edge recovery algorithm proceeds as follows:
1. Form a suitable estimate (cid:98)Σ of the true covariance matrix Σ.
Algorithm 1 (Graphical Lasso).
2. Optimize the graphical Lasso program (6) with parameter λn , denoting the solution by (cid:98)Θ.
3. Threshold the entries of (cid:98)Θ at level τn to obtain an estimate of Θ∗ .
of Θ∗ and concentration properties of (cid:98)Σ:
We then have the following consistency result, a straightforward consequence of the graph structure
mutual incoherence condition (8). If n (cid:37) d2 log p, then Algorithm 1 with (cid:98)Σ the sample covariance
(cid:113) log p
(cid:113) log p
Corollary 3. Suppose we have a tree-structured Ising model with degree at most d, satisfying the
(cid:9) recovers all edges (s, t) with
(cid:8) c1
matrix and parameters λn ≥ c1
n and τn = c2
n + λn
α
α
|Θ∗
st | > τn/2, with probability at least 1 − c exp(−c(cid:48) log p).
st | > τn/2 for all edges (s, t) ∈ E , Corollary 3 guarantees that the log-determinant
Hence, if |Θ∗
method plus thresholding recovers the full graph exactly. In the case of the standard sample co-
variance matrix, this method has been implemented by Banerjee et al. [18]; our analysis estab-
lishes consistency of their method for discrete trees. The scaling n (cid:37) d2 log p is unavoidable, as
shown by information-theoretic analysis [21], and also appears in other past work on Ising mod-
els [10, 9, 11]. Our analysis also has a cautionary message: the proof of Corollary 3 relies heavily
on the population-level result in Corollary 2, which ensures that Θ∗ is tree-structured. For a general
graph, we have no guarantees that Θ∗ will be graph-structured (e.g., see panel (b) in Figure 1), so
the graphical Lasso (6) is inconsistent in general.
since it relies only on an estimate (cid:98)Σ of the population covariance Σ∗ that satisﬁes the deviation
On the positive side, if we restrict ourselves to tree-structured graphs, the estimator (6) is attractive,
all we require is a sufﬁciently good estimate (cid:98)Σ of Σ∗ . Furthermore, the program (6) is always convex
condition (7). In particular, when the samples {xi }n
i=1 are contaminated by noise or missing data,
even when the estimator (cid:98)Σ is not positive semideﬁnite (as will often be the case for missing/corrupted
data).
As a concrete example of how we may correct the program (6) to handle corrupted data, consider
(cid:33)
(cid:32)
the case when each entry of xi is missing independently with probability ρ, and the corresponding
n(cid:88)
observations zi are zero-ﬁlled for missing entries. A natural estimator is
(cid:98)Σ =
÷ M −
1
1
(1 − ρ)2 zzT ,
zi zT
i
n
i=1
where ÷ denotes elementwise division by the matrix M with diagonal entries (1 − ρ) and off-
diagonal entries (1 − ρ)2 , correcting for the bias in both the mean and second moment terms. The
Wainwright [22]). Similarly, we may derive an appropriate estimator (cid:98)Σ and a subsequent version of
deviation condition (7) may be shown to hold w.h.p., where ϕ(Σ∗ ) scales with (1 − ρ) (cf. Loh and
Algorithm 1 in situations when the data are systematically contaminated by other forms of additive
or multiplicative corruption.
Generalizing to the case of m-ary discrete graphical models with m > 2, we may easily modify
the program (6) by replacing the elementwise (cid:96)1 -penalty by the corresponding group (cid:96)1 -penalty,
where the groups are the indicator variables for a given vertex. Precise theoretical guarantees may
be derived from results on the group graphical Lasso [23].

(9)

4 Simulations

Figure 2 depicts the results of simulations we performed to test our theoretical predictions. In all
cases, we generated binary Ising models with node weights 0.1 and edge weights 0.3 (using spin
{−1, 1} variables). The ﬁve curves show the results of our graphical Lasso method applied to
the dinosaur graph in Figure 1. Each curve plots the probability of success in recovering the 15

7

edges of the graph, as a function of the rescaled sample size
log p , where p = 13. The leftmost
n
(red) curve corresponds to the case of fully-observed covariates (ρ = 0), whereas the remaining
four curves correspond to increasing missing data fractions ρ ∈ {0.05, 0.1, 0.15, 0.2}, using the
corrected estimator (9). We observe that all ﬁve runs display a transition from success probability 0
to success probability 1 in roughly the same range of the rescaled sample size, as predicted by our
theory. Indeed, since the dinosaur graph has only singleton separators, Corollary 2 ensures that the
inverse covariance matrix is exactly graph-structured. Note that the curves shift right as the fraction
ρ of missing data increases, since the problem becomes harder.

Figure 2. Simulation results for our graphical Lasso method on binary Ising models, allowing for
missing data in the observations. The ﬁgure shows simulation results for the dinosaur graph. Each
log p .
point represents an average over 1000 trials. The horizontal axis gives the rescaled sample size n

5 Discussion

The correspondence between the inverse covariance matrix and graph structure of a Gauss-Markov
random ﬁeld is a classical fact, with many useful consequences for efﬁcient estimation of Gaussian
graphical models. It has long been an open question as to whether or not similar properties extend
to a broader class of graphical models. In this paper, we have provided a partial afﬁrmative answer
to this question and developed theoretical results extending such relationships to discrete undirected
graphical models.
As shown by our results, the inverse of the ordinary covariance matrix is graph-structured for special
subclasses of graphs with singleton separator sets. More generally, we have shown that it is worth-
while to consider the inverses of generalized covariance matrices, formed by introducing indicator
functions for larger subsets of variables. When these subsets are chosen to reﬂect the structure
of an underlying junction tree, the edge structure is reﬂected in the inverse covariance matrix. Our
population-level results have a number of statistical consequences for graphical model selection. We
have shown how our results may be used to establish consistency (or inconsistency) of the standard
graphical Lasso applied to discrete graphs, even when observations are systematically corrupted
by mechanisms such as additive noise and missing data. As noted by an anonymous reviewer, the
Chow-Liu algorithm might also potentially be modiﬁed to allow for missing or corrupted observa-
tions. However, our proposed method and further offshoots of our population-level result may be
applied even in cases of non-tree graphs, which is beyond the scope of the Chow-Liu algorithm.

Acknowledgments

PL acknowledges support from a Hertz Foundation Fellowship and an NDSEG Fellowship. MJW
and PL were also partially supported by grants NSF-DMS-0907632 and AFOSR-09NL184. The
authors thank the anonymous reviewers for helpful feedback.

8

010020030040050000.20.40.60.81success prob vs. sample size for dino graph with missing datan/log psuccess prob, avg over 1000 trials  rho = 0rho = 0.05rho = 0.1rho = 0.15rho = 0.2References
[1] M.E.J. Newman and D.J. Watts. Scaling and percolation in the small-world network model.
Phys. Rev. E, 60(6):7332–7342, December 1999.
[2] T. Cai, W. Liu, and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix
estimation. Journal of the American Statistical Association, 106:594–607, 2011.
[3] N. Meinshausen and P. B ¨uhlmann. High-dimensional graphs and variable selection with the
Lasso. Annals of Statistics, 34:1436–1462, 2006.
[4] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estima-
tion by minimizing (cid:96)1 -penalized log-determinant divergence. Electronic Journal of Statistics,
4:935–980, 2011.
[5] M. Yuan. High-dimensional inverse covariance matrix estimation via linear programming.
Journal of Machine Learning Research, 99:2261–2286, August 2010.
[6] H. Liu, F. Han, M. Yuan, J.D. Lafferty, and L.A. Wasserman. High dimensional semi-
parametric Gaussian copula graphical models. arXiv e-prints, March 2012. Available at
http://arxiv.org/abs/1202.2169.
[7] H. Liu, J.D. Lafferty, and L.A. Wasserman. The nonparanormal: Semiparametric estimation of
high dimensional undirected graphs. Journal of Machine Learning Research, 10:2295–2328,
2009.
[8] C.I. Chow and C.N. Liu. Approximating discrete probability distributions with dependence
trees. IEEE Transactions on Information Theory, 14:462–467, 1968.
[9] A. Jalali, P.D. Ravikumar, V. Vasuki, and S. Sanghavi. On learning discrete graphical models
using group-sparse regularization. Journal of Machine Learning Research - Proceedings Track,
15:378–387, 2011.
[10] P. Ravikumar, M.J. Wainwright, and J.D. Lafferty. High-dimensional Ising model selection
using (cid:96)1 -regularized logistic regression. Annals of Statistics, 38:1287, 2010.
[11] A. Anandkumar, V.Y.F. Tan, and A.S. Willsky. High-dimensional structure learning of Ising
models: Local separation criterion. Annals of Statistics, 40(3):1346–1375, 2012.
[12] G. Bresler, E. Mossel, and A. Sly. Reconstruction of markov random ﬁelds from samples:
Some observations and algorithms. In APPROX-RANDOM, pages 343–356, 2008.
[13] P. Loh and M.J. Wainwright. Structure estimation for discrete graphical models: Generalized
covariance matrices and their inverses. arXiv e-prints, November 2012.
[14] S.L. Lauritzen. Graphical Models. Oxford University Press, 1996.
[15] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational
inference. Found. Trends Mach. Learn., 1(1-2):1–305, January 2008.
[16] R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.
[17] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[18] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum
likelihood estimation for multivariate Gaussian or binary data. Journal of Machine Learning
Research, 9:485–516, 2008.
[19] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graph-
ical Lasso. Biostatistics, 9(3):432–441, July 2008.
[20] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model.
Biometrika, 94(1):19–35, 2007.
[21] Narayana P. Santhanam and Martin J. Wainwright.
Information-theoretic limits of select-
ing binary graphical models in high dimensions. IEEE Transactions on Information Theory,
58(7):4117–4134, 2012.
[22] P. Loh and M.J. Wainwright. High-dimensional regression with noisy and missing data: Prov-
able guarantees with non-convexity. Annals of Statistics, 40(3):1637–1664, 2012.
[23] L. Jacob, G. Obozinski, and J. P. Vert. Group Lasso with Overlap and Graph Lasso.
International Conference on Machine Learning (ICML), pages 433–440, 2009.

In

9


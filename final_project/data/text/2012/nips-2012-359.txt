Best Arm Identiﬁcation: A Uniﬁed Approach to Fixed
Budget and Fixed Conﬁdence

Victor Gabillon

Mohammad Ghavamzadeh
INRIA Lille - Nord Europe, Team SequeL
Victor Gabillon, Mohammad Ghavamzadeh & Alessandro Lazaric

Alessandro Lazaric

Abstract

We study the problem of identifying the best arm(s) in the stochastic multi-armed
bandit setting. This problem has been studied in the literature from two different
perspectives: ﬁxed budget and ﬁxed conﬁdence. We propose a unifying approach
that leads to a meta-algorithm called uniﬁed gap-based exploration (UGapE), with
a common structure and similar theoretical analysis for these two settings. We
prove a performance bound for the two versions of the algorithm showing that the
two problems are characterized by the same notion of complexity. We also show
how the UGapE algorithm as well as its theoretical analysis can be extended to
take into account the variance of the arms and to multiple bandits. Finally, we
evaluate the performance of UGapE and compare it with a number of existing
ﬁxed budget and ﬁxed conﬁdence algorithms.

1

Introduction

The problem of best arm(s) identiﬁcation [6, 3, 1] in the stochastic multi-armed bandit setting has
recently received much attention. In this problem, a forecaster repeatedly selects an arm and ob-
serves a sample drawn from its reward distribution during an exploration phase, and then is asked to
return the best arm(s). Unlike the standard multi-armed bandit problem, where the goal is to maxi-
mize the cumulative sum of rewards obtained by the forecaster (see e.g., [15, 2]), in this problem the
forecaster is evaluated on the quality of the arm(s) returned at the end of the exploration phase. This
abstract problem models a wide range of applications. For instance, let us consider a company that
has K different variants of a product and needs to identify the best one(s) before actually placing it
on the market. The company sets up a testing phase in which the products are tested by potential
customers. Each customer tests one product at the time and gives it a score (a reward). The objective
of the company is to return a product at the end of the test phase which is likely to be successful once
placed on the market (i.e., the best arm identiﬁcation), and it is not interested in the scores collected
during the test phase (i.e., the cumulative reward).
The problem of best arm(s) identiﬁcation has been studied in two distinct settings in the literature.
Fixed budget. In the ﬁxed budget setting (see e.g., [3, 1]), the number of rounds of the exploration
phase is ﬁxed and is known by the forecaster, and the objective is to maximize the probability of
returning the best arm(s). In the above example, the company ﬁxes the length of the test phase before
hand (e.g., enrolls a ﬁxed number of customers) and deﬁnes a strategy to choose which products to
show to the testers so that the ﬁnal selected product is the best with the highest probability. Audibert
et al. [1] proposed two different strategies to solve this problem. They deﬁned a strategy based
on upper conﬁdence bounds, called UCB-E, whose optimal parameterization is strictly related to a
measure of the complexity of the problem. They also introduced an elimination algorithm, called
Successive Rejects, which divides the budget n in phases and discards one arm per phase. Both
algorithms were shown to have nearly optimal probability of returning the best arm. Deng et al. [5]
and Gabillon et al. [8] considered the extension of the best arm identiﬁcation problem to the multi-

1

bandit setting, where the objective is to return the best arm for each bandit. Recently, Bubeck et
al. [4] extended the previous results to the problem of m-best arm identiﬁcation and introduced a
new version of the Successive Rejects algorithm (with accept and reject) that is able to return the set
of the m-best arms with high probability.
Fixed conﬁdence. In the ﬁxed conﬁdence setting (see e.g., [12, 6]), the forecaster tries to mini-
mize the number of rounds needed to achieve a ﬁxed conﬁdence about the quality of the returned
arm(s). In the above example, the company keeps enrolling customers in the test until it is, e.g., 95%
conﬁdent that the best product has been identiﬁed. Maron & Moore [12] considered a slightly dif-
ferent setting where besides a ﬁxed conﬁdence also the maximum number of rounds is ﬁxed. They
designed an elimination algorithm, called Hoeffding Races, based on progressively discarding the
arms that are suboptimal with enough conﬁdence. Mnih et al. [14] introduced an improved al-
gorithm, built on the Bernstein concentration inequality, which takes into account the empirical
variance of each arm. Even-Dar et al. [6] studied the ﬁxed conﬁdence setting without any budget
constraint and designed an elimination algorithm able to return an arm with a required accuracy
 (i.e., whose performance is at least -close to the optimal arm). Kalyanakrishnan & Stone [10]
further extended this approach to the case where the m-best arms must be returned with a given
conﬁdence. Finally, Kalyanakrishnan et al. [11] recently introduced an algorithm for the case of
m-best arm identiﬁcation along with a thorough theoretical analysis showing the number of rounds
needed to achieve the desired conﬁdence.
Although the ﬁxed budget and ﬁxed conﬁdence problems have been studied separately, they display
several similarities. In this paper, we propose a uniﬁed approach to these two settings in the general
case of m-best arm identiﬁcation with accuracy .1 The main contributions of the paper can be
summarized as follows:
Algorithm. In Section 3, we propose a novel meta-algorithm, called uniﬁed gap-based exploration
(UGapE), which uses the same arm selection and (arm) return strategies for the two settings. This
algorithm allows us to solve settings that have not been covered in the previous work (e.g., the case
of  (cid:54)= 0 has not been studied in the ﬁxed budget setting). Furthermore, we show in Appendix C of
[7] that UGapE outperforms existing algorithms in some settings (e.g., it improves the performance
of the algorithm by Mnih et al. [14] in the ﬁxed conﬁdence setting). We also provide a thorough
empirical evaluation of UGapE and compare it with a number of existing ﬁxed budget and ﬁxed
conﬁdence algorithms in Appendix C of [7].
Theoretical analysis. Similar to the algorithmic contribution, in Section 4, we show that a large
portion of the theoretical analysis required to study the behavior of the two settings of the UGapE
algorithm can be uniﬁed in a series of lemmas. The ﬁnal theoretical guarantees are thus a direct
consequence of these lemmas when used in the two speciﬁc settings.
Problem complexity. In Section 4.4, we show that the theoretical analysis indicates that the two
problems share exactly the same deﬁnition of complexity. In particular, we show that the probability
of success in the ﬁxed budget setting as well as the sample complexity in the ﬁxed conﬁdence setting
strictly depend on the inverse of the gaps of the arms and the desired accuracy .
Extensions. Finally, in Appendix B of [7], we discuss how the proposed algorithm and analysis can
be extended to improved deﬁnitions of conﬁdence interval (e.g., Bernstein-based bounds) and to
more complex settings, such as the multi-bandit best arm identiﬁcation problem introduced in [8].

2 Problem Formulation
In this section, we introduce the notation used throughout the paper. Let A = {1, . . . , K } be the set
of arms such that each arm k ∈ A is characterized by a distribution νk bounded in [0, b] with mean
k . We deﬁne the m-max and m-argmax operators as2
µk and variance σ2
m
m
and
max
max
k∈A
k∈A
where (m) denotes the index of the m-th best arm in A and µ(m) is its corresponding mean so that
µ(1) ≥ µ(2) ≥ . . . ≥ µ(K ) . We denote by Sm ⊂ A any subset of m arms (i.e., |Sm | = m < K ) and
by Sm,∗ the subset of the m best arms (i.e., k ∈ Sm,∗ iif µk ≥ µ(m) ). Without loss of generality, we
1Note that when  = 0 and m = 1 this reduces to the standard best arm identiﬁcation problem.
2Ties are broken in an arbitrary but consistent manner.

(m) = arg

µk ,

µ(m) =

µk

2

µk .

1..m
max
k∈A

assume there exists a unique set Sm,∗ . In the following we drop the superscript m and use S = Sm
and S ∗ = Sm,∗ whenever m is clear from the context. With a slight abuse of notation we further
extend the m-max operator to an operator returning a set of arms, such that
{µ(1) , . . . , µ(m)} =
S ∗ = arg
1..m
and
max
µk
k∈A
(cid:26)µk − µ(m+1)
For each arm k ∈ A, we deﬁne the gap ∆k as
if k ∈ S ∗
µ(m) − µk
if k /∈ S ∗ .
∆k =
This deﬁnition of gap indicates that if k ∈ S ∗ , ∆k represents the “advantage” of arm k over the
suboptimal arms, and if k /∈ S ∗ , ∆k denotes how suboptimal arm k is. Note that we can also write
the gap as ∆k = | m
µi − µk |. Given an accuracy  and a number of arms m, we say that an arm
max
i (cid:54)=k
k is (,m)-optimal if µk ≥ µ(m) − . Thus, we deﬁne the (,m)-best arm identiﬁcation problem as
the problem of ﬁnding a set S of m (,m)-optimal arms.
The (,m)-best arm identiﬁcation problem can be formalized as a game between a stochastic bandit
environment and a forecaster. The distributions {νk } are unknown to the forecaster. At each round t,
the forecaster pulls an arm I (t) ∈ A and observes an independent sample drawn from the distribution
(cid:80)Tk (t)
νI (t) . The forecaster estimates the expected value of each arm by computing the average of the
of round t, then the mean of this arm is estimated as (cid:98)µk (t) = 1
samples observed over time. Let Tk (t) be the number of times that arm k has been pulled by the end
s=1 Xk (s), where Xk (s) is
the s-th sample observed from νk . For any arm k ∈ A, we deﬁne the notion of arm simple regret as
Tk (t)
rk = µ(m) − µk ,
(1)
and for any set S ⊂ A of m arms, we deﬁne the simple regret as
rk = µ(m) − min
(2)
µk .
rS = max
k∈S
k∈S
We denote by Ω(t) ⊂ A the set of m arms returned by the forecaster at the end of the exploration
phase (when the alg. stops after t rounds), and by rΩ(t) its corresponding simple regret. Returning
m (,m)-optimal arms is then equivalent to having rΩ(t) smaller than . Given an accuracy  and a
number of arms m to return, we now formalize the two settings of ﬁxed budget and ﬁxed conﬁdence.
Fixed budget. The objective is to design a forecaster capable of returning a set of m (,m)-optimal
a budget n, the performance of the forecaster is measured by the probability (cid:101)δ of not meeting the
(,m) requirement, i.e., (cid:101)δ = P(cid:2)rΩ(n) ≥ (cid:3), the smaller (cid:101)δ , the better the algorithm.
arms with the largest possible conﬁdence using a ﬁxed budget of n rounds. More formally, given
of m (,m)-optimal arms with a ﬁxed conﬁdence. We denote by (cid:101)n the time when the algorithm stops
Fixed conﬁdence. The goal is to design a forecaster that stops as soon as possible and returns a set
and by Ω((cid:101)n) its set of returned arms. Given a conﬁdence level δ , the forecaster has to guarantee that
P(cid:2)rΩ((cid:101)n) ≥ (cid:3) ≤ δ . The performance of the forecaster is then measured by the number of rounds (cid:101)n
either in expectation or high probability.
Although these settings have been considered as two distinct problems, in Section 3 we introduce
a uniﬁed arm selection strategy that can be used in both cases by simply changing the stopping
criteria. Moreover, we show in Section 4 that the bounds on the performance of the algorithm in the
two settings share the same notion of complexity and can be derived using very similar arguments.

3 Uniﬁed Gap-based Exploration Algorithm

In this section, we describe the uniﬁed gap-based exploration (UGapE) meta-algorithm and show
how it is implemented in the ﬁxed-budget and ﬁxed-conﬁdence settings. As shown in Figure 1, both
ﬁxed-budget (UGapEb) and ﬁxed-conﬁdence (UGapEc) instances of UGapE use the same arm-
selection strategy, SELECT-ARM (described in Figure 2), and upon stopping, return the m-best
arms in the same manner (using Ω). The two algorithms only differ in their stopping criteria. More
precisely, both algorithms receive as input the deﬁnition of the problem (, m), a constraint (the

3

budget n in UGapEb and the conﬁdence level δ in UGapEc), and a parameter (a or c). While
UGapEb runs for n rounds and then returns the set of arms Ω(n), UGapEc runs until it achieves the
desired accuracy  with the requested conﬁdence level δ . This difference is due to the two different
objectives targeted by the algorithms; while UGapEc optimizes its budget for a given conﬁdence
level, UGapEb’s goal is to optimize the quality of its recommendation for a ﬁxed budget.

UGapEb (, m, n, a)
Parameters: accuracy , number of arms m,
Initialize: Pull each arm k once, update (cid:98)µk (K )
budget n, exploration parameter a
and set Tk (K ) = 1
SAMP
for t = K + 1, . . . , n do
SELECT-ARM (t)
end for
SAMP
Return Ω(n) = arg min
J (t)

BJ (t) (t)

UGapEc (, m, δ, c)
Parameters: accuracy , number of arms m,
conﬁdence level δ , exploration parameter c
(cid:98)µk (K ), set Tk (K ) = 1 and t ← K + 1
Initialize: Pull each arm k once, update
SAMP
while BJ (t) (t) ≥  do
SELECT-ARM (t)
t ← t + 1
end while
SAMP
Return Ω(t) = J (t)

Figure 1: The pseudo-code for the UGapE algorithm in the ﬁxed-budget (UGapEb) (left) and ﬁxed-
conﬁdence (UGapEc) (right) settings.

Bk (t)

Regardless of the ﬁnal objective, how to select
SELECT-ARM (t)
an arm at each round (arm-selection strategy) is
Compute Bk (t) for each arm k ∈ A
the key component of any multi-arm bandit al-
Identify the set of m arms J (t) ∈ arg
1..m
gorithm. One of the most important features of
min
k∈A
(cid:0)TI (t) (t − 1) + 1(cid:1) ∼ νI (t)
UGapE is having a unique arm-selection strat-
βk (t − 1)
Pull the arm I (t) = arg max
egy for the ﬁxed-budget and ﬁxed-conﬁdence
k∈{lt ,ut }
Update (cid:98)µI (t) (t) and TI (t) (t)
settings. We now describe the UGapE’s arm-
Observe XI (t)
selection strategy, whose pseudo-code has been
reported in Figure 2. At each time step t,
UGapE ﬁrst uses the observations up to time t−
Figure 2: The pseudo-code for the UGapE’s arm-
Ui (t) −
m
1 and computes an index Bk (t) =
max
selection strategy. This routine is used in both
i (cid:54)=k
Lk (t) for each arm k ∈ A, where
Uk (t) = (cid:98)µk (t − 1) + βk (t − 1)
Lk (t) = (cid:98)µk (t − 1) − βk (t − 1).
UGapEb and UGapEc instances of UGapE.
∀t, ∀k ∈ A
(3)
,
In Eq. 3, βk (t − 1) is a conﬁdence interval,3 and Uk (t) and Lk (t) are high probability upper and
lower bounds on the mean of arm k , µk , after t − 1 rounds. Note that the parameters a and c are used
(cid:115)
in the deﬁnition of the conﬁdence interval βk , whose shape strictly depends on the concentration
(cid:114) a
bound used by the algorithm. For example, we can derive βk from the Chernoff-Hoeffding bound as
c log 4K (t−1)3
δ
Tk (t − 1)
Tk (t − 1)
In Sec. 4, we discuss how the parameters a and c can be tuned and we show that while a should be
tuned as a function of n and  in UGapEb, c = 1/2 is always a good choice for UGapEc. Deﬁning
the conﬁdence interval in a general form βk (t − 1) allows us to easily extend the algorithm by taking
into account different (higher) moments of the arms (see Appendix B of [7] for the case of variance,
where βk (t − 1) is obtained from the Bernstein inequality). From Eq. 3, we may see that the index
Bk (t) is an upper-bound on the simple regret rk of the k th arm (see Eq. 1). We also deﬁne an index
for a set S as BS (t) = maxi∈S Bi (t). Similar to the arm index, BS is also deﬁned in order to
upper-bound the simple regret rS with high probability (see Lemma 1).
After computing the arm indices, UGapE ﬁnds a set of m arms J (t) with minimum upper-bound
1..m
on their simple regrets, i.e., J (t) = arg
Bk (t). From J (t), it computes two arm indices ut =
min
k∈A
arg maxj /∈J (t) Uj (t) and lt = arg mini∈J (t) Li (t), where in both cases the tie is broken in favor of
3To be more precise, βk (t − 1) is the width of a conﬁdence interval or a conﬁdence radius.

UGapEb: βk (t − 1) = b

UGapEc: βk (t − 1) = b

,

.

(4)

4

the arm with the largest uncertainty β (t − 1). Arms lt and ut are the worst possible arm among those
(cid:0)TI (t) (t − 1) + 1(cid:1) from the distribution
in J (t) and the best possible arm left outside J (t), respectively, and together they represent how bad
the choice of J (t) could be. Finally, the algorithm selects and pulls the arm I (t) as the arm with the
νI (t) , and updates the empirical mean (cid:98)µI (t) (t) and the number of pulls TI (t) (t) of the selected arm
larger β (t − 1) among ut and lt , observes a sample XI (t)
I (t).
There are two more points that need to be discussed about the UGapE algorithm. 1) While UGapEc
deﬁnes the set of returned arms as Ω(t) = J (t), UGapEb returns the set of arms J (t) with the
the number of rounds before stopping as (cid:101)n) when BJ ((cid:101)n+1) ((cid:101)n + 1) is less than the given accuracy
smallest index, i.e., Ω(n) = arg minJ (t) BJ (t) (t), t ∈ {1, . . . , n}. 2) UGapEc stops (we refer to
selected set J ((cid:101)n + 1) is smaller than . This guarantees that the simple regret (see Eq. 2) of the set
, i.e., when even the mth worst upper-bound on the arm simple regret among all the arms in the
returned by the algorithm, Ω((cid:101)n) = J ((cid:101)n + 1), to be smaller than  with probability larger than 1 − δ .
4 Theoretical Analysis

In this section, we provide high probability upper-bounds on the performance of the two instances
of the UGapE algorithm, UGapEb and UGapEc, introduced in Section 3. An important feature of
UGapE is that since its ﬁxed-budget and ﬁxed-conﬁdence versions share the same arm-selection
strategy, a large part of their theoretical analysis can be uniﬁed. We ﬁrst report this uniﬁed part of
the proof in Section 4.1, and then provide the ﬁnal performance bound for each of the algorithms,
UGapEb and UGapEc, separately, in Sections 4.2 and 4.3, respectively.
E = (cid:8)∀k ∈ A, ∀t ∈ {1, . . . , T }, (cid:12)(cid:12)(cid:98)µk (t) − µk
(cid:12)(cid:12) < βk (t)(cid:9),
Before moving to the main results, we deﬁne additional notation used in the analysis. We ﬁrst deﬁne
event E as
(5)
where the values of T and βk are deﬁned for each speciﬁc setting separately. Note that event E plays
an important role in the sequel, since it allows us to ﬁrst derive a series of results which are directly
implied by the event E and to postpone the study of the stochastic nature of the problem (i.e., the
probability of E ) in the two speciﬁc settings. In particular, when E holds, we have that for any arm
k ∈ A and at any time t, Lk (t) ≤ µk ≤ Uk (t). Finally, we deﬁne the complexity of the problem as
K(cid:88)
b2
max( ∆i+
, )2
2
i=1
Note that although the complexity has an explicit dependence on , it also depends on the number of
arms m through the deﬁnition of the gaps ∆i , thus making it a complexity measure of the (, m) best
arm identiﬁcation problem. In Section 4.4, we will discuss why the complexity of the two instances
of the problem is measured by this quantity.

H =

.

(6)

4.1 Analysis of the Arm-Selection Strategy
Here we report lower (Lemma 1) and upper (Lemma 2) bounds for indices BS on the event E , which
show their connection with the regret and gaps. The technical lemmas used in the proofs (Lemmas 3
and 4 and Corollary 1) are reported in Appendix A of [7]. We ﬁrst prove that for any set S (cid:54)= S ∗
and any time t ∈ {1, . . . , T }, the index BS (t) is an upper-bound on the simple regret of this set rS .
Lemma 1. On event E , for any set S (cid:54)= S ∗ and any time t ∈ {1, . . . , T }, we have BS (t) ≥ rS .
(cid:0)(cid:98)µj (t − 1) + βj (t − 1)(cid:1) − (cid:0)(cid:98)µi (t − 1) − βi (t − 1)(cid:1)
Proof. On event E , for any arm i /∈ S ∗ and each time t ∈ {1, . . . , T }, we may write
Uj (t) − Li (t) =
m
m
Bi (t) =
max
max
j (cid:54)=i
j (cid:54)=i
≥ m
µj − µi = µ(m) − µi = ri .
max
j (cid:54)=i
Using Eq. 7, we have
Bi (t) ≥ max
Bi (t) ≥ max
ri = rS ,
BS (t) = max
i∈S
i∈(S−S ∗ )
i∈(S−S ∗ )
where the last passage follows from the fact that ri ≤ 0 for any i ∈ S ∗ .

(7)

5

(9)

(8)

BJ (t) (t) ≤ min (cid:0)0, −∆k + 2βk (t − 1)(cid:1) + 2βk (t − 1).
Lemma 2. On event E , if arm k ∈ {lt , ut} is pulled at time t ∈ {1, . . . , T }, we have
B (t) ≤ min (cid:0)0, −∆k + 2βk (t − 1)(cid:1) + 2βk (t − 1).
Proof. We ﬁrst prove the statement for B (t) = Uut (t) − Llt (t), i.e.,
We consider the following cases:
Case 1. k = ut :
Case 1.1. ut ∈ S ∗ : Since by deﬁnition ut /∈ J (t), there exists an arm j /∈ S ∗ such that j ∈ J (t).
Now we may write
(c)≥ Lut (t) = (cid:98)µk (t − 1) − βk (t − 1)
(d)≥ µk − 2βk (t − 1)
(b)≥ Llt (t)
(a)≥ Lj (t)
µ(m+1) ≥ µj
(10)
(a) and (d) hold because of event E , (b) follows from the fact that j ∈ J (t) and from the deﬁnition
of lt , and (c) is the result of Lemma 4. From Eq. 10, we may deduce that −∆k + 2βk (t − 1) ≥ 0,
which together with Corollary 1 gives us the desired result (Eq. 9).
Case 1.2. ut /∈ S ∗ :
Case 1.2.1. lt ∈ S ∗ : In this case, we may write
(a)≤ µut + 2βut (t − 1) − µlt + 2βlt (t − 1)
B (t) = Uut (t) − Llt (t)
(b)≤ µut + 2βut (t − 1) − µ(m) + 2βlt (t − 1)
(c)≤ −∆ut + 4βut (t − 1)
(11)
(a) holds because of event E , (b) is from the fact that lt ∈ S ∗ , and (c) is because ut is pulled, and
thus, βut (t − 1) ≥ βlt (t − 1). The ﬁnal result follows from Eq. 11 and Corollary 1.
lt /∈ S ∗ : Since lt /∈ S ∗ and the fact that by deﬁnition lt ∈ J (t), there exists an
Case 1.2.2.
arm j ∈ S ∗ such that j /∈ J (t). Now we may write
(a)≥ Uut (t)
(d)≥ µ(m)
(c)≥ µj
(b)≥ Uj (t)
µut + 2βut (t − 1)
(12)
(a) and (c) hold because of event E , (b) is from the deﬁnition of ut and the fact that j /∈ J (t), and
(d) holds because j ∈ S ∗ . From Eq. 12, we may deduce that −∆ut + 2βut (t − 1) ≥ 0, which
together with Corollary 1 gives us the ﬁnal result (Eq. 9).

With similar arguments and cases, we prove the result of Eq. 9 for k = lt . The ﬁnal state-
ment of the lemma (Eq. 8) follows directly from BJ (t) (t) ≥ B (t) as shown in Lemma 3.

Using Lemmas 1 and 2, we deﬁne an upper and a lower bounds on BJ (t) in terms of quantities
related to the regret of J (t). Lemma 1 conﬁrms the intuition that the B -values upper-bound the
regret of the corresponding set of arms (with high probability). Unfortunately, this is not enough
to claim that selecting J (t) as the set of arms with smallest B -values actually correspond to arms
with small regret, since BJ (t) could be an arbitrary loose bound on the regret. Lemma 2 provides
this complementary guarantee speciﬁcally for the set J (t), in the form of an upper-bound on BJ (t)
w.r.t. the gap of k ∈ {ut , lt}. This implies that as the algorithm runs, the choice of J (t) becomes
more and more accurate since BJ (t) is constrained between rJ (t) and a quantity (Eq. 8) that gets
smaller and smaller, thus implying that selecting the arms with the smaller B -value, i.e., the set J (t),
corresponds to those which actually have the smallest regret, i.e., the arms in S ∗ . This argument will
be implicitly at the basis of the proofs of the two following theorems.

4.2 Regret Bound for the Fixed-Budget Setting

Here we prove an upper-bound on the simple-regret of UGapEb. Since the setting considered by the
algorithm is ﬁxed-budget, we may set T = n. From the deﬁnition of the conﬁdence interval βi (t)
in Eq. 4 and a union bound, we have that P(E ) ≥ 1 − 2K n exp(−2a).4 We now have all the tools
needed to prove the performance of UGapEb for the m (,m)-best arm identiﬁcation problem.

4The extension to a conﬁdence interval that takes into account the variance of the arms is discussed in
Appendix B of [7].

6

(cid:101)δ = P(cid:0)rΩ(n) ≥ (cid:1) ≤ 2K n exp(−2a),
Theorem 1. If we run UGapEb with parameter 0 < a ≤ n−K
, its simple regret rΩ(n) satisﬁes
4H
and in particular this probability is minimized for a = n−K
.
4H
Proof. The proof is by contradiction. We assume that rΩ(n) >  on event E and consider the
following two steps:
Step 1: Here we show that on event E , we have the following upper-bound on the number of pulls
of any arm i ∈ A:
max (cid:0) ∆i+
, (cid:1)2 + 1.
4ab2
Ti (n) <
2
Let ti be the last time that arm i is pulled. If arm i has been pulled only during the initialization
min (cid:0) − ∆i + 2βi (ti − 1), 0(cid:1) + 2βi (ti − 1)
phase, Ti (n) = 1 and Eq. 13 trivially holds. If i has been selected by SELECT-ARM, then we have
(a)≥ B (ti )
(b)≥ BJ (ti ) (ti )
(c)≥ BΩ(n) (t(cid:96) )
(d)
(14)
> ,
where t(cid:96) ∈ {1, . . . , n} is the time such that Ω(n) = J (t(cid:96) ). (a) and (b) are the results of Lemmas 2
and 3, (c) is by the deﬁnition of Ω(n), and (d) holds because using Lemma 1, we know that if
the algorithm suffers a simple regret rΩ(n) >  (as assumed at the beginning of the proof), then
∀t = 1, . . . , n + 1, BΩ(n) (t) > . By the deﬁnition of ti , we know Ti (n) = Ti (ti − 1) + 1. Using
this fact, the deﬁnition of βi (ti − 1), and Eq. 14, it is straightforward to show that Eq. 13 holds.
Step 2: We know that (cid:80)K
i=1 Ti (n) = n. Using Eq. 13, we have (cid:80)K
,(cid:1)2 + K > n
max (cid:0) ∆i+
4ab2
i=1
on event E . It is easy to see that by selecting a ≤ n−K
2
, the left-hand-side of this inequality will be
smaller than or equal to n, which is a contradiction. Thus, we conclude that rΩ(n) ≤  on event E .
4H
The ﬁnal result follows from the probability of event E deﬁned at the beginning of this section.

(13)

4.3 Regret Bound for the Fixed-Conﬁdence Setting

Here we prove an upper-bound on the simple-regret of UGapEc. Since the setting considered by the
algorithm is ﬁxed-conﬁdence, we may set T = +∞. From the deﬁnition of the conﬁdence interval
Theorem 2. The UGapEc algorithm stops after (cid:101)n rounds and returns a set of m arms, Ω((cid:101)n), that
βi (t) in Eq. 4 and a union bound on Tk (t) ∈ {0, . . . , t}, t = 1, . . . , ∞, we have that P(E ) ≥ 1 − δ .
P(cid:0)rΩ((cid:101)n+1) ≤  ∧ (cid:101)n ≤ N (cid:1) ≥ 1 − δ,
satisﬁes
where N = K + O(H log H
δ ) and c has been set to its optimal value 1/2.
event E , the simple regret of UGapEc upon stopping satisﬁes BJ ((cid:101)n+1) ((cid:101)n + 1) = BΩ((cid:101)n+1) ((cid:101)n + 1) ≥
Proof. We ﬁrst prove the bound on the simple regret of UGapEc. Using Lemma 1, we have that on
rΩ((cid:101)n+1) . As a result, on event E , the regret of UGapEc cannot be bigger than , because then it
contradicts the stopping condition of the algorithm, i.e., BJ ((cid:101)n+1) ((cid:101)n + 1) < . Therefore, we have
P(cid:0)rΩ((cid:101)n+1) ≤ (cid:1) ≥ 1 − δ . Now we prove the bound for the sample complexity. Similar to the proof
of Theorem 1, we consider the following two steps:
Step 1: Here we show that on event E , we have the following upper-bound on the number of pulls
Ti ((cid:101)n) ≤ 2b2 log(4K ((cid:101)n − 1)3/δ)
of any arm i ∈ A:
max (cid:0) ∆i+
, (cid:1)2
(15)
+ 1.
phase, Ti ((cid:101)n) = 1 and Eq. 15 trivially holds. If i has been selected by SELECT-ARM, then we have
2
Let ti be the last time that arm i is pulled. If arm i has been pulled only during the initialization
BJ (ti ) (ti ) ≤ min (cid:0)0, −∆i + 2βi (ti − 1)(cid:1) + 2βi (ti − 1).
BJ (ti ) (ti ) ≥ . Now using Lemma 2, we may write
(16)
We can prove Eq. 15 by plugging in the value of βi (ti − 1) from Eq. 4 and solving Eq. 16 for Ti (ti )
taking into account that Ti (ti − 1) + 1 = Ti (ti ).

7

i=1 Ti ((cid:101)n) = (cid:101)n. Using Eq. 15, on event E , we have 2H log (cid:0)K ((cid:101)n −
Step 2: We know that (cid:80)K
1)3/δ(cid:1) + K ≥ (cid:101)n. Solving this inequality gives us (cid:101)n ≤ N .
4.4 Problem Complexity

Theorems 1 and 2 indicate that both the probability of success and sample complexity of UGapE are
directly related to the complexity H deﬁned by Eq. 6. This implies that H captures the intrinsic
difﬁculty of the (,m)-best arm(s) identiﬁcation problem independently from the speciﬁc setting
considered. Furthermore, note that this deﬁnition generalizes existing notions of complexity. For
example, for  = 0 and m = 1 we recover the complexity used in the deﬁnition of UCB-E [1] for
the ﬁxed budget setting and the one deﬁned in [6] for the ﬁxed accuracy problem. Let us analyze
H in the general case of  > 0. We deﬁne the complexity of a single arm i ∈ A, H,i =
, )2 . When the gap ∆i is smaller than the desired accuracy , i.e., ∆i ≤ , then
b2/ max( ∆i+
2
the complexity reduces to H,i = 1/2 .
In fact, the algorithm can stop as soon as the desired
accuracy  is achieved, which means that there is no need to exactly discriminate between arm i and
the best arm. On the other hand, when ∆i > , then the complexity becomes H,i = 4b2 /(∆i + )2 .
This shows that when the desired accuracy is smaller than the gap, the complexity of the problem is
smaller than the case of  = 0, for which we have H0,i = 4b2 /∆2
i .
More in general, the analysis reported in the paper suggests that the performance of a upper con-
ﬁdence bound based algorithm such as UGapE is characterized by the same notion of complexity
in both settings. Thus, whenever the complexity is known, it is possible to exploit the theoretical
analysis (bounds on the performance) to easily switch from one setting to the other. For instance, as
also suggested in Section 5.4 of [9], if the complexity H is known, an algorithm like UGapEc can
be adapted to run in the ﬁxed budget setting by inverting the bound on its sample complexity. This
would lead to an algorithm similar to UGapEb with similar performance, although the parameter
tuning could be more difﬁcult because of the intrinsic poor accuracy in the constants of the bound.
On the other hand, it is an open question whether it is possible to ﬁnd an “equivalence” between
algorithms for the two different settings when the complexity is not known. In particular, it would
be important to derive a distribution-dependent lower bound in the form of the one reported in [1]
for the general case of  ≥ 0 and m ≥ 1 for both the ﬁxed budget and ﬁxed conﬁdence settings.

5 Summary and Discussion

We proposed a meta-algorithm, called uniﬁed gap-based exploration (UGapE), that uniﬁes the two
settings of the best arm(s) identiﬁcation problem in stochastic multi-armed bandit: ﬁxed budget and
ﬁxed conﬁdence. UGapE can be instantiated as two algorithms with a common structure (the same
arm-selection and arm-return strategies) corresponding to these two settings, whose performance
can be analyzed in a uniﬁed way, i.e., a large portion of their theoretical analysis can be uniﬁed in
a series of lemmas. We proved a performance bound for the UGapE algorithm in the two settings.
We also showed how UGapE and its theoretical analysis can be extended to take into account the
variance of the arms and to multiple bandits. Finally, we evaluated the performance of UGapE and
compare it with a number of existing ﬁxed budget and ﬁxed conﬁdence algorithms.
This uniﬁcation is important for both theoretical and algorithmic reasons. Despite their similarities,
ﬁxed budget and ﬁxed conﬁdence settings have been treated differently in the literature. We believe
that this uniﬁcation provides a better understanding of the intrinsic difﬁculties of the best arm(s)
identiﬁcation problem. In particular, our analysis showed that the same complexity term charac-
terizes the hardness of both settings. As mentioned in the introduction, there was no algorithm
available for several settings considered in this paper, e.g., (,m)-best arm identiﬁcation with ﬁxed
budget. With UGapE, we introduced an algorithm that can be easily adapted to all these settings.
Acknowledgments This work was supported by Ministry of Higher Education and Research, Nord-
Pas de Calais Regional Council and FEDER through the “contrat de projets état region 2007–2013",
French National Research Agency (ANR) under project LAMPADA n◦ ANR-09-EMER-007, Eu-
ropean Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n◦
270327, and PASCAL2 European Network of Excellence.

8

References
[1] J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identiﬁcation in multi-armed bandits. In
Proceedings of the Twenty-Third Annual Conference on Learning Theory, pages 41–53, 2010.
[2] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-armed bandit prob-
lem. Machine Learning, 47:235–256, 2002.
[3] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandit problems. In
Proceedings of the Twentieth International Conference on Algorithmic Learning Theory, pages
23–37, 2009.
[4] S. Bubeck, T. Wang, and N. Viswanathan. Multiple identiﬁcations in multi-armed bandits.
CoRR, abs/1205.3181, 2012.
[5] K. Deng, J. Pineau, and S. Murphy. Active learning for developing personalized treatment.
In Proceedings of the Twenty-Seventh International Conference on Uncertainty in Artiﬁcial
Intelligence, pages 161–168, 2011.
[6] E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for
the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning
Research, 7:1079–1105, 2006.
[7] V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identiﬁcation: A Uniﬁed Approach
to Fixed Budget and Fixed Conﬁdence. Technical report 00747005, October 2012.
[8] V. Gabillon, M. Ghavamzadeh, A. Lazaric, and S. Bubeck. Multi-bandit best arm identiﬁcation.
In Proceedings of Advances in Neural Information Processing Systems 25, pages 2222–2230,
2011.
[9] S. Kalyanakrishnan. Learning Methods for Sequential Decision Making with Imperfect Repre-
sentations. PhD thesis, Department of Computer Science, The University of Texas at Austin,
Austin, Texas, USA, December 2011. Published as UT Austin Computer Science Technical
Report TR-11-41.
[10] S. Kalyanakrishnan and P. Stone. Efﬁcient selection of multiple bandit arms: Theory and prac-
tice. In Proceedings of the Twenty-Seventh International Conference on Machine Learning,
pages 511–518, 2010.
[11] S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. Pac subset selection in stochastic multi-
armed bandits. In Proceedings of the Twentieth International Conference on Machine Learn-
ing, 2012.
[12] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classiﬁca-
tion and function approximation. In Proceedings of Advances in Neural Information Process-
ing Systems 6, pages 59–66, 1993.
[13] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In
22th annual conference on learning theory, 2009.
[14] V. Mnih, Cs. Szepesvári, and J.-Y. Audibert. Empirical Bernstein stopping. In Proceedings of
the Twenty-Fifth International Conference on Machine Learning, pages 672–679, 2008.
[15] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematics Society, 58:527–535, 1952.

9


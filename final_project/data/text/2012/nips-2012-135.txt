Optimal kernel choice for large-scale two-sample tests

Arthur Gretton,1,3 Bharath Sriperumbudur,1 Dino Sejdinovic,1 Heiko Strathmann2
1Gatsby Unit and 2CSD, CSML, UCL, UK; 3MPI for Intelligent Systems, Germany
{arthur.gretton,bharat.sv,dino.sejdinovic,heiko.strathmann}@gmail
Kenji Fukumizu
Massimiliano Pontil
Sivaraman Balakrishnan
ISM, Japan
CSD, CSML, UCL, UK
LTI, CMU, USA
fukumizu@ism.ac.jp
m.pontil@cs.ucl.ac.uk
sbalakri@cs.cmu.edu

Abstract

Given samples from distributions p and q , a two-sample test determines whether
to reject the null hypothesis that p = q , based on the value of a test statistic
measuring the distance between the samples. One choice of test statistic is the
maximum mean discrepancy (MMD), which is a distance between embeddings
of the probability distributions in a reproducing kernel Hilbert space. The kernel
used in obtaining these embeddings is critical in ensuring the test has high power,
and correctly distinguishes unlike distributions with high probability. A means of
parameter selection for the two-sample test based on the MMD is proposed. For a
given test level (an upper bound on the probability of making a Type I error), the
kernel is chosen so as to maximize the test power, and minimize the probability
of making a Type II error. The test statistic, test threshold, and optimization over
the kernel parameters are obtained with cost linear in the sample size. These
properties make the kernel selection and test procedures suited to data streams,
where the observations cannot all be stored in memory. In experiments, the new
kernel selection approach yields a more powerful test than earlier kernel selection
heuristics.

1 Introduction

The two sample problem addresses the question of whether two independent samples are drawn from
the same distribution. In the setting of statistical hypothesis testing, this corresponds to choosing
whether to reject the null hypothesis H0 that the generating distributions p and q are the same, vs.
the alternative hypothesis HA that distributions p and q are different, given a set of independent
observations drawn from each.
A number of recent approaches to two-sample testing have made use of mappings of the distribu-
tions to a reproducing kernel Hilbert space (RKHS); or have sought out RKHS functions with large
amplitude where the probability mass of p and q differs most [8, 10, 15, 17, 7]. The most straight-
forward test statistic is the norm of the difference between distribution embeddings, and is called
the maximum mean discrepancy (MMD). One difﬁculty in using this statistic in a hypothesis test,
however, is that the MMD depends on the choice of the kernel. If we are given a family of kernels,
we obtain a different value of the MMD for each member of the family, and indeed for any positive
deﬁnite linear combination of the kernels. When a radial basis function kernel (such as the Gaus-
sian kernel) is used, one simple choice is to set the kernel width to the median distance between
points in the aggregate sample [8, 7]. While this is certainly straightforward, it has no guarantees of
optimality. An alternative heuristic is to choose the kernel that maximizes the test statistic [15]: in
experiments, this was found to reliably outperform the median approach. Since the MMD returns
a smooth RKHS function that minimizes classiﬁcation error under linear loss, then maximizing the

1

MMD corresponds to minimizing this classiﬁcation error under a smoothness constraint.
If the
statistic is to be applied in hypothesis testing, however, then this choice of kernel does not explicitly
address the question of test performance.
We propose a new approach to kernel choice for hypothesis testing, which explicitly optimizes the
performance of the hypothesis test. Our kernel choice minimizes Type II error (the probability of
wrongly accepting H0 when p ￿= q ), given an upper bound on Type I error (the probability of
wrongly rejecting H0 when p = q ). This corresponds to optimizing the asymptotic relative efﬁ-
ciency in the sense of Hodges and Lehmann [13, Ch. 10]. We address the case of the linear time
statistic in [7, Section 6], for which both the test statistic and the parameters of the null distribu-
tion can be computed in O(n), for sample size n. This has a higher variance at a given n than
the U-statistic estimate costing O(n2 ) used in [8, 7], since the latter is the minimum variance un-
biased estimator. Thus, we would use the quadratic time statistic in the “limited data, unlimited
time” scenario, as it extracts the most possible information from the data available. The linear time
statistic is used in the “unlimited data, limited time” scenario, since it is the cheapest statistic that
still incorporates each datapoint: it does not require the data to be stored, and is thus appropriate
for analyzing data streams. As a further consequence of the streaming data setting, we learn the
kernel parameter on a separate sample to the sample used in testing; i.e., unlike the classical testing
scenario, we use a training set to learn the kernel parameters. An advantage of this setting is that our
null distribution remains straightforward, and the test threshold can be computed without a costly
bootstrap procedure.
We begin our presentation in Section 2 with a review of the maximum mean discrepancy, its linear
time estimate, and the associated asymptotic distribution and test. In Section 3 we describe a cri-
terion for kernel choice to maximize the Hodges and Lehmann asymptotic relative efﬁciency. We
demonstrate the convergence of the empirical estimate of this criterion when the family of kernels is
a linear combination of base kernels (with non-negative coefﬁcients), and of the kernel coefﬁcients
themselves. In Section 4, we provide an optimization procedure to learn the kernel weights. Finally,
in Section 5, we present experiments, in which we compare our kernel selection strategy with the
approach of simply maximizing the test statistic subject to various constraints on the coefﬁcients of
the linear combination; and with a cross-validation approach, which follows from the interpretation
of the MMD as a classiﬁer. We observe that a principled kernel choice for testing outperforms com-
peting heuristics, including the previous best-performing heuristic in [15]. A Matlab implementation
is available at: www.gatsby.ucl.ac.uk/ ∼ gretton/adaptMMD/adaptMMD.htm
2 Maximum mean discrepancy, and a linear time estimate

We begin with a brief review of kernel methods, and of the maximum mean discrepancy [8, 7, 14].
We then describe the family of kernels over which we optimize, and the linear time estimate of the
MMD.

2.1 MMD for a family of kernels
Let Fk be a reproducing kernel Hilbert space (RKHS) deﬁned on a topological space X with repro-
ducing kernel k , and p a Borel probability measure on X . The mean embedding of p in Fk is a unique
element µk (p) ∈F k such that Ex∼p f (x) = ￿f , µk (p)￿Fk for all f ∈F k [4]. By the Riesz rep-
resentation theorem, a sufﬁcient condition for the existence of µk (p) is that k be Borel-measurable
and Ex∼pk1/2 (x, x) < ∞. We assume k is a bounded continuous function, hence this condition
holds for all Borel probability measures. The maximum mean discrepancy (MMD) between Borel
probability measures p and q is deﬁned as the RKHS-distance between the mean embeddings of p
and q . An expression for the squared MMD is thus
ηk (p, q) = ￿µk (p) − µk (q)￿2
= Exx￿ k(x, x￿ ) + Eyy ￿ k(y , y ￿ ) − 2Exy k(x, y),
Fk
where x, x￿ i.i.d.∼ p and y , y ￿ i.i.d.∼ q . By introducing
hk (x, x￿ , y , y ￿ ) = k(x, x￿ ) + k(y , y ￿ ) − k(x, y ￿ ) − k(x￿ , y),
ηk (p, q) = Exx￿ yy ￿ hk (x, x￿ , y , y ￿ ) =: Ev hk (v),

we can write

(1)

(2)

2

βuku ,

where we have deﬁned the random vector v := [x, x￿ , y , y ￿ ]. If µk is an injective map, then k is said
to be a characteristic kernel, and the MMD is a metric on the space of Borel probability measures,
i.e., ηk (p, q) = 0 iff p = q [16]. The Gaussian kernels used in the present work are characteristic.
Our goal is to select a kernel for hypothesis testing from a particular family K of kernels, which we
now deﬁne. Let {ku }d
u=1 be a set of positive deﬁnite functions ku : X×X→ R. Let
βu = D , βu ≥ 0, ∀u ∈{ 1, . . . , d}￿
K := ￿k : k =
d￿u=1
d￿u=1
for some D > 0, where the constraint on the sum of coefﬁcients is needed for the consistency proof
(see Section 3). Each k ∈K is associated uniquely with an RKHS Fk , and we assume the kernels
are bounded, |ku |≤ K, ∀u ∈{ 1, . . . , d}. The squared MMD becomes
d￿u=1
ηk (p, q) = ￿µk (p) − µk (q)￿2
Fk
where ηu (p, q) := Ev hu (v). It is clear that if every kernel ku , u ∈{ 1, . . . , d}, is characteristic and at
least one βu > 0, then k is characteristic. Where there is no ambiguity, we will write ηu := ηu (p, q)
and Ehu := Ev hu (v). We denote h = (h1 , h2 , . . . , hd )￿ ∈ Rd×1 , β = (β1 ,β 2 , . . . ,β d )￿ ∈ Rd×1 ,
and η = (η1 ,η 2 , . . . ,η d )￿ ∈ Rd×1 . With this notation, we may write
ηk (p, q) = E(β￿h) = β￿ η.

βu ηu (p, q),

(3)

=

2.2 Empirical estimate of the MMD, asymptotic distribution, and test

We now describe an empirical estimate of the maximum mean discrepancy, given i.i.d. samples
X := {x1 , . . . , xn } and Y := {y1 , . . . , yn } from p and q , respectively. We use the linear time
estimate of [7, Section 6], for which both the test statistic and the parameters of the null distribution
can be computed in time O(n). This has a higher variance at a given n than a U-statistic estimate
costing O(n2 ), since the latter is the minimum variance unbiased estimator [13, Ch. 5]. That
said, it was observed experimentally in [7, Section 8.3] that the linear time statistic yields better
performance at a given computational cost than the quadratic time statistic, when sufﬁcient data
are available (bearing in mind that consistent estimates of the null distribution in the latter case are
computationally demanding [9]). Moreover, the linear time statistic does not require the sample
to be stored in memory, and is thus suited to data streaming contexts, where a large number of
observations arrive in sequence.
The linear time estimate of ηk (p, q) is deﬁned in [7, Lemma 14]: assuming for ease of notation that
n is even,
n/2￿i=1
2
n
where vi := [x2i−1 , x2i , y2i−1 , y2i ] and hk (vi ) := k(x2i−1 , x2i ) + k(y2i−1 , y2i ) − k(x2i−1 , y2i ) −
k(x2i , y2i−1 ); this arrangement of the samples ensures we get an expectation over independent
variables as in (2) with cost O(n). We use ˇηk to denote the empirical statistic computed over the
samples being tested, to distinguish it from the training sample estimate ˆηk used in selecting the
kernel. Given the family of kernels K in (3), this can be written ˇηk = β￿ ˇη , where we again use
the convention ˇη = ( ˇη1 , ˇη2 , . . . , ˇηd )￿ ∈ Rd×1 . The statistic ˇηk has expectation zero under the null
hypothesis H0 that p = q , and has positive expectation under the alternative hypothesis HA that
p ￿= q .
Since ˇηk is a straightforward average of independent random variables, its asymptotic distribution
is given by the central limit theorem (e.g. [13, Section 1.9]). From [7, corollary 16], under the
k ) < ∞ (which is true for bounded continuous k),
assumption 0 < E(h2
n1/2 ( ˇηk − ηk (p, q)) D→N (0, 2σ2
k ),
where the factor of two arises since the average is over n/2 terms, and
k (v) − [Ev (hk (v))]2 .
σ2
k = Ev h2

hk (vi ),

ˇηk =

(6)

(4)

(5)

3

k = β￿ ˇQk β, where
ˇσ2

Unlike the case of a quadratic time statistic, the null and alternative distributions differ only in
mean; by contrast, the quadratic time statistic has as its null distribution an inﬁnite weighted sum of
χ2 variables [7, Section 5], and a Gaussian alternative distribution.
To obtain an estimate of the variance based on the samples X, Y , we will use an expression derived
from the U-statistic of [13, p. 173] (although as earlier, we will express this as a simple average so
as to compute it in linear time). The population variance can be written
1
Ev ,v ￿ (hk (v) − hk (v ￿ ))2 .
k (v) − Ev ,v ￿ (hk (v)hk (v ￿ )) =
σ2
k = Ev h2
2
Expanding in terms of the kernel coefﬁcients β , we get
k := β￿Qk β,
σ2
where Qk := cov(h) is the covariance matrix of h. A linear time estimate for the variance is
n/4￿i=1
￿ ˇQk ￿uu￿ =
4
n
and wi := [v2i−1 , v2i ],1 h∆,k (wi ) := hk (v2i−1 ) − hk (v2i ).
We now address the construction of a hypothesis test. We denote by Φ the CDF of a standard Normal
random variable N (0, 1), and by Φ−1 the inverse CDF. From (5), a test of asymptotic level α using
the statistic ˇηk will have the threshold
tk,α = n−1/2σk√2Φ−1 (1 − α),
(8)
bearing in mind the asymptotic distribution of the test statistic, and that ηk (p, p) = 0. This threshold
is computed empirically by replacing σk with its estimate ˇσk (computed using the data being tested),
which yields a test of the desired asymptotic level.
The asymptotic distribution (5) holds only when the kernel is ﬁxed, and does not depend on the
sample X, Y . If the kernel were a function of the data, then a test would require large deviation
probabilities over the supremum of the Gaussian process indexed by the kernel parameters (e.g.
[1]). In practice, the threshold would be computed via a bootstrap procedure, which has a high
computational cost. Instead, we set aside a portion of the data to learn the kernel (the “training
data”), and use the remainder to construct a test using the learned kernel parameters.

h∆,u (wi )h∆,u￿ (wi ),

(7)

3 Choice of kernel

The choice of kernel will affect both the test statistic itself, (4), and its asymptotic variance, (6).
Thus, we need to consider how these statistics determine the power of a test with a given level α (the
upper bound on the Type I error). We consider the case where p ￿= q . A Type II error occurs when
the random variable ˇηk falls below the threshold tk,α deﬁned in (8). The asymptotic probability of a
Type II error is therefore
σk√2 ￿ .
P ( ˇηk < tk,α ) =Φ ￿Φ−1 (1 − α) −
ηk (p, q)√n
As Φ is monotonic, the Type II error probability will decrease as the ratio ηk (p, q)σ−1
k
Therefore, the kernel minimizing this error probability is
ηk (p, q)σ−1
k∗ = arg sup
k ,
k∈K
with the associated test threshold tk∗ ,α . In practice, we do not have access to the population estimates
ηk (p, q) and σk , but only their empirical estimates ˆηk , ˆσk from m pairs of training points (xi , yi )
(this training sample must be independent of the sample used to compute the test parameters ˇηk , ˇσk ).
We therefore estimate tk∗ ,α by a regularized empirical estimate tˆk∗ ,α , where
ˆηk ( ˆσk,λ )−1 ,
ˆk∗ = arg sup
k∈K
1This vector is the concatenation of two four-dimensional vectors, and has eight dimensions.

increases.

(9)

4

and

and we deﬁne the regularized standard deviation ˆσk,λ = ￿β￿ ￿ ˆQ + λm I ￿ β = ￿ ˆσ2
2 .
k + λm ￿β ￿2
The next theorem shows the convergence of supk∈K ˆηk ( ˆσk,λ )−1 to supk∈K ηk (p, q)σ−1
k , and of ˆk∗
to k∗ , for an appropriate schedule of decrease for λm with increasing m.
Theorem 1. Let K be deﬁned as in (3). Assume supk∈K,x,y∈X |k(x, y)| < K and σk is bounded
away from zero. Then if λm =Θ ￿m−1/3 ￿,
￿￿￿￿ sup
k ￿￿￿￿ = OP ￿m−1/3￿
P→ k∗ .
ˆk∗
ηk σ−1
ˆηk ˆσ−1
k,λ − sup
k∈K
k∈K
Proof. Recall from the deﬁnition of K that ￿β ￿1 = D , and that ￿β ￿2 ≤ ￿β ￿1 and ￿β ￿1 ≤ √d ￿β ￿2
[11, Problem 3 p. 278], hence ￿β ￿2 ≥ Dd−1/2 . We begin with the bound
￿￿￿￿ sup
k ￿￿￿￿ ≤ sup
k∈K ￿￿￿ ˆηk ˆσ−1
k ￿￿￿
k,λ − ηk σ−1
ˆηk ˆσ−1
ηk σ−1
k,λ − sup
k∈K
k∈K
k∈K ￿￿￿ ˆηk ˆσ−1
k∈K ￿￿￿ηk σ−1
k,λ ￿￿￿ + sup
k ￿￿￿
k,λ − ηk σ−1
k,λ − ηk σ−1
≤ sup
σk,λ (σk,λ + σk ) ￿￿￿￿￿
σk ￿￿￿￿￿
ˆσk,λ σk,λ ￿￿￿￿ + sup
ηk ￿￿￿￿
k,λ − σ2
σ2
2λm ￿−1/2 | ˆηk − ηk | + sup
k∈K ￿ ˆσ2
ˆσk,λ − σk,λ
ηk
k
k + ￿β ￿2
≤ sup
k∈K
k∈K
ηk ￿￿￿￿￿
m )1/2 ￿￿￿￿￿
C1√d
ˆσk,λ − σk,λ
D√λm
k∈K | ˆηk − ηk | + sup
sup
k ˆσ2
(σ2
k ) + ￿β ￿2
k + ￿β ￿2
2λm (σ2
k + ˆσ2
2λ2
k∈K
σk ￿ ￿β ￿2
k ￿
ηk
2λm
+ sup
￿β ￿2
2λm + σ2
k∈K
√d
D√λm ￿C1 sup
k∈K | ˆσk,λ − σk,λ |￿ + C3D2λm ,
≤
k∈K | ˆηk − ηk | + C2 sup
where constants C1 , C2 , and C3 follow from the boundedness of σk and ηk . The the ﬁrst result in the
theorem follows from supk∈K | ˆηk − ηk | = OP (m−1/2 ) and supk∈K | ˆσk,λ − σk,λ | = OP (m−1/2 ),
which are proved using McDiarmid’s Theorem [12] and results from [3]: see Appendix A of the
supplementary material.
Convergence of ˆk∗ to k∗ : For k ∈K deﬁned in (3), we show in Section 4 that ˆk∗ and k∗ are unique
P→ supk∈K
k,λ and ηk σ−1
optimizers of ˆηk ˆσ−1
ˆηk
ηk
k , respectively. Since supk∈K
, the result follows
σk
ˆσk,λ
from [18, Corollary 3.2.3(i)].

≤

We remark that other families of kernels may be worth considering, besides K. For instance, we
could use a family of RBF kernels with continuous bandwidth parameter θ ≥ 0. We return to this
point in the conclusions (Section 6).

4 Optimization procedure
We wish to select kernel k = ￿d
ˆβ ∗uku ∈K that maximizes the ratio ˆηk / ˆσk,λ . We perform
u=1
this optimization over training data, then use the resulting parameters ˆβ ∗ to construct a hypothesis
test on the data to be tested (which must be independent of the training data, and drawn from the
same p, q ). As discussed in Section 2.2, this gives us the test threshold without requiring a bootstrap
procedure. Recall from Sections 2.2 and 3 that ˆηk = β￿ ˆη , and ˆσk,λ = ￿β￿ ￿ ˆQ + λm I ￿ β ,
where ˆQ is a linear-time empirical estimate of the covariance matrix cov(h). Since the objective
α(β ; ˆη, ˆQ) := ￿β￿ ˆη￿ ￿β￿ ￿ ˆQ + λm I ￿ β￿−1/2
is a homogenous function of order zero in β , we
can omit the constraint ￿β ￿1 = D , and set
ˆβ ∗ = arg max
β￿0

α(β ; ˆη, ˆQ).

(10)

5


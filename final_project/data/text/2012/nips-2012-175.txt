Proper losses for learning from partial labels

Jes ´us Cid-Sueiro
Department of Signal Theory and Communications
Universidad Carlos III de Madrid
Legans-Madrid, 28911 Spain
jcid@tsc.uc3m.es

Abstract

This paper discusses the problem of calibrating posterior class probabilities from
partially labelled data. Each instance is assumed to be labelled as belonging to
one of several candidate categories, at most one of them being true. We generalize
the concept of proper loss to this scenario, we establish a necessary and sufﬁcient
condition for a loss function to be proper, and we show a direct procedure to
construct a proper loss for partial labels from a conventional proper loss. The
problem can be characterized by the mixing probability matrix relating the true
class of the data and the observed labels. The full knowledge of this matrix is not
required, and losses can be constructed that are proper for a wide set of mixing
probability matrices.

1

Introduction

The problem of learning multiple classes from data with imprecise label information has attracted
a recent attention in the literature. It arises in many different applications: Cour [1] cites some of
them: picture collections containing several faces per image and a caption that only speciﬁes who is
in the picture but not which name matches which face, or video collections with labels taken from
annotations.
In a partially labelled data set, each instance is assigned to a set of candidate categories, at most only
one of them true. The problem is closely related to learning from noisy labels, which is common
in human-labelled data bases with multiple annotators [2] [3] medical imaging, crowdsourcing, etc.
Other related problems can be interpreted as particular forms of partial labelling: semisupervised
learning, or hierarchical classiﬁcation in databases where some instances could be labelled with
respect to parent categories only. It is also a particular case of the more general problems of learning
from soft labels [4] or learning from measurements [5].
Several algorithms have been proposed to deal with partial labelling [1] [2] [6] [7] [8]. Though
some theoretical work has been addressed in order to analyze the consistency of algorithms [1] or
the information provided by uncertain data [8], little effort has been done to analyze the conditions
under which the true class can be inferred from partial labels.
In this paper we address the problem of estimating posterior class probabilities from partially la-
belled data. In particular, we obtain general conditions under which the posterior probability of the
true class given the observation can be estimated from training data with ambiguous class labels. To
do so, we generalize the concept of proper losses to losses that are functions of ambiguous labels,
and show that the capability to estimate posterior class probabilities using a given loss depends on
the probability matrix relating the ambiguous labels with the true class of the data. Each general-
ized proper loss can be characterized by the set (a convex polytope) of all admissible probability
matrices. Analyzing the structure of these losses is one of the main goals of this paper. Up to our
knowledge, the design of proper losses for learning from imperfect labels has not been addressed in
the area of Statistical Learning.

1

The paper is organized as follows: Sec. 2 formulates the problem discussed in the paper, Sec. 3
generalizes proper losses to scenarios with ambiguous labels, Sec. 4 proposes a procedure to design
proper losses for wide sets of mixing matrices, Sec. 5 discusses estimation errors and Sec. 6 states
some conclusions.

2 Formulation

2.1 Notation

Vectors are written in boldface, matrices in boldface capital and sets in calligraphic letters. For any
i is a n-dimensional unit vector with all zero components apart from the i-th component
integer n, en
simplex of n-dimensional probability vectors is Pn = {p ∈ [0, 1]n : (cid:80)n−1
which is equal to one, and 1n is a n-dimensional all-ones vector. Superindex T denotes transposition.
We will use (cid:96)() to denote a loss based on partial labels, and ˜(cid:96) to losses based on true labels. The
i=0 pi = 1} and the set of
all left-stochastic matrices is M = {M ∈ [0, 1]d×c : MT1d = 1c }. The number of classes is c, and
the number of possible partial label vectors is d ≤ 2c .

(1)

2.2 Learning from partial labels
Let X be a sample set, Y = {ec
j , j = 0, 1, . . . , c − 1}, a set of labels, and Z ⊂ {0, 1}c a set of
partial labels. Sample (x, z) ∈ X × Z is drawn from an unknown distribution P .
Partial label vector z ∈ Z is a noisy version of the true label y ∈ Y . Several authors [1] [6] [7] [8]
assume that the true label is always present in z, i.e., zj = 1 when yj = 1, but this assumption is
not required in our setting, which admits noisy label scenarios (as, for instance, in [2]). Without loss
of generality, we assume that Z contains only partial labels with nonzero probability (i.e. P {z =
b} > 0 for any b ∈ Z ).
In general, we model the relationship between z and y through an arbitrary d × c conditional mixing
probability matrix M(x) with components
mij (x) = P {z = bi |yj = 1, x}
where bi ∈ Z is the i-th element of Z for some arbitrary ordering.
Note that, in general, the mixing matrix could depend on x, though a constant mixing matrix [2] [6]
[7] [8] is a common assumption, as well as the statistical independence of the incorrect labels [6] [7]
[8]. In this paper we do not impose these assumptions.
The goal is to infer y given x without knowing model P . To do so, a set of partially labelled samples,
S = {(xk , zk ), k = 1, . . . , K } is available. True labels yk are not observed.
We will illustrate different partial label scenarios with a 3-class problem. Consider that each column
of MT corresponds to a label pattern (z0 , z1 , z2 ) following the ordering (0, 0, 0), (1, 0, 0), (0, 1, 0),
(0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1) (e.g. the ﬁrst column contains P {z = (0, 0, 0)T |yj =
(cid:33)T
(cid:32) 0
1), for j = 0, 1, 2).
0
0
1
0
(cid:32) 0
A. Supervised learning: M =
0
1
0
0
0
0
1
0
0
0
1 − α α/2
1 − β
(cid:32) α 1 − α 0
0 β /2
γ /2
γ /2
0
1 − β
C. Semisupervised learning: M =
0
β
(cid:32) 0
γ
0
0
D. True label with independent noisy labels:
1 − α − α2
0
0
1 − β − β 2
0
0
0
1 − γ − γ 2
0
0
0

(cid:33)T
0
0
(cid:33)T
0

B. Single noisy labels: M =

α/2 α/2
0
β /2
0
γ /2

α2
0
β /2 β 2
γ 2
γ /2

0
0
0

0
0
0

0
0
0

0
0
1 − γ

0
0
0

0
0
0

0
0
0

0
0
0

0
0
0

M =

0
0
0

0
0
0

α/2
β /2
1 − γ

(cid:33)T

2

E. Two labels, one of them true: M =

(cid:32) 0
0
0

1 − α 0
1 − β
0
0
0

0
0
1 − γ

α/2 α/2
0
β /2
0
γ /2

0
β /2
γ /2

0
0
0

(cid:33)T

The question that motivates our work is the following: knowing M (i.e. knowing the scenario and
the value of parameters α, β and γ ), we can estimate accurate posterior class probabilities from
partially labelled data in all these cases, however, is it possible if α, β and γ are unknown? We will
see that the answer is negative for scenarios B,C,D, but it is positive for E. In the positive case, no
information is lost by the partial label process for inﬁnite sample sizes. In the negative case, some
performance is lost as a consequence of the mixing process, that persists even for inﬁnite sample
sizes 1

2.3

Inference through partial label probabilities

mij (x)P {yj = 1|x},

If the mixing matrix is known, a conceptually simple strategy to solve the partial label problem
consists of estimating posterior partial label probabilities, using them to estimate posterior class
c−1(cid:88)
probabilities and predict y. Since
P {z = bi |x} =
j=0
we can deﬁne vectors p(x) and η(x) with components pi = P {z = bi |x} and ηj = P {yj = 1|x},
to write (2) as p(x) = M(x)η(x) and, thus,
η(x) = M+ (x)p(x)
where M+ (x) = (MT (x)M(x))−1MT (x) is the left inverse (pseudoinverse) of M(x).
Thus, a ﬁrst condition to estimate η from p given M is that the conditional mixing matrix has a left
inverse (i.e., the columns of M(x) are linearly independent).
There are some trivial cases where the mixing matrix has no pseudoinverse (for instance, if
P {z|y, x} = P {z|x}, all rows in M(x) are equal, and MT (x)M(x) is a rank 1 matrix, which
has no inverse), but these are degenerate cases of no practical interest. From a practical point of
view, the application of (3) states two major problems: (1) when the model P is unknown, even
knowing M, estimating p from data may be infeasible for d close to 2c and a large number of
classes (furthermore, posterior probability estimates will not be accurate if the sample size is small),
and (2) M(x) is generally unknown, and cannot be estimated from the partially labelled set, S .
The solution adopted in this paper for the ﬁrst problem consists of estimating η from data without
estimating p. This is discussed in the next section. The second problem is discussed in Section 4.

(2)

(3)

3 Loss functions for posterior probability estimation

The estimation of posterior probabilities from labelled data is a well known problem in statistics and
machine learning, that has received some recent attention in the machine learning literature [9] [10].
In order to estimate posteriors from labelled data, a loss function ˜(cid:96)(y, ˆη) is required such that η is
a member of arg min ˆη Ey { ˜(cid:96)(y, ˆη)}. Losses satisfying this property are said to be Fisher consistent
and are known as proper scoring rules. A loss is strictly proper if η is the only member of this set.
A loss is regular if it is ﬁnite for any y, except possibly that ˜(cid:96)(y, ˆη) = ∞ if yj = 1 and ˆηj = 0.
Proper scoring rules can be characterized by the Savage’s representation [11] [12]
Theorem 3.1 A regular scoring rule ˜(cid:96) : Z × Pc → R is (strictly) proper if and only if
˜(cid:96)(y, ˆη) = h( ˆη) + g( ˆη)(y − ˆη)
(4)
where h is a (strictly) concave function and g( ˆη) is a supergradient of h at the point ˆη , for all
ˆη ∈ Pc .
1 If the sample size is large (in particular for scenarios C and D), one could think of simply ignoring samples
with imperfect labels, and training the classiﬁer with the samples whose class is known. However, in general,
there is some bias in this process, which eventually can degrade performance.

3

(Remind that g is a supergradient of h at ˆη if h(η) ≤ h( ˆη) + gT (η − ˆη)).
In order to deal with partial labels, we generalize proper losses as follows
Deﬁnition Let y and z be random vectors taking values in Y and Z , respectively. A scoring rule
(cid:96)(z, ˆη) is proper to estimate η (with components ηj = P {yj = 1}) from z if
η ∈ arg min
Ez {(cid:96)(z, ˆη)}
ˆη
It is strictly proper if η is the only member of this set.

(5)

This generalized family of proper scoring rules can be characterized by the following.

Theorem 3.2 Scoring rule (cid:96)(z, ˆη) is (strictly) proper to estimate η from z if and only if the equiva-
lent loss

˜(cid:96)(y, ˆη) = yTMT l( ˆη),
(6)
where l( ˆη) is a vector with components (cid:96)i ( ˆη) = (cid:96)(bi , ˆη) and bi is the i-th element in Z (according
to some arbitrary ordering), is (strictly) proper.
c−1(cid:88)
d−1(cid:88)
d−1(cid:88)
Proof The proof is straightforward by noting that the expected loss can be expressed as
j=0
i=0
i=0
= ηTMT l( ˆη) = Ey {yTMT l( ˆη)} = Ey { ˜(cid:96)(y, ˆη)}
(7)
Therefore, arg min ˆη Ez{(cid:96)(z, ˆη)} = arg min ˆη Ey { ˜(cid:96)(y, ˆη)} and, thus, (cid:96) is (strictly) proper with
respect to y iff ˜(cid:96) is (strictly) proper.

P {z = bi }(cid:96)i ( ˆη) =

Ez{(cid:96)(z, ˆη)} =

mij ηj (cid:96)i ( ˆη)

Note that, deﬁning vector ˜l( ˆη) with components ˜(cid:96)j ( ˆη) = ˜(cid:96)(ec
j , ˆη), we can write
˜l( ˆη) = MT l( ˆη)
We will use this vector representation of losses extensively in the following.
Th. 3.2 states that the proper character of a loss for estimating η from z depends on M. For this
reason, in the following we will say that (cid:96)(z, ˆη) is M-proper if it is proper to estimate η from z.

(8)

4 Proper losses for sets of mixing matrices

Eq. (8) may be useful to check if a given loss is M-proper. However, note that, since matrix MT is
d × c, it has no left inverse, and we cannot take MT out from the left side of (8) to compute (cid:96) from ˜(cid:96).
For any given M and any given equivalent loss ˜l( ˆη), there is an uncountable number of losses l( ˆη)
satisfying (8).

Example Let ˜(cid:96) be an arbitrary proper loss for a 3-class problem. The losses
(cid:96)(z, ˆη) = (z0 − z1 z2 ) ˜(cid:96)0 ( ˆη) + (z1 − z0 z2 ) ˜(cid:96)1 ( ˆη) + (z2 − z0 z1 ) ˜(cid:96)2 ( ˆη)
(cid:96)(cid:48) (z, ˆη) = z0
˜(cid:96)0 ( ˆη) + z1
(cid:20) 1
are M-proper for the mixing matrix M given by
if bi = ec
j
otherwise
0
Note that M corresponds to a situation where labels are perfectly labelled, and z contains perfect
information about y (in fact, z = y with probability one).

˜(cid:96)1 ( ˆη) + z2

˜(cid:96)2 ( ˆη)

mij =

(9)

(10)

(11)

Also, for any (cid:96)(z, ˆη), there are different mixing matrices such that the equivalent loss is the same.

4

Example The loss given by (9) is M-proper for the mixing matrix M in (11) and it is also N-proper,
(cid:20) 1/2
for N with components
0
Matrix N corresponds to a situation where label z contains the true class and another noisy compo-
nent taken at random from the other classes.

k , for some k (cid:54)= j
if bi = ec
j + ec
otherwise

nij =

(12)

In general, if l( ˆη) is M-proper and N-proper with equivalent loss ˜l( ˆη), then it is also Q-proper with
the same equivalent loss, for any Q in the form
Q = M(I − D) + ND
(13)
where D is a diagonal nonnegative matrix (note that Q is a probability matrix, because QT1d = 1c ).
This is because
QT l( ˆη) = (I − D)MT l( ˆη) + DNT l( ˆη) = (I − D)˜l( ˆη) + D˜l( ˆη) = ˜l( ˆη)
More generally, for arbitrary non-diagonal matrices D, provided that Q is a probability matrix, l( ˆη)
is Q-proper.

(14)

Example Assuming diagonal D, if M and N are the mixing matrices deﬁned in (11) and (12),
respectively, the loss (9) is Q-proper for any mixing matrix Q in the form (13). This corresponds to
 dj j
a matrix with components
(1 − dj j )/2
0
That is, the loss in (9) is proper for any situation where the label z contains the true class and
possibly another class taken at random, and the probability that the true label is corrupted may be
class-dependent.

if bi = ec
k , for some k (cid:54)= j
j
if bi = ec
j + ec
otherwise

qij =

(15)

4.1 Building proper losses from ambiguity sets

The ambiguity on M for a given loss l( ˆη) can be used to deal with the second problem mentioned
in Sec. 2.3: in general, the mixing matrix may be unknown, or, even if it is known, it may depend
on the observation, x. Thus, we need a procedure to design losses that are proper for a wide family
of mixing matrices. In general, given a set of mixing matrices, Q, we will say that (cid:96) is Q-proper if
it is M-proper for any M ∈ Q
The following result provides a way to construct a proper loss (cid:96) for partial labels from a given
conventional proper loss ˜(cid:96).
vectors with dimension d, such that (cid:80)c−1
i ∈ Pd , 1 ≤ i ≤ nj } be a set of nj > 0 probability
Theorem 4.1 For 0 ≤ j ≤ c − 1, let Vj = {vj
j=0 nj = d and span(∪c−1
j=0Vj ) = Rd and let Q = {M ∈
M : Mec
j ∈ span(Vj ) ∩ Pd}.
Then, for any (strictly) proper loss ˜(cid:96)(y, ˆη), there exists a loss (cid:96)(z, ˆη) which is (strictly) Q-proper.
Proof The proof is constructive. Let V be a d×d matrix whose columns are the elements of ∪c−1
j=0Vj ,
which is invertible since span(∪c−1
j=0Vj ) = Rd . Let c( ˆη) be a d × 1 vector such that ci ( ˆη) = ˜(cid:96)j ( ˆη)
i ∈ Vj .
if Ved
Let (cid:96)(z, ˆη) be a loss deﬁned by vector l( ˆη) = (VT )−1c( ˆη).
Consider the set R = {M ∈ M : Mec
j ∈ Vj for all j } (which is not empty because nj > 0).
Since the columns of any M ∈ R are also columns of V, then MT l( ˆη) = ˜l( ˆη) and, thus, (cid:96)(z, ˆη) is
M-proper. Therefore, it is also proper for any afﬁne combination of matrices in R inside Pd . But
span(R) ∩ Pd = Q. Thus, (cid:96)(z, ˆη) is M-proper for all M ∈ Q (i.e. it is Q-proper).

5

Theorem 4.1 shows that we can construct proper losses for learning from partial labels by specifying
the points of sets Vj , j = 0, . . . , c − 1. Each of these sets deﬁnes an ambiguity set Aj = span(Vj ) ∩
Pd which represents all admissible conditional distributions for P (z|yj = 1). If the columns of the
true mixing matrix M are members of the ambiguity set, the resulting loss can be used to estimate
posterior class probabilities from the observed partial labels.
Thus, a general procedure to design a loss function for learning from partial labels is:

1. Select a proper loss, ˜(cid:96)(y, ˆη)
2. Deﬁne the ambiguity sets by choosing, for each class j , a set Vj of nj linearly independent
basis vectors for each class. The whole set of d basis vectors must be linearly independent.
3. Construct matrix V whose columns comprise all basis vectors.
4. Construct binary matrix U with uj i = 1 if the i-th column of V is in Vj , and uj i = 0
otherwise.
5. Compute the desired proper loss vector as
l( ˆη) = (VT )−1U˜l( ˆη)
(16)
Since the ambiguity set Aj is the intersection of a nj -dimensional linear subspace with the d-
dimensional probability simplex, it is a nj − 1 dimensional convex polytope whose vertices lie
in distinct (nj − 1)-faces of Pd . These vertices must have a set of at least nj − 1 zero components
which cannot be a set of zeros in any other vertex.
This has two consequences: (1) we can deﬁne the ambiguity sets from these vertices, and (2), the
choice is not unique, because the number of vertices can be higher than nj − 1.
If proper loss ˜(cid:96)(y, ˆη) is non degenerate, Q contains all mixing matrices for which a loss is proper:

Theorem 4.2 Let us assume that, if aT˜l( ˆη) = 0 for any ˆη , then a = 0. Under the conditions of
Theorem 4.1, for any M ∈ M \ Q, (cid:96)(z, ˆη) is not M-proper.
Rd . Thus, the n-th column of any arbitrary M can be represented as mn = (cid:80)c
Proof Since the columns of V are in the ambiguity sets and form a basis of Rd , span(∪c−1
j=0Aj ) =
n l( ˆη) = (cid:80)l
j=1 αn,j wj for some
wj ∈ Aj and some coefﬁcients αnj . If M /∈ Q, αnj (cid:54)= 0 for some j (cid:54)= n and some n. Then
˜(cid:96)j ( ˆη), which cannot be equal to (cid:96)n ( ˆη) for all ˆη . Therefore, (cid:96)(z, ˆη) is not
mT
j=1 αnj
M-proper.

4.2 Virtual labels

The analysis above shows a procedure to construct proper losses from ambiguity sets. The main
result of this section is to show that (16) is actually a universal representation, in the sense that any
proper loss can be represented in this form, and we generalize the Savage’s representation providing
and explicit formula for Q-proper losses.
Theorem 4.3 Scoring rule (cid:96)(z, ˆη) is (strictly) Q-proper for some matrix set Q with equivalent loss
˜(cid:96)(y, ˆη) if and only if

(cid:96)(z, ˆη) = h( ˆη) + g( ˆη)T (UTV−1z − ˆη).
(17)
where h is the (strictly) concave function from the Savage’s representation for ˜(cid:96), g( ˆη) is a super-
gradient of h, V is a d × d non-singular matrix and U is a binary matrix with only one unit value
at each row.
Moreover, the ambiguity set of class j is Aj = span(Vj ), where Vj is the set of all columns in V
such that uj i = 1.

Proof See the Appendix.

Comparing (4) with (17), the effect of imperfect labelling becomes clear: the unknown true label y
is replaced by a virtual label ˜y = UTV−1z, which is a linear combination of the partial labels.

6

4.3 Admissible scenarios

The previous analysis shows that, in order to calibrate posterior probabilities from partial labels
in scenarios where the mixing matrix is known, two conditions are required: (1) the rows of any
admissible mixing matrix must be contained in the admissible sets, (2) the basis of all admissible
sets must be linearly independent. It is not difﬁcult to see that the parametric matrices in scenarios B,
C and D deﬁned in Section 2.2 cannot be generated using a set of basis satisfying these constraints.
On the contrary, scenario E is admissible, as we have shown in the example in Section 4.

5 Estimation Errors
If the true mixing matrix M is not in Q, a Q-proper loss may fail to estimate η . The consequences
of this can be analyzed using the expected loss, given by
= E{(cid:96)(z, ˆη)} = ηTMT l( ˆη) = ηTMT (VT )−1U˜l( ˆη)
.
(18)
L(η , ˆη)
If M ∈ Q, then L(η , ˆη) = ηT˜l( ˆη). However, if M /∈ Q, then we can decompose M = MQ + N,
where MQ is the orthogonal projection of M in Q. Then
L(η , ˆη) = ηTNT (VT )−1U˜l( ˆη) + ηT˜l( ˆη)
Example The effect of a bad choice of the ambiguity set can be illustrated using the loss in (9) in
two cases: ˜lj ( ˆη) = (cid:107)ec
j − ˆη(cid:107)2 (the square error) and ˜lj ( ˆη) = − ln( ˆηj ) (the cross entropy). As we
have discussed before, loss (9) is proper for any scenario where label z contains the true class and
(cid:32) 0.5
(cid:33)T
possibly another class taken at random. Let us assume that the true mixing matrix is
0
0.1
0.4
0
0
0.2
0
0.3
0
0.5
0
0.2
0.2
0
0.6
0
0
(were each column of MT corresponds to a label vector (z0 , z1 , z2 ) following the ordering (1, 0, 0),
(0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1). Fig. 1 shows the expected loss in (19) for the square
error (left) and the cross entropy (right), as a function of ˆη over the probability simplex P3 , for
η = (0.45, 0.15, 0.4)T . Since M /∈ Q, the estimated posterior minimizing expected loss, ˆη∗ (which
is unique because both losses are strictly proper), does not coincide with the true posterior.

M =

(19)

(20)

Figure 1: Square loss (left) and cross entropy (right) in the probability simplex, as a function of ˆη
for η = (0.45, 0.15, 0.4)T

∗ does not depend on the choice of the cost and, thus,
It is important to note that the minimum ˆη
the estimation error is invariant to the choice of the strict proper loss (though this could be not true
when η is estimated from an empirical distribution). This is because, using (19) and noting that the
expected proper loss is

˜L(η , ˆη)

.
= Ey

˜(cid:96)(y, ˆη) = ηT˜l( ˆη)

(21)

7

we have

∗

Since (22) is minimum for ˆη

L(η , ˆη) = L(UTV−1Mη , ˆη)
= UTV−1Mη , the estimation error is
(cid:107)η − ˆη
∗ (cid:107)2 = (cid:107)(I − UTV−1M)η(cid:107)2
which is independent on the particular choice of the equivalent loss.
If ˜(cid:96) is proper but not strictly proper, the minimum may be not unique. For instance, for the 0 − 1
loss, any ˆη providing the same decisions than η is a minimum of ˜L(η , ˆη). Therefore, those values
of η with η and UTV−1Mη in the same decision region are not inﬂuenced by a bad choice of the
ambiguity set. Unfortunately, since the set of boundary decision points is not linear (but piecewise
linear) one can always ﬁnd points η that are affected by this choice. Therefore, a wrong choice
of the ambiguity set always changes the boundary decision. Summarizing, the ambiguity set for
probability estimation is not larger than that for classiﬁcation.

(22)

(23)

6 Conclusions

In this paper we have generalized proper losses to deal with scenarios with partial labels. Proper
losses based on partial labels can be designed to cope with different mixing matrices. We have also
generalized the Savage’s representation of proper losses to obtain an explicit expression for proper
losses as a function of a concave generator.

Appendix: Proof of Theorem 4.3
Let us assume that (cid:96)(z, ˆη) is (strictly) Q-proper for some matrix set Q with equivalent loss ˜(cid:96)(y, ˆη).
Let Qj be the set of the j -th rows of all matrices in Q, and take Aj = span(Qj ) ∩ Pd . Then any
vector m ∈ Aj is afﬁne combination of vectors in Qj and, thus, mT l( ˆη) = ˜l( ˆη). Therefore, if
span(Qi ) has dimension ni , we can take a basis Vi ∈ Qi of ni linearly independent vectors such
that Ai = span(Qi ) ∩ Pd .
By construction l( ˆη) = (VT )−1U˜l( ˆη). Combining this equation with the Savage’s representation
in (4), we get
(cid:96)(z, ˆη) = zT l( ˆη) = zT (VT )−1U(h( ˆη)1c + (I − ˆη1T
c )Tg( ˆη)
= h( ˆη)zT1d + zT (VT )−1 (U − 1d ˆηT )g( ˆη)
= h( ˆη) + g( ˆη)T (UTV−1z − ˆη)

(24)

which is the desired result.
Now, let us assume that (17) is true. Then
l( ˆη) = h( ˆη)1d + ((VT )−1U − 1d ˆηT )g( ˆη).
For any matrix M ∈ M such that MTec
j ∈ Aj , we have
MT l( ˆη) = h( ˆη)MT1d + (MT (VT )−1U − M1d ˆηT )g( ˆη)
(26)
If M ∈ Q, then we can express each column, j , of M as a convex combination of the columns in
V with uj i = 1, thus M = VΛ for some matrix Λ with the coefﬁcients of the convex combination
at the corresponding positions of unit values in U. Then MT (VT )−1U = ΛU = I. Using this in
(26), we get

(25)

MT l( ˆη) = h( ˆη)1c + (Ic − 1c ˆηT )g( ˆη) = ˜l( ˆη).
Applying Theorem 3.2, the proof is complete.

(27)

Acknowledgments

This work was partially funded by project TEC2011-22480 from the Spanish Ministry of Science
and Innovation, project PRI-PIBIN-2011-1266 and by the IST Programme of the European Com-
munity, under the PASCAL2 Network of Excellence, IST-2007-216886. Thanks to Ra ´ul Santos-
Rodr´ıguez and Dar´ıo Garc´ıa-Garc´ıa for their constructive comments about this manuscript

8

References
[1] T. Cour, B. Sapp, and B. Taskar, “Learning from partial labels,” Journal of Machine Learning
Research, vol. 12, pp. 1225–1261, 2011.
[2] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy, “Learning
from crowds,” Journal of Machine Learning Research, vol. 99, pp. 1297–1322, August 2010.
[3] V. S. Sheng, F. Provost, and P. G. Ipeirotis, “Get another label? improving data quality and
data mining using multiple, noisy labelers,” in Procs. of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining, ser. KDD ’08. New York, NY, USA:
ACM, 2008, pp. 614–622.
[4] E. C ˆome, L. Oukhellou, T. Denux, and P. Aknin, “Mixture model estimation with soft la-
bels,” in Soft Methods for Handling Variability and Imprecision, ser. Advances in Soft Com-
puting, D. Dubois, M. Lubiano, H. Prade, M. Gil, P. Grzegorzewski, and O. Hryniewicz, Eds.
Springer Berlin / Heidelberg, 2008, vol. 48, pp. 165–174.
[5] P. Liang, M. Jordan, and D. Klein, “Learning from measurements in exponential families,” in
Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009,
pp. 641–648.
[6] R. Jin and Z. Ghahramani, “Learning with multiple labels,” Advances in Neural Information
Processing Systems, vol. 15, pp. 897–904, 2002.
[7] C. Ambroise, T. Denoeux, G. Govaert, and P. Smets, “Learning from an imprecise teacher:
probabilistic and evidential approaches,” in Applied Stochastic Models and Data Analysis,
2001, vol. 1, pp. 100–105.
[8] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,” 2005.
[9] M. Reid and B. Williamson, “Information, divergence and risk for binary experiments,” Jour-
nal of Machine Learning Research, vol. 12, pp. 731–817, 2011.
[10] H. Masnadi-Shirazi and N. Vasconcelos, “Risk minimization, probability elicitation, and cost-
sensitive svms,” in Proceedings of the International Conference on Machine Learning, 2010,
pp. 204–213.
[11] L. Savage, “Elicitation of personal probabilities and expectations,” Journal of the American
Statistical Association, pp. 783–801, 1971.
[12] T. Gneiting and A. Raftery, “Strictly proper scoring rules, prediction, and estimation,” Journal
of the American Statistical Association, vol. 102, no. 477, pp. 359–378, 2007.

9


Separation Of Speech From Noise Challenge

NagaChaitanya Vellanki
vellanki@stanford.edu

December 14, 2012

1 Introduction
The goal of this project is to implement the methods submit-
ted for the PASCAL CHiME Speech Separation and Recog-
nition Challenge1 [1]. In particular, estimating the spectro-
graphic mask using SVM for missing feature methods [15] of
noise compensation.
2 CHiME
The main task in the CHiME challenge is to recognise the
letter and digit in each noisy utterance.The dataset consists
of utterances of simple sentences by 34 speakers (18 male
and 16 female) in a domestic environment in the presence
of noise sources of a typical family home:
two adults and
two children, TV, footsteps, electronic gadgets(laptops and
game console), toys, some trafﬁc noise from outside and
noises arriving from a kitchen via connecting hallway. The
recordings were made using a mannequin with built-in
left and right ear simulators that record signals that are an
approximation of the acoustic signals that would be received
by the ears of an average adult listener. The sentences consist
of simple six word commands of the following form:

Command format:
($command = $color $preposition $letter $number $adverb)
$command = bin (cid:12)(cid:12) lay (cid:12)(cid:12) place(cid:12)(cid:12)set;
where each word can have the following alternatives,
$colour = blue (cid:12)(cid:12) green (cid:12)(cid:12) red (cid:12)(cid:12) white;
$prep = at (cid:12)(cid:12) by (cid:12)(cid:12) in (cid:12)(cid:12) with;
$letter = A (cid:12)(cid:12) B (cid:12)(cid:12) C (cid:12)(cid:12) ... (cid:12)(cid:12) U (cid:12)(cid:12) V (cid:12)(cid:12) X (cid:12)(cid:12) Y (cid:12)(cid:12) Z;
$number = zero (cid:12)(cid:12) one (cid:12)(cid:12) two ... seven (cid:12)(cid:12) eight (cid:12)(cid:12) nine;
$adverb = again (cid:12)(cid:12) now (cid:12)(cid:12) please (cid:12)(cid:12) soon;
Example commands:
lay blue by H ﬁve again
lay blue in T four again

The training data consists of 3600 stereo 16 bit WAV
ﬁles (600 utterances at 6 different SNR (-6 dB, -3 dB, 0 dB,
3 dB, 6 dB, 9 dB)) at 16 kHz or 48 kHz. Each WAV ﬁle
contains a single noisy utterance. The noise background can
have multiple sources but not more than 4 active sources at
a time. The speech and noise backgrounds are two channel

1 http://spandh.dcs.shef.ac.uk/projects/chime/
challenge.html

signals. The SNR deﬁned as

SN RdB = 10 log10

(cid:19)

(cid:18) Es,l + Es,r
En,l + En,r

where l, r refer to the left and right channels and s,n are speech
and noise backgrounds. E is the energy which is the sum of
the squared sample amplitudes measured for the speech or
background signals between the start and end points of the
utterance.
The data set also has 17,000 ﬁles containing 500 utterances
of each of the 34 speakers to train acoustic speech models.
These utterances were provided with reverberation but free
of additive noise. Additional 6 hours of background noise
data for train background models. The test set is similar to
the training set (600 utterances at 6 different SNR (-6 dB, -3
dB, 0 dB, 3 dB, 6 dB, 9 dB)) at 16 kHz. There is no overlap
between the backgrounds of the test set and the noisy back-
ground data. Under the challenge guidlines, models should
not take advantage of the SNR labels and should not ex-
ploit the fact that same utterances are used at different
SNR.
3 Representing data using spectrograms
The methods used in this project operate on log-mel spectro-
grams2 of the utterances. These log-mel features are com-
puted from WAV ﬁles using HCopy of the HTK toolkit with
TARGETKIND is set to FBANK 0. The log-mel spectro-
grams of Speaker 34 for Command: lay blue in T four again
are shown in Figure 1, 2 and 3

Figure 1: command with a child’s voice in background at 0
dB SNR

2A spectrogram is a two-dimensional representation of a speech signal.
In spectrogram time is displayed on x-axis and the frequency on y-axis. Each
time-frequency location in the spectrogram represents the power of the sig-
nal. In log-mel spectrogram, time is displayed on x-axis and logarithm of the
output of kth mel ﬁlter on y-axis. See section 2.2 of [12] for more details on
spectrogram and variants

1

vided in training set. These oracle masks will be used to pro-
vide reliability labels for the features of the SVM classiﬁer.
4.2 Feature for the SVM
’Subband energy to subband noise ﬂoor ratio’, ’Subband
energy to fullband noise ﬂoor ratio’, ’Flatness’, ’Subband
energy to full band energy ratio’, ’Kurtosis’, ’Spectral-
subtraction-based SNR estimate’ are used as the features for
the classiﬁer. Missing eature methods do not make any as-
sumptions about the nature of the corrupting noise so the
mask estimation process should also be be free of assump-
tions about the noise. The above features make minimal as-
sumptions about the background noise and rely only on the
characteristics of the speech signal. The details of the features
will be described here brieﬂy (refer to [7][12] more details):
4.2.1 Subband energy to full band energy ratio
Subband energy to full band energy ratio is the log ratio of
the energy in subband to the overall frame energy. As back-
ground noise is added to speech, the spectral shape changes
as a function of the spectral characteristics of the noise. Sub-
band energy to full band energy ratio is a measure of the ef-
fect of background noise on a particular subband and on the
overall frame.
4.2.2 Subband energy to subband noise ﬂoor ratio
Noise ﬂoor of a the noise-corrupted speech signal is useful
for estimating the SNR. The energies of all frames of an sub-
band are put into a histogram and the lower peak is found.
The energy bin in the histogram corresponding to this peak
value is considered as noise ﬂoor. The ratio of the energy
of a subband of a frame to the noise ﬂoor in the subband
will help determine that a speciﬁc spectrographic location has
been corrupted by noise.
4.2.3 Subband energy to fullband noise ﬂoor ratio
The energies of all frames of an utterance are put in a his-
togram and the lower energy peak is found. The energy bin
in the histogram corresponding to this peak value is the noise
ﬂoor of the noisy speech signal. The ratio of the energy of a
subband of a frame to the noise ﬂoor of the noisy speech sig-
nal will help determine that a speciﬁc spectrographic location
has been corrupted by noise.
4.2.4 Spectral-subtraction-based SNR estimate
The SNR estimate used to compute the oracle masks. Includ-
ing SNR estimation was shown to provide improvement over
baseline recognition in [13].
4.2.5 Flatness
Flatness is the variance of subband energy in a neighbor-
hood of spectrographic locations around a given pixel. Noise-
corrupted spectrographic locations have a lower variance than
cleaner ones. Flatness is given by the following equation

σ2
f l at (n, ωi ) =

1
9

i+1∑
k=i−1

n+1∑
j=n−1

(s( j, ωk ) − µs (n, ωi ))2

Figure 2: command with no background noise

Figure 3: background noise at 0 dB SNR

4 Spectrographic Mask Estimation
Spectrographic mask estimation methods divide the observed
log-mel spectral features into speech, noise dominated re-
gions. The speech dominated time-frequency components
of are considered reliable estimates of clean speech. S(t,f)
is the clean speech that could have been observed if the
signal was not corrupted with noise. The noise dominated
time-frequency components N(t,f) are considered unreliable,
they only provide a upper bound on the speech values [2]
N (t, f ) ≥ S(t, f ) .We can see that clean speech informa-
tion is missing in unreliable components. The spectrographic
masks are used in Missing Feature methods of noise com-
pensation for speech recognition in order to identify unreli-
able components. Missing feature methods were shown to
be very successful at compensating noise when the spectro-
graphic mask labeling every time-frequency location as reli-
able or unreliable is known [15][16].In missing feature meth-
ods the recognition is then performed using the reliable com-
ponents or by reconstructing the unreliable components prior
to the recognition.
4.1 Oracle Mask
The ’oracle mask’ [5] can be constructed by comparing the
log-mel spectral features of the clean speech S with the added
(cid:40)
noise N. The reliability of time-frequency cell is given by [3]

S(k, j) ≥ N (k, j) − θ
de f
= rel i abl e
de f
= unrel i abl e
0
where k is the frequency band, j is the time-frame and θ = −2
dB is the ﬁxed mask threshold.
The oracle masks were computed for all utterances across

M(k, j) =

1

Figure 4: oracle mask with a threshold of -2 dB SNR, black
regions in the mask denote unreliable features

SNRS using the clean speech, background noise ﬁles pro-

2

Kx =

for a 3 × 3 neighborhood of pixels where s(n, ωi ) represents
the subband energy of frame n and subband ωi , and µs (n, ωi )
is the mean of the subband energy values in 3 × 3 neighbor-
hood around frame n and ωi
4.2.6 Kurtosis
Kurtosis is deﬁned as

E{x4 }
{E{x2 }}2
where the expectations are calculated for each subband.
4.3 SVM Mask Estimation
An SVM classiﬁer is trained for each of the F(26) mel-
frequency bands for each of the 34 speakers using LIBSVM
[8] on 5400 frames randomly extracted from the utterances
of the particular speaker in the training set across different
SNR (-6 dB, -3 dB, 0 dB, 3 dB, 6 dB, 9 dB)), with a total
of 26 × 34 models. Reliability labels used in training were
derived from the oracle mask of the utterances obtained from
the clean speech and background noise data. Each classiﬁer
used the same set of single-frame based features ’Subband
energy to subband noise ﬂoor ratio’, ’Subband energy to full-
band noise ﬂoor ratio’, ’Flatness’, ’Subband energy to full
band energy ratio’, ’Kurtosis’, ’Spectral-subtraction-based
SNR estimate’ features derived from the noisy mel-features
along with the noise mel-features. The features were normal-
ized to mean 0 and variance 1 before training the SVM. The
SVM was trained using the RBF Kernel and the hyperparam-
eters c, γ were chosen using grid search in A × A where
A = {2−7 , 2−5 , 2−3 , 2−1 , 21 , 23 , 25 , 27 } by doing a 5-fold
cross validation on additional held-out 600 frames. This setup
was used in [2] for SVM mask estimation. Each model was
tested on 5000 additional held-out frames in the training set.
The results for each of the 26 × 34 model were captured, only
results for the speaker 33, 34 on some randomly selected ut-
terances will be described in this report and the results for rest
of the speakers will be handed in along with the report. The
SVM estimated masks were obtained by testing the above
trained models on utterances at SNR (-6 dB, -3 dB, 0 dB,
3 dB, 6 dB, 9 dB).Figure 5-10, SVM Estimated Masks at
different SNR along with the oracle mask of threshold at
-2 dB SNR for Speaker 33, Command: lay blue by H 5
again

Figure 6: -3 dB SNR

Figure 7: 0 dB SNR

Figure 8: 3 dB SNR

Figure 9: 6 dB SNR

5 Evaluation and Experiments
The performance of the mask estimated by the SVM classiﬁer
can be evaluated in two ways
1. The classiﬁcation accuracy of the estimated mask com-
pared to the oracle mask.
2. The improvement in recognition accuracy achieved by

Figure 5: -6 dB SNR

3

Figure 10: 9 dB SNR

using the classiﬁer-generated masks in missing feature
methods.
In this project,the performance of the mask estimated by the
classiﬁer is evaluated by comparing it to the oracle mask as
described in [12].
5.1 Evaluation
There are two types of errors the classiﬁer can make ’miss’
and ’false alarm’. A ’miss’ can be deﬁned as incorrect label-
ing of unreliable spectrographic location as reliable and ’false
alarm’ as incorrect labeling of a reliable spectrographic loca-
tion as unreliable. Similarly, there are two types of correct
identiﬁcations the classiﬁer can make: ’hit’ and ’correct re-
jection’.A ’hit’ can be deﬁned as correct labeling of a unreli-
able spectrographic location and ’correct rejection’ as correct
labeling of a reliable spectrographic location. The classiﬁer
is considered optimal if it maximizes hits and minimizes false
alarms. As seen in Figure 11, the classiﬁer clearly needs more
information to correctly identify reliable spectrographic loca-
tions as SNR information cannot be used in the models. Fur-
ther experimentation can be done by adding additional fea-
tures like Harmonic [2], aperiodic part of the harmonic de-
composition [6] , long term energy estimate [2], gain factor
[2], VAD [14], Comb ﬁlter ratio [7][12], Autocorrelation peak
ratio [7][12] to the classiﬁer and also by including neighbor-
ing N × N features around a spectrographic location as there
is some correlation between a reliable spectrographic location
and its neighbors[12].

Figure 11: Percentage Hit, Miss, False Alarm, Correct Rejec-
tion for Speaker 33, Command: lay blue by H 5 again at SNR
(-6 dB, -3 dB, 0 dB, 3 dB, 6 dB, 9 dB)

Figure 12: Classiﬁcation Accuracy for Speaker 33, Com-
mand: lay blue by H 5 again at SNR (-6 dB, -3 dB, 0 dB,
3 dB, 6 dB, 9 dB)

5.2 Experiments

5.2.1 Varying Training set size

The classiﬁer was trained training set with varying training
set sizes from 5400 to 11400 in steps of 1000 for Speaker
34, Command: lay blue in T 4 again at 0 dB SNR and the
masks were obtained for each training set size. There was
little improvement in the accuracy but the original problem
of correctly identifying reliable and unreliable spectrographic
locations remained.

Figure 13: Mask obtained after training with 11400 frames
and tested with Speaker 34, Command: lay blue in T 4 again
at 0 dB SNR

5.2.2 Spectral-based-subtraction SNR estimate as fea-
ture

The classiﬁer was trained without, with the Spectral-
subtraction-based SNR estimate feature to see the im-
provement
The classiﬁca-
in classiﬁcation accuracy.
tion accuracy improved when SNR estimate was one of
the feature as stated in [13].
This experiment also
shows that
the classiﬁcation accuracy is not exclusively
controlled by the SNR estimate as shown in Table 1.

4

Table 1: classiﬁcation accuracy of SVM for speaker 34,
across 26 mel-frequency bands

Without SNR estimate With SNR estimate
Test
Train
Test
Train
99.6
99.79
97.84
100
1
75.46
73.74
99.77
99.42
2
99.38
99.75
70.12
70.24
3
99.04
99.77
65.8
65.96
4
99.56
99.87
62.2
63.07
5
99.34
99.79
65.92
71.64
6
69.37
68.72
99.85
99.74
7
99.4
99.74
66.44
68.33
8
99.48
99.74
62.46
61.90
9
99.26
99.77
65.52
70.31
10
77.57
66.34
99.70
99.3
11
99.08
99.77
64.78
66.40
12
99.48
99.90
69.28
71.66
13
98.88
99.64
68.16
70.88
14
69.48
67
99.75
99.64
15
98.38
99.72
67.68
69.33
16
98.9
99.44
69.56
70.22
17
99.2
99.68
66.74
66.05
18
99.54
99.83
64.46
68.14
19
65.96
62.36
99.59
99.56
20
99.14
99.81
68.86
70.53
21
98.96
99.61
75.82
76.62
22
98.4
99.81
80.64
82.42
23
83.64
83.28
99.68
99.56
24
99.44
99.81
82.14
83.16
25
26
79.59
78.6
99.92
99.28
6 Future work
1. Converting features from log-spectra to cepstral domain.
Since log-spectra and cepstra are related by a linear
transform, a solution for converting from log-spectra to
cepstral domain has been described in [10].
2. Add additional features like Harmonic [2], aperiodic
part of the harmonic decomposition [6] , long term en-
ergy estimate [2], gain factor [2], VAD [14], Comb ﬁlter
ratio [7][12], Autocorrelation peak ratio [7][12] in the
classiﬁer
3. Use the spectrographic mask obtained using the SVM
classiﬁer in missing feature compensation methods of
speech recognition and run the baseline recognizer sys-
tem to compare the results with submissions
7 Acknowledgements
I would like to thank Andrew Maas3 , Stanford University for
this project suggestion and for helping through the project,
Jort Florent Gemmeke4 , ESAT-PSI speech group, KU Leu-
ven, Belgium.for providing MDT Tools package to under-
stand the mask estimation process. Special thanks to Mike
Seltzer5 , Speech Technology group, Microsoft Research for

3 http://ai.stanford.edu/˜amaas/
4 http://www.amadana.nl/
5 http://research.microsoft.com/en- us/people/
mseltzer/

5

State de-
features

explaining about extracting training data for the SVM classi-
ﬁer using the log-mel features and reliablity labels from the
oracle mask. His thesis [12] has been very useful in under-
standing the details of the mask estimation process.
8 References
[1] The pascal chime speech separation and recognition challenge
(2011) by J Barker, H Christensen, N Ma, P Green, E Vincent.
[2] Kallasjoki, H., Keronen, S., Brown, G. J., Gemmeke, J. F.,
Remes, U., Palomaki, K. J., 2011. Mask estimation and sparse
imputation for missing data speech recognition in multisource
reverberant environments. In: Proc. 1st Int. Workshop on Machine
Listening in Multisource Environments (CHiME). pp. 5863.
[3] J. Gemmeke, B. Cranen, and L. ten Bosch, On the relation be-
tween statistical properties of spectrographic masks and recognition
accuracy, in SPPRA- 2008, 2008, pp. 200206.
[4] Jort F Gemmeke, B Cranen (2009) TR02 :
pendent
oracle masks
for
improved
dynamical
http://arxiv.org/abs/0903.3198
[5] Christophe Cerisara, Sebastien Demange, and Jean-Paul Haton,
On noise masking for automatic missing data speech recognition:
A survey and discussion,Comput. Speech Lang., vol. 21, no. 3, pp.
443457,2007.
[6] H. Van hamme, Robust speech recognition using cepstral
domain missing data techniques and noisy masks, in Proc. ICASSP,
Montreal,Quebec, Canada, May 2004, pp. 213216.
[7] M. Seltzer, B. Raj, and R. Stern, A Bayesian classiﬁer for spec-
trographic mask estimation for missing feature speech recognition,
Speech Communication, vol. 43, no. 4, pp. 379393, 2004.
[8] C. Chang and C. Lin, LIBSVM: a library for support vector ma-
chines, 2001. [Online]. Available: http://citeseerx.ist.
psu.edu/viewdoc/summary?doi=10.1.1.20.9020
[9] Van Hamme, H.; , ”Handling Time-Derivative Features in a
Missing Data Framework for Robust Automatic Speech Recogni-
tion,” Acoustics, Speech and Signal Processing, 2006.
ICASSP
2006 Proceedings. 2006 IEEE International Conference on , vol.1,
no., pp.I, 14-19 May 2006.
[10] Van hamme, H., Robust Speech Recognition Using Missing
Feature Theory in the Cepstral or LDA Domain, Proc. Eurospeech,
Geneva, Sept. 2003, pp. 3089-3092.
[11] VOICEBOX: Speech Processing Toolbox for MATLAB,
http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/
voicebox.html
[12] M. L. Seltzer, Automatic Detection of Corrupted Speech
Features for Robust Speech Recognition, Master’s Thesis, Depart-
ment of Electrical and Computer Engineering, Carnegie Mellon
University, May, 2000
[13] Vizinho, A., Green, P., Cooke, M., Josifovski, L., 1999. Miss-
ing data theory, spectral subtraction and signal-to-noise estimation
for robust ASR: an integratedstudy. Proc.Eurospeech’99.
[14] J. Ramrez, J. Gorriz, J. Segura, C. Puntonet, and A. Rubio,
Speech/non-speech discrimination based on contextual information
integrated bispectrum LRT, in IEEE Signal Processing Letters, vol.
13, no. 8, 2006, pp. 497500.
[15] M. P. Cooke, P. D. Green, L. Josifovski, and A. Vizinho,
Robust automatic speech recognition with missing and unreliable
acoustic data, Speech Commun., vol. 34, pp. 267285, 2001
[16] Raj, B., Reconstruction of Incomplete Spectrograms for
Robust Speech Recognition, Ph.D.Dissertation, Carnegie Mellon
University, May 2000.


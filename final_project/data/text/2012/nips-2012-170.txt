Bayesian nonparametric models for ranked data

Franc¸ ois Caron
INRIA
IMB - University of Bordeaux
Talence, France
Francois.Caron@inria.fr

Yee Whye Teh
Department of Statistics
University of Oxford
Oxford, United Kingdom
y.w.teh@stats.ox.ac.uk

Abstract

We develop a Bayesian nonparametric extension of the popular Plackett-Luce
choice model that can handle an inﬁnite number of choice items. Our framework
is based on the theory of random atomic measures, with the prior speciﬁed by a
gamma process. We derive a posterior characterization and a simple and effective
Gibbs sampler for posterior simulation. We develop a time-varying extension of
our model, and apply it to the New York Times lists of weekly bestselling books.

1

Introduction

P (ρ) =

Data in the form of partial rankings, i.e. in terms of an ordered list of the top-m items, arise in many
contexts. For example, in this paper we consider datasets consisting of the top 20 bestselling books
as published each week by the New York Times. The Plackett-Luce model [1, 2] is a popular model
for modeling such partial rankings of a ﬁnite collection of M items. It has found many applications,
including choice modeling [3], sport ranking [4], and voting [5]. [6, Chap. 9] provides detailed
discussions on the statistical foundations of this model.
In the Plackett-Luce model, each item k ∈ [M ] = {1, . . . , M } is assigned a positive rating parame-
ter wk , which represents the desirability or rating of a product in the case of choice modeling, or the
skill of a player in sport rankings. The Plackett-Luce model assumes the following generative story
for a top-m list ρ = (ρ1 , . . . , ρm ) of items ρi ∈ [M ]: At each stage i = 1, . . . , m, an item is chosen
m(cid:89)
to be the ith item in the list from among the items that have not yet appeared, with the probability
(cid:1) .
(cid:1) − (cid:0) (cid:80)i−1
(cid:0) (cid:80)M
that ρi is selected being proportional to its desirability wρi . The overall probability of a given partial
ranking ρ is then:
wρi
(1)
k=1 wk
j=1 wρj
i=1
with the denominator in (1) being the sum over all items not yet selected at stage i.
In many situations the collection of available items can be very large and potentially unknown.
In this case, a nonparametric approach can be sensible, where the pool of items is assumed to
be inﬁnite and the model allows for the possibility of items not observed in previous top-m lists
to appear in new ones.
In this paper we propose such a Bayesian nonparametric Plackett-Luce
model. Our approach is built upon recent work on Bayesian inference for the (ﬁnite) Plackett-Luce
model and its extensions [7, 8, 9]. Our model assumes the existence of an inﬁnite pool of items
k=1 , each with its own rating parameter, {wk }∞
{Xk }∞
m(cid:89)
k=1 . The probability of a top-m list of items,
(cid:0) (cid:80)∞
(cid:1) − (cid:0) (cid:80)i−1
(cid:1) .
say (Xρ1 , . . . , Xρm ), is then a direct extension of the ﬁnite case (1):
wρi
k=1 wk
j=1 wρj
∞(cid:88)
i=1
To formalize the framework, a natural representation to encapsulate the pool of items along with
their ratings is using an atomic measure:
k=1

P (Xρ1 , . . . , Xρm ) =

G =

wk δXk

(2)

(3)

1

Using this representation, note that the top item Xρ1 in our list is simply a draw from the probability
measure obtained by normalizing G, while subsequent items in the top-m list are draws from prob-
ability measures obtained by ﬁrst removing from G the atoms corresponding to previously picked
items and normalizing. Described this way, it is clear that the Plackett-Luce model is basically a par-
tial size-biased permutation of the atoms in G [10], and the existing machinery of random measures
and exchangeable random partitions [11] can be brought to bear on our problem.
In particular, in Section 2 we will use a gamma process as the prior over the atomic measure G.
This is a completely random measure [12] with gamma marginals, such that the corresponding
normalized probability measure is a Dirichlet process. We will show that with the introduction of
a suitable set of auxiliary variables, we can characterize the posterior law of G given observations
of top-m lists distributed according to (2). A simple Gibbs sampler can then be derived to simulate
from the posterior distribution. In Section 3 we develop a time-varying extension of our model and
derive a simple and effective Gibbs sampler for posterior simulation. In Section 4 we apply our
time-varying Bayesian nonparametric Plackett- Luce model to the aforementioned New York Times
bestsellers datasets, and conclude in Section 5.

2 A Bayesian nonparametric model for partial ranking

We start this section by brieﬂy describing a Bayesian approach to inference in ﬁnite Plackett-Luce
models [9], and taking the inﬁnite limit to arrive at the nonparametric model. This will give good
intuitions for how the model operates, before we rederive the same nonparametric model more
formally using gamma processes. Throughout this paper we will suppose that our data consists
of L partial rankings, with ρ(cid:96) = (ρ(cid:96)1 , . . . , ρ(cid:96)m ) for (cid:96) ∈ [L]. For notational simplicity we assume
that all the partial rankings are length m.

2.1 Finite Plackett-Luce model with gamma prior
Suppose we have M choice items, with item k ∈ [M ] having a positive desirability parameter wk .
A partial ranking ρ(cid:96) = (ρ(cid:96)1 , . . . , ρ(cid:96)m ) can be constructed generatively by picking the ith item ρ(cid:96)i
at the ith stage for i = 1, . . . , m, with probability proportional to wρ(cid:96)i as in (1). An alternative
Thurstonian interpretation, which will be important in the following, is as follows: For each item k
let z(cid:96)k ∼ Exp(wk ) be exponentially distributed with rate wk . Thinking of z(cid:96)k as the arrival time of
item k in a race, let ρ(cid:96)i be the index of the ith item to arrive (the ith smallest value among (z(cid:96)k )M
k=1 ).
The resulting probability of ρ(cid:96) can then be shown to still be (1). In this interpretation (z(cid:96)k ) can be
understood as latent variables, and the EM algorithm can be applied to derive an algorithm to ﬁnd
a ML parameter setting for (wk )M
k=1 given multiple partial rankings. Unfortunately the posterior
distribution of (z(cid:96)k ) given ρ(cid:96) is difﬁcult to compute directly, so we instead consider an alternative
parameterization: Let Z(cid:96)i = zρ(cid:96)i − zρ(cid:96) i−1 be the waiting time for the ith item to arrive after the
i − 1th item (with zρ(cid:96)0 deﬁned to be 0). Then it can be shown that the joint probability is:
(cid:17)(cid:17)
(cid:16)(cid:80)M
(cid:16)−Z(cid:96)i
m(cid:89)
L(cid:89)
k=1 wk − (cid:80)i−1
(cid:96)=1,i=1 |(wk )M
(cid:96)=1 , (Z(cid:96)i )L,m
P ((ρ(cid:96) )L
i=1 is simply factorized with Z(cid:96)i |ρ, w ∼ Exp((cid:80)M
j=1 wρ(cid:96)j
wρ(cid:96)i exp
k=1 ) =
(cid:80)i−1
i=1
(cid:96)=1
k=1 wk −
Note that the posterior of (Z(cid:96)i )m
j=1 wρ(cid:96)j ), and the ML parameter setting can be easily derived as well. Taking a further step,
we note that a factorized gamma prior over (wk ) is conjugate to (4), say wk ∼ Gamma( α
M , τ )
with hyperparameters α, τ > 0. Now Bayesian inference can be carried out either with a VB EM
(cid:17)
(cid:16) α
algorithm, or a Gibbs sampler. In this paper we shall consider only Gibbs sampling algorithms. In
(cid:80)m
M + nk , τ + (cid:80)L
this case the parameter updates are of the form
wk |(ρ(cid:96) ), (Z(cid:96)i ), (wk(cid:48) )k(cid:48) (cid:54)=k ∼ Gamma
i=1 δ(cid:96)ikZ(cid:96)i
(cid:96)=1
where nk is the number of occurrences of item k among the observed partial rankings, and δ(cid:96)ik = 0
if there is a j < i with ρ(cid:96)j = k and 1 otherwise. These terms arise by regrouping those in the
exponential in (4).
A nonparametric Plackett-Luce model can now be easily derived by taking the limit as the number
of choice items M → ∞. For those items k that have appeared among the observed partial rankings,

(5)

(4)

2

in the observations, (5) becomes degenerate at 0. Instead we can deﬁne w∗ = (cid:80)
the limiting conditional distribution (5) is well deﬁned since nk > 0. For items that did not appear
k:nk=0 wk to be the
(cid:16)
(cid:17)
(cid:80)m
α, τ + (cid:80)L
total desirability among all inﬁnitely many previously unobserved items, and show that
w∗ |(ρ(cid:96) ), (Z(cid:96)i ), (wk )k:nk>0 ∼ Gamma
i=1 Z(cid:96)i
(cid:96)=1
The Gibbs sampler thus alternates between updating (Z(cid:96)i ), and updating the ratings of the observed
items (wk )k:nk>0 and of the unobserved ones w∗ . This nonparametric model allows us to estimate
the probability of seeing new items appearing in future partial rankings in a consistent manner. While
intuitive, this derivation is ad hoc in the sense that it arises as the inﬁnite limit of the Gibbs sampler
for ﬁnite models, and is unsatisfying as it did not directly capture the structure of the underlying
inﬁnite dimensional object, which we will show in the next subsection to be a gamma process.

(6)

2.2 A Bayesian nonparametric Plackett-Luce model
Let X be a measurable space of choice items. A gamma process is a completely random measure
over X with gamma marginals. Speciﬁcally, it is a random atomic measure of the form (3), such
that for each measurable subset A, the (random) mass G(A) is gamma distributed. Assuming that
G has no ﬁxed atoms (that is, for each element x ∈ X we have G({x}) = 0 with probability one)
and that the atom locations {Xk } are independent of their masses {wk }, it can be shown that such
a random measure can be constructed as follows: each Xk is iid according to a base distribution
H (which we assume is non-atomic with density h(x)), while the set of masses {wk } is distributed
according to a Poisson process over R+ with intensity λ(w) = αw−1 e−wτ where α > 0 is the
concentration parameter and τ > 0 the inverse scale. We write this as G ∼ Γ(α, τ , H ). Under this
parametrization, we have that G(A) ∼ Gamma(αH (A), τ ).
Each atom Xk is a choice item, with its mass wk > 0 corresponding to the desirability parameter.
The Thurstonian view described in the ﬁnite model can be easily extended to the nonparametric one,
where a partial ranking (Xρ(cid:96)1 . . . Xρ(cid:96)m ) can be generated as the ﬁrst m items to arrive in a race. In
particular, for each atom Xk let z(cid:96)k ∼ Exp(wk ) be the time of arrival of Xk and Xρ(cid:96)i the ith item to
arrive. The ﬁrst m items to arrive (Xρ(cid:96)1 . . . Xρ(cid:96)m ) then constitutes our top-m list, with probability
as given in (2). Again reparametrizing using inter-arrival durations, let Z(cid:96)i = zρ(cid:96)i − zρ(cid:96)i−1 for
i = 1, 2, . . . (with zρ0 = 0). Then the joint probability is:
(cid:18) m(cid:89)
(cid:19)(cid:18) (cid:89)
(cid:19)
(cid:18)
(cid:19)(cid:19)
(cid:18) ∞(cid:88)
i=1 |G) = P ((zρ(cid:96)1 . . . zρ(cid:96)m ), and z(cid:96)k > zρ(cid:96)m for all k (cid:54)∈ {ρ(cid:96)1 , . . . , ρ(cid:96)m}) (7)
m(cid:89)
wk − i−1(cid:88)
P ((Xρ(cid:96)i )m
i=1 , (Z(cid:96)i )m
− Z(cid:96)i
wρ(cid:96)i e−wρ(cid:96)i zρ(cid:96)i
e−wk zρ(cid:96)m
wρ(cid:96)i exp
k (cid:54)∈{ρ(cid:96)i }m
i=1
j=1
i=1
k=1
i=1
Marginalizing out (Z(cid:96)i )m
i=1 gives the probability of (Xρ(cid:96)i )m
i=1 in (2). Further, conditional on ρ(cid:96)
(cid:18) ∞(cid:88)
(cid:19)
wk − i−1(cid:88)
it is seen that the inter-arrival durations Z(cid:96)1 . . . Z(cid:96)m are mutually independent and exponentially
distributed:
Z(cid:96)i |(Xρ(cid:96)i )m
i=1 , G ∼ Exp
j=1
k=1
The above construction is depicted on Figure 1(left). We visualize on right some top-m lists gener-
ated from the model, with τ = 1 and different values of α.

wρ(cid:96)j

=

=

wρ(cid:96)j

(8)

2.3 Posterior characterization
Consider a number L of partial rankings, with the (cid:96)th list denoted Y(cid:96) = (Y(cid:96)1 . . . Y(cid:96)m(cid:96) ) , for (cid:96) ∈ [L].
While previously our top-m list (Xρ1 . . . Xρm ) consists of an ordered list of the atoms in G. Here G
is unobserved and (Y(cid:96)1 . . . Y(cid:96)m(cid:96) ) is simply a list of observed choice items, which is why they were
not expressed as an ordered list of atoms in G. The task here is then to characterize the posterior law
of G under a gamma process prior and supposing that the observed partial rankings were drawn iid
m(cid:96)(cid:89)
from the nonparametric Plackett-Luce model given G. Re-expressing the conditional distribution
(2) of Y(cid:96) given G, we have:
G({Y(cid:96)i })
G(X\{Y(cid:96)1 . . . Y(cid:96) i−1 })
i=1

P (Y(cid:96) |G) =

(9)

3

Left: G and U = (cid:80)
Figure 1: Bayesian nonparametric Plackett-Luce model.
k uk δXk where uk = − log(zk ). The
top-3 ranking is (ρ1 , ρ2 , ρ3 ). Right: Visualization of top-5
rankings with rows corresponding to different rankings and
columns to items sorted by size biased order. A lighter shade
corresponds to a higher rank. Each ﬁgure is for a different
G, with α = .1, 1, 3.

(10)

(11)

G({Y(cid:96)i }) exp(−Z(cid:96)iG(X\{Y(cid:96)1 , . . . , Y(cid:96) i−1 }))

As before, for each (cid:96), we will also introduce a set of auxiliary variables Z(cid:96) = (Z(cid:96)1 . . . Z(cid:96)m(cid:96) ) (the
inter-arrival times) that are conditionally mutually independent given G and Y(cid:96) , with:
Z(cid:96)i |Y(cid:96) , G ∼ Exp(G(X\{Y(cid:96)1 , . . . , Y(cid:96)i−1}))
L(cid:89)
m(cid:96)(cid:89)
The joint probability of the item lists and auxiliary variables is then (c.f. (7)):
(cid:96)=1 |G) =
P ((Y(cid:96) , Z(cid:96) )L
i=1
(cid:96)=1
Note that under the generative process described in Section 2.2, there is positive probability that an
item appearing in a list Y(cid:96) appears in another list Y(cid:96)(cid:48) with (cid:96)(cid:48) (cid:54)= (cid:96). Denote the unique items among all
K , and for each k = 1, . . . , K let nk be the number of occurrences of X ∗
L lists by X ∗
1 . . . X ∗
(cid:26)0
k among
the item lists. Finally deﬁne occurrence indicators
if ∃j < i with Y(cid:96)j = X ∗
k ;
δ(cid:96)ik =
otherwise.
1
i.e. δ(cid:96)ik is the indicator of the occurence that item X ∗
k does not appear at a rank lower than i in the
k })nk × L(cid:89)
K(cid:89)
m(cid:96)(cid:89)
(cid:96)th list. Then the joint probability under the nonparametric Plackett-Luce model is:
exp(−Z(cid:96)iG(X\{Y(cid:96)1 , . . . , Y(cid:96) i−1}))
G({X ∗
(cid:96)=1 |G) =
(cid:32)
(cid:33) K(cid:89)
(cid:32)
P ((Y(cid:96) , Z(cid:96) )L
(cid:88)
(cid:88)
i=1
k=1
(cid:96)=1
−G(X)
−G({X ∗
k })
(δ(cid:96)ik − 1)Z(cid:96)i
(cid:96)i
k=1
(cid:96)i
(13)

G({X ∗
k })nk exp

= exp

(12)

(cid:33)

Z(cid:96)i

Taking expectation of (13) with respect to G using the Palm formula gives:
(cid:18)
(cid:19)
K(cid:89)
(cid:88)
Theorem 1 The marginal probability of the L partial rankings and auxiliary variables is:
(cid:96)=1 ) = e−ψ((cid:80)
h(X ∗
P ((Y(cid:96) , Z(cid:96) )L
(cid:96)i Z(cid:96)i )
k )κ
δ(cid:96)ikZ(cid:96)i
(cid:90)
(cid:96)i
k=1
e−zG(X) (cid:105)
ψ(z ) = − log E (cid:104)
(cid:16)
where ψ(z ) is the Laplace transform of λ,
λ(w)(1 − e−zw )dw = α log
=
1 +
R+
(cid:90)
and κ(n, z ) is the nth moment of the exponentially tilted L ´evy intensity λ(w)e−zw :
λ(w)wn e−zw dw =
α
(z + τ )n Γ(n)
R+

κ(n, z ) =

(cid:17)

nk ,

z
τ

(14)

(15)

(16)

Details are given in the supplementary material. Another application of the Palm formula now allows
us to derive a posterior characterisation of G:

4

Guρ1uρ2uρ3E510152025302468101214161820510152025302468101214161820510152025302468101214161820Theorem 2 Given the observations and associated auxiliary variables (Y(cid:96) , Z(cid:96) )L
(cid:96)=1 , the posterior
K(cid:88)
law of G is also a gamma process, but with atoms with both ﬁxed and random locations. Speciﬁcally,
G|(Y(cid:96) , Z(cid:96) )L
(cid:96)=1 = G∗ +
w∗
k δX ∗
k
(cid:88)
k=1
K are mutually independent. The law of G∗ is still a gamma process,
where G∗ and w∗
1 , . . . , w∗
G∗ |(X(cid:96) , Z(cid:96) )L
(cid:96)=1 ∼ Γ(α, τ ∗ , h)
τ ∗ = τ +
Z(cid:96)i
(cid:19)
(cid:18)
(cid:88)
(cid:96)i
(cid:96)i

while the masses have distributions,
k |(Y(cid:96) , Z(cid:96) )L
(cid:96)=1 ∼ Gamma
w∗

nk , τ +

δ(cid:96)ikZ(cid:96)i

(18)

(19)

(17)

2.4 Gibbs sampling

Given the results of the previous section, a simple Gibbs sampler can now be derived, where all the
conditionals are of known analytic form. In particular, we will integrate out all of G∗ except for its
total mass w∗
∗ , (w∗
∗ = G∗ (X). This leaves the latent variables to consist of the masses w∗
k ) and the
Z(cid:96)i |rest ∼ Exp (cid:0)w∗
(cid:1)
∗ + (cid:80)
auxiliary variables (Z(cid:96)i ). The update for Z(cid:96)i is given by (10), while those for the masses are given
in Theorem 2:
k |rest ∼ Gamma (cid:0)nk , τ + (cid:80)
(cid:1)
k δ(cid:96)ikw∗
Gibbs update for Z(cid:96)i :
∗ |rest ∼ Gamma (cid:0)α, τ + (cid:80)
(cid:1)
k
w∗
Gibbs update for w∗
k :
(cid:96)i δ(cid:96)ikZ(cid:96)i
w∗
Gibbs update for w∗
∗ :
(cid:96)i Z(cid:96)i
Note that the auxiliary variables are conditionally independent given the masses and vice versa. Hy-
perparameters of the gamma process can be simply derived from the joint distribution in Theorem 1.
Since the marginal probability of the partial rankings is invariant to rescaling of the masses, it is
sufﬁcient to keep τ ﬁxed at 1. As for α, if a Gamma(a, b) prior is placed on it, its conditional
(cid:1)(cid:1)
α|rest ∼ Gamma (cid:0)a + K, b + log (cid:0)1 +
(cid:80)
distribution is still gamma:
(cid:96)i Z(cid:96)i
(23)
Gibbs update for α:
τ
Note that this update was derived with w∗
∗ marginalized out, so after an update to α it is necessary
to immediately update w∗
∗ via (22) before proceeding to update other variables.

(20)
(21)
(22)

3 Dynamic Bayesian nonparametric ranking models

In this section we develop an extension of the Bayesian nonparametric Plackett-Luce model to model
time-varying rankings, where the rating parameters of items may change smoothly over time and
reﬂected in a changing series of rankings. Given a series of times indexed by t = 1, 2, . . ., we
may model the rankings at time t using a gamma process distributed random measure Gt as in
Section 2.2, with Markov dependence among the sequence of measures (Gt ) enabling dependence
among the rankings over time.

3.1 Pitt-Walker dependence model

We will construct a dependent sequence (Gt ) which marginally follow a gamma process Γ(α, τ , H )
∞(cid:88)
using the construction of [13]. Suppose Gt ∼ Γ(α, τ , H ). Since Gt is atomic, we can write it in the
form:
(24)
k=1
∞(cid:88)
Deﬁne a random measure Ct with conditional law:
Ct |Gt =
k=1
where φt > 0 is a dependence parameter. Using the same method as in Section 2.3, we can show:

ctk |Gt ∼ Poisson(φtwtk )

wtk δXtk

ctk δXtk

Gt =

(25)

5

(26)

(27)

∞(cid:88)
Proposition 3 Suppose the law of Gt is Γ(α, τ , H ). The conditional law of Gt given Ct is then:
Gt = G∗
w∗
t +
tk δXtk
k=1
k=1 are all mutually independent. The law of G∗
t and (w∗
where G∗
tk )∞
t is given by a gamma process,
while the masses are conditionally gamma,
t |Ct ∼ Γ(α, τ + φt , H )
tk |Ct ∼ Gamma(ctk , τ + φt )
G∗
w∗
The idea of [13] is to deﬁne the conditional law of Gt+1 given Gt and Ct to coincide with the
∞(cid:88)
conditional law of Gt given Ct as in Proposition 3. In other words, deﬁne
Gt+1 = G∗
(28)
t+1 +
wt+1,k δXtk
k=1
t+1 ∼ Γ(α, τ + φt , H ) and wt+1,k ∼ Gamma(ctk , τ + φt ) are mutually independent. If
where G∗
the prior law of Gt is Γ(α, τ , H ), the marginal law of Gt+1 will be Γ(α, τ , H ) as well when both
Gt and Ct are marginalized out, thus maintaining a form of stationarity. Further, although we have
described the process in order of increasing t, the joint law of Gt , Ct , Gt+1 can equivalently be
described in the reverse order with the same conditional laws as above. Note that if ctk = 0, the
conditional distribution of wt+1,k will be degenerate at 0. Hence Gt+1 has an atom at Xtk if and
only if Ct has an atom at Xtk , that is, if ctk > 0. In addition, it also has atoms (those in G∗
t+1 ) where
Ct does not (nor does Gt ). Finally, the parameter φt can be interpreted as controlling the strength
of dependence between Gt+1 and Gt . Indeed it can be shown that
E[Gt+1 |Gt ] =
φt
τ
φt + τ
φt + τ
Another measure of dependence can be gleaned by examining the “lifetime” of an atom. Suppose
X is an atom in G1 with mass w > 0. The probability that X is an atom in C2 with positive mass
is 1 − exp(−φ1w), in which case it has positive mass in G2 as well. Conversely, once it is not an
atom, it will never be an atom in the future since the base distribution H is non-atomic. The lifetime
of the atom is then the smallest t such that it is no longer an atom. We can show by induction that:
(details in supplementary material)

Gt +

H.

(29)

Proposition 4 The probability that an atom X in G1 with mass w > 0 is dead at time t is given by
P (Gt ({X }) = 0|w) = exp(−yt|1w)
where yt|1 can be obtained by the recurrence yt|t−1 = φt−1 and yt|s−1 = yt|s φs−1
φs−1+τ +yt|s

.

3.2 Posterior characterization and Gibbs sampling

Assume for simplicity that at each time step t = 1, . . . , T we observe one top-m list Yt =
(Yt1 , . . . , Ytm ) (it trivially extends to multiple partial rankings of differing sizes). We extend the
results of the previous section in characterizing the posterior and developing a Gibbs sampler for the
dynamical model.
Since each observed item at time t has to be an atom in its corresponding random measure Gt , and
atoms in Gt can propagate to neighboring random measures via the Pitt-Walker dependence model,
we conclude that the set of all observed items (through all times) has to include all ﬁxed atoms in
the posterior of Gt . Thus let X ∗ = (X ∗
k ), k = 1, . . . , K be the set of unique items observed in
Y1 , . . . , YT , let ntk ∈ {0, 1} be the number of times the item X ∗
k appears at time t, and let ρt be
). We write the masses of the ﬁxed atoms as wtk = Gt ({X ∗
k }),
deﬁned as Yt = (X ∗
, . . . , X ∗
ρ1
ρm
while the total mass of all other random atoms is denoted wt∗ = Gt (X\X ∗ ). Note that wtk has
to be positive on a random contiguous interval of time that includes all observations of X ∗
k —it’s
lifetime—but is zero outside of the interval. We also write ctk = Ct ({X ∗
k }) and ct∗ = Ct (X\X ∗ ).
(cid:19)
(cid:18)
wtk − i−1(cid:88)
K(cid:88)
As before, we introduce, for t = 1, . . . , T and i = 1, . . . , m, latent variables
Zti ∼ Exp
j=1
k=1

wt∗ +

(30)

wtρj

6

Figure 2: Sample path drawn from the Dawson-Watanabe superprocess. Each colour represents an
atom, with height being its (varying) mass. Left shows (Gt ) and right (Gt/Gt (X)), a Fleming-Viot
process.

Each iteration of the Gibbs sampler then proceeds as follows (details in supplementary material).
The latent variables (Zti ) are updated as above. Conditioned on the latent variables (Zti ), (ctk )
and (ct∗ ), we update the masses (wtk ), which are independent and gamma distributed since all
likelihoods are of gamma form. Note that the total masses (Gt (X)) are not likelihood identiﬁable, so
we introduce an extra step to improve mixing by sampling them from the prior (integrating out (ctk ),
(ct∗ )), scaling all masses along with it. Directly after this step we update (ctk ), (ct∗ ). We update
α along with the random masses (wt∗ ) and (ct∗ ) efﬁciently using a forward-backward recursion.
Finally, the dependence parameters (φt ) are updated.

3.3 Continuous time formulation using superprocesses

The dynamic model described in the previous section is formulated for discrete time data. When
the time interval between ranking observations is not constant, it is desirable to work with dynamic
models evolving over continuous-time instead, with the underlying random measures (Gt ) deﬁned
over all t ∈ R, but with observations at a discrete set of times t1 < t2 < · · · . Here we propose
a continuous-time model based on the Dawson-Watanabe superprocess [14, 15] (see also [16, 17,
(cid:18)(cid:90)
(cid:19)
(cid:90)
(cid:90)
18, 19]). This is a diffusion on the space of measures with the gamma process Γ(α, τ , H ) as its
equilibrium distribution. It is deﬁned by a generator
∂ 2
L = ξ
∂
∂
∂G(X )2 + α
∂G(X )
∂G(X )
with ξ parametrizing the rate of evolution. Figure 2 gives a sample path, where we see that it is
continuous but non-differentiable. For efﬁcient inference, it is desirable to be able to integrate out all
Gt ’s except those Gt1 , Gt2 , . . . at observation times. An advantage to using the Dawson-Watanabe
superprocess is that, the conditional distribution of Gts given Gts−1 is remarkably simple [20]. In
particular it is simply given by the discrete-time process of the previous section with dependence
. Thus the inference algorithm developed previously is directly
parameter φts |ts−1 =
τ
eτ ξ(ts−ts−1 )−1
applicable to the continuous-time model too.

H (dX )

G(dX )

− τ

G(dX )

4 Experiments

We apply the discrete-time dynamic Plackett-Luce model to the New York Times bestsellers data.
These consist of the weekly top-20 best-sellers list from June 2008 to April 2012 in various cate-
gories. We consider here the categories paperback nonﬁction (PN) and hardcover ﬁction (HF), for
which respectively 249 and 916 books appear at least once in the top-20 lists over the 200 weeks.
We consider that the correlation parameter φt = φ is constant over time, and assign ﬂat improper
priors p(α) ∝ 1/α and p(φ) ∝ 1/φ. In order to take into account the publication date of a book,
we do not consider books in the likelihood before their ﬁrst appearance in a list. We run the Gibbs
sampler with 10000 burn-in iterations followed by 10000 samples. Mean normalized weights for
the more popular books in both categories are shown in Figure 3.
The model is able to estimate the weights associated to each book that appeared at least once, as
well as the total weight associated to all other books, i.e. the probability that a new book enters at
the ﬁrst rank in the list, represented by the black curve. Moreover, the Bayesian approach enables us
to have a measure of the uncertainty on the weights. The hardcover ﬁction category is characterized
by rapid changes in successive lists, compared to the paperback nonﬁction. This is quantiﬁed by the
estimated value of the parameter φ, which are respectively 85 ± 20 and 140 ± 40 for PN and HF.
The estimated values of the shape parameter α are 7 ± 1.5 and 2 ± 1 respectively.

7

Figure 3: Mean normalized weights for paperback nonﬁction (left) and hardcover ﬁction (right).
The black lines represent the weight associated to all the books that have not appear in the top-20
lists.

5 Discussion

We have proposed a Bayesian nonparametric Plackett-Luce model for ranked data. Our approach
is based on the theory of atomic random measures, where we showed that the Plackett-Luce gener-
ative model corresponds exactly to a size-biased permutation of the atoms in the random measure.
We characterized the posterior distribution, and derived a simple MCMC sampling algorithm for
posterior simulation. Our approach can be see as a multi-stage generalization of posterior inference
in normalized random measures [21, 22, 23], and can be easily extended from gamma processes to
general completely random measures.
We also proposed dynamical extensions of our model for both discrete and continuous time data,
and applied it to modeling the bestsellers’ lists on the New York Times. Our dynamic extension
may be useful for modeling time varying densities or clusterings as well. In our experiments we
found that our model is insufﬁcient to capture the empirical observation that bestsellers often start
off high on the lists and tail off afterwards, since our model has continuous sample paths. We
adjusted for this by simply not including books in the model prior to their publication date. It may
be possible to model this better using models with discontinuous sample paths, for example, the
Orstein-Uhlenbeck approach of [24] where the process evolves via a series of discrete jump events
instead of continuously.

Acknowledgements

YWT thanks the Gatsby Charitable Foundation for generous funding.

8

Nov2008Mar2009Aug2009Dec2009May2010Oct2010Feb2011Jul2011Nov2011Apr201200.050.10.150.20.250.3EAT, PRAY, LOVETHE AUDACITY OF HOPEMARLEY AND MEDREAMS FROM MY FATHERTHREE CUPS OF TEAI HOPE THEY SERVE BEER IN HELLGLENN BECK’S ‘COMMON SENSE’THE BLIND SIDETHE LOST CITY OF ZA PATRIOT’S HISTORY OF THE UNITED STATESCONSERVATIVE VICTORYMENNONITE IN A LITTLE BLACK DRESSINSIDE OF A DOGTHE VOWHEAVEN IS FOR REALNormalized weightsDateReferences
[1] R.D. Luce. Individual choice behavior: A theoretical analysis. Wiley, 1959.
[2] R. Plackett. The analysis of permutations. Applied Statistics, 24:193–202, 1975.
Journal of Mathematical Psychology,
[3] R.D. Luce. The choice axiom after twenty years.
15:215–233, 1977.
[4] D.R. Hunter. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics,
32:384–406, 2004.
[5] I.C. Gormley and T.B. Murphy. Exploring voting blocs with the Irish electorate: a mixture
modeling approach. Journal of the American Statistical Association, 103:1014–1027, 2008.
[6] P. Diaconis. Group representations in probability and statistics, IMS Lecture Notes, volume 11.
Institute of Mathematical Statistics, 1988.
[7] I.C. Gormley and T.B. Murphy. A grade of membership model for rank data. Bayesian Analy-
sis, 4:265–296, 2009.
[8] J. Guiver and E. Snelson. Bayesian inference for Plackett-Luce ranking models. In Interna-
tional Conference on Machine Learning, 2009.
[9] F. Caron and A. Doucet. Efﬁcient Bayesian inference for generalized Bradley-Terry models.
Journal of Computational and Graphical Statistics, 21(1):174–196, 2012.
[10] G.P. Patil and C. Taillie. Diversity as a concept and its implications for random communities.
Bulletin of the International Statistical Institute, 47:497–515, 1977.
[11] J. Pitman. Combinatorial stochastic processes. Ecole d’ ´et ´e de Probabilit ´es de Saint-Flour
XXXII - 2002, volume 1875 of Lecture Notes in Mathematics. Springer, 2006.
[12] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics, 21(1):59–78,
1967.
[13] M.K. Pitt and S.G. Walker. Constructing stationary time series models using auxiliary variables
with applications. Journal of the American Statistical Association, 100(470):554–564, 2005.
[14] S. Watanabe. A limit theorem of branching processes and continuous state branching pro-
cesses. Journal of Mathematics of Kyoto University, 8:141–167, 1968.
[15] D. A. Dawson. Stochastic evolution equations and related measure processes. Journal of
Multivariate Analysis, 5:1–52, 1975.
[16] S.N. Ethier and RC Grifﬁths. The transition function of a measure-valued branching diffusion
with immigration. Stochastic Processes. A Festschrift in Honour of Gopinath Kallianpur (S.
Cambanis, J. Ghosh, RL Karandikar and PK Sen, eds.), 71:79, 1993.
[17] R.H. Mena and S.G. Walker. On a construction of Markov models in continuous time. Metron-
International Journal of Statistics, 67(3):303–323, 2009.
[18] S. Feng. Poisson-Dirichlet Distribution and Related Topics. Springer, 2010.
[19] J.C. Cox, J.E. Ingersoll Jr, and S.A. Ross. A theory of the term structure of interest rates.
Econometrica: Journal of the Econometric Society, pages 385–407, 1985.
[20] S. N. Ethier and R. C. Grifﬁths. The transition function of a measure-valued branching diffu-
sion with immigration. Stochastic Processes, 1993.
[21] L.F. James, A. Lijoi, and I. Pr ¨unster. Posterior analysis for normalized random measures with
independent increments. Scandinavian Journal of Statistics, 36(1):76–97, 2009.
[22] J.E. Grifﬁn and S.G. Walker. Posterior simulation of normalized random measure mixtures.
Journal of Computational and Graphical Statistics, 20(1):241–259, 2011.
[23] S. Favaro and Y.W. Teh. MCMC for normalized random measure mixture models. Technical
report, University of Turin, 2012.
[24] J. E. Grifﬁn. The Ornstein-Uhlenbeck Dirichlet process and other time-varying processes for
Bayesian nonparametric inference. Journal of Statistical Planning and Inference, 141:3648–
3664, 2011.

9


NonparametricMax-MarginMatrixFactorizationforCollaborativePredictionMinjieXu,JunZhuandBoZhangStateKeyLaboratoryofIntelligentTechnologyandSystems(LITS)TsinghuaNationalLaboratoryforInformationScienceandTechnology(TNList)DepartmentofComputerScienceandTechnology,TsinghuaUniversity,Beijing100084,Chinachokkyvista06@gmail.com,{dcszj,dcszb}@mail.tsinghua.edu.cnAbstractWepresentaprobabilisticformulationofmax-marginmatrixfactorizationandbuildaccordinglyanonparametricBayesianmodelwhichautomaticallyresolvestheunknownnumberoflatentfactors.Ourworkdemonstratesasuccessfulexam-plethatintegratesBayesiannonparametricsandmax-marginlearning,whichareconventionallytwoseparateparadigmsandenjoycomplementaryadvantages.Wedevelopanefﬁcientvariationalalgorithmforposteriorinference,andourexten-siveempiricalstudiesonlarge-scaleMovieLensandEachMoviedatasetsappeartojustifytheaforementioneddualadvantages.1IntroductionCollaborativepredictionisataskofpredictingusers’potentialpreferencesoncurrentlyunrateditems(e.g.,movies)basedontheircurrentlyobservedpreferencesandtheirrelationswithothers’.Onetypicalsettingformalizesitasamatrixcompletionproblem,i.e.,toﬁllinmissingentries(or,preferences)intoapartiallyobserveduser-by-itemmatrix.Oftenthereisextrainformationavailable(e.g.,users’age,gender;movies’genre,year,etc.)[10]tohelpwiththetask.Amongotherpopularapproaches,factor-basedmodelshavebeenusedextensivelyincollaborativeprediction.Theunderlyingideabehindsuchmodelsisthatthereisonlyasmallnumberoflatentfactorsinﬂuencingthepreferences.Inalinearfactormodel,auser’sratingofanitemismodeledasalinearcombinationofthesefactors,withuser-speciﬁccoefﬁcientsanditem-speciﬁcfactorvalues.Thus,givenaN×MpreferencematrixforNusersandMitems,aK-factormodelﬁtsitwithaN×KcoefﬁcientmatrixUandaM×KfactormatrixVasUV>.Variouscomputationalmethodshavebeensuccessfullydevelopedtoimplementsuchanidea,includingprobabilisticmatrixfactorization(PMF)[13,12]anddeterministicreconstruction/approximationerrorminimization,e.g.,max-marginmatrixfactorization(M3F)withhingeloss[14,11,16].Onecommonprobleminlatentfactormodelsishowtodeterminethenumberoffactors,whichisunknownapriori.Atypicalsolutionreliesonsomegeneralmodelselectionprocedure,e.g.,cross-validation,whichexplicitlyenumeratesandcomparesmanycandidatemodelsandthuscanbecomputationallyexpensive.Ontheotherhand,probabilisticmatrixfactorizationmodelshavelendthemselvesnaturallytoleveragerecentadvancesinBayesiannonparametricstobypassexplicitmodelselection[17,1].However,itremainslargelyunexploredhowtoborrowsuchadvantagesintodeterministicmax-marginmatrixfactorizationmodels,particularlytheverysuccessfulM3F.Toaddresstheaboveproblem,thispaperpresentsinﬁniteprobabilisticmax-marginmatrixfactor-ization(iPM3F),anonparametricBayesian-styleM3FmodelthatutilizesnonparametricBayesiantechniquestoautomaticallyresolvetheunknownnumberoflatentfactorsinM3Fmodels.TheﬁrstkeysteptowardsiPM3FisageneralprobabilisticformulationofthestandardM3F,whichisbasedonthemaximumentropydiscriminationprinciple[4].Wecanthenprincipallyextendittoanon-1parametricmodel,whichintheoryhasanunboundednumberoflatentfactors.Toavoidoverﬁttingweimposeasparsity-inducingIndianbuffetprocessprioronthelatentcoefﬁcientmatrix,selectingonlyanappropriatenumberofactivefactors.Wedevelopanefﬁcientvariationalmethodtoinferposteriordistributionsandlearnparameters(ifeverexist)andourextensiveempiricalresultsonMovieLensandEachMoviedemonstrateappealingperformances.Therestofthepaperisstructuredasfollows.InSection2,webrieﬂyreviewtheformalizationofmax-marginmatrixfactorization;InSection3,wepresentageneralprobabilisticformulationofM3F,andthenitsnonparametricextensionandafullyBayesianformulation;InSection4,wediscusshowtoperformlearningandinference;InSection5,wegiveempiricalresultson2prevalentcollaborativeﬁlteringdatasets;Andﬁnally,weconcludeinSection6.2Max-marginmatrixfactorizationGivenapreferencematrixY∈RN×M,whichispartiallyobservedandusuallysparse,wedenotetheobservedentryindicesbyI.Thetaskoftraditionalmatrixfactorizationistoﬁndalow-rankmatrixX∈RN×MtoapproximateYundersomelossmeasure,e.g.,thecommonlyusedsquarederror,anduseXijasthereconstructionofthemissingentriesYijwhereverij/∈I.Max-marginmatrixfactorization(M3F)[14]extendsthemodelbyusingasparsity-inducingnormregularizerforalow-normfactorizationandadoptinghingelossfortheerrormeasure,whichisapplicabletobinary,discreteordinal,orcategoricaldata.ForthebinarycasewhereYij∈{±1}andonepredictsbybYij=sign(Xij),theoptimizationproblemofM3FisdeﬁnedasminXkXk∗+CXij∈Ih(YijXij),(1)whereh(x)=max(0,1−x)isthehingelossandkXk∗isthenuclearnormofX.M3Fcanbeequivalentlyreformulatedasasemi-deﬁniteprogramming(SDP)andthuslearnedusingstandardSDPsolvers,butitisunfortunatelyveryslowandcanonlyscaleuptothousandsofusersanditems.Asshownin[14],thenuclearnormcanbewritteninavariationalform,namelykXk∗=minX=UV>12(cid:0)kUk2F+kVk2F(cid:1).(2)Basedontheequivalence,afastM3Fmodelisproposedin[11],whichusesgradientdescenttosolveanequivalentproblem,onlyonUandVinsteadminU,V12(cid:0)kUk2F+kVk2F(cid:1)+CXij∈Ih(cid:16)YijUiV>j(cid:17),(3)whereU∈RN×Kistheusercoefﬁcientmatrix,V∈RM×Ktheitemfactormatrix,andKthenumberoflatentfactors.WeuseUitodenotetheithrowofU,andVjlikewise.ThefastM3Fmodelcanscaleuptomillionsofusersanditems.Butoneunaddressedresultingproblemisthatitneedstospecifytheunknownnumberoflatentfactors,K,apriori.BelowwepresentanonparametricBayesianapproach,whicheffectivelybypassesthemodelselectionproblemandproducesveryrobustprediction.Wealsodesignablockwisecoordinatedescentalgorithmthatdirectlysolvesproblem(3)ratherthanworkingonasmoothingrelaxation[11],anditturnsouttobeasefﬁcientandaccurate.Tosavespace,wedeferthisparttoAppendixB.3NonparametricBayesianmax-marginmatrixfactorizationNowwepresentthenonparametricBayesianmax-marginmatrixfactorizationmodels.Westartwithabriefintroductiontomaximumentropydiscrimination,whichlaysthebasisforourmethods.3.1MaximumentropydiscriminationWeconsiderthebinaryclassiﬁcationsettingsinceitsufﬁcesforourmodel.Givenasetoftrainingdata{(xd,yd)}Dd=1(yd∈{±1})andadiscriminantfunctionF(x;η)parameterizedbyη,max-imumentropydiscrimination(MED)[4]seekstolearnadistributionp(η)ratherthanperformapointestimationofηasisthecasewithstandardSVMsthattypicallylackadirectprobabilisticinterpretation.Accordingly,MEDtakesexpectationovertheoriginaldiscriminantfunctionwithrespecttop(η)andhasthenewpredictionruleˆy=sign(Ep[F(x;η)]).(4)2Toﬁndp(η),MEDsolvesthefollowingrelative-entropicregularizedriskminimizationproblemminp(η)KL(p(η)kp0(η))+CXdh‘(ydEp[F(xd;η)]),(5)wherep0(η)isthepre-speciﬁedpriordistributionofη,KL(pkp0)theKullback-Leiblerdivergence,orrelativeentropy,betweentwodistributions,Ctheregularizationconstantandh‘(x)=max(0,‘−x)(‘>0)thegeneralizedhingeloss.BydeﬁningFasthelog-likelihoodratioofaBayesiangenerativemodel1,MEDprovidesanelegantwaytointegratediscriminativemax-marginlearningandBayesiangenerativemodeling.Infact,MEDsubsumesSVMasaspecialcaseandhasbeenextendedtoincorporatelatentvariables[5,18]andperformstructuredoutputprediction[21].RecentworkhasfurtherextendedMEDtouniteBayesiannonparametricsandmax-marginlearning[20,19],whichhavebeenlargelytreatedasisolatedtopics,forlearningbetterclassiﬁcationmodels.ThepresentworkcontributesbyintroducinganovelgeneralizationofMEDtohandlethechallengingmatrixfactorizationproblems.3.2Probabilisticmax-marginmatrixfactorizationLikePMF[12],wetreatUandVasrandomvariables,whosejointpriordistributionisdenotedbyp0(U,V).Then,ourgoalistoinfertheirposteriordistributionp(U,V)2afterasetofobservationshavebeenprovided.WeﬁrstconsiderthebinarycasewhereYijtakesvaluefrom{±1}.Ifthefactorization,UandV,isgiven,wecannaturallydeﬁnethediscriminantfunctionFasF((i,j);U,V)=UiV>j.(6)Furthermore,sincebothUandVarerandomvariables,weneedtoresolvetheuncertaintyinordertoderiveapredictionrule.Here,wechoosethecanonicalMEDapproach,namelytheexpectationop-erator,whichislinearandhasshownpromisein[18,19],ratherthanthelog-marginalized-likelihoodratioapproach[5],whichrequiresanextralikelihoodmodel.Hence,substitutingthediscriminantfunction(6)into(4),wehavethepredictionrulebYij=sign(cid:0)Ep[UiV>j](cid:1).(7)ThenfollowingtheprincipleofMEDlearning,wedeﬁneprobabilisticmax-marginmatrixfactor-ization(PM3F)assolvingthefollowingoptimizationproblemminp(U,V)KL(p(U,V)kp0(U,V))+CXij∈Ih‘(cid:16)YijEp[UiV>j](cid:17).(8)NotethatourprobabilisticformulationisstrictlymoregeneralthantheoriginalM3Fmodel,whichisinfactaspecialcaseofPM3FunderastandardGaussianpriorandamean-ﬁeldassumptiononp(U,V).Speciﬁcally,ifweassumep0(U,V)=QiN(Ui|0,I)QjN(Vj|0,I)andp(U,V)=p(U)p(V),thenonecanprovep(U)=QiN(Ui|Φi,I),p(V)=QjN(Vj|Ψj,I)andPM3FreducesaccordinglytoaM3Fproblem(3),namelyminΦ,Ψ12(kΦk2F+kΨk2F)+CXij∈Ih‘(cid:16)YijΦiΨ>j(cid:17).(9)Ratings:ForordinalratingsYij∈{1,2,...,L},weusethesamestrategyasin[14]todeﬁnethelossfunction.Speciﬁcally,weintroducethresholdsθ0≤θ1≤···≤θL,whereθ0=−∞andθL=+∞,todiscretizeRintoLintervals.ThepredictionruleischangedaccordinglytobYij=max(cid:8)r|Ep[UiV>j]≥θr(cid:9)+1.(10)Inahard-marginsetting,wewouldrequirethatθYij−1+‘≤Ep[UiV>j]≤θYij−‘.(11)Whileinasoft-marginsetting,wedeﬁnethelossasXij∈I(cid:16)Yij−1Xr=1h‘(Ep[UiV>j]−θr)+L−1Xr=Yijh‘(θr−Ep[UiV>j])(cid:17)=Xij∈IL−1Xr=1h‘(cid:16)Trij(θr−Ep[UiV>j])(cid:17)(12)1Fcanalsobedirectlyspeciﬁedwithoutanyreferencetoprobabilisticmodels[4],asisourcase.2Weabbreviatedtheposteriorp(U,V|Y)sincewedon’tspecifythelikelihoodp(Y|U,V)anyway.3whereTrij=(+1forr≥Yij−1forr<Yij.Thelossthusdeﬁnedisanupperboundtothesumofabsolutedifferencesbetweenthepredictedratingsandthetrueratings,alossmeasurecloselyrelatedtoNormalizedMeanAbsoluteError(NMAE)[7,14].Furthermore,wecanlearnamoreﬂexiblemodeltocaptureusers’diverseratingcriteriabyreplacinguser-commonthresholdsθrinthepredictionrule(10)andtheloss(12)withuser-speciﬁconesθir.Finally,wemayaswelltreattheadditionallyintroducedthresholdsθirasrandomvariablesandinfertheirposteriordistribution,herebygivingthefullPM3Fmodelassolvingminp(U,V,θ)KL(p(U,V,θ)kp0(U,V,θ))+CXij∈IL−1Xr=1h‘(cid:16)Trij(Ep[θir]−Ep[UiV>j])(cid:17).(13)3.3InﬁnitePM3F(iPM3F)Aswehavestated,onecommonproblemwithﬁnitefactor-basedmodels,includingPM3F,isthatweneedtoexplicitlyselectthenumberoflatentfactors,i.e.,K.Inthissection,wepresentaninﬁnitePM3Fmodelwhich,throughBayesiannonparametrictechniques,automaticallyadaptsandselectsthenumberoflatentfactorsduringlearning.Withoutlossofgenerality,weconsiderlearningabinary3coefﬁcientmatrixZ∈{0,1}N×∞.Forﬁnite-sizedbinarymatrices,wemaydeﬁnetheirpriorasgivenbyaBeta-Bernoulliprocess[8].Whileintheinﬁnitecase,weallowZtohaveaninﬁnitenumberofcolumns.Similartothenonparametricmatrixfactorizationmodel[17],weadoptIBPprioroverunboundedbinarymatricesaspreviouslyestablishedin[3]andfurthermore,wefocusonitsstick-breakingconstruction[15],whichfacilitatesthedevelopmentofefﬁcientinferencealgorithms.Speciﬁcally,letπk∈(0,1)beaparameterassociatedwitheachcolumnofZ(withrespecttoitsleft-orderedequivalentclass).ThentheIBPpriorcanbedescribedasgivenbythefollowinggenerativeprocessZik∼Bernoulli(πk)i.i.d.fori=1,...,N(∀k),(14)π1=ν1,πk=νkπk−1=kYi=1νi,whereνi∼Beta(α,1)i.i.d.fori=1,...,+∞.(15)Thisprocessresultsinadescendingsequenceofπk.Speciﬁcally,givenaﬁnitedataset(N<+∞),theprobabilityofseeingthekthfactordecreasesexponentiallywithkandthenumberofactivefactorsK+followsaPoisson(αHN),whereHNistheNthharmonicnumber.Alternatively,wecanuseaBetaprocessprioroverZasin[9].Asforthecounterpart,weplaceanisotropicGaussianpriorovertheitemfactormatrixV.Priorspeciﬁed,wemayfollowtheaboveprobabilisticframeworktoperformmax-margintraining,withUreplacedbyZ.Insummary,thestick-breakingconstructionfortheIBPpriorresultsinanaugmentediPM3Fproblemforbinarydataasminp(ν,Z,V)KL(p(ν,Z,V)kp0(ν,Z,V))+CXij∈Ih‘(cid:16)YijEp[ZiV>j](cid:17),(16)wherep0(ν,Z,V)=p0(ν)p0(Z|ν)p0(V)withνk∼Beta(α,1)i.i.d.fork=1,...,+∞,Zik|ν∼Bernoulli(πk)i.i.d.fori=1,...,N(∀k),Vjk∼N(0,σ2)i.i.d.forj=1,...,M,k=1,...,+∞.Forordinalratings,weaugmenttheiPM3Fproblemfrom(13)likewiseand,apartfromadoptingthesamepriorassumptionsforν,ZandV,assumep0(θ)=p0(θ|ν,Z,V)withθir∼N(ρr,ς2)i.i.d.fori=1,...,N,r=1,...,L−1,whereρ1<···<ρL−1arespeciﬁedasapriorguidancetowardsanascendingsequenceoflarge-marginthresholds.3Learningreal-valuedcoefﬁcientscanbeeasilydoneasin[3]bydeﬁningU=Z◦W,whereWisareal-valuedmatrixand◦denotestheHadamardproductorelement-wiseproduct.43.4ThefullyBayesianmodel(iBPM3F)TotakeiPM3FonestepfurthertowardsaBayesian-stylemodel,weintroducepriorsforhyper-parametersandperformfully-Bayesianinference[12],wheremodelparametersandhyper-parametersareintegratedoutwhenmakingprediction.ThisapproachnaturallyﬁtsinourMED-basedmodelthankstotheadoptionoftheexpectationoperatorwhendeﬁningpredictionrule(7)and(10).Anotherobservationisthatthehyper-parameterσinawayservesthesameroleastheregularizationconstantC,andthuswealsotrysimplifyingthemodelbyomittingCiniBPM3F.Weadmitthough,howevermanylevelofhyper-parametersarestackedandtreatedasstochasticandintegratedout,therealwaysexistsagapbetweenourmodelandacanonicalBayesianonesincewerejectalikelihood.WebelievetheconnectionisbetterjustiﬁedunderthegeneralregularizedBayesianinferenceframework[19]withatrivialnon-informativelikelihood.HereweusethesameGaussian-WishartprioroverthelatentfactormatrixVaswellasitshyper-parametersµandΩ,thusyieldingadoublyaugmentedproblemforbinarydataasminp(ν,Z,µ,Ω,V)KL(p(ν,Z,µ,Ω,V)kp0(ν,Z,µ,Ω,V))+Xij∈Ih‘(cid:16)YijEp[ZiV>j](cid:17),(17)wherewe’veomittedtheregularizationconstantCandsetp0(ν,Z,µ,Ω,V)tobefactorizedasp0(ν)p0(Z|ν)p0(µ,Ω)p0(V|µ,Ω),withνandZenjoyingthesamepriorsasiniPM3Fand(µ,Ω)∼GW(µ0,β0,W0,τ0)=N(µ|µ0,(β0Ω)−1)W(Ω|W0,τ0),Vj|µ,Ω∼N(Vj|µ,Ω−1)i.i.d.forj=1,...,M.Andnotethatexactlythesameprocessappliesaswelltothefullmodelforordinalratings.4Learningandinferenceundertruncatedmean-ﬁeldassumptionsNow,webrieﬂydiscusshowtoperformlearningandinferenceiniPM3F.ForiBPM3F,similarproceduresareapplicable.WedeferallthedetailstoAppendixDforsavingspace.Speciﬁ-cally,weintroduceasimplevariationalinferencemethodtoapproximatetheoptimalposterior,whichturnsouttoperformwellinpractice.Wemakethefollowingtruncatedmean-ﬁeldassumptionp(ν,Z,V)=p(ν)p(Z)p(V)=KYk=1p(νk)·NYi=1KYk=1p(Zik)·p(V),(18)whereKisthetruncationlevelandνk∼Beta(γk1,γk2)i.i.d.fork=1,...,K,(19)Zik∼Bernoulli(ψik)i.i.d.fori=1,...,N,k=1,...,K.(20)Notethatwemakenofurtherassumptiononthefunctionalformofp(V)andthatwefactorizep(Z)intoelement-wisei.i.d.p(Zik)andparameterizeitwithBernoulli(ψik)merelyoutofthepursuitofasimplerdenotationforsubsequentdeduction.Actuallyitcanbeshownthatp(Z)indeedenjoysallthesepropertiesgiventhemildesttruncatedmean-ﬁeldassumptionp(ν,Z,V)=p(ν)p(Z)p(V).Forordinalratings,wemakeanadditionalmean-ﬁeldassumptionp(ν,Z,V,θ)=p(ν,Z,V)p(θ),(21)wherep(ν,Z,V)istreatedexactlythesameasforbinarydataandp(θ)isleftinfreeforms.Onenoteworthypointisthatgivenp(Z),wemaycalculatetheexpectationoftheposterioreffectivedimensionalityofthelatentfactorspaceasEp[K+]=KXk=1 1−NYi=1(1−ψik)!.(22)Thentheproblemcanbesolvedusinganiterativeprocedurethatalternatesbetweenoptimizingeachcomponentatatime,asoutlinedbelow(WedeferthedetailstoAppendixD.):Inferp(V):ThelineardiscriminantfunctionandtheisotropicGaussianprioronVleadstoanisotropicGaussianposteriorp(V)=QMj=1N(Vj|Λj,σ2I)whiletheMmeanvectorsΛjcanbeobtainedviasolvingMindependentbinarySVMsminΛj12σ2kΛjk2+CXi|ij∈Ih‘(cid:16)YijΛjψ>i(cid:17).(23)5Inferp(ν)andp(Z):Sinceνismarginalizedbeforeexertinganyinﬂuenceinthelossterm,itsupdateisindependentofthelossandhenceweadoptthesameupdaterulesasin[2];Thesub-problemonp(Z)decomposesintoNindependentconvexoptimizationproblems,oneforeachψiasminψiKXk=1(cid:16)EZ[logp(Zik)]−Eν,Z[logp0(Zik|ν)](cid:17)+CXj|ij∈Ih‘(cid:16)YijψiΛ>j(cid:17),(24)whereEZ[logp(Zik)]=ψiklogψik+(1−ψik)log(1−ψik),Eν,Z[logp0(Zik|ν)]=ψikPkj=1Eν[logνj]+(1−ψik)Eν[log(1−Qkj=1νj)]andEν[logνj]=ψ(γk1)−ψ(γk1+γk2),Eν[log(1−Qkj=1νj)]≥Lνk,whereLνkinturnisthemultivariatelowerboundasin[2].Wemayusethesimilarsubgradienttechniqueasin[19]toapproximatelysolveforψi.Hereweintroduceanalternativesolution,whichisasefﬁcientandguaranteesconvergenceasiterationgoeson.Weupdateψiviacoordinatedescent,witheachconditionaloptimalψiksoughtbybinarysearch.(SeeAppendixD.1.3fordetails.)Inferp(θ):p(θ)remainsanisotropicGaussianasp(θ)=QNi=1QL−1r=1N(θir|ir,ς2)andthemeanirofeachcomponentissolutiontothecorrespondingsubproblemminir12ς2(ir−ρr)2+CXj|ij∈Ih‘(cid:16)Trij(ir−ψiΛ>j)(cid:17),(25)towhichthebinarysearchsolverforeachψikalsoapplies.Notethatasς→+∞,theGaussiandistributionregressestoauniformdistributionandproblem(25)reducesaccordinglytothecorre-spondingconditionalsubproblemforθintheoriginalM3F(AppendixB.3).5ExperimentsanddiscussionsWeconductexperimentsontheMovieLens1MandEachMoviedatasets,andcompareourresultswithfastM3F[11]andtwoprobabilisticmatrixfactorizationmethods,PMF[13]andBPMF[12].Datasets:TheMovieLensdatasetcontains1,000,209anonymousratings(rangingfrom1to5)of3,952moviesmadeby6,040users,amongwhich3,706moviesareactuallyratedandeveryuserhasatleast20ratings.TheEachMoviedatasetcontains2,811,983ratingsof1,628moviesmadeby72,916users,amongwhich1,623moviesareactuallyratedand36,656usershasatleast20ratings.Asin[7,11],wediscardeduserswithfewerthan20ratings,leavinguswith2,579,985ratings.Thereare6possibleratingvalues,{0,0.2,...,1}andwemappedthemto{1,2,...,6}.Protocol:Asin[7,11],wetestourmethodinapurecollaborativepredictionsetting,neglectinganyexternalinformationotherthantheuser-item-ratingtripletsinthedatasets.Weadoptaswelltheall-but-oneprotocoltopartitionthedatasetintotrainingsetandtestset,thatistorandomlywithholdoneoftheobservedratingsfromeachuserintotestsetandusetherestastrainingset.Validationset,whenneeded,isconstructedlikewisefromtheconstructedtrainingset.Alsoasdescribedin[7],weconsiderbothweakandstronggeneralization.Forweak,thetrainingratingsforallusersarealwaysavailable,soasingle-stagetrainingprocesswillsufﬁce;whileforstrong,trainingisﬁrstcarriedoutonasubsetofusers,andthenkeepingthelearnedlatentfactormatrixVﬁxed,wetrainthemodelasecondtimeontheotherusersfortheiruserproﬁles(coefﬁcientsZandthresholdsθ)andperformpredictionontheseusersonly.Wepartitiontheusersaccordinglyasin[7,11],namely5,000and1,040usersforweakandstrongrespectivelyinMovieLens,and30,000and6,565inEachMovie.Werepeattherandompartitionthrice.WecomputeNormalizedMeanAbsoluteError(NMAE)astheerrormeasureandreporttheaveragedperformance.4Implementationdetails:Weperformcross-validationtochoosethebestregularizationconstantCforiPM3Faswellastoguideearly-stoppingduringthelearningprocess.ThecandidateCvaluesarethesame11valueswhicharelog-evenlydistributedbetween0.13/4and0.12asin[11].WesetthetruncationlevelK=100(sameforM3FandPMFmodels),α=3,σ=1,ς=1.5‘;ρ1,...,ρL−1aresettobesymmetricwithrespectto0,withastep-sizeof2‘;Wesetthemarginparameter‘=9.AlthoughM3Fisinvariantto‘(AppendixB.4),weﬁndthatsetting‘=9achievedagoodbalancebetweenperformanceandtrainingtime(Figure1).ThedifferenceislargelybelievedtoattributetotheuniformconvergencestandardweusedwhensolvingSVMsubproblems.Finally,foriBPM3F,weﬁndthatalthoughremovingCcanachievecompetitiveresultswithiPM3F,keepingCwillproduceevenbetterperformance.HencewelearniBPM3FusingtheselectedCforiPM3F.6Table1:NMAEperformanceofdifferentmodelsonMovieLensandEachMovie.MovieLensEachMovieAlgorithmweakstrongweakstrongM3F[11].4156±.0037.4203±.0138.4397±.0006.4341±.0025PMF[13].4332±.0033.4413±.0074.4466±.0016.4579±.0016BPMF[12].4235±.0023.4450±.0085.4352±.0014.4445±.0005M3F∗.4176±.0016.4227±.0072.4348±.0023.4301±.0034iPM3F.4031±.0030.4135±.0109.4211±.0019.4224±.0051iBPM3F.4050±.0029.4089±.0146.4268±.0029.4403±.00405.1ExperimentalresultsTable1presentstheNMAEperformanceofdifferentmodels,wheretheperformanceofM3Fiscitedfromthecorrespondingpaper[11]andrepresentsthestate-of-the-art.WeobservethatiPM3FsigniﬁcantlyoutperformsM3F,PMFandBPMFintermsoftheNMAEerrormeasureonbothdatasetsforbothsettings.Moreover,weﬁndthatthefullyBayesianformulationofiPM3FachievescomparableperformancesinmostcasesasiPM3FandthatourcoordinatedescentalgorithmforM3F(M3F∗)performsquitesimilartotheoriginalgradientdescentalgorithmforM3F.Insummary,theeffectofendowingM3FmodelswithaprobabilisticformulationisintriguinginthatnotonlytheperformanceofthemodelislargelyimprovedbutwiththehelpofBayesiannon-parametrictechniques,theeffortofselectingthenumberoflatentfactorsissavedaswell.Table2:NMAEonthepurgedEachMovie.AlgorithmweakstrongM3F[11].4009±.0012.4028±.0064PMF[13].4153±.0016.4329±.0059BPMF[12].4021±.0011.4119±.0062M3F∗.4059±.0012.4095±.0052iPM3F.3954±.0026.3977±.0034iBPM3F.3982±.0021.4026±.0067AnotherobservationfromTable1isthatingener-alalmostallmodelsperformworseonEachMoviethanonMovieLens.AcloserinvestigationﬁndsthattheEachMoviedatasethasaspecialrating.Whenauserhasratedanitemaszerostar,hemighteitherexpressagenuinedislikeor,whentheweightoftheratingislessthan1,indicatethatheneverplanstoseethatmoviesinceitjust“soundsawful”.Ideallyweshouldtreatsuchadeclarationaslessauthorita-tivethanaregularratingofzerostarandhenceomititfromthedataset.Wehavetriedthissettingbyremovingthesespecialratings.5Table2presentstheNMAEresultsofdifferentmodels.Again,thecoordinatedescentM3FperformscomparablywithfastM3F;iPM3Fperformsbetterthanalltheothermethods;AndiBPM3FperformscomparablywithiPM3F.0.51925491004009000.420.430.440.450.46marginparameter:‘NMAE  0.51925491004009000500100015002000Average time per iteration (s)NMAEtimeFigure1:Inﬂuenceof‘onM3F.Weﬁxed‘=9acrosstheexper-iments.0102030405077.588.599.5x 105# of iterationsObjective value  partition #1partition #2partition #3Figure2:Objectivevaluesdur-ingthetrainingofiPM3FonMovieLens1M.010203040500.380.40.420.440.460.480.5# of iterationsNMAE  partition #1partition #2partition #3Figure3:NMAEduringthetrainingofiPM3FonMovieLen-s1M.5.2CloseranalysisofiPM3FTheposteriordimensionality:AsindicatedinEq.(22),wemaycalculatetheexpectationoftheeffectivedimensionalityK+ofthelatentfactorspacetoroughlyhaveasenseofhowtheiPM3Fmodelautomaticallychoosesthelatentdimensionality.Sincewetakeα=3intheIBPprior(15)andN∼104,theexpectedpriordimensionalityαHNisabout30.WeﬁndthatwhenthetruncationlevelKissetsmall,e.g.,60or80,theexpectedposteriordimensionalityveryquicklysaturates,4NotethatM3FmodelsoutputdiscretizedordinalratingswhilePMFmodelsoutputreal-valuedratings.5Afterdiscardinguserswithlessthan20normalratings,weareleftwith35,281usersand2,315,060ratings.7Table3:PerformanceofiPM3FwithandwithoutprobabilistictreatmentofθAlgorithmMovieLensEachMoviepEachMoview/prob..4031±.0030.4211±.0019.3954±.0026w/oprob..4056±.0043.4256±.0011.4026±.0023margin.0024±.0013.0045±.0016.0072±.0045oftenwithintheﬁrstfewiterations;WhileforsufﬁcientlylargeKs,e.g.,150or200,iPM3FtendstooutputasparseZofexpecteddimensionalityaround135or110respectively.(Foreachtruncationlevel,wererunourmodelandperformcross-validationtoselectthebestregularizationconstantC.)Thisinterestingobservationveriﬁesourmodel’scapabilityofautomaticmodelcomplexitycontrol.Stability:AsFigure2and3shows,iPM3Fperformsquitestablyagainst3differentrandomlypar-titionedsubsets.iBPM3Fexpressesasimilartrait,butthetestperformancedoesnotkeepdroppingwiththedecreasingoftheobjectivevalue.Thereforeweuseavalidationsettoguidetheearly-stoppingduringthelearningprocess,terminatingwhenvalidationerrorstartstorebound.Treatingthresholdsθ:Whenpredictingordinalratings,theintroducedthresholdsθareveryim-portantsincetheyunderpinthelarge-marginprincipleofmax-marginmatrixfactorizationmodels.Neverthelesswithoutaproperprobabilistictreatment,thesubproblemsonthresholds(25)arenotstrictlyconvex,veryoftengivingrisetoasectionofcandidatethresholdsthatare“equallyoptimal”forthesolution.Underourprobabilisticmodelhowever,wecaneasilygetridofthisnon-strictconvexitybyintroducingforthemaGaussianpriorasstatedaboveinsection3.3.Wecompareper-formancesofiPM3FbothwithandwithouttheprobabilistictreatmentofθandasshowninTable3,theimprovementisoutstanding.Table4:Runningtimeofdifferentmodels.AlgorithmMovieLensEachMovieItersM3F[11]∼5h∼15h100PMF[13]8.7m25m50BPMF[12]19m1h50M3F∗4h10h50U,V3.8h9.5hθ125s750siPM3F4.6h5.5h50V4.3h4.3hψ18m1hFinally,Table4presentstherunningtimeofvari-ousmodelsonbothEachMovieandMovieLensdatasets.ForM3F,theoriginalpaper[11]reportedabout5honMovieLenswithastandard3.06GhzPentium4CPUandabout15honEachMovie,whicharefair-lyacceptableforfactorizingamatrixwithmillionsofentries.OurcurrentimplementationsofM3FandiPM3Fconsumeabout4.5hand10honMovieLensandEachMovierespectivelywitha3.00GhzCorei5CPU.AcloserinvestigationdiscoversthatmostoftherunningtimeisspentonlearningU(orZ)andVinPM3Fmodels,whichbreaksdownintoasetofSVMoptimizationproblemsthatarelearnedbySVMstruct.MoreefﬁcientSVMsolverscanbeim-mediatelyappliedtofurtherimprovetheefﬁciency.Furthermore,theblockwisecoordinatedescentalgorithmcannaturallybeparallelized,sincethesub-problemsoflearningdifferentUi(orVj)arenotcoupled.Weleavethisimprovementinfuturework.6ConclusionsWe’vepresentedaninﬁniteprobabilisticmax-marginmatrixfactorizationmethod,whichutilizestheadvantagesofnonparametricBayesiantechniquestobypassthemodelselectionproblemofmax-marginmatrixfactorizationmethods.We’vealsodevelopedefﬁcientblockwisecoordinatedescentalgorithmsforvariationalinferenceandperformedextensiveevaluationontwolargebenchmarkdatasets.Empiricalresultsdemonstrateappealingperformance.AcknowledgmentsThisworkissupportedbytheNationalBasicResearchProgram(973Program)ofChina(Nos.2013CB329403,2012CB316301),NationalNaturalScienceFoundationofChina(Nos.91120011,61273023),andTsinghuaUniversityInitiativeScientiﬁcResearchProgram(No.20121088071).8References[1]N.Ding,Y.Qi,R.Xiang,I.Molloy,andN.Li.NonparametricBayesianmatrixfactorizationbypower-EP.InProceedingsofthe24thAAAIConferenceonArtiﬁcialIntelligence,2010.[2]F.Doshi-Velez,K.Miller,J.VanGael,andY.W.Teh.VariationalinferencefortheIndianbuffetprocess.JournalofMachineLearningResearch,5:137–144,2009.[3]T.GrifﬁthsandZ.Ghahramani.InﬁnitelatentfeaturemodelsandtheIndianbuffetprocess.2005.[4]T.Jaakkola,M.Meila,andT.Jebara.Maximumentropydiscrimination.InAdvancesinNeuralInforma-tionProcessingSystems,1999.[5]T.Jebara.Discriminative,generativeandimitativelearning.PhDThesis,2002.[6]T.Joachims,T.Finley,andC.N.Yu.Cutting-planetrainingofstructuralSVMs.MachineLearning,77(1):27–59,2009.[7]B.MarlinandR.S.Zemel.Themultiplemultiplicativefactormodelforcollaborativeﬁltering.InPro-ceedingsofthe21stInternationalConferenceonMachineLearning,2004.[8]E.Meeds,Z.Ghahramani,R.Neal,andS.Roweis.Modelingdyadicdatawithbinarylatentfactors.InAdvancesinNeuralInformationProcessingSystems,2007.[9]J.PaisleyandL.Carin.NonparametricfactoranalysiswithBetaprocesspriors.InProceedingsofthe26thInternationalConferenceonMachineLearning,2009.[10]I.Porteous,A.Asuncion,andM.Welling.BayesianmatrixfactorizationwithsideinformationandDirichletprocessmixtures.InProceedingsofthe24thAAAIConferenceonArtiﬁcialIntelligence,2010.[11]J.D.M.RennieandN.Srebro.Fastmaximummarginmatrixfactorizationforcollaborativeprediction.InProceedingsofthe22ndInternationalConferenceonMachineLearning,2005.[12]R.SalakhutdinovandA.Mnih.BayesianprobabilisticmatrixfactorizationusingMarkovchainMonteCarlo.InProceedingsofthe25thInternationalConferenceonMachineLearning,2008.[13]R.SalakhutdinovandA.Mnih.Probabilisticmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems,2008.[14]N.Srebro,J.D.M.Rennie,andT.Jaakkola.Maximum-marginmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems,2005.[15]Y.W.Teh,D.Gorur,andZ.Ghahramani.Stick-breakingconstructionoftheIndianbuffetprocess.InProceedingsofthe21thAAAIConferenceonArtiﬁcialIntelligence,2007.[16]M.Weimer,R.Karatzoglou,andA.Smola.Improvingmaximummarginmatrixfactorization.MachineLearning,72(3):263–276,2008.[17]F.WoodandT.L.Grifﬁths.ParticleﬁlteringfornonparametricBayesianmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems,2007.[18]J.Zhu,A.Ahmed,andE.P.Xing.MedLDA:Maximummarginsupervisedtopicmodelsforregressionandclassiﬁcation.InProceedingsofthe26thInternationalConferenceonMachineLearning,2009.[19]J.Zhu,N.Chen,andE.P.Xing.InﬁnitelatentSVMforclassiﬁcationandmulti-tasklearning.InAdvancesinNeuralInformationProcessingSystems,2011.[20]J.Zhu,N.Chen,andE.P.Xing.InﬁniteSVM:aDirichletprocessmixtureoflarge-marginkernelma-chines.InProceedingsofthe28thInternationalConferenceonMachineLearning,2011.[21]J.ZhuandE.P.Xing.MaximumentropydiscriminationMarkovnetworks.JournalofMachineLearningResearch,10:2531–2569,2009.9
Robustness and risk-sensitivity in Markov decision
processes

Takayuki Osogami
IBM Research - Tokyo
5-6-52 Toyosu, Koto-ku, Tokyo, Japan
osogami@jp.ibm.com

Abstract

We uncover relations between robust MDPs and risk-sensitive MDPs. The objec-
tive of a robust MDP is to minimize a function, such as the expectation of cumu-
lative cost, for the worst case when the parameters have uncertainties. The objec-
tive of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost
when the parameters are known. We show that a risk-sensitive MDP of minimiz-
ing the expected exponential utility is equivalent to a robust MDP of minimizing
the worst-case expectation with a penalty for the deviation of the uncertain pa-
rameters from their nominal values, which is measured with the Kullback-Leibler
divergence. We also show that a risk-sensitive MDP of minimizing an iterated
risk measure that is composed of certain coherent risk measures is equivalent to
a robust MDP of minimizing the worst-case expectation when the possible devi-
ations of uncertain parameters from their nominal values are characterized with a
concave function.

1

Introduction

Robustness against uncertainties and sensitivity to risk are major issues that have been addressed
in recent development of the Markov decision process (MDP). The robust MDP [3, 4, 10, 11, 12,
20, 21] deals with uncertainties in parameters; that is, some of the parameters of the MDP are not
known exactly. The objective of a robust MDP is to minimize a function for the worst case when
the values of its parameters vary within a predeﬁned set called an uncertainty set. The standard
objective function is the expected cumulative cost [11]. When the uncertainty set is trivial, the
robust MDP is reduced to the standard MDP [17]. The risk-sensitive MDP [5, 7, 13, 14, 19], on the
other hand, assumes that the parameters are exactly known. The objective of a risk-sensitive MDP is
to minimize the value of a risk measure, such as the expected exponential utility [5, 7, 8, 15, 18], of
the cumulative cost. When the risk measure is expectation, the risk-sensitive MDP is reduced to the
standard MDP. The robust MDP and the risk-sensitive MDP have been developed independently.
The goal of this paper is to reveal relations between these two seemingly unrelated models of MDPs.
Such unveiled relations will provide insights into the two models of MDPs. For example, it is not
always clear what it means to minimize the value of a risk measure or to minimize the worst case
expected cumulative cost under an uncertainty set. In particular, the iterated risk measure studied
in [13, 14, 19] is deﬁned recursively, which prevents an intuitive understanding of its meaning. The
unveiled relation to a robust MDP can allow us to understand what it means to minimize the value of
an iterated risk measure in terms of uncertainties. In addition, the optimal policy for a robust MDP
is often found too conservative [3, 4, 10, 21], or it becomes intractable to ﬁnd the optimal policy
particularly when the transition probabilities have uncertainties [3, 4, 10]. The unveiled relations to
a risk-sensitive MDP, for which the optimal policy can be found efﬁciently, can allow us to ﬁnd the
optimal robust policy efﬁciently, avoiding that the policy is too conservative. We will explore these
possibilities.

1

The contributions of this paper can be summarized in two points. First, we prove that a risk-sensitive
MDP with the objective of minimizing the value of an iterated risk measure is equivalent to a ro-
bust MDP with the objective of minimizing the expected cumulative cost for the worst case when
the probability mass functions for the transition and cost from a state have uncertainties. More
speciﬁcally, the iterated risk measure of the risk-sensitive MDP is deﬁned recursively with a class of
coherent risk measures [9], and it evaluates the riskiness of the sum of the value of a coherent risk
measure of immediate cost. The uncertainty set of the robust MDP is characterized by the use of a
representation of the coherent risk measure. See Section 2.
Second, we prove that a risk-sensitive MDP with the objective of minimizing an expected exponen-
tial utility is equivalent to a robust MDP whose objective is to minimize the expected cumulative
cost minus a penalty function for the worst case when the probability mass functions for the tran-
sition and cost from a state have uncertainties. More speciﬁcally, the expected exponential utility
evaluates the riskiness of the sum of the value of an entropic risk measure [6] of immediate cost.
The penalty function measures the deviation of the values of the probability mass functions from
their nominal values using the Kullback-Leibler divergence. See Section 3.

2 Robust representations of iterated coherent risk measures

Throughout this paper, we consider Markov decision processes over a ﬁnite horizon, so that there
are N decision epochs. Let Sn be the set of possible states at the n-th decision epoch for n =
0, . . . , N − 1. Let A(s) be the set of candidate actions from the state, s. We assume that a nominal
transition probability, p0 (s(cid:48) |s, a), is associated with the transition from each state s ∈ Sn to each
state s(cid:48) ∈ Sn+1 given that the action a ∈ A(s) is taken at s for n = 0, . . . , N − 1. For a robust MDP,
the corresponding true transition probability, p(s(cid:48) |s, a), has the uncertainty that will be speciﬁed in
the sequel. The random cost, C (s, a), is associated with each pair of a state, s, and an action,
a ∈ S (a). We assume that C (s, a) has a nominal probability distribution, but the true probability
distribution for a robust MDP has the uncertainty that will be speciﬁed in the sequel. We assume
that Si and Sj are disjoint for any j (cid:54)= i (e.g., the state space is augmented with time).

2.1 Special case of the iterated conditional tail expectation
We start by studying a robust MDP where the uncertainty is speciﬁed by the factor, α, such that
p0 (s(cid:48) |s, a), ∀s(cid:48) ∈ Sn+1 and (cid:88)
0 < α < 1, which determines the possible deviation from the nominal value. Speciﬁcally, for each
pair of s ∈ Sn and a ∈ A(s), the true transition probabilities are in the following uncertainty set:
0 ≤ p(s(cid:48) |s, a) ≤ 1
p(s(cid:48) |s, a) = 1.
(1)
α
s(cid:48)∈Sn+1
Throughout Section 2.1, we assume that the cost C (s, a) is deterministic and has no uncertainty.
Because the uncertainty set (1) is convex, the existing technique [11] can be used to efﬁciently ﬁnd
the optimal policy that minimizes the expected cumulative cost for the worst case where the true
probability is chosen to maximize the expected cumulative cost within the uncertainty set:
Ep [ ˜C (π)],
max
min
p∈Up
π
where ˜C (π) is the cumulative cost with a policy π , and Ep is the expectation with respect to p, which
is chosen from the uncertainty set, Up , deﬁned with (1).
Our key ﬁnding is that there is a risk-sensitive MDP that is equivalent to the robust MDP having
the objective (2). Speciﬁcally, consider the risk-sensitive MDP, where the transition probability is
given by p0 , and the cost C (s, a) is deterministic given s and a. This risk-sensitive MDP becomes
equivalent to the robust MDP having the objective (2) when the objective of the risk-sensitive MDP
is to ﬁnd the optimal π with respect to an iterated conditional tail expectation (ICTE) [13]:
α [ ˜C (π)],
ICTE(N )
min
π
(cid:105)(cid:105)
(cid:105) ≡ CTEα
(cid:104) ˜C (π)
(cid:104)
(cid:104) ˜C (π)|Si
where ICTE(N )
denotes the ICTE deﬁned for N decision epochs with parameter α. Speciﬁcally,
α
ICTE(N )
is deﬁned recursively with conditional tail expectation (CTE) as follows [13]:
α
ICTE(N −i)
ICTE(N −i+1)
, for i = 1, . . . , N ,
α
α

(2)

(3)

(4)

2

(5)
α [ ˜C (π)] ≡ ˜C (π). In (4), ICTE(N −i)
[ ˜C (π)|Si ] denotes the ICTE of ˜C (π)
where we deﬁne ICTE(0)
α
conditioned on the state at the i-th decision epoch. When Si is random, so is ICTE(N −i)
[ ˜C (π)|Si ].
α
The right-hand side of (4) evaluates the CTE of this random ICTE(N −i)
[ ˜C (π)|Si ]. CTE is also
α
known as conditional value at risk or average value at risk and is formally deﬁned as follows for a
random variable Y :
CTEα [Y ] ≡ (1 − β )E[Y |Y > Vα ] + (β − α)Vα
(6)
1 − α
,
where Vα ≡ min{y | FY (y) ≥ α}, and FY is the cumulative distribution function of Y . For a
continuous Y , or unless there is a mass probability at Vα , we have CTEα [Y ] = E[Y |Y > Vα ].
The equivalence between the robust MDP with the objective (2) and the risk-sensitive MDP with the
objective (3) can be shown by the use of the following alternative deﬁnition of CTE:
CTEα [Y ] ≡ max
q∈Q Eq [Y ],
(7)
where Q is the set of probability mass functions, q , whose support is a subset of the probability mass
function, q0 , of Y such that q(y) ≤ q0 (y)/α for every y in the support of q0 . Speciﬁcally, let C π
i be
0 + · · · + C π
(cid:104) ˜C (π) | Si−1
(cid:105)
the cost incurred at the i-th epoch with policy π so that ˜C (π) = C π
N −1 . Then, by the
recursive deﬁnition of ICTE and the translation invariance1 of CTE, it can be shown that

 | Si−1
 N −1(cid:88)
C π
ICTE(N −i+1)
i−1(cid:88)
α
i + ICTE(N −i)
j | Si
 ,
 | Si−1
 N −1(cid:88)
C π
i + CTEα
C π
C π
α
i−1(cid:88)
j=i+1
j=0
i + ICTE(N −i)
j | Si
Ep
C π
α
j=i+1
j=0
where the second equality follows from (7). What (9) suggests is that the ICTE of the cumulative
0 + · · ·+C π
cost given Si can be represented by the cost already accumulated C π
i−1 plus the maximum
possible expected value of the sum of the cost incurred at Si and the ICTE of the cumulative cost to
be incurred from the (i + 1)st epoch. Induction can now be used to establish the following theorem,
which will be proved formally for the general case in Section 2.3:
Theorem 1. When the immediate cost from a state is deterministic given that state and the action
from that state, the risk-sensitive MDP with the objective (3) is equivalent to the robust MDP with
the objective (2).

C π
i + max
p∈Up

(8)

=

=

(9)

Throughout, we say that a risk-sensitive MDP is equivalent to a robust MDP if the two MDPs have a
common state space, and, regardless of the values of the parameters of the MDPs, the optimal action
for one MDP coincides with that for the other for every state.

2.2 Relation between cost uncertainty and risk-sensitivity

In addition to the transition probabilities, we now assume that the probability distribution of cost has
uncertainty. Speciﬁcally, for each pair of s ∈ Sn and a ∈ A(s), the true probability mass function2 ,
f (·|s, a), for the random cost, C (s, a), is in the following uncertainty set that is characterized with
f0 (x|s, a), ∀x ∈ X and (cid:88)
the nominal probability mass function, f0 (·|s, a):
0 ≤ f (x|s, a) ≤ 1
α
s∈X (s,a)
where X (s, a) is the support of C (s, a). Because the uncertainty sets, (1) and (10), are both convex,
the existing technique [11] can still be used to efﬁciently ﬁnd the optimal policy with respect to
Ep,f [ ˜C (π)],
(11)
max
min
p∈Up ,f ∈Uf
π

f (x|s, a) = 1,

(10)

1CTEα [Y + b] = CTEα [Y ] + b for a random Y and a deterministic b.
2Continuous cost is discussed in the supplementary material.

3

(a) g(t) = min{t/α, 1}
Figure 1: An illustration of the probabilities that give the worst case expectation.

(b) concave g

(c) piecewise linear g

where Uf is deﬁned analogously to Up .
Again, our key ﬁnding is that there is a risk-sensitive MDP that is equivalent to the robust MDP
having the objective (11). To deﬁne the objective of the equivalent risk-sensitive MDP, let D(s, a) ≡
(cid:104) ˜D(π)
(cid:105)
CTEα [C (s, a)] and let ˜D(π) be the cumulative value of D(s, a) along the sequence of (s, a) with a
policy π . Then the objective of the equivalent risk-sensitive MDP is given by
min
π
By ﬁrst applying (7) to D(s, a) and following the arguments that have led to Theorem 1, we can
establish the following theorem, which will be proved formally for the general case in Section 2.3:
Theorem 2. The risk-sensitive MDP with the objective (12) is equivalent to the robust MDP with
the objective (11).

ICTE(N )
α

(12)

.

2.3 General case of coherent risk measures

s.t.

(13)

The robust MDPs considered in Section 2.1 and Section 2.2 are not quite ﬂexible, can lead to too
conservative policies depending on the value of α, or might be too sensitive to the particular value
of α. We now introduce a broader class of robust MDPs and equivalent risk-sensitive MDPs.
To deﬁne the broader class of robust MDPs, we study the uncertainty set of (1) and (10) in more
detail. Given a random variable that takes value vi with nominal probability pi for i = 1, . . . , m, a
step of ﬁnding the optimal robust policy calculates the maximum possible expected value:
q1 v1 + · · · + qm vm
max
q
0 ≤ qi ≤ 1
pi , ∀i = 1, . . . m
α
q1 + · · · + qm = 1.
(cid:32) i(cid:88)
(cid:33)
Without loss of generality, let v1 > v2 > . . . > vm . Then the optimal solution to (13) can be
illustrated with Figure 1(a): for i = 1, . . . , m, the optimal solution q ≡ (q1 , . . . , qm ) satisﬁes
i(cid:88)
(cid:96)=1
(cid:96)=1
where g(t) = min{t/α, 1}. Relaxing the constraints in (13), we obtain the following optimization
problem, whose optimal solution is still given by (14):
(cid:32) i(cid:88)
(cid:33)
q1 v1 + · · · + qm vm
i(cid:88)
max
q
, ∀i = 1, . . . , m
q(cid:96) ≤ g
p(cid:96)
(cid:96)=1
(cid:96)=1
0 ≤ qi , ∀i = 1, . . . , m.
The inﬂexibility of (15) stems from the inﬂexibility of g(t) = min{t/α, 1}, which has only one
adjustable parameter, α. When α is small (speciﬁcally, 0 < α ≤ 1 − pm ), some of the qi s become

q(cid:96) = g

(14)

(15)

p(cid:96)

,

s.t.

4

p1p2p31p4q1q2q31y= t/ aytp1p2p31p4q1q2q31ytq4p1p2p31p4q1q2q31g= g1+ g2ytq4g1g2zero. This means that the corresponding optimistic cases (those resulting in small vi s) are ignored.
Otherwise (speciﬁcally, 1−pm < α ≤ 1), the uncertainty set can become too small as qi ≤ pi /α, ∀i.
This inﬂexibility motivates us to generalize g to a concave function such that g(0) = 0 and g(1) = 1
(see Figure 1(b)). The optimal solution to (15) with the concave g is still given by (14). With an
appropriate g , we can consider a sufﬁciently large uncertainty set for the pessimistic cases (e.g.,
q1 (cid:29) p1 ) and at the same time consider the possibility of the optimistic cases (e.g. qm > 0).
To formally deﬁne the uncertainty set for p(s(cid:48) |s, a), s ∈ Sn and a ∈ A(s), with the concave g ,
let Qp/p0 (·) denote the quantile function of a random variable that takes value p(s(cid:48) |s, a)/p0 (s(cid:48) |s, a)
with probability p0 (s(cid:48) |s, a) for s(cid:48) ∈ Sn+1 . Analogously, let Qf /f0 (·) denote the quantile function of
a random variable that takes value f (x|s, a)/f0 (x|s, a) with probability f0 (x|s, a) for x ∈ X (s, a).
(cid:90) 1
(cid:90) 1
Then p(s(cid:48) |s, a) and f (x|s, a) are in the uncertainty set iff we have, for 0 < t < 1, that
Qf /f0 (u) du ≤ g(t).
Qp/p0 (u) du ≤ g(t)
1−t
1−t

(16)

and

(17)

CRMH [Y ] =

CTEα [Y ] dH (α),

Now (7) suggests that expectation with respect to the q illustrated in Figure 1(a) is the CTE with
parameter α with respect to the corresponding p. It can be shown that the expectation with respect
(cid:90) 1
to the q illustrated in Figure 1(b) is a coherent risk measure, CRM, of the following form [9]:
0
for a nondecreasing function H such that H (0) = 0 and H (1) = 1, where Y denotes a generic
random variable. Notice that (17) is a weighted average of CTEα [Y ] for varying αs. One can
balance the weights on worse cases (higher α) and the weights on better cases (lower α).
Let K (s, a) ≡ CRMH [C (s, a)] and let ˜K (π) be the cumulative value of K (s, a) along the sequence
(cid:104) ˜K (π)
(cid:105) ≡ CRMH
(cid:104)
(cid:104) ˜K (π)|Si
(cid:105)(cid:105)
of (s, a) with a policy, π . We deﬁne an iterated coherent risk measure (ICRM) of ˜K (π) as follows:
ICRM(N −i)
ICRM(N −i+1)
, for i = 1, . . . , N ,
H
H
H [ ˜K (π)] ≡ ˜K (π). Now we are ready to prove the general results in this section.
where ICRM(0)
(cid:105)
(cid:104) ˜K (π)
Theorem 3. Consider the risk-sensitive MDP with the following objective:
ICRM(N )
min
H
π
(cid:90) 1
This risk-sensitive MDP is equivalent to the robust MDP with the objective (11) if
t

dg(t)
dt

(18)

=

dH (s)

for

0 < t < 1.

(19)

(20)

.

1
s

To gain an intuition behind (20), consider the g illustrated in Figure 1(c), where g1 (t) =
min{x/α1 , r1}, g2 (t) = min{x/α2 , r2}, and g(t) = g1 (t) + g2 (t) for 0 ≤ t ≤ 1. The expectation
with respect to the q illustrated in Figure 1(c) can be represented by r1 CRMα1 [·] + r2 CRMα2 [·]
with respect to the corresponding p. The H is thus a piecewise constant function with a step of size
ri at αi for i = 1, 2. The slope dg(t)
is either 1/α1 + 1/α2 , 1/α2 , or 0, depending on the particular
dt
value of t in such a way that (20) holds.

Proof of Theorem 3. Notice that Bellman’s optimality equations are satisﬁed both for the robust
MDP and for the risk-sensitive MDP under consideration. For the robust MDP, Bellman’s optimal-
ity equations are established in [11]. For our risk-sensitive MDP, note that the coherent risk measure
satisﬁes strong monotonicity, translation-invariance, and positive homogeneity that are used to es-
tablish Bellman’s optimality equations in [13]. A difference between the risk-sensitive MDP in [13]
and our risk-sensitive MDP is that the former minimizes the value of an iterated risk measure for ˜C ,
while the latter minimizes the value of an iterated risk measure (speciﬁcally, ICRM(0)
H ) for ˜K . This
difference does not affect whether Bellman’s optimality equations are satisﬁed.

5

(23)

(24)

(21)

(22)

x f (x|s, a) +

v(s) = min
a∈A(s)

max
p∈Up ,f ∈Uf

The equivalence between our risk-sensitive MDP and our robust MDP can thus be established by
showing that the two sets of Bellman’s optimality equations are equivalent. For s ∈ Sn , Bellman’s
 ,
 (cid:88)
(cid:88)
optimality equation for our robust MDP is
v(s(cid:48) ) p(s(cid:48) |s, a)
s(cid:48)∈Sn+1
x∈X (s,a)
where v(s) denotes the value function representing the worst-case expected cumulative cost from s.
For s ∈ Sn , Bellman’s optimality equation for our risk-sensitive MDP is
CRMH [CRMH [C0 (s, a)] + W (s, a)] ,
w(s) = min
a∈A(s)
where w(s) denotes the value function representing the value of the iterated coherent risk measure
from s, C0 (s, a) is a random variable that takes value x with probability f0 (x|s, a) for x ∈ X (s, a),
and W (s, a) is a random variable that takes value w(s(cid:48) ) with probability p0 (s(cid:48) |s, a) for s(cid:48) ∈ Sn+1 .
Note that the inner CRMH is calculated with respect to f0 (·|s, a); the outer CRMH is with respect
to p0 (·|s, a). In the following, we will show the equivalence between (21) and (22).
(cid:88)
We ﬁrst show the following equality:
x f (x|s, a) = CRMH [C0 (s, a)].
max
f ∈Uf
x∈X (s,a)
Let {x1 , x2 , . . .} = X (s, a) such that x1 > x2 > . . .. As we have seen with Figure 1, the maximizer,
(cid:33)
(cid:32) i(cid:88)
i(cid:88)
f (cid:63) , of the left-hand side of (23) satisﬁes
f0 (x(cid:96) |s, a)
f (cid:63) (x(cid:96) |s, a) = g
for i = 1, 2, . . .. For brevity, let Ri ≡ (cid:80)i
(cid:96)=1
(cid:96)=1
(cid:90) Ri
(cid:90) Ri
(cid:90) 1
(cid:88)
(cid:88)
(cid:88)
(cid:96)=1 f0 (x(cid:96) |s, a). Then we have
x f (x|s, a) =
max
xi
dg(t) =
xi
f ∈Uf
x∈X (s,a)
Ri−1
Ri−1
t
i
i
where the ﬁrst equality follows from (24), and the second equality follows from (20). Exchanging
(cid:90) 1
(cid:90) min{u,Ri }
(cid:88)
(cid:88)
the integrals in the last expression, we obtain
x f (x|s, a) =
(cid:90) 1
(cid:88)
x∈X (s,a)
Ri−1
Ri−1
i
xi (min{u, Ri } − Ri−1 )
(cid:80)
Ri−1
(cid:90) 1
i
(cid:88)
Exchanging the integral and the summation in the last expression, we obtain
i:Ri−1≤u xi (min{u, Ri } − Ri−1 )
x f (x|s, a) =
(cid:90) 1
u
x∈X (s,a)
0
0
which establishes (23). To understand the last equality, plug in the following expressions in (6):
α = 1 − u, Vα = xi(cid:63) , and β = 1 − Ri(cid:63)−1 , where i(cid:63) ≡ min{i|Ri > u}.
= CRMH [C0 (s, a)] + max
 (cid:88)
(cid:88)
(cid:88)
Finally, we show the equivalence between (21) and (22). By (23), we have
x f (x|s, a) +
(cid:48) |s, a)
(cid:48)
(cid:48)
v(s
max
) p(s
v(s
p∈Up
p∈Up ,f ∈Uf
s(cid:48) ∈Sn+1
s(cid:48)∈Sn+1
x∈X (s,a)
(cid:88)
Analogously to (23), we can show
s(cid:48)∈Sn+1

v(s(cid:48) ) p(s(cid:48) |s, a) = CRMH [V0 (s, a)],

CTEu [C0 (s, a)] dH (u),

(cid:48) |s, a). (30)
) p(s

dH (u).

(27)

dH (u) dt,

(25)

dt

dH (u)

1
u

=

=

max
f ∈Uf

max
f ∈Uf

dH (u)

(28)

(26)

(29)

(31)

1
u

1
u

xi

max
p∈Up

6

where V0 (s, a) denotes the random variable that takes value v(s(cid:48) ) with probability p0 (s(cid:48) |s, a) for
s(cid:48) ∈ Sn+1 . By (30) and (31), we have
(CRMH [C0 (s, a)] + CRMH [V (s, a)]) ,
(32)
v(s) = min
a∈A(s)
where the ﬁrst CTEH is with respect to f0 (·|s, a); the second is with respect to p0 (·|s, a). Because
f0 (·|s, a) is independent of the state at n + 1, the translation invariance3 of CRMH implies
CRMH [CRMH [C0 (s, a)] + V (s, a)],
(33)
v(s) = min
a∈A(s)
where the inner CRMH is with respect to f0 (·|s, a); the outer is with respect to p0 (·|s, a). Because
v(s(cid:48) ) = w(s(cid:48) ) = 0, ∀s(cid:48) ∈ SN , we can show, by induction, that (33) is equivalent to (22).

3 Robust representations of expected exponential utilities

In this section, we study risk-sensitive MDPs whose objectives are deﬁned with expected exponential
utilities. We will see that there are robust MDPs that are equivalent to these risk-sensitive MDPs.
We start by the standard risk-sensitive MDP [5, 7, 8, 15, 18] whose objective is to minimize
E[exp(γ ˜C (π))] for γ > 0. Because γ > 0, minimizing E[exp(γ ˜C (π))] is equivalent to mini-
mizing an entropic risk measure (ERM) [6, 13]: ERMγ [ ˜C (π)] ≡ 1
γ ln E[exp(γ ˜C (π))].
The key property of ERM that we exploit in this section is
{Eq0 [Y ] − γ KL(q ||q0 )} ,
ERMγ [Y ] = max
(34)
q∈P (q0 )
where Y is a generic discrete random variable, q0 is the probability mass function for Y , P (q0 ) is
the set of probability mass functions whose support is contained in the support of q0 (i.e., q(y) = 0
if q0 (y) = 0 for q ∈ P (q0 )), Eq is the expectation with respect to q ∈ P (q0 ), and KL(q ||q0 ) is the
Kullback-Leibler divergence [2] from q to q0 . The property (34) has been discussed in the context
of optimal control [1, 16]. See [6] for a proof of (34). Observe that the maximizer of the right-hand
side of (34) trades a large value of Eq [Y ] for the closeness of q to q0 .
It is now evident that the risk-sensitive MDP with the objective of minimizing E[exp(γ ˜C (π))] is
equivalent to a “robust” MDP with the objective of minimizing Eq [ ˜C (π)] − γ KL(q ||q0 ) for the
worst choice of q ∈ P (q0 ), where q0 denotes the probability mass function for ˜C (π). Here, the
uncertainty is in the distribution of the cumulative cost, and it is nontrivial how this uncertainty is
related to the uncertainty in the parameters, p and f , of the MDP.
Our goal is to explicitly relate the risk-sensitive MDP of minimizing E[exp(γ ˜C (π))] to uncertainties
in the parameters of the MDP. For a moment, we assume that C (s, a) has no uncertainty and is
deterministic given s and a, which will be relaxed later.
0 (si+1 |si ) be the nominal transition
To see the relation, we study ERMγ [ ˜C (π)] for a given π . Let pπ
probability from si ∈ Si to si+1 ∈ Si+1 for i = 0, . . . , N − 1. By the translation invariance and the
(cid:34)
(cid:34)N −1(cid:88)
(cid:35)(cid:35)
recursiveness4 of ERM [13], we have
i |S1
ERMγ [ ˜C (π)] = C π
0 + ERMγ
1 + ERMγ
(35)
C π
C π
,
i=2
0 (·|s0 ). By (34), the
0 (·|S1 ); the outer is with respect to pπ
(cid:34)N −1(cid:88)
(cid:40)
(cid:34)
(cid:35)(cid:35)
(cid:41)
where the inner ERM is with respect to pπ
second term of the right-hand side of (35) can be represented as follows:
− γ KL (pπ (·|s0 )||pπ
0 (·|s0 ))
| S1
1 + ERMγ
C π
i=2

max
pπ (·|s0 )∈P (pπ
0 (·|s0 ))

(36)

Epπ (·|s0 )

C π
i

Thus, by induction, we can establish the following theorem5 :
3CRMH [Y + c] = CRMH [Y ] + c for a deterministic constant c.
4ERMγ [Y + c] = ERMγ [Y ] + c and ERMγ [Y ] = ERMγ [ERMγ [Y |Z ]], where Y and Z are generic
random variables, and c is a deterministic constant.
5The proof is omitted, because this is a spacial case of Theorem 5.

7

Epπ

C π
i

− γ Epπ

,

(37)

KL (pπ (·|Si )||pπ
0 (·|Si ))

Theorem 4. When the immediate cost from a state is deterministic given that state and the action
from that state, the risk-sensitive MDP with the objective of minimizing E[exp(γ ˜C (π))] is equivalent
(cid:40)
(cid:34)N −1(cid:88)
(cid:35)
(cid:34)N −2(cid:88)
(cid:35)(cid:41)
to the robust MDP with the following objective:
min
max
pπ ∈P (pπ
π
0 )
i=0
i=0
0 ) denotes that pπ (·|si ) ∈ Ppπ
where pπ ∈ P (pπ
0 (·|si ) , ∀si ∈ Si , i = 0, . . . , N − 1.
Our results in Section 2.2 motivate us to extend Theorem 4 to the case where C (s, a) has uncer-
0 (·|s) be the nominal probability mass function for the immediate cost from a state s
tainties. Let f π
(cid:104) ˜L(π)
(cid:105)
under a policy π . Consider the risk-sensitive MDP with the following objective:
ERMγ
(38)
min
,
π
where ˜L(π) is the cumulative value of L(s, a) ≡ ERMγ [C (s, a)] along the sequence of (s, a) with
a policy π . Then we have the following theorem, which is proved in the supplementary material.
Theorem 5. The risk-sensitive MDP with the objective (38) is equivalent to the robust MDP with
 Epπ ,f π
(cid:105)  . (39)
(cid:104)(cid:80)N −1
(cid:105)
the following objective, where f π ∈ P (f π
0 ) is deﬁned analogously to pπ ∈ P (pπ
0 ) in Theorem 4:
(cid:104)(cid:80)N −2
0 (·|Si )) + (cid:80)N −1
i=0 C π
i
min
max
−γ Epπ
i=0 KL (pπ (·|Si )||pπ
i=0 KL (f π (·|Si )||f π
0 (·|Si ))
pπ∈P (pπ
π
0 )
f π∈P (f π
0 )

4 Conclusion

We have shown relations between risk-sensitive MDPs and robust MDPs. Because ERM is also an
iterated risk measure [13], the objectives of the risk-sensitive MDPs studied in this paper are all
with respect to some iterated risk measures. The signiﬁcance of iterated risk measures is intensively
discussed in [13], but it can represent one’s preferences that cannot be represented by standard
expected exponential utility and yet allows efﬁcient optimization and consistent decision making.
While the prior work [13, 14, 19] minimizes the iterated risk measure of the cumulative cost ( ˜C (π)
in Section 2), our study on the relation to a robust MDP suggests that one might want to minimize
the iterated risk measure of the sum of the values of risk measures for immediate costs (e.g., ˜K (π) in
Section 2.3 or ˜L(π) in Section 3), because the latter is related to the robustness against uncertainty
in cost. The optimal policy with respect to an iterated risk measure can be found efﬁciently with
dynamic programming (speciﬁcally, the computational effort that is required in addition to that of the
dynamic programming for minimizing the expected cumulative cost is in the time to calculate a risk
measure such as CTE instead of expectation at each step of the dynamic programming) [13]. This
means that the optimal policy for the robust MDP studied in this paper can be found quite efﬁciently.
In particular, the robust MDP in Theorem 5 might not seem to allow an efﬁcient optimization without
the knowledge of the relation to the corresponding risk-sensitive MDP, whose optimal policy is
readily available with dynamic programming. Overall, the relation to a robust MDP can provide
strong motivation for the corresponding risk-sensitive MDP and vice versa.
For simplicity, the uncertainty sets in Section 2 are characterized by a single parameter, α, or a single
function, g , but it is trivial to extend our results to the cases where the uncertainty sets are deﬁned
differently depending on the particular states, actions, and other elements of the MDP. In such cases,
the objective of the corresponding risk-sensitive MDP is composed of various risk measures. The
uncertainty set in Section 3 depends only on the support of the nominal probability mass function.
The penalty for the deviation from the nominal value can be adjusted with a single parameter, γ , but
it is also trivial to extend our results to the cases, where this parameter varies depending on the partic-
ular elements of the MDP. In such cases, the objective of the corresponding risk-sensitive MDP is an
iterated risk measure composed of ERM having varying parameters. It would also be an interesting
direction to extend our results to convex risk measures, which allows robust representations.

8

References
[1] C. D. Charalambous, F. Rezaei, and A. Kyprianou. Relations between information theory,
robustness, and statistical mechanics of stochastic systems. In Proceedings of the 43rd IEEE
Conference on Decision and Control, volume 4, pages 3479–3484, 2004.
[2] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc.,
Hoboken, New Jersey, 2nd edition, 2006.
[3] E. Delage and S. Mannor. Percentile optimization in uncertain MDP with application to ef-
ﬁcient exploration. In Proceedings of the 24th Annual International Conference on Machine
Learning (ICML 2007), pages 225–232, June 2007.
[4] E. Delage and S. Mannor. Percentile optimization for MDP with parameter uncertainty. Oper-
ations Research, 58(1):203–213, 2010.
[5] E. V. Denardo and U. G. Rothblum. Optimal stopping, exponential utility, and linear program-
ming. Mathematical Programming, 16:228–244, 1979.
[6] H. F ¨ollmer and A. Schied. Stochastic Finance: An Introduction in Discrete Time. Walter de
Gruyter, Berlin, Germany, 3rd edition, 2010.
[7] R. Howard and J. Matheson. Risk-sensitive Markov decision processes. Management Science,
18(7):356–369, 1972.
[8] S. C. Jaquette. A utility criterion for Markov decision processes. Management Science,
23(1):43–49, 1976.
[9] S. Kusuoka. On law invariant coherent risk measures. In S. Kusuoka and T. Maruyama, editors,
Advances in Mathematical Economics, volume 3, pages 83–95. Springer, Tokyo, 2001.
[10] S. Mannor, O. Mebel, and H. Xu. Lightning does not strike twice: Robust MDPs with coupled
In Proceedings of the International Conference on Machine Learning (ICML
uncertainty.
2012), pages 385–392, 2012.
[11] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798, 2005.
[12] A. Nilim and L. E. Ghaoui. Robustness in Markov decision problems with uncertain transition
matrices. In S. Thrun, L. Saul, and B. Sch ¨olkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA, 2004.
[13] T. Osogami.
Iterated risk measures for risk-sensitive Markov decision processes with dis-
counted cost. In Proceedings of the 27th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI 2011), pages 567–574, July 2011.
[14] T. Osogami and T. Morimura. Time-consistency of optimization problems. In Proceedings of
the 26th Conference on Artiﬁcial Intelligence (AAAI-12), July 2012.
[15] S. D. Patek. On terminating Markov decision processes with a risk-averse objective function.
Automatica, 37(9):1379–1386, 2001.
[16] I. R. Petersen, M. R. James, and P. Dupuis. Minimax optimal control of stochastic uncer-
IEEE Transactions on Automatic Control,
tain systems with relative entropy constraints.
45(3):398–412, 2000.
[17] M. L. Puterman. Markov Decision Processes: Discrete Dynamic Programming. Wiley-
Interscience, Hoboken, NJ, second edition, 2005.
[18] U. G. Rothblum. Multiplicative Markov decision chains. Mathematics of Operations Research,
9(1):6–24, 1984.
[19] A. Ruszczy ´nski. Risk-averse dynamic programming for Markov decision processes. Mathe-
matical Programming, 125:235–261, 2010.
[20] H. Xu and S. Mannor. The robustness-performance tradeoff in Markov decision processes. In
B. Sch ¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing
Systems 19, pages 1537–1544. MIT Press, Cambridge, MA, 2007.
[21] H. Xu and S. Mannor. Distributionally robust Markov decision processes. In J. Lafferty, C. K. I.
Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information
Processing Systems 23, pages 2505–2513. MIT Press, Cambridge, MA, 2010.

9


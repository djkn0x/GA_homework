Risk Aversion in Markov Decision Processes
via Near-Optimal Chernoff Bounds

Teodor Mihai Moldovan
Department of Computer Science
University of California at Berkeley
Berkeley CA 94720, USA
moldovan@cs.berkeley.edu

Pieter Abbeel
Department of Computer Science
University of California at Berkeley
Berkeley CA 94720, USA
pabbeel@cs.berkeley.edu

Abstract

The expected return is a widely used objective in decision making under uncer-
tainty. Many algorithms, such as value iteration, have been proposed to optimize
it. In risk-aware settings, however, the expected return is often not an appropriate
objective to optimize. We propose a new optimization objective for risk-aware
planning and show that it has desirable theoretical properties. We also draw con-
nections to previously proposed objectives for risk-aware planing: minmax, ex-
ponential utility, percentile and mean minus variance. Our method applies to an
extended class of Markov decision processes: we allow costs to be stochastic as
long as they are bounded. Additionally, we present an efﬁcient algorithm for op-
timizing the proposed objective. Synthetic and real-world experiments illustrate
the effectiveness of our method, at scale.

1

Introduction

The expected return is often the objective function of choice in planning problems where outcomes
not only depend on the actor’s decisions but also on random events. Often expectations are the
natural choice, as the law of large numbers guarantees that the average return over many independent
runs will converge to the expectation. Moreover, the linearity of expectations can often be leveraged
to obtain efﬁcient algorithms.
Some games, however, can only be played once, either because they take a very long time (investing
for retirement), because we are not given a chance to try again if we lose (skydiving, crossing
the road), or because i.i.d. versions of the game are not available (stock market). In this setting,
we can no longer take advantage of the law of large numbers to ensure that the return is close
to its expectation with high probability, so the expected return might not be the best objective to
optimize. If we were pessimistic, we might assume that everything that can go wrong will go wrong
and try to minimize the losses under this assumption. This is called minmax optimization and is
sometimes useful, but, most often, the resulting policies are overly cautious. A more balanced and
general approach would include minmax optimization and expectation optimization, corresponding
respectively to absolute risk aversion and risk ignorance, but would also allow a spectrum of policies
between these extremes.
As a motivating example, consider buying tickets to ﬂy to a very important meeting. Shorter travel
time is preferable, but even more importantly, it would be disastrous if you arrived late. Some ﬂights
arrive on time more often than others, and the delays might be ampliﬁed if you miss connecting
ﬂights. With these risks in mind, would you rather take a route with an expected travel time of 12:21
and no further guarantees, or would you prefer a route that takes less than 16:19 with 99% probabil-
ity? Our method produces these options when traveling from Shreveport Regional Airport (SHV) to
Rafael Hern ´andez Airport (BQN). According to historical ﬂight data, if you chose the former alter-

1

native you could end up travelling for 22 hours with 8% probability. Another example comes from
software quality assurance. Amazon.com requires its sub-services to report and optimize perfor-
mance at the 99.9th percentile, rather than in expectation, to make sure that all of its customers have
a good experience, not just the majority [1]. In the economics literature, this percentile criterion
is known as value at risk and has become a widely used measure of risk after the market crash of
1987 [2]. At the same time, the classical method for managing risk in investment is Markovitz port-
folio optimization where the objective is to optimize expectation minus weighted variance. These
examples suggest that proper risk-aware planning should allow a trade-off between expectation and
variance, and, at the same time, should provide some guarantees about the probability of failure.
Risk-aware planning for Markov decision processes (MDPs) is difﬁcult for two main reasons. First,
optimizing many of the intuitive risk-aware objectives seems to be intractable computationally. Both
mean minus variance optimization and percentile optimization for MDPs have been shown to be
NP-hard in general [3, 4]. Consequently, we can only optimize relaxations of these objectives in
practice. Second, it seems to be difﬁcult to ﬁnd an optimization objective which correctly models
our intuition of risk awareness. Even though expectation, variance and percentile levels relate to
risk awareness, optimizing them directly can lead to counterintuitive policies as illustrated recently
in [3], for the case of mean minus variance optimization, and in the appendix of this paper, for
percentile optimization.
Planning under uncertainty in MDPs is an old topic that has been addressed by many authors. The
minmax objective has been proposed in [5, 6], which propose a dynamic programming algorithm for
optimizing it efﬁciently. Unfortunately, minmax policies tend to be overly cautious. A number of
methods have been proposed for relaxations of mean minus variance optimization [3, 7]. Percentile
optimization has been shown to be tractable when dealing with ambiguity in MDP parameters [8, 9],
and it has also been discussed in the context of risk [10, 11]. Our approach is closest to the line of
work on exponential utility optimization [12, 13]. This problem can be solved efﬁciently and the
resulting policies conform to our intuition of risk awareness. However, previous methods give no
guarantees about probability of failure or variance. For an overview of previously used objectives
for risk-aware planning in MDPs, see [14, 15].
Our method arises from approaching the problem in the context of probability theory. We observe
connections between exponential utility maximization, Chernoff bounds, and cumulant generating
functions, which enables formulating a new optimization objective for risk-aware planning. This
new objective is essentially a re-parametrization of exponential utility, and inherits both the efﬁ-
cient optimization algorithms and the concordance to intuition about risk awareness. We show that
optimizing the proposed objective includes, as limiting cases, both minmax and expectation opti-
mization and allows interpolation between them. Additionally, we provide guarantees at a certain
percentile level, and show connections to mean minus variance optimization.
Two experiments, one synthetic and one based on real-world data, support our theoretical guaran-
tees and showcase the proposed optimization algorithms. Our largest MDP has 124791 state-action
pairs—signiﬁcantly larger than experiments in most past work on risk-aware planning. Our exper-
iments illustrate the ability of our approach to—out of the exponentially many policies available—
produce a family of policies that agrees with the human intuition of varying risk.

2 Background and Notation

An MDP consists of a state space S , an action space A, state transition dynamics, and a cost function
G. Assume that, at time t, the system is in state st ∈ S . Once the player chooses an action at ∈ A,
the system transitions stochastically to state st+1 ∈ S , with probability p(st+1 |st , at ), and the
player incurs a stochastic cost of Gt (st , at , st+1 ). The process continues for a number of time steps,
h, called the horizon. We eventually care about the total cost obtained. We represent the player’s
J h , deﬁned by J h := (cid:80)h−1
strategy as a time dependent policy, which is a measure on the space of state-actions. Finally, we
set the starting state to some ﬁxed s0 ∈ S . Then, the objective is to “optimize” the random variable
t=0 Gt (St , At , St+1 ). Traditionally, “optimizing” J means minimizing its
expected value, that is solving minπ Es,π [J ]. The classical solution to this problem is to run value

2

ps(cid:48) |s,a

Es,π [J t ]

q t+1 (s, a) :=

j t (s) := min
a

q t (s, a) = min
π

(cid:88)
iteration, summarized below:
s(cid:48)

(cid:0)Gt
s,a,s(cid:48) + j t (s(cid:48) )(cid:1) ,
We will refer to policies obtained by standard value iteration as expectimin policies. We use upper
case letters for random variables. We assume that the state-action space is ﬁnite and that sums with
zero terms, for example J 0 , are equal to zero. The notation Es,π signiﬁes taking the expectation
starting from S0 = s, and following policy π . We assume that costs are upper bounded, that is there
exists jM such that J ≤ jM almost surely for any start state and any policy, and that the expected
costs are ﬁnite. Finally, in this paper we will not consider discounting explicitly.
If necessary,
discounting can be introduced in one of two ways: either by adding a transition from every state,
for all actions, to an absorbing “end game” state, with probability γ , or by setting a time dependent
cost as Gt
old . Note that these two ways of introducing discounting are equivalent when
new = γ tGt
optimizing the expected cost, but they can differ in the risk-aware setting we are considering. We
refer the reader to [16] and [17] for further background on MDPs.

.

(1)

θ log Es,π

C δ
s,π [J ] = inf
θ>0

3 The Chernoff Functional as Risk-Aware Objective
(cid:16)
(cid:104)
eJ/θ (cid:105) − θ log(δ)
(cid:17)
We propose optimizing the following functional of the cost, which we call the Chernoff functional
since it often appears in proving Chernoff bounds:
First, note the total cost appears in the expression of the Chernoff functional as an exponential utility
(Es,π [eJ/θ ]). This shows that there is a strong connection between our method and exponential
utility optimization. Speciﬁcally, all policies proposed by our algorithm, including the ﬁnal solution,
are optimal policies with respect to the exponential utility for some parameter. These policies are
known to show risk-awareness in practice [12, 13], and our method inherits this property. In some
sense, our proposed objective is a re-parametrization of exponential utility, which was obtained
through its connections to Chernoff bounds and cumulant generating functions. The theorem below,
which is one of the main contributions of this paper, provides more reasons for optimizing the
Chernoff functional in the risk-aware setting. We will state and discuss the theorem here, but leave
the proof for the appendix.
Theorem 1. Let δ ∈ [0, 1], and let J be a random variable that has a cumulant generating function,
that is E exp(J/θ) < ∞ for all θ > 0. Then, the Chernoff functional of this random variable,
C δ [J ], is well deﬁned, and has the following properties:
(i) P (J ≥ C δ [J ]) ≤ δ
(ii) C 1 [J ] = limθ→∞ θ log E [eJ/θ ] = E [J ]
(iv) C δ [J ] = E [J ] + (cid:112)2 log(1/δ)Var[J ]
(iii) C 0 [J ] := limδ→0 C δ [J ] = limθ→0 θ log E [eJ/θ ] = sup{j : P {J ≥ j } > 0} < ∞.
(v) As δ → 1, C δ [J ] ≈ E [J ] + (cid:112)2 log(1/δ)Var[J ]
if J is Gaussian.
(vi) C δ [J ] is a smooth, decreasing function of δ .

Proof sketch. Property (i) is simply a Chernoff bound and follows by applying Markov’s inequality
to the random variable eJ/θ . Property (iv) follows from the fact that all but the ﬁrst two cumulants of
Gaussian random variables are zero [18]. Properties (ii), (iii), (v) and (vi) follow from the following
(a) log E ezJ = (cid:80)∞
properties of cumulant generating function, log E ezJ , [18]:
i=1 z iki /i! where ki are the cumulants [18], e.g. k1 = E [J ], k2 = Var[J ].
(b) log E ezJ as a function of z ∈ R is strictly convex, analytic and inﬁnitely differentiable in a
neighborhood of zero, if it is ﬁnite in that neighborhood.

3

Minimax cost

exact (f )
approximate ( ˆf )

Expectimin cost

θ

Figure 1: Plot showing the exact function f deﬁned in Equation 2 and the approximation that our
algorithm constructs ˆf for the Grid World MDP described in Section 5.1.

Properties (ii) and (iii) show that we can use the δ parameter to interpolate between the nominal
policy, which ignores risk, at δ = 1, and the minmax policy, which corresponds to extreme risk
aversion, at δ = 0. Property (i) shows that the value of the Chernoff functional is with probability
at least 1 − δ an upper bound on the cost obtained by following the corresponding Chernoff policy.
These two observations suggests that by tuning δ from 0 to 1 we can ﬁnd a family of risk-aware
policies, in order of risk aversion. Our experiments support this hypothesis (Section 5).
Property (i) shows a connection between our approach and percentile optimization. Although we
are not optimizing the δ -percentile directly, our method provides guarantees about it. Properties
(iv) and (v) show a connection between optimizing the Chernoff functional and mean minus vari-
ance optimization, which has been proposed before for risk-aware planning, but was found to be
intractable in general [3]. Via property (v), we can optimize mean minus variance with a low weight
on variance if we set δ close to 1. In the limit, this allows us to optimize the expectation, while
breaking ties in favor of lower variance. Property (iv) show that we can optimize mean minus scaled
standard deviation exactly if the total cost is Gaussian. Typically, this will not be the case, but, if
the MDP is ergodic and the time horizon is large enough, the total cost will be close to Gaussian, by
the central limit theorem. To see why this is true, note that, by the Markov property, costs between
successive returns to the same state are i.i.d. random variables [19]. Our formulation ties into mean
minus standard deviation optimization, which is of consistent dimensionality, unlike the classical
mean minus variance objective.

min
π

C δ
s,π [J ] = inf
θ>0

4 Optimizing the Proposed Objective
Finding the policy that optimizes our proposed objective at a given risk level δ amounts to a joint
eJ/θ (cid:105)(cid:17) − θ log(δ)
(cid:16)
(cid:16)
(cid:104)
(cid:17)
optimization problem (Bellman optimality does not hold for our objective; see Appendix for discus-
sion):
(cid:104)
(cid:16)
eJ/θ (cid:105)(cid:17)
θ log
Es,π
min
π
(f (θ) − θ log(δ)) where
f (θ) := θ log
= inf
min
π
θ>0
The inner optimization problem, the optimization over policies π , is simply exponential utility opti-
mization, a classical problem that can be solved efﬁciently. For brevity, we will not discuss solutions
to this problem and, instead, refer the readers to [12, 13]. The main difﬁculty is solving the outer
optimization problem, over the scale variable θ . Unfortunately, this problem is not convex and may
have a large number of local minima. Our main algorithmic contribution consists of an approach for
solving the outer (non-convex) optimization problem efﬁciently to some speciﬁed precision ε.
Based on Theorems 1 and 2 (below), we propose a method for ﬁnding the policy that minimizes the
Chernoff functional, to precision ε, with worst case time complexity O(h|S |2 |A|/ε). It is summa-
rized in Algorithm 1. Our approach is to solve the optimization problem in (2) with an approximation
of the function f (Figure 1 shows a example plot of this function). The algorithm maintains such
an approximation and improves it as needed up to a precision of ε. In practice we might want to
run the algorithm for more than one setting of δ to ﬁnd policies for the same planning task at dif-
ferent levels of risk aversion, say at n different levels. Naively, the time complexity of doing this

(2)

Es,π

.

4

Algorithm 1 Near optimal Chernoff bound algorithm
ˆf ← empty hash map
(cid:46) will store incremental approximation of f deﬁned in Eq. 2
ˆf [0] ← f (0)
(cid:46) minimax cost of the MDP
ˆf [∞] ← f (∞)
(cid:46) expectimin cost of the MDP
for θ ∈ {1, 10, 100, · · · }, until ˆf [∞] − ˆf [θ ] < ε, do
(cid:46) ﬁnd upper bound
ˆf [θ ] ← f (θ)
(cid:46) exponential utility optimization
for θ ∈ {1, 0.1, 0.01, · · · }, until ˆf [θ ] − ˆf [0] < ε, do
(cid:46) ﬁnd lower bound
ˆf [θ ] ← f (θ)
(cid:46) exponential utility optimization
θ ← (cid:16)
θ∗ · min{θ > θ∗ , θ ∈ keys( ˆf )}(cid:17)1/2
repeat
θ∗ ← argmin{θ ∈ keys( ˆf ) : ˆf [θ] − θ log(δ)},
(cid:46) argmin over previously computed costs
(cid:46) split interval at geometric mean
ˆf [θ ] ← f (θ)
(cid:46) exponential utility optimization
until ˆf [θ∗ ] − ˆf [θ ] < ε
(cid:46) until ˆf is an ε-accurate approximation of f
return optimal exponential utility policy(MDP, 1/θ∗ ).

would be O(nh|S |2 |A|/ε) but, fortunately, our function approximation can be reused between sub-
sequent runs of the algorithm, saving computation time, so the total complexity will, in fact, be only
O(h|S |2 |A|/ε + n).
Properties (ii) and (iii) of Theorem 1 imply that f (0) can be computed by minimax optimization
and f (∞) can be computed by value iteration (expectimin optimization), which both have the same
time complexity as exponential utility optimization: O(h|S |2 |A|). Once we have computed these
limits, the next step in the algorithm is ﬁnding some appropriate bounding interval, [θ1 , θ2 ], such that
f (0) − f (θ1 ) < ε and f (θ2 ) − f (∞) < ε. We do this by ﬁrst searching over θ = 1, .1, 10−2 , · · · ,
and, then, over θ = 1, 10, 102 , · · · . For a given machine architecture, the number of θ values is
bounded by the number format used in the implementation. For example, working with double
precision ﬂoating-point numbers limits the number of θ evaluations to 2 · 1023, implied by the
fact that exponents are only assigned 11 bits. In our experiments, this step takes 10-15 function
evaluations. Now, for any given risk level, δ , we will ﬁnd θ∗ that minimizes the objective, f (θ) −
θ log(δ), among those θ where we have already evaluated f . We will, then, evaluate f at a new point:
the geometric mean of θ∗ and its closest neighbor to the right. We stop iterating when the function
value at the new point is less than ε away from the function value at θ∗ , and return the corresponding
optimal exponential utility policy. Consequently, our algorithm evaluates f at a subset of the points
{θ1 (θ2/θ1 )i/n : i = 0, · · · , n} where n is a power of 2. Theorem 2 guarantees that to get an ε
guarantee for the accuracy of the optimization it sufﬁces to perform n(ε) = O(1/ε) evaluations of
f , where we are now treating log(δ2 ) − log(δ1 ) as a constant. Therefore, the number of functions
evaluations is O(1/ε), and, since the time complexity of every evaluation is O(h|S |2 |A|), the total
time complexity of the algorithm is O(h|S |2 |A|/ε).
Theorem 2. Consider the interval 0 < θ1 < θ2 split up into n sub-intervals by θn
i = θ1 (θ2 /θ1 )i/n ,
and let ˆfn (θ) := f (maxi∈0···n{θn
i < θ}) be our piecewise constant approximation to the func-
tion f (θ) deﬁned in Equation (2). Then, for a given approximation error ε there exists n(ε) =
O((log(δ2 ) − log(δ1 ))/ε) such that | ˆfn(ε) (θ) − f (θ)| ≤ ε for all θ ∈ [θ1 , θ2 ].
(cid:2)eJ/θ (cid:3) is a convex function since it is the perspective
Proof sketch. The key insight when proving this theorem is bounding rate of change of f . We
can immediately see that fπ (θ) := θ log Es,π
transformation of a convex function, namely, the cumulant generating function of the total cost J .
Additionally, Theorem 1 shows that fπ is lower bounded by Es,π [J ], assumed to be ﬁnite, which
implies that fπ is non-increasing. On the other hand, by directly differentiating the deﬁnition of fπ ,
π (θ) = fπ (θ) − Es,π [J eJ/θ ]/Es,π [eJ/θ ].
we get that θf (cid:48)
Since we assumed that the costs, J , are upper bounded, there exist a maximum cost jM such that
J ≤ jM almost surely for any starting state s, and any policy π . We have also shown that fπ (θ) ≥
Es,π [J ] ≥ jm := minπ (cid:48) Es,π (cid:48) [J ], so we conclude that −(jM − jm )/θ ≤ f (cid:48)
π (θ) ≤ 0 for any policy,
π . Now that we have bounded the derivative of fπ we can see that the value of f can not change too

5

#
#
$
#
# #
# #
# #

# #
#

#

← δ ∈ {0.75, 0.9, 0.99, 1.0 (expectimin)}
← δ = .6
← δ ∈ {0.1, 0.3}
← δ ∈ {10−3 , 10−4 , 10−5 , 10−6 , 10−7 }
← δ ∈ {10−10 , 10−8 }

Figure 2: Chernoff policies for the Grid World MDP. See text for complete description. The colored
arrows indicate the most likely paths under Chernoff policies for different values of δ . The minimax
policy (δ = 0) acts randomly since it assumes that any action will lead to a trap.

i ) and πi+1 := argminπ fπ (θn
i ]. Let πi := argminπ fπ (θn
much over an interval [θn
i+1 ). Then:
i+1 , θn
i ) − fπi+1 (θn
i+1 ) ≤ fπi+1 (θn
i ) − fπi+1 (θn
i ) − f (θn
0 ≤ f (θn
i+1 ) ≤
i+1 ) = fπi (θn
(cid:32)(cid:18) θ2
(cid:33)
i ) · (θn
i+1 − θn
i ) = −f (cid:48)
|f (cid:48)
≤ max
(θ)| · (θn
i+1 − θn
i ) ≤
(cid:19)1/n − 1
(θn
i ≤θ≤θn
πi+1
πi+1
θn
i+1
i+1 − θn
≤ (jM − jm ) · θn
i
θn
θ1
i
i ) ≤ fπi+1 (θn
where we ﬁrst used the fact that fπi (θn
i ), then the convexity of fπi+1
i ) = minπ fπ (θn
which implies that f (cid:48)
πi+1 is increasing, and, ﬁnally, our previous derivative bound. Our ﬁnal goal is
to ﬁnd a value of n(ε) such that the last expression in Equation 3 is less than ε. One can easily verify
that the following n(ε) satisﬁes this requirement (the detailed derivation appears in the Appendix):
n(ε) = (cid:100)(jM − jm )/ε log (θ2/θ1 ) + log (θ2/θ1 )(cid:101) .

= (jM − jm )

,

(3)

5 Experiments

We ran a number of experiments to test that our proposed objective indeed captures the intuitive
meaning of risk-aware planning. The ﬁrst experiment models a situation where it is immediately
obvious what the family of risk-aware policies should be. We show that optimizing the Chernoff
functional with increasing values of δ produces the intuitively correct family of policies. The second
experiment shows that our method can be applied successfully to a large scale, real world problem,
where it is difﬁcult to immediately “see” the risk-aware family of policies.
Our experiments empirically conﬁrm some of the properties of the Chernoff functional proven in
Theorem 1: the probability that the return is lower than the value of the Chernoff policy at level δ
is always less than δ , setting δ = 1 corresponds to optimizing the expected return with the added
beneﬁt of breaking ties in favor of lower variance, and setting δ = 0 leads to the minmax policy
whenever it is deﬁned. Additionally, we observed that policies at lower risk levels, δ , tend to have
lower expectation but also lower variance, if the structure of the problem allows it. Generally, the
probability of extremely bad outcomes decreases as we lower δ .

5.1 Grid world

We ﬁrst tested our algorithm on the Grid-World MDP (Figure 2). It models an obstacle avoidance
problem with stochastic dynamics. Each state corresponds to a square in the grid, and the actions,
{N, NE, E, SE, S, SW, W, NW}, typically cause a move in the respective direction. In unmarked
squares, the actor’s intention is executed with probability .93. Each of the seven remaining actions
might be executed instead, each with probability 0.01. Squares marked with $ and # are absorbing
states. The former gives a reward of 35 when entered, and the latter gives a penalty of 35. Any
other state transitions cost 1. The horizon is 35. To make the problem ﬁnite, we simply set the

6

δ ∈ {.99, .999, 1.0 (expectimin)}:
15:45 SHV - DFW 16:45
18:25 DFW - MCO 21:50
23:15 MCO - BQN 02:46
δ ∈ {.3, .4, .5, .6, .7, .8, .9}:
10:46 SHV - ATL 13:31
14:10 ATL - EWR 16:30
18:00 EWR - BQN 23:00
δ = 0.2:
12:35 SHV - DFW 13:30
18:25 DFW - MCO 21:50
23:15 MCO - BQN 02:46
δ ∈ {0 (minimax) , .001, .01, .1}:
12:35 SHV - DFW 13:30
14:25 DFW - MSY 15:50
17:50 MSY - JFK 21:46
23:40 JFK - BQN 04:20

(a) Paths under Chernoff policies as-
suming all ﬂight arrive on time, shown
using International Air Transport As-
sociation (IATA) airport codes.

Cumulative distribution function: P (V < v)

δ ∈ {.99, .999, 1}
δ ∈ {0.3, .4, · · · .9}
δ = 0.2
δ ∈ {0, .001, .01, .1}

1

0.8

0.6

0.4

0.2

0

−8

−5
−6
−7
−4
·104
Total reward: v (seconds)
(b) Cumulative distribution functions of rewards (equals minus
cost) under Chernoff policies at different risk levels. The aster-
isk (*) indicates the value of the policy. The big O indicates the
expected reward and the small o’s correspond to expectation plus-
minus standard deviation. 10000 samples.

Figure 3: Chernoff policies to travel from Shreveport Regional Airport (SHV) to Rafael Hern ´andez
Airport (BQN) at different risk levels.

probability of all transitions outside the grid boundary to zero, and re-normalize. We set the precision
to ε = 1. With this setting, our algorithm performed exponential utility optimization for 97 different
parameters when planning for 14 values of the risk level δ . For low values of δ , the algorithm behaves
cautiously, preferring longer, but safer routes. For higher values of δ , the algorithm is willing to take
shorter routes, but also accepts increasing amounts of risk.

5.2 Air travel planning

The aerial travel planning MDP (Figure 3) illustrates that our method applies to real-world problems
at a large scale. It models the problem of buying airplane tickets to travel between two cities, when
you care only about reaching the destination in a reliable amount of time. We assume that, if you
miss a connecting ﬂight due to delays, the airline will re-issue a ticket for the route of your choice
leading to the original destination. In this case, a cautious traveler will consider a number of aspects:
choosing ﬂights that usually arrive on time, choosing longer connection times and making sure that,
in case of a missed connection, there are good alternative routes.
In our implementation, the state space consists of pairs of all airports and times when ﬂights depart
from those airports. At every state there are two actions: either take the ﬂight that departs at that time,
or wait. The total number of state-action pairs is 124791. To keep the horizon low, we introduce
enough wait transitions so that it takes no more than 10 transitions to wait a whole day in the busiest
airport (about 1000 ﬂights per day) and we set the horizon at 100. Costs are deterministic and cor-
respond to the time difference between the scheduled departure time of the ﬁrst ﬂight and the arrival
time. We compute transition probabilities based on historical data, available from the Ofﬁce of Air-
line Information, Bureau of Transportation Statistics, at http://www.transtats.bts.gov/.
Particularly, we have used on-time statistics for February 2011. Airlines often try to conceal statistics
for ﬂights with low on-time performance by slightly changing departure times and ﬂight numbers.
Sometimes, they do this every week. Consequently, we ﬁrst clustered together all ﬂights with the
same origin and destination that were scheduled to depart within 15 minutes of each other, under the
assumption they would have the same on-time statistics. We, then, remove all clusters with fewer
than 7 recorded ﬂights, since these usually correspond to incidental ﬂights.

7

20

10

0

80

60

40

20

0

0
100
80
60
40
20
(a) Number of exponential utility optimization runs
to compute the Chernoff policies.

0
12
10
8
6
4
2
(b) Number of distinct Chernoff policies found.

Figure 4: Histograms demonstrating the efﬁciency and relevance of our algorithm on 500 randomly
chosen origin - destination airport pairs, at 15 risk levels.

To test our algorithm on this problem, we randomly chose 500 origin - destination airport pairs and
computed the Chernoff policies for risk levels: δ ∈ {1.0, .999, .99, .9, .8, · · · , .1, 0.01, 0.001, 0.0},
and precision ε = 10 minutes. Figure 3 shows the resulting policies and corresponding cost (travel
time) histograms for one such randomly chosen route. To address the question of computational
efﬁciency, Figure 4a shows a histogram of the total number of different parameters for which our
algorithm ran exponential utility optimization. To address the question of relevance, Figure 4b shows
the number of distinct Chernoff policies found among the risk levels. Two policies, π and π (cid:48) , are
considered distinct if the total variation distance of the induced state - action occupation measures
is more than 10−6 ; that is, if there exists t, s, and a such that |Pπ {St = s, At = a} − Pπ (cid:48) {St =
s, At = a}| ≥ 10−6 . For most origin - destination pairs we found a rich spectrum of distinct
policies, but there are also cases where all the Chernoff policies are identical or only the expectimax
and minimax policies differ.
Many air travel routes exhibit only two phases mainly because they connect small airports where
only one or two ﬂights of the type we consider land or take off per day. Consequently there will be
few policies to choose from in these cases. In our experiment, we chose 200 origin and destination
pairs at random and, of these, 72 routes show only two phases. In 41 of these cases, either the
origin or the destination airport serves only one or two ﬂights per day total. Only 9 of the two-phase
routes connect airports which both serve more than 10 ﬂights per day total, and, of course, not all of
these ﬂight will help reach the destination. Thus, typically the reason we see only two phases is that
the choice of policies is very limited. Additionally, airlines have an incentive to provide sufﬁcient
margin such that passengers can make connections and they don’t have to re-ticket them. That is,
they tend to set up routes such that, even in a worse than average scenario, the original route will
tend to succeed.

6 Conclusion

We proposed a new optimization objective for risk-aware planning called the Chernoff functional.
Our objective has a free parameter δ that can be used to interpolate between the nominal policy,
which ignores risk, at δ = 1, and the minmax policy, which corresponds to extreme risk aversion,
at δ = 0. The value of the Chernoff functional is with probability at least 1 − δ an upper bound on
the cost incurred by following the corresponding Chernoff policy. We established a close connec-
tion between optimizing the Chernoff functional and mean minus variance optimization, which has
been proposed before for risk-aware planning, but was found to be intractable in general. We also
establish a close connection with optimization of mean minus scaled standard deviation.
We proposed an efﬁcient algorithm that optimizes the Chernoff functional to any desired accuracy ε
requiring O(1/ε) runs of exponential utility optimization. Our experiments illustrate the capability
of our approach to recover a spread of policies in the spectrum from risk neutral to minmax requiring
a running time that was on average about ten times the running of value iteration.

8

References
[1] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasub-
ramanian, P. Vosshall, and W. Vogels. Dynamo: amazon’s highly available key-value store.
ACM SIGOPS Operating Systems Review, 41(6):205–220, 2007.
[2] Philippe Jorion. Value at risk: the new benchmark for managing ﬁnancial risk, volume 1.
McGraw-Hill Professional, 2007.
[3] Shie Mannor and John N. Tsitsiklis. Mean-Variance Optimization in Markov Decision Pro-
cesses. In Proceedings of the 28 International Conference on Machine Learning, 2011.
[4] Erick Delage and Shie Mannor. Percentile optimization in uncertain Markov decision processes
with application to efﬁcient exploration. ICML; Vol. 227, page 225, 2007.
[5] Jay K. Satia and Roy E. Lave Jr. Markovian Decision Processes with Uncertain Transition
Probabilities. Operations Research, 21(3):728–740, 1973.
[6] Matthias Heger. Consideration of risk in reinforcement learning. In Proceedings of the 11th
International Machine Learning Conference (1994), pages 105–111. Morgan Kaufmann, 1994.
[7] Steve Levitt and Adi Ben-Israel. On Modeling Risk in Markov Decision Processes. Optimiza-
tion and Related Topics, pages 27–41, 2001.
[8] Erick Delage and Shie Mannor. Percentile Optimization for Markov Decision Processes with
Parameter Uncertainty. Operations Research, 58(1):203–213, 2010.
[9] Arnab Nilim and Laurent El Ghaoui. Robust Control of Markov Decision Processes with
Uncertain Transition Matrices. Operations Research, 53(5):780–798, 2005.
[10] M. Bouakiz and Y. Kebir. Target-level criterion in Markov decision processes. Journal of
Optimization Theory and Applications, 86(1):1–15, July 1995.
[11] Congbin Wu and Yuanlie Lin. Minimizing Risk Models in Markov Decision Processes with
Policies Depending on Target Values. Journal of Mathematical Analysis and Applications,
231(1):47–67, 1999.
[12] S.I. Marcus, E. Fern ´andez-Gaucherand, D. Hern ´andez-Hernandez, S. Coraluppi, and P. Fard.
Risk sensitive Markov decision processes. Systems and Control in the Twenty-First Century,
29:263–281, 1997.
[13] VS Borkar and SP Meyn. Risk-sensitive optimal control for Markov decision processes with
monotone cost. Mathematics of Operations Research, 27(1):192–209, 2002.
[14] B. Defourny, D. Ernst, and L. Wehenkel. Risk-aware decision making and dynamic program-
ming. In NIPS 2008 Workshop on Model Uncertainty and Risk in RL, 2008.
[15] Yann Le Tallec. Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Pro-
cesses. PhD thesis, Massachusetts Institute of Technology, 2007.
[16] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. MIT Press,
1998.
[17] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc,
October 1996.
[18] J. F. Kenney and E. S. Keeping. Cumulants and the cumulant-generating function, additive
property of cumulants, and Sheppard’s correction. In Mathematics of Statistics, chapter 4.10-
4.12, pages 77–82. Van Nostrand, Princeton, NJ, 2 edition, 1951.
[19] Richard Durrett. Probability: Theory and Examples. Cambridge University Press, 2010.

9


Dual-Space Analysis of the Sparse Linear Model

David Wipf and Yi Wu
Visual Computing Group, Microsoft Research Asia
davidwipf@gmail.com, jxwuyi@gmail.com

Abstract

Sparse linear (or generalized linear) models combine a standard likelihood func-
tion with a sparse prior on the unknown coefﬁcients. These pr iors can conve-
niently be expressed as a maximization over zero-mean Gaussians with different
variance hyperparameters. Standard MAP estimation (Type I) involves maximiz-
ing over both the hyperparameters and coefﬁcients, while an empirical Bayesian
alternative (Type II) ﬁrst marginalizes the coefﬁcients an
d then maximizes over
the hyperparameters, leading to a tractable posterior approximation. The under-
lying cost functions can be related via a dual-space framework from [22], which
allows both the Type I or Type II objectives to be expressed in either coefﬁcient
or hyperparmeter space. This perspective is useful because some analyses or ex-
tensions are more conducive to development in one space or the other. Herein we
consider the estimation of a trade-off parameter balancing sparsity and data ﬁt. As
this parameter is effectively a variance, natural estimators exist by assessing the
problem in hyperparameter (variance) space, transitioning natural ideas from Type
II to solve what is much less intuitive for Type I. In contrast, for analyses of update
rules and sparsity properties of local and global solutions, as well as extensions to
more general likelihood models, we can leverage coefﬁcient -space techniques de-
veloped for Type I and apply them to Type II. For example, this allows us to prove
that Type II-inspired techniques can be successful recovering sparse coefﬁcients
when unfavorable restricted isometry properties (RIP) lead to failure of popular
ℓ1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian
likelihood models lead to intractable integrations.

1

Introduction

We begin with the likelihood model

(1)
y = Φx + ǫ,
where Φ ∈ Rn×m is a dictionary of unit ℓ2 -norm basis vectors, x ∈ Rm is a vector of unknown
coefﬁcients we would like to estimate, y ∈ Rn is the observed signal, and ǫ is noise distributed as
N (ǫ; 0, λI ) (later we consider more general likelihood models). In many practical situations where
large numbers of features are present relative to the signal dimension, the problem of estimating x
given y becomes ill-posed. A Bayesian framework is intuitively appealing for formulating these
types of problems because prior assumptions must be incorporated, whether explicitly or implicitly,
to regularize the solution space.

Recently, there has been a growing interest in models that employ sparse priors p(x) to encourage
solutions x with mostly small or zero-valued coefﬁcients and a few large or unrestricted values, i.e.,
we are assuming the generative x is a sparse vector. Such solutions can be favored by using
exp (cid:20)−
i (cid:1)(cid:21) ,
g(xi )(cid:21) = Yi
exp (cid:20)−
1
1
p(x) ∝ Yi
h (cid:0)x2
2
2
with h concave and non-decreasing on [0, ∞) [15, 16]. Virtually all sparse priors of interest can
be expressed in this manner, including the popular Laplacian, Jeffreys, Student’s t, and generalized

(2)

1

Gaussian distributions. Roughly speaking, the ‘more concave’ h, the more sparse we expect x to be.
For example, with h(z ) = z , we recover a Gaussian, which is not sparse at all, while h(z ) = √z
gives a Laplacian distribution, with characteristic heavy tails and a sharp peak at zero.

All sparse priors of the form (2) can be conveniently framed in terms of a collection of non-negative
latent variables or hyperparameters γ , [γ1 , . . . , γm ]T for purposes of optimization, approximation,
and/or inference. The hyperparameters dictate the structure of the prior via
p(x) = Yi
γi≥0 N (xi ; 0, γi )ϕ(γi ),
p(xi ), p(xi ) = max
where ϕ(γi ) is some non-negative function that is sometimes treated as a hyperprior, although it will
not generally integrate to one. For the purpose of obtaining sparse point estimates of x, which will
be our primary focus herein, models with latent variable sparse priors are frequently handled in one
of two ways. First, the latent structure afforded by (3) offers a very convenient means of obtaining
(possibly local) maximum a posteriori (MAP) estimates of x by iteratively solving
2 + λ Xi (cid:20) x2
+ log γi + f (γi )(cid:21) ,
x;γ(cid:23)0 ky − Φxk2
i
x − log p(y |x)p(x) = arg min
x(I ) = arg min
γi
where f (γi ) , −2 log ϕ(γi ) and x(I ) is commonly referred to as a Type I estimator. Examples
include minimum ℓp -norm approaches [4, 11, 16], Jeffreys prior-based methods sometimes called
FOCUSS [7, 6, 9], algorithms for computing the basis pursuit (BP) or Lasso solution [6, 16, 18],
and iterative reweighted ℓ1 methods [3].
Secondly, instead of maximizing over both x and γ as in (4), Type II methods ﬁrst integrate out
(marginalize) the unknown x and then solve the empirical Bayesian problem [19]
γ Z p(y |x) Yi
p(γ |y) = arg max
m
Xi=1
yT Σ−1
y y + log |Σy | +
f (γi ),
where Σy , λI + ΦΓΦT and Γ , diag[γ ]. Once γ(I I ) is obtained, the conditional distribution
p(x|y ; γ(I I ) ) is Gaussian, and a point estimate for x naturally emerges as the posterior mean
x(I I ) = E (cid:2)x|y ; γ(I I ) (cid:3) = Γ(I I )ΦT (cid:0)λI + ΦΓ(I I )ΦT (cid:1)−1
(6)
y .
Pertinent examples include sparse Bayesian learning and the relevance vector machine (RVM) [19],
automatic relevance determination (ARD) [14], methods for learning overcomplete dictionaries [8],
and large-scale experimental design [17].

N (x; 0, γi )ϕ(γi )dxi

γ(I I ) = arg max
γ

= arg min
γ

(3)

(4)

(5)

While initially these two approaches may seem vastly different, both can be directly compared using
a dual-space view [22] of the underlying cost functions. In brief, this involves expressing both the
Type I and Type II objective solely in terms of either x or γ as reviewed in Section 2. The dual-space
view is advantageous for several reasons, such as establishing connections between algorithms, de-
veloping efﬁcient update rules, or handling more general (n on-Gaussian) likelihood functions. In
Section 3, we utilize γ -space cost functions to develop a principled method for choosing the trade-
off parameter λ (which accompanies the Gaussian likelihood model and essentially balances sparsity
and data ﬁt) and demonstrate its effectiveness via simulati ons. Section 4 then derives a new Type
II-inspired algorithm in x-space that can compute maximally sparse (minimal ℓ0 norm) solutions
even with highly coherent dictionaries, proving a result for clustered dictionaries that previously has
only been shown empirically [21]. Finally, Section 5 leverages duality to address Type II methods
with generalized likelihood functions that previously were rendered untenable because of intractable
integrals. In general, some tasks and analyses are easier to undertake in γ -space (Section 3), while
others are more transparent in x-space (Sections 4 and 5). Here we consider both with the goal of
advancing the proper understanding and full utilization of the sparse linear model.

2 Dual-Space View of the Sparse Linear Model

Type I is based on a natural cost function in x-space, p(x|y), while Type II involves an analogous
function in γ -space, p(γ |y). The dual-space view deﬁnes a corresponding γ -space cost function for
Type I and a x-space cost function for Type II to complete the symmetry.

2

Type II in x-Space: Using the relationship

(7)

(9)

and

f (γi ).

1
λ ky − Φxk2
2 + xT Γ−1x
yΣ−1
y y = min
x
as in [22], it can be shown that the Type II coefﬁcients from (6 ) satisfy x(I I ) = arg minx L(I I ) (x),
where
L(I I ) (x) , ky − Φxk2
(8)
2 + λg(I I ) (x),
x2
+ log |Σy | + Xi
γ(cid:23)0 Xi
i
g(I I ) (x) , min
γi
This reformulation of Type II in x-space is revealing for multiple reasons (Sections 4 and 5 will
address additional reasons in detail). For many applications of the sparse linear model, the primary
goal is simply a point estimate that exhibits some degree of sparsity, meaning many elements of ˆx
near zero and a few relatively large coefﬁcients. This requi res a penalty function g(x) that is concave
m ]T . In the context of Type I, any prior p(x) expressible via
and non-decreasing in x2 , [x2
1 , . . . , x2
(2) will satisfy this condition by deﬁnition; such priors ar e said to be strongly super-Gaussian and
will always have positive kurtosis [15]. Regarding Type II, because the associated x-space penalty
(9) is represented as a minimum of upper-bounding hyperplanes with respect to x2 (and the slopes
are all non-negative given γ (cid:23) 0), it must therefore be concave and non-decreasing in x2 [1].
For compression, interpretability, or other practical reasons, it is sometimes desirable to have exactly
sparse point estimates, with many (or most) elements of x equal to exactly zero. This then necessi-
tates a penalty function g(x) that is concave and non-decreasing in |x| , [|x1 |, . . . , |xm |]T , a much
stronger condition. In the case of Type I, if log γ + f (γ ) is concave and non-decreasing in γ , then
g(x) = Pi g(xi ) satis ﬁes this condition. The Type II analog, which emerges b y further inspection
of (9) stipulates that if
log |Σy | + Xi
f (γi ) = log (cid:12)(cid:12)λ−1ΦT Φ + Γ−1 (cid:12)(cid:12) + log |Γ| + Xi
(10)
f (γi )
is a concave and non-decreasing function of γ , then g(I I ) (x) will be a concave, non-decreasing
function of |x|. For this purpose it is sufﬁcient, but not necessary, that f be a concave and non-
decreasing function. Note that this is a somewhat stronger criteria than Type I since the ﬁrst term
on the righthand side of (10) (which is absent from Type I) is actually convex in γ . Regardless, it is
now very transparent how Type II may promote sparsity akin to Type I.
The dual-space view also leads to efﬁcient, convergent algo rithms such as iterative reweighted ℓ1
minimization and its variants as discussed in [22]. However, building on these ideas, we can demon-
strate here that it also elucidates the original, widely applied update procedures developed for im-
plementing the relevance vector machine (RVM), a popular Type II method for regression and clas-
si ﬁcation that assumes f (γ ) = 0 [19]. In fact these updates, which were inspired by a ﬁxed-po int
heuristic from [12], have been widely used for a number of Bayesian inference tasks without any
formal analyses or justi ﬁcation. 1 The dual-space formulation can be leveraged to show that these
updates are in fact executing a coordinate-wise, iterative min-max procedure in search of a saddle
point. Speci ﬁcally we have the following result (all proofs are in the supplementary material):

Theorem 1. The original RVM update rule from [19, Equation (16)] is equivalent to a closed-form,
coordinate-wise optimization of
z(cid:23)0 "ky − Φxk2
+ zi log γi(cid:19) − ϑ(z )#
2 + Xi (cid:18) x2
i
max
min
γi
x;γ(cid:23)0
over x, γ , and z , where ϑ(z ) is the convex conjugate function [1] of log (cid:12)(cid:12)λI + Φdiag[exp(u)]ΦT (cid:12)(cid:12)
with respect to u.
1Although a more recent, step-wise variant of the RVM has been shown to be substantially faster [20],
the original version is still germane since it can easily be extended to handle more general structured sparsity
problems. The step-wise method cannot without introducing additional approximations [10].

(11)

3

Type I in γ -Space: Similar methodology and the expansion of yT Σ−1
y y can be used to express
the Type I optimization problem in γ -space, which serves several useful purposes. Let γ(I )
,
arg minγ(cid:23)0 L(I ) (γ ), with

L(I ) (γ ) , yT Σ−1
y y + log |Γ| +
Then the Type I coefﬁcients obtained from (4) satisfy
x(I ) = Γ(I )ΦT (cid:0)λI + ΦΓ(I )ΦT (cid:1)−1
Section 3 will use γ -space cost functions to derive well-motivated approaches for learning the trade-
off parameter λ.

f (γi ).

(12)

(13)

y .

m
Xi=1

3 Choosing the Trade-off Parameter λ

[log λ + f (λ)]

[log γi + f (γi )] +

= yT Σ−1
y y +

L(I ) (γ , λ) , yT Σ−1
y y +

The trade-off parameter is crucial for obtaining good estimates of x. In general, if λ is too large,
ˆx → 0; too small and ˆx is over ﬁtted to the noise. In practice, either expensive cro ss-validation or
some heuristic procedure is often required. However, because λ can be interpreted as a variance, it is
useful to address its estimation in γ -space, in which existing unknowns (i.e., γ ) are also variances.
Learning λ with Type I: Consider the Type I cost function L(I ) (γ ). The data-dependent term can be
shown to be a convex, non-increasing function of γ , which encourages each element to be large. The
second term is a penalty factor that regulates the size of γ . It is here that a convenient regularizer
for λ can be incorporated.
This can be accomplished as follows. First we expand Σy via Σy = Pm
·i +Pn
j ,
j=1 λej eT
j=1 γiφ·iφT
where φ·i denotes the i-th column of Φ and ej is a column vector of zeros with a ‘1’ in the j -th
location. Thus we observe that λ is embedded in the data-dependent term in the exact same fashion
as each γi . This motivates a penalty on λ with similar correspondence, leading to the objective
n
m
Xj=1
Xi=1
m
Xi=1
[log γi + f (γi )] + n log λ + nf (λ).
While admittedly simple, this construction is appealing because, regardless of how each γi is penal-
ized, λ is penalized in a proportional manner, so both γ and λ have a properly balanced chance of
explaining the observed data. This is important because the optimal λ will be highly dependent on
both the true noise level, and crucially, the particular sparse prior assumed p(x) (as reﬂected by f ).
For analysis or implementational purposes, we may convert L(I ) (γ , λ) back to x-space, with λ-
dependency now removed. It can then be shown that solving (4), with λ ﬁxed to the value that
minimizes (14), is equivalent to solving
√n kuk2(cid:19) ,
g(xi ) + ng (cid:18) 1
x,u Xi
min
If x∗ and u∗ minimize (15), then we can demonstrate using [15] that the corresponding λ estimate,
which also minimizes (14), is given by λ∗ = ∂h(z )/∂ z evaluated at z = 1/nku∗ k2
2 . Note that if we
were just performing maximum likelihood estimation of λ given x∗ , the optimal value would reduce
to simply λ∗ = 1/nku∗ k2
2 , with no inﬂuence from the prior on x. This is a fundamental weakness.
Solving (15), or equivalently (14), can be accomplished using simple iterative reweighted least
squares, or if g is concave in |xi |, an iterative reweighted second-order-cone (SOC) minimization.
Learning λ with Type II: The same procedure can be adopted for Type II yielding the cost function
y y + log |Σy | + Xi
L(I I ) (γ , λ) = yT Σ−1
4

s.t. y = Φx + u.

f (γi ) + nf (λ),

(14)

(15)

(16)

where we note that, unlike in the Type I case above, the log-based term is already naturally balanced
between λ and γ by virtue of the symmetric embedding in Σy . It is important to stress that this
Type II prescription for learning λ is not the same as originally proposed in the literature for Type
II models of this genre. In this context, ϕ(γi ) is interpreted a hyperprior on γi , and an equivalent
distribution is assumed on the noise variance λ. Importantly, these assumptions leave out the factor
of n in (16), and so an asymmetry is created.
Simulation Examples: Empirical tests help to illustrate the efﬁcacy of this proc edure. As in many
applications of sparse reconstruction, here we are only concerned with accurately estimating x,
whose nonzero entries may have physical signi ﬁcance (e.g.,
source localization [16], compressive
sensing [2], etc.), as opposed to predicting new values of y . Therefore, automatically learning the
value of λ is particularly relevant, since cross-validation is often not possible.2 Simulations are
helpful for evaluation purposes since we then have access to the true sparse generating vector.

Figure 1 compares the estimation performance obtained by minimizing (15) with two different se-
lections for g : g(x) = kxkp
p = Pi |xi |p , with p = 0.01 and p = 1.0. Data generation proceeds
as follows: We create a random 100 × 50 dictionary Φ, with ℓ2 -normalized, iid Gaussian columns.
x is randomly generated with 10 unit Gaussian nonzero elements. We then compute y = Φx + ǫ,
where ǫ is iid Gaussian noise producing an SNR of 0dB. To determine what λ values lead to optimal
performance we solve (4) with the appropriate g over a range of ﬁxed λ values (10−4 to 101 ) and
then compute the error between x and ˆx. The minimum of this curve reﬂects the best performance
we can hope to achieve when learning λ blindly. In Figure 1 (Top) we plot these curves for both
Type I methods averaged over 1000 independent trials.
Next we solve (15), which produces an estimate of both x and λ. We mark with an ‘+’ the learned
λ versus the corresponding error of ˆx. In both cases the learned λ’s (averaged across trials) perform
just as well as if we knew the optimal value a priori. Results using other noise levels, problem di-
mensions n and m, sparsity levels kxk0 , and sparsity penalties g are similar. See the supplementary
material for more examples.

Figure 1 (Bottom) shows the average sparsity of estimates ˆx, as quanti ﬁed by the
ℓ0 norm k ˆxk0 ,
across λ values (kxk0 returns a count of the number of nonzero elements in x). The ‘+’ indicates
the average sparsity of each ˆx for the learned λ as before. In general, the ℓ(0.01) penalty produces
a much sparser estimate, very near the true value of kxk0 = 10 at the optimal λ. The ℓ1 penalty,
which is substantially less concave/sparsity-inducing, still sets some elements to exactly zero, but
also substantially shrinks nonzero coefﬁcients in achievi ng a similar overall reconstruction error.
This highlights the importance of learning a λ via a penalty that is properly matched to the prior on
x: if we instead tried to force a particular sparsity value (in this case 10), then the ℓ1 solution would
be very suboptimal. Finally we note that maximum likelihood (ML) estimation of λ performs very
poorly (not shown), except in the special case where the ML estimate is equivalent to solving (14)
as occurs when f (γ ) = 0 (see [6]). The proposed method can be viewed as adding a principled
hyperprior on λ, properly matched to p(x), that compensates for this shortcoming of standard ML.
Type II λ estimation has been explored elsewhere for the special case where f (γ ) = 0 [19], which
renders the factor of n in (16) irrelevant; however, for other selections we have found this factor
to improve performance (not shown). For space considerations we have focused our attention here
on Type I, which has frequently been noted for not lending itself well to λ estimation (or related
parameters) [6, 13]. In fact, the symmetry afforded by the dual-space perspective reveals that Type
I is just as natural a candidate for this task as Type II, and may be preferred in high-dimensional
settings where computational resources are at a premium.

4 Maximally Sparse Estimation

With the advent of compressive sensing and other related applications, there has been growing inter-
est in ﬁnding maximally sparse signal representations from redundant dictionaries (m ≫ n) [3, 5].
The canonical form of this problem involves solving
x0 , arg min
x kxk0 ,
2For example, in non-stationary environments, the value of both x and λ may be completely different for
any new y , which then necessitates that we estimate both jointly.

s.t. y = Φx.

(17)

5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

E
S
M

ℓ(0.01)
ℓ1

 

 

50

45

40

35

30

0
k
25
ˆx
k
20

15

10

5

ℓ(0.01)
ℓ1

0
 
10−4

100

101

0
 
10−4

100

101

10−3

10−2

10−3

10−1
10−1
λ value
λ value
Figure 1: Left: Normalized mean-squared error (MSE) given by (cid:10)kx − ˆxk2
2 /kxk2 (cid:11) (where the
average is across 1000 trials) plotted versus λ for two different Type I approaches. Each black ‘+’
represents the estimated value of λ (averaged across trials) and the associated MSE produced with
this estimate. In both cases the estimated value achieves the lowest possible MSE (it can actually
be slightly lower than the curve because its value is allowed to ﬂuctuate from t rial to trial). Right:
Solution sparsity k ˆxk0 versus λ. Even though they both lead to similar MSE, the ℓ(0.01) penalty
produces a much sparser estimate at the optimal λ value.

10−2

While (17) is NP-hard, whenever the dictionary Φ satis ﬁes a restricted isometry property (RIP) [2]
or a related structural assumption, meaning that each kx0 k0 columns of Φ are sufﬁciently close
to orthonormal (i.e., mutually uncorrelated), then replacing ℓ0 with ℓ1 in (17) leads to a convex
problem with an equivalent global solution. Unfortunately however, in many situations (e.g., feature
selection, source localization) these RIP equivalence conditions are grossly violated, implying that
the ℓ1 solution may deviate substantially from x0 .
An alternative is to instead replace (17) with minimization of (8) and then take the limit as λ → 0.
(Note that the extension to the noisy case with λ > 0 is straightforward, but analysis is more
difﬁcult.) In this regime the optimization problem reduces
to

s.t. y = Φx.
(18)
x(I I ) = lim
arg min
g(I I ) (x),
λ→0
x
If log |Σy | + Pi f (γi ) is concave, then (18) can be minimized using reweighted ℓ1 minimization.
With initial weight vector w(0) = 1, the (k + 1)-th iteration involves computing
∂ g(I I ) (x)
(cid:12)(cid:12)(cid:12)(cid:12)x=x(k+1)
w(k)
x: y=Φx Xi
w(k+1) ←
x(k+1) ← arg min
(19)
|xi |,
.
i
∂ |xi |
With f (γ ) = 0, iterating (19) will provably lead to an estimate of x0 that is as good or better than the
ℓ1 solution [21], in particular when Φ has highly correlated columns. Additionally, the assumption
f (γ ) = 0 leads to a closed-form expression for the weights w(k+1) . Let
φ·i(cid:21)q
ηi (x; α, q) , (cid:20)φT
·i (cid:16)αI + Φ|X (k+1) |2ΦT (cid:17)−1
where |X (k+1 | denotes a diagonal matrix with i-th diagonal entry given by |x(k+1)
|. Then w(k+1)
i
can be computed via w(k+1)
= ηi (x; 0, 1/2), ∀i. It remains unclear however in what circum-
i
stances this type of update can lead to guaranteed improvement nor if the functions ηi (x; 0, 1/2)
are even the optimal choice. We will now demonstrate that for certain selections of α and q , we
can guarantee that reweighted ℓ1 using ηi (x; α, q) is guaranteed to recover x0 exactly if Φ is drawn
from what we call a clustered dictionary model.

(20)

,

Deﬁnition 1. Clustered Dictionary Model: Let Φ(d)
uncorr denote any dictionary such that ℓ1 mini-
mization succeeds in solving (17) for all kx0 k0 ≤ d. Let Φ(d,ǫ)
corr denote any dictionary obtained
by replacing each column of Φ(d)
uncorr with a “cluster ” of mi basis vectors such that the angle be-
tween any two vectors within a cluster is less than some ǫ > 0. We also deﬁne the cluster support

6

Ω0 ⊂ {1, 2, . . . , m} as the set of cluster indices whereby x0 has at least one nonzero element.
Finally, we assume that the resulting Φ(d,ǫ)
corr is such that every n × n submatrix is full rank.
Theorem 2. For any sparse vector x0 and any dictionary Φ(d,ǫ)
corr obtained from the clustered
dictionary model with ǫ sufﬁciently small, reweighted ℓ1 minimization using weights ηi (x; λ, q)
with some q ≥ 1 and α sufﬁciently small will recover x0 exactly provided that |Ω0 | ≤ d,
mi ≤ n, and within each cluster k ∈ Ω0 the coefﬁcients do not sum to zero.
Pi∈Ω0
Theorem 2 implies that even though ℓ1 may fail to ﬁnd the maximally sparse x0 because of severe
RIP violations (high correlations between groups of dictionary columns as dictated by ǫ lead directly
to a poor RIP), a Type II-inspired method can still be successful. Moreover, because whenever ℓ1
does succeed, Type II will always succeed as well (assuming a reweighted ℓ1 implementation), the
converse (RIP violation leading to Type II failure but not ℓ1 failure) can never happen. Recent work
from [21] has argued that Type II may be useful for addressing the sparse recovery problem with
correlated dictionaries, and empirical evidence is provided showing vastly superior performance on
clustered dictionaries. However, we stress that no results proving global convergence to the correct,
maximally sparse solution have been shown before in the case of structured dictionaries (except
in special cases with strong, unveri ﬁable constraints on co efﬁcient magnitudes [21]). Moreover,
the proposed weighting strategy ηi (x; λ, q) accomplishes this without any particular tuning to the
clustered dictionary model under consideration and thus likely holds in many other cases as well.

5 Generalized Likelihood functions

Type I methods naturally accommodate alternative likelihood functions. We simply must replace the
quadratic data ﬁt term from (4) with some preferred function and then coordinate-wise optimization
may proceed provided we have an efﬁcient means of computing a weighted ℓ2 -norm penalized
solution. In contrast, generalizing Type II is substantially more complicated because it is no longer
possible to compute the marginalization (5) or the posterior distribution p(x|y ; γ(I I ) ). Therefore, to
obtain a tractable estimate x(I I ) additional heuristics are required. For example, the RVM classi ﬁer
from [19] employs a Laplace approximation for this purpose; however, it is not clear what cost
function is being minimized nor rigorous properties of the estimated solutions.

Fortunately, the dual x-space view provides a natural mechanism for generalizing the basic Type II
methodology to address alternative likelihood functions in a more principled manner. In the case
of classi ﬁcation problems, we might want to replace the Gaus sian likelihood p(y |x) implied by (1)
with a multivariate Bernoulli distribution p(y |x) ∝ log[−ψ(y , x)] where ψ(y , x) is the function
ψ (y , x) , Xj
(21)
(yj log [σj (x)] + (1 − yj ) log [1 − σj (x)]) .
Here yj ∈ {0, 1} and σj (x) , 1/[1+exp(φT
j ·x)], with φj · denoting the j -th row of Φ. This function
may be naturally substituted into the x-space Type II cost function (8) giving us the candidate
penalized logistic regression function
min
x
Importantly, recasting Type II classi ﬁcation using x-space in this way, with its attendant well-
speci ﬁed cost function, facilitates more concrete analyse s (see below) regarding properties of global
and local minima that were previously rendered inaccessible because of intractable integrals and
compensatory approximations. Moreover, we retain a tight connection with the original Type II
marginalization process as follows.

ψ (y , x) + λg(I I ) (x).

(22)

Consider the strict upper bound on the function ψ(y , x) (obtained by a Taylor series approximation
and a Hessian bound) given by
ψ(y , x) ≤ π(y , x, v) , ψ(y , v) + (v − x)T ΦT t + 1/8 (v − x)T ΦT Φ (v − x) ,
(23)
where t = [t1 , . . . , tn ]T with tj , yj − σj (v). This bound holds for all v with equality
when v = x. Using this result we obtain the lower bound on the marginal likelihood given by
R log[−ψ(y , x)]p(x)dx ≥ R log[−π(y , x, v)]p(x)dx. The dual-space framework can then be used
to derive the following result:

7

Theorem 3. Minimization of (22) with λ = 4 is equivalent to solving
v ;γ(cid:23)0 Z exp [−π(y , x, v)] Yi
N (x; 0, γi )ϕ(γi )dxi
max
and then computing x(I I ) by plugging the resulting γ into (6).

(24)

Thus we may conclude that (22) provides a principled approximation to (5) when a Bernoulli like-
lihood function is used for classi ﬁcation purposes. In empi rical tests on benchmark data sets (see
supplementary material) using f (γ ) = 0, it performs nearly identically to the original RVM (which
also implicitly assumes f (γ ) = 0), but nonetheless provides a more solid theoretical justi ﬁ cation
for Type II classi ﬁers because of the underlying similariti es and identical generative model. But
while the RVM and its attendant approximations are difﬁcult
to analyze, (22) is relatively transpar-
ent. Additionally, for other sparse priors, or equivalently other selections for f , we can still perform
optimization and analyze cost functions without any conjugacy requirements on the implicit p(x).
Theorem 4. If log |Σy | + Pi f (γi ) is a concave, non-decreasing function of γ (as will be the case
if f is concave and non-decreasing), then every local optimum of (24) is achieved at a solution with
at most n nonzero elements in γ and therefore x(I I ) . In contrast, if − log p(x) is convex, then (24)
can be globally solved via a convex program.

Despite the practical success of the RVM and related Bayesian techniques, and empirical evidence of
sparse solutions, there is currently no proof that the standard variants of these classi ﬁcation methods
will always produce exactly sparse estimates. Thus Theorem 4 provides some analytical validation
of these types of classi ﬁers.

Finally, if we take (22) as our starting point, we may naturally consider modi ﬁcations tailored to
speci ﬁc sparse classi ﬁcation tasks (that may or may not reta
in an explicit connection with the original
Type II probabilistic model). For example, suppose we would like to obtain a maximally sparse
classi ﬁer, where regularization is provided by a kxk0 penalty. Direct optimization is combinatorial
because of what we call the global zero attraction property: Whenever any individual coefﬁcient xi
goes to zero, we are necessarily at a local minimum with respect to this coefﬁcient because of the
inﬁnite slope (discontinuity) of the ℓ0 norm at zero. However, (22) can be modi ﬁed to approximate
the ℓ0 without this property as follows.

ˆx, ˆγ = arg min
x;γ(cid:23)0

Theorem 5. Consider the Type II-inspired minimization problem
x2
ψ (y , x) + α1 Xi
i
+ log (cid:12)(cid:12)α2 I + ΦΓΦT (cid:12)(cid:12)
γi
which is equivalent to (22) with f (γ ) = 0 when α1 = α2 = λ. For some α1 and α2 sufﬁciently
small (but not necessarily equal), the support3 of ˆx will match the support of arg minx ψ (y , x) +
λkxk0 . Moreover, (25) does not satisfy the global zero attraction property.
Thus Type II affords the possibility of mimicking the ℓ0 norm in the presence of generalized like-
lihoods but with the advantageous potential for drastically fewer local minima. This is a direction
for future research. Additionally, while here we have focused our attention on classi ﬁcation via
logistic regression, these ideas can presumably be extended to other likelihood functions provided
certain conditions are met. To the best of our knowledge, while already demonstrably successful
in an empirical setting, Type II classi ﬁers and other relate d Bayesian generalized likelihood models
have never been analyzed in the context of sparse estimation as we have done in this section.

(25)

6 Conclusion

The dual-space view of sparse linear or generalized linear models naturally allows us to transition
x-space ideas originally developed for Type I and apply them to Type II, and conversely, apply γ -
space techniques from Type II to Type I. The resulting symmetry promotes a mutual understanding
of both methodologies and helps ensure that they are not underutilized.

3 Support refers to the index set of the nonzero elements.

8

References

[1] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[2] E. Cand `es, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information,”
IEEE Trans. Information Theory, vol. 52, no.
2, pp. 489–509, Feb. 2006.
[3] E. Cand `es, M. Wakin, and S. Boyd, “Enhancing sparsity by reweighted ℓ1 minimization,”
Fourier Anal. Appl., vol. 14, no. 5, pp. 877–905, 2008.
[4] R. Chartrand and W. Yin, “Iteratively reweighted algori thms for compressive sensing,” Proc.
Int. Conf. Accoustics, Speech, and Signal Proc., 2008.
[5] D.L. Donoho and M. Elad,
“Optimally sparse representati on in general (nonorthogonal) dic-
tionaries via ℓ1 minimization,” Proc. National Academy of Sciences, vol. 100, no. 5, pp. 2197–
2202, March 2003.
[6] M.A.T. Figueiredo, “Adaptive sparseness using Jeffrey s prior,” Advances in Neural Informa-
tion Processing Systems 14, pp. 697–704, 2002.
[7] C. F ´evotte and S.J. Godsill, “Blind separation of sparse source s using Jeffreys inverse prior and
the EM algorithm,” Proc. 6th Int. Conf. Independent Component Analysis and Blind Source
Separation, Mar. 2006.
[8] M. Girolami,
“A variational method for learning sparse a nd overcomplete representations,”
Neural Computation, vol. 13, no. 11, pp. 2517–2532, 2001.
[9] I.F. Gorodnitsky and B.D. Rao,
“Sparse signal reconstru ction from limited data using FO-
CUSS: A re-weighted minimum norm algorithm,”
IEEE Transactions on Signal Processing,
vol. 45, no. 3, pp. 600–616, March 1997.
[10] S. Ji, D. Dunson, and L. Carin,
“Multi-task compressive sensing,”
cessing, vol. 57, no. 1, pp. 92–106, Jan 2009.
[11] K. Kreutz-Delgado, J. F. Murray, B.D. Rao, K. Engan, T.-W. Lee, and T.J. Sejnowski,
“Dic-
tionary learning algorithms for sparse representation,” Neural Computation, vol. 15, no. 2, pp.
349–396, February 2003.
[12] D.J.C. MacKay,
“Bayesian interpolation,”
1992.
[13] J. Mattout, C. Phillips, W.D. Penny, M.D. Rugg, and K.J. Friston, “MEG source localization
under multiple constraints: An extended Bayesian framework,” NeuroImage, vol. 30, pp. 753–
767, 2006.
[14] R.M. Neal, Bayesian Learning for Neural Networks, Springer-Verlag, New York, 1996.
[15] J.A. Palmer, D.P. Wipf, K. Kreutz-Delgado, and B.D. Rao,
“Variational EM algorithms for
non-Gaussian latent variable models,” Advances in Neural Information Processing Systems
18, pp. 1059–1066, 2006.
[16] B.D. Rao, K. Engan, S. F. Cotter, J. Palmer, and K. Kreutz-Delgado, “Subset selection in noise
based on diversity measure minimization,”
IEEE Trans. Signal Processing, vol. 51, no. 3, pp.
760–770, March 2003.
[17] M. Seeger and H. Nickisch,
“Large scale Bayesian infere nce and experimental design for
sparse linear models,” SIAM J. Imaging Sciences, vol. 4, no. 1, pp. 166–199, 2011.
[18] R. Tibshirani,
“Regression shrinkage and selection vi a the Lasso,”
Journal of the Royal
Statistical Society, vol. 58, no. 1, pp. 267–288, 1996.
[19] M.E. Tipping,
“Sparse Bayesian learning and the releva nce vector machine,”
Machine Learning Research, vol. 1, pp. 211–244, 2001.
[20] M.E. Tipping and A.C. Faul,
“Fast marginal likelihood m aximisation for sparse Bayesian
models,” Ninth Int. Workshop. Arti ﬁcial Intelligence and Statistic s, Jan. 2003.
[21] D.P. Wipf “Sparse estimation with structured dictiona ries,” Advances in Nerual Information
Processing 24, 2011.
[22] D.P. Wipf, B.D. Rao, and S. Nagarajan,
“Latent variable Bayesian models for promoting
IEEE Trans. Information Theory, vol. 57, no. 9, Sept. 2011.
sparsity,”

Neural Computation, vol. 4, no. 3, pp. 415–447,

IEEE Trans. Signal Pro-

J.

Journal of

9


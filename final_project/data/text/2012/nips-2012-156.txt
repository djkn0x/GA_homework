Symbolic Dynamic Programming for Continuous
State and Observation POMDPs

Zahra Zamani
ANU & NICTA
Canberra, Australia
zahra.zamani@anu.edu.au

Scott Sanner
NICTA & ANU
Canberra, Australia
scott.sanner@nicta.com.au

Pascal Poupart
U. of Waterloo
Waterloo, Canada
ppoupart@uwaterloo.ca

Kristian Kersting
Fraunhofer IAIS & U. of Bonn
Bonn, Germany
kristian.kersting@iais.fraunhofer.de

Abstract

Point-based value iteration (PBVI) methods have proven extremely effective for
ﬁnding (approximately) optimal dynamic programming solutions to partially-
observable Markov decision processes (POMDPs) when a set of initial belief
states is known. However, no PBVI work has provided exact point-based back-
ups for both continuous state and observation spaces, which we tackle in this
paper. Our key insight is that while there may be an inﬁnite number of observa-
tions, there are only a ﬁnite number of continuous observation partitionings that
are relevant for optimal decision-making when a ﬁnite, ﬁxed set of reachable be-
lief states is considered. To this end, we make two important contributions: (1) we
show how previous exact symbolic dynamic programming solutions for continu-
ous state MDPs can be generalized to continuous state POMDPs with discrete ob-
servations, and (2) we show how recently developed symbolic integration methods
allow this solution to be extended to PBVI for continuous state and observation
POMDPs with potentially correlated, multivariate continuous observation spaces.

1

Introduction

Partially-observable Markov decision processes (POMDPs) are a powerful modeling formalism for
real-world sequential decision-making problems [3]. In recent years, point-based value iteration
methods (PBVI) [5, 10, 11, 7] have proved extremely successful at scaling (approximately) optimal
POMDP solutions to large state spaces when a set of initial belief states is known.
While PBVI has been extended to both continuous state and continuous observation spaces, no prior
work has tackled both jointly without sampling. [6] provides exact point-based backups for contin-
uous state and discrete observation problems (with approximate sample-based extensions to contin-
uous actions and observations), while [2] provides exact point-based backups (PBBs) for discrete
state and continuous observation problems (where multivariate observations must be conditionally
independent). While restricted to discrete states, [2] provides an important insight that we exploit in
this work: only a ﬁnite number of partitionings of the observation space are required to distinguish
between the optimal conditional policy over a ﬁnite set of belief states.
We propose two major contributions: First, we extend symbolic dynamic programming for con-
tinuous state MDPs [9] to POMDPs with discrete observations, arbitrary continuous reward and
transitions with discrete noise (i.e., a ﬁnite mixture of deterministic transitions). Second, we extend
this symbolic dynamic programming algorithm to PBVI and the case of continuous observations

1

(while restricting transition dynamics to be piecewise linear with discrete noise, rewards to be piece-
wise constant, and observation probabilities and beliefs to be uniform) by building on [2] to derive
relevant observation partitions for potentially correlated, multivariate continuous observations.

2 Hybrid POMDP Model

(1)

(2)

(cid:48)
(cid:48)
s ,d
s , a)

(cid:48)
(cid:48)
s ,d
s , a).

T : p(x

si |xs ,ds , a)
(cid:48)

p(d

Z : p(xo ,do |x

(cid:48)
(cid:48)
s ,d
s , a) =

sj |xs ,ds , d
(cid:48)
p(x

(cid:48)
s , a).

s |xs ,ds , a) =
(cid:48)
(cid:48)
s ,d

A hybrid (discrete and continuous) partially observable MDP (H-POMDP)
is a tuple
(cid:104)S , A, O , T , R, Z , γ , h(cid:105). States S are given by vector (ds , xs ) = (ds1 , . . . , dsn , xs1 , . . . , xsm )
where each dsi ∈ {0, 1} (1 ≤ i ≤ n) is boolean and each xsj ∈ R (1 ≤ j ≤ m) is continuous. We
assume a ﬁnite, discrete action space A = {a1 , . . . , ar }. Observations O are given by the vector
(do , xo ) = (do1 , . . . , dop , xo1 , . . . , xoq ) where each doi ∈ {0, 1} (1 ≤ i ≤ p) is boolean and each
xoj ∈ R (1 ≤ j ≤ q ) is continuous.
Three functions are required for modeling H-POMDPs: (1) T : S × A × S → [0, 1] a Markovian
transition model deﬁned as the probability of the next state given the action and previous state; (2)
R : S × A → R a reward function which returns the immediate reward of taking an action in
some state; and (3) an observation function deﬁned as Z : S × A × O → [0, 1] which gives the
probability of an observation given the outcome of a state after executing an action. A discount
factor γ , 0 ≤ γ ≤ 1 is used to discount rewards t time steps into the future by γ t .
We use a dynamic Bayes net (DBN)1 to compactly represent the transition model T over the factored
state variables and we use a two-layer Bayes net to represent the observation model Z :
m(cid:89)
n(cid:89)
q(cid:89)
p(cid:89)
j=1
i=1
p(doi |x
p(xoj |x
j=1
i=1
|xs ,ds ,a) and p(doi|x(cid:48)
Probabilities over discrete variables p(d(cid:48)
s ,d(cid:48)
s ,a) may condition on both dis-
si
crete variables and (nonlinear) inequalities of continuous variables; this is further restricted to
linear inequalities in the case of continuous observations. Transitions over continuous variables
|xs ,ds ,d(cid:48)
p(x(cid:48)
s ,a) must be deterministic (but arbitrary nonlinear) piecewise functions; in the case of
sj
continuous observations they are further restricted to be piecewise linear; this permits discrete noise
in the continuous transitions since they may condition on stochastically sampled discrete next-state
s . Observation probabilities over continuous variables p(xoj|x(cid:48)
variables d(cid:48)
s ,d(cid:48)
s ,a) only occur in the
case of continuous observations and are required to be piecewise constant (a mixture of uniform dis-
tributions); the same restriction holds for belief state representations. The reward R(d, x, a) may be
an arbitrary (nonlinear) piecewise function in the case of deterministic observations and a piecewise
constant function in the case of continuous observations. We now provide concrete examples.
Example (Power Plant) [1] The steam generation system of a power plant evaporates feed-water
under restricted pressure and temperature conditions to turn a steam turbine. A reward is obtained
when electricity is generated from the turbine and the steam pressure and temperature are within safe
ranges. Mixing water and steam makes the respective pressure and temperature observations po ∈ R
and to ∈ R on the underlying state ps ∈ R and ts ∈ R highly uncertain. Actions A = {open , close }
control temperature and pressure by means of a pressure valve.
We initially present two H-POMDP variants labeled 1D-Power Plant using a single temperature
state variable ts . The transition and reward are common to both — temperature increments (decre-
(a = open )
ments) with a closed (opened) valve, a large negative reward is given for a closed valve with ts
(cid:35)
(cid:40)
(cid:34)
exceeding critical threshold 15, and positive reward is given for a safe, electricity-producing state:
: −1
: ts − 5
: −1000
(a = close ) ∧ (ts > 15)
t
(a = close ) ∧ ¬(ts > 15)
: ts + 7
: 100
Next we introduce the Discrete Obs. 1D-Power Plant variant where we deﬁne an observation space
with a single discrete binary variable o ∈ O = {high , low }:
1We disallow general synchronic arcs for simplicity of exposition but note their inclusion only places re-
strictions on the variable elimination ordering used during the dynamic programming backup operation.

(a = open )
(a = close )

s |ts , a) = δ
(cid:48)

p(t

s −
(cid:48)

R(ts , a) =

(3)

2

p(o = high |t

(cid:48)
s , a = open ) =

Figure 1: (left) Example conditional plan β h for discrete observations; (right) example α-function for β h over
state b ∈ {0, 1}, x ∈ R in decision diagram form: the true (1) branch is solid, the false (0) branch is dashed.
(cid:40)
(cid:40)

s ≤ 15 : 0.9
s ≤ 15 : 0.7
t(cid:48)
t(cid:48)
p(o = high |t
t(cid:48)
t(cid:48)
s > 15 : 0.1
s > 15 : 0.3
Finally we introduce the Cont. Obs. 1D-Power Plant variant where we deﬁne an observation space
(cid:40)
with a single continuous variable to uniformly distributed on an interval of 10 units centered at t(cid:48)
s .
s − 5) ∧ (to < t(cid:48)
(to > t(cid:48)
p(to |t
s − 5, t
(cid:48)
(cid:48)
(cid:48)
: 0.1
s + 5)
(to ≤ t(cid:48)
s − 5) ∨ (to ≥ t(cid:48)
s , a = open ) = U (to ; t
s + 5) =
s + 5)
: 0
While simple, we note no prior method could perform exact point-based backups for either problem.
3 Value Iteration for Hybrid POMDPs

(cid:48)
s , a = close ) =

(5)

(4)

γ t · rt

V h
π (b) = Eπ

In an H-POMDP, the agent does not directly observe the states and thus must maintain a belief state
b(xs ,ds ) = p(xs ,ds ). For a given belief state b = b(xs ,ds ), a POMDP policy π can be represented
by a tree corresponding to a conditional plan β . An h-step conditional plan β h can be deﬁned
recursively in terms of (h − 1)-step conditional plans as shown in Fig. 1 (left). Our goal is to ﬁnd a
(cid:12)(cid:12)(cid:12)b0 = b
(cid:104)(cid:88)h
(cid:105)
policy π that maximizes the value function, deﬁned as the sum of expected discounted rewards over
horizon h starting from initial belief state b:
t=0
where rt is the reward obtained at time t and b0 is the belief state at t = 0. For ﬁnite h and belief
state b, the optimal policy π is given by an h-step conditional plan β h . For h = ∞, the optimal
discounted (γ < 1) value can be approximated arbitrarily closely by a sufﬁciently large h [3].
Even when the state is continuous (but the actions and observations are discrete), the optimal
POMDP value function for ﬁnite horizon h is a piecewise linear and convex function of the be-
(cid:90)
(cid:88)
lief state b [6], hence V h is given by a maximum over a ﬁnite set of “α-functions” αh
i :
i , b(cid:105) = max
(cid:104)αh
i (xs ,ds ) · b(xs ,ds ) dxs
V h (b) = max
αh
i ∈Γh
i ∈Γh
αh
αh
xs
ds
Later on when we tackle continuous state and observations, we note that we will dynamically derive
an optimal, ﬁnite partitioning of the observation space for a given belief state and hence reduce the
continuous observation problem back to a discrete observation problem at every horizon.
The Γh in this optimal h-stage-to-go value function can be computed via Monahan’s dynamic pro-
1 }, and assuming
1 = 0, Γ0 = {α0
(cid:90)
gramming approach to value iteration (VI) [4]. Initializing α0
(cid:88)
discrete observations o ∈ Oh , Γh is obtained from Γh−1 as follows:2
s |xs ,ds , a)αh−1
p(o|x
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:110)
(cid:111)
gh
s , a)p(x
s ,d
s ,d
a,o,j (xs ,ds ) =
(x
s ,d
j
xs(cid:48)
ds(cid:48)
a = R(xs ,ds , a) + γ(cid:1)o∈O
(cid:91)
gh
Γh
a,o,j (xs ,ds )
j
Γh
Γh =
(10)
a
a
2The (cid:1) of sets is deﬁned as (cid:1)j∈{1,...,n}Sj = S1(cid:1) · · · (cid:1)Sn where the pairwise cross-sum P (cid:1)Q =
{p + q|p ∈ P , q ∈ Q}.

s )dxs(cid:48) ; ∀αh−1
(cid:48)
j

∈ Γh−1

(9)

(8)

(6)

(7)

3

121 + (3 * x)(1 * x) >= 50(1 * x) <= 39234 + (1.5 * x)197 + (2 * x)250b (1 * x) >= 150(1 * x) <= 139else

i and ∀xi : xi → x(cid:48)
) // ∀di : di → d(cid:48)
i

// Discrete observations and model already known
Oh
i := {do }; p(Oh
i |x(cid:48)
s ,d(cid:48)
s , a) := see Eq (2)
foreach o ∈ Oh
i do
∈ Γh−1
foreach αh−1
P BV I do
j
αh−1
:= Prime(αh−1
j
j
a,o,j := see Eq (8)
gh

Algorithm 1: PBVI(H-POMDP, H , B = {bi }) −→ (cid:104)V h (cid:105)
begin
1
P BV I = {α0
1 }
V 0 := 0, h := 0, Γ0
2
while h < H do
3
h := h + 1, Γh := ∅, Γh
P BV I := ∅
4
foreach bi ∈ B do
5
foreach a ∈ A do
6
a := ∅
Γh
7
if (continuous observations: q > 0) then
8
// Derive relevant observation partitions Oh
i for belief bi
9
(cid:104)Oh
i , p(Oh
i |x(cid:48)
s , a)(cid:105) := GenRelObs(Γh−1
s ,d(cid:48)
P BV I , a, bi)
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

// Retain only α-functions optimal at each belief point
foreach bi ∈ B do
bi := arg maxαj ∈Γh αj · bi
αh
P BV I ∪ αh
P BV I := Γh
Γh
bi

a := see Eq (9)
Γh
Γh := Γh ∪ Γh
a

// Terminate if early convergence
P BV I = Γh−1
P BV I then
if Γh
break

return ΓP BV I

end

Point-based value iteration (PBVI) [5, 11] computes the value function only for a set of belief
states {bi } where bi := p(xs ,ds ). The idea is straightforward and the main modiﬁcation needed to
Monahan’s VI approach in Algorithm 1 is the loop from lines 23–25 where only α-functions optimal
at some belief state are retained for subsequent iterations. In the case of continuous observation
variables (q > 0), we will need to derive a relevant set of observations on line 10, a key contribution
of this work as described in Section 4.3. Otherwise if the observations are only discrete (q = 0),
then a ﬁnite set of observations is already known and the observation function as given in Eq (2).
We remark that Algorithm 1 is a generic framework that can be used for both PBVI and other
variants of approximate VI. If used for PBVI, an efﬁcient direct backup computation of the optimal
α-function for belief state bi should be used in line 17 that is linear in the number of observations [5,
11] and which obviates the need for lines 23–25. However, for an alternate version of approximate
value iteration that will often produce more accurate values for belief states other than those in B ,
one may instead retain the full cross-sum backup of line 17, but omit lines 23–25 — this yields
an approximate VI approach (using discretized observations relevant only to a chosen set of belief
states B if continuous observations are present) that is not restricted to alpha-functions only optimal
at B , hence allowing greater ﬂexibility in approximating the value function over all belief states.
Whereas PBVI is optimal if all reachable belief states within horizon H are enumerated in B , in
the H-POMDP setting, the generation of continuous observations will most often lead to an inﬁnite
number of reachable belief states, even with ﬁnite horizon — this makes it quite difﬁcult to provide
optimality guarantees in the general case of PBVI for continuous observation settings. Nonethe-
less, PBVI has been quite successful in practice without exhaustive enumeration of all reachable
beliefs [5, 10, 11, 7], which motivates our use of PBVI in this work.

4

4 Symbolic Dynamic Programming

f =

In this section we take a symbolic dynamic programming (SDP) approach to implementing VI and
PBVI as deﬁned in the last section. To do this, we need only show that all required operations can
be computed efﬁciently and in closed-form, which we do next, building on SDP for MDPs [9].
4.1 Case Representation and Extended ADDs

The previous Power Plant examples represented all functions in case form, generally deﬁned as
f1
φ1 :
...
...
φk :
fk
and this is the form we use to represent all functions in an H-POMDP. The φi are disjoint logical
formulae deﬁned over xs ,ds and/or xo ,do with logical (∧, ∨, ¬) combinations of boolean variables
and inequalities (≥, >, ≤, <) over continuous variables. For discrete observation H-POMDPs, the
fi and inequalities may use any function (e.g., sin(x1 ) > log(x2 ) · x3 ); for continuous observations,
they are restricted to linear inequalities and linear or piecewise constant fi as described in Section 2.
For unary operations such as scalar multiplication c · f (for some constant c ∈ R) or negation
−f on case statements is simply to apply the operation on each case partition fi (1 ≤ i ≤ k). A
binary operation on two case statements, takes the cross-product of the logical partitions of each

case statement and performs the corresponding operation on the resulting paired partitions. The
(cid:40)
(cid:40)
cross-sum ⊕ of two cases is deﬁned as the following:
φ1 ∧ ψ1 :
f1 + g1
φ1 ∧ ψ2 :
f1 + g2
ψ1 :
φ1 :
φ2 ∧ ψ1 :
f2 + g1
φ2 :
ψ2 :
φ2 ∧ ψ2 :
f2 + g2
Likewise (cid:9) and ⊗ are deﬁned by subtracting or multiplying partition values. Inconsistent partitions

can be discarded when they are irrelevant to the function value. A symbolic case maximization is
(cid:32) (cid:40)
(cid:40)
(cid:33)
φ1 ∧ ψ1 ∧ f1 > g1 : f1
deﬁned as below:
φ1 ∧ ψ1 ∧ f1 ≤ g1 : g1
φ1 ∧ ψ2 ∧ f1 > g2 : f1
casemax
φ1 ∧ ψ2 ∧ f1 ≤ g2 : g2
...
...
The following SDP operations on case statements require more detail than can be provided here,
hence we refer the reader to the relevant literature:
• Substitution f σ : Takes a set σ of variables and their substitutions (which may be case
• Integration (cid:82)
statements themselves), and carries out all variable substitutions [9].
f dx1 : There are two forms: If x1 is involved in a δ -function (cf.
the
x1
transition in Eq (3)) then the integral is equivalent to a symbolic substitution and can be
applied to any case statement (cf. [9]). Otherwise, if f is in linearly constrained polynomial
case form, then the approach of [8] can be applied to yield a result in the same form.

ψ1 : g1
ψ2 : g2

φ1 : f1
φ2 : f2

f1
f2

g1
g2

=

⊕

,

=

Case operations yield a combinatorial explosion in size if na¨ıvely implemented, hence we use the
data structure of the extended algebraic decision diagram (XADD) [9] as shown in Figure 1 (right)
to compactly represent case statements and efﬁciently support the above case operations with them.

4.2 VI for Hybrid State and Discrete Observations
For H-POMDPs with only discrete observations o ∈ O and observation function p(o|x(cid:48)
s ,d(cid:48)
s , a) as in
(cid:35)
(cid:33)
(cid:33)
(cid:34)
(cid:32) n(cid:79)
(cid:32) m(cid:79)
the form of Eq (4), we introduce a symbolic version of Monahan’s VI algorithm. In brief, we note
(cid:90)
(cid:77)
that all VI operations needed in Section 3 apply directly to H-POMDPs, e.g., rewriting Eq (8):
⊗αh−1
sj|xs ,ds , d
⊗
si|xs ,ds ,a)
s ,a)⊗
p(o|x
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
gh
dxs(cid:48)
s )
s ,d
s ,a)
p(x
a,o,j (xs ,ds ) =
s ,d
j
xs(cid:48)
ds(cid:48)
j=1
i=1

p(d

(x

(11)

5

Algorithm 2: GenRelObs(Γh−1 , a, bi ) −→ (cid:104)Oh , p(Oh |x(cid:48)
s , a)(cid:105)
s ,d(cid:48)
begin
1
j (xs ,ds , xo ,do ) := (cid:82)
s ) ∈ Γh−1 and a ∈ A do
(cid:76)
foreach αj (x(cid:48)
s ,d(cid:48)
2
// Perform exact 1-step DP backup of α-functions at horizon h − 1
3
s |xs ,ds , a) ⊗ αj (x(cid:48)
s , a) ⊗ p(x(cid:48)
p(xo ,do |x(cid:48)
s ) dx(cid:48)
s ,d(cid:48)
s ,d(cid:48)
s ,d(cid:48)
αa
d(cid:48)
x(cid:48)
4
s
j (xo ,do ) := (cid:82)
(cid:76)
s
s
foreach αa
j (xs ,ds , xo ,do ) do
5
// Generate value of each α-vector at belief point bi (xs ,ds ) as a function of observations
6
bi (xs ,ds ) ⊗ αa
δa
j (xs ,ds , xo ,do ) dxs
7
ds
xs
// Using casemax, generate observation partitions relevant to each policy – see text for details
8
Oh := extract-partition-constraints[casemax(δa1
1 (xo ,do ), δa2
1 (xo ,do ), . . . , δar
j (xo ,do ))]
9
foreach ok ∈ Oh do
s , a) := (cid:82)
(cid:76)
10
// Let φok be the partition constraints for observation ok ∈ Oh
11
p(Oh = ok |x(cid:48)
p(xo ,do |x(cid:48)
s ,d(cid:48)
s ,d(cid:48)
s , a)I[φok ]dxo
12
do
xo
return (cid:104)Oh , p(Oh |x(cid:48)
s , a)(cid:105)
s ,d(cid:48)
13
14

end

Figure 2: (left) Beliefs b1 , b2 for Cont. 1D-Power Plant; (right) derived observation partitions for b2 at h = 2.
deﬁned with Dirac δ ’s (e.g., Eq 3) as described in Section 2, the integral (cid:82)
|xs ,ds ,d(cid:48)
Crucially we note since the continuous transition cpfs p(x(cid:48)
s ,a) are deterministic and hence
sj
xs(cid:48) can always be computed
in closed case form as discussed in Section 4.1. In short, nothing additional is required for PBVI on
H-POMDPs in this case — the key insight is simply that α-functions are now represented by case
statements and can “grow” with the horizon as they partition the state space more and more ﬁnely.

4.3 PBVI for Hybrid State and Hybrid Observations

In general, it would be impossible to apply standard VI to H-POMDPs with continuous observations
since the number of observations is inﬁnite. However, building on ideas in [2], in the case of PBVI,
it is possible to derive a ﬁnite set of continuous observation partitions that permit exact point-based
backups at a belief point. This additional operation (GenRelObs) appears on line 10 of PBVI in
Algorithm 1 in the case of continuous observations and is formally deﬁned in Algorithm 2.
To demonstrate the generation of relevant continuous observation partitions, we use the second
iteration of the Cont. Obs. 1D-Power Plant along with two belief points represented as uniform
distributions: b1 : U (ts ; 2, 6) and b2 : U (ts ; 6, 11) as shown in Figure 2 (left). Letting h = 2, we will
assume simply for expository purposes that |Γ1 | = 1 (i.e., it contains only one α-function) and that
in lines 2–4 of Algorithm 2 we have computed the following two α-functions for a ∈ {open , close }:
(ts < 15) ∧ (ts − 10 < to < ts ) : 10
(cid:40)
(ts ≥ 15) ∧ (ts − 10 < to < ts ) : −100
¬(ts − 10 < to < ts )
: 0
We now need the α-vectors as a function of the observation space for a particular belief state, thus
next we marginalize out xs ,ds in lines 5–7. The resulting δ -functions are shown as follows where
for brevity from this point forward, 0 partitions are suppressed in the cases:

(ts − 10 < to < ts )
: 0.1
¬(ts − 10 < to < ts ) : 0

(ts , to ) =

αclose
1

(ts , to ) =

αopen
1

6

tsP(t  )0.250.226111b2bs18t57.5-72.5ocloseopenP(o ) =0.0127P(o ) =0.983210414815-750.15.1(t  )o(to ) =

(to )

=

(to ) =

δ close
1

δ close
1

δopen
1

casemax

(to ), δopen
1

: 0.025to − 0.45
: −0.1
: −0.025to − 0.1


(14 < to < 18)
: 25to − 450
(15 < to < 18)
: −2.5to − 37.5
(14 < to < 15)
: −72.5
(8 < to < 14)
(8 < to < 14)
: −25to + 127.5
(4 < to < 8)
(5 < to < 8)
: 2.5to − 10
(4 < to < 5)
(to ) and δopen
(to ) are drawn graphically in Figure 2 (right). These observation-
Both δclose
1
1
dependent δ ’s divide the observation space into regions which can yield the optimal policy according
to the belief state b2 . Following [2], we need to ﬁnd the optimal boundaries or partitions of the ob-
servation space; in their work, numerical solutions are proposed to ﬁnd these boundaries in one
dimension (multiple observations are handled through an independence assumption). Instead, here
we leverage the symbolic power of the casemax operator deﬁned in Section 4.1 to ﬁnd all the parti-

tions where each potentially correlated, multivariate observation δ is optimal. For the two δ ’s above,
the following partitions of the observation space are derived by the casemax operator in line 9:
(cid:17)
(cid:16)
o1 : (14 < to ≤ 18)
: 0.025to − 0.45
o1 : (8 < to ≤ 14)
: −0.1
: −0.025to − 0.1
o1 : (5.1 < to ≤ 8)
o2 : (5 < to ≤ 5.1)
: −25to + 127.5
o2 : (4 < to ≤ 5)
: 2.5to − 10
Here we have labeled with o1 the observations where δclose
is maximal and with o2 the observations
1
where δopen
is maximal. What we really care about though are just the constraints identifying o1
1
and o2 and this is the task of extract-partition-constraints in line 9. This would associate with o1
the partition constraint φo1 ≡ (5.1 < to ≤ 8) ∨ (8 < to ≤ 14) ∨ (14 < to ≤ 18) and with o2 the
partition constraint φo2 ≡ (4 < to ≤ 5) ∨ (5 < to ≤ 5.1) — taking into account the 0 partitions
and the 1D nature of this example, we can further simplify φo1 ≡ (to > 5.1) and φo2 ≡ (to ≤ 5.1).
Given these relevant observation partitons, our ﬁnal task in lines 10-12 is to compute the probabil-
ities of each observation partition φok . This is simply done by marginalizing over the observation
function p(Oh |x(cid:48)
s ,d(cid:48)
s , a) within each region deﬁned by φok (achieved by multiplying by an indicator
function I[φok ] over these constraints). To better understand what is computed here, we can compute
(cid:90)
(cid:90)
(cid:77)
(cid:77)
the probability p(ok |bi , a) of each observation for a particular belief, calculated as follows:
p(ok |bi , a) :=
p(ok |x
s , a)⊗p(x
s |xs ,ds , a)⊗αj (x
s )⊗bi (xs ,ds ) dx
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
(cid:48)
s dxs (12)
s ,d
s ,d
s ,d
x(cid:48)
d(cid:48)
xs
ds
s
s
Speciﬁcally, for b2 , we obtain p(o1 |b2 , a = close ) = 0.0127 and p(o2 |b2 , a = close ) = 0.933 as
shown in Figure 2 (right).
In summary, in this section we have shown how we can extend the exact dynamic programming
algorithm for the continuous state, discrete observation POMDP setting from Section 4.2 to com-
pute exact 1-step point-based backups in the continuous observation setting; this was accomplished
through the crucial insight that despite the inﬁnite number of observations, using Algorithm 2 we
can symbolically derive a set of relevant observations for each belief point that distinguish the op-
timal policy and hence value as graphically illustrated in Figure 2 (right). Next we present some
empirical results for 1- and 2-dimensional continuous state and observation spaces.

5 Empirical Results

We evaluated our continuous POMDP solution using XADDs on the 1D-Power Plant example and
another variant of this problem with two variables, described below.3
2D-Power Plant: We consider the more complex model of the power plant similar to [1] where the
pressure inside the water tank must be controlled to avoid mixing water into the steam (leading to
explosion of the tank). We model an observable pressure reading po as a function of the underlying
(cid:34)
(cid:40)
(cid:35)
pressure state ps . Again we have two actions for opening and closing a pressure valve. The close
s − (ts + 10)(cid:3)
s |ts , a = close ) = δ (cid:2)t
action has transition
s |ps , a = close ) = δ
(cid:48)
(cid:48)
(cid:48)
3Full problem speciﬁcations and Java code to reproduce these experiments are available online in Google
Code: http://code.google.com/p/cpomdp .

(p + 10 > 20)
¬(p + 10 > 20)

: 20
: ps + 10

s −
(cid:48)

p(p

p(t

p

7

R(ts , ps , a = close ) =

Figure 3: (left) time vs. horizon, and (right) space (total # XADD nodes in α-functions) vs. horizon.

and yields high reward for staying within the safe temperature and pressure range:
(5 ≤ ps ≤ 15) ∧ (95 ≤ ts ≤ 105)
: 50
: −1
(5 ≤ ps ≤ 15) ∧ (ts ≤ 95)
(ps ≥ 15)
: −5
: −3
else
Alternately, for the open action, the transition functions reduce the temperature by 5 units and the
pressure by 10 units as long as the pressure stays above zero. For the open reward function, we
assume that there is always a small constant penalty (-1) since no electricity is produced.
(cid:40)
(cid:40)
Observations are distributed uniformly within a region depending on their underlying state:
p(to |t
p(po |p
(cid:48)
(cid:48)
: 0.1
: 0.04
(ts + 80 < to < ts + 105)
(ps < po < ps + 10)
¬(ts + 80 < to < ts + 105)
¬(ps < po < ps + 10)
s ) =
s ) =
: 0
: 0
Finally for PBVI, we deﬁne two uniform beliefs as follows: b1 : U [ts ; 90, 100] ∗ U [ps ; 0, 10] and
b2 : U [ts ; 90, 130] ∗ U [ps ; 10, 30]
In Figure 3, a time and space analysis of the two versions of Power Plant have been performed for
up to horizon h = 6. This experimental evaluation relies on one additional approximation over the
PBVI approach of Algorithm 1 in that it substitutes p(Oh |b, a) in place of p(Oh |x(cid:48)
s ,d(cid:48)
s , a) — while
this yields correct observation probabilities for a point-based backup at a particular belief state b,
the resulting α-functions represent an approximation for other belief states. In general, the PBVI
framework in this paper does not require this approximation, although when appropriate, using it
should increase computational efﬁciency.
Figure 3 shows that the computation time required per iteration generally increases since more com-
plex α-functions lead to a larger number of observation partitions and thus a more expensive backup
operation. While an order of magnitude more time is required to double the number of state and
observation variables, one can see that the PBVI approach leads to a fairly constant amount of com-
putation time per horizon, which indicates that long horizons should be computable for any problem
for which at least one horizon can be computed in an acceptable amount of time.

6 Conclusion

We presented the ﬁrst exact symbolic operations for PBVI in an expressive subset of H-POMDPs
with continuous state and observations. Unlike related work that has extended to the continuous
state and observation setting [6], we do not approach the problem by sampling. Rather, follow-
ing [2], the key contribution of this work was to deﬁne a discrete set of observation partitions on
the multivariate continuous observation space via symbolic maximization techniques and derive the
related probabilities using symbolic integration. An important avenue for future work is to extend
these techniques to the case of continuous state, observation, and action H-POMDPs.

Acknowledgments

NICTA is funded by the Australian Government as represented by the Department of Broadband, Communi-
cations and the Digital Economy and the ARC through the ICT Centre of Excellence program. This work was
supported by the Fraunhofer ATTRACT fellowship STREAM and by the EC, FP7-248258-First-MM.

8

123456102103104105HorizonTime(ms)Power Plant  1 state & 1 observ var2 state & 2 observ vars123456010203040506070HorizonNumber of NodesPower Plant  1 state & 1 observ var2 state & 2 observ varsReferences
[1] Mario Agueda and Pablo Ibarguengoytia. An architecture for planning in uncertain domains.
In Proceedings of the ICTAI 2002 Conference, Dallas,Texas, 2002.
[2] Jesse Hoey and Pascal Poupart. Solving pomdps with continuous or large discrete observation
spaces. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI),
Edinburgh, Scotland, 2005.
[3] Leslie P. Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.
[4] G. E. Monahan. Survey of partially observable markov decision processes: Theory, models,
and algorithms. Management Science, 28(1):1–16, 1982.
[5] Joelle Pineau, Geoffrey J. Gordon, and Sebastian Thrun. Anytime point-based approximations
for large pomdps. J. Artif. Intell. Res. (JAIR), 27:335–380, 2006.
[6] J. M. Porta, N. Vlassis, M.T.J. Spaan, and P. Poupart. Point-based value iteration for continuous
pomdps. Journal of Machine Learning Research, 7:195220, 2006.
[7] Pascal Poupart, Kee-Eung Kim, and Dongho Kim. Closing the gap: Improved bounds on
optimal pomdp solutions. In In Proceedings of the 21st International Conference on Automated
Planning and Scheduling (ICAPS-11), 2011.
[8] Scott Sanner and Ehsan Abbasnejad. Symbolic variable elimination for discrete and continuous
graphical models. In In Proceedings of the 26th AAAI Conference on Artiﬁcial Intelligence
(AAAI-12), Toronto, Canada, 2012.
[9] Scott Sanner, Karina Valdivia Delgado, and Leliane Nunes de Barros. Symbolic dynamic
programming for discrete and continuous state mdps. In Proceedings of the 27th Conference
on Uncertainty in AI (UAI-2011), Barcelona, 2011.
[10] Trey Smith and Reid G. Simmons. Point-based POMDP algorithms: Improved analysis and
implementation. In Proc. Int. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), 2005.
[11] M. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for pomdps. Jour-
nal of Articial Intelligence Research (JAIR), page 195220, 2005.

9


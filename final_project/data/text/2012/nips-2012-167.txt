Forward-Backward Activation Algorithm for
Hierarchical Hidden Markov Models

Kei Wakabayashi
Faculty of Library, Information and Media Science
University of Tsukuba, Japan
kwakaba@slis.tsukuba.ac.jp

Takao Miura
Department of Engineering
Hosei University, Japan
miurat@hosei.ac.jp

Abstract

Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic mod-
els that enable us to capture a hierarchical context characterization of sequence
data. However, existing HHMM parameter estimation methods require large com-
putations of time complexity O(T N 2D ) at least for model inference, where D is
the depth of the hierarchy, N is the number of states in each level, and T is the
sequence length. In this paper, we propose a new inference method of HHMMs
for which the time complexity is O(T N D+1 ). A key idea of our algorithm is ap-
plication of the forward-backward algorithm to state activation probabilities. The
notion of a state activation, which offers a simple formalization of the hierarchical
transition behavior of HHMMs, enables us to conduct model inference efﬁciently.
We present some experiments to demonstrate that our proposed method works
more efﬁciently to estimate HHMM parameters than do some existing methods
such as the ﬂattening method and Gibbs sampling method.

1 Introduction

Latent structure analysis of sequence data is an important technique for many applications such
as speech recognition, bioinformatics, and natural language processing. Hidden Markov Models
(HMMs) play a key role in solving these problems. HMMs assume a single Markov chain of hidden
states as the latent structure of sequence data. Because of this simple assumption, HMMs tend to
capture only local context patterns of sequence data. Hierarchical Hidden Markov Models (HH-
MMs) are stochastic models which assume hierarchical Markov chains of hidden states as the latent
structure of sequence data [3]. HHMMs have a hierarchical state transition mechanism that yields
the capability of capturing global and local sequence patterns in various granularities. By their na-
ture, HHMMs are applicable to problems of many kinds including handwritten letter recognition [3],
information extraction from documents [11], musical pitch structure modeling [12], video structure
modeling [13], and human activity modeling [8, 6].
For conventional HMMs, we can conduct unsupervised learning efﬁciently using the forward-
backward algorithm, which is a kind of dynamic programming [9].
In situations where few or
no supervised data are available, the existence of the efﬁcient unsupervised learning algorithm is
a salient advantage of using HMMs. The unsupervised learning of HHMMs is an important tech-
nique, as it is for HMMs. In this paper, we discuss unsupervised learning techniques for HHMMs.
We introduce a key notion, activation probability, to formalize the hierarchical transition mecha-
nism naturally. Using this notion, we propose a new exact inference algorithm which has less time
complexity than existing methods have.
The remainder of the paper is organized as follows. In section 2, we overview HHMMs. In section
3, we survey HHMM parameter estimation techniques proposed to date. In section 4, we introduce
our parameter estimation algorithm. Section 5 presents experiments to show the effectiveness of our
algorithm. We conclude our discussion in section 6.

1

Figure 1: (left) Dynamic Bayesian network of the HHMM. (top-right) Tree representation of the
HHMM state space. (bottom-right) State identiﬁcation by the absolute path of the tree.

= k ; F d+1
t

= b) =

= k) =

2 Hierarchical Hidden Markov Models
Let O = fO1 ; :::; Ot ; :::; OT g be a sequence of observations in which subscript t denotes the time
in the sequence. We designate time as an integer index of observation numbered from the beginning
t for 1 (cid:20) t (cid:20) T ; 1 (cid:20) d (cid:20) D as a hidden state at time t and
of the sequence. HHMMs deﬁne Qd
level d, where d = 1 represents the top level and d = D represents the bottom level. HHMMs also
t = 1, then it is indicated that the
t , called termination indicators. If F d
deﬁne binary variables F d
Markov chain of level d terminates at time t. In HHMMs, a state transition at level d is permitted
only when the Markov chain of level d + 1 terminates, i.e. Qd
t = Qd
t(cid:0)1 = 0. A terminated
t(cid:0)1 if F d+1
Markov chain is initialized again at the next time. Figure 1 (left) presents a Dynamic Bayesian
 (cid:14)(i; j )
Network (DBN) expression for an HHMM of hierarchical depth D = 3. The conditional probability
distribution of Q, F and O is deﬁned as follows [7].
(if b = 0)
{
t = j jQd
t(cid:0)1 = f ; Q1:d(cid:0)1
(if b = 1; f = 0)
k (i; j )
t(cid:0)1 = b; F d
t(cid:0)1 = i; F d+1
p(Qd
Ad
t
(if b = 1; f = 1)
k (j )
(cid:25)d
0
(if b = 0)
(if b = 1)
k (i; end)
Ad

t = 1jQd
t = i; Q1:d(cid:0)1
p(F d
t
p(Ot = v jQ1:D
t = k) = Bk (v)
g. Probabilities of the initializa-
as a combination of states fQ1
t ; :::; Qd(cid:0)1
We use a notation Q1:d(cid:0)1
tion and the state transition of Markov chains at level d depend on all higher states Q1:d(cid:0)1 . Ad
t
t
k (i; j )
is a model parameter of the transition probability at level d from state i to j when Q1:d(cid:0)1
= k .
t
k (i; end) denotes a termination probability that state i terminates the Markov chain at level d
Ad
when Q1:d(cid:0)1
k (j ) is an initial state probability of state j at level d when Q1:d(cid:0)1
= k . (cid:25)d
= k . Bk (v)
t
t
t = k .
is an output probability of observation v when Q1:D
A state space of HHMM is expressed as a tree structure [3]. Figure 1 (top-right) presents a tree
expression of state space of an HHMM for which the depth D = 3 and the number of states in each
level N = 3. The level of the tree corresponds to the level of HHMM states. Each node at level d
corresponds to a combination of states Q1:d . Each node has N children because there are N possible
states for each level. The rectangles in the ﬁgure denote local HMMs in which nodes can mutually
transit directly using the transition probability A. For the analysis described herein, we assume the
balanced N-ary tree to simplify discussions of computational complexity. However, arbitrary state
space trees do not change the substance of what follows.
The behavior of Markov chain at level d depends on the combination of all higher-up states Q1:d(cid:0)1 ,
not only on the individual Qd . In the tree structure, the absolute path which corresponds to Q1:d
is meaningful, rather than the relative path which corresponds to Qd . We refer to Q1:d as Z d and
call it absolute path state. Figure 1 (bottom-right) presents an absolute path state identiﬁcation. The
set of values taken by an absolute path state at level d, denoted by (cid:10)d , contains N d elements in the
balanced N-ary tree state space. We deﬁne a function to obtain the parent absolute path state of Z d
as parent(Z d ). Similarly, we deﬁne a function to obtain the set of child absolute path states of Z d
as child(Z d ), and a function to obtain the set of siblings of Z d as sib(Z d ) = child(parent(Z d )).

2

Table 1: Notation for HHMMs.
Depth of hierarchy
D
Number of states in each level
N
Set of values taken by absolute path state at level d
(cid:10)d
t 2 (cid:10)d
Absolute path state at time t and level d
Z d
t 2 f0; 1g
Termination indicator at time t and level d
F d
Ot 2 f1; :::; V g Observation at time t
t = i to state Z d
State transition probability from state Z d
t+1 = j at level d
Adij
Termination probability of Markov chain at level d from state Z d
t = i
AdiEnd
Initial state probability of state Z d = i at level d
(cid:25)di
Output probability of observation v with Z D = i
Biv

Table 1 presents the notation used for the HHMM description. We use the notation of the absolute
path state Z d rather than Qd throughout the paper. Therefore, we deﬁne compatible notations for
k (j ) denotes the initial state probability
the model parameters. Whereas the conventional notation (cid:25)d
∑
∑
of Qd = j when Q1:d(cid:0)1 = k , we aggregate Qd and Q1:d(cid:0)1 into Q1:d = Z d and deﬁne (cid:25)di as the
initial state probability of Z d = i. Similarly, we deﬁne Adij as the state transition probability from
Z d = i to j . Note that
i02sib(i) (cid:25)di0 = 1 and
j 02fsib(i)[Endg Adij 0 = 1.

3 Existing Parameter Estimation Methods for HHMMs

The ﬁrst work for HHMMs [3] proposed the generalized Baum-Welch algorithm. This algorithm is
based on an inside-outside algorithm used for inference of probabilistic context free grammars. This
method takes O(T 3 ) time complexity, which is not practical for long sequence data.
A more efﬁcient approach is the ﬂattening method [7]. The hierarchical state sequence can be
g. If we regard
reduced to a single sequence of the bottom level absolute path states fZ D
1 ; :::; Z D
T
Z D as a ﬂat HMM state, then we can conduct the inference by using the forward-backward algorithm
with O(T N 2D ) time complexity since j(cid:10)D j = N D . Notice that the ﬂat state Z D can transit to any
other ﬂat state, and we cannot apply efﬁcient algorithms for HMMs of sparse transition matrix. In
the ﬂattening method, we must make a weak constraint on the HHMM parameters, say minimally
self-referential (MinSR) [12], which restricts the self-transition at higher levels i.e. Adii = 0 for 1 (cid:20)
d (cid:20) D (cid:0) 1. The MinSR constraint enables us to identify the path connecting two ﬂat states uniquely.
This property is necessary for estimating HHMM parameters by using the ﬂattening method.
We also discuss a sampling approach as an alternative parameter estimation technique. The Gibbs
sampling is often used for parameter estimation of probabilistic models including latent variables
[4]. We can estimate HMM parameters using a Gibbs sampler, which sample each hidden states
iteratively. This method is applicable to inference of HHMMs in a straightforward manner on the ﬂat
HMM. This straightforward approach, called the Direct Gibbs Sampler (DGS), takes the O(T N D )
time complexity for a single iteration.
The convergence of a posterior distribution by the DGS method is said to be extremely slow for
HMMs [10] because the DGS ignores long time dependencies. Chib [2] introduced an alternative
method, called the Forward-Backward Gibbs Sampler (FBS), which calculates forward probabilities
in advance. FBS samples hidden states from the end of the sequence regarding the forward proba-
bilities. FBS method requires larger computations for a single iteration than DGS does, but it can
bring a posterior of hidden states to its stationary distribution with fewer iterations [10].
Heller [5] proposed Inﬁnite Hierarchical Hidden Markov Models (IHHMMs) which can have an
inﬁnitely large depth by weakening the dependency between the states at different levels. They pro-
posed the inference method for IHHMMs based on a blocked Gibbs sampler of which the sampling
unit is a state sequence from t = 1 to T at a single level. This inference takes only O(T D) time
for a single iteration. In HHMMs, the states in each level are strongly dependent, so resampling
a state at an intermediate level causes all lower states to alter into a state which has a completely
different behavior. Therefore, it is not practical to apply this Gibbs sampler to HHMMs in terms of
the convergence speed.

3

4 Forward-Backward Activation Algorithm

In this section, we introduce a new parameter estimation algorithm for HHMMs, which theoretically
has O(T N D+1 ) time complexity. The basic idea of our algorithm is a decomposition of the ﬂat
jZ D
transition probability distribution p(Z D
t ), which the ﬂattening method calculates directly for
t+1
all pairs of the ﬂat states. We can rewrite the ﬂat transition probability distribution into a sum of two
cases that the Markov chain at level D terminates or not, as follows.
jZ D
jZ D
t = 0jZ D
t ) = p(Z D
p(Z D
t = 0)p(F D
t ) +
t ; F D
t+1
t+1
t = 1jZ D
jZ D(cid:0)1
jZ D(cid:0)1
t = 1)p(Z D(cid:0)1
t = 1)p(F D
p(Z D
t )
t+1 ; F D
; F D
t
t+1
t+1
The ﬁrst term corresponds to the direct transition without the Markov chain termination. The ac-
tual computational complexity for calculating this term is O(N D+1 ) because the direct transition
is permitted only between the sibling states, i.e. ADij = 0 if j =2 sib(i). The second term, cor-
responding to the case in which the Markov chain terminates at level D , contains two factors: The
jZ D(cid:0)1
upper level transition probability p(Z D(cid:0)1
t = 1) and the state initialization probability
; F D
jZ D(cid:0)1
t
t+1
for the terminated Markov chain p(Z D
t = 1). We attempt to compute these probability
t+1 ; F D
t+1
distributions efﬁciently in a dynamic programming manner.
 p(Z d
jZ d
The transition probability at level d has the form p(Z d
= 1). We deﬁne ending activa-
t ; F d+1
t
t+1
tion ed
t , as the condition of the transition probability from Z d
t , formally:
(if i 6= null and d < D)
t = i; F d+1
= 1)
(if i 6= null and d = D)
t
t = i) =
p(ed
p(Z d
t = i)
(if i = null)
= 0)
p(F d+1
t
t indicates that the Markov chain at level d + 1 does not terminate at time t.
The null value in ed
 p(Z d
jZ d
t(cid:0)1 = 1). We deﬁne
The state initialization probability for level d + 1 has the form p(Z d+1
t ; F d+1
t
beginning activation bd
t , formally, as
t , as the condition of the state initialization probability from Z d
(if i 6= null and d < D and t > 1)
t = i; F d+1
t(cid:0)1 = 1)
(if i 6= null and (d = D or t = 1))
p(Z d
t = i)
p(F d+1
t(cid:0)1 = 0)
(if i = null)
t indicates that the Markov chain at level d + 1 does not terminate at time t (cid:0) 1.
The null value in bd
Using these notations, we can represent the ﬂat transition with propagations of activation probabil-
jeD
jZ D
t ). This representation naturally
t ) = p(bD
ities as shown in ﬁgure 2 (left) because p(Z D
t+1
t+1
describes the decomposition of the ﬂat transition probability distribution discussed above, and it
{ ∑
enables us to apply the decomposition recursively for all levels. We can derive the conditional
∑
t+1 as
probability distributions of ed
t and bd
(if i 6= null)
{
∑
t = ijed+1
c2child(i) p(ed+1
t = c)A(d+1)cEnd
p(ed
t = c)(1(cid:0)A(d+1)cEnd )+ p(ed+1
t
c2(cid:10)d+1 p(ed+1
(if i = null)
t = null)
(if i 6= null)
p(bd(cid:0)1
t+1 = ijed
j2sib(i) p(ed
t = j )Adj i
t+1 = parent(i))(cid:25)di +
t ; bd(cid:0)1
t+1 ) =
p(bd
p(ed
t = null)
(if i = null)
In the following subsections, we show the efﬁcient inference algorithm and the parameter estimation
algorithm using the activation probabilities.

t = i) =
p(bd

) =

4.1

Inference using Forward and Backward Activation Probabilities

We can translate the DBN of HHMMs in ﬁgure 1 (left) equivalently into simpler DBN using acti-
vation probabilities. The translated DBN is portrayed in ﬁgure 2 (right). The inference algorithm
proposed herein is based on a forward-backward calculation over this DBN. We deﬁne forward
activation probability (cid:11) and backward activation probability (cid:12) as follows.
(i) = p(ed
t = i; O1:t )
(cid:11)ed
t
t = i; O1:t(cid:0)1 )
(i) = p(bd
(cid:11)bd
T = 1jed
t
(i) = p(Ot+1:T ; F 1
t = i)
(cid:12)ed
T = 1jbd
t
(i) = p(Ot:T ; F 1
t = i)
(cid:12)bd
t

4

Figure 2: (left) Propagation of activation probabilities for calculating the ﬂat transition probability
from time t to t + 1. (right) Equivalent DBN of the HHMM using activation probabilities.

(parent(i))(cid:25)di

Algorithm 1 Calculate forward activation probabilities
1: for t = 1 to T do
if t = 1 then
2:
(i 2 (cid:10)1 ) = (cid:25)1i
3:
(cid:11)b1
1
for d = 2 to D do
4:
(i 2 (cid:10)d ) = (cid:11)bd(cid:0)1
∑
5:
(cid:11)bd
1
1
end for
6:
else
7:
(i 2 (cid:10)1 ) =
j2sib(i)(cid:11)e1
8:
(cid:11)b1
t(cid:0)1
t
for d = 2 to D do
9:
(i 2 (cid:10)d ) = (cid:11)bd(cid:0)1
10:
(cid:11)bd
t
t
end for
11:
end if
∑
12:
(i 2 (cid:10)D ) = (cid:11)bD
(i)BiOt
13:
(cid:11)eD
for d = D (cid:0) 1 to 1 do
t
t
14:
(i 2 (cid:10)d ) =
c2child(i) (cid:11)ed+1
15:
(cid:11)ed
t
t
end for
16:
17: end for

(parent(i))(cid:25)di +

(j )A1j i

∑

j2sib(i)(cid:11)ed
t(cid:0)1

(j )Adj i

(c)A(d+1)cEnd

These probabilities are efﬁciently calculable in a dynamic programming manner. Algorithm 1
presents the pseudocodes to calculate whole (cid:11). (cid:11)bd
to (cid:11)bD
are derived downward from (cid:11)b1
by
t
t
t
summing up to the initialization probability from the parent and the transition probabilities from the
siblings (Line 8 to 11). (cid:11)ed
are propagated upward from (cid:11)eD
to (cid:11)e1
by summing up to the probabil-
∑
t
t
t
ities of the child Markov chain termination (Line 13 to 16). This algorithm includes the calculation
of j(cid:10)d j = N d quantities involving the summation of jsib(i)j = N terms for d = 1 to D and for
D
t = 1 to T . Therefore, the time complexity of algorithm 1 is O(T
d=1 N d+1 ) = O(T N D+1 ).
Algorithm 2 propagates the backward activation probabilities similarly in backward order.
T = 1g given ed
We can derive the conditional independence of O1:t and fOt+1:T ; F 1
6= null or
6= null indicates that the Markov chains at level
6= null and bd
6= null, because both of ed
t
bd
t
t+1
t+1
d + 1; :::; D terminates at time t. On the basis of this conditional independence, the exact inference
of a posterior of activation probabilities can be obtained using (cid:11) and (cid:12) as presented below.
T = 1jed
T = 1) / p(ed
t = ijO1:T ; F 1
(i)
(i)(cid:12)ed
t = i) = (cid:11)ed
t = i; O1:t )p(Ot+1:T ; F 1
p(ed
t = ijO1:T ; F 1
T = 1) / p(bd
T = 1jbd
t
t
t = i; O1:t(cid:0)1 )p(Ot:T ; F 1
(i)(cid:12)bd
t = i) = (cid:11)bd
p(bd
(i)
t
t
jO1:T ; F 1
∑
∑
The inference of the ﬂat state p(Z D
T = 1) is identical to of the bottom level activation
jO1:T ; F 1
t
T = 1). We can calculate the likelihood of the whole observation as follows.
probability p(eD
t
T = 1je1
p(e1
T = i; O1:T )p(F 1
T = i) =
i2(cid:10)1
i2(cid:10)1

p(O1:T ; F 1
T = 1) =

(i)(cid:12)e1
T

(cid:11)e1
T

(i)

5

(parent(i))AdiEnd

Algorithm 2 Calculate backward activation probabilities
1: for t = T to 1 do
if t = T then
2:
(i 2 (cid:10)1 ) = A1iEnd
3:
(cid:12)e1
T
for d = 2 to D do
4:
(i 2 (cid:10)d ) = (cid:12)ed(cid:0)1
∑
5:
(cid:12)ed
T
T
end for
6:
else
7:
(i 2 (cid:10)1 ) =
j2sib(i) (cid:12)b1
8:
(cid:12)e1
t+1
t
for d = 2 to D do
9:
(i 2 (cid:10)d ) = (cid:12)ed(cid:0)1
10:
(cid:12)ed
t
t
end for
11:
end if
∑
12:
(i 2 (cid:10)D ) = (cid:12)eD
(i)BiOt
13:
(cid:12)bD
for d = D (cid:0) 1 to 1 do
t
t
14:
(i 2 (cid:10)d ) =
c2child(i) (cid:12)bd+1
15:
(cid:12)bd
t
t
end for
16:
17: end for

(parent(i))AdiEnd +

(c)(cid:25)(d+1)c

(j )A1ij

∑
j2sib(i)(cid:12)bd
t+1

(j )Adij

4.2 Updating Parameters

Using the forward and backward activation probabilities, we can estimate HHMM parameters efﬁ-
∑
ciently in the EM framework. In the EM algorithm, the function Q((cid:18); (cid:22)(cid:18)) is deﬁned, where (cid:18) is a
parameter set before updating and (cid:22)(cid:18) is a parameter set after updating, as described below.
p(cid:18) (Y jX ) log p (cid:22)(cid:18) (X; Y )
Q((cid:18) ; (cid:22)(cid:18)) =
Y
In that equation, X represents a set of observed variables, and Y is a set of latent variables. The dif-
ference of log likelihood between the models of (cid:18) and (cid:22)(cid:18) is known to be greater than Q((cid:18); (cid:22)(cid:18))(cid:0)Q((cid:18); (cid:18))
[1]. For this reason, we can increase the likelihood monotonically by selecting a new parameter (cid:22)(cid:18) to
maximize the function Q. For HHMMs, the set of parameters is (cid:18) = fA; (cid:25) ; B g. The set of observed
variables is X = fO1:T ; F 1
T = 1g. The set of latent variables is Y = fZ 1:D
g. Therefore,
∑
1:T (cid:0)1
1:T ; F 1:D
the function Q can be represented as shown below.
Q((cid:18) ; (cid:22)(cid:18)) /
T = 1; Z 1:D
1:T (cid:0)1 ) log p (cid:22)(cid:18) (O1:T ; F 1
p(cid:18) (O1:T ; F 1
T = 1; Z 1:D
1:T (cid:0)1 )
1:T ; F 1:D
1:T ; F 1:D
Z 1:D
1:T ;F 1:D
1:T (cid:0)1
The joint probability of observed variables and latent variables is given below.
T (cid:0)1∏
D∏
D∏
T∏
D∏
1:T (cid:0)1 )
T = 1; Z 1:D
p(cid:18) (O1:T ; F 1
1:T ; F 1:D
(1(cid:0)F d
t EndAF d+1
(AF d
t )
=
)
(cid:25)dZ d
BZD
t
t
t Ot
dZ d
dZ d
t Z d
1
t+1
t=1
t=1
d=1
d=1
d=1
We substitute this equation for the joint probability in equation (1). We integrate out irrelevant
V∑
∑
D∑
∑
∑
Q((cid:18); (cid:22)(cid:18)) / D∑
∑
variables and organize around each parameter. Thereby, we obtain the following.
j2fsib(i)[Endg
i2(cid:10)D
i2(cid:10)d
i2(cid:10)d
v=1
d=1
d=1
Therein, g(cid:25)di , gAdij , gB iv are shown by equation (2)(3)(4)(5). They are calculable using forward
T (cid:0)1∑
and backward activation probabilities.
T (cid:0)1∑
(i)(cid:12)bd
g(cid:25)di = (cid:11)bd
1
1
t=1
t=1

(parent(i))(cid:25)di(cid:12)bd
t+1

(parent(i)) + (cid:11)ed
T

(i)AdiEnd(cid:12)ed(cid:0)1
t

gAdij log (cid:22)Adij +

g(cid:25)di log (cid:22)(cid:25)di +

gAdiEnd =

(i)(cid:12)ed
T

(i)

(2)

(3)

(i) +

(cid:11)bd(cid:0)1
t+1

(cid:25)F d
t
dZ d
t+1

AdZ d
T End

(i)

(1)

gB iv log (cid:22)Biv

(cid:11)ed
t

6

gAdij =

100
-447.90
-448.52
-448.52

50
-457.66
-453.09
-453.09

Table 2: Log-likelihood achieved at each iteration.
10
4
3
2
1
5
Iteration
-577.33
-610.63
-631.30
-668.50
-672.44
FBA w/o MinSR
-773.47
FBA with MinSR -773.89
-672.47
-670.40
-643.62
-614.98
-573.84
-672.47
-573.84
-614.98
-643.62
-670.40
-773.89
FFB
T (cid:0)1∑
∑
(cid:11)ed
t
t=1
gB iv =
(i)(cid:12)eD
(cid:11)eD
∑
∑
∑
t
t
t:Ot=v
Using Lagrange multipliers, we can obtain parameters (cid:22)(cid:25) ; (cid:22)A; (cid:22)B , which maximize the function Q
j 02fsib(i)[Endg (cid:22)Adij 0 = 1,
(cid:22)Biv = 1 as shown below.
∑
∑
; (cid:22)Biv = gB iv∑
i02sib(i) (cid:22)(cid:25)di0 = 1,
under the constraint
v
gAdij
g(cid:25)di
j 02fsib(i)[Endg gAdij 0
i02sib(i) g(cid:25)di0
v gB iv
Consequently, we can calculate the update parameters using (cid:11) and (cid:12) . The time complexity for
computing a single EM iteration is O(T N D+1 ), which is identical to the calculation of forward and
backward activation probabilities.

(i)Adij (cid:12)bd
t+1

(j )

(cid:22)(cid:25)di =

; (cid:22)Adij =

(i)

(4)

(5)

5 Experiments

Firstly, we experimentally conﬁrm that the forward-backward activation algorithm yields exactly
identical parameter estimation to the ﬂattening method does. Remind that we must make the MinSR
constraint on the HHMM parameter set in the ﬂattening method (see section 3). We compare
three parameter estimation algorithms: our forward-backward activation algorithm for a MinSR
HHMM (FBA with MinSR), for a HHMM without MinSR (FBA w/o MinSR), and the ﬂattening
method(FFB). The dataset to learn includes 5 sequences of 10 length, which are artiﬁcially gener-
ated by a MinSR HHMM of biased parameter set. We execute three algorithms and examine the
log-likelihood achieved at each iteration.
Table 2 presents the result. The FBA with MinSR and the FFB achieve the identical log-likelihood
through the training. This result provides experimental evidence that our algorithm estimates
HHMM parameters exactly identically to the ﬂattening method does. Furthermore, the FBA enables
us to conduct the parameter estimation of HHMMs which has non-zero self-transition parameters.
To evaluate the computational costs empirically, we compare four methods of HHMM parameter
estimation. Two are based on the EM algorithm with inference by the forward-backward activation
algorithm (FBA), and by the ﬂattening forward-backward method (FFB). Another two are based on
a sampling approach: direct Gibbs sampling for the ﬂat HMMs (DGS) and forward-backward acti-
vation sampling (FBAS). FBAS is a straightforward application of the forward-backward sampling
scheme to the translated DBN presented in ﬁgure 2. In FBAS, we ﬁrst calculate forward activation
probabilities. Then we sample state activation variables from e1
T to b1
1 in the backward order with
respect to forward activation probabilities. We evaluate four methods based on three aspects: execu-
tion time, convergence speed, and scalability of the state space size. We apply each method to four
different HHMMs of (D = 3,N = 3), (D = 3,N = 4), (D = 4,N = 3), and (D = 4,N = 4). We
examine the log-likelihood of the training dataset achieved at each iteration to ascertain the learn-
ing convergence. As a training dataset, we use 100 documents from the Reuters corpus as word
sequences. The dataset includes 36,262 words in all, with a 4,899 word vocabulary.
Figure 3 presents the log-likelihood of the training data. The horizontal axis shows the logarith-
mically scaled execution time. Table 2 presents the average execution time for a single iteration.
From these results, we can say primarily that FBA outperforms FFB in terms of execution time. The
improvement is remarkable, especially for the HHMMs of large state space size because FBA has
less time complexity for N and D than FFB has.

7

Figure 3: Convergence of log-likelihood for the training data on the Reuters corpus. Log-likelihood
(vertical) is shown against the log-scaled execution time (horizontal) to display the execution time
necessary to converge the learning of each algorithm. (top-left) HHMM of D = 3, N = 3. (top-
right) D = 3, N = 4. (bottom-left) D = 4, N = 3. (bottom-right) HHMM of D = 4, N = 4.

Table 3: Average execution time for a single iteration (ms).
Method D = 3; N = 3 D = 3; N = 4 D = 4; N = 3 D = 4; N = 4
(N D = 256)
(N D = 81)
(N D = 64)
(N D = 27)
1652.03
476.92
391.73
186.65
FBA
FFB
1729.90
9242.35
19257.80
220224.00
581.58
183.39
142.20
82.45
FBAS
DGS
24.19
37.50
45.43
265.98

The results show that the likelihood convergence using DGS is much slower than that of other
methods.The execution time of DGS is less than that of other methods for a single iteration, but
this cannot compensate for the low convergence speed. However, FBAS achieves a competitive
likelihood in comparison to FBA. Results show that FBAS might be appropriate for some situations
because FBAS ﬁnds a better solution than that FBA do in some results.

6 Conclusion

In this work, we proposed a new inference algorithm for HHMMs based on the activation probability.
Results show that the performance of our proposed algorithm surpasses that of existing methods.
The forward-backward activation algorithm described herein enables us to conduct unsupervised
parameter learning with a practical computational cost for HHMMs of larger state space size.

References

[1] C. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.
[2] S. Chib. Calculating posterior distributions and modal estimates in markov mixture models.
Journal of Econometrics, 1996.
[3] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden markov model: Analysis and appli-
cations. Machine Learning, 1998.

8

-260000-250000-240000-230000-220000-210000-200000-190000-180000 100 1000 10000 100000 1e+006Log-LikelihoodExecution Time (ms)FB ActivationFlattening FBFB Activation SamplingDirect Gibbs Sampling-260000-250000-240000-230000-220000-210000-200000-190000-180000 100 1000 10000 100000 1e+006Log-LikelihoodExecution Time (ms)FB ActivationFlattening FBFB Activation SamplingDirect Gibbs Sampling-260000-250000-240000-230000-220000-210000-200000-190000-180000 100 1000 10000 100000 1e+006Log-LikelihoodExecution Time (ms)FB ActivationFlattening FBFB Activation SamplingDirect Gibbs Sampling-260000-250000-240000-230000-220000-210000-200000-190000-180000 1000 10000 100000 1e+006Log-LikelihoodExecution Time (ms)FB ActivationFlattening FBFB Activation SamplingDirect Gibbs Sampling[4] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proc. the National Academy of Sciences
of the United States of America, 2004.
[5] K. Heller, Y. Teh, and D. Gorur. Inﬁnite hierarchical hidden markov models. In Proc. Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, 2009.
[6] S. Luhr, H. Bui, S. Venkatesh, and G. West. Recognition of human activity through hierarchical
stochastic learning. In Proc. Pervasive Computing and Communication, 2003.
In Proc. Neural
[7] K. Murphy and M. Paskin. Linear time inference in hierarchical hmms.
Information Processing Systems, 2001.
[8] N. Nguyen, D. Phung, and S. Venkatesh. Learning and detecting activities from movement tra-
jectories using the hierarchical hidden markov models. In Proc. Computer Vision and Pattern
Recognition, 2005.
[9] L. Rabiner. A tutorial on hidden markov models and selected applications in speech recogni-
tion. Proc. IEEE, 1989.
[10] S. Scott. Bayesian methods for hidden markov models: Recursive computing in the 21st
century. Journal of the American Statistical Association, 2002.
[11] M. Skounakis, M. Craven, and S. Ray. Hierarchical hidden markov models for information
extraction. In Proc. International Joint Conference on Artiﬁcial Intelligence, 2003.
[12] M. Weiland, A. Smaill, and P. Nelson. Learning musical pitch structures with hierarchical
hidden markov models. In Proc. Journees Informatiques Musicales, 2005.
[13] L. Xie, S. Chang, A. Divakaran, and H. Sun. Learning hierarchical hidden markov models for
video structure discovery. Technical report, Columbia University, 2002.

9


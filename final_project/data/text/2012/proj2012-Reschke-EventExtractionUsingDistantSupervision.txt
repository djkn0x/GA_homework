Event Extraction Using Distant Supervision

Kevin Reschke∗. kreschke@stanford.edu

December 14, 2012

1 Introduction

The purpose of this paper is to explore a distant super-
vision approach to event extraction—that is, the extrac-
tion of template-based facts about events from unstruc-
tured text.
In a distantly supervised system, training
texts are labeled automatically (and noisily) by leverag-
ing an existing database of known facts. This approach
has been applied successfully to the extraction of binary
relations such as a person’s employer or a ﬁlm’s director
(e.g., Surdeanu et al., 2011), but it has not previously
been applied to event extraction.
Concretely, I develop a system which extracts airplane
crash events from a corpus of news documents using
Wikipedia infoboxes as a source for distant supervision.1
The news corpus is a collection of newswire texts span-
ning 1988 to the present.2
I selected 80 plane crash
infoboxes in that time frame from Wikipedia: 36 for
training; 8 for development; 40 for testing. An example
is shown in Table 1. At training time, facts from the 36
training infoboxes are used to automatically label train-
ing sentences from the news corpus. At test time, the
system takes the ﬂight number of a test infobox as in-
put and produces values for the seven template slots as
output.
The paper is structured as follows. First I detail the
event extraction process and my distant supervision ap-
proach. Then I describe a series of experiments testing
various models within this framework.

2 Event Extraction

At test time, event extraction has three steps. 1) Can-
didate Generation: run Named-Entity Recognition soft-
ware3 on relevant documents from the corpus to identify
candidate mentions. In this setting, I use the ﬂight num-
ber as a proxy for document relevance; if a document
∗ I thank Mihai Surdeanu, Martin Jankowiak, David McClosky,
and Christopher Manning for guidance on this pro ject.
1http://en.wikipedia.org/wiki/Help:Infobox
2 I use Gigaword-5, Tipster-1, Tipster-2, and Tipster-3. See
www.ldc.upenn.edu/.
3Stanford CoreNLP NER: nlp.stanford.edu/software/CRF-
NER.shtml

Table 1: Sample plane crash infobox.
Slot Value
Slot Type
Flight 967
Flight Number
Operator
Armavia
Aircraft Type
Airbus A320-211
Alder-Sochi Airport, Black Sea
Crash Site
105
Passengers
Crew
8
Fatalities
113
Injuries
0
Survivors
0

has the string “Flight x ”, then the document is consid-
ered relevant to the Flight x plane crash event. 2) Men-
tion Classiﬁcation: Classify candidate mentions based
on contextual features (surrounding unigrams, syntac-
tic dependencies, etc). The label space is the set of slot
types in the event template, plus NIL for mentions which
don’t ﬁt any slot. 3) Label Aggregation: Merge labels
from diﬀerent mentions of the same value to produce
ﬁnal slot value predictions. For the bulk of this paper
I assume Exhaustive Aggregation—that is, all non-NIL
labels are included in the ﬁnal prediction. But see Sec-
tion 4.6 for an improved aggregation scheme.
As an example of the test time procedure, suppose we
are extracting facts about the crash of Flight 13, and we
identify the candidate Mississippi in the sentence “Flight
13 crashed in Mississippi.” A properly trained mention
classiﬁer will give this mention the label (cid:104)CrashSite(cid:105)
based on the words crashed and in which precede it.
Now suppose that over all of the mentions of Missis-
sippi, three mentions were classiﬁed as (cid:104)CrashSite(cid:105), two
mentions were classiﬁed as NIL, and one mention was
(incorrectly) classiﬁed as (cid:104)Operator(cid:105). Label aggregation
will gives us the ﬁnal predictions that Mississippi is both
the crash site and the operator of Flight 13.

3 Distant Supervision

The mention classiﬁcation step mentioned above re-
quires a trained classiﬁer—this is where distant supervi-
sion comes in. In a fully supervised approach, we would

1

have humans label a set of mentions and train a classiﬁer
on those gold labels, but in this case supervision comes
indirectly from a set of training infoboxes.
How are infoboxes mapped to text labels? Consider
ﬁrst the relation extraction setting for which distant su-
pervision was ﬁrst introduced (Mintz et al. 2009). In
distant supervision for relation extraction, the training
set is a database of binary relations such as (cid:104)Steve Jobs,
Apple (cid:105) for the FounderOf relation. Training sentences
are labeled by the following rule:
if both entities ap-
pear in a single sentence, that sentence is a positive
instance of the relation; otherwise it is NIL. This rule
ensures that we apply the label FounderOf to the sen-
tence “Steve Jobs co-founded Apple in 1976,” but not to
random, unrelated mentions of Apple.
Unfortunately, for event extraction, this sentence level
rule doesn’t work. We might think of template slots as
binary relations between the slot value and the ﬂight
number, but as we see below, slot values often occur in
isolation.
• The plane went down in central Texas.
• 10 died and 30 were injured in yesterdays tragic
incident.

Instead, I adopt a document-level heuristic. Given a
slot value and ﬂight number pair from a training infobox,
if the slot value occurs in the same document as the
ﬂight number, mark the mention as a positive example
for that slot type. Since we’re using the presence of
the ﬂight number as a heuristic for document relevance,
this is equivalent to only labeling mentions that occur
in documents relevent to the training event.
Named entities that occur in a relevant document but
don’t match any slot values are given NIL labels. After
the process is complete, NIL examples are subsampled,
resulting in a training set with a 50/50 split between
NIL and non-NIL examples.
Due to the heuristic nature of this noisy labeling
scheme, the resulting training examples are extremely
noisy. In fact, training data noise is a hallmark of distant
supervision. Noise is prevalent in the relation extraction
setting—for example, any sentence containing both Ap-
ple and Steve Jobs will be marked with the FounderOf
relation, even the sentence “Steve Jobs was ﬁred from
Apple in 1985.” Likewise there are many false labelings
in the event extraction setting, such as when an airline’s
name is mentioned, but the sentence has nothing to do
with that airline being the operator in the target crash
event. In fact, by manually checking 50 examples from
each slot type, I found that 39% were wrong. This high
degree of noise is a central challenge to the distant su-
pervision approach, and will be a theme that resurfaces
in the experiments that follow.

4 Experiments

Having introduced the general framework for distantly
supervise event extraction, in this section I present ex-
periments testing various models in this framework. For
all test-set scores that I present, the model has been
tuned to maximize F1 -Score on the 8-infobox dev-set.

4.1 Experiment 1: Simple Local Classiﬁer

First I use multi-class logistic regression to train a
model which classiﬁes each mention independently, us-
ing the noisy training data described above.
Fea-
tures include the mention’s part of speech, named en-
tity type, surrounding unigrams, incoming and outgo-
ing syntactic depencies, the location within the doc-
ument and the mention string itself.4 For example,
the Mississippi example from Section 2 might have the
following binary features: LexIncEdge-prep in-crash-
VBD, UnLexIncEdge-prep in-VBD, PREV WORD-in,
2ndPREV WORD-crash, NEType-LOCATION, Sent-
NEType-ORGANIZATION, etc.
I compare this local classiﬁer with a ma jority class
baseline. Table 2 shows the distribution of labels in the
distantly generated training data. The ma jority baseline
simply picks the ma jority class for each named entity
type: (cid:104)Site(cid:105) for locations, (cid:104)Operator(cid:105) for organizations,
and (cid:104)Fatalities(cid:105) for numbers.

Table 2: Label frequency in noisy training data.
Label
Frequency Named Entity Type
19196
<NIL>
Site
10365
4869
Operator
2241
Fatalities
Aircraft Type
1028
470
Crew
143
Survivors
121
Passengers
0
Injuries

LOCATION
ORGANIZATION
NUMBER
ORGANIZATION
NUMBER
NUMBER
NUMBER
NUMBER

To compare performance on the ﬁnal slot prediction
task, I deﬁne precision and recall as follows. Precision is
the number of correct guesses over the total number of
guesses. Recall is the number of slots correctly ﬁlled over
the number of ﬁndable slots. A slot is ﬁndable if its true
value appears somewhere as a candidate mention.
In
other words, we don’t penalize the extraction model for
missing a slot that either wasn’t in the corpus or didn’t
occur under our heuristic notion of relevant document.

4Parsing, POS tagging, and NER: Stanford Core NLP.
nlp.stanford.edu/software/corenlp.shtml

2

Table 3: Performance of Local Classiﬁer vs. Ma jority
baseline.

Ma j. Baseline
Local Classiﬁer

Precision Recall F1 -Score
0.047
0.237
0.026
0.159
0.407
0.229

Table 4: Accuracy of local classiﬁer by slot type
8/50 = 0.16
Site
5/25 = 0.20
Operator
Fatalities
7/35 = 0.20
4/19 = 0.21
Aircraft Type
15/170 = 0.09
Crew
1/1 = 1.0
Survivors
Passengers
11/42 = 0.26
Injuries
0/0 = NA

Some slots, such as (cid:104)CrashSite(cid:105), can have multiple val-
ues. This metric only requires identifying one or more
of them.
The performance of the local and ma jority classiﬁers
are shown in table 3. The test set contained 40 test
infoboxes for a total of 135 ﬁndable slots. The local
classiﬁer signiﬁcantly outperforms the baseline. Table 4
breaks down the accuracy of the local classiﬁer by slot
type.

4.2 Experiment 2: Training set bias

Recall that during distant supervision, training exam-
ples are generated for a training infobox for every doc-
ument relevent to that infobox. Figure 1 shows the fre-
quency of relevant documents for each infobox. We see
that most infoboxes selected a hundred or so documents,
but one event in particular had several thousand relevant
documents. (Incidentally, this high frequency event is
the crash of Pan Am Flight 103—a.k.a. the Lockerbie
bombing). Consequently a large portion of the noisy
training examples are due to this single event.

Figure 1: Relevant documents per training infobox

To see whether this imbalance hurts system perfor-
mance, I trained a new local classiﬁer using only 200 doc-
uments for Flight 103. This improved dev-set precision
slightly (0.26 vs. 0.30) but hurt recall (0.13 vs. 0.30).
However, this eﬀect appears to be a consequence of less
training data, not training set imbalance. I trained an-
other model using all 3000+ Flight 103 documents but
only using 24 training events instead of 32. The results
were comparable (prec:0.33; rec:0.13).

4.3 Experiment 3: Sentence relevance

With the simple local classiﬁer described in Section 4.1,
a lot of errors come from sentences that are irrelevant
to the event. For example, Northwest Airlines was clas-
siﬁed as (cid:104)Operator(cid:105) in the sentence below, but in fact
neither the sentence nor Northwest Airlines had any rel-
evance to the target plane crash event.

• Clay Foushee, vice president for ﬂying operations
for Northwest Airlines, which also once suﬀered
from a coalition of diﬀerent pilot cultures, said the
process is “long and involved and acrimonious.”

These errors would be mitigated if we could eliminate
irrelevant sentences from consideration during mention
classiﬁcation. To this end, I trained a binary sentence
relevance classiﬁer over unigram and bigram features.
Like the mention models, the relevance classiﬁer was
distantly supervised—during noisy labeling, a training
sentence was marked relevant if it contained at least one
slot value from one event.
Two new local models incorporate this sentence rele-
vance signal. In LocalWithHardSent, all mentions from
non-relevant sentences are classiﬁed NIL. In LocalWith-
SoftSent, sentence relevance is used as a feature in men-
tion classiﬁcation.
The test-set results for these new models are shown in
Table 5. Surprisingly, the new models signiﬁcantly un-
derperform the simple local model. One explanation is
that the distant supervision for sentence relevance was
just too noisy to train a good classiﬁer. Still, it is surpris-
ing that this hurt performance. If we can’t get a signal
from sentence relevance, we would expect LocalWith-
SoftSent to ignore the relevance feature, not to perform
worse.

Table 5: Classiﬁers using sentence relevance.
Precision Recall F1 -Score
0.23
0.41
0.16
0.12
0.24
0.16
0.15
0.23
0.11

Local Classiﬁer
LocalWithHardSent
LocalWithSoftSent

3

I also applied sentence relevance to the model up-
grades described in Sections 4.4 and 4.5. In every case,
sentence relevance hurt performance.

4.4 Experiment 4: Pipeline model

So far I have presented only local models which classify
mentions independently, but in reality there are depen-
dencies between mention labels. For example, (cid:104)Crew(cid:105)
and (cid:104)Passenger(cid:105) go together; (cid:104)Site(cid:105) often follows (cid:104)Site(cid:105);
and (cid:104)Fatalities(cid:105) never follows (cid:104)Fatalities(cid:105):

• 4 crew and 200 passengers were on board.
• The plane crash landed in Beijing, China.
• * 20 died and 30 were killed in last Wednesday’s
crash.

In this experiment, I compare a pipeline model with
the simple local model. In the pipeline model, mentions
in a sentence are classiﬁed sequentially. At each step,
the label of the previous non-NIL mention is used as a
feature for the current mention. At training time, this is
the previous mention’s noisy “gold” label. At test time,
this is the classiﬁer’s output on the previous mention.
The pipeline model boosted recall, but took a slight
hit on precision. Table 6 shows test-set results. A qual-
itative analysis of the pipeline model’s feature weights
revealed that the classiﬁer learned the patterns men-
tioned above, as well as others. However, this wasn’t
enough to signiﬁcantly improve performance.

Table 6: Performance of Pipeline Model
Precision Recall F1 -Score
0.159
0.407
0.229
0.226
0.422
0.154

Local Model
Pipeline Model

4.5 Experiment 5: Joint model (Searn)

There is a common problem with pipeline models which
may explain the performance reported above: pipeline
models propagate error. Consider the example in Figure
2. At training time, USAirways has the feature PREV-
LABEL-INJURY. But suppose that at inference time,
we mislabel 15 as (cid:104)Survivors(cid:105). Now USAirways has the
feature PREV-LABEL-SURVIVOR, and we are in a fea-
ture space that we never saw in training. Thus we are
liable to make the wrong classiﬁcation for USAirways.
And if we make the wrong decision there, then again we
are in an unfamiliar feature space for Boeing 747 which
may lead to another incorrect decision.

Figure 2: Error propagation in pipeline classiﬁcation.

This error propagation is particularly worrisome in
our distant supervision setting due to the high amount
of noise in the training data. To extend the example,
suppose instead that at distant supervision time, 15 was
given the incorrect “gold” label (cid:104)Fatalities(cid:105). Now at test
time, we might correctly classify 15 as (cid:104)Injuries(cid:105), but
this will put us in an unseen feature space for subsequent
decisions because USAirways saw (cid:104)Fatalities(cid:105) at training
time, not (cid:104)Injuries(cid:105).
An ideal solution to this error propagation problem
should do two things. First, it should allow suboptimal
local decisions that lead to optimal global decisions. For
the previous example, this means that our choice for
15 should take into account our future performance on
USAirways and Boeing 747. Second, models of sequence
information should be based on actual classiﬁer output,
not gold labels. This way we won’t be in an unfamiliar
feature space each time our decision diﬀers from the gold
label.
In essence, we want a joint mention model—one which
optimizes an entire sequence of mentions jointly rather
than one at a time. To this end, I applied the Searn al-
gorithm (Daum´e, 2006) to mention classiﬁcation. Searn
stands for Search-based Structured Prediction. At a
high level, Searn is an iterative solution to the follow-
ing chicken-and-egg problem: we want a set of decision
costs based on an optimal global policy; and we want a
global policy to be learned from these decision costs. A
sketch of the algorithm is given in Figure 3. The cur-
rent hypothesis is an interpolation of the optimal policies
learned at each iteration. The algorithm is seeded with
an initial policy that simply chooses gold labels (akin to
a pipeline approach). At each iteration, the hypothesis
moves away the gold policy, and ultimately this inital
policy is dropped from the ﬁnal hypothesis.
At each iteration, Searn requires a cost-sensitive clas-
siﬁer. For this I follow Vlachos and Craven (2011) in
using the algorithm in Cramer et al.
(2006), which
amounts to a passive-aggressive multiclass perceptron.
Searn has a number of hyperparameters. By hill climb-
ing on my development set, I arrived at the following
settings: 4 Searn iterations; 8 perceptron epochs per it-
eration; interpolation β = 0.3; perceptron aggressiveness
= 1.0.
The test-set results comparing Searn to the pipeline
and local models are shown in Table 7. Searn clearly

4

Figure 3: The Searn algorithm for mention classiﬁcation.

Table 7: Performance Searn, Pipeline and Local models.
Precision Recall F1 -Score
0.229
0.407
0.159
0.226
0.422
0.154
0.213
0.422
0.283

Local Model
Pipeline Model
Searn Model

dominates. A likely explanation is that Searn was able
to model the inter-mention dependencies described in
Section 4.4 while avoiding the error propagation endemic
to the pipeline model.

4.6 Experiment 6: Noisy-OR Aggregation

As described in Section 2, the ﬁnal step in event extrac-
tion, after mention classiﬁcation, is label aggregation.
So far I have assumed exhaustive aggregation—as long
as at least one mention of a value gets a particular slot
label, we use that value in our ﬁnal slot-ﬁlling decision.
Intuitively, this approach is suboptimal, especially in a
noisy data environment where we are more likely to mis-
classify the occassional mention. In fact, a proper ag-
gregation scheme can act as fortiﬁcation against noise
induced misclassiﬁcations.
With this in mind, I adopted a Max Aggregation
scheme: when multiple non-NIL labels occur for men-
tions of a particular value, choose the label that occurs
most often.
Interestingly, this scheme had little eﬀect
(+0.01 precision) on system performance. It turns out
the scenario with multiple non-NIL labels is relatively
rare. Instead, it was most common to see mention la-
bels split between a single non-NIL label and NIL. In
this case, Exhaustive and Max Aggregation always re-
turn the non-NIL label, but we would prefer a scheme
that can select NIL under the right cirumstances.
To achieve this I use Noisy-OR Aggregation. The key
idea is that classiﬁers gives us distributions over labels,
not just hard assignments. A simpliﬁed example is given
below.
• Stockholm (cid:104)NIL:0.8; Site: 0.1, Crew:0.01, etc.(cid:105)

• Stockholm (cid:104)Site: 0.5; NIL: 0.3, Crew:0.1, etc.(cid:105)

Given a distribution over labels (cid:96) for each mention
m in M , the set of mentions for a particular candidate
value, we can compute Noisy-OR for each label as fol-
lows.
N oisyOr((cid:96)) = P ((cid:96)|M ) = 1 − (cid:89)
m∈M

1 − P r((cid:96)|m)

If the Noisy-OR of a label is above some threshold, we
use the label—otherwise we return NIL. I found 0.9 to
be an optimal threshold by tuning on the development
set. Table 8 shows test-set results comparing Noisy-OR
and Exhaustive Aggregation on the simple local clas-
siﬁer. We see that Noisy-OR improves precision while
decreasing recall. (This is to be expected as Noisy-OR
is strictly more conservative—i.e. NIL-prefering—than
Exhaustive Aggregation). In terms of F1 -Score, Noisy-
OR Aggregation is the better method.

Table 8: Two Label Aggregation schemes applied to Lo-
cal Model.

Exhaust. Agg.
Noisy-OR. Agg.

Precision Recall F1 -Score
0.16
0.41
0.23
0.26
0.39
0.19

5 Conclusion

I have presented a distant supervision approach to event
extraction using plane crash events as a test bed and de-
scribed how various models perform in the framework.
In future work, I will explore better notions of document
relevance (to replace the naive “contains ﬂight number”
heuristic), and I will look into better methods for ap-
plying sentence relevance to the system. Additionally,
I will apply this framework to the MUC-4 shared event
extraction task to compare distantly supervised event
extraction with the fully supervised state of the art.

References: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
ShalevShwartz, and Yoram Singer. (2006). Online passiveaggres-
sive algorithms. Journal of Machine Learning Research, 7:551585.
• Hal Daum´e (2006). Practical Structured Learning Techniques
for Natural Language Processing. PhD Thesis (USC). • Mihai
Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X.
Chang, Valentin I. Spitkovsky, Christopher D. Manning (2011).
Stanford’s Distantly-Supervised Slot-Filling System. Proceedings
of the TAC-KBP Workshop. • Mike Mintz, Steven Bills, Rion
Snow, Daniel Jurafsky (2009) Distant supervision for relation ex-
traction without labeled data. ACL/AFNLP: 1003-1011. • An-
dreas Vlachos, Mark Craven (2011). Search-based structured pre-
diction applied to biomedical event extraction.
In Proceedings
of the Fifteenth Conference on Computational Natural Language
Learning. Association for Computational Linguistics:49-57.

5


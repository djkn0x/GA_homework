Finding Exemplars from Pairwise Dissimilarities
via Simultaneous Sparse Recovery

Ehsan Elhamifar
EECS Department
University of California, Berkeley

Guillermo Sapiro
ECE, CS Department
Duke University

Ren ´e Vidal
Center for Imaging Science
Johns Hopkins University

Abstract

Given pairwise dissimilarities between data points, we consider the problem of
ﬁnding a subset of data points, called representatives or exemplars, that can efﬁ-
ciently describe the data collection. We formulate the problem as a row-sparsity
regularized trace minimization problem that can be solved efﬁciently using con-
vex programming. The solution of the proposed optimization program ﬁnds the
representatives and the probability that each data point is associated with each one
of the representatives. We obtain the range of the regularization parameter for
which the solution of the proposed optimization program changes from selecting
one representative for all data points to selecting all data points as representatives.
When data points are distributed around multiple clusters according to the dissim-
ilarities, we show that the data points in each cluster select representatives only
from that cluster. Unlike metric-based methods, our algorithm can be applied to
dissimilarities that are asymmetric or violate the triangle inequality, i.e., it does
not require that the pairwise dissimilarities come from a metric. We demonstrate
the effectiveness of the proposed algorithm on synthetic data as well as real-world
image and text data.

1

Introduction

Finding a subset of data points, called representatives or exemplars, which can efﬁciently describe
the data collection, is an important problem in scientiﬁc data analysis with applications in ma-
chine learning, computer vision, information retrieval, etc. Representatives help to summarize and
visualize datasets of images, videos, text and web documents. Computational time and memory re-
quirements of classiﬁcation algorithms improve by working on representatives, which contain much
of the information of the original data collection. For example, the efﬁciency of the NN method
improves [1] by comparing tests samples to K representatives as opposed to all N training samples,
where typically we have K (cid:28) N . Representatives provide clustering of data points, and, as the
most prototypical data points, can be used for efﬁcient synthesis/generation of new data points.
The problem of ﬁnding representative data has been well-studied in the literature [2, 3, 4, 5, 6, 7, 8].
Depending on the type of the information that should be preserved by the representatives, algorithms
can be divided into two categories. The ﬁrst group of algorithms ﬁnds representatives from data that
lie in one or multiple low-dimensional subspaces and typically operate on the measurement data
vectors directly [5, 6, 7, 8, 9, 10, 11]. The Rank Revealing QR (RRQR) algorithm [6, 9] assumes
that the data come from a low-rank model and tries to ﬁnd a subset of columns of the data matrix
that corresponds to the best conditioned submatrix. Randomized and greedy algorithms have also
been proposed to ﬁnd a subset of the columns of a low-rank matrix [5, 8, 10]. Assuming that the
data can be expressed as a linear combination of the representatives, [7, 11] formulate the problem
of ﬁnding representatives as a joint-sparse recovery problem, [7] showing that when the data lie in a
union of low-rank models, the algorithm ﬁnds representatives from each low-rank model.

1

The second group of algorithms ﬁnds representatives by assuming that there is a natural grouping
of the data collection based on an appropriate measure of similarity between pairs of data points [2,
4, 12, 13, 14]. As a result, such algorithms typically operate on similarities/dissimilarities between
data points. The Kmedoids algorithm [2] tries to ﬁnd K representatives from pairwise dissimilarities
between data points. As solving the original optimization program is, in general, NP-hard [12], an
iterative approach is employed. The performance of Kmedoids, similar to Kmeans [15], depends on
initialization and decreases as the number of representatives, K , increases. The Afﬁnity Propagation
(AP) algorithm [4, 13, 14] tries to ﬁnd representatives from pairwise similarities between data points
by using a message passing algorithm. While AP has suboptimal properties and ﬁnds approximate
solutions, it does not require initialization and has been shown to perform well in problems such as
unsupervised image categorization [16] and facility location problems [17].
In this paper, we propose an algorithm for selecting representatives of a data collection given dis-
similarities between pairs of data points. We propose a row-sparsity regularized [18, 19] trace mini-
mization program whose objective is to ﬁnd a few representatives that encode well the collection of
data points according to the provided dissimilarities. The solution of the proposed optimization pro-
gram ﬁnds the representatives and the probability that each data point is associated with each one of
the representatives. Instead of choosing the number of representatives, the regularization parameter
puts a trade-off between the number of representatives and the encoding cost of the data points via
the representatives based on the dissimilarities. We obtain the range of the regularization parameter
where the solution of the proposed optimization program changes from selecting one representative
for all data points to selecting each data point as a representative. When there is a clustering of
data points, deﬁned based on their dissimilarities, we show that, for a suitable range of the regu-
larization parameter, the algorithm ﬁnds representatives from each cluster. Moreover, data points
in each cluster select representatives only from the same cluster. Unlike metric-based methods, we
do not require that the dissimilarities come from a metric. Speciﬁcally, the dissimilarities can be
asymmetric or can violate the triangle inequality. We demonstrate the effectiveness of the proposed
algorithm on synthetic data and real-world image and text data.

2 Problem Statement

We consider the problem of ﬁnding representatives from a collection of N data points. Assume
we are given a set of nonnegative dissimilarities {dij }i,j=1,...,N between every pair of data points
i and j . The dissimilarity dij indicates how well the data point i is suited to be a representative of
the data point j . More speciﬁcally, the smaller the value of dij is, the better the data point i is a
representative of the data point j .1 Such dissimilarities can be built from measured data points, e.g.,
by using the Euclidean/geodesic distances or the inner products between data points. Dissimilarities
can also be given directly without accessing or measuring the data points, e.g., they can be subjective
d
 d11
 =
 ∈ RN ×N ,
measurements of the relationships between different objects. We can arrange the dissimilarities into
a matrix of the form
(cid:62)
1
...
...
(cid:62)
dN 1
d
N
where di ∈ RN denotes the i-th row of D .
Remark 1 We do not require the dissimilarities to satisfy the triangle inequality. In addition, we
do not assume symmetry on the pairwise dissimilarities. D can be asymmetric, where dij (cid:54)= dj i
for some pairs of data points. In other words, how well data point i represents data point j can be
different from how well j represents i. In the experiments, we will show an example of asymmetric
dissimilarities for ﬁnding representative sentences in text documents.

d1N
...
dN N

d12
...
dN 2

· · ·

· · ·

D (cid:44)

(1)

Given D , our goal is to select a subset of data points, called representatives or exemplars, that efﬁ-
ciently represent the collection of data points. We consider an optimization program that promotes
selecting a few data points that can well encode all data points via the dissimilarities. To do so, we
consider variables zij associated with dissimilarities dij and denote by the matrix of all variables as

1 dii can be set to have a nonzero value, as we will show in the experiments on the text data.

2

· · ·

Figure 1: Data points (blue dots) in two clusters and the representatives (red circles) found by the proposed
optimization program in (4) for several values of λ with λmax,q deﬁned in (6). Top: q = 2, Bottom: q = ∞.
 =
 z11
 ∈ RN ×N ,
z(cid:62)
z1N
z12
1
Z (cid:44)
...
...
...
...
· · ·
z(cid:62)
zN N
zN 2
zN 1
N
where z i ∈ RN denotes the i-th row of Z . We interpret zij as the probability that data point i be a
which case zij > 0 for all the indices i of the representatives. As a result, we must have (cid:80)N
representative for data point j , hence zij ∈ [0, 1]. A data point j can have multiple representatives in
i=1 zij =
1, which ensures that the total probability of data point j choosing all its representatives is equal
to one. Our goal is to select a few representatives that well encode the data collection according to
the dissimilarities. To do so, we propose a row-sparsity regularized trace minimization program on
Z that consists of two terms. First, we want the representatives to encode well all data points via
its representatives is (cid:80)N
dissimilarities. If the data point i is chosen to be a representative of a data point j with probability
zij , the cost of encoding j with i is dij zij ∈ [0, dij ]. Hence, the total cost of encoding j using all
i=1 dij zij . Second, we would like to have as few representatives as possible
for all the data points. When the data point i is a representative of some of the data points, we have
z i (cid:54)= 0, i.e., the i-th row of Z is nonzero. Having a few representatives then corresponds to having
a few nonzero rows in the matrix Z . Putting these two goals together, we consider the following
N(cid:88)
N(cid:88)
N(cid:88)
N(cid:88)
minimization program
I((cid:107)z i (cid:107)q )
s. t. zij ≥ 0, ∀i, j ;
zij = 1, ∀j,
i=1
i=1
i=1
j=1
where I(·) denotes the indicator function, which is zero when its argument is zero and is one other-
wise. The ﬁrst term in the objective function corresponds to the total cost of encoding all data points
using the representatives and the second term corresponds to the cost associated with the number
of the representatives. The parameter λ > 0 sets the trade-off between the two terms. Since the
minimization in (3) that involves counting the number of nonzero rows of Z is, in general, NP-hard,
N(cid:88)
N(cid:88)
N(cid:88)
N(cid:88)
we consider the following standard convex relaxation
s. t. zij ≥ 0, ∀i, j ;
(cid:107)z i (cid:107)q
j=1
i=1
i=1
i=1
where, instead of counting the number of nonzero rows of Z , we use the sum of the (cid:96)q -norms of the
rows of Z . Typically, we choose q ∈ {2, ∞} for which the optimization program (4) is convex.2
Note that the optimization program (4) can be rewritten in the matrix form as
where tr(·) denotes the trace operator, (cid:107)Z (cid:107)1,q (cid:44) (cid:80)N
s. t. Z ≥ 0, 1(cid:62)
Z ) + λ(cid:107)Z (cid:107)1,q
(cid:62)
Z = 1(cid:62) ,
(5)
min tr(D
i=1 (cid:107)z i (cid:107)q , and 1 denotes an N -dimensional
vector whose elements are all equal to one.
2 It is typically the case that q = ∞ favors having 0 and 1 elements for Z , while q = 2 allows elements that
more often take other values in [0, 1]. Note that q = 1 also imposes sparsity in the nonzero rows of Z , which
is not desirable since it promotes only a few data points to be associated with each representative.

zij = 1, ∀j,

dij zij + λ

min

dij zij + λ

(2)

(3)

(4)

min

3

−1012345−1−0.500.51Representatives for λ =0.002 λmax,2  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.005 λmax,2  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.01 λmax,2  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.1 λmax,2  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =1 λmax,2  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.007 λmax,∞  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.05 λmax,∞  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.1 λmax,∞  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =0.9 λmax,∞  data pointsrepresentatives−1012345−1−0.500.51Representatives for λ =1 λmax,∞  data pointsrepresentativesFigure 2: For the data points shown in Fig. 1, the matrix Z obtained by the proposed optimization program in
(4) is shown for several values of λ, where λmax,q is deﬁned in (6). Top: q = 2, Bottom: q = ∞.

As we change the regularization parameter λ in (4), the number of representatives found by the
algorithm changes. For small values of λ, where we put more emphasis on better encoding data
points via representatives, we obtain more representatives. In the limiting case of λ → 0 all points
are selected as representatives, each point being the representative of itself, i.e., z ii = 1 for all i.
On the other hand, for large values of λ, where we put more emphasis on the row-sparsity of Z ,
we select a small number of representatives. In the limiting case of λ → ∞, we select only one
representative for all data points. Figures 1 and 2 illustrate the representatives and the matrix Z ,
respectively, for several values of λ. In Section 3, we compute the range of λ for which the solution
of (4) changes from a single representative to all points being representatives. Note that, similar to
the relationship between sparse dictionary leaning [20] and Kmeans, there is a relationship between
our method and Kmedoids. A discussion of this is part of a future publication.
Once we have solved the optimization program (4), we can ﬁnd the representative indices from the
nonzero rows of Z . We can also obtain the clustering of data points into K clusters associated with
K representatives by assigning each data point to its closets representative. More speciﬁcally, if
i1 , · · · , iK denote the indices of the representatives, data point j is assigned to the representative
R(j ) according to R(j ) = argmin(cid:96)∈{i1 ,··· ,iK } d(cid:96)j . As mentioned before, the solution Z gives
the probability that each data point is associated with each one of the representatives, which also
provides a soft clustering of data points to the representatives. In Section 3 we show that when
there is a clustering of data points based on their dissimilarities (see Deﬁnition 1), each point selects
representatives from its own cluster.

3 Theoretical Analysis

In this section, we consider the optimization program (4) and study the behavior of its solution as
a function of the regularization parameter. First, we analyze the solution of (4) for a sufﬁciently
large value of λ. We obtain a threshold value on λ after which the solution of (4) remains the same,
selecting only one representative data point. More speciﬁcally, we show the following result.
Theorem 1 Consider the optimization program (4). Let (cid:96) (cid:44) argmini 1(cid:62)
di and
√
(cid:107)di − d(cid:96)(cid:107)1
· (cid:107)di − d(cid:96)(cid:107)2
N
λmax,∞ (cid:44) max
λmax,2 (cid:44) max
2
(di − d(cid:96) )
1(cid:62)
i(cid:54)=(cid:96)
i (cid:54)=(cid:96)
2
2
For q ∈ {2, ∞}, when λ ≥ λmax,q , the solution of the optimization program (4) is equal to Z =
e(cid:96)1(cid:62) , where e(cid:96) denotes the vector whose elements are all zero except its (cid:96)-th element, which is
equal to 1. In other words, the solution of (4) for λ ≥ λmax,q corresponds to choosing only the (cid:96)-th
data point as the representative of all the data points.

(6)

,

.

Note that the threshold value of the regularization parameter, for which we obtain only one repre-
sentative, is different for q = 2 and q = ∞. However, the two cases obtain the same representative
given by the data point for which 1(cid:62)
di is minimum, i.e., the data point with the smallest sum of

4

Z  matrix for λ =0.002 λmax,2  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.005 λmax,2  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.01 λmax,2  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.1 λmax,2  10203040506010203040506000.20.40.60.81Z  matrix for λ =1 λmax,2  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.007 λmax,∞  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.05 λmax,∞  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.1 λmax,∞  10203040506010203040506000.20.40.60.81Z  matrix for λ =0.9 λmax,∞  10203040506010203040506000.20.40.60.81Z  matrix for λ =1 λmax,∞  10203040506010203040506000.20.40.60.81Figure 3: Data points in two clusters with dissimilarities given by pairwise Euclidean distances. For λ <
∆ − max{δ1 , δ2 }, in the solution of the optimization program (4), points in each cluster are represented by
representatives from the same cluster.

dissimilarities to other data points. Notice also that when the dissimilarities are the Euclidean dis-
tances between the data points, the single representative corresponds to the data point closest to the
geometric median of all data points, as shown in the right plot of Figure 1.
When the regularization parameter λ is smaller than the threshold in (6), the optimization program
in (4) can ﬁnd multiple representatives for each data point. However, when there is a clustering
of data points based on their dissimilarities (see Deﬁnition 1), we expect to select representatives
from each cluster. In addition, we expect that the data points in each cluster be associated with the
representatives in that cluster only.
Deﬁnition 1 Given dissimilarities {dij }i,j=1,...,N between N data points, we say that the data par-
titions into n clusters {Ci }n
i=1 according to the dissimilarities, if for any data point j (cid:48) in any Cj , the
largest dissimilarity to other data points in Cj is strictly smaller than the smallest dissimilarity to
the data points in any Ci different from Cj , i.e.,
∀j = 1, . . . , n, ∀j (cid:48) ∈ Cj .
di(cid:48) j (cid:48) ,
di(cid:48) j (cid:48) < min
min
max
i(cid:48)∈Ci
i(cid:48)∈Cj
i(cid:54)=j
In other words, the data partitions into clusters {Ci}n
i=1 , when the interclass dissimilarity is smaller
than the intraclass dissimilarity.

(7)

Next, we show that for a suitable range of the regularization parameter that depends on the intraclass
and interclass dissimilarities, the probability that a point chooses representatives from other clusters
is zero. More precisely, we have the following result.
Theorem 2 Given dissimilarities {dij }i,j=1,...,N between N data points, assume that the data par-
titions into n clusters {Ci }n
i=1 according to Deﬁnition 1. Let λc be deﬁned as
di(cid:48) j (cid:48) − max
λc (cid:44) min
(8)
di(cid:48) j (cid:48) ).
(min
min
min
i(cid:48)∈Cj
i(cid:48)∈Ci
j (cid:48)∈Cj
i (cid:54)=j
j
Then for λ ≤ λc , the optimization program (4) ﬁnds representatives in each cluster, where the data
c ≤ λc on
points in every Ci select representatives only from Ci . A less tight clustering threshold λ(cid:48)
the regularization parameter is given by
λ(cid:48)
(cid:44) min
i (cid:54)=j
c

di(cid:48) j (cid:48) − max
i

min
i(cid:48)∈Ci ,j (cid:48)∈Cj

max
i(cid:48) (cid:54)=j (cid:48)∈Ci

di(cid:48) j (cid:48) .

(9)

The ﬁrst term in the right-hand-side of (9) shows the minimum dissimilarity between data points
in two different clusters. The second term in the right-hand-side of (9) shows the maximum, over
all clusters, of the dissimilarity between different data points in each cluster. When λc or λ(cid:48)
c in-
crease, e.g., when the intraclass dissimilarities increase or the interclass dissimilarities decrease, the
maximum possible λ for which we obtain clustering increases. As an illustrative example, con-
sider Figure 3, where data points are distributed in two clusters according to the dissimilarities
given by the pairwise Euclidean distances of the data points. Let δi denote the diameter of clus-
(cid:20)Z 1
(cid:21)
ter i and ∆ be the minimum distance among pairs of data points in different clusters. Assuming
max{δ1 , δ2} < ∆, for λ < ∆ − max{δ1 , δ2 }, the solution of the optimization program (4) is of the
, where Γ ∈ RN ×N is a permutation matrix corresponding to the separation
0
form Z = Γ
0 Z 2
of the data into the two clusters.

Remark 2 The results of Theorems 1 and 2 suggest that there is a range of the regularization
parameter for which we obtain only one representative from each cluster.
In other words, if

5

∆δ1δ2Figure 4: Number of representatives obtained by the proposed optimization program in (4) for data points in
the two clusters shown in Fig. 1 as a function of the regularization parameter λ = αλmax,q with q ∈ {2, ∞}.

Figure 5: Representatives and the probability matrix Z obtained by our proposed algorithm in (4) for q = ∞.
20 random data points are added to 120 data points generated by a mixture of 3 Gaussian distributions.

λmax,q (Ci ) denotes the threshold on λ after which we obtain only one representative from Ci , then
for maxi λmax,q (Ci ) ≤ λ < λc , the data points in each Ci select only one representative that is in
Ci . As we will show in the experiments, such an interval often exists and can, in fact, be large.
For a sufﬁciently small value of λ, where we put less emphasis in the row-sparsity term in the
optimization program (4), each data point becomes a representative, i.e., zii = 1 for all i. In such
a case, each data point forms its own cluster. From the result in Theorem 2, we obtain a threshold
λmin such that for λ ≤ λmin the solution Z is equal to the identity matrix.
Corollary 1 Let λmin,q (cid:44) minj (mini (cid:54)=j dij − dj j ) for q ∈ {2, ∞}. For λ ≤ λmin,q , the solution
of the optimization program (4) for q ∈ {2, ∞} is equal to the identity matrix. In other words, each
data point is the representative of itself.

4 Experiments
In this section, we evaluate the performance of the proposed algorithm on synthetic and real datasets.
As scaling of D and λ by the same value does not change the solution of (4), we always scale
dissimilarities to lie in [0, 1] by dividing the elements of D by its largest element. Unless stated
otherwise, we typically set λ = αλmax,q with α ∈ [0.01, 0.1], for which we obtain good results.
4.1 Experiments on Synthetic Data
We consider the synthetic dataset shown in Figure 1 that consists of data points distributed around
two clusters. We run the proposed optimization program in (4) for both q = 2 and q = ∞ for several
values of λ. Figures 1 and 2 show the representatives and the matrix of variables Z , respectively, for
several values of the regularization parameter. Notice that, as discussed before, for small values of λ,
we obtain more representatives and as we increase λ, the number of representatives decreases. When
the regularization parameter reaches λmax,q , computed using our theoretical analysis, we obtain
only one representative for the dataset. It is important to note that, as we showed in the theoretical
analysis, when the regularization parameter is sufﬁciently small, data points in each cluster only
select representatives from that cluster (see Figure 2), i.e., Z has a block-diagonal structure when
its columns are permuted according to the clusters. Moreover, as Figure 2 shows, for a sufﬁciently
large range of the regularization parameter, we obtain only one representative from each cluster. To
better see this, we run the optimization program with λ = αλmax,q for different values of α. The
two left-hand side plots in Figure 4 show the number of the representatives for q = 2 and q = ∞,
respectively, from each of the two clusters.
As shown, when λ gets larger than λmax,q , we obtain only one representative from the right cluster
and no representative from the left cluster, i.e., as expected, we obtain one representative for all
the data points. Also, when λ gets smaller than λmin,q , all data points become representatives, as

6

10−410−2100051015202530αNumber of Representativesq = 2 , ∆ / δ =1.1  Left clusterRight cluster10−410−2100051015202530αNumber of Representativesq = ∞ , ∆ / δ =1.1  Left clusterRight cluster10−410−2100051015202530αNumber of Representativesq = 2 , ∆ / δ =4  Left clusterRight cluster10−410−2100051015202530αNumber of Representativesq = ∞ , ∆ / δ =4  Left clusterRight cluster−1.5−1−0.500.511.5−10123Representatives for λ =0.05 λmax,∞  data pointsrepresentativesZ  matrix for λ =0.05 λmax,∞  204060801001201402040608010012014000.20.40.60.81−1.5−1−0.500.511.5−10123Representatives for λ =0.5 λmax,∞  data pointsrepresentativesZ  matrix for λ =0.5 λmax,∞  204060801001201402040608010012014000.20.40.60.81Figure 6: Classiﬁcation error on the USPS (left) and ISOLET (right) datasets using representatives obtained
by different algorithms. Horizontal axis shows the percentage of the selected representatives from each class
(averaged over all classes). Dashed line shows the classiﬁcation error (%) using all the training samples.

expected from our theoretical result. It is also important to note that, for a sufﬁciently large range of
the values of λ, we select only one representative from each cluster. The two right-hand side plots
in Figure 4 show the number of the representatives when we increase the distance between the two
clusters. Notice that we obtain similar results as before except that the range of λ for which we
select one representative from each cluster has increased. This is also expected from our theoretical
analysis, since λc in (8) increases as the distance between the two clusters increases.
Note that we also obtain similar results for larger number of clusters. For better visualization, we
have shown the results for only two clusters. Also, when there is not a clear partitioning of the data
points into clusters according to Deﬁnition 1, e.g., when there are data points distributed between
different clusters, as shown in Figure 5, we still obtain similar results to what we have discussed
in our theoretical analysis. This suggests the existence of stronger theoretical guarantees for our
proposed algorithm, which is the subject of our future work.

4.2 Experiments on Real Data
In this section, we evaluate the performance of our proposed algorithm on real image and text data.
We report the result for q = ∞ as it typically obtains better results than q = 2.

4.2.1 NN Classiﬁcation using Representatives

First, we consider the problem of ﬁnding prototypes for classiﬁcation using the nearest neighbor
(NN) algorithm [15]. Finding representatives that correspond to the modes of the data distribution
helps to signiﬁcantly reduce the computational cost and memory requirements of classiﬁcation al-
gorithms, while maintaining their performance. To investigate the effectiveness of our proposed
method for ﬁnding informative prototypes for classiﬁcation, we consider two datasets of USPS [21]
and ISOLET [22]. We ﬁnd the representatives of the training data in each class of a dataset and use
the representatives as a reduced training set to perform NN classiﬁcation on the test data. We obtain
the representatives by taking dissimilarities to be pairwise Euclidean distances between data points.
We compare our proposed algorithm with AP [4], Kmedoids [2], and random selection of data points
(Rand) as the baseline. Since Kmedoids depends on initialization, we run the algorithm 1000 times
with different random initializations and report the results corresponding to the best solution (lowest
energy) and the worst solution (highest energy) as Kmedoids-w and Kmedoids-b, respectively. To
have a fair comparison, we run all algorithms so that they obtain the same number of representatives.
Figure 6 shows the average classiﬁcation errors using the NN method for the two datasets. The
classiﬁcation error using all training samples of each dataset is also shown with a black dashed
line. As the results show, the classiﬁcation performance using the representatives found by our
proposed algorithm is close to that of using all the training samples. Speciﬁcally, in the USPS
dataset, using representatives found by our proposed method, which consist of only 16% of the
training samples, we obtain 6.2% classiﬁcation error compared to 4.7% error obtained using all the
training samples. In the ISOLET dataset, with representatives corresponding to less than half of the
training samples, we obtain very close classiﬁcation performance to using all the training samples
(12.4% error compared to 11.4% error). Notice that when the number of representatives decreases,
as expected, the classiﬁcation performance also decreases. However, in all cases, our proposed
algorithm as well as AP are less affected by the decrease in the number of the representatives.

7

4%16%0510152025Percentage of Selected Training SamplesClassification Error (%)USPS Dataset  RandKmedoids−wKmedoids−bAPProposed20%40%0510152025Percentage of Selected Training SamplesClassification Error (%)ISOLET Dataset  RandKmedoids−wKmedoids−bAPProposedFigure 7: Some frames of a political debate video, which consists of multiple shots, and the automatically
computed representatives (inside red rectangles) of the whole video sequence using our proposed algorithm.

4.2.2 Video Summarization using Representatives

We now evaluate our proposed algorithm for ﬁnding representative frames of video sequences. We
take a political debate video [7], downsample the frames to 80 × 100 pixels, and convert each frame
to a grayscale image. Each data point then corresponds to an 8000-dimensional vector obtained by
vectorizing each grayscale downsampled frame. We set the dissimilarities to be the Euclidean dis-
tances between pairs of data points. Figure 7 shows some frames of the video and the representatives
computed by our method. Notice that we obtain a representative for each shot of the video. It is
worth mentioning that the computed representatives do not change for λ ∈ [2.68, 6.55].

4.2.3 Finding Representative Sentences in Text Documents

As we discussed earlier, our proposed algorithm can deal with dissimilarities that are not necessarily
metric, i.e., can be asymmetric or violate the triangle inequality. We consider now an example of
asymmetric dissimilarities where we ﬁnd representative sentences in the text document of this pa-
per. We compute the dissimilarities between sentences using an information theory-based criterion
as follows [4]: we treat each sentence as a “bag of words” and compute dij (how well sentence i
represents sentence j ) based on the sum of the costs of encoding every word in sentence j using the
words in sentence i. More precisely, for sentences in the text of the paper, we extract the words de-
limited by spaces, we remove all punctuations, and eliminate words that have less than 5 characters.
For each word in sentence j , if the word matches3 a word in sentence i, we set the encoding cost for
the word to the logarithm of the number of words in sentence i, which is the cost of encoding the
index of the matched word. Otherwise, we set the encoding cost for the word to the logarithm of the
number of the words in the text dictionary, which is the cost of encoding the index of the word in all
the text. We also compute dii using the same procedure, i.e., dii (cid:54)= 0, which penalizes selecting very
long sentences. We found that 96% of the dissimilarities are asymmetric. The four representative
sentences obtained by our algorithm summarize the paper as follows:
–Given pairwise dissimilarities between data points, we consider the problem of ﬁnding a subset of data points,
called representatives or exemplars, that can efﬁciently describe the data collection.
–We obtain the range of the regularization parameter for which the solution of the proposed optimization pro-
gram changes from selecting one representative for all data points to selecting all data points as representatives.
–When there is a clustering of data points, deﬁned based on their dissimilarities, we show that, for a suitable
range of the regularization parameter, the algorithm ﬁnds representatives from each cluster.
–As the results show, the classiﬁcation performance using the representatives found by our proposed algorithm
is close to that of using all the training samples.

Acknowledgment

E. Elhamifar and R. Vidal are supported by grants NSF CNS-0931805, NSF ECCS-0941463, NSF
OIA-0941362, and ONR N00014-09-10839. G. Sapiro acknowledges partial support by ONR,
DARPA, NSF, NGA, and AFOSR grants.

3We consider a word to match another word, if either word is a substring of the other.

8

References
[1] S. Garcia, J. Derrac, J. R. Cano, and F. Herrera, “Prototype selection for nearest neighbor classiﬁca-
tion: Taxonomy and empirical study,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 34, no. 3, pp. 417–435, 2012.
[2] L. Kaufman and P. Rousseeuw, “Clustering by means of medoids,” In Y. Dodge (Ed.), Statistical Data
Analysis based on the L1 Norm (North-Holland, Amsterdam), pp. 405–416, 1987.
[3] M. Gu and S. C. Eisenstat, “Efﬁcient algorithms for computing a strong rank-revealing qr factorization,”
SIAM Journal on Scientiﬁc Computing, vol. 17, pp. 848–869, 1996.
[4] B. J. Frey and D. Dueck, “Clustering by passing messages between data points,” Science, vol. 315, pp.
972–976, 2007.
[5] J. A. Tropp, “Column subset selection, matrix factorization, and eigenvalue optimization,” ACM-SIAM
Symp. Discrete Algorithms (SODA), pp. 978–986, 2009.
[6] C. Boutsidis, M. W. Mahoney, and P. Drineas, “An improved approximation algorithm for the column
subset selection problem,” in Proceedings of SODA, 2009, pp. 968–977.
[7] E. Elhamifar, G. Sapiro, and R. Vidal, “See all by looking at a few: Sparse modeling for ﬁnding represen-
tative objects,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012.
[8] J. Bien, Y. Xu, and M. W. Mahoney, “CUR from a sparse optimization viewpoint,” NIPS, 2010.
[9] T. Chan, “Rank revealing QR factorizations,” Lin. Alg. and its Appl., vol. 88-89, pp. 67–82, 1987.
[10] L. Balzano, R. Nowak, and W. Bajwa, “Column subset selection with missing data,” in NIPS Workshop
on Low-Rank Methods for Large-Scale Machine Learning, 2010.
[11] E. Esser, M. Moller, S. Osher, G. Sapiro, and J. Xin, “A convex model for non-negative matrix factoriza-
tion and dimensionality reduction on physical space,” IEEE Transactions on Image Processing, vol. 21,
no. 7, pp. 3239–3252, 2012.
[12] M. Charikar, S. Guha, A. Tardos, and D. B. Shmoys, “A constant-factor approximation algorithm for the
k-median problem,” Journal of Computer System Sciences, vol. 65, no. 1, pp. 129–149, 2002.
[13] B. J. Frey and D. Dueck, “Mixture modeling by afﬁnity propagation,” Neural Information Processing
Systems, 2006.
[14] I. E. Givoni, C. Chung, and B. J. Frey, “Hierarchical afﬁnity propagation,” Conference on Uncertainty in
Artiﬁcial Intelligence, 2011.
[15] R. Duda, P. Hart, and D. Stork, Pattern Classiﬁcation. Wiley-Interscience, October 2004.
[16] D. Dueck and B. J. Frey, “Non-metric afﬁnity propagation for unsupervised image categorization,” Inter-
national Conference in Computer Vision, 2007.
[17] N. Lazic, B. J. Frey, and P. Aarabi, “Solving the uncapacitated facility location problem using message
passing algorithms,” International Conference on Artiﬁcial Intelligence and Statistics, 2007.
[18] R. Jenatton, J. Y. Audibert, and F. Bach, “Structured variable selection with sparsity-inducing norms,”
Journal of Machine Learning Research, vol. 12, pp. 2777–2824, 2011.
[19] J. A. Tropp., “Algorithms for simultaneous sparse approximation. part ii: Convex relaxation,” Signal
Processing, special issue “Sparse approximations in signal and image processing”, vol. 86, pp. 589–602,
2006.
[20] M. Aharon, M. Elad, and A. M. Bruckstein, “K-SVD: An algorithm for designing overcomplete dic-
tionaries for sparse representation,” IEEE Trans. on Signal Processing, vol. 54, no. 11, pp. 4311–4322,
2006.
[21] J. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 16, no. 5, pp. 550–554, 1994.
[22] M. Fanty and R. Cole, “Spoken letter recognition,” in Neural Information Processing Systems, 1991.

9


Author Identiﬁcation on Twitter

Antonio Castro
antonio.alfredo.castro@gmail.com

Brian Lindauer
brian@shendauer.com

I . IN TRODUC T ION
As of June 2012, Twitter had 500M users, 140M of
whom are in the United States. These users, especially those
outside of the United States, may assume that they have a
certain level of anonymity among this sea of tweets. Our
project investigates whether the identity of an anonymous
Twitter user can, in fact, be uncovered using only linguistic
stylometry. Authorship recognition is a very well-studied
domain, but the scale is almost always limited to no more
than a few hundred authors. Narayanan, et al. [2] study
authorship recognition at Internet scale, by looking at the
characteristics of different classiﬁers when applied to a
corpus of approximately 100k blogs. We set out to answer
the question of whether similar results could be achieved
on tweets rather than blogs, despite their much shorter
length. Starting with the set of features, classiﬁers, and
normalization methods that yielded the best results over blog
data in [2], we adapt them to Twitter data and measure the
results.

I I . DATA CO LL EC T ION
Our set of training data spans more than 800 users, most
having at least 1000 tweets per user. This provides enough
data to our classiﬁers despite the sparsity of the derived
features and the limitations imposed on the length of a tweet.
Our data set also includes a number of Twitter accounts
where we have prior knowledge that they are authored by
the same user as at least one other account in the set.
Table I describes the various sources we reviewed and
how we ended up using the data. We initially utilized a web
site called discovertext.com to follow the desired accounts
and begin collecting data. However, the service is insufﬁcient
for our needs because it fails to capture information about
retweets and is unable to provide historical tweet data. As
a result, we utilize the Twitter API directly to gather the
last 1000 tweets of each of the users identiﬁed in Table I.
Primarily because of Twitter rate limit limitations, the data
gathering requires several days to complete.
Our experimental methodology requires that we identify
a collection of Twitter account pairs where one author is re-
sponsible for both accounts. Our primary methods for iden-
tifying those accounts where issuing a request to employees
of Dell with ofﬁcial Dell Twitter accounts, and using Google
to search for phrases indicating multiple Twitter accounts.

Source
Klout

Twitter Firehose

Desired Usage
Top thousand klout
users as a source of a
variety of users with
rich content.
Obtain tons of tweets
across
a massive
number of users.

Twitaholic

Dell Solicitation

Web Search

Google Plus Proﬁle
Scrape

of
who
two
Twitter

Provides a top 1000
most followed list of
Twitter users.
Obtain
a
list
employees
maintain
separate
accounts.
Search for phrases
such as also follow
me at on Twitter pro-
ﬁle pages.

Discover users that
report having multi-
ple Twitter feeds to
be followed.

Table I
DATA SOURC E S

Actual Usage
Not viable for use.
lists of
Only small
top Klout
rankings
are published.
Not viable for use.
the data
While
is
broad,
it does not
provide us with a
long enough history
of
any
individual
user to populate our
sparse set of features.
Provides the basis of
our twitter users to
gather data from.
set of
Provides
a
known Twitter
ac-
counts that are au-
thored by the same
person.
set of
Provides
a
known Twitter
ac-
counts that are au-
thored by the same
person.
Adds to set of Twitter
accounts with known
authors.

We also obtained some meta-information about Google Plus
proﬁles from the authors of ”How Unique and Traceable are
Usernames?.” [4] and used that to target a crawl of Google
Plus proﬁle data. This produced an additional set of account
pairs. After eliminating non-English feeds, feeds containing
only links, etc., we are left with 58 labeled accounts from
27 different authors.
Due to the small number of valid account pairs in which
we have prior knowledge of the account being authored by
the same user, we simulate dual authorship by splitting each
feed 70/30 and performing cross-validation and measuring
the error. Since this measures error against the same feed,
rather than another feed by the same author, it is not ideal,

but in most cases, it should approximate a lower bound,
and as such it gives us a good indication of the algorithm’s
general performance in de-anonymizing tweets.
Additionally, some accounts exhibit
issues that might
introduce confounds into the experiment. Examples of these
include foreign languages and tweets automatically gener-
ated by applications, such as Foursquare. We do not incor-
porate any cleansing of these issues in our experiments other
than the ﬁnal test we performed using the known accounts
with duplicate authors. For those accounts, we manually
review their feeds to remove any feeds that contained any
obvious issues.
The results in this paper are based on a collection of 844
Twitter streams containing approximately 777k total tweets.
These include the 27 sets of accounts that are known to be
maintained by the same author.

I I I . F EATUR E S
Our selection of features is inspired by Narayanan [2],
Writeprints [1], and Ireland [3]. It is, in fact, mostly a
subset of the Narayanan features. These features aim to
focus on the style of the tweet rather than its topic. So, for
example, we speciﬁcally look at function/stop words rather
than ignoring them and focusing on words with large TF.IDF
scores. In adapting Narayanan’s previous work to Twitter, we
add several Twitter-speciﬁc features, described in Table II.

Category
Length
Word shape

Word length

Character frequencies

Unicode
Function/stop words

Twitter conventions
Retweets

Description
words/characters per post
frequency of words in uppercase,
lowercase, capitalized, camelcase,
and other capitalization schemes
histogram of word lengths from 1-
20
frequency of letters a-z (ignoring
case), digits, and many ASCII sym-
bols
frequency of non-ASCII characters
frequency of words like “the”, “of ”,
and “then”
existence of “RT” or “MT”
whether the post is an exact retweet,
or a modiﬁed retweet

Count
2
5

20

68

1
293

2
2

Table II
F EATUR E S U S ED , ADA P TED FROM [2 ]

We extract all 393 of these features using a Ruby script,
and store them in CSV for ingestion by the classiﬁer
programs, which are implemented in MATLAB. Extracting
these features presents no notable challenges in scalability
or algorithmic complexity.
We were curious to know which of these features has the
most impact on our classiﬁcation accuracy. To ﬁnd out, we

adoped the deﬁnition of information gain used in Narayanan.
That is,
IG(Fi ) = H (T ) − H (T |Fi ) = H (T ) + H (Fi ) − H (T , Fi )

where H is the Shannon Entropy, T is the random variable
for the Twitter account number, and Fi
is the random
variable for feature i [2]. By using the same information
gain metric as Narayanan, we are also able to compare the
effective features in detecting blog authorship to those in
detecting Twitter authorship. The most inﬂuential features
for our classiﬁer are listed in Table III.

Feature
Freq. of non-ASCII characters
Number of words per tweet
Freq. of all lowercase words
Freq. of o
Number of characters per tweet
Freq. of a
Freq. of t
Freq. of e
Freq. of .
Freq. of words with only the ﬁrst letter capitalized
Freq. of h
Freq. of n
Freq. of i
Freq. of r
Freq. of @

Information gain (bits)
0.60209
0.46262
0.3711
0.35851
0.3576
0.32181
0.31425
0.29231
0.29063
0.28206
0.25791
0.25051
0.24
0.2162
0.21448

Table III
F EATUR E S W I TH H IGH E ST IN FORMAT ION GA IN

Comparing this list with Narayanan’s top 10 features for
blogs, we see that both ﬁnd the length of the post and the
capitalization style to be highly discriminative. However, the
most discriminative individual characters are different for
blog posts and tweets. Narayanan found that apostrophes,
periods, and commas were the most important characters
for blogs, while we found that o, a, t, e, and period were
the most
important for tweets. This result was initially
puzzling, but we soon realized that all of the characters
comprising http:// are near the top of our list. It may be
that vowels are highly ranked becuase they help distinguish
tweets consisting primarily of English words from those
containing mostly URLs.
Far and away, our most inﬂuential feature is the frequency
of non-ASCII characters. Some Twitter users routinely in-
clude unicode in their tweets, while others never do. Since
we are nominally ﬁltering out non-English accounts, these
unicode characters are mostly special characters, such as

hearts and smilies, rather than characters in non-English
words.
Finally, we note that the frequency of “@” is highly
ranked, but not in our top ten. This character is used on
Twitter to denote a reference to another Twitter feed. It far
outperforms our more Twitter-speciﬁc features, such as the
presence of “RT” (0.1 bits), or whether a retweet includes
an exact/inexact copy of the original tweet (0.06 and 0.04
bits, respectively).

IV. C LA S S I FIER S
Because Narayanan, et al. reported their best results with
a combination of nearest neighbors (NN) and regularized
least squares classiﬁcation (RLSC), we implement
these
classiﬁers and run them against the data.
Initially focusing on NN, we implement the variation
described by Narayanan along with the normalization pro-
cedure from that same paper. Rather than keeping all data
points in memory, which would be very expensive consid-
ering the number of Twitter users, we compute one centroid
for each Twitter account. To do this, we read in all extracted
tweet features from all users, then normalize by both column
and row. First, each column value is normalized by the mean
of the non-zero values in that column. Then, each row value
is divided by the norm of that row.
At prediction time, we read the extracted features of
each tweet in the test stream. For each of those tweets,
we measure the Euclidean distance to each of the centroids
computed in training. Then we take the sum of the distances
of all the tweets to each of the centroids and rank the cen-
troids by their average nearness to a tweet in the test stream.
We ask whether the account by the same author appears in
the top N% of the ranking, including whether it is ranked
ﬁrst. We also measure generalization error using 70/30 cross
validation on tweets from the same account. In computing
both training error and cross validation generalization error,
we count a prediction as correct only if the correct account
appeared ﬁrst in the ranked list. With our training and cross
validations set, NN yields a 0.61% training error and a
2.5% generalization error. This generalization accuracy is
surprising considering the relative simplicity and lack of
domain speciﬁcity in our featureset. In fact, the feature set
is composed largely of one-grams and function words.
Though our sample size is small, we are able to use our
58 labeled accounts to get a measure of accuracy when the
algorithm is applied to our intended use case. Given one of
the accounts in the pair, the NN classiﬁer ranks the other
account ﬁrst 29% of the time – an error rate of 71%. But
the correct author is ranked at least 2nd 38% of the time.
And they appear in the top 10% of the ranking over 70%
of the time. Figure 1 shows the cumulative distribution over
these rankings.
This NN algorithm is relatively fast, since it only needs
to compare each test point to one centroid for each training

Figure 1. Cumulative distribution of percentile rank over test examples

class. Because these tests are independent and lightweight,
we are able to utilize MATLAB’s parallel computing capa-
bilities and process the entire dataset in minutes.
We implement the Regularized Least Square Classiﬁer
algorithm starting with the original closed form solution as
described in Rifkin’s paper [5].
W = (X T X + λI )−1X T (cid:126)y
Given the size of the training set of approximately 500,000
tweets in our design matrix X, and the multiclass {-1,
+1} convention Y matrix with 844 classiﬁer vectors the
algorithm requires approximately 12 hours to complete for
each classiﬁer vector. However, we found that the training
error performance is not affected by converting to a {0,1}
convention and we are able to complete the entire classiﬁer
calculation using sparse matrices in MATLAB in a single
step
W = (X T X + λI )−1X T Y
within a few minutes.
As a matter of convention, we classify each tweet as either
a positive or negative by determining the maximum value
of each linear classiﬁer method for a single tweet, setting
that classifer to one and the others to zero. We used the
frequency of a classiﬁer being chosen across the sample set
as an aggregate to determine conﬁdence accross a sampling
of tweets. However, this result does not vary notably from
using the sum of the raw values produced by each tweet.
Similar to the results in the Narayanan paper, the resulting
training error of this One Vs. All (OVA) method, ranges
between 20% and 30% which is more than an order of
magnitude worse than our NN classiﬁer. This is a known
problem for OVA classiﬁers with heavily skewed data. For
each label in our data set, the RLSC generally has a thousand
positive examples compared to nearly half a million negative
examples. We explored a handful of methods to level the

data as the algorithm. Rescaling the alone is not an option
as the algorithm is invariant to rescaling of the input data.
We try a few methods [7] to reduce this error by including
pruning [6] negative examples for each OVA being trained
and artiﬁcally creating like samples for training. However,
we ﬁnd that these methods only improved the classiﬁcation
error by only 5 − 7%.
Inspired by the penalization concept in the Narayanan
paper, we set out to modify the cost function so that it
penalizes false negatives proportionally more heavliy than
false positives. The closed form solution for appropriatley
balancing the ratio of positive and negative examples is as
follows. We let Φj be an m × m diagonal matrix where Φjii
is equal to the ratio of negative to positive examples in rows
corresponding to a positive example in classiﬁer j , and 1’s
(cid:80)
for all other diagonal values.
(cid:80)
More formally:
i 1{yji = 0}
i 1{yji = 1}
wj =
Φjii = wj yi + (1 − yi )
Using these classiﬁer speciﬁc Φj and (cid:126)yj values, we can
derive θj for each classiﬁer j in closed form as follows.

(X θj − (cid:126)yj )T Φj (X θj − (cid:126)yj ) + λ||θj ||2
1
J (θj ) =
2
∇θj J (θj ) = X T Φj X θj − X T Φj (cid:126)yj + 2λθj = 0
θj = (X T Φj X + 2λI )−1X T Φj (cid:126)yj
Using this closed form requires the generation of Φ ∈
(cid:60)500kx500k for each of the 844 classiﬁers and requires
iterating through each classiﬁer independently. Due to the
memory requirements of this algorithm in MATLAB, even
using sparse matrices, this closed form algorithm is difﬁcult
to parallelize beyond a handful of threads and thus still
required several hours to complete. However, with these
results we are able to reduce our training error to 1.1%
and our generalization error to 4.7%. Most importantly, the
algorithm performs signiﬁcantly better than NN on the set
of known Twitter dual accounts, classifying 41% of them
correctly. Comparing RLSC to NN in Figure 1, we see that
RLSC outperforms NN at every threshold.
In accomplishing the above results, we do not experiment
much with the regularization paramater λ. Our choice of
lambda is the minimum order of magnitude that did not re-
sult in the classiﬁers calculation resulting in nearly singular
matrices for the algorithm, which, in this case was 10−9 .
We minimally explore methods to improve the performance
and memory requirements of the algorithm without notable
impact, but do not explore methods that do not require large
matrix operations such as conjugate descent and dynamic
programming as referenced by Naranayan and Rifkin [5]
which may be necessary to scale signiﬁcantly beyond the
number of users we are working with in this experiment.

V. R ECOMM ENDAT ION S
Given the prominant use of Twitter by political dissidents,
it’s alarming to learn that the author of an “anonymous” feed
might be identiﬁed with 40% accuracy based only publicly
viewable information. By using other information, such as
the time of posts and IP access logs, the identiﬁcation rate is
likely to be much higher. However, having examined many
of the failed matches, we note that evasion is simple, as long
as the author is aware of these risks. In several of the failed
matches, one of the accounts is written in a deliberately
different voice. In one case, the author tweets in the voice
of his dog. In other cases, the author of limits the amount
of commentary in the second feed and mostly posts URLs.
By following these examples and either deliberately altering
their writing voice, or limiting the amount of text posted,
Twitter users can help maintain their anonymity.

V I . FU TUR E WORK
Given the initial promising results, there is opportunity
for continued work in testing classiﬁer accuracy and per-
formance at larger scales, as well as in several other areas,
including:
• Expanding the exploration of stylometry features in
previous work to further impact generalzation error.
• Collecting a broader set of known account pairs and/or
identifying a list of potential pairs to be conﬁrmed.
• Exploring conjugate descent, dynamic programming, or
stochastic methods to improve the speed and memory
usage of the RLSC algorithm.
• Exploring ensemble learning to combine RLSC and NN
algorithms and its impact on generalization error.
• Applying these methods on a much larger number of
accounts (e.g. 1,000,000 twitter accounts).
• Removing language and application data that may be
introducing confounds.

V I I . CONC LU S ION
We are encouraged by our results. The classiﬁers both
exhibit excellent accuracy in cross validation, and do fairly
well in the general use case where the test stream comes
from a different Twitter account by the same author. We
expected less accuracy from Twitter than from the blog
or email data used in previous work because the size of
the tweets are signiﬁcantly smaller in comparison. With the
small number of labeled examples in our data set, we cannot
make a judgement about whether performance was better or
worse on Twitter vs. blog data, but we can conclude that
stylometric analysis does, in fact, perform well on tweets.

R E FER ENC E S
[1] Abbasi, Ahmed, and Hsinchun Chen. ”Writeprints: A stylo-
metric approach to identity-level identiﬁcation and similarity
detection in cyberspace.” ACM Transactions on Information
Systems 26.2 (2008): 7.

[2] Narayanan, Arvind, et al. ”On the feasibility of internet-scale
author identiﬁcation.” Security and Privacy (SP), 2012 IEEE
Symposium on. IEEE, 2012.

[3] Ireland, M.E., & Pennebaker, J.W. (2010). Language style
matching in writing: Synchrony in essays, correspondence, and
poetry. Journal of Personality and Social Psychology, 99.

[4] Perito, Daniele, et al. ”How unique and traceable are
usernames?.” Privacy Enhancing Technologies. Springer
Berlin/Heidelberg, 2011.

[5] Rifkin, Ryan, Gene Yeo, and Tomaso Poggio. ”Regularized
least-squares classiﬁcation.” Nato Science Series Sub Series
III Computer and Systems Sciences 190 (2003): 131-154.

[6] Ofer Dekel Ohad Shamir. ”Multiclass-Multilabel Classiﬁcation
with More Classes than Examples.” Proceedings of the 13th in-
ternational Conference on Artiﬁcial Intelligence and Statistics
(AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume
9 of JMLR: W&CP 9.

[7] Schapire, Robert E., and Yoram Singer. ”BoosTexter: A
boosting-based system for text categorization.” Machine learn-
ing 39.2 (2000): 135-168.


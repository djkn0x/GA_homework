Trajectory-Based Short-Sighted Probabilistic
Planning

Manuela M. Veloso
Felipe W. Trevizan
Machine Learning Department
Computer Science Department
Carnegie Mellon University - Pittsburgh, PA
{fwt,mmv}@cs.cmu.edu

Abstract

Probabilistic planning captures the uncertainty of plan execution by probabilisti-
cally modeling the effects of actions in the environment, and therefore the proba-
bility of reaching different states from a given state and action. In order to compute
a solution for a probabilistic planning problem, planners need to manage the un-
certainty associated with the different paths from the initial state to a goal state.
Several approaches to manage uncertainty were proposed, e.g., consider all paths
at once, perform determinization of actions, and sampling. In this paper, we in-
troduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs),
a novel approach to manage uncertainty for probabilistic planning problems in
which states reachable with low probability are substituted by arti ﬁcial goals that
heuristically estimate their cost to reach a goal state. We also extend the theoretical
results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP
always ﬁnishes and is asymptotically optimal under sufﬁcie
nt conditions on the
structure of short-sighted SSPs. We empirically compare SSiPP using trajectory-
based short-sighted SSPs with the winners of the previous probabilistic planning
competitions and other state-of-the-art planners in the triangle tireworld problems.
Trajectory-based SSiPP outperforms all the competitors and is the only planner
able to scale up to problem number 60, a problem in which the optimal solution
contains approximately 1070 states.

1

Introduction

The uncertainty of plan execution can be modeled by using probabilistic effects in actions, and
therefore the probability of reaching different states from a given state and action. This search space,
deﬁned by the probabilistic paths from the initial state to a goal state, challenges the scalability of
planners. Planners manage the uncertainty by choosing a search strategy to explore the space. In
this work, we present a novel approach to manage uncertainty for probabilistic planning problems
that improves its scalability while still being optimal.

One approach to manage uncertainty while searching for the solution of probabilistic planning prob-
lems is to consider the complete search space at once. Examples of such algorithms are value
iteration and policy iteration [2]. Planners based on these algorithms return a closed policy, i.e., a
universal mapping function from every state to the optimal action that leads to a goal state. Assum-
ing the model correctly captures the cost and uncertainty of the actions in the environment, closed
policies are extremely powerful as their execution never “f ails,” and the planner does not need to
be re-invoked ever. Unfortunately the computation of such policies is prohibitive in complexity as
problems scale up. Value iteration based probabilistic planners can be improved by combining asyn-
chronous updates and heuristic search [3–7]. Although thes e techniques allow planners to compute
compact policies, in the worst case, these policies are still linear in the size of the state space, which
itself can be exponential in the size of the state or goals.

1

Another approach to manage uncertainty is basically to ignore uncertainty during planning, i.e., to
approximate the probabilistic actions as deterministic actions. Examples of replanners based on
determinization are FF-Replan [8], the winner of the ﬁrst In ternational Probabilistic Planning Com-
petition (IPPC) [9], Robust FF [10], the winner of the third IPPC [11] and FF-Hindsight [12, 13].
Despite the major success of determinization, this simpli ﬁ cation in the action space results in algo-
rithms oblivious to probabilities and dead-ends, leading to poor performance in speci ﬁc problems,
e.g., the triangle tireworld [14].

Besides the action space simpli ﬁcation, uncertainty manag ement can be performed by simplifying
the problem horizon, i.e., look-ahead search [15]. Based on sampling, the Upper Conﬁdence bound
for Trees (UCT) algorithm [16] approximates the look-ahead search by focusing the search in the
most promising nodes.

The state space can also be simpli ﬁed to manage uncertainty i n probabilistic planning. One example
of such approach is Envelope Propagation (EP) [17]. EP computes an initial partial policy π and
then prunes all the states not considered by π . The pruned states are represented by a special meta
state. Then EP iteratively improves its approximation of the state space. Previously, we introduced
short-sighted planning [1], a new approach to manage uncertainty in planning problems: given a
state s, only the uncertainty structure of the problem in the neighborhood of s is taken into account
and the remaining states are approximated by arti ﬁcial goal s that heuristically estimate their cost to
reach a goal state.

In this paper, we introduced trajectory-based short-sighted Stochastic Shortest Path Problems
(SSPs), a novel model to manage uncertainty in probabilistic planning problems. Trajectory-based
short-sighted SSPs manage uncertainty by pruning the state space based on the most likely trajectory
between states and deﬁning arti ﬁcial goal states that guide
the solution towards the original goal. We
also contribute by deﬁning a class of short-sighted models a nd proving that the Short-Sighted Proba-
bilistic Planner (SSiPP) [1] always terminates and is asymptotically optimal for models in this class
of short-sighted models.

The remainder of this paper is organized as follows: Section 2 introduces the basic concepts and
notation. Section 3 deﬁnes formally trajectory-based shor t-sighted SSPs. Section 4 presents our
new theoretical results for SSiPP. Section 5 empirically evaluates SSiPP using trajectory-based short-
sighted SSPs with the winners of the previous IPPCs and other state-of-the-art planner. Section 6
concludes the paper.

2 Background

A Stochastic Shortest Path Problem (SSP) is deﬁned by the tup le S = hS, s0 , G, A, P , C i, in which
[1, 18]: S is the ﬁnite set of state; s0 ∈ S is the initial state; G ⊆ S is the set of goal states; A is
the ﬁnite set of actions; P (s′ |s, a) represents the probability that s′ ∈ S is reached after applying
action a ∈ A in state s ∈ S; C (s, a, s′ ) ∈ (0, +∞) is the cost incurred when state s′ is reached after
applying action a in state s and this function is required to be deﬁned for all s ∈ S, a ∈ A, s′ ∈ S
such that P (s′ |s, a) > 0.
A solution to an SSP is a policy π , i.e., a mapping from S to A. If π is deﬁned over the entire space
S, then π is a closed policy. A policy π deﬁned only for the states reachable from s0 when following
π is a closed policy w.r.t. s0 and S(π , s0 ) denotes this set of reachable states. For instance, in the
SSP depicted in Figure 1(a), the policy π0 = {(s0 , a0 ), (s′
3 , a0 )} is a closed policy
1 , a0 ), (s′
2 , a0 ), (s′
w.r.t. s0 and S(π0 , s0 ) = {s0 , s′
3 , sG }.
1 , s′
2 , s′
Given a policy π , we deﬁne trajectory as a sequence Tπ = hs(0) , . . . , s(k) i such that, for all
i ∈ {0, · · · , k − 1}, π(s(i) ) is deﬁned and P (s(i+1) |s(i) , π(s(i) )) > 0. The probability of a tra-
jectory Tπ is deﬁned as P (Tπ ) = Qi<|Tπ |
i=0 P (s(i+1) |s(i) , π(s(i) )) and maximum probability of a
trajectory between two states Pmax (s, s′ ) is deﬁned as maxπ P (Tπ = hs, . . . , s′ i).
An optimal policy π∗ for an SSP is any policy that always reaches a goal state when followed from
s0 and also minimizes the expected cost of Tπ∗ . For a given SSP, π∗ might not be unique, however
the optimal value function V ∗ , i.e., the mapping from states to the minimum expected cost to reach
a goal state, is unique. V ∗ is the ﬁxed point of the set of equations deﬁned by (1) for all
s ∈ S \ G
and V ∗ (s) = 0 for all s ∈ G. Notice that under the optimality criterion given by (1), SSPs are

2

a 0
a 1

.6

s 0

.4

.75

.25

t>1

t>2

t>3

t>4

p<1.0

p<0.4

p<0.4

2

.6

.6

s 1

.4

.75

’
s 1

s 2

’
s 2

.4

.75

s 3

’
s 3

.25

.25

.6

.4

.75

.25

.6

sG

s 0

.4

.75

25

.6

.6

s 1

.4

s 2

.4

’
s 1

.75

.25

.75

’
s 2

.25

s 3

’
s 3

.6

.4

.75

.25

sG

.6

s 0

25

.4

.75

s 1

’
s 1

.6

.4

.6

s 2

.4

.75

.25

.75

’
s 2

.25

s 3

’
s 3

.6

.4

.75

.25

sG

p<0.75 3
p<0.75
p<0.75
Figure 1: (a) Example of an SSP. The initial state is s0 , the goal state is sG , C (s, a, s′ ) = 1, ∀s ∈ S,
a ∈ A and s′ ∈ S. (b) State-space partition of (a) according to the depth-based short-sighted SSPs:
Gs0 ,t contains all the states in dotted regions which their conditions hold for the given value of t. (c)
State-space partition of (a) according to the trajectory-based short-sighted SSPs: Gs0 ,ρ contains all
the states in dotted regions which their conditions hold for the given value of ρ.

2

more general than Markov Decision Processes (MDPs) [19], therefore all the work presented here
is directly applicable to MDPs.

a∈A Xs′∈S hC (s, a, s′ ) + P (s′ |s, a)V ∗ (s′ )i
V ∗ (s) = min
Deﬁnition 1 (reachability assumption). An SSP satis ﬁes the reachability assumption if, for all s ∈ S,
there exists sG ∈ G such that Pmax (s, sG ) > 0.

(1)

Given an SSP S, if a goal state can be reached with positive probability from every state s ∈ S,
then the reachability assumption (Deﬁnition 1) holds for S and 0 ≤ V ∗ (s) < ∞ [19]. Once V ∗ is
known, any optimal policy π∗ can be extracted from V ∗ by substituting the operator min by argmin
in equation (1).

A possible approach to compute V ∗ is the value iteration algorithm: deﬁne V i+1 (s) as in (1)
with V i in the right hand side instead of V ∗ and the sequence hV 0 , V 1 , . . . , V k i converges to
V ∗ as k → ∞ [19]. The process of computing V i+1 from V i is known as Bellman up-
date and V 0 (s) can be initialized with an admissible heuristic H (s), i.e., a lower bound for
V ∗ .
In practice we are interested in reaching ǫ-convergence, that is, given ǫ, ﬁnd V such that
maxs |V (s) − mina Ps′ [C (s, a, s′ ) + P (s′ |s, a)V (s′ )]| ≤ ǫ. The following well-known result is
necessary in most of our proofs [2, Assumption 2.2 and Lemma 2.1]:
Theorem 1. Given an SSP S, if the reachability assumption holds for S, then the admissibility and
monotonicity of V are preserved through the Bellman updates.

3 Trajectory-Based Short-Sighted Stochastic SSPs

Short-sighted Stochastic Path Problems (short-sighted SSPs) [1] are a special case of SSPs in which
the original problem is transformed into a smaller one by: (i) pruning the state space; and (ii) adding
arti ﬁcial goal states to heuristically guide the search tow ards the goals of the original problem.
Depth-based short-sighted SSPs are deﬁned based on the acti on-distance between states [1]:
Deﬁnition 2 (action-distance). The non-symmetric action-distance δ(s, s′ ) between two states s and
s′ is argmink {Tπ = hs, s(1) , . . . , s(k−1) , s′ i|∃π and Tπ is a trajectory}.
Deﬁnition 3 (Depth-Based Short-Sighted SSP). Given an SSP S = hS, s0 , G, A, P , C i, a state s ∈
S, t > 0 and a heuristic H , the (s, t)-depth-based short-sighted SSP Ss,t = hSs,t , s, Gs,t , A, P , Cs,t i
associated with S is deﬁned as:
• Ss,t = {s′ ∈ S|δ(s, s′ ) ≤ t};
• Gs,t = {s′ ∈ S|δ(s, s′ ) = t} ∪ (G ∩ Ss,t );
• Cs,t (s′ , a, s′′ ) = (cid:26)C (s′ , a, s′′ ) + H (s′′ )
C (s′ , a, s′′ )
Figure 1(b) shows, for different values of t, Ss0 ,t for the SSP in Figure 1(a); for instance, if t = 2
then Ss0 ,2 = {s0 , s1 , s′
2 } and Gs0 ,2 = {s2 , s′
2 }. In the example shown in Figure 1(b), we can
1 , s2 , s′

∀s′ ∈ Ss,t , a ∈ A, s′′ ∈ Ss,t

if s′′ ∈ Gs,t
otherwise

,

3

see that generation of Ss0 ,t is independent of the trajectories probabilities: for t = 2, s2 ∈ Ss0 ,2 and
3 6∈ Ss0 ,2 , however Pmax (s0 , s2 ) = 0.16 < Pmax (s0 , s′
3 ) = 0.753 ≈ 0.42.
s′

Deﬁnition 4 (Trajectory-Based Short-Sighted SSP). Given an SSP S = hS, s0 , G, A, P , C i, a
state s ∈ S, ρ ∈ [0, 1] and a heuristic H , the (s, ρ)-trajectory-based short-sighted SSP Ss,ρ =
hSs,ρ , s, Gs,ρ , A, P , Cs,ρ i associated with S is deﬁned as:

• Ss,ρ = {s′ ∈ S|∃ˆs ∈ S and a ∈ A s.t. Pmax (s, ˆs) ≥ ρ and P (s′ |ˆs, a) > 0};

• Gs,ρ = (G ∩ Ss,ρ ) ∪ (Ss,ρ ∩ {s′ ∈ S|Pmax (s, s′ ) < ρ});
• Cs,ρ (s′ , a, s′′ ) = (cid:26)C (s′ , a, s′′ ) + H (s′′ )
C (s′ , a, s′′ )
For simplicity, when H is not clear by context nor explicit, then H (s) = 0 for all s ∈ S.

if s′′ ∈ Gs,ρ
otherwise

,

∀s′ ∈ Ss,ρ , a ∈ A, s′′ ∈ Ss,ρ

Our novel model, the trajectory-based short-sighted SSPs (Deﬁnition 4), addresses the issue of
states with low trajectory probability by explicitly deﬁni ng its state space Ss,ρ based on maxi-
mum probability of a trajectory between s and the candidate states s′ (Pmax (s, s′ )). Figure 1(c)
shows, for all values of ρ ∈ [0, 1], the trajectory-based Ss0 ,ρ for the SSP in Figure 1(a): for in-
3 , sG } and Gs0 ,0.75 = {s1 , sG }. This example
stance, if ρ = 0.753 then Ss0 ,0.753 = {s0 , s1 , s′
2 , s′
1 , s′
shows how trajectory-based short-sighted SSP can manage uncertainty efﬁciently: for ρ = 0.753 ,
|Ss0 ,ρ | = 6 and the goal of the original SSP sG is already included in Ss0 ,ρ while, for the depth-
based short-sighted SSPs, sG ∈ Ss0 ,t only for t ≥ 4 case in which |Ss0 ,t | = |S| = 8.
Notice that the deﬁnition of Ss,ρ cannot be simpli ﬁed to {ˆs ∈ S|Pmax (s, ˆs) ≥ ρ} since not all
the resulting states of actions would be included in Ss,ρ . For example, consider S = {s, s′ , s′′ },
P (s′ |s, a) = 0.9 and P (s′′ |s, a) = 0.1, then for ρ ∈ (0.1, 1], {ˆs ∈ S|Pmax (s, ˆs) ≥ ρ} = {s, s′ },
generating an invalid SSP since not all the resulting states of a would be contained in the model.

4 Short-Sighted Probabilistic Planner

The Short-Sighted Probabilistic Planner (SSiPP) is an algorithm that solves SSPs based on short-
sighted SSPs [1]. SSiPP is reviewed in Algorithm 1 and consists of iteratively generating and solving
short-sighted SSPs of the given SSP. Due to the reduced size of the short-sighted problems, SSiPP
solves each of them by computing a closed policy w.r.t.
their initial state. Therefore, we obtain
a “fail-proof ” solution for each short-sighted SSP, thus if
this solution is directly executed in the
environment, then replanning is not needed until an arti ﬁci al goal is reached. Alternatively, an
anytime behavior is obtained if the execution of the computed closed policy for the short-sighted
SSP is simulated (Algorithm 1 line 4) until an arti ﬁcial goal sa is reached and this procedure is
repeated, starting sa , until convergence or an interruption.
In [1], we proved that SSiPP always terminates and is asymptotically optimal for depth-based short-
sighted SSPs. We generalize the results regarding SSiPP by: (i) providing the sufﬁcient conditions
for the generation of short-sighted problems (Algorithm 1, line 1) in Deﬁnition 5; and (ii) proving
that SSiPP always terminates (Theorem 3) and is asymptotically optimal (Corollary 4) when the
short-sighted SSP generator respects Deﬁnition 5. Notice t hat, by deﬁnition, both depth-based and
trajectory-based short-sighted SSPs meet the sufﬁcient co nditions presented on Deﬁnition 5.

Deﬁnition 5. Given an SSP hS, s0 , G, A, P , C i, the sufﬁcient conditions on the short-sighted SSPs
hS′ , ˆs, G′ , A, P ′ , C ′ i returned by the generator in Algorithm 1 line 1 are:

1. G ∩ S′ ⊆ G′ ;
2. ˆs 6∈ G → ˆs 6∈ G′ ; and

3. for all s ∈ S, a ∈ A and s′ ∈ S′ \ G′ , if P (s|s′ , a) > 0 then s ∈ S′ and P ′ (s|s′ , a) =
P (s|s′ , a).
Lemma 2. SSiPP performs Bellman updates on the original SSP S.

4

SS IPP(SSP S = hS, s0 , G, A, P , C i, H a heuristic for V ∗ and params the parameters to generate
short-sighted SSPs)
begin
V ← Value function for S initialized by H
s ← s0
while s 6∈ G do
hS′ , s, G′ , A, P , C ′ i ← G EN ERAT E -SHORT-S IGH T ED -SSP(S, s, V , params)
( ˆπ∗ , ˆV ∗ ) ← O P T IMA L -SSP -SO LV ER(hS′ , s, G′ , A, P , C ′ i, V )
forall s′ ∈ S′ ( ˆπ∗ , s) do
V (s′ ) ← ˆV ∗ (s′ )
while s 6∈ G′ do
s ← execute-action( ˆπ∗ (s))

1

2

3
4

return V
end
Algorithm 1: SSiPP algorithm [1]. G EN ERAT E -SHORT-S IGH T ED -SSP represents a procedure to
generate short-sighted SSPs, either depth-based or trajectory-based. In the former case params = t
and params = ρ for the latter. O P T IMA L -SSP -SO LV ER returns an optimal policy π∗ w.r.t. s0 for S
and V ∗ associated to π∗ , i.e., V ∗ needs to be deﬁned only for s ∈ S(π∗ , s0 ).

Proof. In order to show that SSiPP performs Bellman updates implicitly, consider the loop
Since O P T IMA L -SO LV ER computes ˆV ∗ , by deﬁnition of short-
in line 2 of Algorithm 1.
sighted SSP: (i) ˆV ∗ (sG ) equals V (sG ) for all sG ∈ G′ , therefore the value of V (sG ) remains
the same; and (ii) mina∈A Ps′∈S [C (s, a, s′ ) + P (s′ |s, a)V (s′ )] ≤ ˆV ∗ (s) for s ∈ S′ \ G′ ,
the assignment V (s) ← ˆV ∗ is equivalent to at least one Bellman update on V (s), be-
i.e.,
cause V is a lower bound on ˆV ∗ and Theorem 1. Because s 6∈ G′ and Deﬁnition 5,
mina∈A (cid:2)Ps′∈S C (s, a, s′ ) + P (s′ |s, a)V (s′ )(cid:3) ≤ ˆV ∗ (s) is equivalent to the one Bellman update
in the original SSP S.
Theorem 3. Given an SSP S = hS, s0 , G, A, P , C i such that the reachability assumption holds, an
admissible heuristic H and a short-sighted problem generator that respects Deﬁnit ion 5, then SSiPP
always terminates.

on,
Proof. Since O P T IMA L -SO LV ER always ﬁnishes and the short-sighted SSP is an SSP by deﬁniti
then a goal state sG of the short-sighted SSP is always reached, therefore the loop in line 3 of
Algorithm 1 always ﬁnishes. If sG ∈ G, then SSiPP terminates in this iteration. Otherwise, sG
is an arti ﬁcial goal and sG 6= s (Deﬁnition 5), i.e., sG differs from the state s used as initial state
for the short-sighted SSP generation. Thus another iteration of SSiPP is performed using sG as
s. Suppose, for contradiction purpose, that every goal state reached during SSiPP execution is an
arti ﬁcial goal, i.e., SSiPP does not terminate. Then inﬁnit
ely many short-sighted SSPs are solved.
Since S is ﬁnite, then there exists s ∈ S that is updated inﬁnitely often, therefore V (s) → ∞.
However, V ∗ (s) < ∞ by the reachability assumption. Since SSiPP performs Bellman updates
(Lemma 2) then V (s) ≤ V ∗ (s) by monotonicity of Bellman updates (Theorem 1) and admissibility
G ∈ G and therefore
of H , a contradiction. Thus every execution of SSiPP reaches a goal state s′
terminates.

Corollary 4. Under the same assumptions of Theorem 3, the sequence hV 0 , V 1 , · · · , V t i, where
V 0 = H and V t = SSiPP(S, t, V t−1 ), converges to V ∗ as t → ∞ for all s ∈ S(π∗ , s0 ).

Proof. Let S∗ ⊆ S be the set of states being visited inﬁnitely many times. Clea rly, S(π∗ , s0 ) ⊆ S∗
since a partial policy cannot be executed ad inﬁnitum withou t reaching a state in which it is not
deﬁned. Since SSiPP performs Bellman updates in the origina l SSP space (Lemma 2) and ev-
ery execution of SSiPP terminates (Theorem 3), then we can view the sequence of lower bounds
hV 0 , V 1 , · · · , V t i generated by SSiPP as asynchronous value iteration. The convergence of V t−1 (s)
to V ∗ (s) as t → ∞ for all s ∈ S(π∗ , s0 ) ⊆ S∗ follows by [2, Proposition 2.2, p. 27] and guarantees
the convergence of SSiPP.

5

)
e
l
a
c
s
 
g
o
l
(
 
s
e
t
a
t
S
 
f
o
 
r
e
b
m
u
N

1080

1070

1060

1050

1040

1030

1020

1010

100

 
0

 

|S(π*,s0)|
|S|

5

10

15

20

25
35
30
Triangle Tireworld Problem Size

40

45

50

55

60

(b)
(a)
Figure 2: (a) Map of the triangle tireworld for the sizes 1, 2 and 3. Circles (squares) represent
locations in which there is one (no) spare tire. The shades of gray represent, for each location l,
maxπ P (car reaches l and the tire is not ﬂat when following the policy π from s0 ). (b) Log-lin plot
of the state space size (|S|) and the size of the states reachable from s0 when following the optimal
policy π∗ (|S(π∗ , s0 )|) versus the number of the triangle tireworld problem.

5 Experiments

We present two sets of experiments using the triangle tireworld problems [9, 11, 20], a series of
probabilistic interesting problems [14] in which a car has to travel between locations in order to
reach a goal location from its initial location. The roads are represented as directed graph in a shape
of a triangle and, every time the car moves between locations, a ﬂat tire happens with probability
0.5. Some locations have a spare tire and in these locations the car can deterministically replace
its ﬂat tire by new one. When the car has a ﬂat tire, it cannot cha
nge its location, therefore the car
can get stuck in locations that do not have a spare tire (dead-ends). Figure 2(a) depicts the map of
the triangle tireworld problems 1, 2 and 3 and Figure 2(b) shows the size of S and S(π∗ , s0 ) for
problems up to size 60. For example, the problem number 3 has 28 locations, i.e., 28 nodes in the
corresponding graph on Figure 2(a), its state space has 19562 states and its optimal policy reaches
8190 states.

Every triangle tireworld problem is a probabilistic interesting problem [14]: there is only one policy
that reaches the goal with probability 1 and all the other policies have probability at most 0.5 of
reaching the goal. Also, the solution based on the shortest path has probability 0.52n−1 of reaching
the goal, where n is the problem number. This property is illustrated by the shades of gray in
Figure 2(a) that represents, for each location l, maxπ P (car reaches l and the tire is not ﬂat when
following the policy π from s0 ).
For the experiments in this section, we use the zero-heuristic for all the planners, i.e., V (s) = 0 for
all s ∈ S and LRTDP [4] as O P T IMA L -SO LV ER for SSiPP. For all planners, the parameter ǫ (for
ǫ-convergence) is set to 10−4 . For UCT, we disabled the random rollouts because the probability
of any policy other than the optimal policy to reach a dead-end is at least 0.5 therefore, with high-
probability, UCT would assign ∞ (cost of a dead-end) as the cost of all the states including the
initial state.

The experiments are conducted in a Linux machine with 4 cores running at 3.07GHz using MDP-
SIM [9] as environment simulator. The following terminology is used for describing the experi-
ments: round, the computation for a solution for the given SSP; and run, a set of rounds in which
learning is allowed between rounds, i.e., the knowledge obtained from one round can be used to
solve subsequent rounds. The solution computed during one round is simulated by MDPSIM in a
client-server loop: MDPSIM sends a state s and requests an action from the planner, then the plan-
ner replies by sending the action a to be executed in s. The evaluation is done by the number of
rounds simulated by MDPSIM that reached a goal state. The maximum number of actions allowed
per round is 2000 and rounds that exceed this limit are stopped by MDPSIM and declared as failure,
i.e., goal not reached.

6

Planner
SSiPP depth=8
UCT
SSiPP trajectory

5
50.0
50.0
50.0

10
40.7
50.0
50.0

15
41.2
50.0
50.0

Triangle Tireworld Problem Number
45
40
35
30
25
20
40.6
40.0
40.9
41.0
41.1
40.8
50.0
50.0
8.2
12.1
15.7
43.1
50.0
50.0
50.0
50.0
50.0
50.0

50
40.8
6.8
50.0

55
40.3
5.0
50.0

60
40.4
4.0
50.0

Table 1: Number of rounds solved out of 50 for experiment in Section 5.1. Results are averaged
over 10 runs and the 95% conﬁdence interval is always less than 1.0. In all the problems, SSiPP
using trajectory-based short-sighted SSPs solves all the 50 round in all the 10 runs, therefore its 95%
conﬁdence interval is 0.0 for all the problems. Best results shown in bold font.

Planner
SSiPP depth=8
LRTDP
UCT (4, 100)
UCT (8, 100)
UCT (2, 100)
SSiPP ρ = 1.0
SSiPP ρ = 0.50
SSiPP ρ = 0.25
SSiPP ρ = 0.125

5
50.0
50.0
50.0
50.0
50.0
50.0
50.0
50.0
50.0

10
45.4
23.0
50.0
50.0
50.0
27.9
50.0
50.0
50.0

15
41.2
14.1
50.0
50.0
50.0
29.1
50.0
50.0
50.0

Triangle Tireworld Problem Number
45
40
35
30
20
25
41.2
42.3
44.1
42.4
32.7
20.6
-
-
-
-
0.3
-
2.5
4.0
6.5
12.3
24.0
48.8
2.2
3.7
6.7
12.3
24.0
46.3
2.2
3.5
7.5
12.0
23.2
49.5
26.8
26.0
26.6
28.6
27.2
26.6
50.0
50.0
50.0
50.0
50.0
50.0
50.0
41.9
42.7
41.1
45.0
47.6
50.0
50.0
50.0
50.0
50.0
49.8

50
14.1
-
1.3
1.2
1.2
27.6
50.0
40.7
37.4

55
9.9
-
1.0
1.0
1.0
26.2
50.0
40.1
26.4

60
7.0
-
0.7
0.6
0.6
26.9
50.0
40.4
18.9

Table 2: Number of rounds solved out of 50 for experiment in Section 5.2. Results are averaged
over 10 runs and the 95% conﬁdence interval is always less than 2.6. UCT (c, w) represents UCT
using c as bias parameter and w samples per decision. In all the problems, trajectory-based SSiPP
for ρ = 0.5 solves all the 50 round in all the 10 runs, therefore its 95% conﬁdence interval is 0.0 for
all the problems. Best results shown in bold font.

5.1 Fixed number of search nodes per decision

In this experiment, we compare the performance of UCT, depth-based SSiPP, and trajectory-based
SSiPP with respect to the number of nodes explored by depth-based SSiPP. Formally, to decide what
action to apply in a given state s, each planner is allowed to use at most B = |Ss,t | search nodes,
i.e., the size of the search space is bounded by the equivalent (s, t)-short-sighted SSP. We choose t
equals to 8 since it obtains the best performance in the triangle tireworld problems [1]. Given the
search nodes budget B , for UCT we sample the environment until the search tree contains B nodes;
and for trajectory-based SSiPP we use ρ = argmaxρ {|Ss,ρ | s.t. B ≥ |Ss,ρ |}.
The methodology for this experiment is as follows: for each problem, 10 runs of 50 rounds are
performed for each planner using the search nodes budget B . The results, averaged over the 10 runs,
are presented in Table 1. We set as time and memory cut-off 8 hours and 8 Gb, respectively, and
UCT for problems 35 to 60 was the only planner preempted by the time cut-off. Trace-based SSiPP
outperforms both depth-based SSiPP and UCT, solving all the 50 rounds in all the 10 runs for all the
problems.

5.2 Fixed maximum planning time

In this experiment, we compare planners by limiting the maximum planning time. The methodology
used in this experiment is similar to the one in IPPC’04 and IPPC’06: for each problem, planners
need to solve 1 run of 50 rounds in 20 minutes. For this experiment, the planners are allowed to per-
form internal simulations, for instance, a planner can spend 15 minutes solving rounds using internal
simulations and then use the computed policy to solve the required 50 rounds through MDPSIM in
the remaining 5 minutes. The memory cut-off is 3Gb.

For this experiment, we consider the following planners: depth-based SSiPP for t = 8 [1], trajectory-
based SSiPP for ρ ∈ {1.0, 0.5, 0.25, 0.125}, LRTDP using 3-look-ahead [1] and 12 different
parametrizations of UCT obtained by using the bias parameter c ∈ {1, 2, 4, 8} and the number
of samples per decision w ∈ {10, 100, 1000}. The winners of IPPC’04, IPPC’06 and IPPC’08 are

7

omitted since their performance on the triangle tireworld problems are strictly dominated by depth-
base SSiPP for t = 8. Table 2 shows the results of this experiment and due to space limitations we
show only the top 3 parametrizations of UCT: 1st (c = 4, w = 100); 2nd (c = 8, w = 100); and 3rd
(c = 2, w = 100).
All the four parametrizations of trajectory-based SSiPP outperform the other planners for problems
of size equal or greater than 45. Trajectory-based SSiPP using ρ = 0.5 is especially noteworthy
because it achieves the perfect score in all problems, i.e., it reaches a goal state in all the 50 rounds
in all the 10 runs for all the problems. The same happens for ρ = 0.125 and problems up to size
40. For larger problems, trajectory-based SSiPP using ρ = 0.125 reaches the 20 minutes time
cut-off before solving 50 rounds, however all the solved rounds successfully reach the goal. This
interesting behavior of trajectory-based SSiPP for the triangle tireworld can be explained by the
following theorem:
Theorem 5. For the triangle tireworld, trajectory-based SSiPP using an admissible heuristic never
falls in a dead-end for ρ ∈ (0.5i+1 , 0.5i ] for i ∈ {1, 3, 5, . . . }.

Proof Sketch. The optimal policy for the triangle tireworld is to follow the longest path: move from
the initial location l0 to the goal location lG passing through location lc , where l0 , lc and lG are the
vertices of the triangle formed by the problem’s map. The path from lc to lG is unique, i.e., there
is only one applicable move-car action for all the locations in this path. Therefore all the decision
making to ﬁnd the optimal policy happens between the locatio ns l0 and lc . Each location l′ in the
path from l0 to lc has either two or three applicable move-car actions and we refer to the set of
locations l′ with three applicable move-car actions as N. Every location l′ ∈ N is reachable from
l0 by applying an even number of move-car actions (Figure 2(a)) and the three applicable move-car
actions in l′ are: (i) the optimal action ac , i.e., move the car towards lc ; (ii) the action aG that moves
the car towards lG ; and (iii) the action ap that moves the car parallel to the shortest-path from l0 to
lG . The location reached by ap does not have a spare tire, therefore ap is never selected by a greedy
choice over any admissible heuristic since it reaches a dead-end with probability 0.5. The locations
reached by applying either ac or aG have a spare tire and the greedy choice between them depends
on the admissible heuristic used, thus aG might be selected instead of ac . However, after applying
aG , only one move-car action a is available and it reaches a location that does not have a spare
tire. Therefore, the greedy choice between ac and aG considering two or more move-car actions is
optimal under any admissible heuristic: every sequence of actions haG , a, . . . i reaches a dead-end
with probability at least 0.5 and at least one sequence of actions starting with ac has probability 0 to
reach a dead-end, e.g., the optimal solution.
Given ρ, we denote as Ls,ρ the set of all locations corresponding to states in Ss,ρ and as ls the
location corresponding to the state s. Thus, Ls,ρ contains all the locations reachable from ls using
up to m = ⌊log0.5 ρ⌋ + 1 move-car actions.
If m is even and ls ∈ N, then every location in
Ls,ρ ∩ N represents a state either in Gs,ρ or at least two move-car actions away from any state
in Gs,ρ . Therefore the solution of the (s, ρ)-trajectory-based short-sighted SSP only chooses the
action ac to move the car. Also, since m is even, every state s used by SSiPP for generating
(s, ρ)-trajectory-based short-sighted SSPs has ls ∈ N. Therefore, for even values of m, i.e., for
ρ ∈ (0.5i+1 , 0.5i ] and i ∈ {1, 3, 5, . . . }, trajectory-based SSiPP always chooses the actions ac to
move the car to lc , thus avoiding the all dead-ends.

6 Conclusion

In this paper, we introduced trajectory-based short-sighted SSPs, a new model to manage uncertainty
in probabilistic planning problems. This approach consists of pruning the state space based on the
most likely trajectory between states and deﬁning arti ﬁcia
l goal states that guide the solution towards
the original goals. We also deﬁned a class of short-sighted m odels that includes depth-based and
trajectory-based short-sighted SSPs and proved that SSiPP always terminates and is asymptotically
optimal for short-sighted models in this class.

We empirically compared trajectory-based SSiPP with depth-based SSiPP and other state-of-the-art
planners in the triangle tireworld. Trajectory-based SSiPP outperforms all the other planners and it
is the only planner able to scale up to problem number 60, a problem in which the optimal solution
contains approximately 1070 states, under the IPPC evaluation methodology.

8

References

[1] F. W. Trevizan and M. M. Veloso. Short-sighted stochastic shortest path problems. In In Proc.
of the 22nd International Conference on Automated Planning and Scheduling (ICAPS), 2012.
[2] D. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scienti ﬁc, 1996.
[3] A.G. Barto, S.J. Bradtke, and S.P. Singh. Learning to act using real-time dynamic program-
ming. Arti ﬁcial Intelligence , 72(1-2):81–138, 1995.
[4] B. Bonet and H. Geffner. Labeled RTDP: Improving the convergence of real-time dynamic
programming.
In Proc. of the 13th International Conference on Automated Planning and
Scheduling (ICAPS), 2003.
[5] H.B. McMahan, M. Likhachev, and G.J. Gordon. Bounded real-time dynamic programming:
RTDP with monotone upper bounds and performance guarantees. In Proc. of the 22nd Inter-
national Conference on Machine Learning (ICML), 2005.
[6] Trey Smith and Reid G. Simmons. Focused Real-Time Dynamic Programming for MDPs:
Squeezing More Out of a Heuristic.
In Proc. of the 21st National Conference on Arti ﬁcial
Intelligence (AAAI), 2006.
[7] S. Sanner, R. Goetschalckx, K. Driessens, and G. Shani. Bayesian real-time dynamic program-
ming. In Proc. of the 21st International Joint Conference on Arti ﬁci al Intelligence (IJCAI),
2009.
[8] S. Yoon, A. Fern, and R. Givan. FF-Replan: A baseline for probabilistic planning. In Proc. of
the 17th International Conference on Automated Planning and Scheduling (ICAPS), 2007.
[9] H.L.S. Younes, M.L. Littman, D. Weissman, and J. Asmuth. The ﬁrst probabilistic track of the
international planning competition. Journal of Arti ﬁcial Intelligence Research , 24(1):851–887,
2005.
[10] F. Teichteil-Koenigsbuch, G. Infantes, and U. Kuter. RFF: A robust, FF-based mdp planning
algorithm for generating policies with low probability of failure. 3rd International Planning
Competition (IPPC-ICAPS), 2008.
[11] D. Bryce and O. Buffet. 6th International Planning Competition: Uncertainty Track. In 3rd
International Probabilistic Planning Competition (IPPC-ICAPS), 2008.
[12] S. Yoon, A. Fern, R. Givan, and S. Kambhampati. Probabilistic planning via determinization
in hindsight. In Proc. of the 23rd National Conference on Arti ﬁcial Intellig ence (AAAI), 2008.
[13] S. Yoon, W. Ruml, J. Benton, and M. B. Do.
Improving Determinization in Hindsight for
Online Probabilistic Planning. In Proc. of the 20th International Conference on Automated
Planning and Scheduling (ICAPS), 2010.
[14] I. Little and S. Thi ´ebaux. Probabilistic planning vs replanning. In Proc. of ICAPS Workshop
on IPC: Past, Present and Future, 2007.
[15] J. Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-
Wesley, Menlo Park, California, 1985.
[16] Levente Kocsis and Csaba Szepesvri. Bandit based Monte-Carlo Planning.
European Conference on Machine Learning (ECML), 2006.
[17] T. Dean, L.P. Kaelbling, J. Kirman, and A. Nicholson. Planning under time constraints in
stochastic domains. Arti ﬁcial Intelligence , 76(1-2):35–74, 1995.
[18] D.P. Bertsekas and J.N. Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-
matics of Operations Research, 16(3):580–595, 1991.
[19] D.P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scienti ﬁc, 1995.
[20] Blai Bonet and Robert Givan. 2th International Probabilistic Planning Competition (IPPC-
ICAPS). http://www.ldc.usb.ve/˜ bonet/ipc5/ (accessed on Dec 13, 2011),
2007.

In Proc. of the

9


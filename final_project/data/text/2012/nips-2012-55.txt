Stochastic Gradient Descent with
Only One Projection

Mehrdad Mahdavi† , Tianbao Yang‡ , Rong Jin† , Shenghuo Zhu(cid:63) , and Jinfeng Yi†
†Dept. of Computer Science and Engineering, Michigan State University, MI, USA
‡Machine Learning Lab, GE Global Research, CA, USA
(cid:63)NEC Laboratories America, CA, USA
† {mahdavim,rongjin,yijinfen}@msu.edu,‡tyang@ge.com,(cid:63)zsh@nec-labs.com

Abstract

Although many variants of stochastic gradient descent have been proposed for
large-scale convex optimization, most of them require projecting the solution at
each iteration to ensure that the obtained solution stays within the feasible domain.
For complex domains (e.g., positive semideﬁnite cone), the projection step can
be computationally expensive, making stochastic gradient descent unattractive for
large-scale optimization problems. We address this limitation by developing novel
stochastic optimization algorithms that do not need intermediate projections. In-
stead, only one projection at the last iteration is needed to obtain a feasible solution
√
in the given domain. Our theoretical analysis shows that with a high probability,
the proposed algorithms achieve an O(1/
T ) convergence rate for general con-
vex optimization, and an O(ln T /T ) rate for strongly convex optimization under
mild conditions about the domain and the objective function.

1

Introduction

With the increasing amount of data that is available for training, it becomes an urgent task to devise
efﬁcient algorithms for optimization/learning problems with unprecedented sizes. Online learning
algorithms, such as celebrated Stochastic Gradient Descent (SGD) [16, 2] and its online counterpart
Online Gradient Descent (OGD) [22], despite of their slow rate of convergence compared with the
batch methods, have shown to be very effective for large scale and online learning problems, both
theoretically [16, 13] and empirically [19]. Although a large number of iterations is usually needed
to obtain a solution of desirable accuracy, the lightweight computation per iteration makes SGD
attractive for many large-scale learning problems.
To ﬁnd a solution within the domain K that optimizes the given objective function f (x), SGD
computes an unbiased estimate of the gradient of f (x), and updates the solution by moving it in
the opposite direction of the estimated gradient. To ensure that the solution stays within the domain
K, SGD has to project the updated solution back into the K at every iteration. Although efﬁcient
algorithms have been developed for projecting solutions into special domains (e.g., simplex and (cid:96)1
ball [6, 14]); for complex domains, such as a positive semideﬁnite (PSD) cone in metric learning
and bounded trace norm matrices in matrix completion (more examples of complex domains can
be found in [10] and [11]), the projection step requires solving an expensive convex optimization,
leading to a high computational cost per iteration and consequently making SGD unappealing for
large-scale optimization problems over such domains. For instance, projecting a matrix into a PSD
cone requires computing the full eigen-decomposition of the matrix, whose complexity is cubic in
the size of the matrix.
The central theme of this paper is to develop a SGD based method that does not require projection
at each iteration. This problem was ﬁrst addressed in a very recent work [10], where the authors
extended Frank-Wolfe algorithm [7] for online learning. But, one main shortcoming of the algo-

1

rithm proposed in [10] is that it has a slower convergence rate (i.e., O(T −1/3 )) than a standard
SGD algorithm (i.e., O(T −1/2 )). In this work, we demonstrate that a properly modiﬁed SGD algo-
rithm can achieve the optimal convergence rate of O(T −1/2 ) using only ONE projection for general
stochastic convex optimization problem. We further develop an SGD based algorithm for strongly
convex optimization that achieves a convergence rate of O(ln T /T ), which is only a logarithmic
ization mechanism, the average solution (cid:98)xT obtained by the SGD after T iterations will be very
factor worse than the optimal rate [9]. The key idea of both algorithms is to appropriately penalize
the intermediate solutions when they are outside the domain. With an appropriate design of penal-
(cid:101)xT can be obtained by projecting (cid:98)xT into the domain K, the only projection that is needed for the
close to the domain K, even without intermediate projections. As a result, the ﬁnal feasible solution
entire algorithm. We note that our approach is very different from the previous efforts in developing
projection free convex optimization algorithms (see [8, 12, 11] and references therein), where the
key idea is to develop appropriate updating procedures to restore the feasibility of solutions at every
iteration.
We close this section with a statement of contributions and main results made by the present work:
• We propose a stochastic gradient descent algorithm for general convex optimization that in-
troduces a Lagrangian multiplier to penalize the solutions outside the domain and performs
√
primal-dual updating. The proposed algorithm achieves the optimal convergence rate of
T ) with only one projection;
O(1/
• We propose a stochastic gradient descent algorithm for strongly convex optimization that
constructs the penalty function using a smoothing technique. This algorithm attains an
O(ln T /T ) convergence rate with only one projection.

2 Related Works

Generally, the computational complexity of the projection step in SGD has seldom been taken into
account in the literature. Here, we brieﬂy review the previous works on projection free convex op-
timization, which is closely related to the theme of this study. For some speciﬁc domains, efﬁcient
algorithms have been developed to circumvent the high computational cost caused by projection
step at each iteration of gradient descent methods. The main idea is to select an appropriate direc-
tion to take from the current solution such that the next solution is guaranteed to stay within the
domain. Clarkson [5] proposed a sparse greedy approximation algorithm for convex optimization
over a simplex domain, which is a generalization of an old algorithm by Frank and Wolfe [7] (a.k.a
conditional gradient descent [3]). Zhang [21] introduced a similar sequential greedy approximation
algorithm for certain convex optimization problems over a domain given by a convex hull. Hazan [8]
devised an algorithm for approximately maximizing a concave function over a trace norm bounded
PSD cone, which only needs to compute the maximum eigenvalue and the corresponding eigenvec-
tor of a symmetric matrix. Ying et al. [20] formulated the distance metric learning problems into
eigenvalue maximization and proposed an algorithm similar to [8].
Recently, Jaggi [11] put these ideas into a general framework for convex optimization with a gen-
eral convex domain. Instead of projecting the intermediate solution into a complex convex domain,
Jaggi’s algorithm solves a linearized problem over the same domain. He showed that Clark’s algo-
rithm , Zhang’s algorithm and Hazan’s algorithm discussed above are special cases of his general
algorithm for special domains. It is important to note that all these algorithms are designed for batch
optimization, not for stochastic optimization, which is the focus of this work.
Our work is closely related to the online Frank-Wolfe (OFW) algorithm proposed in [10]. It is a
projection free online learning algorithm, built on the the assumption that it is possible to efﬁciently
minimize a linear function over the complex domain. One main shortcoming of the OFW algorithm
is that its convergence rate for general stochastic optimization is O(T −1/3 ), signiﬁcantly slower than
that of a standard stochastic gradient descent algorithm (i.e., O(T −1/2 )). It achieves a convergence
rate of O(T −1/2 ) only when the objective function is smooth, which unfortunately does not hold
for many machine learning problems where either a non-smooth regularizer or a non-smooth loss
function is used. Another limitation of OFW is that it assumes a linear optimization problem over
the domain K can be solved efﬁciently. Although this assumption holds for some speciﬁc domains
as discussed in [10], but in many settings of practical interest, this may not be true. The proposed
algorithms address the two limitations explicitly. In particular, we show that how two seemingly
different modiﬁcations of the SGD can be used to avoid performing expensive projections with
similar convergency rates as the original SGD method.

2

3 Preliminaries

Throughout this paper, we consider the following convex optimization problem:
(1)
min
x∈K f (x),
where K is a bounded convex domain. We assume that K can be characterized by an inequality
constraint and without loss of generality is bounded by the unit ball, i.e.,
K = {x ∈ Rd : g(x) ≤ 0} ⊆ B = {x ∈ Rd : (cid:107)x(cid:107)2 ≤ 1},
(2)
where g(x) is a convex constraint function. We assume that K has a non-empty interior, i.e., there
exists x such that g(x) < 0 and the optimal solution x∗ to (1) is in the interior of the unit ball B , i.e.,
(cid:107)x∗ (cid:107)2 < 1. Note that when a domain is characterized by multiple convex constraint functions, say
gi (x) ≤ 0, i = 1, . . . , m, we can summarize them into one constraint g(x) ≤ 0, by deﬁning g(x) as
g(x) = max1≤i≤m gi (x).
To solve the optimization problem in (1), we assume that the only information available to the al-
gorithm is through a stochastic oracle that provides unbiased estimates of the gradient of f (x).
tion xt , the oracle returns (cid:101)∇f (xt ; ξt ), an unbiased estimate of the true gradient ∇f (xt ), i.e.,
More precisely, let ξ1 , . . . , ξT be a sequence of independently and identically distributed (i.i.d)
random variables sampled from an unknown distribution P . At each iteration t, given solu-
Eξt [ (cid:101)∇f (xt , ξt )] = ∇f (xt ). The goal of the learner is to ﬁnd an approximate optimal solution
by making T calls to this oracle.
Before proceeding, we recall a few deﬁnitions from convex analysis [17].
Deﬁnition 1. A function f (x) is a G-Lipschitz continuous function w.r.t a norm (cid:107) · (cid:107), if
|f (x1 ) − f (x2 )| ≤ G(cid:107)x1 − x2(cid:107), ∀x1 , x2 ∈ B .
(3)
In particular, a convex function f (x) with a bounded (sub)gradient (cid:107)∂ f (x)(cid:107)∗ ≤ G is G-Lipschitz
continuous, where (cid:107) · (cid:107)∗ is the dual norm to (cid:107) · (cid:107).
Deﬁnition 2. A convex function f (x) is β -strongly convex w.r.t a norm (cid:107) · (cid:107) if there exists a constant
β > 0 (often called the modulus of strong convexity) such that, for any α ∈ [0, 1], it holds:
f (αx1 + (1 − α)x2 ) ≤ αf (x1 ) + (1 − α)f (x2 ) − 1
α(1 − α)β (cid:107)x1 − x2(cid:107)2 , ∀x1 , x2 ∈ B .
2
When f (x) is differentiable, the strong convexity is equivalent to f (x1 ) ≥ f (x2 ) + (cid:104)∇f (x2 ), x1 −
x2 (cid:105) + β
2 (cid:107)x1 − x2 (cid:107)2 , ∀x1 , x2 ∈ B . In the sequel, we use the standard Euclidean norm to deﬁne
Lipschitz and strongly convex functions. Stochastic gradient descent method is an iterative algorithm
xt+1 = ΠK (xt − ηt (cid:101)∇f (xt , ξt )),
and produces a sequence of solutions xt , t = 1, . . . , T , by
(4)
domain K, and (cid:101)∇f (x, ξt ) is an unbiased stochastic gradient of f (x), for which we further assume
where {ηt}T
t=1 is a sequence of step sizes, ΠK (x) is a projection operator that projects x into the
Eξt [exp((cid:107) (cid:101)∇f (x, ξt ) − ∇f (x)(cid:107)2
bounded gradient variance as
2 /σ2 )] ≤ exp(1).
(5)
√
T ) con-
For general convex optimization, stochastic gradient descent methods can obtain an O(1/
vergence rate in expectation or in a high probability provided (5) [16]. As we mentioned in the
Introduction section, SGD methods are computationally efﬁcient only when the projection ΠK (x)
can be carried out efﬁciently. The objective of this work is to develop computationally efﬁcient
stochastic optimization algorithms that are able to yield the same performance guarantee as the
standard SGD algorithm but with only ONE projection when applied to the problem in (1).

4 Algorithms and Main Results

We now turn to extending the SGD method to the setting where only one projection is allowed to
perform for the entire sequence of updating. The main idea is to incorporate the constraint function
g(x) into the objective function to penalize the intermediate solutions that are outside the domain.
The result of the penalization is that, although the average solution obtained by SGD may not be
feasible, it should be very close to the boundary of the domain. A projection is performed at the end
of the iterations to restore the feasibility of the average solution.

3

Algorithm 1 (SGDP-PD): SGD with ONE Projection by Primal Dual Updating
1: Input: a sequence of step sizes {ηt}, and a parameter γ > 0
2: Initialization: x1 = 0 and λ1 = 0
t+1 = xt − ηt ( (cid:101)∇f (xt , ξt ) + λt∇g(xt ))
3: for t = 1, 2, . . . , T do
Compute x(cid:48)
4:
t+1/ max ((cid:107)x(cid:48)
t+1(cid:107)2 , 1),
Update xt+1 = x(cid:48)
5:
8: Output: (cid:101)xT = ΠK ((cid:98)xT ), where (cid:98)xT = (cid:80)T
Update λt+1 = [(1 − γ ηt )λt + ηt g(xt )]+
6:
7: end for
t=1 xt/T .
The key ingredient of proposed algorithms is to replace the projection step with the gradient com-
putation of the constraint function deﬁning the domain K, which is signiﬁcantly cheaper than pro-
jection step. As an example, when a solution is restricted to a PSD cone, i.e., X (cid:23) 0 where X
is a symmetric matrix, the corresponding inequality constraint is g(X ) = λmax (−X ) ≤ 0, where
λmax (X ) computes the largest eigenvalue of X and is a convex function. In this case, ∇g(X ) only
requires computing the minimum eigenvector of a matrix, which is cheaper than a full eigenspectrum
computation required at each iteration of the standard SGD algorithm to restore feasibility.
Below, we state a few assumptions about f (x) and g(x) often made in stochastic optimization as:
Eξt [exp((cid:107) (cid:101)∇f (x, ξt ) − ∇f (x)(cid:107)2
(cid:107)∇f (x)(cid:107)2 ≤ G1 ,
(cid:107)∇g(x)(cid:107)2 ≤ G2 ,
|g(x)| ≤ C2 ,
∀x ∈ B ,
A1
(6)
2/σ2 )] ≤ exp(1),
∀x ∈ B .
A2
(7)
We also make the following mild assumption about the boundary of the convex domain K as:
(cid:107)∇g(x)(cid:107)2 ≥ ρ.
A3
there exists a constant ρ > 0 such that min
g(x)=0
Remark 1. The purpose of introducing assumption A3 is to ensure that the optimal dual variable
for the constrained optimization problem in (1) is well bounded from the above, a key factor for our
analysis. To see this, we write the problem in (1) into a convex-concave optimization problem:
x∈B max
min
f (x) + λg(x).
λ≥0
Let (x∗ , λ∗ ) be the optimal solution to the above convex-concave optimization problem. Since we
assume g(x) is strictly feasible, x∗ is also an optimal solution to (1) due to the strong duality
theorem [4]. Using the ﬁrst order optimality condition, we have ∇f (x∗ ) = −λ∗∇g(x∗ ). Hence,
λ∗ = 0 when g(x∗ ) < 0, and λ∗ = (cid:107)∇f (x∗ )(cid:107)2/(cid:107)∇g(x∗ )(cid:107)2 when g(x∗ ) = 0. Under assumption
A3, we have λ∗ ∈ [0, G1 /ρ].
We note that, from a practical point of view, it is straightforward to verify that for many domains
including PSD cone and Polytope, the gradient of the constraint function is lower bounded on the
boundary and therefore assumption A3 does not limit the applicability of the proposed algorithms
for stochastic optimization. For the example of g(X ) = λmax (−X ), the assumption A3 implies
ming(X )=0 (cid:107)∇g(X )(cid:107)F = (cid:107)uu(cid:62)(cid:107)F = 1, where u is an orthonomal vector representing the corre-
sponding eigenvector of the matrix X whose minimum eigenvalue is zero.
We propose two different ways of incorporating the constraint function into the objective function,
which result in two algorithms, one for general convex and the other for strongly convex functions.
4.1 SGD with One Projection for General Convex Optimization

(8)

To incorporate the constraint function g(x), we introduce a regularized Lagrangian function,
L(x, λ) = f (x) + λg(x) − γ
λ ≥ 0.
λ2 ,
2
The summation of the ﬁrst two terms in L(x, λ) corresponds to the Lagrangian function in dual anal-
ysis and λ corresponds to a Lagrangian multiplier. A regularization term −(γ /2)λ2 is introduced in
L(x, λ) to prevent λ from being too large. Instead of solving the constrained optimization problem
in (1), we try to solve the following convex-concave optimization problem
L(x, λ).
x∈B max
min
λ≥0
The proposed algorithm for stochastically optimizing the problem in (9) is summarized in Algo-
rithm 1. It differs from the existing stochastic gradient descent methods in that it updates both the
primal variable x (steps 4 and 5) and the dual variable λ (step 6), which shares the same step sizes.

(9)

4

We note that the parameter ρ is not employed in the implementation of Algorithm 1 and is only
required for the theoretical analysis. It is noticeable that a similar primal-dual updating is explored
in [15] to avoid projection in online learning. Our work differs from [15] in that their algorithm
and analysis only lead to a bound for the regret and the violation of the constraints in a long run,
which does not necessarily guarantee the feasibility of ﬁnal solution. Also our proof techniques
differ from [16], where the convergence rate is obtained for the saddle point; however our goal is to
attain bound on the convergence of the primal feasible solution.
Remark 2. The convex-concave optimization problem in (9) is equivalent to the following mini-
mization problem:

,

(10)

[g(x)]2
+
min
x∈B f (x) +
2γ
where [z ]+ outputs z if z > 0 and zero otherwise. It thus may seem attractive to directly optimize
√
√
the penalized function f (x) + [g(x)]2
+/(2γ ) using the standard SGD method, which unfortunately
√
does not yield a regret of O(
T ), we need
T ). This is because, in order to obtain a regret of O(
T ), which unfortunately will lead to a blowup of the gradients and consequently a
to set γ = Ω(
√
poor regret bound. Using a primal-dual updating schema allows us to adjust the penalization term
more carefully to obtain an O(1/
T ) convergence rate.
2/(cid:112)(G2
2 ), t = 1, · · · , T , and
Theorem 1. For any general convex function f (x), if we set ηt = γ /(2G2
(cid:19)
(cid:18) 1√
2 + (1 + ln(2/δ))σ2 )T in Algorithm 1, under assumptions A1-A3, we have,
γ = G2
1 + C 2
with a probability at least 1 − δ ,
f ((cid:101)xT ) ≤ min
,
x∈K f (x) + O
T
where O(·) suppresses polynomial factors that depend on ln(2/δ), G1 , G2 , C2 , ρ, and σ .

4.2 SGD with One Projection for Strongly Convex Optimization

We ﬁrst emphasize that it is difﬁcult to extend Algorithm 1 to achieve an O(ln T /T ) convergence
rate for strongly convex optimization. This is because although −L(x, λ) is strongly convex in λ,
its modulus for strong convexity is γ , which is too small to obtain an O(ln T ) regret bound.
To achieve a faster convergence rate for strongly convex optimization, we change assumptions A1
(cid:107) (cid:101)∇f (x, ξt )(cid:107)2 ≤ G1 ,
and A2 to
(cid:107)∇g(x)(cid:107)2 ≤ G2 ,
∀x ∈ B ,
A4
where we slightly abuse the same notation G1 . Note that A1 only requires that (cid:107)∇f (x)(cid:107)2 is
convex optimization we need to assume a bound on the stochastic gradient (cid:107) (cid:101)∇f (x, ξt )(cid:107)2 . Al-
bounded and A2 assumes a mild condition on the stochastic gradient.
In contrast, for strongly
though assumption A4 is stronger than assumptions A1 and A2, however, it is always possible
sampling over the training examples. Given the bound on (cid:107) (cid:101)∇f (x, ξt )(cid:107)2 , we can easily have
to bound the stochastic gradient for machine learning problems where f (x) usually consists of
a summation of loss functions on training examples, and the stochastic gradient is computed by
(cid:107)∇f (x)(cid:107)2 = (cid:107)E (cid:101)∇f (x, ξt )(cid:107)2 ≤ E(cid:107) (cid:101)∇f (x, ξt )(cid:107)2 ≤ G1 , which is used to set an input parameter
λ0 > G1/ρ to the algorithm. According to the discussion in the last subsection, we know that the
optimal dual variable λ∗ is upper bounded by G1/ρ, and consequently is upper bounded by λ0 .
Similar to the last approach, we write the optimization problem (1) into an equivalent convex-
concave optimization problem:
f (x) + λg(x) = min
min
f (x) = min
x∈B max
x∈B f (x) + λ0 [g(x)]+ .
0≤λ≤λ0
g(x)≤0
To avoid unnecessary complication due to the subgradient of [·]+ , following [18], we introduce a
smoothing term H (λ/λ0 ), where H (p) = −p ln p − (1 − p) ln(1 − p) is the entropy function, into
(cid:19)(cid:19)
(cid:18) λ0 g(x)
(cid:18)
the Lagrangian function, leading to the optimization problem min
x∈B F (x), where F (x) is deﬁned as
F (x) = f (x) + max
0≤λ≤λ0
γ
where γ > 0 is a parameter whose value will be determined later. Given the smoothed objective
function F (x), we ﬁnd the optimal solution by applying SGD to minimize F (x), where the gradient

λg(x) + γH (λ/λ0 ) = f (x) + γ ln

1 + exp

,

5

exp (λ0 g(xt )/γ )
1 + exp(λ0 g(xt )/γ )

Algorithm 2 (SGDP-ST): SGD with ONE Projection by a Smoothing Technique
1: Input: a sequence of step sizes {ηt}, λ0 , and γ
(cid:18) (cid:101)∇f (xt , ξt ) +
(cid:19)
2: Initialization: x1 = 0.
3: for t = 1, . . . , T do
t+1 = xt − ηt
λ0∇g(xt )
Compute x(cid:48)
4:
7: Output: (cid:101)xT = ΠK ((cid:98)xT ), where (cid:98)xT = (cid:80)T
t+1/ max((cid:107)x(cid:48)
t+1(cid:107)2 , 1)
Update xt+1 = x(cid:48)
5:
6: end for
t=1 xt/T .
of F (x) is computed by
∇F (x) = ∇f (x) +
λ0∇g(x).
exp (λ0 g(x)/γ )
1 + exp (λ0 g(x)/γ )
Algorithm 2 gives the detailed steps. Unlike Algorithm 1, only the primal variable x is updated in
each iteration using the stochastic gradient computed in (11).
The following theorem shows that Algorithm 2 achieves an O(ln T /T ) convergence rate if the cost
functions are strongly convex.
Theorem 2. For any β -strongly convex function f (x), if we set ηt = 1/(2β t), t = 1, . . . , T , γ =
(cid:19)
(cid:18) ln T
ln T /T , and λ0 > G1/ρ in Algorithm 2, under assumptions A3 and A4, we have with a probability
at least 1 − δ ,
f ((cid:101)xT ) ≤ min
x∈K f (x) + O
,
T
where O(·) suppresses polynomial factors that depend on ln(1/δ), 1/β , G1 , G2 , ρ, and λ0 .
It is well known that the optimal convergence rate of SGD for strongly convex optimization is
O(1/T ) [9] which has been proven to be tight in stochastic optimization setting [1]. According to
Theorem 2, Algorithm 2 achieves an almost optimal convergence rate except for the factor of ln T .
It is worth mentioning that although it is not explicitly given in Theorem 2, the detailed expression
√
for the convergence rate of Algorithm 2 exhibits a tradeoff in setting λ0 (more can be found in the
proof of Theorem 2). Finally, under assumptions A1-A3, Algorithm 2 can achieve an O(1/
T )
convergence rate for general convex functions, similar to Algorithm 1.

(11)

5 Convergence Rate Analysis

We here present the proofs of main theorems. The omitted proofs are provided in the Appendix. We
use O(·) notation in a few inequalities to absorb constants independent from T for ease of exposition.

5.1 Proof of Theorem 1

To pave the path for the proof, we present a series of lemmas. The lemma below states two key
inequalities, which follows the standard analysis of gradient descent.
(cid:1) + 2ηtG2
(cid:0)(cid:107)x − xt(cid:107)2
Lemma 1. Under the bounded assumptions in (6) and (7), for any x ∈ B and λ > 0, we have
(xt − x)(cid:62)∇xL(xt , λt ) ≤ 1
2 − (cid:107)x − xt+1 (cid:107)2
+ (x − xt )(cid:62) ( (cid:101)∇f (xt , ξt ) − ∇f (xt ))
+ 2ηt (cid:107) (cid:101)∇f (xt , ξt ) − ∇f (xt )(cid:107)2
2λ2
1 + ηtG2
2
t
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
2ηt
,
2
(cid:0)|λ − λt |2 − |λ − λt+1 |2 (cid:1) + 2ηtC 2
≡∆t
≡ζt (x)
(λ − λt )∇λL(xt , λt ) ≤ 1
2 .
2ηt
An immediate result of Lemma 1 is the following which states a regret-type bound.
[(cid:80)T
T(cid:88)
T(cid:88)
T(cid:88)
2 ), t = 1, · · · , T , we have
Lemma 2. For any general convex function f (x), if we set ηt = γ /(2G2
t=1 g(xt )]2
1 + C 2
(G2
≤ G2
(f (xt ) − f (x∗ )) +
ζt (x∗ ),
γ
2 )
+
2
G2
G2
2(γT + 2G2
2 /γ )
γ
2
2
t=1
t=1
t=1
where x∗ = arg minx∈K f (x).

∆t +

+

γT +

6

t=1 ζt (x∗ ) ≤ 2σ(cid:112)3 ln(2/δ)
1 − δ/2, we have (cid:80)T
1 − δ/2, we have (cid:80)T
√
Proof of Therorem 1. First, by martingale inequality (e.g., Lemma 4 in [13]), with a probability
T . By Markov’s inequality, with a probability
t=1 ∆t ≤ (1 + ln(2/δ))σ2T . Substituting these inequalities into Lemma 2,
T(cid:88)
(cid:2) T(cid:88)
g(xt )(cid:3)2
plugging the stated value of γ , we have with a probability 1 − δ
√
√
(f (xt ) − f (x∗ )) +
+ ≤ O(
2 + (1 + ln(2/δ))σ2 + 2(cid:112)G2
where C = 2G2 (1/(cid:112)G2
1
T ),
T
C
t=1
t=1
2 + (1 + ln(2/δ))σ2 ) and O(·)
1 + C 2
1 + C 2
Recalling the deﬁnition of (cid:98)xT = (cid:80)T
suppresses polynomial factors that depend on ln(2/δ), G1 , G2 , C2 , σ .
(cid:18) 1√
(cid:19)
t=1 xt/T and using the convexity of f (x) and g(x), we have
f ((cid:98)xT ) − f (x∗ ) +
[g((cid:98)xT )]2
√
+ ≤ O
T
(12)
Assume g((cid:98)xT ) > 0, otherwise (cid:101)xT = (cid:98)xT and we easily have f ((cid:101)xT ) ≤ minx∈K f (x) + O(1/
.
√
C
T
Since (cid:101)xT is the projection of (cid:98)xT into K, i.e., (cid:101)xT = arg ming(x)≤0 (cid:107)x − (cid:98)xT (cid:107)2
T ).
2 , then by ﬁrst order
g((cid:101)xT ) = 0, and (cid:98)xT − (cid:101)xT = s∇g((cid:101)xT )
optimality condition, there exists a positive constant s > 0 such that
which indicates that (cid:98)xT − (cid:101)xT is in the same direction to ∇g( ˜xT ). Hence,
g((cid:98)xT ) = g((cid:98)xT ) − g((cid:101)xT ) ≥ ((cid:98)xT − (cid:101)xT )(cid:62)∇g((cid:101)xT ) = (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 (cid:107)∇g((cid:101)xT )(cid:107)2 ≥ ρ(cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 ,
(13)
f (x∗ ) − f ((cid:98)xT ) ≤ f (x∗ ) − f ((cid:101)xT ) + f ((cid:101)xT ) − f ((cid:98)xT ) ≤ G1(cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 ,
where the last inequality follows the deﬁnition of ming(x)=0 (cid:107)∇g(x)(cid:107)2 ≥ ρ. Additionally, we have
due to f (x∗ ) ≤ f ((cid:101)xT ) and Lipschitz continuity of f (x). Combining inequalities (12), (13), and (14)
(14)
T ) + G1 (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 .
T (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2
yields
√
√
(cid:16)(cid:113) C
(cid:17)
ρ2
2 ≤ O(1/
By simple algebra, we have (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 ≤ G1C
C
(cid:18) 1√
(cid:19)
(cid:19)
(cid:18) 1√
√
. Therefore
+ O
f ((cid:101)xT ) ≤ f ((cid:101)xT ) − f ((cid:98)xT ) + f ((cid:98)xT ) ≤ G1 (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 + f (x∗ ) + O
ρ2 T
ρ2
T
≤ f (x∗ ) + O
where we use the inequality in (12) to bound f ((cid:98)xT ) by f (x∗ ) and absorb the dependence on ρ, G1 , C
,
T
T
into the O(·) notation.
Remark 3. From the proof of Theorem 1, we can see that the key inequalities are (12), (13), and (14).
In particular, the regret-type bound in (12) depends on the algorithm. If we only update the primal
variable using the penalized objective in (10), whose gradient depends on 1/γ , it will cause a blowup
in the regret bound with (1/γ + γT + T /γ ), which leads to a non-convergent bound.

5.2 Proof of Theorem 2

Our proof of Theorem 2 for the convergence rate of Algorithm 2 when applied to strongly convex
functions starts with the following lemma by analogy of Lemma 2.
T(cid:88)
T(cid:88)
T(cid:88)
Lemma 3. For any β -strongly convex function f (x), if we set ηt = 1/(2β t), we have
0G2
1 + λ2
(F (x) − F (x∗ )) ≤ (G2
(cid:107)x∗ − xt(cid:107)2
ζt (x∗ ) − β
2 )(1 + ln T )
2
2β
4
t=1
t=1
t=1
where x∗ = arg minx∈K f (x).
Lemma 4. For any ﬁxed x ∈ B , deﬁne DT = (cid:80)T
2 , ΛT = (cid:80)T
In order to prove Theorem 2, we need the following result for an improved martingale inequality.
t=1 (cid:107)xt − x(cid:107)2
(cid:114)
(cid:19)
(cid:18)
(cid:19)
(cid:18)
t=1 ζt (x), and m =
(cid:100)log2 T (cid:101). We have
≥ 1 − δ.
ΛT ≤ 4G1
DT ≤ 4
m
+ 4G1 ln
Pr
DT ln
T
δ

m
δ

+

+ Pr

7

.

.

ln

m
δ

,

(cid:19)

+ 4G1

Proof of Theorem 2. We substitute the bound in Lemma 4 into the inequality in Lemma 3 with
T(cid:88)
T(cid:88)
(cid:112)
x = x∗ . We consider two cases. In the ﬁrst case, we assume DT ≤ 4/T . As a result, we have
(∇f (xt ) − (cid:101)∇f (xt , ξt ))(cid:62) (x∗ − xt ) ≤ 2G1
T DT ≤ 4G1 ,
ζt (x∗ ) =
T(cid:88)
t=1
t=1
which together with the inequality in Lemma 3 leads to the bound
0G2
(G2
1 + λ2
(F (xt ) − F (x∗ )) ≤ 4G1 +
2 )(1 + ln T )
2β
(cid:114)
(cid:18) 16G2
T(cid:88)
t=1
In the second case, we assume
≤ β
ζt (x∗ ) ≤ 4G1
m
m
1
DT ln
+ 4G1 ln
DT +
4
δ
δ
β
√
(cid:19)
(cid:18) 16G2
t=1
T(cid:88)
ab ≤ a2 + b2 . We thus have
where the last step uses the fact 2
0G2
1 + λ2
(G2
(F (xt ) − F (x∗ )) ≤
m
2 )(1 + ln T )
1
+ 4G1
ln
+
β
δ
2β
(cid:19)
(cid:18) 16G2
T(cid:88)
t=1
Combing the results of the two cases, we have, with a probability 1 − δ ,
(cid:123)(cid:122)
(cid:124)
(cid:125)
0G2
1 + λ2
(G2
(F (xt ) − F (x∗ )) ≤
2 )(1 + ln T )
m
1
.
+ 4G1 +
+ 4G1
ln
β
δ
2β
By convexity of F (x), we have F ((cid:98)xT ) ≤ F (x∗ ) + O (ln T /T ). Noting that x∗ ∈ K, g(x∗ ) ≤ 0,
t=1
O(ln T )
(cid:18) λ0 g((cid:98)xT )
(cid:18)
(cid:19)(cid:19)
we have F (x∗ ) ≤ f (x∗ ) + γ ln 2. On the other hand,
F ((cid:98)xT ) = f ((cid:98)xT ) + γ ln
≥ f ((cid:98)xT ) + max (0, λ0 g((cid:98)xT )) .
1 + exp
(cid:19)
(cid:18) ln T
γ
f ((cid:98)xT ) ≤ f (x∗ ) + O
Therefore, with the value of γ = ln T /T , we have
(cid:18) ln T
(cid:19)
,
f ((cid:98)xT ) + λ0 g((cid:98)xT ) ≤ f (x∗ ) + O
T
.
(cid:18) ln T
(cid:19)
T
λ0ρ(cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 ≤ G1(cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 + O
Applying the inequalities (13) and (14) to (16), and noting that γ = ln T /T , we have
For λ0 > G1/ρ, we have (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 ≤ (1/(λ0ρ − G1 ))O(ln T /T ). Therefore
.
(cid:19)
(cid:18) ln T
T
f ((cid:101)xT ) ≤ f ((cid:101)xT ) − f ((cid:98)xT ) + f ((cid:98)xT ) ≤ G1 (cid:107)(cid:98)xT − (cid:101)xT (cid:107)2 + f (x∗ ) + O
≤ f (x∗ ) + O
T
where in the second inequality we use inequality (15).

(cid:18) ln T
T

(cid:19)

,

(15)

(16)

6 Conclusions

In the present paper, we made a progress towards making the SGD method efﬁcient by proposing a
framework in which it is possible to exclude the projection steps from the SGD algorithm. We have
proposed two novel algorithms to overcome the computational bottleneck of the projection step in
√
applying SGD to optimization problems with complex domains. We showed using novel theoretical
analysis that the proposed algorithms can achieve an O(1/
T ) convergence rate for general convex
functions and an O(ln T /T ) rate for strongly convex functions with a overwhelming probability
which are known to be optimal (up to a logarithmic factor) for stochastic optimization.

Acknowledgments

The authors would like to thank the anonymous reviewers for their helpful suggestions. This work
was supported in part by National Science Foundation (IIS-0643494) and Ofﬁce of Navy Research
(Award N000141210431 and Award N00014-09-1-0663).

8

References
[1] A. Agarwal, P. L. Bartlett, P. D. Ravikumar, and M. J. Wainwright.
Information-theoretic
lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions
on Information Theory, 58(5):3235–3249, 2012.
[2] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for
machine learning. In NIPS, pages 451–459, 2011.
[3] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2nd edition, 1999.
[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[5] K. L. Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM
Transactions on Algorithms, 6(4), 2010.
[6] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the l1-ball
for learning in high dimensions. In ICML, pages 272–279, 2008.
[7] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics,
3, 1956.
[8] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In LATIN, pages 306–316,
2008.
[9] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. Journal of Machine Learning Research - Proceedings
Track, 19:421–436, 2011.
[10] E. Hazan and S. Kale. Projection-free online learning. In ICML, 2012.
[11] M. Jaggi. Sparse Convex Optimization Methods for Machine Learning. PhD thesis, ETH
Zurich, Oct. 2011.
[12] M. Jaggi and M. Sulovsk ´y. A simple algorithm for nuclear norm regularized problems. In
ICML, pages 471–478, 2010.
[13] G. Lan. An optimal method for stochastic composite optimization. Math. Program., 133(1-
2):365–397, 2012.
[14] J. Liu and J. Ye. Efﬁcient euclidean projections in linear time. In ICML, page 83, 2009.
[15] M. Mahdavi, R. Jin, and T. Yang. Trading regret for efﬁciency: online convex optimization
with long term constraints. JMLR, 13:2465–2490, 2012.
[16] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM J. on Optimization, 19:1574–1609, 2009.
[17] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Aca-
demic Publishers, 2004.
[18] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127–
152, 2005.
[19] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In ICML, pages 807–814, 2007.
[20] Y. Ying and P. Li. Distance metric learning with eigenvalue optimization. JMLR., 13:1–26,
2012.
[21] T. Zhang. Sequential greedy approximation for certain convex optimization problems. Infor-
mation Theory, IEEE Transactions on, 49:682–691, 2003.
[22] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
ICML, pages 928–936, 2003.

9


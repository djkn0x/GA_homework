Scalable nonconvex inexact proximal splitting

Suvrit Sra
Max Planck Institute for Intelligent Systems
72076 T ¨ubigen, Germany
suvrit@tuebingen.mpg.de

Abstract

We study a class of large-scale, nonsmooth, and nonconvex optimization prob-
lems. In particular, we focus on nonconvex problems with composite objectives.
This class includes the extensively studied class of convex composite objective
problems as a subclass. To solve composite nonconvex problems we introduce a
powerful new framework based on asymptotically nonvanishing errors, avoiding
the common stronger assumption of vanishing errors. Within our new framework
we derive both batch and incremental proximal splitting algorithms. To our knowl-
edge, our work is ﬁrst to develop and analyze incremental nonconvex proximal-
splitting algorithms, even if we were to disregard the ability to handle nonvanish-
ing errors. We illustrate one instance of our general framework by showing an
application to large-scale nonsmooth matrix factorization.

1
Introduction
This paper focuses on nonconvex composite objective problems having the form
x ∈ X ,
(1)
minimize Φ(x) := f (x) + h(x)
where f : Rn → R is continuously differentiable, h : Rm → R ∪ {∞} is lower semi-continuous
(lsc) and convex (possibly nonsmooth), and X is a compact convex set. We also make the common
assumption that ∇f is locally (in X ) Lipschitz continuous, i.e., there is a constant L > 0 such that
(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107)
for all x, y ∈ X .
(2)

Problem (1) is a natural but far-reaching generalization of composite objective convex problems,
which enjoy tremendous importance in machine learning; see e.g., [2, 3, 11, 34]. Although, convex
formulations are extremely useful, for many difﬁcult problems a nonconvex formulation is natu-
ral. Familiar examples include matrix factorization [20, 23], blind deconvolution [19], dictionary
learning [18, 23], and neural networks [4, 17].
The primary contribution of this paper is theoretical. Speciﬁcally, we present a new algorithmic
framework: Nonconvex Inexact Proximal Splitting (N IPS). Our framework solves (1) by “splitting”
the task into smooth (gradient) and nonsmooth (proximal) parts. Beyond splitting, the most notable
feature of N IPS is that it allows computational errors. This capability proves critical to obtaining
a scalable, incremental-gradient variant of N IPS, which, to our knowledge, is the ﬁrst incremental
proximal-splitting method for nonconvex problems.
N IPS further distinguishes itself in how it models computational errors. Notably, it does not require
the errors to vanish in the limit, which is a more realistic assumption as often one has limited to no
control over computational errors inherent to a complex system. In accord with the errors, N IPS also
does not require stepsizes (learning rates) to shrink to zero. In contrast, most incremental-gradient
methods [5] and stochastic gradient algorithms [16] do assume that the computational errors and
stepsizes decay to zero. We do not make these simplifying assumptions, which complicates the
convergence analysis a bit, but results in perhaps a more satisfying description.

1

Our analysis builds on the remarkable work of Solodov [29], who studied the simpler setting of
differentiable nonconvex problems (which correspond with h ≡ 0 in (1)). N IPS is strictly more
general: unlike [29] it solves a non-differentiable problem by allowing a nonsmooth regularizer
h (cid:54)≡ 0, and this h is tackled by invoking proximal-splitting [8].
Proximal-splitting has proved to be exceptionally fruitful and effective [2, 3, 8, 11]. It retains the
simplicity of gradient-projection while handling the nonsmooth regularizer h via its proximity op-
erator. This approach is especially attractive because for several important choices of h, efﬁcient
implementations of the associated proximity operators exist [2, 22, 23]. For convex problems, an
alternative to proximal splitting is the subgradient method; similarly, for nonconvex problems one
may use a generalized subgradient method [7, 12]. However, as in the convex case, the use of sub-
gradients has drawbacks: it fails to exploit the composite structure, and even when using sparsity
promoting regularizers it does not generate intermediate sparse iterates [11].
Among batch nonconvex splitting methods, an early paper is [14]. More recently, in his pioneering
paper on convex composite minimization, Nesterov [26] also brieﬂy discussed nonconvex problems.
Both [14] and [26], however, enforced monotonic descent in the objective value to ensure conver-
gence. Very recently, Attouch et al. [1] have introduced a generic method for nonconvex nonsmooth
problems based on Kurdyka-Łojasiewicz theory, but their entire framework too hinges on descent.
A method that uses nonmontone line-search to eliminate dependence on strict descent is [13].
In general, the insistence on strict descent and exact gradients makes many of the methods unsuitable
for incremental, stochastic, or online variants, all of which usually lead to a nonmonotone objective
values especially due to inexact gradients. Among nonmonotonic methods that apply to (1), we are
aware of the generalized gradient-type algorithms of [31] and the stochastic generalized gradient
methods of [12]. Both methods, however, are analogous to the usual subgradient-based algorithms
that fail to exploit the composite objective structure, unlike proximal-splitting methods.
But proximal-splitting methods do not apply out-of-the-box to (1): nonconvexity raises signiﬁcant
obstructions, especially because nonmonotonic descent in the objective function values is allowed
and inexact gradient might be used. Overcoming these obstructions to achieve a scalable non-descent
based method that allows inexact gradients is what makes our N IPS framework novel.

2 The N IPS Framework
To simplify presentation, we replace h by the penalty function
g(x) := h(x) + δ(x|X ),
(3)
where δ(·|X ) is the indicator function for X : δ(x|X ) = 0 for x ∈ X , and δ(x|X ) = ∞ for x (cid:54)∈ X .
With this notation, we may rewrite (1) as the unconstrained problem:

(4)
minx∈Rn Φ(x) := f (x) + g(x),
and this particular formulation is our primary focus. We solve (4) via a proximal-splitting approach,
so let us begin by deﬁning our most important component.
Deﬁnition 1 (Proximity operator). Let g : Rn → R be an lsc, convex function. The proximity
(cid:0)g(x) + 1
2η (cid:107)x − y(cid:107)2 (cid:1).
operator for g , indexed by η > 0, is the nonlinear map [see e.g., 28; Def. 1.22]:
y (cid:55)→ argmin
P g
η :
x∈Rn
The operator (5) was introduced by Moreau [24] (1962) as a generalization of orthogonal projec-
tions. It is also key to Rockafellar’s classic proximal point algorithm [27], and it arises in a host of
proximal-splitting methods [2, 3, 8, 11], most notably in forward-backward splitting (FBS) [8].
FBS is particularly attractive because of its simplicity and algorithmic structure. It minimizes convex
composite objective functions by alternating between “forward” (gradient) steps and “backward”
(proximal) steps. Formally, suppose f in (4) is convex; for such f , FBS performs the iteration
(xk − ηk∇f (xk )),
xk+1 = P g
(6)
k = 0, 1, . . . ,
ηk
where {ηk } is a suitable sequence of stepsizes. The usual convergence analysis of FBS is intimately
tied to convexity of f . Therefore, to tackle nonconvex f we must take a different approach. As

(5)

2

previously mentioned, such approaches were considered by Fukushima and Mine [14] and Nesterov
[26], but both proved convergence by enforcing monotonic descent.
This insistence on descent severely impedes scalability. Thus, the key challenge is: how to retain
the algorithmic simplicity of FBS and allow nonconvex losses, without sacriﬁcing scalability?
We address this challenge by introducing the following inexact proximal-splitting iteration:
(xk − ηk∇f (xk ) + ηk e(xk )),
xk+1 = P g
(7)
k = 0, 1, . . . ,
ηk
where e(xk ) models the computational errors in computing the gradient ∇f (xk ). We also assume
that for η > 0 smaller than some stepsize ¯η , the computational error is uniformly bounded, that is,
η(cid:107)e(x)(cid:107) ≤ ¯,
for some ﬁxed error level ¯ ≥ 0,
and ∀x ∈ X .
(8)
(cid:88)
Condition (8) is weaker than the typical vanishing error requirements
k→∞ η(cid:107)e(xk )(cid:107) = 0,
η(cid:107)e(xk )(cid:107) < ∞,
lim
k
which are stipulated by most analyses of methods with gradient errors [4, 5]. Obviously, since errors
are nonvanishing, exact stationarity cannot be guaranteed. We will, however, show that the iterates
produced by (7) do progress towards reasonable inexact stationary points. We note in passing that
even if we assume the simpler case of vanishing errors, N IPS is still the ﬁrst nonconvex proximal-
splitting framework that does not insist on monotonicity, which complicates convergence analysis
but ultimately proves crucial to scalability.

Algorithm 1 Inexact Nonconvex Proximal Splitting (N IPS)
η , and a sequence {ηk } satisfying
Input: Operator P g
c ≤ lim inf k ηk ,
lim supk ηk ≤ min {1, 2/L − c} ,
Output: Approximate solution to (7)
k ← 0; Select arbitrary x0 ∈ X
Compute approximate gradient (cid:101)∇f (xk ) := ∇f (xk ) − e(xk )
while ¬ converged do
ηk (xk − ηk (cid:101)∇f (xk ))
Update: xk+1 = P g
k ← k + 1
end while

0 < c < 1/L.

(9)

2.1 Convergence analysis
We begin by characterizing inexact stationarity. A point x∗ is a stationary point for (4) if and only if
it satisﬁes the inclusion
0 ∈ ∂C Φ(x∗ ) := ∇f (x∗ ) + ∂ g(x∗ ),
(10)
where ∂C φ denotes the Clarke subdifferential [7]. A brief exercise shows that this inclusion may be
equivalently recast as the ﬁxed-point equation (which augurs the idea of proximal-splitting)
η (x∗ − η∇f (x∗ )),
x∗ = P g
for η > 0.
This equation helps us deﬁne a measure of inexact stationarity: the proximal residual
1 (x − ∇f (x)).
ρ(x) := x − P g
(12)
Note that for an exact stationary point x∗ the residual norm (cid:107)ρ(x∗ )(cid:107) = 0. Thus, we call a point x to
be -stationary if for a prescribed error level (x), the corresponding residual norm satisﬁes
(cid:107)ρ(x)(cid:107) ≤ (x).
(13)
that the iterates (cid:8)xk (cid:9) generated by (7) satisfy an approximate stationarity condition of the form (13),
Assuming the error-level (x) (say if ¯ = lim supk (xk )) satisﬁes the bound (8), we prove below
by allowing the stepsize η to become correspondingly small (but strictly bounded away from zero).
We start by recalling two basic facts, stated without proof as they are standard knowledge.

(11)

3

Lemma 2 (Lipschitz-descent [see e.g., 25; Lemma 2.1.3]). Let f ∈ C 1
L (X ). Then,
|f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)| ≤ L
∀ x, y ∈ X .
2 (cid:107)x − y(cid:107)2 ,
(14)
Lemma 3 (Nonexpansivity [see e.g., 9; Lemma 2.4]). The operator P g
η is nonexpansive, that is,
η (x) − P g
(cid:107)P g
η (y)(cid:107) ≤ (cid:107)x − y(cid:107),
∀ x, y ∈ Rn .
(15)
Next we prove a crucial monotonicity property that actually subsumes similar results for projection
operators derived by Gafni and Bertsekas [15; Lem. 1], and may therefore be of independent interest.
Lemma 4 (Prox-Monotonicity). Let y , z ∈ Rn , and η > 0. Deﬁne the functions
:= (cid:107)P g
η (cid:107)P g
η (y − ηz ) − y(cid:107),
η (y − ηz ) − y(cid:107).
and qg (η)
:= 1
pg (η)
Then, pg (η) is a decreasing function of η , and qg (η) an increasing function of η .

(16)

Proof. Our proof exploits properties of Moreau-envelopes [28; pp. 19,52], and we present it in the
language of proximity operators. Consider the “deﬂected” proximal objective
mg (x, η ; y , z ) := (cid:104)z , x − y(cid:105) + 1
2η (cid:107)x − y(cid:107)2 + g(x),
for some y , z ∈ X .
Associate to objective mg the deﬂected Moreau-envelope
(18)
x∈X mg (x, η ; y , z ),
Eg (η) := inf
η (y − ηz ). Thus, Eg (η) is differentiable, and its
whose inﬁmum is attained at the unique point P g
η (y − ηz ) − y(cid:107)2 = − 1
g (η) = − 1
2η2 (cid:107)P g
derivative is given by E (cid:48)
2 p(η)2 . Since Eg is convex in η ,
E (cid:48)
g is increasing ([28; Thm. 2.26]), or equivalently p(η) is decreasing. Similarly, deﬁne ˆeg (γ ) :=
Eg (1/γ ); this function is concave in γ as it is a pointwise inﬁmum (indexed by x) of functions linear
in γ [see e.g., §3.2.3 in 6]. Thus, its derivative ˆe(cid:48)
2 (cid:107)P g
1/γ (x − γ−1 y) − x(cid:107)2 = qg (1/γ ), is a
g (γ ) = 1
decreasing function of γ . Set η = 1/γ to conclude the argument about qg (η).

(17)

We now proceed to bound the difference between objective function values from iteration k to k + 1,
by developing a bound of the form

Φ(xk ) − Φ(xk+1 ) ≥ h(xk ).
(19)
Obviously, since we do not enforce strict descent, h(xk ) may be negative too. However, we show
that for sufﬁciently large k the algorithm makes enough progress to ensure convergence.
Lemma 5. Let xk+1 , xk , ηk , and X be as in (7), and that ηk (cid:107)e(xk )(cid:107) ≤ (xk ) holds. Then,
(cid:107)xk+1 − xk (cid:107)2 − 1
Φ(xk ) − Φ(xk+1 ) ≥ 2−Lηk
(xk )(cid:107)xk+1 − xk (cid:107).
2ηk
ηk

(20)

(21)

Proof. For the deﬂected Moreau envelope (17), consider the directional derivative dmg with respect
to x in the direction w; at x = xk+1 , this derivative satisﬁes the optimality condition
dmg (xk+1 , η ; y , z )(w) = (cid:104)z + η−1 (xk+1 − y) + sk+1 , w(cid:105) ≥ 0,
sk+1 ∈ ∂ g(xk+1 ).
Set z = ∇f (xk ) − e(xk ), y = xk , and w = xk − xk+1 in (21), and rearrange to obtain
(cid:104)∇f (xk ) − e(xk ), xk+1 − xk (cid:105) ≤ (cid:104)η−1 (xk+1 − xk ) + sk+1 , xk − xk+1 (cid:105).
From Lemma 2 it follows that
Φ(xk+1 ) ≤ f (xk ) + (cid:104)∇f (xk ), xk+1 − xk (cid:105) + L
2 (cid:107)xk+1 − xk (cid:107)2 + g(xk+1 ),
(23)
whereby upon adding and subtracting e(xk ), and then using (22) we further obtain
≤ f (xk ) + g(xk+1 ) + (cid:104)sk+1 , xk − xk+1 (cid:105) + (cid:0) L
(cid:1)(cid:107)xk+1 − xk (cid:107)2 + (cid:104)e(xk ), xk+1 − xk (cid:105)
f (xk ) + (cid:104)∇f (xk ) − e(xk ), xk+1 − xk (cid:105) + L
2 (cid:107)xk+1 − xk (cid:107)2 + g(xk+1 ) + (cid:104)e(xk ), xk+1 − xk (cid:105)
2 − 1
ηk
≤ f (xk ) + g(xk ) − 2−Lηk
(cid:107)xk+1 − xk (cid:107)2 + (cid:104)e(xk ), xk+1 − xk (cid:105)
2ηk
≤ Φ(xk ) − 2−Lηk
(cid:107)xk+1 − xk (cid:107)2 + (cid:107)e(xk )(cid:107)(cid:107)xk+1 − xk (cid:107)
2ηk
≤ Φ(xk ) − 2−Lηk
(xk )(cid:107)xk+1 − xk (cid:107).
(cid:107)xk+1 − xk (cid:107)2 + 1
2ηk
ηk
The second inequality above follows from convexity of g , the third one from Cauchy-Schwarz, and
the last one by assumption on (xk ). Now ﬂip signs and apply (23) to conclude the bound (20).

(22)

4

Next we further bound (20) by deriving two-sided bounds on (cid:107)xk+1 − xk (cid:107).
Lemma 6. Let xk+1 , xk , and (xk ) be as before; also let c and ηk satisfy (9). Then,
c(cid:107)ρ(xk )(cid:107) − (xk ) ≤ (cid:107)xk+1 − xk (cid:107) ≤ (cid:107)ρ(xk )(cid:107) + (xk ).

(24)

qg (ηk ).

Proof. First observe that from Lemma 4 that for ηk > 0 it holds that
and if ηk ≤ 1 then pg (1) ≤ pg (ηk ) = 1
if 1 ≤ ηk then q(1) ≤ qg (ηk ),
ηk
Using (25), the triangle inequality, and Lemma 3, we have
min {1, ηk } qg (1) = min {1, ηk } (cid:107)ρ(xk )(cid:107) ≤ (cid:107)P g
(xk − ηk∇f (xk )) − xk (cid:107)
ηk
≤ (cid:107)xk+1 − xk (cid:107) + (cid:107)xk+1 − P g
(xk − ηk∇f (xk ))(cid:107)
ηk
≤ (cid:107)xk+1 − xk (cid:107) + (cid:107)ηk e(xk )(cid:107) ≤ (cid:107)xk+1 − xk (cid:107) + (xk ).
From (9) it follows that for sufﬁciently large k we have (cid:107)xk+1 − xk (cid:107) ≥ c(cid:107)ρ(xk )(cid:107) − (xk ). For the
upper bound note that
(cid:107)xk+1 − xk (cid:107) ≤ (cid:107)xk − P g
(xk − ηk∇f (xk ))(cid:107) + (cid:107)P g
(xk − ηk∇f (xk )) − xk+1(cid:107)
ηk
ηk
≤ max {1, ηk } (cid:107)ρ(xk )(cid:107) + (cid:107)ηk e(xk )(cid:107) ≤ (cid:107)ρ(xk )(cid:107) + (xk ).

(25)

Lemma 5 and Lemma 6 help prove the following crucial corollary.
Corollary 7. Let xk , xk+1 , ηk , and c be as above and k sufﬁciently large so that c and ηk satisfy (9).
(cid:1)(cid:107)ρ(xk )(cid:107)(xk ) − (cid:0) 1
2(2−2Lc) (cid:107)ρ(xk )(cid:107)2 − (cid:0) L2 c2
(cid:1)(xk )2 .
Then, Φ(xk ) − Φ(xk+1 ) ≥ h(xk ) holds with h(xk ) given by
c − L2 c
h(xk ) := L2 c3
2−cL + 1
2(2−cL)
c
Proof. Plug in the bounds (24) into (20), invoke (9), and simplify—see [32] for details.
Let (cid:8)xk (cid:9) ⊂ X be a sequence generated by (7), and let condition (8) on each (cid:107)e(xk )(cid:107) hold. There
We now have all the ingredients to state the main convergence theorem.
L (X ) such that inf X f > −∞ and let g be lsc, convex on X .
Theorem 8 (Convergence). Let f ∈ C 1
exists a limit point x∗ of the sequence (cid:8)xk (cid:9), and a constant K > 0, such that (cid:107)ρ(x∗ )(cid:107) ≤ K (x∗ ).
If (cid:8)Φ(xk )(cid:9) converges, then for every limit point x∗ of (cid:8)xk (cid:9) it holds that (cid:107)ρ(x∗ )(cid:107) ≤ K (x∗ ).
Proof. Lemma 5, 6, and Corollary 7 have done all the hard work. Indeed, they allow us to reduce
our convergence proof to the case where the analysis of the differentiable case becomes applicable,
and an appeal to the analysis of [29; Thm. 2.1] grants us our claim.

(26)

Theorem 8 says that we can obtain an approximate stationary point for which the norm of the resid-
ual is bounded by a linear function of the error level. The statement of the theorem is written in a
old, the behavior of (cid:8)xk (cid:9) may be arbitrary. This, however, is a small price to pay for having the
conditional form, because nonvanishing errors e(x) prevent us from making a stronger statement.
In particular, once the iterates enter a region where the residual norm falls below the error thresh-
added ﬂexibility of nonvanishing errors. Under the stronger assumption of vanishing errors (and
diminishing stepsizes), we can also obtain guarantees to exact stationary points.
3 Scaling up N IPS: incremental variant
(cid:88)T
We now apply N IPS to the large-scale setting, where we have composite objectives of the form
(27)
ft (x) + g(x),
Φ(x) :=
t=1
gradient (cid:80)
where each ft : Rn → R is a C 1
(X ) function. For simplicity, we use L = maxt Lt in the sequel.
Lt
It is well-known that for such decomposable objectives it can be advantageous to replace the full
t ∇ft (x) by an incremental gradient ∇fσ(t) (x), where σ(t) is some suitable index.

5

Nonconvex incremental methods for differentiable problems have been extensively analyzed, e.g.,
backpropagation algorithms [5, 29], which correspond to g(x) ≡ 0. However, when g(x) (cid:54)= 0, the
only incremental methods that we are aware of are stochastic generalized gradient methods of [12]
or the generalized gradient methods of [31]. As previously mentioned, both of these fail to exploit
the composite structure of the objective function, a disadvantage even in the convex case [11].
In stark contrast, we do exploit the composite structure of (27). Formally, we propose the following
(cid:88)T
xk+1 = M(cid:0)xk − ηk
∇ft (xk,t )(cid:1),
incremental nonconvex proximal-splitting iteration:
k = 0, 1, . . . ,
t=1
t = 1, . . . , T − 1,
xk,1 = xk , xk,t+1 = O(xk,t − ηk∇ft (xk,t )),
where O and M are appropriate operators, different choices of which lead to different algorithms.
For example, when X = Rn , g(x) ≡ 0, M = O = Id, and ηk → 0, then (28) reduces to the classic
incremental gradient method (IGM) [4], and to the IGM of [30], if lim ηk = ¯η > 0. If X a closed
convex set, g(x) ≡ 0, M is orthogonal projection onto X , O = Id, and ηk → 0, then iteration (28)
reduces to (projected) IGM [4, 5].
We may consider four variants of (28) in Table 1; to our knowledge, all of these are new. Which
of the four variants one prefers depends on the complexity of the constraint set X and cost to apply
P g
η . The analysis of all four variants is similar, so we present details only for the most general case.
X
M O
g
(cid:54)≡ 0
Rn
P g
Id
(cid:54)≡ 0
η
Rn
η P g
P g
h(x) + δ(X |x) P g
η
Convex
Id
h(x) + δ(X |x) P g
η
η P g
Convex
η

Proximity operator calls
once every major (k) iteration
once every minor (k , t) iteration
once every major (k) iteration
once every minor (k , t) iteration

Penalty and constraints
penalized, unconstrained
penalized, unconstrained
penalized, constrained
penalized, constrained

(28)

Table 1: Different variants of incremental N IPS (28).

xk+1 = P g
η

3.1 Convergence analysis
Speciﬁcally, we analyze convergence for the case M = O = P g
η by generalizing the differentiable
(cid:88)T
(cid:0)xk − ηk
∇ft (xk,t )(cid:1)
case treated by [30]. We begin by rewriting (28) in a form that matches the main iteration (7):
(cid:88)T
(cid:2)(cid:88)T
(cid:0)xk − ηk
ft (xk ) − ft (xk,t )(cid:3)(cid:1)
t=1
(cid:88)
∇ft (xk ) + ηk e(xk )(cid:1).
(cid:0)xk − ηk
∇ft (xk ) + ηk
t=1
t=1
t
To show that iteration (29) is well-behaved and actually ﬁts the main N IPS iteration (7), we must
ensure that the norm of the error term is bounded. We show this via a sequence of lemmas.
Lemma 9 (Bounded-increment). Let xk,t+1 be computed by (28), and let st ∈ ∂ g(xk,t ). Then,
(cid:107)xk,t+1 − xk,t (cid:107) ≤ 2ηk (cid:107)∇ft (xk,t ) + st(cid:107).

= P g
η

= P g
η

(29)

(30)

Proof. From the deﬁnition of a proximity operator (5), we have the inequality
2 (cid:107)xk,t+1 − xk,t + ηk∇ft (xk,t )(cid:107)2 + ηk g(xk,t+1 ) ≤ 1
2 (cid:107)ηk∇ft (xk,t )(cid:107)2 + ηk g(xk,t ),
1
=⇒ 1
2 (cid:107)xk,t+1 − xk,t (cid:107)2 ≤ ηk (cid:104)∇ft (xk,t ), xk,t − xk,t+1 (cid:105) + ηk (g(xk,t ) − g(xk,t+1 )).
Since st ∈ ∂ g(xk,t ), we have g(xk,t+1 ) ≥ g(xk,t ) + (cid:104)st , xk,t+1 − xk,t (cid:105). Therefore,
2 (cid:107)xk,t+1 − xk,t (cid:107)2 ≤ ηk (cid:104)st , xk,t − xk,t+1 (cid:105) + (cid:104)∇ft (xk,t ), xk,t − xk,t+1 (cid:105)
1
≤ ηk (cid:107)st + ∇ft (xk,t )(cid:107)(cid:107)xk,t − xk,t+1(cid:107)
=⇒ (cid:107)xk,t+1 − xk,t (cid:107) ≤ 2ηk (cid:107)∇ft (xk,t ) + st(cid:107).

Lemma 9 proves helpful in bounding the overall error.

6

Lemma 10 (Bounded error). If for all xk ∈ X , (cid:107)∇ft (xk )(cid:107) ≤ M and (cid:107)∂ g(xk )(cid:107) ≤ G, then there
exists a constant K1 > 0 such that (cid:107)e(xk )(cid:107) ≤ K1 .

(31)

Proof. To bound the error of using xk,t instead of xk ﬁrst deﬁne the term
t := (cid:107)∇ft (xk,t ) − ∇ft (xk )(cid:107),
t = 1, . . . , T .
(cid:88)t−1
Then, an inductive argument (see [32] for details) shows that for 2 ≤ t ≤ T
Since (cid:107)e(xk )(cid:107) = (cid:80)T
t ≤ 2ηkL
(1 + 2ηkL)t−1−j (cid:107)∇fj (xk ) + sj (cid:107).
(32)
j=1
(cid:16)(cid:88)T −t−1
(1 + 2ηkL)j (cid:17)
(cid:88)T
(cid:88)t−1
(cid:88)T −1
(cid:88)T
t=1 t , and 1 = 0, (32) then leads to the bound
≤ (cid:88)T −1
(1 + 2ηkL)T −tβt ≤ (1 + 2ηkL)T −1 (cid:88)T −1
t ≤ 2ηkL
(1 + 2ηkL)t−1−j βj = 2ηkL
βt
t=1
j=1
t=2
t=2
j=0
(cid:107)∇ft (x) + st(cid:107)
t=1
t=1
≤ C1 (T − 1)(M + G) =: K1 .
Thus, the error norm (cid:107)e(xk )(cid:107) is bounded from above by a constant, whereby it satisﬁes the require-
ment (8), making the incremental N IPS method (28) a special case of the general N IPS framework.
This allows us to invoke the convergence result Theorem 8 for without further ado.
4
Illustrative application
The main contribution of our paper is the new N IPS framework, and a speciﬁc application is not
(cid:88)T
one of the prime aims of this paper. We do, however, provide an illustrative application of N IPS to
a challenging nonconvex problem: sparsity regularized low-rank matrix factorization
2 (cid:107)Y − X A(cid:107)2
(33)
1
min
F + ψ0 (X ) +
ψt (at ),
X,A≥0
t=1
where Y ∈ Rm×T , X ∈ Rm×K and A ∈ RK×T , with a1 , . . . , aT as its columns. Problem (33)
generalizes the well-known nonnegative matrix factorization (NMF) problem of [20] by permitting
arbitrary Y (not necessarily nonnegative), and adding regularizers on X and A. A related class of
problems was studied in [23], but with a crucial difference: the formulation in [23] does not allow
nonsmooth regularizers on X . The class of problems studied in [23] is in fact a subset of those cov-
ered by N IPS. On a more theoretical note, [23] considered stochastic-gradient like methods whose
analysis requires computational errors and stepsizes to vanish, whereas our method is deterministic
and allows nonvanishing stepsizes and errors.
(cid:88)T
Following [23] we also rewrite (33) in a form more amenable to N IPS. We eliminate A and consider
g(X ) := ψ0 (X ) + δ(X |≥ 0),
ft (X ) + g(X ), where
minX φ(X ) :=
t=1
and where each ft (X ) for 1 ≤ t ≤ T is deﬁned as
2 (cid:107)yt − X a(cid:107)2 + gt (a),
(35)
1
ft (X ) := mina
where gt (a) := ψt (a) + δ(a|≥ 0). For simplicity, assume that (35) attains its unique1 minimum,
say a∗ , then ft (X ) is differentiable and we have ∇X ft (X ) = (X a∗ − yt )(a∗ )T . Thus, we can
instantiate (28), and all we need is a subroutine for solving (35).2
We present empirical results on the following two variants of (34): (i) pure unpenalized NMF (ψt ≡
0 for 0 ≤ t ≤ T ) as a baseline; and (ii) sparsity penalized NMF where ψ0 (X ) ≡ λ(cid:107)X (cid:107)1 and
ψt (at ) ≡ γ (cid:107)at(cid:107)1 . Note that without the nonnegativity constraints, (34) is similar to sparse-PCA.
(cid:74)i(cid:75) RAND: 4000 × 4000 dense random (uniform
[0, 1]); rank-32 factorization; (λ, γ ) = (10−5 , 10); (cid:74)ii(cid:75) CBCL: CBCL database [33]; 361 × 2429;
We use the following datasets and parameters:
rank-49 factorization; (cid:74)iii(cid:75) YALE: Yale B Database [21]; 32256×2414 matrix; rank-32 factorization;
(cid:74)iv(cid:75) WEB: Web graph from google; sparse 714545 × 739454 (empty rows and columns removed)
matrix; ID: 2301 in the sparse matrix collection [10]); rank-4 factorization; (λ = γ = 10−6 ).
1Otherwise, at the expense of more notation, we can add a small strictly convex perturbation to ensure
uniqueness; this perturbation can be then absorbed into the overall computational error.
2 In practice, it is better to use mini-batches, and we used the same sized mini-batches for all the algorithms.

(34)

7

Figure 1: Running times of N IPS (Matlab) versus SPAMS (C++) for NMF on RAND, CBCL, and YALE
datasets. Initial objective values and tiny runtimes have been suppressed for clarity of presentation.

On the NMF baseline (Fig. 1), we compare N IPS against the well optimized state-of-the-art C++
toolbox SPAMS (version 2.3) [23]. We compare against SPAMS only on dense matrices, as its NMF
code seems to be optimized for this case. Obviously, the comparison is not fair: unlike SPAMS,
N IPS and its subroutines are all implemented in MATLAB, and they run equally easily on large
sparse matrices. Nevertheless, N IPS proves to be quite competitive: Fig. 1 shows that our MAT LAB
implementation runs only slightly slower than SPAMS. We expect a well-tuned C++ implementation
of N IPS to run at least 4–10 times faster than the MAT LAB version—the dashed line in the plots
visualizes what such a mere 3X-speedup to N IPS might mean.
Figure 2 shows numerical results comparing the stochastic generalized gradient (SGGD) algorithm
of [12] against N IPS, when started at the same point. As in well-known, SGGD requires careful
stepsize tuning; so we searched over a range of stepsizes, and have reported the best results. N IPS
too requires some stepsize tuning, but substantially lesser than SGGD. As predicted, the solutions
returned by N IPS have objective function values lower than SGGD, and have greater sparsity.

Figure 2: Sparse NMF: N IPS versus SGGD. The bar plots show the sparsity (higher is better) of the factors
X and A. Left plots for RAND dataset; right plots for WEB. As expected, SGGD yields slightly worse objective
function values and less sparse solutions than N IPS .

5 Discussion

We presented a new framework called N IPS, which solves a broad class of nonconvex composite
objective problems. N IPS permits nonvanishing computational errors, which can be practically use-
ful. We specialized N IPS to also obtain a scalable incremental version. Our numerical experiments
on large scale matrix factorization indicate that N IPS is competitive with state-of-the-art methods.
We conclude by mentioning that N IPS includes numerous other algorithms as special cases. For ex-
ample, batch and incremental convex FBS, convex and nonconvex gradient projection, the proximal-
point algorithm, among others. Theoretically, however, the most exciting open problem resulting
from this paper is: extend N IPS in a scalable way when even the nonsmooth part is nonconvex.
This case will require very different convergence analysis, and is left to the future.

References
[1] H. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent methods for semi-algebraic and tame prob-
lems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Math.

8

010203040506070100Running time (seconds)Objective function value  NIPSSPAMS0510152025102.1102.2102.3Running time (seconds)Objective function value  NIPSSPAMS050100150200250300350400102.3102.4102.5102.6102.7102.8Running time (seconds)Objective function value  NIPSSPAMS01020304050607080101102103104Running time (seconds)Objective function value  NIPSSGGD00.10.20.30.40.50.60.70.80.9SGGD−ANIPS−ASGGD−XNIPS−XSparsityProgramming Series A, Aug. 2011. Online First.
[2] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. In
S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning. MIT Press, 2011.
[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Prob-
lems. SIAM J. Imgaging Sciences, 2(1):183–202, 2009.
[4] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, second edition, 1999.
[5] D. P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A
Survey. Technical Report LIDS-P-2848, MIT, August 2010.
[6] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, March 2004.
[7] F. H. Clarke. Optimization and nonsmooth analysis. John Wiley & Sons, Inc., 1983.
[8] P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. arXiv:0912.3522v4,
May 2010.
[9] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale
Modeling and Simulation, 4(4):1168–1200, 2005.
[10] T. A. Davis and Y. Hu. The University of Florida Sparse Matrix Collection. ACM Transactions on
Mathematical Software, 2011. To appear.
[11] J. Duchi and Y. Singer. Online and Batch Learning using Forward-Backward Splitting. J. Mach. Learning
Res. (JMLR), Sep. 2009.
[12] Y. M. Ermoliev and V. I. Norkin. Stochastic generalized gradient method for nonconvex nonsmooth
stochastic optimization. Cybernetics and Systems Analysis, 34:196–215, 1998.
[13] M. A. T. Figueiredo, R. D. Nowak, and S. J. Wright. Gradient Projection for Sparse Reconstruction:
Application to Compressed Sensing and Other Inverse Problems. IEEE J. Selected Topics in Sig. Proc., 1
(4):586–597, 2007.
[14] M. Fukushima and H. Mine. A generalized proximal point algorithm for certain non-convex minimization
problems. Int. J. Systems Science, 12(8):989–1000, 1981.
[15] E. M. Gafni and D. P. Bertsekas. Two-metric projection methods for constrained optimization. SIAM
Journal on Control and Optimization, 22(6):936–964, 1984.
[16] A. A. Gaivoronski. Convergence properties of backpropagation for neural nets via theory of stochastic
gradient methods. Part 1. Optimization methods and Software, 4(2):117–134, 1994.
[17] S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall PTR, 1st edition, 1994.
[18] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and T. J. Sejnowski. Dictionary
learning algorithms for sparse representation. Neural Computation, 15:349–396, 2003.
[19] D. Kundur and D. Hatzinakos. Blind image deconvolution. IEEE Signal Processing Magazine, 13(3),
May 1996.
[20] D. D. Lee and H. S. Seung. Algorithms for Nonnegative Matrix Factorization. In NIPS, 2000.
[21] K.C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting.
IEEE Trans. Pattern Anal. Mach. Intelligence, 27(5):684–698, 2005.
[22] J. Liu and J. Ye. Efﬁcient Euclidean projections in linear time. In ICML, Jun. 2009.
[23] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online Learning for Matrix Factorization and Sparse Coding.
JMLR, 11:10–60, 2010.
[24] J. J. Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien. C. R. Acad. Sci.
Paris Sr. A Math., 255:2897–2899, 1962.
[25] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.
[26] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 2007/76,
Universit catholique de Louvain, September 2007.
[27] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM J. Control and Optimiza-
tion, 14, 1976.
[28] R. T. Rockafellar and R. J.-B. Wets. Variational analysis. Springer, 1998.
[29] M. V. Solodov. Convergence analysis of perturbed feasible descent methods. J. Optimization Theory and
Applications, 93(2):337–353, 1997.
[30] M. V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computational
Optimization and Applications, 11:23–35, 1998.
[31] M. V. Solodov and S. K. Zavriev. Error stability properties of generalized gradient-type algorithms. J.
Optimization Theory and Applications, 98(3):663–680, 1998.
[32] S. Sra. Nonconvex proximal-splitting: Batch and incremental algorithms. Sep. 2012. arXiv:1109.0258v2.
[33] K.-K. Sung. Learning and Example Selection for Object and Pattern Recognition. PhD thesis, MIT, 1996.
[34] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In NIPS,
2009.

9


Active Learning of Multi-Index Function Models

Hemant Tyagi and Volkan Cevher
LIONS – EPFL

Abstract

f (x) = g(Ax) = (cid:80)k
We consider the problem of actively learning multi-index functions of the form
i x) from point evaluations of f . We assume that
i=1 gi (aT
the function f is deﬁned on an (cid:96)2 -ball in Rd , g is twice continuously differen-
tiable almost everywhere, and A ∈ Rk×d is a rank k matrix, where k (cid:28) d. We
propose a randomized, active sampling scheme for estimating such functions with
uniform approximation guarantees. Our theoretical developments leverage recent
techniques from low rank matrix recovery, which enables us to derive an estima-
tor of the function f along with sample complexity bounds. We also characterize
the noise robustness of the scheme, and provide empirical evidence that the high-
dimensional scaling of our sample complexity bounds are quite accurate.

1
Introduction
Learning functions f : x → y based on training data (yi , xi )m
i=1 : R × Rd is a fundamental problem
with many scientiﬁc and engineering applications. Often, the function f has a parametric model,
model parameters. In this setting, obtaining an approximate model (cid:98)f when d (cid:29) 1 is challenging due
as in linear regression when f (x) = aT x, and hence, learning the function amounts to learning the
to the curse-of-dimensionality. Fortunately, low-dimensional parameter models, such as sparsity and
low-rank models, enable successful learning from dimensionality reduced or incomplete data [1, 2].
Since any parametric form is at best an approximation, non-parametric models remain as important
alternatives where we also attempt to learn the structure of the mapping f from data [3–15]. Unfortu-
nately, the curse-of-dimensionality problem in non-parametric function learning in high-dimensions
is particularly difﬁcult even with smoothness assumptions on f [16–18]. For instance, learning
functions f ∈ C s (i.e., the derivatives f (cid:48) , . . . , f (s) exist and are continuous), deﬁned over compact
(cid:107)f − (cid:98)f (cid:107)L∞ ≤ δ ) [17]. Surprisingly, even inﬁnitely differentiable functions (s = ∞) are not immune
supports, require m = Ω((1/δ)d/s ) samples for a uniform approximation guarantee of δ (cid:28) 1 (i.e.,
to this problem (m = Ω(2(cid:98)d/2(cid:99) )) [18]. Therefore, further assumptions on the multivariate functions
beyond smoothness are needed for the tractability of successful learning [13, 14, 16, 19].
k(cid:88)
k(cid:88)
To this end, we seek to learn low-dimensional function models f (x) = g(Ax) that decompose as
i x) | Model 2: f (x) = aT
gi (aT
1 x +
i=1
i=2
thereby constraining f to effectively live on k-dimensional subspaces, where k (cid:28) d. The models in
(1) have several important machine learning applications, and are known as the multi-index models
in statistics and econometrics, and multi-ridge functions in signal processing [4–7, 20–24].
i=1 is given a priori, we posit the
In stark contrast to the classical regression setting where (yi , xi )m
active learning setting where we can query the function to obtain ﬁrst an explicit approximation of
A and subsequently of f . As a stylized example of the active learning setting, consider numerical
solutions of parametric partial differential equations (PDE). Given PDE(f , x) = 0, where f (x):

Model 1: f (x) =

gi (aT
i x),

(1)

1

Ω → R is the implicit solution, obtaining a function sample typically requires running a computa-
tionally expensive numerical solver. As we have the ability to choose the samples, we can minimize
the number of queries to the PDE solver in order to learn an explicit approximation of f [13].

Background: To set the context for our contributions, it is necessary to review the (rather ex-
tensive) literature that revolve around the models (1). We categorize the earlier works by how the
samples are obtained (regression (passive) vs. active learning), what the underlying low-dimensional
model is (low-rank vs. sparse), and how the smoothness is encoded (kernels vs. C s ).
Regression/low-rank [3–7]: We consider the function model f (x) = g(Ax) to be kernel smooth
or C s . Noting the differentiability of f , we observe that the gradients ∇f (x) = AT ∇g(Ax) live
within the low-dimensional subspaces of AT . Assuming that ∇g has sufﬁcient richness to span k-
dimensional subspaces of the rows of A, we use the given samples to obtain Hessian estimates via
local smoothing techniques, such as kernel estimates, nearest-neighbor, or spline methods. We then
use the k-principal vectors of the estimated Hessian to approximate A. In some cases, we can even
establish asymptotic distribution of A estimates but not ﬁnite sample complexity bounds.
Regression/sparse [8–12]: We add sparsity restrictions on the function models: for instance, we
assume only one coordinate is active per additive term in (1). To encode smoothness, we restrict
f to a particular functional space, such as the reproducing kernel Hilbert or Sobolev spaces. We
with guarantees of the form (cid:107)f − (cid:98)f (cid:107)L2 ≤ δ , which grow logarithmically with d as well as match the
employ greedy algorithms, back-ﬁtting approaches, or convex regularizers to not only estimate the
active coordinates but also the function itself. We can then establish ﬁnite sample complexity rates
minimax bounds for the learning problem. Moreover, the function estimation incurs a linear cost in
k since the problem formulation affords a rotation-free structure between x and gi ’s.
function learning makes sparsity assumptions on A to obtain guarantees of the form (cid:107)f − (cid:98)f (cid:107)L∞ ≤ δ ,
Active learning [13–15]: The majority of the (rather limited) literature on active non-parametric
where f ∈ C s with s > 1.1 For instance, we consider the form f (x) = g(Ax), where the rows
of A live in a weak (cid:96)q ball with q < 2 (i.e., they are approximately sparse).2 We then leverage
a prescribed random sampling, and prove that the sample complexity grows logarithmically with
d and is inversely proportional to the k-th singular value of a “Hessian” matrix H f (for a precise
deﬁnition of H f , see (7)). Thus far, the only known characterization for the k-th singular value of
H f is for radial basis functions, i.e., f (x) = g((cid:107)Ax(cid:107)2 ). Just recently, we also see a low-rank model
to handle f (x) = g(aT x) for a general a (k = 1) with a sample complexity proportional to d [15].

Our contributions:
In this paper, which is a summary of [26], we take the active learning per-
spective via low-rank methods, where we have a general A with only C s assumptions on gi ’s.
Our main contributions are as follows:

1. k-th singular value of H f [14, 15]: Based on the random sampling schemes of [14, 15], we
rigorously establish the ﬁrst high-dimensional scaling characterization of the k-th singular
value of H f , which governs the sample complexity in both sparse and general A for the
multi-index models in (1). To achieve this result, we introduce an easy-to-verify, new
analysis tool based on Lipschitz continuous second order partial derivatives.
2. Generalization of [13–15]: We derive the ﬁrst sample complexity bound for the C s func-
tions in (1) with arbitrary number of linear parameters k without the compressibility as-
sumptions on the rows of A. Along the way, we leverage the conventional low-rank mod-
els in regression approaches and bridge them with the recent low-rank recovery algorithms.
Our result also lifts the sparse additive models in regression [8–12] to a basis-free setting.
3. Impact of additive noise: We analytically show how additive white Gaussian noise in the
function queries impacts the sample complexity of our low-rank approach.

1Not to be confused with the online active learning approaches, which “optimize” a function, such as ﬁnding
its maximum [25]. In contrast, we would like to obtain uniform approximation guarantees on f , which might
lead to redundant samples if we truly are only interested in ﬁnding a critical point of the function.
2As having one known basis to sparsify all k-dimensions in order to obtain a sparse A is rather restrictive,
this model does not provide a basis-free generalization of the sparse additive models in regression [8–12].

2

2 A recipe for active learning of low-dimensional non-parametric models

This section provides the preliminaries for our low-rank active learning approach for multi-index
models in (1). We ﬁrst introduce our sampling scheme (based on [14, 15]), summarize our main
observation model (based on [6, 7, 14, 15]), and explain our algorithmic approach (based on [15]).
This discussion sets the stage for our main theoretical contributions, as described in Section 4.

where BRd

with probability 1/2

Our sampling scheme: Our sampling approach relies on a speciﬁc interaction of two sets: sam-
pling centers and an associated set of directions for each center. We denote the set of sampling
centers as X = {ξj ∈ Sd−1 ; j = 1, . . . , mX }. We form X by sampling points uniformly at random
in Sd−1 (the unit sphere in d-dimensions) according to the uniform measure µSd−1 . Along with
each ξj ∈ X , we deﬁne a directions vector Φj = [φ1,j | . . . |φmΦ ,j ]T , and construct the sampling
(cid:26)
(cid:27)
(cid:16)(cid:112)d/mΦ
(cid:17)
directions operator Φ for j = 1, . . . , mX , i = 1, . . . , mΦ , and l = 1, . . . , d as
φi,j ∈ BRd
: [φi,j ]l = ± 1√
(cid:17)
(cid:16)(cid:112)d/mΦ
is the (cid:96)2 -ball with radius r = (cid:112)d/mΦ .
Φ =
mΦ
Our low-rank observation model: We ﬁrst write the Taylor series approximation of f as follows
f (x + φ) = f (x) +  (cid:104)φ, ∇f (x)(cid:105) + E (x, , φ); E (x, , φ) =
φT ∇2 f (ζ (x, φ))φ,

(3)
2
where  (cid:28) 1, E (x, , φ) is the curvature error, and ζ (x, φ) ∈ [x, x + φ] ∈ BRd (1 + r). Substi-
(cid:10)φ, AT ∇g(Ax)(cid:11) =
tuting f (x) = g(Ax) into (3), we obtain a perturbed observation model (∇g(·) is a k × 1 vector):
(f (x + φ) − f (x)) − E (x, , φ).
1

We then introduce a matrix X := AT G with G := [∇g(Aξ1 )|∇g(Aξ2 )| · · · |∇g(AξmX )]k×mX .
Based on (4), we then derive the following linear system via the operator Φ : Rd×mX → RmΦ
mX(cid:88)
[f (ξj + φi,j ) − f (ξj )] ,
y = Φ(X) + ε; yi = −1
where y ∈ RmΦ are the perturbed measurements of X with [Φ(X)]j = trace (cid:0)ΦT
j X(cid:1), and ε =
(5)
j=1
E (X , , Φ) is the curvature perturbations. The formulation (5) motivates us to leverage afﬁne rank-
minimization algorithms [27–29] for low-rank matrix recovery since rank(X) ≤ k (cid:28) d.

,

(2)

(4)

Our active low-rank learning algorithm Algorithm 1 outlines the main steps involved in our
low-rank matrix to (cid:98)A using the singular value decomposition (SVD) and rank-k approximation.
approximation scheme. Step 1 constructs the operator Φ and the measurements y, given mΦ , mX ,
and . Step 2 revolves around the afﬁne-rank minimization algorithms. Step 3 maps the recovered
Given (cid:98)A, step 4 constructs (cid:98)f (x) = (cid:98)g( (cid:98)Ax) as our estimator, where (cid:98)g(y) = f ( (cid:98)AT y).
Algorithm 1: Active learner algorithm for the non-parametric model f (x) = g(Ax)
2: Obtain (cid:98)X via a stable low-rank recovery algorithm (see Section 3 for an example).
1: Choose mΦ , mX , and  and construct the sets X and Φ, and the measurements y.
3: Compute SVD( (cid:98)X) = (cid:98)U (cid:98)Σ (cid:98)VT and set (cid:98)AT = (cid:98)U(k) , corresponding to k largest singular values.
4: Obtain an approximation (cid:98)f (x) := (cid:98)g( (cid:98)Ax) via quasi interpolants where (cid:98)g(y) := f ( (cid:98)AT y).
Remark 1. We uniformly approximate the function (cid:98)g by ﬁrst sampling it on a rectangular grid:
hZk ∩ (−(1 + ¯), (1 + ¯))k with uniformly spaced points in each direction (step size h). We then
use quasi-interpolants to interpolate in between the points thereby obtaining the approximation ˆgh ,
where the complexity is exponential in k (see the tractability discussion in the introduction). We
refer the reader to Chapter 12 of [17] regarding the construction of these operators.

3

3 Stable low-rank recovery algorithms within our learning scheme
By stable low-rank recovery in Algorithm 1, we mean any algorithm that returns an (cid:98)X with the
following guarantee: (cid:107)X − (cid:98)X(cid:107)F ≤ c1 (cid:107)X − Xk (cid:107)F + c2(cid:107)ε(cid:107)2 , where c1,2 are constants, and Xk is the
best rank-k approximation of X. Since there exists a vast set of algorithms with such guarantees, we
use the matrix Dantzig selector [29] as a running example. This discussion is intended to expose the
reader to the key elements necessary to re-derive the sample complexity of our scheme in Section 4
for different algorithms, which might offer additional computational trade-offs.

Stable embedding: We ﬁrst explain an elementary result stating that our sampling mechanism
satisﬁes the restricted isometry property (RIP) for all rank-k matrices with overwhelming probabil-
≤ (1 + κk ) (cid:107)Xk (cid:107)2
F ≤ (cid:107)Φ(Xk )(cid:107)2
ity. That is, (1 − κk ) (cid:107)Xk (cid:107)2
F , where κk is the RIP constant [29]).
(cid:96)2
This property can be used in establishing stability of virtually all low-rank recovery algorithms.
As Φ in (5) is a Bernoulli random measurement ensemble, it follows from standard concentration in-
equalities [30, 31] that for any rank-k X ∈ Rd×mX , we have P(| (cid:107)Φ(X)(cid:107)2
− (cid:107)X(cid:107)2
F | > t (cid:107)X(cid:107)2
F ) ≤
(cid:96)2
(cid:17)
(cid:16) 36
(cid:17)
(cid:16)
t ∈ (0, 1). By using a standard covering argument, as shown in Theorem 2.3 of
2e− mΦ
2 (t2 /2−t3 /3) ,
[29], we can verify that our Φ satisﬁes RIP with isometry constant 0 < κk < κ < 1 with probability
√
κ2 − κ3
at least 1 − 2e−mΦ q(κ)+k(d+mX +1)u(κ) , where q(κ) = 1
.
and u(κ) = log
2
144
9
κ
(cid:98)XDS = arg min
Recovery algorithm and its tuning parameters: The Dantzig selector criteria is given by
(cid:107)M (cid:107)∗ s.t. (cid:107)Φ∗ (y − Φ(M ))(cid:107) ≤ λ,
(6)
M
where (cid:107)·(cid:107)∗ and (cid:107)·(cid:107) are the nuclear and spectral norms, respectively, and λ is a tuning parameter. We
require the true X to be feasible, i.e., (cid:107)Φ∗ (ε)(cid:107) ≤ λ. Hence, the parameter λ can be obtained via
√
Proposition 1. In (5), we have (cid:107)ε(cid:107)(cid:96)mΦ
. Moreover, it holds that (cid:107)Φ∗ (ε)(cid:107) ≤ λ =
≤ C2 dmX k2
2
mΦ
2
√
(1 + κ)1/2 , with probability at least 1 − 2e−mΦ q(κ)+(d+mX +1)u(κ) .
C2 dmX k2
2
mΦ
Proposition 1 is a new result that provides the typical low-rank recovery algorithm tuning parameters
rows of A are compressible, that is ((cid:80)d
for the random sampling scheme in Section 2. We prove Proposition 1 in [26]. Note that the
dimension d appears in the bound as we do not make any compressibility assumption on A. If the
j=1 |aij |q )1/q ≤ D1 ∀ i = 1, . . . , k for some 0 < q <
1, D1 > 0, we can then remove the explicit d-dependence in the bound here.
Stability of low-rank recovery: We ﬁrst restate a stability result from [29] for bounded noise in
error bound for the rank-k approximation (cid:98)X(k)
Theorem 1. We then exploit this result in Corollary 1 along with Proposition 1 in order to obtain the
Theorem 1 (Theorem 2.4 in [29]). Let rank(X) ≤ k and let (cid:98)XDS be the solution to (6). If κ4k <
DS to X in step 4 of our Algorithm 1:
√
(cid:13)(cid:13)(cid:13) (cid:98)XDS − X
(cid:13)(cid:13)(cid:13)2
2−1 and (cid:107)Φ∗ (ε)(cid:107) ≤ λ, then we have with probability at least 1−2e−mΦ q(κ)+4k(d+mX +1)u(κ)
κ <
≤ C0kλ2 ,
Corollary 1. Denoting (cid:98)XDS to be the solution of (6), if (cid:98)X(k)
F
where C0 depends only on the isometry constant κ4k .
(cid:98)XDS in the sense of (cid:107)·(cid:107)F , and if κ4k < κ <
DS is the best rank-k approximation to
√
(cid:13)(cid:13)(cid:13)X − (cid:98)X(k)
(cid:13)(cid:13)(cid:13)2
2 − 1, then we have
2 k5 2d2m2X
C0C 2
≤ 4C0kλ2 =
DS
mΦ
F
with probability at least 1 − 2e−mΦ q(κ)+4k(d+mX +1)u(κ) .
Corollary 1 is the main result of this section, which is proved in [26]. The approximation guarantee
in Corollary 1 can be tightened if other low-rank recovery algorithms are employed in estimation of
X. However, we note again that the Dantzig selector enables us to highlight the key steps that lead
to the sample complexity of our approach in the next section.

(1 + κ),

4

sup|β |≤2

4 Main results
Overview: Below, we study mΦ , mX , and  that together achieve and balance three objectives:
mX : Sampling centers X are chosen so that the matrix G has rank-k . This is critical in ensuring
that G explores the full k-dimensional subspaces as spanned by AT lest X is rank deﬁcient.
mΦ : Sampling directions Φ (2) are designed to satisfy the RIP for rank-k matrices (cf., Section
3). This property is typically key in proving low-rank recovery guarantees.
: The step-size  in (3) manages the impact of the curvature effects E in the linear system
(5). Unfortunately, this leads to a collateral damage of amplifying the impact of noise if the
queries are corrupted. We provide a remedy below based on sampling the same data points.
Assumptions: We explicitly mention our assumptions here. Without loss of generality, we assume
A = [a1 , . . . , ak ]T is an arbitrary k × d matrix with orthogonal rows so that AAT = Ik , and the
function f is deﬁned over the unit ball, i.e., f : BRd (1) → R.3 For simplicity, we carry out our
analysis by assuming g to be a C 2 function. By our set up, g also lives over a compact set, hence all
(cid:13)(cid:13)Dβ g(cid:13)(cid:13)∞ ≤ C2 ; Dβ g =
its partial derivatives till the order of two are bounded as a result of the Stone-Weierstrass theorem:
∂ |β |
|β | = β1 + · · · + βk ,
1 . . . ∂ yβk
∂ yβ1
k
(cid:90)
for some constant C2 > 0. Finally, the effectiveness of our sampling approach depends on whether
or not the following “Hessian” matrix H f is well-conditioned:
∇f (x)∇f (x)T dµSd−1 (x).
(7)
H f :=
Sd−1
That is, for the singular values of H f , we assume σ1 (H f ) ≥ · · · ≥ σk (H f ) ≥ α > 0 for some α.
This assumption ensures X has full rank-k so that A can be successfully learned.
Restricted singular values of multi-index models: Our ﬁrst main technical contribution provides
a local condition in Proposition 2 that fully characterizes α for multi-index models in (1) below. We
derivatives in an open neighborhood of the origin, Uθ = BRk (θ) for some ﬁxed θ = O (cid:0)d−(s+1) (cid:1),
prove Proposition 2 and the ensuing Proposition 3 in [26].
Proposition 2. Assume that g ∈ C 2 : BRk → R has Lipschitz continuous second order partial
(cid:12)(cid:12)(cid:12)(cid:12) ∂ 2 g
(cid:12)(cid:12)(cid:12)(cid:12)
and for some s > 0:
(y1 ) − ∂ 2 g
∂ yi∂ yj
∂ yi∂ yj
≤ Li,j ∀y1 , y2 ∈ Uθ , y1 (cid:54)= y2 , i, j = 1, . . . , k .
(cid:12)(cid:12)(cid:12)y=0
(cid:107)y1 − y2(cid:107)(cid:96)k
2
(cid:54)= 0 ∀i = 1, . . . , k for Model 1 and
Denote L = max1≤i,j≤k Li,j . Also assume, ∂ 2 g(y)
∂ y2
∀i = 2, . . . , k for Model 2 in (1). Then, we have α = Θ(1/d) as d → ∞.
i
The proof of Proposition 2 also leads to the following proposition for tractability of learning the
general set f (x) = g(Ax) without the particular modular decomposition as in (1):
Proposition 3. With the same Lipschitz continuous second order partial derivative assumption as
in Proposition 2, if ∇2 g(0) is rank-k , then we have α = Θ(1/d) as d → ∞

;

(y2 )

Sampling complexity of active multi-index model learning: The importance of Proposition 2
and Proposition 3 is further made explicit in our second main technical contribution as Theorem 2
below, which characterizes the sample complexity of our low-rank learning recipe in Section 2 for
non-parametric models along with the Dantzig selector algorithm. Its proof can be found in [26].

3Unless further assumptions are made on f or gi ’s, we can only identify the subspace spanned by the rows of
A up to a rotation. Hence, while we discuss approximation results on A, the reader should keep in mind that our
ﬁnal guarantees only apply to the function f and not necessarily for A and g individually. Moreover, if f lives
in some other convex body other than BRd (1), say L∞ -ball, our analysis can be extended in a straightforward
fashion (cf., the concluding discussion in [14]). We also assume that an enlargement of the unit ball BRd (1) on
the domain of f for a sufﬁciently small ¯ > 0 is allowed. This is not a restriction, but is a consequence of our
scheme as we work with directional derivatives of f at points on the unit sphere Sd−1 .

5

√
2 − 1 be
Theorem 2. [Sample complexity of Algorithm 1] Let δ ∈ R+ , ρ (cid:28) 1, and κ <
(cid:18) (1 − ρ)mΦα
(cid:19)1/2
αρ2 log(k/p1 ), mΦ ≥ log(2/p2 ) + 4k(d + mX + 1)u(κ)
ﬁxed constants. Choose mX ≥ 2kC 2
, and
2
q(κ)
(cid:13)(cid:13)(cid:13)L∞
(cid:13)(cid:13)(cid:13)f − (cid:98)f
√
 ≤
δ
our function estimator (cid:98)f in step 4 of Algorithm 1 obeys
. Then, given m = mX (mΦ + 1) samples,
(1 + κ)C0mX
C2k5/2d(δ + 2C2
2k)
≤ δ with probability at least
1 − p1 − p2 .
mX = O (cid:16) k log k
, mΦ = O(k(d + mX )), and  = O(cid:16) α√
(cid:17)
(cid:17)
Theorem 2 characterizes the necessary scaling of the sample complexity for our active learning
scheme in order to obtain uniform approximation guarantees on f with overwhelming probability:
. Note the important role played
α
d
by α in the sample complexity. Finally, we also mention that the sample complexity can be written
differently to trade-off δ among mX , mΦ , and . For instance, we can remove δ dependence in the
sampling bound for : let δ < 1, then we just need to scale mX by δ−2 , and mΦ by δ−4 .
O(cid:16)
(cid:17)
Remark 2. Note that the sample complexity in [14] for learning compressible A is m =
with uniform approximation guarantees on f ∈ C 2 . However, the authors
4−q
2
2−q d
2−q log(k)
k
tures a better dimensional dependence for q ∈ (1, 2) : m = O(cid:0)k3d2 (log(k))2 (cid:1). Of course, we
are able to obtain this result only for a restricted set of radial basis functions. Surprisingly, our
sample complexity for multi-index models (1) not only generalizes this result for general A but fea-
require more computation since we use low-rank recovery as opposed to sparse recovery methods.

Impact of noisy queries: Here, we focus on how α impacts  in particular. Our motivation is to
understand how additive noise in function queries, a realistic assumption in many applications, can
impact our learning scheme, which will form the basis of our third main technical contribution.
under this noise model, (5) changes to y = Φ(X) + ε + z, where z ∈ RmΦ and zi = (cid:80)mX
Let us assume that the evaluation of f at a point x yields: f (x) + Z , where Z ∼ N (0, σ2 ). Thus
zij
N (cid:16)
(cid:17)
 .
j=1
Assuming independent and identically distributed (iid) noise, we have zij ∼ N (0, 2σ2 ), and zi ∼
0, 2mX σ2
. Therefore, the noise variance gets ampliﬁed by a factor of 2mX
2 .
2
In our analysis in Section 3, recall that we require the true matrix X to be feasible. Then, from
(cid:112)2(1 + κ)mX mΦ +
(γ > 2(cid:112)log 12).
Lemma 1.1 in [29] and Proposition 1, it follows that the bound below holds with high probability.
C2 dmX k2
√
(cid:107)Φ∗ (ε + z)(cid:107) ≤ 2γσ
(8)
(1 + κ)1/2 ,
mΦ
2

Unfortunately, we cannot control the upper bound λ on (cid:107)Φ∗ (ε + z)(cid:107) by simply choosing smaller ,
due to the appearance of the (1/) term. Hence, unless σ is O() or less, (e.g., σ reduces with d), we
can only declare that our learning scheme with the matrix Dantzig selector is sensitive to noise unless
(cid:17)
which our scheme can handle. The sample complexity then becomes m = O(cid:16)√
we resample the same data points O(−1 )-times and average. If the noise variance σ2 is constant,
this would keep the impact of noise below a constant times the impact of the curvature errors,
O(cid:16)√
(cid:17)
-times. Unfortunately, we cannot qualitatively improve the O(cid:16)√
(cid:17)
mX (mΦ +
d/α
1), since we choose mX (mΦ + 1) unique points, and then re-query and average the same points
-expansion for
d/α
d/α
noise robustness by simply changing the low-rank recovery algorithm since it depends on the relative
ratio of the curvature errors (cid:107)ε(cid:107)2 to the norm of the noise vector (cid:107)z(cid:107). As Φ satisﬁes the RIP
assumption, we can verify that this relative ratio is approximately preserved in (8) for iid Gaussian
noise.

5 Numerical Experiments

We present simulation results on toy examples to empirically demonstrate the tightness of the sam-
pling bounds. In the sequel, we assume A to be row orthonormal and concern ourselves only with
the recovery of A upto an orthonormal transformation. Therefore, we seek a guaranteed lower

6

(cid:13)(cid:13)(cid:13)A (cid:98)AT (cid:13)(cid:13)(cid:13)F
≥ (kη)1/2 for some 0 < η < 1. Then it is possible to show, along the lines of
bound on
(cid:18) (1 − ρ)mΦα(1 − η)
(cid:19)1/2
the proof for Theorem 2 (see [26]), we would need to pick  as follows:
C2k2d((cid:112)k(1 − η) +
√
 ≤
1
(9)
.
(1 + κ)C0mX
2)
(cid:12)(cid:12)g (β ) (y)(cid:12)(cid:12) = 1. Furthermore we
Logistic function (k = 1) We ﬁrst consider f (x) = g(aT x) where g(y) = (1 + e−y )−1 is the
compute the value of α through the approximation: α = (cid:82) (cid:12)(cid:12)g (cid:48) (aT x)(cid:12)(cid:12)2
logistic function. This case allows us to explicitly calculate all the necessary parameters within
our paper. For instance, we can easily verify that C2 = sup|β |≤2
dµSd−1 ≈ |g (cid:48) (0)|2 = (1/16),
√
which holds for large d. We require |(cid:104)ˆa, a(cid:105)| to be greater then η = 0.99. We ﬁx values of κ <
2−1,
ρ ∈ (0, 1) and  = 10−3 . The value of mX (number of points sampled on Sd−1 ) is ﬁxed at 20 and
we vary d over the range 200–3000. For each value of d, we increase mΦ till |(cid:104)ˆa, a(cid:105)| reaches
the speciﬁed performance criteria of η . We remark that for each value of d and mΦ , we choose 
according to the derived equation (9) for the speciﬁed performance criteria given by η .
Figure 1 depicts the scaling of mΦ with the dimension d. The results are obtained by selecting a
uniformly at random on Sd−1 and averaging the value of |(cid:104)ˆa, a(cid:105)| over 10 independent trials using
the Danzig selector. We observe that for large values of d, the minimum number of directional
derivatives needed to achieve the performance bound on |(cid:104)ˆa, a(cid:105)| scales approximately linearly with
d, with a scaling factor of around 1.45.

Figure 1: Plot of mΦ
d versus d for mX = 20 , with mΦ chosen to be minimum value needed to achieve
|(cid:104)ˆa, a(cid:105)| ≥ 0.99.  is ﬁxed at 10−3 . mΦ scales approximately linearly with d where the constant is 1.45.
i )−1/2 exp (cid:0)−(y + bi )2/(2σ2
i )(cid:1). We ﬁx d = 100,
b) = (cid:80)k
Sum of Gaussian functions (k > 1) We next consider functions of the form f (x) = g(Ax +
(cid:13)(cid:13)(cid:13)A (cid:98)A
(cid:13)(cid:13)(cid:13)2
i x + bi ), where gi (y) = (2πσ2
i=1 gi (aT
 = 10−3 , mX = 100 and vary k from 8 to 32 in steps of 4. For each value of k we are interested
≥ 0.99. In Figure 2(a), we see that
in the minimum value of mΦ needed to achieve 1
k
F
mΦ scales approximately linearly with the number of Gaussian atoms k . The results are averaged
over 10 trials. In each trial, we select the rows of A over the left Haar measure on Sd−1 , and the
parameter b uniformly at random on Sk−1 scaled by a factor 0.2. Furthermore we generate the
standard deviations of the individual Gaussian functions uniformly over the range [0.1, 0.5].
Impact of Noise (k > 1) We now consider quadratic forms, i.e. f (x) = g(Ax) = (cid:107)Ax − b(cid:107)2
with the point queries corrupted with Gaussian noise. Here, we take α to be 1/d. We ﬁx k = 5,
mX = 30,  = 10−1 and vary d from 30 to 120 in steps of 15. For each d we perturb the point queries
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)A (cid:98)A
with Gaussian noise of standard deviation: 0.01/d3/2 . This is the same as repeatedly sampling each
random location approximately d3/2 times followed by averaging. We then compute the minimum
≥ 0.99. We average the results over 10 trials, and in
value of mΦ needed to achieve 1
k
F
each trial, we select the rows of A over the left Haar measure on Sd−1 . The parameter b is chosen
uniformly at random on Sk−1 . In Figure 2(b), we see that mΦ scales approximately linearly with d,
which follows our sample complexity bound for mΦ in Theorem 2.

7

(a) k > 1 (Gaussian)

(b) k > 1 (quadratic) with noise

Figure 2: The empirical performance of our oracle-based low-rank learning scheme (circles) agrees
well with the theoretical scaling (dashed). Section 5 has further details.

6 Conclusions

In this work, we consider the problem of learning non-parametric low-dimensional functions
f (x) = g(Ax), which can also have a modular decomposition as in (1), for arbitrary A ∈ Rk×d
where rank(A) = k . The main contributions of the work are three-fold. By introducing a new anal-
ysis tool based on Lipschitz property on the second order derivatives, we provide the ﬁrst rigorous
characterization of the dimension dependence of the k-restricted singular value of the “Hessian” ma-
trix H f for general multi-index models. We establish the ﬁrst sample complexity bound for learning
non-parametric multi-index models with low-rank recovery algorithms and also analyze the impact
of additive noise to the sample complexity of the scheme. Lastly, we provide empirical evidence
on toy examples to show the tightness of the sampling bounds. Finally, while our active learning
scheme ensures the tractability of learning non-parametric multi-index models, it does not establish
a lowerbound on the sample complexity, which is left for future work.

7 Acknowledgments

This work was supported in part by the European Commission under Grant MIRG-268398, ERC
Future Proof, SNF 200021-132548, ARO MURI W911NF0910383, and DARPA KeCoM program
#11-DARPA-1055. VC also would like to acknowledge Rice University for his Faculty Fellowship.
The authors thank Jan Vybiral for useful discussions and Anastasios Kyrillidis for his help with the
low-rank matrix recovery simulations.

References
[1] P. B ¨uhlmann and S. Van De Geer. Statistics for High-Dimensional Data: Methods, Theory and
Applications. Springer-Verlag New York Inc, 2011.
[2] L. Carin, R.G. Baraniuk, V. Cevher, D. Dunson, M.I. Jordan, G. Sapiro, and M.B. Wakin.
Learning low-dimensional signal models. Signal Processing Magazine, IEEE, 28(2):39–51,
2011.
[3] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimen-
sion reduction. The Annals of Statistics, 29(6):1537–1566, 2001.
[4] K.C. Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical
Association, pages 316–327, 1991.
[5] P. Hall and K.C. Li. On almost linearity of low dimensional projections from high dimensional
data. The Annals of Statistics, pages 867–889, 1993.
[6] Y. Xia, H. Tong, WK Li, and L.X. Zhu. An adaptive estimation of dimension reduction space.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):363–410,
2002.
[7] Y. Xia. A multiple-index model and dimension reduction. Journal of the American Statistical
Association, 103(484):1631–1640, 2008.

8

[8] Y. Lin and H.H. Zhang. Component selection and smoothing in multivariate nonparametric
regression. The Annals of Statistics, 34(5):2272–2297, 2006.
[9] L. Meier, S. Van De Geer, and P. B ¨uhlmann. High-dimensional additive modeling. The Annals
of Statistics, 37(6B):3779–3821, 2009.
[10] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models
over kernel classes via convex programming. Technical Report, UC Berkeley, Department of
Statistics, August 2010.
[11] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 71(5):1009–1030, 2009.
[12] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics,
38(6):3660–3695, 2010.
[13] A. Cohen, I. Daubechies, R. A. DeVore, G. Kerkyacharian, and D. Picard. Capturing ridge
functions in high dimensions from point queries. Constr. Approx., pages 1–19, 2011.
[14] M. Fornasier, K. Schnass, and J. Vyb´ıral. Learning functions of few arbitrary linear parameters
in high dimensions. Preprint, 2010.
[15] H. Tyagi and V. Cevher. Learning ridge functions with randomized sampling in high dimen-
sions. In ICASSP, 2011.
[16] J.F. Traub, G.W Wasilkowski, and H. Wozniakowski. Information-based complexity. Aca-
demic Press, New York, 1988.
[17] R. DeVore and G.G. Lorentz. Constructive approximation. vol. 303, Grundlehren, Springer
Verlag, N.Y., 1993.
[18] E.Novak and H.Woniakowski. Approximation of inﬁnitely differentiable multivariate functions
is intractable. J. Complex., 25:398–404, August 2009.
[19] W. Hardle. Applied nonparametric regression, volume 26. Cambridge Univ Press, 1990.
[20] J.H. Friedman and W. Stuetzel. Projection pursuit regression. J. Amer. Statist. Assoc., 76:817–
823, 1981.
[21] D.L. Donoho and I.M. Johnstone. Projection based regression and a duality with kernel meth-
ods. Ann. Statist., 17:58–106, 1989.
[22] P.J. Huber. Projection pursuit. Ann. Statist., 13:435–475, 1985.
[23] A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica,
8:143–195, 1999.
[24] E.J Cand `es. Harmonic analysis of neural networks. Appl. Comput. Harmon. Anal., 6(2):197–
218, 1999.
[25] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Information-theoretic regret bounds for
gaussian process optimization in the bandit setting. To appear in the IEEE Trans. on Informa-
tion Theory, 2012.
[26] Hemant Tyagi and Volkan Cevher. Learning non-parametric basis independent models from
point queries via low-rank methods. Technical Report, Infoscience EPFL, 2012.
[27] E.J. Cand `es and B. Recht. Exact matrix completion via convex optimization. Foundations of
Computational Mathematics, 9(6):717–772, 2009.
[28] E.J. Cand `es and T. Tao. The power of convex relaxation: near-optimal matrix completion.
IEEE Trans. Inf. Theor., 56:2053–2080, May 2010.
[29] E.J. Cand `es and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal
number of random measurements. CoRR, abs/1001.0339, 2010.
[30] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM REVIEW, 52:471–501, 2010.
[31] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.
The Annals of Statistics, 28(5):1302–1338.

9


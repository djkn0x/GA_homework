S TAN FO RD UN I V E R S I T Y, CS 2 2 9 - M ACH IN E L E A RN ING

Don’t Get Kicked - Machine Learning Predictions for Car Buying

Albert Ho, Robert Romano, Xin Alice Wu

December 14, 2012

1 Introduction

When you go to an auto dealership with the in-
tent to buy a used car, you want a good selection to
choose from and you want to be able to trust the
condition of the car that you buy. Auto dealerships
purchase many of their used cars through auto
auctions with the same goals that you have: they
want to buy as many cars as they can in the best
condition possible. The problem that these dealer-
ships often face is the risk of buying used cars that
have serious issues, preventing them from being
sold to customers. These bad purchases are called
"kicks", and they can be hard to spot for a variety
of reasons. Many kicked cars are purchased due
to tampered odometers or mechanical issues that
could not be predicted ahead of time. For these
reasons, car dealerships can beneﬁt greatly from
the predictive powers of machine learning. If there
is a way to determine if a car would be kicked a pri-
ori, car dealerships can not only save themselves
money, but also provide their customers with the
best inventory selection possible.

The following paper is split up into 5 main
sections describing our approach to solve this
problem:
Initial Data Preprocessing, Early Algo-
rithm Selection, Data Normalization and Balanc-
ing, Performance Evaluation, and Boosting. First
we identiﬁed the key characteristics of our data and
formed strategies for preprocessing. Next, we ran
several simple machine learning algorithms. This

led us to update our data processing strategy and
determine a better way to evaluate and compare
different learning algorithms. Finally, we imple-
mented boosting and tailored our ﬁnal algorithm
selection based on initial successes.

2 Initial Data Preprocessing
We obtained our data set from the Kaggle.com
challenge "Don’t Get Kicked" hosted by Carvana.
The data set contained 32 unique features with
73,041 samples along with a labeling of 0 for good
car purchases and 1 for "kicks". Some key fea-
tures included odometer readings, selling prices,
vehicle age, and vehicle model. One thing that
we immediately noticed was that good cars were
heavily overrepresented in the data set, represent-
ing 87.7% of samples. The consequences of this
became more apparent once we began comparing
machine learning algorithms across different met-
rics.

2.1 Word Bins

Our ﬁrst major challenge was the preprocess-
ing of data. For data such as the name of the ve-
hicle’s model, manufacturer, and color, we had to
assign unique identiﬁers to speciﬁc strings in the
feature space. This was straightforward for a fea-
ture like transmission since we could assign 0 for
Auto and 1 for Manual. The process became more
involved with multivariate features such as the car
submodel. We decided that even though there were

1

many different submodels, categorizing them with
unique identiﬁers rather than grouping them was
the more conservative option.

2.2 Missing Features

Some of the samples had missing features. We
had the option of throwing out the sample com-
pletely, but we believed that it would be a waste.
We decided to implement the following rules: if the
feature was represented with a continuous value,
we would replace the missing value with the av-
erage of the feature over the other samples and if
the feature was represented with a discrete value,
we would create a new value speciﬁcally to identify
missing data.

2.3 Data Visualization

Before running any algorithms, we visualized
the data with plots to gain some intuition about the
features. The training data was separated into good
and bad datasets and compared, looking for trends.
Histograms were plotted over each feature with the
frequency normalized so that good and bad cars
were equally represented. This allowed compar-
ison of the relative frequency over a feature. An
example is Figure 1a, showing that bad cars were
generally older. To get an idea of how discriminat-
ing a feature was, the ratio of the relative frequency
of bad to good was plotted. Figure 1b shows that
Current Auction Average Price was a strong feature,
however this needed to be taken with a grain of
salt because the areas where the features were most
discriminating were generally in small tail regions
that applied to a very small subset of cars.

3 Early Algorithm Selection
With the data parsed and some initial in-
sights to guide us, we applied some basic machine
learning algorithms that would identify where we
needed improvement and what strategy would be
most effective. At this point, we chose generaliza-
tion error as a metric to evaluate our algorithms’
performances.

3.1 Support Vector Machine

First, we tested our data with an SVM. We used
libLINEAR v. 1.92 and the method of cross valida-
tion by training on 70% of our data set and testing

(a) Ratio of Scaled VehicleAge

(b) Ratio of CurrAuctnAvgPrice

Figure 1: Histogram plots depicting ratio of scaled
vehicle age and current auction average
price

on the remaining 30%. Initial runs yielded about
12% generalization error, which on ﬁrst glance was
very good.

3.2 Logistic Regression

Since the feature we were trying to predict
was binary, we decided to try a logistic regression
model as a ﬁrst pass. Logistic regression via New-
ton’s method was implemented in MATLAB with
the same method of cross validation as that in SVM.
We found that the algorithm converged after 7 it-
erations, yielding a generalization error of about
12%.

3.3 Observations

Using generalization error as a metric, both lo-
gistic regression and SVM seemed to have yielded
promising results. Upon further investigation,
however, these runs would nearly always predict
the null hypothesis, i.e. a good car prediction for
every testing sample. This was where we started to
question the use of generalization error as a per-
formance metric in favor of performance metrics
that took into account false positives and false neg-

2

atives. We also conducted a literature review in
hopes of ﬁnding alternative algorithms more suit-
able for skewed data sets.

4 Data Normalization and
Balancing

4.1 Feature Normalization

After evaluating the performance of our early
attempts, we made several changes to the data pre-
processing procedure in hopes of achieving better
results. Through our literature search, we found
that data normalization increases the performance
of many classiﬁcation algorithms [1] As a result, we
normalized our numeric features over the range 0
to 1.

4.2 Data Balancing

In addition to data normalization, we also dis-
covered that "up-sampling" the data from the mi-
nority class is an effective way of solving the class
imbalance problem.
([2], [3], [4]). To do this
we again split our data in a 70/30 cross-validation
scheme. From the data split intended for training,
we created a balanced training data set by over-
sampling the bad cars. Both balanced and unbal-
anced data sets were used for the algorithms we
tested from this point forward to observe the ef-
fects of artiﬁcial data balancing.

5 Performance Evaluation
As mentioned earlier, we found that using gen-
eralization error alone as a performance metric
was misleading due to the bias of our data towards
good cars. A prediction of all good cars, for exam-
ple, would yield 12.3% accuracy. In the context of
our problem, it is more relevant to evaluate an al-
gorithm’s performance based on precision and re-
call

p r e c i s i on =
r e c a l l =

T P
T P + F P
T P
T P + F N
rather than predictive accuracy, since the number
of false positive (FP) and false negatives (FN) pre-
dicted by an algorithm is more directly related to
proﬁt and opportunity cost, which is ultimately

(1)

what car dealers care about. In general, you want
a balance between precision and recall, so we used
AUC and F1, which are derived from FP and FN, to
ﬁnd that balance.
Through our literature search, we found that
when studying problems with imbalanced data,
using the classiﬁers produced by standard machine
learning algorithms without adjusting the output
threshold may cause poor performance [3]. In this
respect, AUC is a good metric since it takes into
account sensitivity (recall) and speciﬁcity over the
entire range of possible output threshold values.
AUC is a good indicator of one classiﬁer’s ability
for correct prediction over another. In addition, we
also used the F1 score as a performance metric to
account for the inverse relationship between pre-
cision and recall [5]. We deﬁne F1 as the harmonic
mean between precision and recall:
F 1 = 2 ∗ p r e c i s i on ∗ r e c a l l
p r e c i s i on + r e c a l l
If precision and recall has been traded off, the F1
score will not change. That way we can identify a
superior algorithm as one that increases both pre-
cision and recall.

(2)

6 Boosting
After applying data normalization and balanc-
ing, we returned to our initial approaches using
SVM and logistic regression. We found that by
using these algorithms with normalized and bal-
anced data sets, we were able to achieve better AUC
and F1 scores, and therefore better results than be-
fore. We also tried tuning the C parameter in li-
bLINEAR to little effect. From our own research
and discussion with the TAs, we found that boost-
ing might be a promising approach for our learning
problem. The idea behind boosting is to combine
many weak learners into a strong learner ([6], [7]).
To implement boosting, along with a slew of other
learning algorithms, we used Weka (Waikato Envi-
ronment for Knowledge Analysis) v. 3.7.7.
Weka made it easy to try many different learn-
ing algorithms quickly. Due to the nature of our
data, we were very interested in comparing the per-
formance of traditional classiﬁcation algorithms
with meta-classiﬁers such as boosting and ensem-
ble learning. However, Weka is also very mem-

3

ory intensive. The program could not run logis-
tic regression without crashing even with 5.0GB of
memory allocated. As a result, logistic regression
was still implemented in MATLAB, while all others
were implemented in Weka.

7 Results

We used Weka to implement several meta-
classiﬁers, speciﬁcally AdaBoostM1, RealAdaBoost,
LogitBoost, and ensemble selection. The weak
classiﬁers we used were decision stump, decision
table, REPTree, J48, and naive bayes. Decision
stump is a one level decision tree. Decision table
is a simple majority classiﬁer. REPTree is a fast de-
cision tree learner, based on information gain and
pruning using reduced-error pruning with backﬁt-
ting.
J48 is an implementation of the C4.5 deci-
sion tree, which is based on maximizing informa-
tion gain.
AdaBoostM1 is a general nominal classiﬁer
boosting algorithm. Using decision stump as its
classiﬁer,
it performed reasonably well with an
AUC of 0.724. We tried using more sophisticated
classiﬁers such as J48, random forest, and REP-
Tree, however they all performed worse. RealAd-
aBoost is an implementation of AdaBoost that is
optimized for binary classiﬁcation. Using decision
stump as its classiﬁer, it performed well with an
AUC of 0.744. Similarly, other more sophisticated
classiﬁers did worse, perhaps due to overﬁtting.
LogitBoost using decision stump performed better
than AdaBoostM1, with an AUC of 0.746. Logit-
Boost using decision table performed slightly bet-
ter, with an AUC of .758. Because of this we decided
to stick with logitBoost as our boosting algorithm of
choice.
Ensemble selection can use any combination
of weak classiﬁers to make a strong classiﬁer, so it is
very ﬂexible. One implementation is to additively
build a strong classiﬁer by selecting the strongest
weak classiﬁer, and then one by one adding the
next strongest weak classiﬁer. We chose to use AUC
as the metric for evaluating classiﬁer strength. Be-
cause ensemble selection uses a greedy optimiza-
tion algorithm, it is prone to overﬁtting. To over-
come this, strategies such as model bagging, re-
placement, and sort initialization were used. Ten

model bags were used as well as sort initializa-
tion. The ensemble selection algorithm with most
promise was one that incorporated many different
classiﬁers, including naive bayes, J48, and REPTree
classiﬁers. This resulted in an AUC of .752 along
with an F1 of .279, just shy of LogitBoost.
It was found that contrary to literature, bal-
ancing the data did not generally improve clas-
siﬁer performance.
In fact, classiﬁers generally
performed worse when trained on the balanced
data set. While balancing the data yielded reduced
number of false negatives, it also dramatically in-
creased the number of false positives.

8 Discussion

We found through our investigation that Log-
itBoost was the best at predicting whether or not a
car would be a kick. It produced a prediction with
the highest AUC value of 0.758 and an F1 of 0.368.
The F1 value was not as high as we would have
liked, but depending on the relationship between
Gross_Proﬁt and Loss in the Total_Proﬁt equation,
F1 may not even be a great metric to maximize the
parameter of interest.
T o t a l _P r o f i t = T N ∗ G r o s s _P r o f i t + F N ∗ Lo s s
O p p o r t un i t y _C o s t = F P ∗ G r o s s _P r o f i t
(3)

Total_Proﬁt represents the proﬁt that a car
dealership will make if they follow the predictions
of an algorithm. All cars that are classiﬁed as
good and are actually good will make the dealer-
ship some Gross_Proﬁt per car. At the same time,
all cars that are classiﬁed as good, but are actually
not will cause the dealership to incur some Loss.
The Opportunity_Cost represents the Gross_Proﬁt
lost from any car classiﬁed as bad that actually was
not. What these formulas boil down to is a trade-
off between false negatives, false positives, and true
negatives through Gross_Proﬁt and Loss. If Loss is
higher for the end user, they would tailor the algo-
rithm to produce less FN, while if Gross_Proﬁt is
higher, they would want less FP.
Of all the procedures and algorithms we used,
the most useful were data normalization, boosting,
and using AUC and F1 as performance metrics.

4

Table 1: Algorithm comparison: a. Decision Stump, b. Decision Stump 100 Iterations, c. Decision Table,
d. J48 Decision Tree, e. Maximize for ROC, f. assortment

9 Future Work

There are several strategies we would pur-
sue in order to further improve prediction perfor-
mance. One would be to evaluate our algorithms
on a separated data set created by the removal of
overlapping data via PCA [8]. Literature suggested
that if a data set is overlapped, one could run algo-
rithms on the portion of the data that is not over-
lapping to get better results. The reason we did not
pursue this in the beginning is that doing so would
create a high variance classﬁer may overﬁt the data.
Another strategy that we did not get working would
be to use RUSBoost, which has been shown to im-
prove performance on imbalanced datasets, such
as our own [9]. Finally, we would want to use lib-
SVM with a nonlinear kernel such as Gaussian to
compare with our other algorithms. Due to com-
putational performance limitations, we were un-
able to implement this method.

10 Acknowledgements

We would like to thank Professor Andrew Ng
and the TAs (especially Andrew Maas, Sonal Gupta,
and Chris Lengerich) for all their help on this
project along with Kaggle and CARVANA for pro-
viding data.

References

[1] Graf, A., Borer, S. (2001). Normalization in support vector
machines. Pattern Recognition, 277-282.

[2] Menardi G, Torelli N. (2010) Training and assessing classif-
cation rules with unbalanced data. Working Paper Series

[3] Provost, F. (2000) Learning with Imbalanced Data Sets
101. Invited paper for the AAAI’2000 Workshop on Imbal-
anced Data Sets.

[4] Japkowicz, N. (2000). The Class Imbalance Problem:
Signifcance and Strategies. In Proceedings of the 2000
International Conference on Artifcial Intelligence (IC-
AI’2000): Special Track on Inductive Learning Las Vegas,
Nevada.

[5] Forman, G., Scholz. M. (2009.) Apples-to-Apples in Cross-
Validation Studies: Pitfalls in Classifer Performance Mea-
surement. ACM SIGKDD Explorations, 12(1), 49 ˝U57.

[6] Hastie,
T.
(2003).
Boosting.
from
Stanford
University
Web
http://www.stanford.edu/ hastie/TALKS/boost.pdf

Retrieved
Site:

[7] Friedman, J., Hastie, T., Tibshirani, R. (2000). Additive lo-
gistic regression: a statistical view of boosting (With dis-
cussion and a rejoinder by the authors). The annals of
statistics, 28(2), 337-407.

[8] Das, B., Krishnan, N. C., Cook, D. J. (2012) Handling Im-
balanced and Overlapping Classes in Smart Environments
Prompting Dataset.

[9] Seiffert, C., Khoshgoftaar, T. M., Van Hulse, J., Napoli-
tano, A. (2008, December).RUSBoost: Improving classi-
ﬁcation performance when training data is skewed. In
Pattern Recognition, 2008. ICPR 2008. 19th International
Conference on (pp. 1-4). IEEE.

5


Nonparametric Reduced Rank Regression

Rina Foygel† ,∗ , Michael Horrell† , Mathias Drton† , ‡ , John Lafferty†
∗Department of Statistics
‡ Department of Statistics
† Department of Statistics
Stanford University
University of Chicago
University of Washington

Abstract

We propose an approach to multivariate nonparametric regression that generalizes
reduced rank regression for linear models. An additive model is estimated for each
dimension of a q -dimensional response, with a shared p-dimensional predictor
variable. To control the complexity of the model, we employ a functional form of
the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low
rank. Backﬁ
tting algorithms are derived and justiﬁ ed using a nonparametric form
of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived
that exhibit the scaling behavior of the procedure in the high dimensional setting.
The methods are illustrated on gene expression data.
In the multivariate regression problem the objective is to estimate the conditional mean E(Y ∣ X ) =
1
Introduction
m(X ) = (m1 (X ), . . . , mq (X ))⊺ where Y is a q -dimensional response vector and X is a p-
literature. We are given a sample of n iid pairs {(Xi , Yi )} from the joint distribution of X and Y .
Under a linear model, the mean is estimated as m(X ) = BX where B ∈ R
q×p is a q × p matrix
dimensional covariate vector. This is also referred to as multi-task learning in the machine learning
In reduced rank regression the matrix B is estimated under a rank constraint r = rank(B ) ≤ C , so
of regression coefﬁ cients. When the dimensions p and q are large relative to the sample size n, the
coefﬁ cients of B cannot be reliably estimated, without further assumptions.
q or R
p . Intuitively, this implies
that the rows or columns of B lie in an r-dimensional subspace of R
that the model is based on a smaller number of features than the ambient dimensionality p would
suggest, or that the tasks representing the components Y k of the response are closely related. In low
dimensions, the constrained rank model can be computed as an orthogonal projection of the least
The nuclear norm ∥B ∥∗ , also known as the trace or Ky-Fan norm, is the sum of the singular vectors
squares solution; but in high dimensions this is not well deﬁ ned.
Recent research has studied the use of the nuclear norm as a convex surrogate for the rank constraint.
of B . A rank constraint can be thought of as imposing sparsity, but in an unknown basis; the nuclear
norm plays the role of the (cid:1)1 norm in sparse estimation. Its use for low rank estimation problems
was proposed by Fazel in [2]. More recently, nuclear norm regularization in multivariate linear
regression has been studied by Yuan et al. [10], and by Neghaban and Wainwright [4], who analyzed
on additive models, so that the regression function m(X ) = (m1 (X ), . . . , mq (X ))⊺ has each com-
the scaling properties of the procedure in high dimensions.
ponent mk (X ) = ∑p
j (Xj ) equal to a sum of p functions, one for each covariate. The objective
In this paper we study nonparametric parallels of reduced rank linear models. We focus our attention
j (Xj )].
is then to estimate the q × p matrix of functions M (X ) = [mk
j=1 mk
penalty ∥B ∥∗ in the linear model. Because we must estimate a matrix of functions, the analogue of
The ﬁ rst problem we address, in Section 2, is to determine a replacement for the regularization
the nuclear norm is not immediately apparent. We propose two related regularization penalties for

1

nonparametric low rank regression, and show how they specialize to the linear case. We then study,
in Section 4, the (inﬁ nite dimensional) subdifferential of these penalties. In the population setting,
this leads to stationary conditions for the minimizer of the regularized mean squared error. This
subdifferential calculus then justiﬁ es penalized backﬁ
tting algorithms for carrying out the optimiza-
tion for a ﬁ nite sample. Constrained rank additive models (CRAM) for multivariate regression are
analogous to sparse additive models (S PAM) for the case where the response is 1-dimensional [6]
(studied also in the reproducing kernel Hilbert space setting by [5]), but with the goal of recovering
tting algorithms we derive in
a low-rank matrix rather than an entry-wise sparse vector. The backﬁ
Section 5 are analogous to the iterative smoothing and soft thresholding backﬁ
tting algorithms for
S PAM proposed in [6]. A uniform bound on the excess risk of the estimator relative to an oracle
is given Section 6. This shows the statistical scaling behavior of the methods for prediction. The
analysis requires a concentration result for nonparametric covariance matrices in the spectral norm.
Experiments with gene data are given in Section 7, which are used to illustrate different facets of the
proposed nonparametric reduced rank regression techniques.

2 Nonparametric Nuclear Norm Penalization
that f 1 (x), . . . , f q (x) are q smooth one-dimensional functions with a common domain. What
We begin by presenting the penalty that we will use to induce nonparametric regression esti-
mates to be low rank. To motivate our choice of penalty and provide some intuition, suppose
of points in the common domain of the functions. We require that the n × q matrix of function values
F(x1∶n ) = [f k (xi )] is low rank. This matrix is of rank at most r < q for every set {xi } of arbitrary
does it mean for this collection of functions to be low rank? Let x1 , x2 , . . . , xn be a collection
size n if and only if the functions {f k } are r-linearly independent—each function can be written as
(q > 1 and p = 1), we have a random sample X1 , . . . , Xn . Consider the n × q sample matrix
a linear combination of r of the other functions.
M = [mk (Xi )] associated with a vector M = (m1 , . . . , mq ) of q smooth (regression) functions,
In the multivariate regression setting, but still assuming the domain is one-dimensional for simplicity
and suppose that n > q . We would like for this to be a low rank matrix. This suggests the penalty
√
∥M∥∗ = ∑q
s=1 σs (M) = ∑q
λs (M⊺M), where {λs (A)} denotes the eigenvalues of a symmetric
matrix A and {σs (B )} denotes the singular values of a matrix B . Now, assuming the columns of M
M as the sample covariance ̂Σ(M )
are centered, and E[mk (X )] = 0 for each k , we recognize 1
s=1
of the population covariance Σ(M ) ∶= Cov(M (X )) = [E(mk (X )ml (X ))]. This motivates the
⊺
M
n
∥Σ(M )1/2 ∥∗ = ∥ Cov(M (X ))1/2 ∥∗
following sample and population penalties, where A1/2 denotes the matrix square root:
∥̂Σ(M )1/2 ∥∗ = 1√
∥M∥∗ .
population penalty:
(2.1)
With Y denoting the n × q matrix of response values for the sample (Xi , Yi ), this leads to the fol-
sample penalty:
(2.2)
n
E∥Y − M (X )∥2
2 + λ∥Σ(M )1/2 ∥∗
lowing population and empirical regularized risk functionals for low rank nonparametric regression:
1
F + λ√
∥Y − M∥2
∥M∥∗ .
population penalized risk:
2
1
We recall that if A ⪰ 0 has spectral decomposition A = U DU ⊺ then A1/2 = U D1/2U ⊺ .
empirical penalized risk:
2n
n
3 Constrained Rank Additive Models (CRAM)
covariate. We consider the family of additive models, with regression functions of the form m(X ) =
We now consider the case where X is p-dimensional. Throughout the paper we use superscripts to
(m1 (X ), . . . , mq (X ))⊺ = ∑p
j=1 Mj (Xj ), where each term Mj (Xj ) = (m1
j (Xj ), . . . , m
j (Xj ))⊺ is
denote indices of the q -dimensional response, and subscripts to denote indices of the p-dimensional
q
a q -vector of functions evaluated at Xj .

(2.3)

(2.4)

2

(3.5)

vector (m1
j (Xj ), . . . , m
j (Xj )) to be low rank, for each j . Assume that the functions mk
j (Xj )
In this setting we propose two different penalties. The ﬁ rst penalty, intuitively, encourages the
Σj = Σ(Mj ) = Cov(Mj (Xj )) denote the covariance matrix of the j -th component functions, with
q
sample version ̂Σj . The population and sample versions of the ﬁ rst penalty are then given by
all have mean zero; this is required for identiﬁ ability in the additive model. As a shorthand, let
∥Σ1/2
+ ⋯ + ∥Σ1/2
+ ∥Σ1/2
p ∥
2 ∥
1 ∥
+ ∥̂Σ1/2
+ ⋯ + ∥̂Σ1/2
∥̂Σ1/2
p ∥
2 ∥
1 ∥
= 1√
∥Mj ∥∗ .
p∑
(3.1)
∗
∗
∗
p )⊺
The second penalty, intuitively, encourages the set of q vector-valued functions (mk
(3.2)
∗
∗
∗
n
j=1
2 , . . . , mk
1 , mk
∥(Σ1/2
p )∥
1 ⋯Σ1/2
to be low rank. This penalty is given by
∥(̂Σ1/2
1 ⋯̂Σ1/2
p )∥
= 1√
∥M1∶p ∥∗
(3.3)
∗
1 ⋯M
where, for convenience of notation, M1∶p = (M
p )⊺
is an np × q matrix. The corresponding
(3.4)
∗
n
⊺
⊺
j ∥
∥Σ1/2
E∥Y − p∑
Mj (X )∥2
+ λ
p∑
population and empirical risk functionals, for the ﬁ rst penalty, are then
1
+ λ√
∥Y − p∑
∥Mj ∥∗
Mj ∥2
p∑
2
∗
2
j=1
j=1
1
(3.6)
2n
n
F
j ) = 1. In the linear case we have Mj (Xj ) =
Now suppose that each Xj is normalized so that E(X 2
j=1
j=1
q . Let B = (B1⋯Bp ) ∈ R
Xj Bj where Bj ∈ R
and similarly for the second penalty.
j=1 ∥Σ1/2
j ∥∗ = ∑p
j=1 ∥Bj ∥2 for the ﬁ rst penalty, and ∥Σ1/2
1 ⋯Σ1/2
p ∥∗ = ∥B ∥∗
the penalties reduce to ∑p
q×p . Some straightforward calculation shows that
the group lasso [11]. The second penalty reduces to the nuclear norm regularization ∥B ∥∗ used for
for the second. Thus, in the linear case the ﬁ rst penalty is encouraging B to be column-wise sparse,
so that many of the Bj s are zero, meaning that Xj doesn ’t appear in the ﬁ
t. This is a version of
high-dimensional reduced-rank regression.
4 Subdifferentials for Functional Matrix Norms
tials of the penalties. We are interested in (q × p)-dimensional matrices of functions F = [f k
j ]. For
A key to deriving algorithms for functional low-rank regression is computation of the subdifferen-
each column index j and row index k , f k
j is a function of a random variable Xj , and we will take
expectations with respect to Xj implicitly. We write Fj to mean the j th column of F , which is a
j Gj ) = tr (E(F G
⊺ )) ,
⟪F , G⟫ ∶= p∑
E(f k
j ) = p∑
E(F
q∑
q -vector of functions of Xj . We deﬁ ne the inner product between two matrices of functions as
and write ∥F ∥2 = √⟪F , F ⟫. Note that ∥F ∥2 = ∥√
where E(F F ⊺ ) = ∑j E(Fj F ⊺
E(F F ⊺ )∥
j ) ⪰ 0
⊺
j gk
(4.1)
is a positive semideﬁ nite q × q matrix.
j=1
j=1
k=1
F
∣∣∣F ∣∣∣sp ∶= √∥E(F F ⊺ )∥sp = ∥√
∣∣∣F ∣∣∣∗ ∶= ∥√
E(F F ⊺ )∥∗ ,
E(F F ⊺ )∥
We deﬁ ne two further norms on a matrix of functions F , namely,
where ∥A∥sp is the spectral norm (operator norm), the largest singular value of A, and it is convenient
√
and
A = A1/2 . Each of the norms depends on F only through
sp
E(F F ⊺ ). In fact, these two norms are dual —for any F ,
∣∣∣F ∣∣∣∗ = sup
⟪G, F ⟫ ,
to write the matrix square root as
(4.2)
∣∣∣G∣∣∣sp ≤1

3

where the supremum is attained by setting G = (√
E(F F ⊺ ))−1
Proposition 4.1. The subdifferential of ∣∣∣F ∣∣∣∗ is the set
F , with A−1 denoting the matrix
S (F ) ∶= {(√
pseudo-inverse.
∣∣∣H ∣∣∣sp ≤ 1, E(F H ⊺ ) = 0q×q , E(F F ⊺ )H = 0q×p a.e.} .
F + H ∶
E(F F ⊺ ))−1
Proof. The fact that S (F ) contains the subdifferential ∂ ∣∣∣F ∣∣∣∗ can be proved by comparing our
(4.3)
inclusion, S (F ) ⊆ ∂ ∣∣∣F ∣∣∣∗ . Let D ∈ S (F ) and let G be any element of the function space. We need
setting (matrices of functions) to the ordinary matrix case; see [9, 7]. Here, we show the reverse
∣∣∣F + G∣∣∣∗ ≥ ∣∣∣F ∣∣∣∗ + ⟪G, D⟫ ,
where D = (√
F + H =∶ ̃F + H for some H satisfying the conditions in (4.3) above.
to show
E(F F ⊺ ))−1
(4.4)
∣∣∣F ∣∣∣∗ + ⟪G, D⟫ = ∣∣∣F ∣∣∣∗ + ⟪G, ̃F + H ⟫ = ⟪F + G, ̃F + H ⟫ ≤ ∣∣∣F + G∣∣∣∗ ∣∣∣D ∣∣∣sp ,
Expanding the right-hand side of (4.4), we have
where the second equality follows from ∣∣∣F ∣∣∣∗ = ⟪F , ̃F ⟫, and the fact that ⟪F , H ⟫ = tr(E(F H ⊺ )) =
Finally, we show that ∣∣∣D ∣∣∣sp ≤ 1. We have
0. The inequality follows from the duality of the norms.
⊺ ) = E( ̃F ̃F ⊺ ) + E( ̃F H ⊺ ) + E(H ̃F ⊺ ) + E(HH ⊺ ) = E( ̃F ̃F ⊺ ) + E(HH ⊺ ) ,
E(DD
where we use the fact that E(F H ⊺ ) = 0q×q , implying E( ̃F H ⊺ ) = 0q×q . Next, let E(F F ⊺ ) = V DV ⊺
be a reduced singular value decomposition, where D is a positive diagonal matrix of size q ′ ≤ q .
Then E( ̃F ̃F ⊺ ) = V V ⊺ , and we have
E(F F ⊺ ) ⋅ H = 0q×p a.e. ⇔ V ⊺H = 0q ′ ×p a.e. ⇔ E( ̃F ̃F ⊺ )H = 0q×p a.e. .
This implies that E( ̃F ̃F ⊺ ) ⋅ E(HH ⊺ ) = 0q×q and so these two symmetric matrices have orthogonal
= max {∥E( ̃F ̃F
= ∥E( ̃F ̃F
∥E(DD
⊺ )∥
⊺ )∥
⊺ )∥
⊺ )∥
sp , ∥E(HH
} ≤ 1 ,
⊺ ) + E(HH
row spans and orthogonal column spans. Therefore,
where the last bound comes from the fact that ∣∣∣ ̃F ∣∣∣sp , ∣∣∣H ∣∣∣sp ≤ 1. Therefore ∣∣∣D ∣∣∣sp ≤ 1.
sp
sp
sp
This gives the subdifferential of penalty 2, deﬁ ned in (3.3). We can view the ﬁ rst penalty update as
∣∣∣Fj ∣∣∣∗ = ∥√
just a special case of the second penalty update. For penalty 1 in (3.1), if we are updating Fj and ﬁ x
j )∥
E(Fj F ⊺
all the other functions, we are now penalizing the norm
(4.5)
,
∗
∂ ∣∣∣Fj ∣∣∣∗ = {(√
which is clearly just a special case of penalty 2 with a single q -vector of functions instead of p
j )Hj = 0 a.e.} . (4.6)
j ) = 0, E(Fj F
∣∣∣Hj ∣∣∣sp ≤ 1, E(Fj H
Fj + Hj ∶
j ))−1
E(Fj F ⊺
different q -vectors of functions. So, we have
⊺
⊺
Returning to the base case of p = 1 covariate, consider the population regularized risk optimization
5 Stationary Conditions and Back ﬁ tting Algorithms
E∥Y − M (X )∥2
{ 1
2 + λ∣∣∣M ∣∣∣∗},
min
(5.1)
2
E(Y ∣ X ) = M (X ) + λV (X )
a.e. for some V ∈ ∂ ∣∣∣M ∣∣∣∗ .
M
where M is a vector of q univariate functions. The stationary condition for this optimization is
Deﬁ ne P (X ) ∶= E(Y ∣ X ).

(5.2)

4

Input: Data (Xi , Yi ), regularization parameter λ.
CRAM BACK FI T T ING A LGOR I THM — F IR S T P ENA LTY
Initialize ̂Mj = 0, for j = 1, . . . , p.
For each j = 1, . . . , p:
̂Mk (Xk );
(1) Compute the residual: Zj = Y − ∑k≠j
Iterate until convergence:
(2) Estimate Pj = E[Zj ∣ Xj ] by smoothing: ̂Pj = Sj Zj ;
̂P ⊺
̂Pj
j = U diag(τ )U ⊺
(4) Soft threshold: ̂Mj = U diag([1 − λ/√
)U ⊺ ̂Pj ;
τ ]
(5) Center: ̂Mj ← ̂Mj − mean(̂Mj ).
(3) Compute SVD: 1
n
Output: Component functions ̂Mj and estimator ̂M (Xi ) = ∑j
̂Mj (Xij ).
+
tting algorithm, using the ﬁ rst penalty, which penalizes each component.
Figure 1: The CRAM backﬁ
Proposition 5.1. Let E(P P ⊺ ) = U diag(τ )U ⊺ be the singular value decomposition and deﬁ ne
M = U diag([1 − λ/√
τ ]+ )U
where [x]+ = max(x, 0). Then M satisﬁ es stationary condition (5.2), and is a minimizer of (5.1).
⊺
(5.3)
P
Proof. Assume the singular values are sorted as τ1 ≥ τ2 ≥ ⋯ ≥ τq , and let r be the largest index such
√
E(M M ⊺ ) = U diag([√
√
τr > λ. Thus, M has rank r . Note that
τ − λ]+ )U ⊺ , and therefore
λ(√
M = U diag(λ/√
E(M M ⊺ ))−1
τ1∶r , 0q−r )U
that
where x1∶k = (x1 , . . . , xk ) and ck = (c, . . . , c). It follows that
⊺
(5.4)
P
M + λ(√
E(M M ⊺ ))−1
M = U diag(1r , 0q−r )U ⊺P .
U diag(0r , 1q−r )U ⊺P
H = 1
and take V = (√
Now de ﬁ ne
E(M M ⊺ ))−1
M + H . Then we have M + λV = P .
(5.6)
√
λ
E(HH ⊺ ) =
√
√
U diag(0r ,
τq /λ)U ⊺ we have ∣∣∣H ∣∣∣sp ≤ 1. Also, E(M H ⊺ ) = 0q×q since
τr+1 /λ, . . . ,
diag(1 − λ/√
τ1∶r , 0q−r ) diag(0r , 1q−r /λ) = 0q×q .
It remains to show that H satisﬁ es the conditions of the subdifferential in (4.3). Since
Similarly, E(M M ⊺ )H = 0q×q since
(5.7)
diag((√
τ1∶r − λ)2 , 0q−r ) diag(0r , 1q−r /λ) = 0q×q .
It follows that V ∈ ∂ ∣∣∣M ∣∣∣sp .
tting algorithm for estimating a constrained rank additive model
The analysis above justiﬁ es a backﬁ
E∥Y − p∑
Mj (Xj )∥2
{ 1
+ λ
∣∣∣Mj ∣∣∣∗}.
p∑
with the ﬁ rst penalty, where the objective is
For a given coordinate j , we form the residual Zj = Y − ∑k≠j Mk , and then compute the projection
min
(5.9)
2
Mj
Pj = E(Zj ∣ Xj ), with singular value decomposition E(Pj P ⊺
j ) = U diag(τ )U ⊺ . We then update
2
j=1
j=1
Mj = U diag([1 − λ/√
τ ]+ )U
⊺
(5.10)
Pj
5

(5.8)

(5.5)

Pj = E(Zj ∣ Xj ) by a nonparametric linear smoother, ̂Pj = Sj Zj . The algorithm is given in Figure 1.
and proceed to the next variable. This is a Gauss-Seidel procedure that parallels the population
backﬁ
tting algorithm for S PAM [6]. In the sample version we replace the conditional expectation
using that point; that is, ̂Pj (xj ) = Sj (xj )⊺Zj .
Note that to predict at a point x not included in the training set, the smoother matrices are constructed
1∶p . Then, in step (4) we soft threshold according to ̂M1∶p = U diag([1 − λ/√
̂P1∶p
̂P ⊺
)U ⊺ ̂P1∶p .
τ ]
The algorithm for penalty 2 is similar. In step (3) of the algorithm in Figure 1 we compute the SVD
of 1
n
+
Both algorithms can be viewed as functional projected gradient descent procedures.
The population risk of a q × p regression matrix M (X ) = [M1 (X1 )⋯Mp (Xp )] is
6 Excess Risk Bounds
R(M ) = E∥Y − M (X )1p ∥2
with sample version denoted ̂R(M ). Consider all models that can be written as
2 ,
M (X ) = U ⋅ D ⋅ V (X )⊺
where U is an orthogonal q × r matrix, D is a positive diagonal matrix, and V (X ) = [vj s (Xj )]
satisﬁ es E(V ⊺V ) = Ir . The population risk can be reexpressed as
R(M ) = tr {( −Iq
V (X )⊺ )⊺ ] ( −Iq
V (X )⊺ ) (
E [(
DU ⊺)⊺
DU ⊺)}
= tr {( −Iq
) ( −Iq
Y
Y
DU ⊺)⊺ (ΣY Y ΣY V
DU ⊺)}
and similarly for the sample risk, with ̂Σn (V ) replacing Σ(V ) ∶= Cov((Y , V (X )⊺ )) above. The
Σ⊺
Y V ΣV V
“uncontrollable” contribution to the risk, which does not depend on M , is Ru = tr{ΣY Y }. We can
Rc (M ) = R(M ) − Ru = tr {( −2Iq
Σ(V ) ( 0q
DU ⊺)⊺
DU ⊺)} .
express the remaining “controllable” risk as
Using the von Neumann trace inequality, tr(AB ) ≤ ∥A∥p ∥B ∥p′ where 1/p + 1/p′ = 1,
Rc (M ) − ̂Rc (M ) ≤ ∥( −2Iq
DU ⊺)⊺ (Σ(V ) − ̂Σn (V ))∥
DU ⊺)∥
∥( 0q
≤ ∥( −2Iq
∥Σ(V ) − ̂Σn (V )∥
DU ⊺)⊺∥
∥D∥∗
∗
sp
≤ C max(2, ∥D∥sp ) ∥Σ(V ) − ̂Σn (V )∥
∥D∥∗
sp
∗} ∥Σ(V ) − ̂Σn (V )∥
sp
≤ C max{2, ∥D∥2
sp
sp
∥Σ(V ) − ̂Σn (V )∥
⊺ (Σ(V ) − ̂Σn (V )) w
≤ C sup
where here and in the following C is a generic constant. For the last factor in (6.1), it holds that
where N is a 1/2-covering of the unit (q + r)-sphere, which has size ∣N ∣ ≤ 6q+r ≤ 36q ; see [8]. We
sup
sup
w
now assume that the functions vsj (xj ) are uniformly bounded from a Sobolev space of order two.
sp
w∈N
V
V
Speciﬁ cally, let {ψjk ∶ k = 0, 1, . . .} denote a uniformly bounded, orthonormal basis with respect to
L2 [0, 1], and assume that vsj ∈ Hj where
Hj = {fj ∶ fj (xj ) = ∞∑
jk k4 ≤ K 2}
ajk ψjk (xj ),
∞∑
for some 0 < K < ∞. The L∞ -covering number of Hj satisﬁ es log N (Hj , ) ≤ K /√
a2
k=0
k=0
.
6

(6.1)

d

sup
w∈N

Suppose that Y − E(Y ∣ X ) = W is Gaussian and the true regression function E(Y ∣ X ) is bounded.
Then the family of random variables Z(V ,w) ∶= √
n ⋅ w⊺ (Σ(V ) − ̂Σn (V ))w is sub-Gaussian and
√
w⊺ (Σ(V ) − ̂Σn (V ))w) ≤ C√
q log(36) + log(pq) + K√
E (sup
∫ B
sample continuous. It follows from a result of Cesa-Bianchi and Lugosi [1] that
√
n

0
V
⎞⎠ .
⎛⎝
q + log(pq)
∥Σ(V ) − ̂Σn (V )∥
= OP
for some constant B . Thus, by Markov ’s inequality we conclude that
sup
If ∣∣∣M ∣∣∣∗ = ∥D∥∗ = o (n/(q + log(pq)))1/4 , then returning to (6.1), this gives us a bound on Rc (M )−
(6.2)
sp
n
V
̂Rc (M ) that is oP (1). More precisely, we deﬁ ne a class of matrices of functions:
q + log(pq) )1/4⎫⎪⎪⎬⎪⎪⎭ .
Mn = ⎧⎪⎪⎨⎪⎪⎩M ∶ M (X ) = U DV (X )⊺
V ) = I , vsj ∈ Hj , ∥D∥∗ = o (
, with E(V
n
tted matrix ̂M chosen from Mn , writing M∗ = arg minM ∈Mn R(M ), we have
⊺
R(̂M ) − inf
R(M ) = R(̂M ) − ̂R(̂M ) − (R(M∗ ) − ̂R(M∗ )) + ( ̂R(̂M ) − ̂R(M∗ ))
Then, for a ﬁ
≤ [R(̂M ) − ̂R(̂M )] − [R(M∗ ) − ̂R(M∗ )].
M ∈Mn
Subtracting Ru − ̂Ru from each of the bracketed differences, we obtain that
R(̂M ) − inf
R(M ) ≤ [Rc (̂M ) − ̂Rc (̂M )] − [Rc (M∗ ) − ̂Rc (M∗ )]
{Rc (M ) − ̂Rc (M )}
≤ 2 sup
M ∈Mn
∗ ∥Σ(V ) − ̂Σn (V )∥
by (6.1)≤ O (∥D∥2
) by (6.2)= oP (1).
M ∈Mn
Proposition 6.1. Let ̂M minimize the empirical risk 1
n ∑i ∥Yi − ∑j Mj (Xij )∥2
2 over the class Mn .
sp
This proves the following result.
R(̂M ) − inf
R(M ) PD→ 0 .
Then
M ∈Mn
7 Application to Gene Expression Data

To illustrate the proposed nonparametric reduced rank regression techniques, we consider data on
1 [3]. In this challenge
gene expression in E. coli from the “DREAM 5 Network Inference Challenge”
We focus on predicting the expression levels Y for a particular set of q = 27 TGs, using the expres-
genes were classiﬁ ed as transcription factors (TFs) or target genes (TGs). Transcription factors
sion levels X for p = 6 TFs. Our motivation for analyzing these 33 genes is that, according to the
regulate the target genes, as well as other TFs.
gold standard gene regulatory network used for the DREAM 5 challenge, the 6 TFs form the parent
set common to two additional TFs, which have the 27 TGs as their child nodes. In fact, the two
functional relationship between X and Y is given by the composition of a map g ∶ R
6 → R
intermediate nodes d-separate the 6 TFs and the 27 TGs in a Bayesian network interpretation of this
map h ∶ R
2 → R
27 . If g and h are both linear, their composition h ○ g is a linear map of rank no more
gold standard. This means that if we treat the gold standard as a causal network, then up to noise, the
2 and a
is linear, then h ○ g has rank at most 2 in the sense of penalty 2. Higher rank can in principle occur
than 2. As observed in Section 2, such a reduced rank linear model is a special case of an additive
model with reduced rank in the sense of penalty 2. More generally, if g is an additive function and h
1http://wiki.c2b2.columbia.edu/dream/index.php/D5c4

7

Penalty 1, λ = 20

Penalty 2, λ = 5

Figure 2: Left: Penalty 1 with large tuning parameter. Right: Penalty 2 with tuning parameter ob-
tained through 10-fold cross-validation. Plotted points are residuals holding out the given predictor.
under functional composition, since even a univariate additive map h ∶ R → R
q may have rank up to
q under our penalties (recall that penalty 1 and 2 coincide for univariate maps).
The backﬁ
tting algorithm of Figure 1 with penalty 1 and a rather aggressive choice of the tuning
parameter λ produces the estimates shown in Figure 2, for which we have selected three of the 27
TGs. Under such strong regularization, the 5th column of functions is rank zero and, thus, identically
zero. The remaining columns have rank one; the estimated ﬁ
tted values are scalar multiples of one
another. We also see that scalings can be different for different columns. The third plot in the third
row shows a slightly negative slope, indicating a negative scaling for this particular estimate. The
remaining functions in this row are oriented similarly to the other rows, indicating the same, positive
scaling. This property characterizes the difference between penalties 1 and 2; in an application of
penalty 2, the scalings would have been the same across all functions in a given row.

Next, we illustrate a higher-rank solution for penalty 2. Choosing the regularization parameter λ by
t of rank 5, considerably lower than 27, the maximum possible
ten-fold cross-validation gives a ﬁ
rank. Figure 2 shows a selection of three coordinates of the ﬁ
tted functions. Under rank ﬁ ve, each
row of functions is a linear combination of up to ﬁ ve other, linearly independent rows. We remark
that the use of cross-validation generally produces somewhat more complex models than is necessary
to capture an underlying low-rank data-generating mechanism. Hence, if the causal relationships for
these data were indeed additive and low rank, then the true low rank might well be smaller than ﬁ ve.

8 Summary

This paper introduced two penalties that induce reduced rank ﬁ
ts in multivariate additive nonpara-
metric regression. Under linearity, the penalties specialize to group lasso and nuclear norm penalties
for classical reduced rank regression. Examining the subdifferentials of each of these penalties, we
developed backﬁ
tting algorithms for the two resulting optimization problems that are based on soft-
thresholding of singular values of smoothed residual matrices. The algorithms were demonstrated
on a gene expression data set constructed to have a naturally low-rank structure. We also provided a
persistence analysis that shows error tending to zero under a scaling assumption on the sample size
n and the dimensions q and p of the regression problem.

Acknowledgements

Research supported in part by NSF grants IIS-1116730, DMS-0746265, and DMS-1203762,
AFOSR grant FA9550-09-1-0373, ONR grant N000141210762, and an Alfred P. Sloan Fellowship.

8

References
[1] Nicol `o Cesa-Bianchi and G ´abor Lugosi. On prediction of individual sequences. The Annals of
Statistics, 27(6):1865–1894, 1999.
[2] Maryam Fazel. Matrix rank minimization with applications. Technical report, Stanford Uni-
versity, 2002. Doctoral Dissertation, Electrical Engineering Department.
[3] D. Marbach, J. C. Costello, R. K ¨uffner, N. Vega, R. J. Prill, D. M. Camacho, K. R. Allison,
the DREAM5 Consortium, M. Kellis, J. J. Collins, and G. Stolovitzky. Wisdom of crowds for
robust gene network inference. Nature Methods, 9(8):796–804, 2012.
[4] Sahan Negahban and Martin J. Wainwright. Estimation of (near) low-rank matrices with noise
and high-dimensional scaling. Annals of Statistics, 39:1069–1097, 2011.
[5] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Minimax-optimal rates for sparse addi-
tive models over kernel classes via convex programming. arxiv:1008.3654, 2010.
[6] Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models.
Journal of the Royal Statistical Society, Series B, Methodological, 71(5):1009–1030, 2009.
[7] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum rank solutions to
linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.
[8] Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix?
arxiv:1004.3484, 2010.
[9] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra
and Applications, 170:1039–1053, 1992.
[10] Ming Yuan, Ali Ekici, Zhaosong Lu, and Renato Monteiro. Dimension reduction and coeff-
cient estimation in multivariate linear regression. J. R. Statist. Soc. B, 69(3):329–346, 2007.
[11] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49 –67, 2006.

9


THE K-DISCS ALGORITHM AND ITS KERNELIZATION

MICHAEL LINDSEY

Abstract. A new clustering algorithm called k-discs is introduced. This algorithm, though similar to
k-means, addresses the deciencies of k-means as well as its variant k-subspaces. The k-discs algorithm is
applied to the recovery of manifolds from noisy samplings. A kernelization of k-discs is exhibited, and the
advantages of this modication are demonstrated on synthetic data. For this pro ject, the kernelized and
linear algorithms were implemented from scratch in MATLAB.

1. Introduction and Motivation

A clustering algorithm is usually run on a data set under the assumption that there is some structure of
various connected components underlying the data set (or to test whether this is a good assumption). Suppose
that a set of n-dimensional data has been sampled, with noise, from a smooth submanifold (potentially with
boundary) of Rn . One wishes to recover, in some sense, the underlying structure of the data, given only the
data itself. Arguably the most fundamental aspect of this structure is the characterization of the connected
components of the manifold, manifested as clusters in the data set. Unfortunately, standard clustering
algorithms such as k-means and expectation-maximization often fail to properly characterize the shapes of
these components. Kernel k-means succeeds in many circumstances in clustering data that is not separable
by ordinary linear k-means. However, due to its zero-dimensional characterization of clusters, it fails to
identify clusters drawn from elongated subsets of submanifolds.
k-subspaces is a variant of k-means which seeks to minimize the total distance of the data from a set of k
d-dimensional subspaces. The kernelization of k-subspaces achieves good results in identifying clusters drawn
from nonlinear d-dimensional submanifolds (in particular, submanifolds without boundary). However, the
innite extent of this algorithm's clusters can result in awkward clustering. For example, k-subspaces will
have diculty in identifying bounded clusters drawn from the same subspace (or the same submanifold).

2. The k-discs Algorithm

In short, the k-subspaces algorithm attempts to minimize the total distance of the data from a set of
k d-dimensional discs. Since the extent of data is nite, we do not pay a great price for viewing clusters
as bounded subsets of subspaces. In fact, this algorithm can be seen as a renement of both k-means and
k-subspaces in that it:
â€¢ can identify each point with a cluster centroid (the center of its assigned disc) or with its pro jection
onto the subspace containing the disc
â€¢ endows clusters with some (non-zero-dimensional) notion of shape and scalar measure
â€¢ allows for multiple clusters within or near the same subspace.

3. Algorithmic Details
The k-discs algorithm consists of the following steps. All points are in Rn , and we are given a data set
{x(1) , . . . , x(m) }.
(1) For a given k (number of clusters) and d (dimension of cluster discs), initialize k cluster centroids
Âµj and a random set {uj l } of d orthonormal basis vectors for each cluster j . Select initial values for
the disc radii rj .
(2) Repeat until convergence: {

Date : December 14, 2012.

1

2

THE K-DISCS ALGORITHM AND ITS KERNELIZATION
j = x(i) âˆ’ Âµj . Compute the distance Aij of x(i)
(cid:69)
(cid:68)
(a) For all i, j , let x(i)
j ) = (cid:80)d
to the subspace spanned by
j
{uj l } (equal to the distance of x(i) to the ane space generated by the j -th cluster disc).
We see that Aij = ||x(i)
j âˆ’ Pj (x(i)
j )||, where Pj (x(i)
x(i)
j , uj l
.
l=1
j ) to the origin: Bij = ||Pj (x(i)
(b) Next, for all i, j , compute the distance Bij of Pj (x(i)
j )||. We see
that if Bij â‰¤ rj , then the distance D(i, j ) between x(i) and the j -th disc is Aij .
If Bij > rj , then D(i, j ) = ||x(i)
j âˆ’ rj
j )||.
Pj (x(i)
Bij
(c) For all i, set c(i) := arg min
D(i, j ).
(cid:80)m
j
(cid:80)m
i=1 1{c(i) = j }x(i)
i=1 1{c(i) = j } .
Âµj :=
(e) For all j , set {uj l } to be the set of d principal components of {x(i) âˆ’ Âµj : c(i) = j }. (This step
involves PCA, i.e., nding the d principal eigenvectors of each cluster's data covariance matrix).
||Pj (x(i) âˆ’ Âµj )||, noting that the meaning of Pj has changed due to the
(f ) For all j , set rj := max
i:c(i)=j
update to {uj l }.

(d) For all j , set

}

4. Convergence

(cid:80)m
We can guarantee that k-discs converges to a local optimum. Repeated random initializations are recom-
notation from above, J = (cid:80)m
mended to approximate the global optimum. In specic, we dene our ob jective function J (c(i) , Âµj , uj l , rj ) =
i=1 Dist2 (x(i) , âˆ†ci ), where âˆ†j is the j -th disc, which can be viewed as a function on Âµj , uj l , rj . Using the
i=1 D(i, c(i) )2 . The value of this function is non-increasing over all iterations.
To see this, we can view the k-discs algorithm as coordinate descent on the arguments of J . (Note that
the arguments of J are, more precisely, {c(i) }, {Âµj }, {uj l }, {rj }, though we will omit the curly braces for
visual clarity.) Suppose that at some iteration our parameters have values c(i) , Âµj , uj l , rj . We will write
their respective updated values as c(cid:48)(i) , Âµ(cid:48)
j . Note that in step (c) we set c(cid:48)(i) := arg min
j , u(cid:48)
j l , r (cid:48)
D(i, j ). Thus
j
D(i, c(cid:48)(i) )2 â‰¤ D(i, c(i) )2 for all i, and J (c(cid:48)(i) , Âµj , uj l , rj ) â‰¤ J (c(i) , Âµj , uj l , rj ).
(cid:80)m
It is well known that PCA (taking an expanded view of PCA that includes the computation of means
Âµj ) chooses a set of ane subspaces (or rather, values for Âµj and uj l ) which minimize H (c(i) , Âµj , uj l ) =
i=1 Dist2 (x(i) , Sci ), where Sj is the ane subspace containing the j -th disc âˆ†j . (S (cid:48)
j and âˆ†(cid:48)
j will denote
j l ) â‰¤ H (c(cid:48)(i) , Âµj , uj l ). And of course H (c(cid:48)(i) , Âµj , uj l ) â‰¤
j , u(cid:48)
the updated ane subspace and disc.) So H (c(cid:48)(i) , Âµ(cid:48)
J (c(cid:48)(i) , Âµj , uj l , rj ) because âˆ†j âŠ‚ Sj for all j . Finally, observe that by the update in step (f ), ||Pc(i) (x(i) âˆ’
Âµc(i) )|| â‰¤ r (cid:48)
) = Dist2 (x(i) , âˆ†(cid:48)
c(i) for all i. Hence Dist2 (x(i) , S (cid:48)
), and
ci
ci
j ) = H (c(cid:48)(i) , Âµj , uj l ) â‰¤ J (c(i) , Âµj , uj l , rj ),
J (c(cid:48)(i) , Âµ(cid:48)
j , u(cid:48)
j l , r (cid:48)
as was to be shown. The ob jective function is non-increasing and bounded below by zero, hence convergent.
Note that we can slightly modify step (f ) of the algorithm and still guarantee convergence of J . For
example, we might set rj = constant for the rst several iterations of the algorithm, then update rj as
above. In fact, we can set rj = 0 initially, which is equivalent to running k-means for several iterations.
This is a good heuristic for initializing the cluster centroids before running k-discs proper and allows us to
skip the PCA step until it is particularly useful. For high-dimensional data, PCA involves nding a (partial)
eigendecomposition of an n Ã— n matrix, which is very expensive. Note also that k-subspaces can be recovered
from k-discs by permanently setting rj = âˆž.
Allowed to run as initially described, k-discs almost always converged in many fewer than 20 iterations
for all data sets tested. However, the convergence behavior depends greatly on the shape of the data as well
as initialization procedures. A useful initialization heuristic (to be used before any other processing) which
can enhance convergence rates and values is to select special landmark points as clusters. In particular, the
maxmin landmark selection procedure simply adds a random point to the landmark set L, then repeatedly
adds the point with maximal distance from L to the landmark set L. This procedure gives initial cluster
centers that are roughly evenly spaced. This initialization, however, may sometimes bias the clustering

THE K-DISCS ALGORITHM AND ITS KERNELIZATION

3

algorithm toward a particular set of local minima that are not particularly desirable, and in such cases,
repeated random initialization is preferable.

5. Illustrative Synthetic Linear Examples

The following visualizations on the plane demonstrate the improved capacity of k-discs to recover the set
of line segments from which a random data set is drawn.

Above left: 500 points sampled from indicated lines with Gaussian noise, Î£ = 0.005I . Above right: Result of k-discs on random
sampling. Below left: Result of k-means. Below right: Result of k-subspaces. All clustering initialized with landmark selection.

6. Application to Submanifold Reconstruction

Given a noisy sampling from a d-dimensional submanifold, k-discs (using d-dimensional discs) can be
used to generate a piecewise linear approximation of the manifold. The highest value of k that can be used
eectively relates inversely with the noisiness of the data. For noisy data, a value of k that is too large will
yield cluster centroids that deviate greatly from the submanifold. For (n âˆ’ 1)-dimensional submanifolds, if
we repeat the k-means algorithm many times and associate each data point with the mean of its associated
cluster centroids as well as the mean of its associated normal vectors, we can construct images as below.
This capacity could be of use in computer vision applications in which noisy point cloud representations
of surfaces are obtained. The reconstruction points could be used for surface reconstruction as well as
topological calculations (of Betti numbers, for example, which could be used for hole detection).

THE K-DISCS ALGORITHM AND ITS KERNELIZATION

4

Left: Shown in red are 500 points sampled from the unit circle with Gaussian noise, Î£ = 0.01I .
In blue are reconstructed points
obtained from averaging 50 repetitions of k-discs with k = 7, and in black are the reconstructed normal vectors passing through these
points. Right: Same as left, except Î£ = 0.05I , k = 4. Landmark initialization was used for both trials. Similar results were obtained
for surfaces in three dimensions, such as the sphere and torus, though these cannot be visualized here. Note that for smaller values
of k, the radius of the reconstructed circle is signicantly smaller than 1. This occurs because the cluster centroids are closer to the
origin. Analagous results will occur for any surface of signicant curvature. The topology of the surface, however, will be preserved.

=

=

Ï†(x(i(cid:48) ) ),

7. Kernelization
We observe that k-discs has a natural kernelization. Let Ï† be a mapping from Rn to some high-dimensional
feature space, and suppose that we are given a function K : Rn Ã— Rn â†’ R such that K (x, y) = (cid:104)Ï†(x), Ï†(y)(cid:105).
We do not work directly with vectors in the high-dimensional space; rather, we can only process them
implicitly by using inner products. In particular, the algorithm requires the following inner products to be
â€¢ (cid:68)
(cid:69)
stored in place of the vectors Ï†(x(i) ), Âµj , uj l :
= K (x(i) , x(i(cid:48) ) ) for all i, i(cid:48) = 1, . . . , m. These values are stored in a kernel matrix
Ï†(x(i) ), Ï†(x(i(cid:48) ) )
(cid:11) for all i, j . These values are updated every time the assignments c(i) are modied. They
â€¢ (cid:10)Ï†(x(i) ), Âµj
immediately and never updated.
(cid:80)m
(cid:80)m
(cid:29)
(cid:28)
can be computed
(cid:68)
(cid:69)
(cid:80)m
(cid:80)m
i=1 1{c(i) = j }K (x(i) , x(i(cid:48) ) )
i=1 1{c(i) = j }Ï†(x(i) )
Ï†(x(i(cid:48) ) ), Âµj
i=1 1{c(i) = j }
i=1 1{c(i) = j }
â€¢ (cid:104)Âµj , Âµj (cid:105) for all j , updated when the assignments are modied.
(cid:80)m
(cid:0)(cid:80)m
i=1 1{c(i) = j }(cid:1)2
i,i(cid:48)=1 1{c(i) = j }1{c(i(cid:48) ) = j }K (x(i) , x(i(cid:48) ) )
(cid:104)Âµj , Âµj (cid:105) =
â€¢ (cid:10)Ï†(x(i) ) âˆ’ Âµj , uj l
(cid:11) for all i, j, l. This update is more sophistocated and makes use of the kernel PCA
algorithm. Let Xj l be the matrix whose rows are Ï†(x(i) )âˆ’ Âµc(i) for all i such that c(i) = j . Relabel the
rows Ï†(x(i) )âˆ’ Âµc(i) as x(p) for p = 1, . . . , P . Regular PCA considers the eigendecomposition of X T
j Xj ,
j l X T vj l . So (cid:10)Ï†(x(i) ) âˆ’ Âµj , uj l
(cid:11) =
but we cannot directly compute this matrix. It can be shown that Xj X T
j has the same eigenvalues
(with all additional eigenvalues equal to zero) and that the (orthonormal) eigenvectors vj l of Xj X T
c(i) l (cid:104)y , vj l (cid:105), where the p-th component of y is yp = (cid:10)Ï†(x(i) ) âˆ’ Âµj , x(p) (cid:11), which can in turn be
j
âˆ’ 1
relate to the corresponding eigenvectors uj l of Xj by uj l = Î»
2
âˆ’ 1
Î»
2
written in terms of inner products computed above.
The computations in the description of the linear algorithm above can all be carried out using these inner
products, but the details are unenlightening and will not be reproduced here. The steps of the kernel k-discs
algorithm, including the calculation of rj , follow from those of the linear algorithm.
(cid:10)Ï†(x(i) ), Âµj
(cid:11) and (cid:104)Âµj , Âµj (cid:105) as above. Then clustering assignments are made as in kernel k-means (i.e., using
We note that the initialization of kernel k-discs is somewhat modied. First, cluster centroids are randomly
selected from the original data, then mapped into the high-dimensional feature space by Ï†. We store
(cid:11) for all i, l. The disc radii are then updated in the usual way.
cluster as above to compute (cid:10)Ï†(x(i) ) âˆ’ Âµj , uj l
distances in the high-dimensional space from the cluster centroids). Then kernel PCA is performed on each

8. Illustrative Synthetic Nonlinear Examples

The following visualizations demonstrate the advantage of k-discs in a particular situation.

THE K-DISCS ALGORITHM AND ITS KERNELIZATION

5

All images show 300 points sampled from three quadratic curves with Gaussian noise, Î£ = 0.01I . Above left: Result of kernel k-discs
clustering on random sampling, polynomial kernel of degree 2. Above right: Result of kernel k-means, same kernel. Below left: Result
of kernel k-subspaces, same kernel. Below right: Typical result of linear k-discs. It is of interest that even though the data is separable
with respect to the linear k-discs algorithm, a perfect classication is almost never achieved. Random clustering was used for these
trials. (Landmark selection guaranteed poor results in this case.)

9. Conclusions and Further Work

The k-discs algorithm and its kernelization have shown promise in their intended goal of clustering data
sampled from bounded submanifolds. Of course, one disadvantage (shared with k-means) of the algorithm
is that k must be selected as a parameter. Furthermore, d must also be selected as a parameter, and all
discs are constrained to have the same dimension. A more sophistocated version of the algorithm might
automatically select dimensions for its cluster descriptions and allow for discs of dierent dimensions.
Finally, it remains to be seen how k-discs will perform on real-world data. An application of the algorithm
to three-dimensional point cloud data could demonstrate the feasibility of using the algorithm for surface
reconstruction. It would be interesting to see the results of topological algorithms such as JavaPlex[3] on
such reconstructions. In addition, linear and kernel k-discs could be applied to arbitrary clustering problems
and compared in performance with k-means, k-subspaces, and any of a multitude of clustering algorithms.
Clustering problems are diverse, and the applicability of k-discs in any particular scenario depends mainly
on whether the desired clustering has the structure of an ane subspace, either in the original feature space
itself or in a high-dimensional feature space that respects a kernel. For example, the success of Li and
Fukui[6] in performing facial recognition with linear and kernel k-subspaces suggests that k-discs could have
similar, if not better, results for the same type of data.
Assuming that a data set does have this structure, k-discs may sometimes fail converge to or even approx-
imate a global optimum. It would be worthwhile to try to develop some adaptive initialization techniques
that improve initialization based on the converged values of the ob jective function.

References

[1] A. Nielsen, A Kernel Version of Spatial Factor Analysis , (2009).
[2] A. Tausz, M. Vejdemo-Johansson, and H. Adams, JavaPlex: A research software package for persistent (co)homology , (2011),
software available at http://code.google.com/javaplex.
[3] D. Wang, C. Ding, and T. Li, K-Subspace Clustering , (2009).
[4] G. Carlsson, Topology and Data (2008).
[5] H. Adams and A. Tausz, Javaplex tutorial .
[6] X. Li and K. Fukui Nonlinear k-subspaces based appearances clustering of objects under varying il lumination conditions , (2007).


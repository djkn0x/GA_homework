Transelliptical Graphical Models

Han Liu
Department of Operations Research
and Financial Engineering
Princeton University, NJ 08544
hanliu@princeton.edu

Fang Han
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21210
fhan@jhsph.edu

Cun-hui Zhang
Department of Statistics
Rutgers University
Piscataway, NJ 08854
cunhui@stat.rutgers.edu

Abstract

We advocate the use of a new distribution family—the transelliptical—for robust
inference of high dimensional graphical models. The transelliptical family is an
extension of the nonparanormal family proposed by Liu et al. (2009). Just as the
nonparanormal extends the normal by transforming the variables using univariate
functions, the transelliptical extends the elliptical family in the same way. We
propose a nonparametric rank-based regularization estimator which achieves the
parametric rates of convergence for both graph recovery and parameter estima-
tion. Such a result suggests that the extra robustness and ﬂexibility obtained by
the semiparametric transelliptical modeling incurs almost no efﬁciency loss. We
also discuss the relationship between this work with the transelliptical component
analysis proposed by Han and Liu (2012).

1 Introduction
We consider the problem of learning high dimensional graphical models.
In a typical setting, a
d-dimensional random vector X = (X1 , ..., Xd )T can be represented as an undirected graph de-
noted by G = (V , E ), where V contains nodes corresponding to the d variables in X , and the
edge set E describes the conditional independence relationship among X1 , ..., Xd . Let X\{i,j} :=
{Xk : k 6= i, j }. We say the joint distribution of X is Markov to G if Xi is independent of Xj given
X\{i,j} for all (i, j ) /∈ E . While often G is assumed given, here we want to estimate it from data.
Most graph estimation methods rely on the Gaussian graphical models, in which the random vector
X is assumed to be Gaussian: X ∼ Nd (µ, Σ). Under this assumption, the graph G is encoded
by the precision matrix Θ := Σ−1 . More speciﬁcally, no edge connects Xj and Xk if and only
if Θjk = 0. This problem of estimating G is called covariance selection [5]. In low dimensions
where d < n, [6, 7] develop a multiple testing procedure for identifying the sparsity pattern of the
precision matrix. In high dimensions where d (cid:29) n, [21] propose a neighborhood pursuit approach
for estimating Gaussian graphical models by solving a collection of sparse regression problems using
the Lasso [25, 3]. Such an approach can be viewed as a pseudo-likelihood approximation of the full
likelihood.
In contrast, [1, 30, 10] propose a penalized likelihood approach to directly estimate
Ω. [15, 14, 24] maximize the non-concave penalized likelihood to obtain an estimator with less
bias than the traditional L1 -regularized estimator. Under the irrepresentable conditions [33, 31, 27],
[22, 23] study the theoretical properties of the penalized likelihood methods. More recently, [29, 2]
propose the graphical Dantzig selector and CLIME, which can be solved by linear programming and
possess more favorable theoretical properties than the penalized likelihood approach.

1

Besides Gaussian models, [18] propose a semiparametric procedure named nonparanormal SKE P -
T IC which extends the Gaussian family to the more ﬂexible semiparametric Gaussian copula family.
Instead of assuming X follows a Gaussian distribution, they assume there exists a set of monotone
functions f1 , . . . , fd , such that the transformed data f (X ) := (f1 (X1 ), . . . , fd (Xd ))T is Gaussian.
More details can be found in [18]. [32] has developed a scalable software package to implement
these algorithms. In another line of research, [26] extends the Gaussian graphical models to the
elliptical graphical models. However, for elliptical distributions, only the generalized partial cor-
relation graph can be reliably estimated. These graphs only represent the conditional uncorrelated-
ness, but conditional independence, among variables. Therefore, by extending the Gaussian to the
elliptical family, the gain in modeling ﬂexibility is traded off with a loss in the strength of inference.
In a related work, [9] provide a latent variable interpretation of the generalized partial correlation
graph for multivariate t-distributions. An EM-type algorithm is proposed to ﬁt the model for high
dimensional data. However, the theoretical properties of their estimator is unknown.
In this paper, we introduce a new distribution family named transelliptical graphical model. A key
concept is the transelliptical distribution [12]. The transelliptical distribution is a generalization of
the nonparanormal distribution proposed by [18]. By mimicking how the nonparanormal extends the
normal family, the transelliptical extends the elliptical family in the same way. The transelliptical
family contains the nonparanomral family and elliptical family. To infer the graph structure, a rank-
based procedure using the Kendall’s tau statistic is proposed. We show such a procedure is adaptive
over the transelliptical family: the procedure by default delivers a conditional uncorrelated graphs
among certain latent variables; however, if the true distribution is the nonparanormal, the procedure
automatically delivers the conditional independence graph. Computationally, the only extra cost is a
one-pass data sort, which is almost negligible. Theoretically, even though the transelliptical family
is much larger than the nonparanormal family, the same parametric rates of convergence for graph
recovery and parameter estimation can be established. These results suggest that the transelliptical
graphical model can be used routinely as a replacement of the nonparanormal models. Thorough
numerical results are provided to back up our theory.
2 Background on Elliptical Distributions
Let X and Y be two random variables, we denote by X d= Y if they have the same distribution.
Deﬁnition 2.1 (elliptical distribution [8]). Let µ ∈ Rd and Σ ∈ Rd×d with rank(Σ) = q ≤ d. A
d-dimensional random vector X has an elliptical distribution, denoted by X ∼ ECd (µ, Σ, ξ ), if it
has a stochastic representation: X d= µ + ξAU , where U is a random vector uniformly distributed
on the unit sphere in Rq , ξ ≥ 0 is a scalar random variable independent of U , A ∈ Rd×q is a
deterministic matrix such that AAT = Σ.
Remark 2.1. An equivalent deﬁnition of an elliptical distribution is that its characteristic function
can be written as exp(itT µ)φ(tT Σt), where φ is a properly-deﬁned characteristic function which
has a one-to-one mapping with ξ in Deﬁnition 2.1. In this setting we denote by X ∼ ECd (µ, Σ, φ).
An elliptical distribution does not necessarily have a density. One example is the rank-deﬁcient
Gaussian. More examples can be found in [11]. However, when the random variable ξ is absolutely
p(x) = |Σ|−1/2 g (cid:0)(x − µ)T Σ−1 (x − µ)(cid:1) ,
continuous with respect to the Lebesgue measure and Σ is non-singular, the density of X exists and
has the form
(1)
where g(·) is a scale function uniquely determined by the distribution of ξ . In this case, we can also
denote it as X ∼ ECd (µ, Σ, g). Many multivariate distributions belong to the elliptical family. For
example, when g(x) = (2π)−d/2 exp {−x/2}, X is d-dimensional Gaussian. Another important
Γ (cid:0) v+d
(cid:1)
(cid:18)
(cid:19)− v+d
subclass is the multivariate t-distribution with the degrees of freedom v , in which, we choose
2
2
2 Γ( v
(vπ) d
2 )

1 − c2
v x
v

g(x) = cv

,

(2)

where cv is a normalizing constant.
The model family in Deﬁnition 2.1 is not identiﬁable. For example, given X ∼ ECd (µ, Σ, ξ ) with
rank(Σ) = q , there will be multiple As corresponding to the same Σ. i.e., there exist A1 6= A2 ∈

2

(3)

(4)

2 = Σ. For some constant c 6= 0, we deﬁne ξ ∗ = ξ/c and A∗ = c · A,
Rd×q such that A1AT
1 = A2AT
then ξAU = ξ ∗A∗U . Therefore, the matrix Σ is unique only up to a constant scaling. To make the
model identiﬁable, we impose the condition that max{diag(Σ)} = 1. More discussions about the
identiﬁability issue can be found in [12].
3 Transelliptical Graphical Models
In this paper we only consider distributions with continuous marginals. We introduce the transellip-
tical graphical models in analogy to the nonparanormal graphical models [19, 18]. The key concept
is transelliptical distribution which is also introduced in [12]. However, the deﬁnition of transellip-
tical distribution in this paper is slightly more restrictive than that in [12] due to the complication of
graphical modeling. More speciﬁcally, let
R+
d := {Σ ∈ Rd×d : ΣT = Σ, diag(Σ) = 1, Σ (cid:31) 0},
we deﬁne the transelliptical distribution as follows:
Deﬁnition 3.1 (transelliptical distribution). A continuous random vector X = (X1 , . . . , Xd )T is
transelliptical, denoted by X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ), if there exists a set of monotone univariate
functions f1 , . . . , fd and a nonnegative random variable ξ satisfying P(ξ = 0) = 0, such that
(f1 (X1 ), . . . , fd (Xd ))T ∼ ECd (0, Σ, ξ ), where Σ ∈ R+
d .
Here, Σ is called latent generalized correlation matrix1 .
We then discuss the relationship between the transelliptical family with the nonparanormal family,
which is deﬁned as follows:
Deﬁnition 3.2 (nonparanormal distribution). A ramdom vector X = (X1 , . . . , Xd )T is nonpara-
normal, denoted by X ∼ N P Nd (Σ; f1 , . . . , fd ), if there exist monotone functions f1 , . . . , fd such
that (f1 (X1 ), . . . , fd (Xd ))T ∼ Nd (0, Σ), where Σ ∈ R+
d is called latent correlation matrix.
From Deﬁnitions 3.1 and 3.2, we see the transelliptical is a strict extension of the nonparanormal.
Both families assume there exits a set of univariate transformations such that the transformed data
follow a base distribution: the nonparanormal exploits a normal base distribution; while the transel-
liptical exploits an elliptical base distribution. In the nonparanormal, Σ is the correlation matrix for
the latent normal, therefore it is called latent correlation matrix; In the transelliptical, Σ is the gener-
alized correlation matrix for the latent elliptical distribution, therefore it is called latent generalized
correlation matrix.
We now deﬁne the transelliptical graphical models. Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ) where Σ ∈ R+
d
is the latent generalized correlation matrix. In this paper, we always assume the second moment
matrix Γ as Γjk := −Θjk /pΘj j · Θkk . Let diag(A) be the matrix A with off-diagonal elements
Eξ 2 < ∞. We deﬁne Θ := Σ−1 to be the latent generalized concentration matrix. Let Θjk be the
element of Θ on the j -th row and k-th column. We deﬁne the latent generalized partial correlation
replaced by zero and A1/2 be the squared root matrix of A. It is easy to see that
Γ = −[diag(Σ−1 )]−1/2Σ−1 [diag(Σ−1 )]−1/2 .
(5)
Therefore, Γ has the same nonzero pattern as Σ−1 . We then deﬁne a undirected graph G = (V , E ):
the vertex set V contains nodes corresponding to the d variables in X , and the edge set E satisﬁes
(Xj , Xk ) ∈ E if and only if Γjk 6= 0 for j, k = 1, . . . , d.
(6)
Given a graph G, we deﬁne R+
d (G) to be the set containing all the Σ ∈ R+
d with zero entries at the
positions speciﬁed by the graph G. The transelliptical graphical model induced by G is deﬁned as:
P (G) := (cid:8)all the transelliptical distributions T Ed (Σ, ξ ; f1 , . . . , fd ) satisfying Σ ∈ R+
d (G)(cid:9) .
Deﬁnition 3.3 (transelliptical graphical model). The transelliptical graphical model induced by a
graph G, denoted by P (G), is deﬁned to be the set of distributions:
(7)
In the rest of this section, we prove some properties of the transelliptical family and discuss the inter-
pretation of the meaning of the graph G. This graph is called latent generalized partial correlation
graph. First, we show the transelliptical family is closed under marginalization and conditioning.
1One thing to note is that in [12], the condition that Σ ∈ Rd+ is not required.

3

Lemma 3.1. Let X := (X1 , . . . , Xd )T ∼ T Ed (Σ, ξ ; f1 , . . . , fd ). The marginal and the conditional
distributions of (X1 , X2 )T given the remaining variables are still transellpitical.
Proof. Since X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ), we have (f1 (X1 ), . . . , fd (Xd ))T ∼ ECd (0, Σ, ξ ). Let
Zj := fj (Xj ) for j = 1, . . . , d. From Theorem 2.18 of [8], the marginal distribution of (Z1 , Z2 )T
and the conditional distribution of (Z1 , Z2 )T given the remaining Z3 , . . . , Zd are both elliptical. By
deﬁnition, the marginal distribution of (X1 , X2 )T is transelliptical. To see the conditional case, since
X has continuous marginals and f1 , . . . , fd are monotone, the distribution of (X1 , X2 )T conditional
on X\{1,2} is the same as conditional on Z\{1,2} . Combined with the fact that Z1 = f1 (X1 ),
Z2 = f2 (X2 ), we know that (X1 , X2 )T | X\{1,2} follows a transelliptical distribution.

From (5), we see the matrices Γ and Θ have the same nonzero pattern, therefore, they encode
the same graph G. Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ). The next lemma shows that, if the second
moment of X exists, the absence of an edge in the graph G is equivalent to the pairwise conditional
uncorrelatedness of two corresponding latent variables.
Lemma 3.2. Let X := (X1 , . . . , Xd )T ∼ T Ed (Σ, ξ ; f1 , . . . , fd ) with Eξ 2 < ∞, and Zj := fj (Xj )
for j = 1, . . . , d. Γjk = 0 if and only if Zj and Zk are conditionally uncorrelated given Z\{j,k} .
Proof. Let Z := (Z1 , . . . , Zd )T . Since X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ), we have Z ∼ ECd (0, Σ, ξ ).
Therefore, the latent generalized correlation matrix Σ is the generalized correlation matrix of the
latent variable Z . It sufﬁces to prove that, for elliptical distributions with Eξ 2 < ∞, the generalized
partial correlation matrix Γ as deﬁned in (5) encodes the conditional uncorrelatedness among the
variables. Such a result has been proved in the section 2 of [26].
Let A, B , C ⊂ {1, . . . , d}. We say C separates A and B in the graph G if any path from a node
in A to a node in B goes through at least one node in C . We denote by XA the subvector of X
indexed by A. The next lemma implies the equivalence between the pairwise and global conditional
uncorrelatedness of the latent variables for the transelliptical graphical models. This lemma connects
the graph theory with probability theory.
Lemma 3.3. Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ) be any element of the transelliptical graphical model
P (G) satisfying Eξ 2 < ∞. Let Z := (Z1 , . . . , Zd )T with Zj = fj (Xj ) and A, B , C ⊂ {1, . . . , d}.
Then C separates A and B in G if and only if ZA and ZB are conditional uncorrelated given ZC .
Proof. By deﬁnition, we know Z ∼ ECd (0, Σ, ξ ). It then sufﬁces to show the pairwise conditional
uncorrelatedness implies the global conditional uncorrelatedness for the elliptical family. This fol-
lows from the same induction argument as in Theorem 3.7 of [16].

Compared with the nonparanormal graphical model, the transelliptical graphical model gains a lot
on modeling ﬂexibility, but at the price of inferring a weaker notion of graphs: a missing edge in
the graph only represents the conditional uncorrelatedness of the latent variables. The next lemma
shows that we do not lose any thing compared with the nonparanormal graphical model. The proof
of this lemma is simple and is omitted. Some related discussions can be found in [19].
Lemma 3.4. Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ) be a member of the transelliptical graphical model
P (G). If X is also nonparanormal, the graph G encodes the conditional independence relationship
of X (In other words, the distribution of X is Markov to G).

4 Rank-based Regularization Estimator
In this section, we propose a nonparametric rank-based regularization estimator which achieves the
optimal parametric rates of convergence for both graph recovery and parameter estimation. The
main idea of our procedure is to treat the marginal transformation functions fj and the generating
variable ξ as nuisance parameters, and exploit the nonparametric Kendall’s tau statistic to directly
estimate the latent generalized correlation matrix Σ. The obtained correlation matrix estimate is then
plugged into the CLIME procedure to estimate the sparse latent generalized concentration matrix Θ.
then get a graph estimator by thresholding the estimated bΘ.
From the previous discussion, we know the graph G is encoded by the nonzero pattern of Θ. We
4

4.1 The Kendall’s tau Statistic and its Invariance Property
Let x1 , . . . , xn ∈ Rd be n observations of a random vector X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ). Our task is
(cid:17)
(cid:17) (cid:16)
(cid:16)
X
to estimate the latent generalized concentration matrix Θ := Σ−1 . The Kendall’s tau is deﬁned as:
bτjk =
2
k − xi0
j − xi0
sign
(8)
xi
xi
n(n − 1)
,
k
j
1≤i<i0≤n
random variables Xj and Xk . Let eXj and eXk be two independent copies of Xj and Xk . The
population version of the Kendall’s tau statistic is τjk := Corr(cid:0)sign(Xj − eXj ), sign(Xk − eXk )(cid:1).
which is a monotone transformation-invariant correlation between the empirical realizations of two
Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ), the following theorem from [12] illustrates an important rela-
tionship between the population Kendall’s tau statistic τjk and the latent generalized correlation
coefﬁcient Σjk .
Xk . Then Σjk = sin (cid:0) π
(cid:1).
Theorem 4.1 (Invariance Property of Kendall’s tau Statistic[12]). Let X := (X1 , . . . , Xd )T ∼
T Ed (Σ, ξ ; f1 , . . . , fd ). We denote τjk to be the population Kendall’s tau statistic between Xj and
2 τjk
4.2 Rank-based Regularization Method
matrix. Given a matrix A, we deﬁne kAkmax := maxjk |Ajk | and kAk1 := P
We start with some notations. We denote by I (·) to be the indicator function and Id be the identity
jk |Ajk |.
Motivated by Theorem 4.1, we deﬁne bS = [ bSjk ] ∈ Rd×d to estimate Σ:
(cid:16) π
(cid:17) · I (j 6= k) + I (j = k).
bSjk = sin
2 bτjk
(9)
We then plug bS into the CLIME estimator [2] to get the ﬁnal parameter and graph estimates. More
X
bΘ = arg min
|Θjk | s.t. k bSΘ − Idkmax ≤ λ,
speciﬁcally, the latent generalized concentration matrix Θ can be estimated by solving
(10)
Θ
j,k
where λ > 0 is a tuning parameter.
has the potential to scale to very large problems. Once bΘ is obtained, we can apply an additional
[2] show that this optimization can be decomposed into d
vector minimization problems, each of which can be reformulated as a linear program. Thus it
thresholding step to estimate the graph G. For this, we deﬁne a graph estimator bG = (V , bE ), in
which an edge (j, k) ∈ bE if bΘjk ≥ γ . Here γ is another tuning parameter.
bS , which requires us to evaluate d(d − 1)/2 pairwise Kendal’s tau statistics. A naive implementation
Compared with the original CLIME, the extra cost of our rank-based procedure is the computation of
of the Kendall’s tau requires O(n2 ) computation. However, efﬁcient algorithm based on sorting and
balanced binary trees has been developed to calculate the Kendall’s tau statistic with a computational
complexity O(n log n) [4]. Therefore, the incurred computational burden is negligible.
Remark 4.1. Similar rank-based procedures have been discussed in [19, 18, 28]. Unlike our work,
they focus on the more restrictive nonparanromal family and discuss several rank-based procedures
using the normal-score, Spearman’s rho, and Kendall’s tau. Unlike our results, they advocate the
use of the Spearman’s rho and normal-score correlation coefﬁcients. Their main concern is that,
within the more restrictive nonparanormal family, the Spearman’s rho and normal-score correlations
are slightly easier to compute and have smaller asymptotic variance. In constrast to their results,
the new insight obtained from this current paper is that we advocate the usage of the Kendall’s tau
due to its invariance property within the much larger transelliptical family. In fact, we can show that
the Spearman’s rho is not invariant within the transelliptical family unless the true distribution is
nonparanormal. More details on this issue can be found in [8].
5 Asymptotic Properties
We analyze the theoretical properties of the rank-based regularization estimator proposed in Section
4.2. Our main result shows: under the same conditions on Σ that ensure the parameter estimation

5

and graph recovery consistency of the original CLIME estimator for Gaussian graphical models, our
rank-based regularization procedure achieves exactly the same parametric rates of convergence for
both parameter estimation and graph recovery for the much larger transelliptical family. This result
suggests that the transelliptical graphical model can be used as a safe replacement of the Gaussian
P
graphical models, the nonparanormal graphical models, and the elliptical graphical models.
We introduce some additional notations. Given a symmetric matrix A, for 0 ≤ q < 1, we deﬁne
j |Aij |q and the spectral norm kAkL2 to be its largest eigenvalue. We deﬁne
kAkLq := maxi
Sd (q , s, M ) := {Θ : kΘkL1 ≤ M and kΘkLq ≤ s}.
(11)
For q = 0, the class Sd (0, s, M ) contains all the s-sparse matrices. Our main result is Theorem 5.1
0 ≤ q < 1. Let bΘ be deﬁned in (10). There exist constants C0 and C1 only depending on q , such
Theorem 5.1. Let X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ) with Σ ∈ R+
d and Θ := Σ−1 ∈ Sd (q , S, M ) with
that, whenever λ = C0M p(log d)/n, with probability no less than 1 − d−2 , we have
(cid:18) log d
(cid:19)(1−q)/2
k bΘ − ΘkL2 ≤ C1M 2−2q · s ·
(Parameter estimation)
(12)
.
Let bG be the graph estimator deﬁned in Section 4.2 with the additional tuning parameter γ = 4M λ.
n
P (cid:16) bG 6= G
(cid:17) ≥ 1 − o(1),
If we further assume Θ ∈ Sd (0, s, M ) and minj,k:|Θjk |6=0 |Θjk | ≥ 2γ , then
(Graph recovery)
(13)
where G is the graph determined by the nonzero pattern of Θ.
the Pearson correlation coefﬁcient matrix bR by the Kendall’s tau matrix bS . By examing the proofs
Proof. The difference between the rank-based CLIME and the original CLIME is that we replace
of Theorems 1 and 7 in [2], the only property needed of bR is an exponential concentration inequality
P (cid:16)| bRjk − Σjk | > t
(cid:17) ≤ c1 exp(−c2nt2 )
. Therefore, it sufﬁces if we can prove a similar concentration inequality for | bSjk − Σjk |. Since
(cid:16) π
(cid:17)
(cid:16) π
(cid:17)
bS = sin
2 bτjk
and Σjk = sin
we have | bSjk − Σjk | ≤ |bτjk − τ |. Therefore, we only need to prove
2 τjk
,
P (|bτjk − τjk | > t) ≤ exp (cid:0)−nt2 /(2π)(cid:1) .
P
This result holds since bτjk is a U-statistic: bτjk =
Kτ (xi , xi0 ) = sign(cid:0)xi
(cid:1)(cid:0)xi
(cid:1) is a bounded kernel between -1 and 1. The result fol-
1≤i<i0≤n Kτ (xi , xi0 ), where
2
n(n−1)
k − xi0
j − xi0
j
k
lows from the Hoeffding’s inequality for U-statistic [13].
6 Numerical Experiments
We investigate the empirical performance of the rank-based regularization estimator. We compare
it with the following methods: (1) Pearson: the CLIME using the Pearson sample correlation; (2)
Kendall: the CLIME using the Kendall’s tau; (3) Spearman: the CLIME using the Spearman’s
rho; (4) NPN: the CLIME using the original nonparanormal correlation estimator [19]; (5) NS:
the CLIME using the normal score correlation. The later three methods are discussed under the
nonparanormal graphical model and we refer to [18] for detailed descriptions.
6.1 Simulation Studies
We adopt the same data generating procedure as in [18]. To generate a d-dimensional sparse graph
G = (V , E ) where V = {1, . . . , d} correspond to variables X = (X1 , . . . , Xd ), we associate each
index j ∈ {1, . . . , d} with a bivariate data point (Y (1)
) ∈ [0, 1]2 where Y (k)
n ∼
, Y (2)
, . . . , Y (k)
1
j
j
Uniform[0, 1] for k = 1, 2. Each pair of vertices (i, j ) is included in the edge set E with probability
√
P((i, j ) ∈ E ) = exp(−kyi −yj k2
, y (2)
2π , where yi := (y (1)
) is the empirical observation
n /0.25)/
i
i
) and k · kn represents the Euclidean distance. We restrict the maximum degree of the
, Y (2)
of (Y (1)
i
i

6

Scheme 1

Scheme 2

Scheme 3

Scheme 4

Figure 1: ROC curves for different methods in generating schemes 1 to 4 and different contamination
level r = 0, 0.02, 0.05 (top, middle, bottom) using the CLIME. Here n = 400 and d = 100.

graph to be 4 and build the inverse correlation matrix Ω according to Ωjk = 1 if j = k , Ωjk = 0.145
if (j, k) ∈ E , and Ωjk = 0 otherwise. The value 0.145 guarantees the positive deﬁniteness of Ω.
Let Σ = Ω−1 . To obtain the correlation matrix, we rescale Σ so that all its diagonal elements are 1.
In the simulated study we randomly sample n data points from a certain transelliptical distribution
X ∼ T Ed (Σ, ξ ; f1 , . . . , fd ). We set d = 100. To determine the transelliptical distribution, we ﬁrst
generate Σ as discussed in the previous paragraph. Secondly, three types of ξ are considered:
(1) ξ (1) ∼ χd , i.e., ξ follows a chi-distribution with degree of freedom d;
2 ∼ χ1 , ξ ∗
1 ∼ χd , ξ ∗
(2) ξ (2) d= ξ ∗
1 is independent of ξ ∗
2 , ξ ∗
1 /ξ ∗
2 ;
(3) ξ (3) ∼ F (d, 1), i.e., ξ follows an F -distribution with degree of freedom d and 1.
Thirdly, two type of transformation functions f = {fj }d
j=1 are considered:
(1) linear transformation: f (1) = {f0 , . . . , f0 } with f0 (x) = x;
(2) nonlinear transformation: f (2) = {f1 , . . . , fd} = {h1 , h2 , h3 , h4 , h5 , h1 , h2 , h3 , h4 , h5 , . . .},
√R |t|φ(t)dt
x3√R t6 φ(t)dt
:= sign(x)|x|1/2
, h−1
, h−1
where h−1
:= x, h−1
Φ(x)−R Φ(t)φ(t)dt
exp(x)−R exp(t)φ(t)dt
3 (x)
:=
4 (x)
:=
1 (x)
2 (x)
√R (Φ(y)−R Φ(t)φ(t)dt)2 φ(y)dy
√R (exp(y)−R exp(t)φ(t)dt)2 φ(y)dy
, h−1
5 (x) :=
.
We consider the following four data generating schemes:
• Scheme 1: X ∼ T Ed (Σ, ξ (1) ; f (1) ), i.e., X ∼ N (0, Σ).
• Scheme 2: X ∼ T Ed (Σ, ξ (2) ; f (1) ), i.e., X follows the multivariate Cauchy.
• Scheme 3: X ∼ T Ed (Σ, ξ (3) ; f (1) ), i.e., the distribution is highly related to the multivariate t.
• Scheme 4: X ∼ T Ed (Σ, ξ (3) ; f (2) ).
To evaluate the robustness of different methods, let r ∈ [0, 1) represent the proportion of samples
being contaminated. For each dimension, we randomly select bnrc entries and replace them with

7

0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNS0.00.20.40.60.81.00.00.20.40.60.81.0FPRTPRPearsonKendallSpearmanNPNNSeither 5 or -5 with equal probability. The ﬁnal data matrix we obtained is X ∈ Rn×d . Here we
pick r = 0, 0.02 or 0.05. Under the Scheme 1 to Scheme 4 with different levels of contamination
(r = 0, 0.02 or 0.05), we repeatedly generate the data matrix X for 100 times and compute the
averaged False Positive Rates and False Negative Rates using a path of tuning parameters λ from
0.01 to 0.5 and γ = 10−5 . The feature selection performances of different methods are evaluated
by plotting (FPR(λ), 1 − FNR(λ)). The corresponding ROC curves are presented in Figure 1. We
see: (1) when the data are perfectly Gaussian without contamination, all methods perform well; (2)
when data are non-Gaussian, with outliers existing or latent elliptical distribution different from the
Gaussian, Kendall is better than the other methods in terms of achieving a lower FPR + FNR.
6.2 Equities Data
We compare different methods on the stock price data from Yahoo! Finance (finance.yahoo.
com). We collect the daily closing prices for 452 stocks that are consistently in the S&P 500 index
between January 1, 2003 through January 1, 2008. This gives us altogether 1,257 data points, each
data point corresponding to the vector of closing prices on a trading day. With St,j denoting the
closing price of stock j on day t, we consider the variables Xtj = log (St,j /St−1,j ) and build
graphs over the indices j . Though a time series, we treat the instances Xt as independent replicates.

Figure 2: The graph estimated from the S&P 500 stock data from Jan. 1, 2003 to Jan. 1, 2008 using Pearson,
Kendall,Spearman, NPN, NS (left to right). The nodes are colored according to their GICS sector categories.
The 452 stocks are categorized into 10 Global Industry Classiﬁcation Standard (GICS) sec-
tors, including Consumer Discretionary (70 stocks), Consumer Staples (35 stocks),
Energy (37 stocks), Financials (74 stocks), Health Care (46 stocks), Industrials
(59 stocks), Information Technology (64 stocks) Telecommunications Services
(6 stocks), , Materials (29 stocks), and Utilities (32 stocks).
Figure 2 illustrates the estimated graphs using the same layout, the nodes are colored according to
the GICS sector of the corresponding stock. The tuning parameter is automatically selected using
a stability based approach named StARS [20]. We see that different methods get slightly different
graphs. The layout is drawn by a force-based algorithm using the estimated graph from the Kendall.
We see the stocks from the same GICS sector tends to be grouped with each other, suggesting that
our method delivers an informative graph estimate.
7 Discussion and Comparison with Related Work
The transelliptical distribution is also proposed by [12] for semiparametric scale-invariant principle
component analysis. Though both papers are based on the transelliptical family, the core idea and
analysis are fundamentally different. For scale-invariant principle component analysis, we impose
structural assumption of the latent generalized correlation matrix; For graph estimation, we impose
structural assumption on the latent generalized concentration matrix. Since the latent generalized
correlation matrix encodes marginal uncorrelatedness while the latent generalized concentration ma-
trix encodes conditional uncorrelatedness of the variables, the analysis of the population models are
orthogonal and complementary to each other. In particular, for graphical models, we need to charac-
terize the properties of marginal and conditional distributions of a transelliptical distribution. These
properties are not needed for principle component analysis. Moreover, the model interpretation
of the inferred transelliptical graph is very nontrivial.
In a longer version technical report [17],
we provide a three-layer hierarchal interpretation of the estimated transelliptical graphical model
and sharply characterize the relationships between nonparnaormal, elliptical, meta-elliptical, and
transelliptical families. This research was supported by NSF award IIS-1116730.

8

PearsonKendallSpearmanNPNNSReferences
[1] O. Banerjee, L. E. Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation. Journal of Machine Learning Research, 9(3):485–516, 2008.
[2] T. Cai, W. Liu, and X. Luo. A constrained ‘1 minimization approach to sparse precision matrix estimation.
Journal of the American Statistical Association, 106(494):594–607, 2011.
[3] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on
Scientiﬁc Computing, 20(1):33–61, 1998.
[4] David Christensen. Fast algorithms for the calculation of Kendall’s τ . Computational Statistics, 20(1):51–
62, 2005.
[5] A. Dempster. Covariance selection. Biometrics, 28:157–175, 1972.
[6] M. Drton and M. Perlman. Multiple testing and error control in Gaussian graphical model selection.
Statistical Science, 22(3):430–449, 2007.
[7] M. Drton and M. Perlman. A SINful approach to Gaussian graphical model selection. Journal of Statis-
tical Planning and Inference, 138(4):1179–1200, 2008.
[8] KT Fang, S. Kotz, and KW Ng. Symmetric multivariate and related distributions. Chapman&Hall,
London, 1990.
[9] Michael A. Finegold and Mathias Drton. Robust graphical modeling with t-distributions. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI ’09, pages 169–176, 2009.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9(3):432–441, 2008.
[11] P.R. Halmos. Measure theory, volume 18. Springer, 1974.
[12] F. Han and H. Liu. Tca: Transelliptical principal component analysis for high dimensional non-gaussian
data. Technical Report, 2012.
[13] Wassily Hoeffding. Probability Inequalities for Sums of Bounded Random Variables. Journal of the
American Statistical Association, 58(301):13–30, 1963.
[14] A. Jalali, C. Johnson, and P. Ravikumar. High-dimensional sparse inverse covariance estimation using
greedy methods. International Conference on Artiﬁcial Intelligence and Statistics, 2012. to appear.
[15] C. Lam and J. Fan. Sparsistency and rates of convergence in large covariance matrix estimation. Annals
of Statistics, 37:42–54, 2009.
[16] Steffen L. Lauritzen. Graphical Models. Oxford University Press, 1996.
[17] H. Liu, F. Han, and Zhang C-H. Transelliptical graphical modeling under a hierarchical latent variable
framework. Technical Report, 2012.
[18] H. Liu, F. Han, M. Yuan, J. Lafferty, and L. Wasserman. High dimensional semiparametric gaussian
copula graphical models. Annals of Statistics, 2012.
[19] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-
sional undirected graphs. Journal of Machine Learning Research, 10:2295–2328, 2009.
[20] Han Liu, Kathryn Roeder, and Larry Wasserman. Stability approach to regularization selection (stars) for
high dimensional graphical models. In Proceedings of the Twenty-Third Annual Conference on Neural
Information Processing Systems (NIPS), 2010.
[21] N. Meinshausen and P. B ¨uhlmann. High dimensional graphs and variable selection with the lasso. Annals
of Statistics, 34(3):1436–1462, 2006.
[22] P. Ravikumar, M. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by mini-
mizing ‘1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935–980, 2011.
[23] A. Rothman, P. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Elec-
tronic Journal of Statistics, 2:494–515, 2008.
[24] X. Shen, W. Pan, and Y. Zhu. ). likelihood-based selection and sharp parameter estimation. Journal of the
American Statistical Association, 2012. to appear.
[25] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
Series B, 58(1):267–288, 1996.
[26] D. Vogel and R. Fried. Elliptical graphical modelling. Biometrika, 98(4):935–951, December 2011.
[27] M. Wainwright. Sharp thresholds for highdimensional and noisy sparsity recovery using ‘1 constrained
quadratic programming. IEEE Transactions on Information Theory, 55(5):2183–2201, 2009.
[28] L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal graphical
models. Annals of Statistics, 2012.
[29] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. Journal of
Machine Learning Research, 11(8):2261–2286, 2010.
[30] M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika,
94(1):19–35, 2007.
[31] P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research,
7(11):2541–2563, 2006.
[32] T. Zhao, H. Liu, K. Roeder, J. Lafferty, and L. Wasserman. The huge package for high-dimensional
undirected graph estimation in r. Journal of Machine Learning Research, 2012. to appear.
[33] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476):1418–1429, 2006.

9


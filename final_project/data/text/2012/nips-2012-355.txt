Entropy Estimations Using Correlated Symmetric
Stable Random Projections

Ping Li
Department of Statistical Science
Cornell University
Ithaca, NY 14853
pingli@cornell.edu

Cun-Hui Zhang
Department of Statistics and Biostatistics
Rutgers University
New Brunswick, NJ 08901
czhang@stat.rutgers.edu

Abstract
Methods for efﬁciently estimating Shannon entropy of data streams have impor-
tant applications in learning, data mining, and network anomaly detections (e.g.,
the DDoS attacks). For nonnegative data streams, the method of Compressed
Counting (CC) [11, 13] based on maximally-skewed stable random projections
can provide accurate estimates of the Shannon entropy using small storage. How-
ever, CC is no longer applicable when entries of data streams can be below zero,
which is a common scenario when comparing two streams.
In this paper, we
propose an algorithm for entropy estimation in general data streams which allow
negative entries. In our method, the Shannon entropy is approximated by the ﬁ-
nite difference of two correlated frequency moments estimated from correlated
samples of symmetric stable random variables. Interestingly, the estimator for the
moment we recommend for entropy estimation barely has bounded variance itself,
whereas the common geometric mean estimator (which has bounded higher-order
moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that
this method is able to well approximate the Shannon entropy using small storage.
1 Introduction
Computing the Shannon entropy in massive data have important applications in neural compu-
tation [17], graph estimation [5], query logs analysis in Web search [14], network anomaly de-
tection [21], etc. (See NIPS2003 workshop on entropy estimation www.menem.com/˜ilya/
pages/NIPS03). In modern applications, as massive datasets are often generated in a streaming
fashion, entropy estimation in data streams has become a challenging and interesting problem.
1.1 Data Streams
Massive data generated in a streaming fashion are difﬁcult to transmit and store [15], as the pro-
cessing is often done on the ﬂy in one-pass of the data. The problem of “scaling up for high
dimensional data and high speed data streams” is among the “ten challenging problems in data min-
ing research” [20]. Mining data streams at petabyte scale has become an important research area [1],
as network data can easily reach that scale [20].
In the standard turnstile model [15], a data stream is a vector At of length D , where D = 264 or
even D = 2128 is possible in network applications, e.g., (a pair of) IP addresses + port numbers. At
time t, there is an input stream at = (it , It ), it ∈ [1, D] which updates At by a linear rule:
At [it ] = At−1 [it ] + It .
(1)
where It is the increment/decrement of package size at t. For network trafﬁc, normally At [i] ≥ 0,
which is called the strict turnstile model and sufﬁces for describing certain natural phenomena. On
the other hand, the general turnstile model (which allows At [i] < 0) is often used for comparing
two streams, e.g., in network OD (origin-destination) ﬂow analysis [21].
D(cid:88)
H = − D(cid:88)
An important task is to compute the α-th frequency moment F(α) and the Shannon entropy H :
|At [i]|
|At [i]|
|At [i]|α ,
F1
F1
i=1
i=1
The exact computation of these summary statistics is not feasible because to do so one has to store
the entire vector At of length D , as the entries are time-varying. Also, many applications (such as
anomaly detections of network trafﬁc) require computing the summary statistics in real-time.

F(α) =

(2)

log

,

1

Figure 1: This plot is reproduced from
a DARPA conference [4]. One can view
x-axis as the surrogate for time. Y-axis
is the measured Shannon entropy, which
exhibited a sudden sharp change at the
time when an attack occurred.

1.2 Network Measurement, Monitoring, and Anomaly Detection
Network trafﬁc is a typical example of high-rate data streams. Industries are now prepared to move
to 100 Gbits/second or Terabit/second Ethernet. An effective and reliable measurement of network
trafﬁc in real-time is crucial for anomaly detection and network diagnosis; and one such measure-
ment metric is the Shannon entropy [4, 8, 19, 2, 9, 21]. The exact entropy measurement in real-time
on high-speed links is however computationally prohibitive.
The Distributed Denial of Service (DDoS) attack is a rep-
resentative example of network anomalies. A DDoS at-
tack attempts to make computers unavailable to intended
users, either by forcing users to reset the computers or
by exhausting the resources of service-hosting sites. For
example, hackers may maliciously saturate the victim
machines by sending many external communication re-
quests. DDoS attacks typically target sites such as banks,
credit card payment gateways, or military sites. A DDoS
attack normally changes the statistical distribution of net-
work trafﬁc, which could be reliably captured by the ab-
normal variations in the measurements of Shannon en-
tropy [4]. See Figure 1 for an illustration.
Apparently, the entropy measurements do not have to be
“perfect” for detecting attacks. It is however crucial that
the algorithms should be computationally efﬁcient (i.e., real-time and one-pass) at low memory cost,
because the trafﬁc data generating by large high-speed networks are enormous and transient.
1.3 Symmetric Stable Random Projections and Entropy Estimation Using Moments
It turns out that, for 0 < α ≤ 2, one can use stable random projections to compute F(α) efﬁciently
because the Turnstile model (1) is a linear model and the random projection operation is also linear
(i.e., vector-matrix multiplication) [7]. Conceptually, we multiply the data stream vector At ∈ RD
D(cid:88)
by a random matrix R ∈ RD×k , resulting in a vector X = At × R ∈ Rk with entries
xj = [At × R]j =
rij At [i], j = 1, 2, ..., k
i=1
where rij ∼ S (α, 1) is a symmetric α-stable random variable with unit scale [3, 22]: E (erij t ) =
e−|t|α . The standard normal (or Cauchy) distribution is a special case with α = 2 (or α = 1).
In data stream computations, the matrix R is not materialized. The standard procedure is to
(re)generate entries of R on-demand [7] using pseudo-random numbers [16]. Thus, we only need
to store X ∈ Rk . When a stream element at = (it , It ) arrives, one updates the entries of X :
xj ← xj + It rit j ,
j = 1, 2, ..., k .
(cid:33)
(cid:195)
D(cid:88)
D(cid:88)
By property of stable distributions, the samples xj , j = 1 to k , are also i.i.d. stable
i=1
i=1
Therefore, the task boils down to estimating the scale parameter from k i.i.d. stable samples.
(cid:195)
(cid:33)
Because the Shannon entropy is essentially the derivative of the frequency moment at α = 1, the
popular approach is to approximate the Shannon entropy by the Tsallis entropy [18]:
1
1 − F(α)
Tα =
α − 1
F α
(1)
which approaches the Shannon entropy H as α → 1.
[21] used a slight variant of (5) but the
difference is not essential.1
In their approach, F(α) and F(1) are ﬁrst estimated separately from
1 [21] used F(1+∆)−F(1−∆)
and estimated the two frequency moments independently. The subtle difference
2∆
between the ﬁnite difference approximations is not essential. It is the correlation that plays the crucial role.

|At [i]|α

rij At [i] ∼ S

α, F(α) =

xj =

(3)

(4)

(5)

.

2

20040060080010001200012345678910packet counts (thousands)source IP address: entropy valuetwo independent sets of samples. The estimated moments are then plugged in (5) to estimate the
Shannon entropy H . Immediately, we can see the problem here: the variance of the estimated T(α)
(α−1)2 = 1
(Recall var(cX ) = c2 var(X )).
might be proportional to
∆2 .
1
(cid:161)
(cid:162)
One question is how to choose α (i.e., ∆). [6] proposed a conservative criterion by choosing α
according to the worst case bias |H − Tα |. One can verify that ∆ = |1 − α| < 10−7 is likely in
1014
[6]. In other words, the required sample size could be O
. In practice, [21] exploited the bias-
variance tradeoff but they still had to use an excessive number of samples, e.g., 106 . In comparison,
using our proposed approach, it appears that 100 ∼ 1000 samples might be sufﬁcient.
1.4 Our Proposal
We have made two key contributions. Firstly, instead of estimating F(α) and F(1) separately using
(cid:182)
(cid:181)
two independent sets of samples, we make them highly positively correlated. Intuitively, if the two
∆2 (cid:162)
(cid:161)
consistent estimators, denoted by ˆF(α) and ˆF(1) respectively, are highly positively correlated, then
(cid:181)
(cid:182)
possibly their ratio ˆF(α)
ˆF(α)
can be close to 1 with small variance. Ideally, if V ar
, the
= O
ˆF α
ˆF α
(1)
(1)
(cid:182)
(cid:181)
1 − ˆF(α)
∆2 (cid:162)
(cid:161)
will be essentially independent of ∆.
variance of the estimated Tsallis entropy ˆTα = 1
α−1
ˆF α
(1)
ˆF(α)
It turns out that ﬁnding an estimator with V ar
was not straightforward. It is known
= O
ˆF α
(cid:181)
(cid:182)
(1)
that around α = 1, the geometric mean estimator [10] is nearly statistically optimal. Interestingly,
our analysis and simulation show that using the geometric mean estimator, we can essentially only
(cid:161)
(cid:162)
ˆF(α)
achieve V ar
= O (∆), which, albeit a large improvement, is not small sufﬁcient to cancel
ˆF α
(1)
the O
term. Therefore, our second key component is a new estimator of Tα using a moment
1
(cid:162)
(cid:161)
∆2
estimator which does not have (or barely has) ﬁnite variance. Even though such an estimator is not
good for estimating the single moment compared to the geometric mean, due to the high correlation,
the ratio ˆF(α)
∆2
, as shown in our
is still very well-behaved and its variance is essentially O
ˆF α
(1)
theoretical analysis and experiments.
1.5 Compressed Counting (CC) for Nonnegative Data Streams
The recent work [13] on Compressed Counting (CC) [11] provides an ideal solution to the problem
of entropy estimation in nonnegative data streams. Basically, for nonnegative data streams, i.e.,
t(cid:88)
D(cid:88)
D(cid:88)
At [i] ≥ 0 at all times and all locations, we can compute the ﬁrst moment easily, because
s=0
i=1
i=1
where Is is the increment/decrement at time s. In other words, we just need a single counter to
(cid:161)
(cid:162)
accumulate all the increments Is . This observation lead to the conjecture that estimating F(α) should
be also easy if α ≈ 1, which consequently lead to the development of Compressed Counting which
used maximally-skewed stable random projections instead of symmetric stable projections. The
most recent work of CC [13] provided a new moment estimator to achieve the variance ∝ O
∆2
.
Unfortunately, for general data streams where entries can be negative, we have to resort to sym-
metric stable random projections. Fundamentally, the reason that skewed projections work well on
nonnegative data streams is because the data themselves are skewed. However, when we compare
two streams, the data become more or less symmetric and hence we must use symmetric projections.
1.6 Why Comparing the Difference of Two Streams?
In machine learning research and practice, people routinely use the difference between feature vec-
tors. [21] used the difference between data streams from a slightly different motivation.
The goal of [21] is to measure the entropies of all OD pairs (origin-destination) in a network, because
entropy measurements are crucial for detecting anomaly events such as DDoS attacks and network
failures. They argued that the change of entropy of the trafﬁc distribution may be invisible (i.e., too
small to be detected) in the traditional volume matrix even during the time when an attack occurs.
Instead, they proposed to measure the entropy from a number of locations across the network, i.e.,

|At [i]| =

F(1) =

At [i] =

Is

(6)

3

by examining the entropy of every OD ﬂow in the network. In a similar argument, a DDoS attack
may be invisible in terms of the trafﬁc volume change, if the attack is launched outside the network.
(cid:161)
(cid:162)
While [21] successfully demonstrated that measuring the Shannon entropy of OD ﬂows is effective
for detecting anomaly events, at that time they did not have the tools for efﬁciently estimating the
entropy. Using symmetric stable random projections and independent samples, they needed a large
number of samples (e.g., 106 ) because their variance blows up at the rate of O
.
1
∆2
For anomaly detection, reducing the sample size (k) is crucial because k determines the storage
and estimation speed; and it is often required to detect the events at real time.
In addition, the
pseudo-random numbers have to be (re)-generated on the ﬂy, at a cost proportional to k .

2 Our Proposed Algorithm
Recall that a data stream is a long vector At [i], i = 1 to D . At time t, an incoming element
at = (it , It ) updates one entry: At [it ] ← At−1 [it ] + It . Conceptually, we generate a random
matrix R ∈ RD×k whose entries are sampled from a stable distribution and multiply it with At :
X = At × R. The matrix multiplication is linear and can be conducted incrementally as the new
stream elements arrive. R is not materialized; its entries are re-generated on demand using pseudo-
random numbers, as the standard practice in data stream computations [7]. Our method does not
require At [i] ≥ 0 and hence it can handle the difference between two streams (e.g., the OD ﬂows).
2.1 The Symmetric Stable Law
Our work utilizes the symmetric stable distribution. We adopt the standard approach [3] to sample
from the stable law S (α, 1) with index α and unit scale. We generate two independent random
(cid:104) cos(u − αu)
(cid:105)(1−α)/α ∼ S (α, 1),
variables: w ∼ exp(1) and u ∼ unif om(−π/2, π/2) and feed them to a nonlinear transformation:
sin(αu)
(cos u)1/α
w
to obtain a sample from S (α, 1). An important property is that, for −1 < γ < α, the moment exists:
E |Z |γ = (2/π)Γ(1 − γ /α)Γ(γ ) sin(γπ/2). For convenience, we deﬁne
2
G(α, γ ) = E |g(w, u, α)|γ =
Γ(1 − γ /α)Γ(γ ) sin (γπ/2)
π

Z (α) = g(w, u, α) =

(8)

(7)

xj =

yj =

j = 1, 2, ..., k ,

(9)

At [i]g(wij , uij , α)

At [i]g(wij , uij , 1),

2.2 Our Recommended Estimator
Conceptually, we have two matrices of i.i.d. random numbers:
uij ∼ unif orm(−π/2, π/2),
wij ∼ exp(1),
i = 1, 2, ..., D ,
D(cid:88)
D(cid:88)
As new stream elements arrive, we incrementally maintain two sets of samples, i.e., for i = 1 to k ,
i=1
i=1
Note that xj and yj are highly correlated because they are generated using the same random numbers
(with different α). However, xi and yj are independent if i (cid:54)= j .
1 −

(cid:195)
(cid:33)2α
(cid:112)|yj |
(cid:162) (cid:80)k
Our recommended estimator of the Tsallis entropy Tα is
(cid:161)
(cid:112)|xj |
(cid:80)k
√
π
j=1
1 − 1
2α
j=1
where α = 1 + ∆ > 1 and the meaning of 0.5 will soon be clear. When ∆ is sufﬁciently small, the
estimated Tsallis entropy will be sufﬁciently close to the Shannon entropy. A nice property is that its
∆2 terms. While it is intuitively clear that it is beneﬁcial to make xj and yj
variance is free of 1
∆ or 1
highly correlated for the sake of reducing the variance, it might not be as intuitive why ˆTα,0.5 (11)
is a good estimator for the entropy. We will explain why the obvious geometric mean estimator [10]
is not sufﬁcient for entropy estimation.

1
α − 1

(10)

(11)

ˆTα,0.5 =

Γ

4

(12)

ˆTα,gm =

ˆTα,gm =

3 The Geometric Mean Estimator
(cid:195)
(cid:33)
(cid:81)k
(cid:81)k
For estimating F(α) , the geometric mean estimator [10] is close to be statistically optimal (efﬁciency
≈ 80%) at α ≈ 1. Thus, it was our ﬁrst attempt to test the following estimator of the Tsallis entropy:
j=1 |xj |1/k
j=1 |yj |α/k
1 − ˆF(α),gm
1
ˆF(1),gm =
ˆF(α),gm =
, where
α − 1
Gk (α, α/k) ,
Gk (1, 1/k) ,
ˆF α
(cid:35) .
1 − k(cid:89)
(cid:34)(cid:175)(cid:175)(cid:175)(cid:175) yj
(1),gm
(cid:175)(cid:175)(cid:175)(cid:175)α/k G(1, 1/k)
where G() is deﬁned in (8). After simpliﬁcation, we obtain:
1
α − 1
G(α, α/k)
xj
j=1
3.1 Theoretical Analysis
 ,
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)sα/k
(cid:34)(cid:175)(cid:175)(cid:175)(cid:175) yj
(cid:35)
(cid:175)(cid:175)(cid:175)(cid:175)sα/k
(cid:80)D
The theoretical analysis of ˆT(α),gm , however, turns out to be difﬁcult, as it requires computing
(cid:80)D
i=1 At [i]g(wij , uij , α)
i=1 At [i]g(wij , uij , 1)
xj
where g() is deﬁned in (7). We ﬁrst provide the following Lemma:
Lemma 1 Let w ∼ exp(1) and u ∼ unif orm(−π/2, π/2) be two independent variables. Let
(cid:175)(cid:175)(cid:175)(cid:175) g(w, u, α)
(cid:175)(cid:175)(cid:175)(cid:175)γ
α = 1 + ∆ > 1, for small ∆ > 0. Then, for γ > −1,
γ 2∆3 (cid:162)
γ∆4 (cid:162)
(cid:161)
(cid:161)
E
g(w, u, 1)
=1 − 0.5772γ∆ + 0.5772γ∆2 − 1.6386γ∆3 + 1.6822γ 2∆2 + O
Note that we need to keep higher order terms in order to prove Lemma 2, to show the properties of
the geometric mean estimator, when D = 1 (i.e., a stream with only one element).
(cid:182)
(cid:181)
(cid:182)
(cid:181)
(cid:179)
(cid:180)
(cid:161)
∆3 (cid:162)
Lemma 2 If D = 1, then
(cid:180)
(cid:179)
1
π2
− 2.0935
ˆTα,gm
=
2
k
3.3645
k

(cid:182)
(cid:181)
(cid:182)
+ 1.0614∆2 + O

(cid:164) (16)

V ar

ˆTα,gm

=

+ O

+ O

+ O

+ O

(cid:181)
∆
k

∆
k

s = 1, 2,

(13)

E

= E

+ O

(cid:164) (14)

1
k2

E

∆2
k

1
k2

(15)

In this case, the geometric mean estimator ˆTα,gm is
When D = 1, we know Tα = H = 0.
asymptotically unbiased with variance essentially free of 1
∆ , which is very encouraging.
Will this result in Lemma 2 extend to general D? The answer is no, even for D = 2, i.e.,
= At [1]g(w1j , u1j , α) + At [2]g(w2j , u2j , α)
yj
At [1]g(w1j , u1j , 1) + At [2]g(w2j , u2j , 1)
xj
the denominator At [1]g(w1j , u1j , 1) +
Because g()
it
is possible that
symmetric,
is
the numerator At [1]g(w1j , u1j , α) +
At [2]g(w2j , u2j , 1) might be very small while
(cid:162)
(cid:161)
At [2]g(w2j , u2j , α) is not
too small.
In other words,
there will be more variations when
(cid:162)
(cid:161)
D > 1. In fact, our experiments in Sec. 3.2 and the theoretical analysis of a more general estimator
in Sec. 4 both reveal that the variance of ˆTα,gm is essentially O
, which is of course still a
1
∆
solution.
substantial improvement over the previous O
1
∆2
3.2 Experiments on the Geometric Mean Estimator (Correlated vs. Independent samples)
(cid:161)
(cid:162)
We present some experimental results for evaluating ˆTα,gm , to demonstrate that (i) using correlation
does substantially reduce variance and hence reduces the required sample size, and (ii) the variance
(or MSE, the mean square error) of ˆTα,gm is roughly O
.
1
∆

5

We follow [13] by using static data to evaluate the accuracies of the estimators. The projected vector
X = At × R is the same at the end of the stream, regardless of whether it is computed at once (i.e.,
static) or incrementally (i.e., dynamic). Following [13], we selected 4 word vectors from a chunk of
Web crawl data. For example, the entries for vector “REVIEW” are the numbers of occurrences of
the word “REVIEW” in each document. We group these 4 vectors into 2 pairs: “THIS-HAVE” and
“DO-REVIEW” and we estimate the Shannon entropies of the two resultant difference vectors.
Figure 2 presents the mean square errors (MSE) of the estimated Shannon entropy, i.e., E ( ˆTα −H )2 ,
normalized by the truth (H 2 ). The left panels contain the results using independently sampling
(i.e., the prior work [21]) and the geometric mean estimator. The middle panels contain the results
(cid:161)
(cid:162)
using correlated sampling (i.e., this paper) and the geometric mean estimator (12). The right panels
multiply the results of the middle panels by ∆ to illustrate that the variance of the geometric mean
estimator for entropy ˆTα,gm is essentially O
. See more experiments in Figure 3.
1
∆

ˆTα,γ =

Figure 2: Two pairs of word vectors were selected. We conducted symmetric random projections
using both independent sampling (left panels, as in [21]) and correlated sampling (middle panels, as
our proposal). The Tsallis entropy (of the difference vector) is estimated using the geometric mean
estimator (12) with three sample sizes k = 10, 100, and 1000. The normalized mean square errors
(MSE: E | ˆTα,gm − H |2 /H 2 ) verify that correlated sampling reduces the errors substantially.
4 The General Estimator
Since the geometric mean estimator could not satisfactorily solve the entropy estimation problem,
we resort to estimators which behave dramatically different from the geometric mean. Our rec-
(cid:33)
(cid:195) (cid:80)k
(cid:195) (cid:80)k
(cid:195)
(cid:33)α/γ
(cid:33)1/γ
ommended estimator ˆTα,0.5 as in (11) is a special case (for γ = 0.5) of a more general family of
estimators [12], parameterized by γ ∈ (0, 1):
j=1 |xj |γ
j=1 |yj |γ
1 − ˆF(α),γ
1
, ˆF(α),γ =
α − 1
ˆF α
1 −
kG(α, γ )
kG(1, γ )
(cid:195) (cid:80)k
(1),γ
which, after simpliﬁcation, becomes
(cid:80)k
j=1 |yj |γ
j=1 |xj |γ
√
Recall G(α, γ ) is deﬁned in (8), and G(1,0.5)
π
G(α,0.5) =
2α ) .
Γ(1− 1
(cid:182)
(cid:181) (cid:80)k
To better understand ˆF(α),γ , recall if Z ∼ S (α, 1), then E |Z |γ = G(α, γ ) < ∞ if −1 < γ < α.
j=1 |yj |γ
is an unbiased estimate of F γ /α
(cid:180)
(cid:179)
Therefore,
(α) . To recover F(α) , we need to apply the
kG(α,γ )
power α/γ operation. Thus, it is clear that, as long as 0 < λ < 1, ˆF(α),γ is a consistent estimator of
(cid:181)
(cid:182)
(cid:182)
(cid:181)
(cid:180)
(cid:180)
(cid:179)
(cid:179)
is ﬁnite. In particular, the variance of ˆF(α),γ is bounded if 0 < γ < 0.5:
ˆF(α),γ
F(α) and E
G(α, 2γ ) − G2 (α, γ )
F 2
1
(α)
G2 (α, γ )
k
k

ˆF(1),γ =

(cid:33)α/γ

G(1, γ )
G(α, γ )

= F(α) + O

E

ˆF(α),γ

1
α − 1

,

V ar

ˆF(α),γ

=

α2
γ 2

+ O

1
k2

,

ˆTα,γ =

(17)

6

10−510−410−310−210−110010−2100102104106k = 10100k = 1000THIS−HAVE : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000THIS−HAVE : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−510−410−310−210−1k = 10k = 100k = 1000THIS−HAVE : GM + Corr.∆ = α − 1Normalized MSE × ∆MSE × ∆10−510−410−310−210−110010−2100102104106k = 10100k = 1000DO−REVIEW : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000DO−REVIEW : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−510−410−310−210−1k = 10k = 100k = 1000DO−REVIEW : GM + Corr.∆ = α − 1Normalized MSE × ∆MSE × ∆+ O

(cid:164)

V ar

(1 − 2γ ),

The variance is unbounded if γ = 0.5 and α = 1, because G(1, 1) = ∞ (Γ(0) = ∞). Interestingly,
when γ → 0 and α = 1, the asymptotic variance reaches the minimum. In fact, when γ → 0, ˆF(α),γ
converges to the geometric mean estimator ˆF(α),gm . A variant of ˆF(α),γ was discussed in [12].
(cid:179)
(cid:180)
(cid:179)
(cid:180)
4.1 Theoretical Analysis
Based on Lemma 3 and Lemma 4 (which is a fairly technical proof), we know that the variance of the
, for ﬁxed γ ∈ (0, 1/2). In other words,
∆2γ−1
ˆTα,γ
= O
general estimator is essentially V ar
k
when γ is close to 0, the variance of the entropy estimator is essentially on the order of O (1/(k∆)),
and while γ is close to 1/2, the variance is essentially O(1/k) as desired.
(cid:161)
(cid:162)
(cid:181)
(cid:182)
(cid:179)
(cid:180)
Lemma 3 For any ﬁxed γ ∈ (0, 1),
E (|x1 |γ − |y1 |γ )2
1
1
O
ˆTα,γ
=
∆2
k2
k
Lemma 4 Let 0 < ∆ ≤ 1/2 and α = 1 + ∆. Let γ ∈ (0, 1/2) and m be a positive integer no
(cid:111)(cid:46)
(cid:110)
(cid:180)2 ≤ M F 2γ
(cid:179)
m + (cid:101)H 2
smaller than 1/γ . Then, there exists a universal constant M such that
(cid:161) (cid:80)D
(cid:161)
(cid:162)1/(2m) .
|x1 |γ − |y1 |γ
(2m) + (1 − 2γ )−2
(1)∆1+2γ−1/mm2
where (cid:101)H2m =
E
|At [i]|
log |At [i]|
(cid:164)
)2m
i=1
F(1)
F(1)
We should clarify that our theoretical analysis is only applicable for ﬁxed γ ∈ (0, 1/2). When
γ = 0.5, the estimator ˆT(α),0.5 is still well-behaved, except we are unable to precisely analyze this
case. Also, since we do not compute the exact constant, it is possible that for some carefully chosen
α (data-dependent), ˆT(α),γ with γ < 0.5 may exhibit smaller variance than ˆT(α),0.5 . We recommend
ˆT(α),0.5 for convenience because it essentially frees practitioners from carefully choosing α.
4.2 Experimental Results
Figure 3 presents some empirical results, for testing the general estimator ˆTα,γ (17), using more
word vector pairs (including the same 2 pairs in Figure 2). We can see that when γ = 0.5, the
(normalized) MSEs become ﬂat (as desired) as ∆ = α − 1 → 0. When γ > 1/2, the MSEs increase
although the curves remain ﬂat. When γ < 1/2, the MSEs blow up with increasing ∆. Note that,
when γ < 1/2, it is possible to achieve smaller MSEs if we carefully choose α.
√
How many samples (k) are needed? If the goal is to estimate the Shannon entropy within a few
percentages of the the true value, then k = 100 ∼ 1000 should be sufﬁcient, because
M SE /H <
0.1 when k ≥ 100 as shown in Figure 3.
5 Conclusion
Entropy estimation is an important task in machine learning, data mining, network measurement,
anomaly detection, neural computations, etc. In modern applications, the data are often generated
in a streaming fashion and many operations on the streams can only be conducted in one-pass of the
data. It has been a challenging problem to estimate the Shannon entropy of data streams.
The prior work [21] achieved some success in entropy estimation using symmetric stable random
projections. However, even after aggressively exploiting the bias-variance tradeoff, they still need
to a large number of samples, e.g., 106 , which is prohibitive in both time and space, especially
considering that in streaming applications the pseudo-random numbers have to be re-generated on
the ﬂy, the cost of which is directly proportional to the sample size.
In our approach, we approximate the Shannon entropy using two high correlated estimates of the
frequent comments. The positive correlation can substantially reduce the variance of the Shannon
entropy estimate. However, ﬁnding the appropriate estimator of the frequency moment is another
challenging task. We successfully ﬁnd such an estimator and show that its variance (of the Shannon
entropy estimate) is very small. Experimental results demonstrate that about 100 ∼ 1000 samples
should be sufﬁcient for achieving high accuracies.

7

Acknowledgement
The research of Ping Li is partially supported by NSF-IIS-1249316, NSF-DMS-0808864, NSF-SES-
1131848, and ONR-YIP-N000140910911. The research of Cun-Hui Zhang is partially supported
by NSF-DMS-0906420, NSF-DMS-1106753, NSF-DMS-1209014, and NSA-H98230-11-1-0205.

Figure 3: The ﬁrst two rows are the normalized MSEs for same two vectors used in Figure 2, for
estimating Shannon entropy using the general estimator ˆTα,γ with γ = 0.3, 0.4, 0.5, 0.6, 0.7. For
the rest of the rows, the leftmost panels are the results of using independent samples (i.e., the prior
work [21]) and the geometric mean estimator. The second column of panels are the results of using
correlated samples and the geometric mean estimator. The right three columns of panels are for the
proposed general estimator ˆTα,γ with γ = 0.3, 0.5, 0.7. We recommend γ = 0.5.

8

10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000THIS−HAVE : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000THIS−HAVE : Corr. γ = 0.4∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000THIS−HAVE : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000THIS−HAVE : Corr. γ = 0.6∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000THIS−HAVE : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DO−REVIEW : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DO−REVIEW : Corr. γ = 0.4∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DO−REVIEW : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DO−REVIEW : Corr. γ = 0.6∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DO−REVIEW : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000UNITED−STATES : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000UNITED−STATES : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000UNITED−STATES : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000UNITED−STATES : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000UNITED−STATES : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000A−THE : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000A−THE : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000A−THE : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000A−THE : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000A−THE : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000FOOD−LOVE : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000FOOD−LOVE : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000FOOD−LOVE : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000FOOD−LOVE : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000FOOD−LOVE : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000DATA−PAPER : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000DATA−PAPER : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DATA−PAPER : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DATA−PAPER : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000DATA−PAPER : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000NEWS−WASHINGTON : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000NEWS−WASHINGTON : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000NEWS−WASHINGTON : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000NEWS−WASHINGTON : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000NEWS−WASHINGTON : Corr. γ = 0.7∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10100k = 1000MACHINE−LEARN : GM + Indep.∆ = α − 1Normalized MSE10−510−410−310−210−110010−2100102104106k = 10k = 100k = 1000MACHINE−LEARN : GM + Corr.∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000MACHINE−LEARN : Corr. γ = 0.3∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000MACHINE−LEARN : Corr. γ = 0.5∆ = α − 1Normalized MSE10−510−410−310−210−110010−410−310−210−1100k = 10k = 100k = 1000MACHINE−LEARN : Corr. γ = 0.7∆ = α − 1Normalized MSEReferences
[1] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom. Models and issues
in data stream systems. In PODS, pages 1–16, Madison, WI, 2002.
[2] Daniela Brauckhoff, Bernhard Tellenbach, Arno Wagner, Martin May, and Anukool Lakhina. Impact of
packet sampling on anomaly detection metrics. In IMC, pages 159–164, Rio de Janeriro, Brazil, 2006.
[3] John M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random variables.
Journal of the American Statistical Association, 71(354):340–344, 1976.
[4] Laura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical approaches to
DDoS attack detection and response. In DARPA Information Survivability Conference and Exposition,
pages 303–314, 2003.
[5] Anupam Gupta, John D. Lafferty, Han Liu, Larry A. Wasserman, and Min Xu. Forest density estimation.
In COLT, pages 394–406, Haifa, Israel, 2010.
[6] Nicholas J. A. Harvey, Jelani Nelson, and Krzysztof Onak. Streaming algorithms for estimating entropy.
In ITW, 2008.
[7] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation.
Journal of ACM, 53(3):307–323, 2006.
[8] Anukool Lakhina, Mark Crovella, and Christophe Diot. Mining anomalies using trafﬁc feature distribu-
tions. In SIGCOMM, pages 217–228, Philadelphia, PA, 2005.
[9] Ashwin Lall, Vyas Sekar, Mitsunori Ogihara, Jun Xu, and Hui Zhang. Data streaming algorithms for
estimating entropy of network trafﬁc. In SIGMETRICS, pages 145–156, Saint Malo, France, 2006.
[10] Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random
projections. In SODA, pages 10 – 19, San Francisco, CA, 2008.
[11] Ping Li. Compressed counting. In SODA, New York, NY, 2009.
[12] Ping Li and Trevor J. Hastie. A uniﬁed near-optimal estimator for dimension reduction in lα (0 < α ≤ 2)
using stable random projections. In NIPS, Vancouver, BC, Canada, 2007.
[13] Ping Li and Cun-Hui Zhang. A new algorithm for compressed counting with applications in shannon
entropy estimation in dynamic data. In COLT, 2011.
[14] Qiaozhu Mei and Kenneth Church. Entropy of search logs: How hard is search? with personalization?
with backoff? In WSDM, pages 45 – 54, Palo Alto, CA, 2008.
[15] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical
Computer Science, 1:117–236, 2 2005.
[16] Noam Nisan. Pseudorandom generators for space-bounded computations. In Proceedings of the twenty-
second annual ACM symposium on Theory of computing, STOC, pages 204–212, 1990.
[17] Liam Paninski. Estimation of entropy and mutual information. Neural Comput., 15(6):1191–1253, 2003.
[18] Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics,
52:479–487, 1988.
[19] Kuai Xu, Zhi-Li Zhang, and Supratik Bhattacharyya. Proﬁling internet backbone trafﬁc: behavior models
and applications. In SIGCOMM ’05: Proceedings of the 2005 conference on Applications, technologies,
architectures, and protocols for computer communications, pages 169–180, Philadelphia, Pennsylvania,
USA, 2005.
[20] Qiang Yang and Xindong Wu. 10 challeng problems in data mining research. International Journal of
Information Technology and Decision Making, 5(4):597–604, 2006.
[21] Haiquan Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and Jun Xu. A data stream-
ing algorithm for estimating entropies of od ﬂows. In IMC, San Diego, CA, 2007.
[22] Vladimir M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Provi-
dence, RI, 1986.

9


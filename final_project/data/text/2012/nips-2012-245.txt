Afﬁne Independent Variational Inference

David Barber
Edward Challis
Department of Computer Science
University College London, UK
{edward.challis,david.barber}@cs.ucl.ac.uk

Abstract

We consider inference in a broad class of non-conjugate probabilistic models
based on minimising the Kullback-Leibler divergence between the given target
density and an approximating ‘variational’ density. In particular, for generalised
linear models we describe approximating densities formed from an afﬁne trans-
formation of independently distributed latent variables, this class including many
well known densities as special cases. We show how all relevant quantities can
be efﬁciently computed using the fast Fourier transform. This extends the known
class of tractable variational approximations and enables the ﬁtting for example of
skew variational densities to the target density.

1

Introduction

p(w) =

fn (w) with Z =

fn (w)dw

Whilst Bayesian methods have played a signiﬁcant role in machine learning and related areas (see
[1] for an introduction), improving the class of distributions for which inference is either tractable
or can be well approximated remains an ongoing challenge. Within this broad ﬁeld of research,
variational methods have played a key role by enabling mathematical guarantees on inferences (see
[28] for an overview). Our contribution is to extend the class of approximating distributions beyond
classical forms to approximations that can possess skewness or other non-Gaussian characteristics,
while maintaining computational efﬁciency.
(cid:90) N(cid:89)
N(cid:89)
We consider approximating the normalisation constant Z of a probability density function p(w)
1
Z
n=1
n=1
where w ∈ RD and fn : RD → R+ are potential functions. Apart from special cases, evaluating
Z and other marginal quantities of p(w) is difﬁcult due to the assumed high dimensionality D of
the integral. To address this we may ﬁnd an approximating density q(w) to the target p(w) by
minimising the Kullback-Leibler (KL) divergence
KL(q(w)|p(w)) = (cid:104)log q(w)(cid:105)q(w) − (cid:104)log p(w)(cid:105)q(w) = −H [q(w)] − (cid:104)log p(w)(cid:105)q(w) (1.2)
where (cid:104)f (x)(cid:105)p(x) refers to taking the expectation of f (x) with respect to the distribution p(x) and
H [q(w)] is the differential entropy of the distribution q(w). The non-negativity of the KL diver-
N(cid:88)
gence provides the lower bound
n=1
Finding the best parameters θ of the approximating density q(w|θ) is then equivalent to maximising
the lower bound on log Z . This KL bounding method is constrained by the class of distributions

log Z ≥ H [q(w)] +

(cid:104)log fn (w)(cid:105) := B .

(1.1)

(1.3)

1

(a)

(b)
(c)
2τ e−|wd |/τ with τ = 0.16 and Gaussian likelihood N (cid:0)y |wTx, σ2
(cid:1),
Figure 1: Two dimensional Bayesian sparse linear regression given a single data pair x, y using
a Laplace prior fd (w) ≡ 1
l
l = 0.05. (a) True posterior with log Z = −1.4026. (b) Optimal Gaussian approximation with
σ2
bound value BG = −1.4399. (c) Optimal AI generalised-normal approximation with bound value
BAI = −1.4026.

fn (wTxn )

(1.4)

p(w) ∝ N (w|µ, Σ)

p(w) and q(w) for which (1.3) can be efﬁciently evaluated. We therefore specialise on models of
N(cid:89)
the form
n=1
where {xn}N
n=1 is a collection of ﬁxed D dimensional real vectors and fn : R → R+ ; N (w|µ, Σ)
denotes a multivariate Gaussian in w with mean µ and covariance Σ. This class includes Bayesian
generalised linear models, latent linear models, independent components analysis and sparse linear
models amongst others1 . Many problems have posteriors that possess non-Gaussian characteristics
resulting from strongly non-Gaussian priors or likelihood terms. For example, in ﬁnancial risk mod-
elling it is crucial that skewness and heavy tailed properties of the data are accurately captured [27];
similarly in inverse modelling, sparsity inducing priors can lead to highly non-Gaussian posteriors.
It is therefore important to extend the class of tractable approximating distributions beyond standard
forms such as the multivariate Gaussian [20, 12, 2, 13]. Whilst mixtures of Gaussians [4, 10, 5] have
previously been developed, these typically require additional bounds. Our interest here is to consider
alternative multivariate distribution classes for which the KL method is more directly applicable2 .
2 Afﬁne independent densities
We ﬁrst consider independently distributed latent variables v ∼ qv (v|θ) = (cid:81)D
d=1 qvd(vd |θd ) with
(cid:90)
‘base’ distributions qvd. To enrich the representation, we form the afﬁne transformation w = Av+b
(cid:89)
(cid:0)(cid:2)A−1 (w − b)(cid:3)
(cid:1) (2.1)
where A ∈ RD×D is invertible and b ∈ RD . The distribution on w is then3
d |θd
qw (w|A, b, θ) =
δ (w − Av − b) qv (v|θ)dv =
1
where δ (x) = (cid:81)
|det (A) |
qvd
d
i δ(xi ) is the Dirac delta function, θ = [θ1 , ..., θd ] and [x]d refers to the dth
N (cid:0)w|b, AAT (cid:1). By using, for example, Student’s t, Laplace, logistic, generalised-normal or skew-
element of the vector x. Typically we assume the base distributions are homogeneous, qvd ≡ qv . For
instance, if we constrain each factor qvd(vd |θd ) to be the standard normal N (vd |0, 1) then qw (w) =
normal base distributions, equation (2.1) parameterises multivariate extensions of these univariate
1For p(w) in this model class and Gaussian q(w) = N (cid:0)w|m, CTC(cid:1), B is tighter than ‘local’ bounds
distributions. This class of multivariate distributions has the important property that, unlike the
[14, 11, 22, 18, 16]. For log-concave f , B is jointly concave in (m, C) for C the Cholesky matrix [7].
2The skew-normal q(w) recently discussed in [21] possesses skew in one direction of parameter space only
and is a special case of the AI skew-normal densities used in section 4.2.
3This construction is equivalent to a form of square noiseless Independent Components Analysis. See [9]
and [25] for similar constructions.

2

−4−2024−0.200.20.40.60.81−4−2024−0.200.20.40.60.81−4−2024−0.200.20.40.60.81Gaussian, they can approximate skew and/or heavy-tailed p(w). See ﬁgures 1, 2 and 3, for examples
of two dimensional distributions qw (w|A, b, θ) with skew-normal and generalised-normal base
distributions used to approximate toy machine learning problems.
Provided we choose a base distribution class that includes the Gaussian as a special case (for example
generalised-normal, skew-normal and asymptotically Student’s t) we are guaranteed to perform at
least as well as classical multivariate Gaussian KL approximate inference.
We note that we may arbitrarily permute the indices of v. Furthermore, since every invertible matrix
is expressible as LUP for L lower, U upper and P permutation matrices, without loss of generality,
we may use an LU decomposition A = LU; this reduces the complexity of subsequent computa-
tions.
Whilst deﬁning such Afﬁne Independent (AI) distributions is straightforward, critically we require
that the bound, equation (1.3), is fast to compute. As we explain below, this can be achieved using
the Fourier transform both for the bound and its gradients. Full derivations, including formulae for
skew-normal and generalised-normal base distributions, are given in the supplementary material.

(cid:68)
log fn (wTxn )

2.1 Evaluating the KL bound
(cid:69)
D(cid:88)
N(cid:88)
The KL bound can be readily decomposed as
H [q(vd |θd )]
B = log |det (A)| +
+ (cid:104)log N (w|µ, Σ)(cid:105) +
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
n=1
d=1
where we used H [qw (w)] = log |det (A)| + (cid:80)
Energy
Entropy
d H [qvd(vd |θd )] (see for example [8]). For many
standard base distributions the entropy H [qvd(vd |θd )] is closed form. When the entropy of a uni-
variate base distribution is not analytically available, we assume it can be cheaply evaluated numer-
ically. The energy contribution to the KL bound is the sum of the expectation of the log Gaussian
term (which requires only ﬁrst and second order moments) and the nonlinear ‘site projections’. The
non-linear site projections (and their gradients) can be evaluated using the methods described below.
2.1.1 Marginal site densities
equivalent to a one-dimensional expectation, (cid:10)g (cid:0)wTx(cid:1)(cid:11)
Deﬁning y := wTx, the expectation of the site projection for any function g and ﬁxed vector x is
(cid:90)
(cid:90)
qw (w) = (cid:104)g(y)(cid:105)qy (y) with
δ(y − αTv − β )qv (v)dv
δ(y − xTw)qw (w)dw =
qy (y) =
one dimensional integral using the integral transform δ(x) = (cid:82) e2πitxdt:
where w = Av + b and α := ATx, β := bTx. We can rewrite this D-dimensional integral as a
(cid:90) (cid:90)
(cid:90)
D(cid:89)
D(cid:89)
e2πi(t−β )y
e2πit(y−αT v−β )
qvd(vd )dvdt =
qy (y) =
d=1
d=1
where ˜f (t) denotes the Fourier transform of f (x) and qud (ud |θd ) is the density of the random
|θd ). Equation(2.4) can be interpreted as the
variable ud := αd vd so that qud (ud |θd ) = 1|αd | qvd( ud
αd
(shifted) inverse Fourier transform of the product of the Fourier transforms of {qud (ud |θd )}.
for the product (cid:81)D
Unfortunately, most distributions do not have Fourier transforms that admit compact analytic forms
d=1 ˜qud (t). The notable exception is the family of stable distributions for which
linear combinations of random variables are also stable distributed (see [19] for an introduction).
With the exception of the Gaussian (the only stable distribution with ﬁnite variance), Levy and
Cauchy distributions, stable distributions do not have analytic forms for their density functions and
are analytically expressible only in the Fourier domain. Nevertheless, when qv (v) is stable dis-
tributed, marginal quantities of w such as y can be computed analytically in the Fourier domain
[3].

(2.3)

(2.2)

˜qud (t) dt

(2.4)

3

(a)

(b)

(c)

Figure 2: Two dimensional Bayesian logistic regression with Gaussian prior N (w|0, 10I) and like-
lihood fn (w) = σ(τl cnwTxn ), τl = 5. Here σ(x) is the logistic sigmoid and cn ∈ {−1, +1} the
class labels; N = 4 data points. (a) True posterior with log Z = −1.13. (b) Optimal Gaussian
approximation with bound value BG = −1.42. (c) Optimal AI skew-normal approximation with
bound value BAI = −1.17.

q(ud |θd )dud . (2.5)

qud (ud |θd ) ≈ ˆqud (ud ) :=

πdk δ (ud − lk ) where πdk =

In general, therefore, we need to resort to numerical methods to compute qy (y) and expectations
with respect to it. To achieve this we discretise the base distributions and, by choosing a sufﬁciently
ﬁne discretisation, limit the maximal error that can be incurred. As such, up to a speciﬁed accuracy,
the KL bound may be exactly computed.
First we deﬁne the set of discrete approximations to {qud (ud |θd )}D
d=1 for ud := αd vd . These
(cid:90) lk+ 1
K(cid:88)
‘lattice’ approximations are a weighted sum of K delta functions
2 ∆
lk− 1
2 ∆
k=1
The lattice points {lk }K
k=1 are spaced uniformly over the domain [l1 , lK ] with ∆ := lk+1 − lk . The
weighting for each delta spike is the mass assigned to the distribution qud (ud |θd ) over the interval
[lk − 1
2 ∆].
2 ∆, lk + 1
Given the lattice approximations to the densities {qud (ud |θd )}D
d=1 the fast Fourier transform (FFT)
can be used to evaluate the convolution of the lattice distributions. Doing so we obtain the lattice
(cid:35)
(cid:34) D(cid:89)
K(cid:88)
approximation to the marginal y = wTx such that (see supplementary section 2.2)
qy (y) ≈ ˆqy (y) =
δ(y − lk − β )ρk where ρ = ifft
fft [π (cid:48)
(2.6)
d ]
.
k=1
d=1
calculations being exact. The time complexity for the above procedure scales O (cid:0)D2K log KD(cid:1).
where πd is padded with (D − 1)K zeros, π (cid:48)
d := [πd , 0]. The only approximation used in ﬁnding
the marginal density is then the discretisation of the base distributions, with the remaining FFT
2.1.2 Efﬁcient site derivative computation
seen by inspecting the partial derivative of (cid:10)g(wTx)(cid:11) with respect to Amn
Whilst we have shown that the expectation of the site projections can be accurately computed using
the FFT, how to cheaply evaluate the derivative of this term is less clear. The complication can be
(cid:90)
(cid:17)
qv (v)g (cid:48) (cid:16)
(cid:68)
(cid:69)
∂
g(wTx)
xTAv + bTx
vmdv,
= xn
∂Amn
q(w)
where g (cid:48) (y) = d
dy g(y). Naively, this can be readily reduced to a (relatively expensive) two dimen-
(cid:90)
(cid:90)
sional integral. Critically, however, the computation can be simpliﬁed to a one dimensional integral.
(cid:68)
(cid:16)
(cid:17)(cid:69)
(cid:16)
(cid:17)
To see this we can write
∂
g
∂Amn

g (cid:48) (y)dm (y)dy , where dm (y) :=

(2.7)

wTx

= xn

vm qv (v)δ

y − αTv − β

dv.

4

−50510−8−6−4−202−50510−8−6−4−202−50510−8−6−4−202(a)

(b)

(c)

Figure 3: Two dimensional robust linear regression with Gaussian prior N (w|0, I), Laplace like-
e−|yn−wT xn |/τl with τl = 0.1581 and 2 data pairs xn , yn . (a) True posterior
lihood fn (w) = 1
with log Z = −3.5159. (b) Optimal Gaussian approximation with bound value BG = −3.6102. (c)
2τl
Optimal AI generalised-normal approximation with bound value BAI = −3.5167.
(cid:90) um
(cid:89)
Here dm (y) is a univariate weighting function with Fourier transform:
˜dm (t) = e−2πitβ ˜em (t)
qum (um )e−2πitum dum .
˜qud (t), where ˜em (t) :=
d=1 are required to compute the expectation of (cid:10)g(wTx)(cid:11) the only additional computa-
αm
d (cid:54)=m
Since { ˜q(t)}D
tions needed to evaluate all partial derivatives with respect to A are {˜ed (t)}D
d=1 . Thus the complexity
of computing the site derivative4 is equivalent to the complexity of the site expectation of section
2.1.1.
Even for non-smooth functions g the site gradient has the additional property that it is smooth, pro-
vided the base distributions are smooth. Indeed, this property extends to the KL bound itself, which
has continuous partial derivatives, see supplementary material section 1. This means that gradient
optimisation for AI based KL approximate inference can be applied, even when the target density
is non-smooth. In contrast, other deterministic approximate inference routines are not universally
applicable to non-smooth target densities – for instance the Laplace approximation and [10] both
require the target to be smooth.

2.2 Optimising the KL bound

Given ﬁxed base distributions, we can optimise the KL bound with respect to the parameters A =
LU and b. Provided {fn}N
n=1 are log-concave the KL bound is jointly concave with respect to b and
either L or U. This follows from an application of the concavity result in [7] – see the supplementary
material section 3.
Using a similar approach to that presented in section 2.1.2 we can also efﬁciently evaluate the
gradients of the KL bound with respect to the parameters θ that deﬁne the base distribution. These
parameters θ can control higher order moments of the approximating density q(w) such as skewness
and kurtosis. We can therefore jointly optimise over all parameters {A, b, θ} simultaneously; this
means that we can fully capitalise on the expressiveness of the AI distribution class, allowing us to
capture non-Gaussian structure in p(w).
In many modeling scenarios the best choice for qv (v) will suggest itself naturally. For example,
in section 4.1 we choose the skew-normal distribution to approximate Bayesian logistic regression
posteriors. For heavy-tailed posteriors that arise for example in robust or sparse Bayesian linear
regression models, one choice is to use the generalised-normal as base density, which includes the
Laplace and Gaussian distributions as special cases. For other models, for instance mixed data factor
analysis [15], different distributions for blocks of variables of {vd }D
d=1 may be optimal. However, in
situations for which it is not clear how to select qv (v), several different distributions can be assessed
and then that which achieves the greatest lower bound B is preferred.
4Further derivations and computational scaling properties are provided in supplementary section 2.

5

−0.200.20.4−0.1−0.0500.050.10.150.20.25−0.200.20.4−0.1−0.0500.050.10.150.20.25−0.200.20.4−0.1−0.0500.050.10.150.20.252.3 Numerical issues

The computational burden of the numerical marginalisation procedure described in section 2.1.1
depends on the number of lattice points used to evaluate the convolved density function qy (y). For
y = (cid:80)
the results presented we implemented a simple strategy for choosing the lattice points [l1 , ..., lK ].
Lattice end points were chosen5 such that [l1 , lK ] = [−6σy , 6σy ] where σy is the standard deviation
of the random variable y : σ2
dvar(vd ). From Chebyshev’s inequality, taking six standard
d α2
deviation end points guarantees that we capture at least 97% of the mass of qy (y). In practice this
proportion is often much higher since qy (y) is often close to Gaussian for D (cid:29) 1. We ﬁx the
number of lattice points used during optimisation to suit our computational budget. To compute the
ﬁnal bound value we apply the simple strategy of doubling the number of lattice points until the
evaluated bound changes by less than 10−3 [6].
Fully characterising the overall accuracy of the approximation as a function of the number of lattice
points is complex, see [24, 26] for a related discussion. One determining factor is the condition
number (ratio of largest and smallest eigenvalues) of the posterior covariance. When the condi-
tion number is large many lattice points are needed to accurately discretise the set of distributions
{qud (ud |θd )}D
d=1 which increases the time and memory requirements.
One possible route to circumventing these issues is to use base densities that have analytic Fourier
transforms (such as a mixture of Gaussians).
In such cases the discrete Fourier transform of
qy (y) can be directly evaluated by computing the product of the Fourier transforms of each
{qud (ud |θd )}D
d=1 . The implementation and analysis of this procedure is left for future work.
O (cid:0)N D2K log DK (cid:1). Whilst this might appear expensive it is worth considering it within the
The computational bottleneck for AI inference, assuming N > D , arises from computing the ex-
pectation and partial derivatives of the N site projections. For parameters w ∈ RD this scales
proximate inference has bound and gradient computations which scale O (cid:0)N D2 (cid:1). Similarly, local
variational bounding methods (see below) scale O (cid:0)N D2 (cid:1) when implemented exactly.
broader scope of lower bound inference methods. It was shown in [7] that exact Gaussian KL ap-
3 Related methods

Another commonly applied technique to obtain a lower bound for densities of the form of equation
(1.4) is the ‘local’ variational bounding procedure [14, 11, 22, 18]. Local bounding methods approx-
imate the normalisation constant by bounding each non-conjugate term in the integrand, equation
(1.1), with a form that renders the integral tractable. In [7] we showed that the Gaussian KL bound
dominates the local bound in such models. Hence the AI KL method also dominates the local and
Gaussian KL methods.
Other approaches increase the ﬂexibility of the approximating distribution by expressing qw (w) as a
mixture. However, computing the entropy of a mixture distribution is in general difﬁcult. Whilst one
may bound the entropy term [10, 4], employing such additional bounds is undesirable since it limits
the gains from using a mixture. Another recently proposed method to approximate integrals using
mixtures is split mean ﬁeld which iterates between constructing soft partitions of the integral domain
and bounding those partitioned integrals [5]. The partitioned integrals are approximated using local
or Gaussian KL bounds. Our AI method is complementary to the split mean ﬁeld method since one
may use the AI technique to bound each of the partitioned integrals and so achieve an improved
bound.

4 Experiments

For the experiments below6 , AI KL bound optimisation is performed using L-BFGS7 . Gaussian KL
inference is implemented in all experiments using our own package8 .
5For symmetric densities {qud (ud |θd )} we arranged that their mode coincides with the central lattice point.
6All experiments are performed in Matlab 2009b on a 32 bit Intel Core 2 Quad 2.5 GHz processor.
7We used the minFunc package (www.di.ens.fr/˜mschmidt)
8mloss.org/software/view/308/

6

(a)

(b)

Figure 4: Gaussian KL and AI KL approximate inference comparison for a Bayesian logistic re-
gression model with different training data set sizes Ntrn . w ∈ R10 ; Gaussian prior N (w|0, 5I);
logistic sigmoid likelihood fn = σ(τl cnwTxn ) with τl = 5; covariates xn sampled from the stan-
dard normal, wtrue sampled from the prior and class labels cn = ±1 sampled from the likelihood.
(a) Bound differences, BAI − BG , achieved using Gaussian KL and AI KL approximate inference
for different training dataset sizes Ntrn . Mean and standard errors are presented from 15 randomly
generated models. A logarithmic difference of 0.4 corresponds to 49% improvement in the bound
on the marginal likelihood. (b) Mean and standard error averaged test set log probability (ATLP)
differences obtained with the Gaussian and AI approximate posteriors for different training dataset
sizes Ntrn . ATLP values calculated using 104 test data points sampled from each model.

4.1 Toy problems

We compare the performance of Gaussian KL and AI KL approximate inference methods in three
different two dimensional generalised linear models against the true posteriors and marginal likeli-
hood values obtained numerically. See supplementary section 4 for derivations. Figure 1 presents
results for a linear regression model with a sparse Laplace prior; the AI base density is chosen to be
generalised-normal. Figure 2 demonstrates approximating a Bayesian logistic regression posterior,
with the AI base distribution skew-normal. Figure 3 corresponds to a Bayesian linear regression
model with the noise robust Laplace likelihood density and Gaussian prior; again the AI approxi-
mation uses the generalised-normal as the base distribution. The AI KL procedure achieves a con-
sistently higher bound than the Gaussian case, with the AI bound nearly saturating at the true value
of log Z in two of the models. In addition, the AI approximation captures signiﬁcant non-Gaussian
features of the posterior: the approximate densities are sparse in directions of sparsity of the poste-
rior; their modes are approximately equal (where the Gaussian mode can differ signiﬁcantly); tail
behaviour is more accurately captured by the AI distribution than by the Gaussian.

4.2 Bayesian logistic regression

We compare Gaussian KL and AI KL approximate inference for a synthetic Bayesian logistic regres-
sion model. The AI density has skew-normal base distribution with θd parameterising the skewness
of vd . We optimised the AI KL bound jointly with respect to L, U, b and θ simultaneously with
convergence taking on average 8 seconds with D = N = 10, compared to 0.2 seconds for Gaussian
KL9 . In ﬁgure 4 we plot the performance of the KL bound for the Gaussian versus the skew-normal
AI approximation as we vary the number of datapoints. In (a) we plot the mean and standard error
bound differences BAI − BG . For a small number of datapoints the bound difference is small. This
difference increases up to D = N , and then decreases for larger datasets. This behaviour can be
explained by the fact that when there are few datapoints the Gaussian prior dominates, with little dif-
ference therefore between the Gaussian and optimal AI approximation (which becomes effectively
Gaussian). As more data is introduced, the non-Gaussian likelihood terms have a stronger impact
and the posterior becomes signiﬁcantly non-Gaussian. However as even more data is introduced the
central limit theorem effect takes hold and the posterior becomes increasingly Gaussian. In (b) we

9We note that split mean ﬁeld approximate inference was reported to take approximately 100 seconds for a
similar logistic regression model achieving comparable results [20].

7

01020304000.10.20.30.40.5NtrnBAI−BG010203040−102468x 10−3NtrnATLPAI−ATLPGplot the mean and standard error differences for the averaged test set log probabilities (ATLP) cal-
culated using the Gaussian and AI approximate posteriors obtained in each model. For each model
and each training set size the ATLP is calculated using 104 test points sampled from the model. The
log test set probability of each test data pair x∗ , c∗ is calculated as log (cid:104)p(c∗ |w, x∗ )(cid:105)q(w) for q(w)
the approximate posterior. The bound differences can be seen to be strongly correlated with test set
log probability differences, conﬁrming that tighter bound values correspond to improved predictive
performance.
4.3 Sparse robust kernel regression
a Laplace prior on the weight vectors (cid:81)
In this experiment we consider sparse noise robust kernel regression. Sparsity is encoded using
d fd (wd ) where fd (wd ) = e−|wd |/τp /2τp . The Laplace
distribution is also used as a noise robust likelihood fn (w) = p(yn |w, kn ) = e−|yn−wT kn |/τl /2τl
where kn is the nth vector of the kernel matrix. The squared exponential kernel was used throughout
with length scale 0.05 and additive noise 1, see [23]. In all experiments the prior and likelihood were
ﬁxed with τp = τl = 0.16. Three datasets are considered: Boston housing10 (D = 14, Ntrn = 100,
Ntst = 406); Concrete Slump Test11 (D = 10, Ntrn = 100, Ntst = 930); a synthetic dataset
constructed as described in [17] §5.6.1 (D = 10, Ntrn = 100, Ntst = 406). Results are collected
for each data set over 10 random training and test set partitions. All datasets are zero mean unit
variance normalised based on the statistics of the training data.
¯BG
¯BAI
¯BAI − ¯BG
Dataset
AT LPAI
AT LPG
0.022 ± 0.004 −1.70 ± 0.11 −1.67 ± 0.11
Conc. CS. −2.08 ± 0.09 −2.06 ± 0.09
0.028 ± 0.003 −1.18 ± 0.10 −1.15 ± 0.09
−1.28 ± 0.05 −1.25 ± 0.05
Boston
Synthetic −2.49 ± 0.10 −2.46 ± 0.10
0.028 ± 0.004 −1.84 ± 0.11 −1.83 ± 0.11

AT LPAI − AT LPG
0.024 ± 0.010
0.023 ± 0.006
0.009 ± 0.009

AI KL inference is performed with a generalised-normal base distribution. The parameters θd con-
trol the kurtosis of the base distributions q(vd |θd ); for simplicity we ﬁx θd = 1.5 and optimise jointly
for L, U, b. Bound optimisation took roughly 250 seconds for the AI KL procedure, compared to
5 seconds for the Gaussian KL procedure. Averaged results and standard errors are presented in
the table above where ¯B denotes the bound value divided by the number of points in the training
dataset. Whilst the improvements for these particular datasets are modest, the AI bound dominates
the Gaussian bound in all three datasets, with predictive log probabilities also showing consistent
improvement.
Whilst we have only presented experimental results for AI distributions with simple analytically
expressible base distributions we note the method is applicable for any base distribution provided
{qvd(vd )}D
d=1 are smooth. For example smooth univariate mixtures for qvd(vd ) can be used.

5 Discussion

Afﬁne independent KL approximate inference has several desirable properties compared to existing
deterministic bounding methods. We’ve shown how it generalises on classical multivariate Gaussian
KL approximations and our experiments conﬁrm that the method is able to capture non-Gaussian
effects in posteriors. Since we optimise the KL divergence over a larger class of approximating den-
sities than the multivariate Gaussian, the lower bound to the normalisation constant is also improved.
This is particularly useful for model selection purposes where the normalisation constant plays the
role of the model likelihood.
There are several interesting areas open for further research. The numerical procedures presented
in section 2.1 provide a general and computationally efﬁcient means for inference in non-Gaussian
densities whose application could be useful for a range of probabilistic models. However, our current
understanding of the best approach to discretise the base densities is limited and further study of this
is required, particularly for application in very large systems D (cid:29) 1. It would also be useful to
investigate using base densities that directly allow for efﬁcient computation of the marginals qy (y)
in the Fourier domain.
10archive.ics.uci.edu/ml/datasets/Housing
11archive.ics.uci.edu/ml/datasets/Concrete+Slump+Test

8

In Advances in Neural Information

References
[1] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.
In Advances in Neural
[2] D. Barber and C. M. Bishop. Ensemble Learning for Multi-Layer Networks.
Information Processing Systems, NIPS 10, 1998.
[3] D. Bickson and C. Guestrin. Inference with Multivariate Heavy-Tails in Linear Models. In Advances in
Neural Information Processing Systems, NIPS 23. 2010.
[4] C. M. Bishop, N. Lawrence, T. Jaakkola, and M. I. Jordan. Approximating Posterior Distributions in
Belief Networks Using Mixtures. In Advances in Neural Information Processing Systems, NIPS 10, 1998.
[5] G. Bouchard and O. Zoeter. Split Variational Inference. In International Conference on Artiﬁcial Intelli-
gence and Statistics, AISTATS, 2009.
[6] R. N. Bracewell. The Fourier Transform and its Applications. McGraw-Hill Book Co, Singapore, 2000.
[7] E. Challis and D. Barber. Concave Gaussian Variational Approximations for Inference in Large-Scale
Bayesian Linear Models. In International Conference on Artiﬁcial Intelligence and Statistics, AISTATS,
2011.
[8] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, 1991.
[9] J. T. A. S. Ferreira and M. F. J. Steel. A New Class of Skewed Multivariate Distributions with Applications
To Regression Analysis. Statistica Sinica, 17:505–529, 2007.
[10] G. Gersman, M. Hoffman, and D. Blei. Nonparametric Variational Inference. In International Conference
on Machine Learning, ICML 29, 2012.
[11] M. Girolami. A Variational Method for Learning Sparse and Overcomplete Representations. Neural
Computation, 13:2517–2532, 2001.
[12] A. Graves. Practical Variational Inference for Neural Networks.
Processing Systems, NIPS 24, 2011.
[13] A. Honkela and H. Valpola. Unsupervised Variational Bayesian Learning of Nonlinear Models. In Ad-
vances in Neural Information Processing Systems, NIPS 17, 2005.
[14] T. Jaakkola and M. Jordan. A Variational Approach to Bayesian Logistic Regression Problems and their
Extensions. In Artiﬁcial Intelligence and Statistics, AISTATS 6, 1996.
[15] M. E. Khan, B. Marlin, G. Bouchard, and K. Murphy. Variational Bounds for Mixed-Data Factor Analysis.
In Advances in Neural Information Processing Systems, NIPS 23, 2010.
[16] D. A. Knowles and T. Minka. Non-conjugate Variational Message Passing for Multinomial and Binary
Regression. In Advances in Neural Information Processing Systems, NIPS 23. 2011.
[17] M. Kuss. Gaussian Process Models for Robust Regression, Classiﬁcation, and Reinforcement Learning.
PhD thesis, Technische Universit ¨at Darmstadt, Darmstadt, Germany, 2006.
[18] H. Nickisch and M. Seeger. Convex Variational Bayesian Inference for Large Scale Generalized Linear
Models. In International Conference on Machine Learning, ICML 26, 2009.
[19] J. P. Nolan. Stable Distributions - Models for Heavy Tailed Data. Birkhauser, Boston, 2012. In progress,
Chapter 1 online at academic2.american.edu/∼jpnolan.
[20] M. Opper and C. Archambeau. The Variational Gaussian Approximation Revisited. Neural Computation,
21(3):786–792, 2009.
[21] J. Ormerod. Skew-Normal Variational Approximations for Bayesian Inference. Technical Report CRG-
TR-93-1, School of Mathematics and Statistics, University of Sydney, 2011.
[22] A. Palmer, D. Wipf, K. Kreutz-Delgado, and B. Rao. Variational EM Algorithms for Non-Gaussian Latent
Variable Models. In Advances in Neural Information Processing Systems, NIPS 20, 2006.
[23] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[24] P. Ruckdeschel and M. Kohl. General Purpose Convolution Algorithm in S4-Classes by means of FFT.
Technical Report 1006.0764v2, arXiv.org, 2010.
[25] S. K. Sahu, D. K. Dey, and M. D. Branco. A New Class of Multivariate Skew Distributions with Appli-
cations to Bayesian Regression Models. The Canadian Journal of Statistics / La Revue Canadienne de
Statistique, 31(2):129–150, 2003.
[26] P. Schaller and G. Temnov. Efﬁcient and precise computation of convolutions: applying FFT to heavy
tailed distributions. Computational Methods in Applied Mathematics, 8(2):187–200, 2008.
[27] C. Siddhartha, F. Nardari, and N. Shephard. Markov chain Monte Carlo methods for stochastic volatility
models. Journal of Econometrics, 108(2):281–316, 2002.
[28] M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational Inference.
Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.

9


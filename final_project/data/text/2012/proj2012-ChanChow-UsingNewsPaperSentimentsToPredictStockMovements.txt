CS229	  Project	  Report	  
Using	  Newspaper	  Sentiments	  to	  Predict	  Stock	  Movements	  
Hao	  Yee	  Chan	  	  	  	  	  	  Anthony	  Chow	  
haoyeec@stanford.edu	  	  	  ac1408@stanford.edu	  
	  

	  

Problem	  Statement	  
It	  is	  often	  said	  that	  stock	  prices	  are	  determined	  by	  market	  sentiments.	  Also,	  these	  stock	  prices	  are	  an	  
instant	  reflection	  of	  the	  current	  market	  sentiments.	  Despite	  this,	  investors	  often	  use	  current	  news	  to	  
inform	  their	  next	  investment	  decision.	  The	  problem	  is	  then	  that	  it	  is	  almost	  impossible	  to	  read	  through	  all	  
the	  news	  available	  online.	  Even	  with	  a	  wealth	  of	  readings,	  market	  sentiments	  are	  difficult	  to	  be	  quantified	  
and	  understood.	  	  
	  
This	  project	  looks	  at	  news	  from	  Reuters	  Technology	  to	  be	  used	  as	  sources	  of	  data	  to	  generate	  a	  model	  to	  
capture	  market	  sentiments.	  This	  model	  will	  be	  used	  to	  try	  to	  predict	  the	  movements	  of	  the	  NASDAQ	  
Composite	  in	  the	  immediate	  future.	  
	  
Dataset	  
We	  scrapped	  data	  Reuters	  to	  create	  a	  model	  for	  market	  sentiments.	  We	  will	  be	  using	  yahoo	  share	  prices	  
to	  construct	  our	  classifiers,	  which	  will	  be	  the	  NASDAQ	  Composite,	  which	  is	  highly	  followed	  in	  the	  US	  as	  an	  
indicator	  of	  the	  performance	  of	  stocks	  of	  technology	  companies	  and	  growth	  companies.	  (see	  appendix	  for	  
screenshot	  of	  Reuters	  Technology)	  We	  created	  2	  separate	  datasets,	  one	  with	  1	  year	  of	  data	  (260	  days	  of	  
trading)	  and	  the	  second	  one	  with	  3	  years	  of	  data	  (690	  days	  of	  trading).	  Note	  that	  there	  are	  less	  days	  of	  
trading	  than	  there	  are	  in	  a	  year	  due	  to	  market	  closing	  during	  weekends	  as	  well	  as	  during	  public	  holidays.	  
	  
Problem	  Formulation	  
We	  split	  the	  news	  data	  from	  Reuters	  Technology	  into	  headline	  and	  body	  feature	  sets.	  We	  aim	  to	  predict,	  
𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 =    1 𝑖𝑓   𝑠𝑡𝑜𝑐𝑘   𝑚𝑜𝑣𝑒𝑠   𝑢𝑝𝑤𝑎𝑟𝑑𝑠
given	  today’s	  set	  of	  headline	  and	  body	  features,	  if	  the	  closing	  price	  of	  tomorrow’s	  stock	  will	  be	  higher	  or	  
−1                              𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒                                 	  
lower	  than	  today’s,	  using	  data	  from	  yahoo	  finance.	  
	  
	  
We	  did	  some	  preprocessing	  to	  build	  our	  dataset.	  From	  the	  news,	  we	  first	  split	  them	  into	  2	  sets:	  Headline	  
and	  body.	  For	  each	  set,	  we	  had	  stop	  words	  removed,	  the	  words	  lemmatized	  and	  selected	  500	  words	  using	  
𝑀𝐼=   
Χ ! =(!!!!!!"!!!"!!!! )   !!!!!!!!!"!!" !
the	  following	  heuristics:	  
𝑝 𝑥 , 𝑦 𝑙𝑜𝑔 𝑝(𝑥 , 𝑦)
	  
𝑝 𝑥 𝑝(𝑦)
Mutual	  Information	  
Chi-­‐squared	  
where	  𝑁!" =Number	  of	  news	  
!!!!!!" (!!!!!!" )(!!"!!!! )(!!"!!!! )	  	  
articles	  with	  word	  𝑡 	  and	  class	  𝑐 	  
!∈!
!∈!
Table	  1:	  Feature	  selection	  techniques	  used	  
Our	  Methodology	  	  
We	  used	  a	  mixture	  of	  supervised	  and	  unsupervised	  machine	  learning	  techniques	  to	  figure	  out	  our	  data.	  
Under	  the	  unsupervised	  techniques,	  we	  used	  factor	  analysis	  with	  EM	  to	  look	  at	  some	  of	  the	  key	  
dimensions	  that	  described	  the	  data.	  We	  also	  compared	  the	  performance	  of	  the	  various	  supervised	  
learning	  algorithms	  on	  the	  dataset.	  A	  summary	  of	  the	  supervised	  learning	  algorithms	  implemented	  in	  this	  
	  
1	  
paper	  is	  summarized	  in	  the	  table	  below.	  

Bag-­‐of-­‐Words	  
Most	  frequent	  words	  

	  

Multinomial	  Naïve	  Bayes	  
Gaussian	  Discriminant	  Analysis	  
Support	  Vector	  Machines	  
Headline	  features	  (1	  year)	  
-­‐	  
Headline	  features	  (1	  year)	  
Body	  features	  (1	  year)	  
Body	  features	  (1	  year)	  
Body	  features	  (1	  year)	  
Headline	  features	  (3	  years)	  
-­‐	  
Headline	  features	  (3	  years)	  
Body	  features	  (3	  years)	  
Body	  features	  (3	  years)	  
Body	  features	  (3	  years)	  
Table	  2:	  Table	  of	  summary	  of	  supervised	  learning	  algorithms	  implemented	  

	  
Our	  Results	  	  
Factor	  Analysis	  of	  Data	  
We	  implemented	  factor	  analysis	  on	  the	  body	  feature	  sets.	  We	  present	  the	  results	  obtained	  from	  the	  body	  
feature	  sets	  (1	  year	  and	  3	  years)	  generated	  from	  frequent	  words.	  We	  find	  that	  there	  seem	  to	  exist	  3	  main	  
dimensions	  in	  the	  data	  –	  Finance,	  Facebook	  and	  Apple	  that	  characterized	  the	  news	  from	  last	  year.	  For	  the	  
3	  years	  data,	  it	  seemed	  to	  be	  –	  Finance,	  Apple	  and	  everyone	  else.	  Perhaps	  the	  Facebook	  IPO	  in	  this	  past	  
year	  generated	  enough	  coverage	  to	  create	  this	  unique	  dimension	  in	  the	  data.	  Armed	  with	  the	  idea	  that	  
there	  were	  special	  dimensions	  in	  the	  data,	  that	  it	  was	  not	  as	  random	  as	  we	  thought,	  we	  went	  to	  perform	  
supervised	  learning	  with	  more	  confidence.	  

	  
	  
Table	  3:	  Dimensions	  obtained	  using	  factor	  analysis	  on	  body	  feature	  set	  of	  1-­‐year	  (left)	  and	  3-­‐years	  (right)	  
2	  
of	  data	  respectively	  

	  

Supervised	  Learning	  1	  –	  Multinomial	  Naïve	  Bayes	  
	  
We	  started	  the	  supervised	  learning	  with	  Multinomial	  Naïve	  Bayes.	  The	  datasets	  were	  split	  into	  2/3	  training	  
	   𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑆𝑒𝑡 𝑡 =   𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑆𝑒𝑡 𝑡 +    𝛿!2 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑆𝑒𝑡 𝑡 − 1 +    𝛿!3 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑆𝑒𝑡 𝑡 − 2 +    𝛿!4 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑆𝑒𝑡(𝑡 − 3)	  
and	  1/3	  testing	  sets.	  Observing	  that	  there	  exist	  imperfections	  in	  the	  market,	  we	  created	  a	  cumulative	  
feature	  set	  that	  takes	  into	  account	  information	  from	  past	  news	  in	  the	  following	  fashion:	  
𝛿! = 1  𝑖𝑓   𝑡 = 1,2,3  , 𝑎𝑛𝑑   0  𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒   	  
𝛿! = 1  𝑖𝑓   𝑡 = 2,3  , 𝑎𝑛𝑑   0  𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒   
𝛿! = 1  𝑖𝑓   𝑡 = 3, 𝑎𝑛𝑑   0  𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 	  
  𝑡 = 0,1,2,3  𝑑𝑎𝑦𝑠 	  

	  
Figure	  1:	  An	  example	  of	  test	  statistics	  trained	  using	  chi-­‐squared	  feature	  selection.	  

	  
Remarks:	  1)	  Best	  performance	  comes	  from	  feature	  sets	  that	  contain	  information	  only	  from	  current	  day	  or	  
with	  one	  extra	  day	  behind.	  	  2)	  Higher	  dimension	  feature	  set	  needed	  for	  better	  performance	  for	  more	  
number	  of	  days	  incorporated.	  (See	  figure,	  120	  features	  for	  0	  days,	  240	  features	  for	  1	  day,	  350	  features	  for	  
2	  days)	  
	  
It	  is	  interested	  that	  the	  chi-­‐squared	  and	  mutual	  information	  feature	  selection	  did	  not	  perform	  as	  well	  as	  
feature	  selection	  using	  frequent	  words.	  From	  our	  experiments,	  we	  find	  that	  the	  best	  performance	  came	  
from	  feature	  set	  generated	  from	  1	  year	  of	  data	  with	  frequent	  word	  feature	  selection.	  The	  test	  results	  are	  
summarized	  in	  the	  two	  tables	  below.	  
	  

	  

3	  
	  

Table	  4:	  Summary	  of	  statistics	  of	  MNB	  using	  1	  year	  of	  data	  

Table	  5:	  Summary	  of	  statistics	  of	  MNB	  using	  3	  years	  of	  data	  

	  
Supervised	  Learning	  2	  –	  Gaussian	  Discriminant	  Analysis	  
We	  then	  tried	  using	  GDA	  to	  compare	  performance.	  	  

	  

	  
Figure	  2:	  An	  example	  of	  test	  statistics	  trained	  using	  mutual	  information	  feature	  selection.	  
	  
Remarks:	  1)	  The	  covariance	  matrix	  rapidly	  becomes	  singular	  at	  higher	  feature	  spaces	  due	  to	  insufficient	  
training	  data.	  This	  occurs	  when	  the	  number	  of	  features	  is	  approximately	  equal	  to	  the	  number	  of	  training	  
data.	  2)	  Also,	  we	  observe	  that	  the	  training	  data	  gets	  fitted	  very	  well	  with	  increasing	  number	  of	  features,	  
perhaps	  leading	  to	  over-­‐fitting.	  3)	  We	  are	  able	  to	  obtain	  good	  accuracy	  (60%)	  on	  the	  test	  data	  set	  with	  low	  
number	  of	  features.	  Thus	  when	  it	  is	  expensive	  to	  collect	  features,	  the	  GDA	  presents	  itself	  as	  a	  good	  
alternative	  to	  MNB.	  	  
	  
The	  tables	  below	  summarize	  our	  findings	  with	  GDA.	  We	  see	  that	  in	  general	  the	  best	  performance	  comes	  
from	  number	  of	  features	  that	  are	  significantly	  lower	  than	  that	  required	  by	  the	  MNB.	  

	  

Table	  6:	  Summary	  of	  statistics	  of	  GDA	  using	  1	  year	  of	  data	  

	  

4	  

Table	  7:	  Summary	  of	  statistics	  of	  GDA	  using	  3	  years	  of	  data	  

	  
Supervised	  Learning	  3	  –	  Support	  Vector	  Machines	  
Unlike	  the	  MNB,	  there	  does	  not	  seem	  to	  be	  a	  clear	  trend	  in	  the	  results	  of	  the	  SVM.	  From	  the	  tables	  below,	  
we	  observe	  that	  we	  get	  the	  best	  results	  from	  the	  SVM	  using	  the	  mutual	  information	  feature	  selection.	  All	  
the	  results	  below	  were	  calculated	  using	  linear	  kernel.	  	  

	  

Table	  8:	  Summary	  of	  statistics	  of	  SVM	  using	  1	  year	  of	  data	  

	  

	  

Table	  9:	  Summary	  of	  statistics	  of	  SVM	  using	  3	  years	  of	  data	  

	  
Summary	  and	  Future	  Work	  
We	  compared	  the	  performance	  of	  3	  supervised	  classifiers	  on	  the	  Reuters	  Technology	  news	  section’s	  ability	  
to	  predict	  the	  stock	  movements	  of	  the	  NASDAQ	  composite.	  All	  3	  were	  able	  to	  perform	  better	  than	  
random,	  with	  SVM	  and	  NMB	  being	  able	  to	  perform	  better	  than	  65%	  accuracy	  under	  certain	  conditions	  of	  
feature	  selections,	  number	  of	  features	  and	  number	  of	  days	  of	  information	  included.	  Also,	  with	  the	  
dimensions	  learnt	  from	  the	  factor	  analysis,	  we	  can	  show	  convincingly	  that	  our	  learning	  algorithms	  did	  pick	  
up	  hidden	  trends	  in	  the	  data	  to	  aid	  in	  prediction.	  This	  showed	  that	  there	  is	  ability	  of	  news	  sentiments	  to	  
predict	  stock	  market	  movements	  in	  the	  imperfect	  market	  conditions	  we	  live	  in	  today.	  
	  
The	  next	  step	  would	  be	  to	  build	  a	  stronger	  classifier	  based	  on	  the	  3	  weak	  classifiers	  we	  presented	  in	  this	  
paper.	  Also,	  other	  classification	  techniques	  like	  random	  forests	  could	  be	  attempted	  as	  well.	  More	  
interestingly,	  we	  could	  incorporate	  news	  from	  other	  sections,	  to	  see	  which	  sections	  provide	  best	  
prediction	  capabilities	  for	  tomorrow’s	  stock	  price	  movements.	  
	  
Bibliography	  
1.	  Andrew	  Ng,	  CS	  229	  Machine	  Learning,	  Stanford	  University	  2012	  
	  
	  
	  
	  
	  
	  
	  

5	  

Appendix	  
Screenshots	  of	  Reuters	  Technology	  
Data	  scrapped	  from	  http://www.reuters.com/	  

	  

Example	  of	  Headline	  and	  story	  from	  Reuters	  Technology	  News	  
	  
Screenshots	  of	  Yahoo	  Finance	  
	  

	  

	  

	  

Example	  of	  screen	  shot	  from	  yahoo	  
finance.	  This	  shows	  the	  ticker	  for	  the	  
NASDAQ	  Composite.	  We	  also	  
experimented	  with	  others	  such	  as	  the	  
Dow	  Jones	  Industrial	  Average	  and	  the	  
Nikkei	  Index.	  However,	  it	  appears	  that	  
the	  news	  we	  were	  scraping	  (ie,	  
technology	  news)	  were	  better	  
predictors	  of	  the	  NASDAQ	  Composite	  
due	  to	  the	  large	  number	  of	  technology	  
companies	  in	  this	  stock	  market	  index.	  

	  

6	  


Semiparametric Principal Component Analysis

Fang Han
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21210
fhan@jhsph.edu

Han Liu
Department of Operations Research
and Financial Engineering
Princeton University, NJ 08544
hanliu@princeton.edu

Abstract
We propose two new principal component analysis methods in this paper utilizing
a semiparametric model. The according methods are named Copula Component
Analysis (COCA) and Copula PCA. The semiparametric model assumes that, af-
ter unspeciﬁed marginally monotone transformations, the distributions are multi-
variate Gaussian. The COCA and Copula PCA accordingly estimate the leading
eigenvectors of the correlation and covariance matrices of the latent Gaussian dis-
tribution. The robust nonparametric rank-based correlation coefﬁcient estimator,
Spearman’s rho, is exploited in estimation. We prove that, under suitable condi-
tions, although the marginal distributions can be arbitrarily continuous, the COCA
and Copula PCA estimators obtain fast estimation rates and are feature selection
consistent in the setting where the dimension is nearly exponentially large relative
to the sample size. Careful numerical experiments on the synthetic and real data
are conducted to back up the theoretical results. We also discuss the relationship
with the transelliptical component analysis proposed by Han and Liu (2012).

1 Introduction
The Principal Component Analysis (PCA) is introduced as follows. Given a random vector X ∈ Rd
with covariance matrix Σ and n independent observations of X , the PCA reduces the dimension of
the sample covariance S . By spectral decomposition, Σ = Pd
the data by projecting the data onto a linear subspace spanned by the k leading eigenvectors of Σ,
such that the principal modes of variations are preserved. In practice, Σ is unknown and replaced by
j with eigenvalues ω1 ≥
j=1 ωj uj uT
. . . ≥ ωd and the corresponding orthornormal eigenvectors u1 , . . . , ud . PCA aims at recovering the
ﬁrst k eigenvectors u1 , . . . , uk .
Although the PCA method as a procedure is model free, its theoretical and empirical performances
rely on the distributions. With regard to the empirical concern, the PCA’s geometric intuition is
coming from the major axes of the contours of constant probability of the Gaussian [10]. [5] show
that if X is multivariate Gaussian, then the distribution is centered about the principal component
axes and is therefore “self-consistent” [8]. We refer to [10] for more good properties that the PCA
enjoys under the Gaussian model, which we wish to preserve while designing its generalization.
mensional setting. Given bu1 the dominant eigenvector of S , [9] show that the angle between bu1
lim inf n→∞ E∠(bu1 , u1 ) > 0, where we denote by ∠(bu1 , u1 )
With regard to the theoretical concern, ﬁrstly, the PCA generally fails to be consistent in high di-
and u1 will not converge to 0, i.e.
the angle between the estimated and the true leading eigenvectors. This key observation motivates
0} and card(supp(u1 )) = s < n. The resulting estimator eu1 is:
regularizing Σ, resulting in a series of methods with different formulations and algorithms. The sta-
tistical model is generally further speciﬁed such that u1 is sparse, namely supp(u1 ) := {j : u1j 6=
eu1 = arg max
vT S v sub ject to kvk2 = 1, card(supp(v)) ≤ s.
v∈Rd
To solve Equation (1.1), a variety of algorithms are proposed: greedy algorithms [3], lasso-type
methods including SCoTLASS [11], SPCA [25] and sPCA-rSVD [19], a number of power methods
[12, 23, 16], the biconvex algorithm PMD [21] and the semideﬁnite relaxation DSPCA [4]. Sec-
ondly, it is realized that the distribution where the data are drawn from needs to be speciﬁed, such

(1.1)

1

that the estimator eu1 converges to ¯u1 in a fast rate. [9, 1, 16, 18, 20] all establish their results under
a strong Gaussian or sub-Gaussian assumption in order to obtain a fast rate under certain conditions.
In this paper, we ﬁrst explore the use of the PCA conducted on the correlation matrix Σ0 instead of
the covariance matrix Σ, and then propose a high dimensional semiparametric scale-invariant prin-
cipal component analysis method, named the Copula Component Analysis (COCA). In this paper,
the population version of the scale-invariant PCA is built as the estimator of the leading eigenvector
of the population correlation matrix Σ0 . Secondly, to handle the non-Gaussian data, we general-
ize the distribution family from the Gaussian to the larger Nonparanormal family [15]. A random
variable X = (X1 , . . . , Xd )T belongs to a Nonparanormal family if and only if there exists a set
of univariate monotone functions {f 0
j }d
j=1 such that (f 0
1 (X1 ), . . . , f 0
d (Xd ))T is multivariate Gaus-
the normal score transformation functions { bf 0
sian. The Nonparanormal can have arbitrary continuous marginal distributions and can be far away
from the sub-Gaussian family. Thirdly, to estimate Σ0 robustly and efﬁciently, instead of estimating
j }d
j=1 as [15] did, realizing that {f 0
j }d
j=1 preserve the
ranks of the data, we utilize the nonparametric correlation coefﬁcient estimator, Spearman’s rho, to
estimate Σ0 . [14, 22] prove that the corresponding estimators converge to Σ0 in a parametric rate.
In theory, we analyze the general case that X is following the Nonparanormal and θ1 is weakly
sparse, here θ1 is the leading eigenvector of Σ0 . We obtain the estimation consistency of the COCA
estimator to θ1 using the Spearman’s rho correlation coefﬁcient matrix. We prove that the estimation
consistency rates are close to the parametric rate under Gaussian assumption and the feature selec-
tion consistency can be achieved when d is nearly exponential to the sample size. In this paper, we
also propose a scale variant PCA procedure, named the Copula PCA. The Copula PCA estimates
the leading eigenvector of the latent covariance matrix Σ. To estimate the leading eigenvectors of
Σ, instead of Σ0 , in a fast rate, we prove that extra conditions are required on the transformation
functions.
2 Background
We start with notations: Let M = [Mjk ] ∈ Rd×d and v = (v1 , ..., vd )T ∈ Rd . Let v ’s subvector
with entries indexed by I be denoted by vI , M ’s submatrix with rows indexed by I and columns
deﬁne the ‘q and ‘∞ vector norm as kvkq := (Pd
indexed by J be denoted by MI J . Let MI · and M·J be the submatrix of M with rows in I and
all columns, and the submatrix of M with columns in J and all rows. For 0 < q ≤ ∞, we
Pn
i=1 |vi |q )1/q and kvk∞ := max1≤i≤d |vi |, and
kvk0 := card(supp(v)) · kvk2 . We deﬁne the matrix ‘max norm as the elementwise maximum value:
kM kmax := max{|Mij |} and the ‘∞ norm as kM k∞ := max1≤i≤m
j=1 |Mij |. Let Λj (M ) be
the toppest j−th eigenvalue of M. In special, Λmin (M ) := Λd (M ) and Λmax (M ) := Λ1 (M ) are
the smallest and largest eigenvalues of M . The vectorized matrix of M , denoted by vec(M ), is
deﬁned as: vec(M ) := (M T·1 , . . . , M T·d )T . Let Sd−1 := {v ∈ Rd : kvk2 = 1} be the d-dimensional
‘2 sphere. For any two vectors a, b ∈ Rd and any two squared matrices A, B ∈ Rd×d , denote the
inner product of a and b, A and B by ha, bi := aT b and hA, B i := Tr(AT B ).
Let Σ0 be the correlation matrix of Σ, and by spectral decomposition, Σ = Pd
2.1 The Models of the PCA and Scale-invariant PCA
Pd
j and Σ0 =
j=1 ωj uj uT
j . Here ω1 ≥ ω2 ≥ . . . ≥ ωd > 0 and λ1 ≥ λ2 ≥ . . . ≥ λd > 0 are the eigenvalues
j=1 λj θj θT
proposition claims that the estimators {bu1 , . . . , bud} and {bθ1 , . . . , bθd}, the eigenvectors of the sample
of Σ and Σ0 , with u1 , . . . , ud and θ1 , . . . , θd the corresponding orthonormal eigenvectors. The next
covariance and correlation matrices S and S 0 , are the MLEs of {u1 , . . . , ud} and {θ1 , . . . , θd}:
mators of PCA, {bu1 , . . . , bud}, and the estimators of the scale-invariant PCA, {bθ1 , . . . , bθd}, are the
Proposition 2.1. Let x1 . . . xn ∼ N (µ, Σ) and Σ0 be the correlation matrix of Σ. Then the esti-
MLEs of {u1 , . . . , ud} and {θ1 , . . . , θd}.
Proof. Use Theorem 11.3.1 in [2] and the functional invariance property of the MLE.
Proposition 2.2. For any 1 ≤ i ≤ d, we have supp(ui ) = supp(θi ) and sign(uij ) =
sign(θij ), ∀ 1 ≤ j ≤ d.
Proof. For 1 ≤ i ≤ d, ui = (θi1 /σ1 , θi2 /σ2 , . . . , θid/σd ), where (σ2
d )T := diag(Σ).
1 , . . . , σ2
It is easy to observe that the scale-invariant PCA is a safe procedure for dimension reduction when
variables are measured in different scales. Although there seems no theoretical advantage of scale-
invariant PCA over the PCA under the Gaussian model, in this paper we will show that under a more
general Nonparanormal (or Gaussian Copula) model, the scale-invariant PCA will pose much less
conditions to make the estimator achieve good theoretical performance.

2

2.2 The Nonparanormal
We ﬁrst introduce two deﬁnitions of the Nonparanormal separately deﬁned in [15] and [14].
Deﬁnition 2.1 [15]. A random variable X = (X1 , ..., Xd )T with population marginal means and
standard deviations µ = (µ1 , . . . , µd )T and σ = (σ1 . . . . , σd )T is said to follow a Nonparanormal
distribution N P Nd (µ, Σ, f ) if and only if there exists a set of univariate monotone transformations
f = {fj }d
j=1 such that: f (X ) = (f1 (X1 ), ..., fd (Xd ))T ∼ N (µ, Σ), and σ2
j = Σj j ,
j = 1, . . . , d.
Deﬁnition 2.2 [14]. Let f 0 = {f 0
j }d
j=1 be a set of monotone univariate functions and Σ0 ∈ Rd×d
be a positive deﬁnite correlation matrix with diag(Σ0 ) = 1. We say that a d dimensional random
variable X = (X1 , . . . , Xd )T follows a Nonparanormal distribution, i.e. X ∼ N P Nd (Σ0 , f 0 ), if
d (Xd ))T ∼ N (0, Σ0 ).
f 0 (X ) := (f 0
1 (X1 ), . . . , f 0
The following lemma proves that two deﬁnitions of the Nonparanormal are equivalent.
Lemma 2.1. A random variable X ∼ N P Nd (Σ0 , f 0 ) if and only if there exist µ = (µ1 , . . . , µd )T ,
Σ = [Σjk ] ∈ Rd×d such that for any 1 ≤ j, k ≤ d, E(Xj ) = µj , Var(Xj ) = Σj j and Σ0
jk =
Σjk√
, and a set of monotone univariate functions f = {fj }d
j=1 such that X ∼ N P Nd (µ, Σ, f ).
Σj j ·Σkk
j (x), for j ∈ {1, 2 . . . , d}.
Proof. Using the connection that fj (x) = µj + σj f 0
Lemma 2.1 guarantees that the Nonparanormal is deﬁned properly. Deﬁnition 2.2 is more appealing
because it emphasizes the correlation and hence matches the spirit of the Copula. However, Deﬁni-
tion 2.1 enjoys notational simplicity in analyzing the Copula-based LDA and PCA approaches.
Given n data points x1 , . . . , xn ∈ Rd , where xi = (xi1 , . . . , xid )T , we denote by bµj
q 1
2.3 Spearman’s rho Correlation and Covariance Matrices
Pn
Pn
i=1 (xij − bµj )2 , the marginal sample means and standard devi-
and bσj =
:=
1
i=1 xij
Pn
n
n
ations. Because the Nonparanormal distribution preserves the rank of the data, it is natural to use the
nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, to estimate the latent
Pn
we consider the following statistics: bρjk =
correlation. In detail, let rij be the rank of xij among x1j , . . . , xnj and ¯rj := 1
i=1 rij = n+1
2 ,
√Pn
i=1 (rij − ¯rj )2 ·Pn
n
i=1 (rij − ¯rj )(rik− ¯rk )
trix estimator: bRjk = 2 sin( π
6 bρjk ). The Lemma 2.2, coming from [14], claims that the estimation
i=1 (rik− ¯rk )2 , and the correlation ma-
can reach the parametric rate.
!
 
r log d
Lemma 2.2 ([14]). When x1 , . . . , xn ∼i.i.d N P Nd (Σ0 , f 0 ), for any n ≥ 21
log d + 2,
k bR − Σ0 kmax ≤ 8π
We denote by bR := [ bRjk ] the Spearman’s rho correlation coefﬁcient matrix. In the following let
≥ 1 − 2/d2 .
P
(2.1)
bS := [ bSjk ] = [bσj bσk bRjk ] be the Spearman’s rho covariance matrix.
n
3 Methods
(cid:16) 1
(cid:17)
In Figure 1, we randomly generate 10,000 samples from three different types of Nonparanormal
distributions. We suppose that X ∼ N P N2 (Σ0 , f 0 ). Here we set Σ0 =
0.5
and trans-
0.5
1
1 (x) = sign(x)x2 and
2 (x) = x1/3 ; (B) f 0
1 (x) = x3 and f 0
formation functions as follows: (A) f 0
2 (x) = Φ−1 (x). It can be observed that there does not exist a nice
1 (x) = f 0
2 (x) = x3 ; (C) f 0
f 0
geometric explanation now. For example, researchers might wish to conduct PCA separately on
different clusters in (A) and (B). For (C), the data look very noisy and a nice major axis might be
considered not existing.
However, under the Nonparanormal model and realizing that there is a latent Gaussian distribution
behind, the geometric intuition of the PCA naturally comes back. In the next section, we will present
the model of the COCA and Copula PCA motivated from this observation.
3.1 COCA Model
We ﬁrstly present the model of the Copula Component Analysis (COCA) method, where the idea
(
of scale-invariant PCA is exploited and we wish to estimate the leading eigenvector of the latent
correlation matrix. In particular, the following model M0 (q , Rq , Σ0 , f 0 ) is considered:
x1 , . . . , xn ∼i.i.d N P Nd (Σ0 , f 0 ),
M0 (q , Rq , Σ0 , f 0 ) :
θ1 ∈ Sd−1 ∩ Bq (Rq ),

(3.1)

3

A

B

C

Figure 1: Scatter plots of three Nonparanormals, X ∼ N P N2 (Σ0 , f 0 ). Here Σ0
12 = 0.5 and
1 (x) = x3 and f 0
2 (x) = x1/3 ; (B)
the transformation functions have the form as follows: (A) f 0
2 (x) = Φ−1 (x).
1 (x) = sign(x)x2 and f 0
2 (x) = x3 ; (C) f 0
1 (x) = f 0
f 0
where θ1 is the leading eigenvectors of the latent correlation matrix Σ0 we are interested in estimat-
ing, 0 ≤ q ≤ 1 and the ‘q ball Bq (Rq ) is deﬁned as:
B0 (R0 ) := {v ∈ Rd : card(supp(v)) ≤ R0 };
when q = 0,
(3.2)
Bq (Rq ) := {v ∈ Rd : kvkq
when 0 < q ≤ 1,
q ≤ Rq }.
Inspired by the model M0 (q , Rq , Σ0 , f 0 ), we consider the following COCA estimator eθ1 , which
(3.3)
maximizes the following equation with the constraint that eθ1 ∈ Bq (Rq ) for some 0 ≤ q ≤ 1:
eθ1 = arg max
vT bRv , sub ject to v ∈ Sd−1 ∩ Bq (Rq ).
(3.4)
Here bR is the estimated Spearman’s rho correlation coefﬁcient matrix. The corresponding COCA
v∈Rd
estimator eθ1 can be considered as a nonlinear dimensional reduction procedure and has the potential
to gain more ﬂexibility compared with the classical PCA. In Section 4 we will establish the theo-
retical results on the COCA estimator and will show that it can estimate the latent true dominant
eigenvector θ1 in a fast rate and can achieve feature selection consistency.
3.1.1 Copula PCA Model
In contrast, we provide another model inspired from the classical PCA method, where we wish to
(
estimate the leading eigenvector of the latent covariance matrix. In particular, the following model
M(q , Rq , Σ, f ) is considered:

x1 , . . . , xn ∼i.i.d N P Nd (0, Σ, f ),
M(q , Rq , Σ, f ) :
u1 ∈ Sd−1 ∩ Bq (Rq ),
where u1 is the leading eigenvector of the covariance matrix Σ and it is what we are interested in
vT bS v , sub ject to v ∈ Sd−1 ∩ Bq (Rq ),
eu1 = arg max
estimating. The corresponding Copula PCA estimator is:
(3.6)
where bS is the Spearman’s rho covariance coefﬁcient matrix. This procedure is named the Copula
v∈Rd
COCA to make eu1 converge to u1 in a fast rate.
PCA. In Section 4, we will show that the Copula PCA requires a much stronger condition than
3.2 Algorithms
covariance matrices bR and bS can be directly plugged in to obtain sparse estimators.
In this section we provide three sparse PCA algorithms, where the Spearman’s rho correlation and
convex optimization algorithm to the following problem: arg maxu,v uT bΓv ,
Penalized Matrix Decomposition (PMD) is proposed by [21]. The main idea of the PMD is a bi-
2 ≤
sub ject to kuk2
the following: (1) Input: A symmetric matrix bΓ. Initialize v ∈ Sd−1 ; (2) Iterate until convergence:
1, kvk2
2 ≤ 1, kuk1 ≤ δ, kvk1 ≤ δ. The COCA with PMD and Copula PCA with PMD are listed in
(a) u ← arg maxu∈Rd uT bΓv subject to kuk1 ≤ δ and kuk2
2 ≤ 1.(b) v ← arg maxv∈Rd uT bΓv subject
2 ≤ 1; (3) Output: v . Here bΓ is either bR or bS , corresponding to the COCA with
to kvk1 ≤ δ and kvk2
PMD and Copula PCA with PMD. δ is the tuning parameter. [21] suggest using the ﬁrst leading

(3.5)

4

−1.5−1.0−0.50.00.51.01.5−40−2002040−2−1012−1.5−1.0−0.50.00.51.01.50.00.20.40.60.81.00.00.20.40.60.81.0eigenvector of bΓ to be the initial value of v . The PMD can be considered as a solver to Equation
(3.4) and Equation (3.6) with q = 1.
The SPCA algorithm is proposed by [25]. The main idea of the SPCA algorithm is to exploit a
(1) Input: A symmetric matrix bΓ. Initialize u ∈ Sd−1 . (2). Iterate until convergence: (a) v ←
regression approach to PCA and then utilize lasso and elastic net [24] to calculate a sparse estimator
to the leading eigenvector. The COCA with SPCA and Copula PCA with SPCA are listed as follows:
arg minv∈Rd (u − v)T bΓ(u − v) + δ1 kvk2
2 + δ2 kvk1 ; (b) u ← bΓv/kbΓvk2 . (3) Output: v/kvk2 . Here
bΓ is either bR or bS , corresponding to the COCA with SPCA and Copula PCA with SPCA. δ1 ∈ R
and δ2 ∈ R are two tuning parameters. [25] suggest using the ﬁrst leading eigenvector of bΓ to be the
initial value of v . The SPCA can be considered as a solver to Equations (3.4) and (3.6) with q = 1.
The Truncated Power method (TPower) is proposed by [23]. The main idea is to utilize the power
method, but truncate the vector to a ‘0 ball in each iteration. Actually, TPower can be generalized
to a family of algorithms to solve Equation (3.4) when 0 ≤ q ≤ 1. We name it the ‘q Constraint
Truncated Power Method (qTPM). Especially, when q = 0, the algorithm qTPM coincides with
[23]’s method. The TPower can be considered as a general solver to Equation (3.4) and Equation
(3.6) with q ∈ [0, 1]. In detail, we utilize the classical power method, but in each iteration t we
project the intermediate vector xt to the intersection of the d-dimension sphere Sd−1 and the ‘q ball
with the radius R1/q
. Detailed algorithms are presented in the long version of this paper [6].
q
4 Theoretical Properties
In this section we provide the theoretical properties of the COCA and Copula PCA methods. Espe-
cially, we are interested in the high dimensional case when d > n.
This section is devoted to the statement of our result on quantifying the convergence rate of bR to Σ0
4.1 Rank-based Correlation and Covariance Matrices Estimation
and bS to Σ. In particular, we establish the results on the ‘max convergence rates of the Spearman’s
PCA, however, we still need to quantify the convergence rate of bS to Σ.
rho correlation and covariance matrices to Σ and Σ0 . For COCA, Lemma 2.2 is enough. For Copula
Deﬁnition 4.1 Subgaussian Transformation Function Class. Let Z ∈ R be a random variable
following the standard Gaussian distribution. The Subgaussian Transformation Function Class
TF(K ) is deﬁned as the set of functions {g0 : R → R} which satisﬁes that: E|g0 (Z )|m ≤
2 K m , ∀ m ∈ Z+ .
m!
Here it is easy to see that for any function g0 : R → R, if there exists a constant L < ∞ such that
g0 (z ) ≤ L or g 0
0 (z ) ≤ L or g 00
0 (z ) ≤ L, ∀ z ∈ R, then g0 ∈ TF(K ) for some constant K.
Then we have the following result, which states that Σ can also be recovered in the parametric rate.
{σj } < c0 <
Lemma 4.1. When x1 , . . . , xn ∼i.i.d N P Nd (µ, Σ, f ), 0 < 1/c0 < min
{σj } < max
j
j
∞, for some constant c0 and g := {gj = f −1
j }d
j ∈ T F (K )
j=1 satisﬁes for all j = 1, . . . , K , g2
P(| bSjk − Σjk | > t) ≤ 2 exp(−c1nt2 ),
where K < ∞ is some constant, we have for any 1 ≤ j, k ≤ d, for any n ≥ 21
log d + 2,
where c1 is a constant only depending on the choice of K .
Remark 4.1. The Lemma 4.1 claims that, under certain constraint on the transformation functions,
the latent covariance matrix Σ can be recovered using the Spearman’s rho covariance matrix. How-
ever, in this case, the marginal distributions of the Nonparanormal are required to be sub-gaussian
and cannot be arbitrarily continuous. This makes the Copula PCA a less favored method.
4.2 COCA and Copula PCA
This section is devoted to the statement of our main result on the upper bound of the estimated error
Theorem 4.1 (Upper bound for the COCA). Let eθ1 be the global solution to Equation (3.4) and
of the COCA estimator and Copula PCA estimator.
| sin ∠(v1 , v2 )| = p1 − (vT
the Model M0 (q , Rq , Σ0 , f 0 ) holds. For any two vectors v1 ∈ Sd−1 and v2 ∈ Sd−1 , let
 
2 !
(cid:18) 64π2
(cid:19) 2−q
1 v2 )2 , then we have, for any n ≥ 21
log d + 2,
sin2 ∠(eθ1 , θ1 ) ≤ γqR2
(λ1 − λ2 )2 · log d
P
q
n

≥ 1 − 1/d2 ,

(4.1)

(4.2)

5

√
Proof. The key idea of the proof is to utilize the ‘max norm convergence result of bR to Σ0 . Detailed
where γq = 2 · I (q = 1) + 4 · I (q = 0) + (1 +
3)2 · I (0 < q < 1).
(cid:16)
n )1−q/2(cid:17)
proofs are presented in the long version of this paper [6].
( log d
Generally, when Rq and λ1 , λ2 do not scale with (n, d), the rate is OP
, which is the
(cid:17) 2−q
(cid:16) 64π2 λ2
parametric rate [16, 20, 18] obtain. When (n, d) goes to inﬁnity, the two dominant eigenvalues λ1
and λ2 will typically go to inﬁnity and will at least be away from zero. Hence, our rate shown in
(λ1−λ2 )2 · log d
2
Equation (4.2) is better than the seemingly more state-of-art rate: γqR2
1
.
q
n
The COCA is signiﬁcantly different from [20] and [18]’s results in the sense that: (1) In theory,
(2) In methodology, we utilize the Spearman’s rho correlation coefﬁcient matrix bR to estimate Σ0 ,
the Nonparanormal family can have arbitrary continuous marginal distributions, where a fast rate
cannot be obtained using the techniques built for either Gaussian or sub-Gaussian distributions;
instead of using the sample correlation matrix S 0 . This procedure has been shown to lose little in
rate and will be much more robust under the Nonparanormal model. Given Theorem 4.1, we can
Corollary 4.1 (Feature Selection Consistency of the COCA). Let eθ1 be the global solution to
immediately obtain a feature selection consistency result.
bΘ0 :=
q log d
supp(eθ1 ). If we further have minj∈Θ0 |θ1j | ≥ 16
Equation (3.4) and the Model M0 (0, R0 , Σ0 , f 0 ) holds. Let Θ0 := supp(θ1 )
and
√
P( bΘ0 = Θ0 ) ≥ 1 − 1/d2 .
n , then for any n ≥ 21/ log d + 2,
2R0 π
λ1−λ2
Similarly, we can give an upper bound for the estimation rate of the Copula PCA to the true leading
Theorem 4.2 (Upper bound for Copula PCA). Let eu1 be the global solution to Equation (3.6) and
eigenvalue u1 of the latent covariance matrix Σ. The next theorem provides the detail result.
the Model M(q , Rq , Σ, f ) holds. If g := {gj = f −1
j }d
j ∈ T F (K ) for all 1 ≤ j ≤ d,
j=1 satisﬁes g2
{σj } < max
{σj } < c0 < ∞, then we have, for any n ≥ 21/ log d + 2,
 
2 !
(cid:19) 2−q
(cid:18)
and 0 < 1/c0 < min
j
j
sin2 ∠(eu1 , u1 ) ≤ γqR2
c1 (ω1 − ω2 )2 · log d
4
q
n
√
where γq = 2 · I (q = 1) + 4 · I (q = 0) + (1 +
3)2 · I (0 < q < 1) and c1 is a constant deﬁned in
Corollary 4.2 (Feature Selection Consistency of the Copula PCA). Let eu1 be the global solution
Equation (4.1), only depending on K .
to Equation (3.6) and the Model M(0, R0 , Σ, f ) holds. Let Θ := supp(u1 ) and bΘ := supp(eu1 ).
q log d
j ∈ T F (K ) for all 1 ≤ j ≤ d, and 0 < 1/c0 < minj {σj } <
If g := {gj = f −1
j }d
j=1 satisﬁes g2
√
log d + 2, P( bΘ = Θ) ≥ 1 − 1
maxj {σj } < c0 < ∞, and we further have minj∈Θ |u1j | ≥
√
n , then for any
4
2R0
c1 (ω1−ω2 )
n ≥ 21
d2 .
5 Experiments
In this section we investigate the empirical usefulness of the COCA method. Three sparse PCA
algorithms are considered: PMD proposed by [21], SPCA proposed by [25] and Truncated Power
method (TPower) proposed by [23]. The following three methods are considered: (1) Pearson:
the classic high dimensional PCA using the Pearson sample correlation matrix; (2) Spearman:
the COCA using the Spearman’s rho correlation coefﬁcient matrix; (3) Oracle: the classic high
dimensional PCA using the Pearson sample correlation matrix of the data from the latent Gaussian
(perfect without contaminations).
5.1 Numerical Simulations
In the simulation study we randomly sample n data points x1 , . . . , xn from the Nonparanormal
distribution X ∼ N P Nd (Σ0 , f 0 ). Here we consider the setup of d = 100. We follow the
same generating scheme as in [19, 23] and [7]. A covariance matrix Σ is ﬁrstly synthesized
through the eigenvalue decomposition, where the ﬁrst two eigenvalues are given and the corre-
sponding eigenvectors are pre-speciﬁed to be sparse. In detail, we suppose that the ﬁrst two dom-
√
inant eigenvectors of Σ, u1 and u2 , are sparse in the sense that only the ﬁrst s = 10 entries of
u1 and the second s = 10 entries of u2 are nonzero and set to be 1/
10. ω1 = 5, ω2 = 2,

≥ 1 − 1/d2 ,

P

6

ω3 = . . . = ωd = 1. The remaining eigenvectors are chosen arbitrarily. The correlation matrix
Σ0 is accordingly generated from Σ, with λ1 = 4, λ2 = 2.5, λ3 , . . . , λd ≤ 1 and the two domi-
nant eigenvectors sparse. To sample data from the Nonparanormal, we also need the transformation
functions: f 0 = {f 0
j }d
j=1 . Here two types of transformation functions are considered: (1) Linear
linear = {h0 , h0 , . . . , h0 }, where h0 (x) := x; (2)
transformation (or no transformation): f 0
Nonlinear transformation: there exist ﬁve univariate monotone functions h1 , h2 , . . . , h5 : R → R
Φ(x)−R Φ(t)φ(t)dt
nonlinear = {h1 , h2 , h3 , h4 , h5 , h1 , h2 , h3 , h4 , h5 , . . .}, where h−1
h−1
1 (x) := x,
2 (x) :=
and f 0
√R |t|φ(t)dt
x3√R t6 φ(t)dt
√R (Φ(y)−R Φ(t)φ(t)dt)2 φ(y)dy
sign(x)|x|1/2
, h−1
h−1
, h−1
exp(x)−R exp(t)φ(t)dt
3 (x) :=
4 (x) :=
5 (x) :=
,
√R (exp(y)−R exp(t)φ(t)dt)2 φ(y)dy
. Here φ and Φ are deﬁned to be the probability density and cu-
mulative distribution functions of the standard Gaussian. h1 , . . . , h5 are deﬁned such that for any
Z ∼ N (0, 1), E(h−1
j (Z )) = 1 ∀ j ∈ {1, . . . , 5}. We then generate
j (Z )) = 0 and Var(h−1
n = 100, 200 or 500 data points from:
linear = {h0 , h0 , . . . , h0 } and Σ0 is deﬁned as above.
[Scheme 1] X ∼ N P Nd (Σ0 , f 0
linear ) where f 0
[Scheme 2] X ∼ N P Nd (Σ0 , f 0
nonlinear = {h1 , h2 , h3 , h4 , h5 , . . .}.
nonlinear ) where f 0
To evaluate the robustness of different methods, we adopt a similar data contamination procedure as
in [14]. Let r ∈ [0, 1) represents the proportion of samples being contaminated. For each dimension,
we randomly select bnrc entries and replace them with either 5 or -5 with equal probability. The
employed on X to computer the estimated leading eigenvector eθ1 .
ﬁnal data matrix we obtained is X ∈ Rn×d . The PMD, SPCA and TPower algorithms are then
Under the Scheme 1 and Scheme 2 with different levels of contamination (r = 0 or 0.05), we
repeatedly generate the data matrix X for 1,000 times and compute the averaged False Positive Rates
and False Negative Rates using a path of tuning parameters δ . The feature selection performances
of different methods are then evaluated. The corresponding ROC curves are presented in Figure 2.
More quantitative results are provided in the long version of this paper [6]. It can be observed that
when r = 0 and X is exactly Gaussian, Pearson,Spearman and Oracle can all recover the sparsity
pattern perfectly. However, when r > 0, the performances of Pearson signiﬁcantly decrease, while
Spearman is still very close to the Oracle. In Scheme 2, even when r = 0, Pearson cannot recover
the support set of θ1 , while Spearman can still recover the sparsity pattern almost perfectly. When
r > 0, the performance of Spearman is still very close to the Oracle.

r = 0

r = 0.05

r = 0

r = 0.05

r = 0

r = 0.05

Figure 2: ROC curves for the PMD, SPCA and Truncated Power method (the left two, the middle
two, the right two) with linear (no) and nonlinear transformation (top, bottom) and data contamina-
tion at different levels (r = 0, 0.05). Here n = 100 and d = 100.
5.2 Large-scale Genomic Data Analysis
In this section we investigate the performance of Spearman compared with the Pearson using
one of the largest microarray datasets [17]. In summary, we collect in all 13,182 publicly available
microarray samples from Affymetrixs HGU133a platform. The raw data contain 20,248 probes and
13,182 samples belonging to 2,711 tissue types (e.g., lung cancers, prostate cancer, brain tumor etc.).
There are at most 1,599 samples and at least 1 sample belonging to each tissue type. We merge the
probes corresponding to the same gene. There are remaining 12,713 genes and 13,182 samples. This
dataset is non-Gaussian (see the long version of this paper [6]). The main purpose of this experiment
is to compare the performance of the COCA with the classical high dimensional PCA. We utilize the
Truncated Power method proposed by [23] to achieve the sparse estimated dominant eigenvectors.

7

0.00.20.40.60.81.00.00.20.40.60.81.0PMDFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0PMDFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0SPCAFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0SPCAFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0TPowerFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0TPowerFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0PMDFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0PMDFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0SPCAFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0SPCAFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0TPowerFPRTPRPearsonSpearmanOracle0.00.20.40.60.81.00.00.20.40.60.81.0TPowerFPRTPRPearsonSpearmanOracleWe adopt the same idea of data-preprocessing as in [14]. In particular, we ﬁrstly remove the batch
effect by applying the surrogate variable analysis proposed by [13]. We then extract the top 2,000
genes with the highest marginal standard deviations. There are, accordingly, 2,000 genes left and the
data matrix we are focusing is 2, 000 × 13, 182. We then explore several tissue types with the largest
sample size: (1) Breast tumor, 1,599 samples; (2) B cell lymphoma, 213 samples; (3) Prostate tumor,
148 samples; (4) Wilms tumor, 143 samples.

Figure 3: The scatter plots of the ﬁrst two principal components of the dataset. The Spearman
versus Pearson are compared (top to bottom). b cell lymphoma, breast tumor, prostate tumor and
Wilms tumor are explored (from left to right). Each black point represents a sample and each red
point represents a sample belonging to the corresponding tissue type.
For each tissue type listed above, we apply the COCA (Spearman) and the classic high dimensional
PCA (Pearson) on the data belonging to this speciﬁc tissue type and obtain the ﬁrst two dominant
sparse eigenvectors. Here we set R0 = 100 for both eigenvectors. For COCA, we do a normal score
transformation on the original dataset. We subsequently project the whole dataset to the ﬁrst two
principal components using the obtained eigenvectors. The according 2-dimension visualization is
illustrated in Figure 3. In Figure 3 each black point represents a sample and each red point represents
a sample belonging to the corresponding tissue type. It can be observed that, in 2D plots learnt by
the COCA, the red points are averagely more dense and more close to the border of the sample
cluster. The ﬁrst phenomenon indicates that the COCA has the potential to preserve more common
information shared by samples from the same tissue type. The second phenomenon indicates that
the COCA has the potential to differentiate samples from different tissue types more efﬁciently.
6 Discussion and Comparison with Related Work
A similar principal component analysis procedure is proposed by [7], in which they advocate the
use of the transformed Kendall’s tau correlation matrix (instead of the Spearman’s rho correlation
matrix as in the current paper) for estimating the sparse leading eigenvectors. Though both papers
are working on principal component analysis, the core ideas are quite different: Firstly, the analy-
sis in [7] is based on a different distribution family called transelliptical, while COCA and Copula
PCA are based on the Nonparanormal family. Secondly, by improving the modeling ﬂexibility, in
[7] there does not exist a scale-variant variant since it is hard to quantify the transformation func-
tions. In contrast, by introducing the subgaussian transformation function family, the current paper
provides sufﬁcient conditions for Copula PCA to achieve parametric rates. Thirdly, the method in
[7] cannot explicitly conduct data visualization, due to the fact that the latent elliptical distribution
is unspeciﬁed and accordingly they cannot accurately estimate the marginal transformations. For
Copula PCA, we are able to provide the projection visualization such as in the experiment part of
this paper. Moreover, via quantifying a sharp convergence rate in estimating the marginal transfor-
mations, we can provide the convergence rates in estimating the principal components. Due to space
limit, we refer to the longer version of this paper [6] for more details. Finally, we recommend using
the Spearman’s rho instead of the Kendall’s tau in estimating the correlation coefﬁcients provided
that the Nonparanormal model holds. This is because Spearman’s rho is statistically more efﬁcient
than Kendall’tau within the Nonparanormal family. This research was supported by NSF award
IIS-1116730.

8

−20−15−10−5051015−1001020b cell lymphomaPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−20−15−10−5051015−1001020breast tumorPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−15−10−50510−10−50510prostate tumorPrincipal Component 1Principal Component 2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1001020−10−505101520wilms tumorPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024−4−202b cell lymphomaPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024−4−202breast tumorPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−202−4−2024prostate tumorPrincipal Component 1Principal Component 2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−20246−4−2024wilms tumorPrincipal Component 1Principal Component 2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllReferences
[1] A.A. Amini and M.J. Wainwright. High-dimensional analysis of semideﬁnite relaxations for
In Information Theory, 2008. ISIT 2008. IEEE International
sparse principal components.
Symposium on, pages 2454–2458. IEEE, 2008.
[2] T.W Anderson. An introduction to multivariate statistical analysis, volume 2. Wiley New
York, 1958.
[3] A. d’Aspremont, F. Bach, and L.E. Ghaoui. Optimal solutions for sparse principal component
analysis. The Journal of Machine Learning Research, 9:1269–1294, 2008.
[4] A. d’Aspremont, L. El Ghaoui, M.I. Jordan, and G.R.G. Lanckriet. A direct formulation for
sparse PCA using semideﬁnite programming. Computer Science Division, University of Cali-
fornia, 2004.
[5] B. Flury. A ﬁrst course in multivariate statistics. Springer Verlag, 1997.
[6] F. Han and H. Liu. High dimensional semiparametric scale-invariant principal component
analysis. Technical Report, 2012.
[7] F. Han and H. Liu. Tca: Transelliptical principal component analysis for high dimensional
non-gaussian data. Technical Report, 2012.
[8] T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association,
pages 502–516, 1989.
[9] I.M. Johnstone and A.Y. Lu. On consistency and sparsity for principal components analysis in
high dimensions. Journal of the American Statistical Association, 104(486):682–693, 2009.
[10] I.T. Jolliffe. Principal component analysis, volume 2. Wiley Online Library, 2002.
[11] I.T. Jolliffe, N.T. Trendaﬁlov, and M. Uddin. A modiﬁed principal component technique based
on the lasso. Journal of Computational and Graphical Statistics, 12(3):531–547, 2003.
[12] M. Journ ´ee, Y. Nesterov, P. Richt ´arik, and R. Sepulchre. Generalized power method for sparse
principal component analysis. The Journal of Machine Learning Research, 11:517–553, 2010.
[13] J.T. Leek and J.D. Storey. Capturing heterogeneity in gene expression studies by surrogate
variable analysis. PLoS Genetics, 3(9):e161, 2007.
[14] H. Liu, F. Han, M. Yuan, J. Lafferty, and L. Wasserman. High dimensional semiparametric
gaussian copula graphical models. Annals of Statistics, 2012.
[15] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high
dimensional undirected graphs. The Journal of Machine Learning Research, 10:2295–2328,
2009.
[16] Z. Ma. Sparse principal component analysis and iterative thresholding. Arxiv preprint
arXiv:1112.2432, 2011.
[17] Matthew McCall, Benjamin Bolstad, and Rafael Irizarry. Frozen robust multiarray analysis
(frma). Biostatistics, 11:242–253, 2010.
[18] D. Paul and I.M. Johnstone. Augmented sparse principal component analysis for high dimen-
sional data. Arxiv preprint arXiv:1202.1242, 2012.
[19] H. Shen and J.Z. Huang. Sparse principal component analysis via regularized low rank matrix
approximation. Journal of multivariate analysis, 99(6):1015–1034, 2008.
[20] V.Q. Vu and J. Lei. Minimax rates of estimation for sparse pca in high dimensions. Arxiv
preprint arXiv:1202.0786, 2012.
[21] D.M. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with ap-
plications to sparse principal components and canonical correlation analysis. Biostatistics,
10(3):515–534, 2009.
[22] L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal
graphical models. Annals of Statistics, 2012.
[23] X.T. Yuan and T. Zhang. Truncated power method for sparse eigenvalue problems. Arxiv
preprint arXiv:1112.2679, 2011.
[24] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.
[25] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of compu-
tational and graphical statistics, 15(2):265–286, 2006.

9


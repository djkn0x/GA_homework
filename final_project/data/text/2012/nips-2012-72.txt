A Scalable CUR Matrix Decomposition Algorithm:
Lower Time Complexity and Tighter Bound

Shusen Wang and Zhihua Zhang
College of Computer Science & Technology
Zhejiang University
Hangzhou, China 310027
{wss,zhzhang}@zju.edu.cn

Abstract

The CUR matrix decomposition is an important extension of Nystr ¨om approxima-
tion to a general matrix. It approximates any data matrix in terms of a small num-
ber of its columns and rows. In this paper we propose a novel randomized CUR
algorithm with an expected relative-error bound. The proposed algorithm has the
advantages over the existing relative-error CUR algorithms that it possesses tighter
theoretical bound and lower time complexity, and that it can avoid maintaining the
whole data matrix in main memory. Finally, experiments on several real-world
datasets demonstrate signiﬁcant improvement over the existing relative-error al-
gorithms.

1 Introduction

Large-scale matrices emerging from stocks, genomes, web documents, web images and videos ev-
eryday bring new challenges in modern data analysis. Most efforts have been focused on manipu-
lating, understanding and interpreting large-scale data matrices. In many cases, matrix factorization
methods are employed to construct compressed and informative representations to facilitate com-
putation and interpretation. A principled approach is the truncated singular value decomposition
(SVD) which ﬁnds the best low-rank approximation of a data matrix. Applications of SVD such as
eigenface [20, 21] and latent semantic analysis [4] have been illustrated to be very successful.
However, the basis vectors resulting from SVD have little concrete meaning, which makes it very
√
difﬁcult for us to understand and interpret the data in question. An example in [10, 19] has well
shown this viewpoint; that is, the vector [(1/2)age − (1/
2)height + (1/2)income], the sum of the
signiﬁcant uncorrelated features from a dataset of people’s features, is not particularly informative.
The authors of [17] have also claimed: “it would be interesting to try to ﬁnd basis vectors for all
experiment vectors, using actual experiment vectors and not artiﬁcial bases that offer little insight.”
Therefore, it is of great interest to represent a data matrix in terms of a small number of actual
columns and/or actual rows of the matrix.
The CUR matrix decomposition provides such techniques, and it has been shown to be very useful
in high dimensional data analysis [19]. Given a matrix A, the CUR technique selects a subset of
columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and
computes a matrix U such that ~A = CUR best approximates A. The typical CUR algorithms [7,
8, 10] work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2
does row selection from A and C simultaneously. Thus Stage 2 is more complicated than Stage 1.
The CUR matrix decomposition problem is widely studied in the literature [7, 8, 9, 10, 12, 13, 16,
18, 19, 22]. Perhaps the most widely known work on the CUR problem is [10], in which the authors
devised a randomized CUR algorithm called the subspace sampling algorithm. Particularly, the
algorithm has (1 + ϵ) relative-error ratio with high probability (w.h.p.).

1

Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be
chosen. For example, for an m × n matrix A and a target rank k ≤ min{m, n}, the state-of-
the-art CUR algorithm — the subspace sampling algorithm in [10] — requires exactly O(k4 ϵ
−6 )
rows or O(kϵ
−4 log2 k) rows in expectation to achieve (1 + ϵ) relative-error ratio w.h.p. Moreover,
the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is,
O(min{mn2 , nm2 }).1 The algorithms are therefore impractical for large-scale matrices.
In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory
and experiments. In particular, we show in Theorem 5 a novel randomized CUR algorithm with
lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR
algorithm in [10].
The rest of this paper is organized as follows. Section 3 introduces several existing column selection
algorithms and the state-of-the-art CUR algorithm. Section 4 describes and analyzes our novel
CUR algorithm. Section 5 empirically compares our proposed algorithm with the state-of-the-art
algorithm.
2 Notations
∑
∑
For a matrix A = [aij ] ∈ Rm×n , let a(i) be its i-th row and aj be its j -th column. Let ∥A∥1 =
|aij | be the ℓ1 -norm, ∥A∥F = (
ij )1/2 be the Frobenius norm, and ∥A∥2 be the spectral
∑
i,j a2
norm. Moreover, let Im denote an m × m identity matrix, and 0mn denotes an m × n zero matrix.
i,j
ρ
A,k + UA,k⊥(cid:6)A,k⊥VT
A,k⊥ be the
Let A = UA(cid:6)AVT
A,i = UA,k(cid:6)A,kVT
i=0 σA,iuA,ivT
A =
SVD of A, where ρ = rank(A), and UA,k , (cid:6)A,k , and VA,k correspond to the top k singular values.
−1
†
We denote Ak = UA,k(cid:6)A,kVT
A,k . Furthermore, let A
A,ρ be the Moore-Penrose
A,ρVT
= UA,ρ(cid:6)
inverse of A [1].

3 Related Work

Section 3.1 introduces several relative-error column selection algorithms related to this work. Sec-
tion 3.2 describes the state-of-the-art CUR algorithm in [10]. Section 3.3 discusses the connection
between the column selection problem and the CUR problem.

3.1 Relative-Error Column Selection Algorithms
Given a matrix A ∈ Rm×n , column selection is a problem of selecting c columns of A to construct
C ∈ Rm×c to minimize ∥A − CC
A∥F . Since there are (n
†
c ) possible choices of constructing C,
so selecting the best subset is a hard problem. In recent years, many polynomial-time approximate
algorithms have been proposed, among which we are particularly interested in the algorithms with
relative-error bounds; that is, with c ≥ k columns selected from A, there is a constant η such that
∥A − CC
A∥F ≤ η∥A − Ak ∥F .
†
We call η the relative-error ratio. We now present some recent results related to this work.
We ﬁrst introduce a recently developed deterministic algorithm called the dual set sparsiﬁcation
proposed in [2, 3]. We show their results in Lemma 1. Furthermore, this algorithm is a building
block of some more powerful algorithms (e.g., Lemma 2), and our novel CUR algorithm also relies
on this algorithm. We attach the algorithm in Appendix A.
Lemma 1 (Column Selection via Dual Set Sparsiﬁcation Algorithm). Given a matrix A ∈ Rm×n
√
(cid:13)(cid:13)(cid:13)A − Ak
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)A − CC
(cid:13)(cid:13)(cid:13)
of rank ρ and a target rank k (< ρ), there exists a deterministic algorithm to select c (> k) columns
of A and form a matrix C ∈ Rm×c such that
(1 − √
≤
†
1
1 +
A
k/c)2
F
F
1Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time,
they are all numerical unstable. See [15] for more discussions.

.

2

Moreover, the matrix C can be computed in TVA;k +O(mn+ nck2 ), where TVA;k is the time needed
to compute the top k right singular vectors of A.

There are also a variety of randomized column selection algorithms achieving relative-error bounds
in the literature: [3, 5, 6, 10, 14].
An randomized algorithm in [2] selects only c = 2k
ϵ (1 + o(1)) columns to achieve the expected
relative-error ratio (1 + ϵ). The algorithm is based on the approximate SVD via random projec-
tion [15], the dual set sparsiﬁcation algorithm [2], and the adaptive sampling algorithm [6]. Here we
present the main results of this algorithm in Lemma 2. Our proposed CUR algorithm is motivated
by and relies on this algorithm.
Lemma 2 (Near-Optimal Column Selection Algorithm). Given a matrix A ∈ Rm×n of rank ρ, a
(
)
target rank k (2 ≤ k < ρ), and 0 < ϵ < 1, there exists a randomized algorithm to select at most
2k
1 + o(1)
c =
ϵ
columns of A to form a matrix C ∈ Rm×c such that
E2 ∥A − CC
A∥F ≤ E∥A − CC
A∥2
≤ (1 + ϵ)∥A − Ak ∥2
†
†
F ,
F
where the expectations are taken w.r.t. C. Furthermore, the matrix C can be computed in O((mnk+
−2/3 ).
nk3 )ϵ

3.2 The Subspace Sampling CUR Algorithm

Drineas et al. [10] proposed a two-stage randomized CUR algorithm which has a relative-error
bound w.h.p. Given a matrix A ∈ Rm×n and a target rank k , in the ﬁrst stage the algorithm
chooses exactly c = O(k2 ϵ
−1 ) columns (or c = O(kϵ
−2 log δ
−2 log k log δ
−1 ) in expectation) of
A to construct C ∈ Rm×c ; in the second stage it chooses exactly r = O(c2 ϵ
−1 ) rows (or
−2 log δ
r = O(cϵ
−2 log c log δ
−1 ) in expectation) of A and C simultaneously to construct R and U. With
probability at least 1 − δ , the relative-error ratio is 1 + ϵ. The computational cost is dominated by
the truncated SVD of A and C.
Though the algorithm is ϵ-optimal with high probability, it requires too many rows get chosen: at
least r = O(kϵ
−4 log2 k) rows in expectation. In this paper we seek to devise an algorithm with
mild requirement on column and row numbers.

3.3 Connection between Column Selection and CUR Matrix Decomposition

The CUR problem has a close connection with the column selection problem. As aforementioned,
the ﬁrst stage of existing CUR algorithms is simply a column selection procedure. However, the
If the second stage is na¨ıvely solved by a column selection
second stage is more complicated.
algorithm on AT , then the error ratio will be at least (2 + ϵ).
For a relative-error CUR algorithm, the ﬁrst stage seeks to bound a construction error ratio of
∥A−CC
A∥F
∥A−CC
R∥F
y
y
y
AR
, while the section stage seeks to bound
given C. Actually, the ﬁrst
∥A−Ak ∥F
∥A−CCyA∥F
stage is a special case of the second stage where C = Ak . Given a matrix A, if an algorithm solv-
≤ η , then this algorithm also solves the
R∥F
∥A−CC
y
y
AR
ing the second stage results in a bound
∥A−CCyA∥F
column selection problem for AT with an η relative-error ratio. Thus the second stage of CUR is a
generalization of the column selection problem.

4 Main Results

In this section we introduce our proposed CUR algorithm. We call it the fast CUR algorithm because
it has lower time complexity compared with SVD. We describe it in Algorithm 1 and give a theoret-
ical analysis in Theorem 5. Theorem 5 relies on Lemma 2 and Theorem 4, and Theorem 4 relies on
Theorem 3. Theorem 3 is a generalization of [6, Theorem 2.1], and Theorem 4 is a generalization
of [2, Theorem 5].

3

, target

(
)
1 + o(1)

(
)
Algorithm 1 The Fast CUR Algorithm.
1: Input: a real matrix A ∈ Rm×n , target rank k , ϵ ∈ (0, 1], target column number c = 2k
ϵ
row number r = 2c
;
1 + o(1)
2: // Stage 1: select c columns of A to construct C ∈ Rm×c
ϵ
3: Compute approximate truncated SVD via random projection such that Ak ≈ ~Uk ~(cid:6)k ~Vk ;
4: Construct U1 ← columns of (A − ~Uk ~(cid:6)k ~Vk ); V1 ← columns of ~VT
k ;
5: Compute s1 ← Dual Set Spectral-Frobenius Sparsiﬁcation Algorithm (U1 , V1 , c − 2k/ϵ);
6: Construct C1 ← ADiag(s1 ), and then delete the all-zero columns;
7: Residual matrix D ← A − C1C
†
1A;
8: Compute sampling probabilities: pi = ∥di ∥2
2 /∥D∥2
F , i = 1, · · · , n;
9: Sampling c2 = 2k/ϵ columns from A with probability {p1 , · · · , pn } to construct C2 ;
10: // Stage 2: select r rows of A to construct R ∈ Rr×n
11: Construct U2 ← columns of (A − ~Uk ~(cid:6)k ~Vk )T ; V2 ← columns of ~UT
k ;
12: Compute s2 ← Dual Set Spectral-Frobenius Sparsiﬁcation Algorithm (U2 , V2 , r − 2c/ϵ);
13: Construct R1 ← Diag(s2 )A, and then delete the all-zero rows;
1R1 ; Compute qj = ∥b(j ) ∥2
2 /∥B∥2
14: Residual matrix B ← A − AR
F , j = 1, · · · , m;
†
15: Sampling r2 = 2c/ϵ rows from A with probability {q1 , · · · , qm } to construct R2 ;
†
†
16: return C = [C1 , C2 ], R = [RT
2 ]T , and U = C
.
1 , RT
AR

4.1 Adaptive Sampling

The relative-error adaptive sampling algorithm is established in [6, Theorem 2.1]. The algorithm
is based on the following idea: after selecting a proportion of columns from A to form C1 by
an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the
residual A − C1C
†
1A. Boutsidis et al. [2] used the adaptive sampling algorithm to decrease the
residual of the dual set sparsiﬁcation algorithm and obtained an (1 + ϵ) relative-error bound. Here
we prove a new bound for the adaptive sampling algorithm.
Interestingly, this new bound is a
generalization of the original one in [6, Theorem 2.1]. In other words, Theorem 2.1 of [6] is a direct
corollary of our following theorem in which C = Ak is set.
Theorem 3 (The Adaptive Sampling Algorithm). Given a matrix A ∈ Rm×n and a matrix C ∈
A) = ρ, (ρ ≤ c ≤ n), we let R1 ∈ Rr1×n consist of r1
Rm×c such that rank(C) = rank(CC
†
1R1 . Additionally, for i = 1, · · · , m, we deﬁne
rows of A, and deﬁne the residual B = A − AR
†
2/∥B∥2
pi = ∥b(i)∥2
F .
We further sample r2 rows i.i.d. from A, in each trial of which the i-th row is chosen with probability
2 ]T ∈ R(r1+r2 )×n . Then
pi . Let R2 ∈ Rr2×n contains the r2 sampled rows and let R = [RT
1 , RT
the following inequality holds:
A∥2
≤ ∥A − CC
R∥2
E∥A − CC
1R1 ∥2
∥A − AR
†
†
†
F ,
F +
F
where the expectation is taken w.r.t. R2 .

†
AR

ρ
r2

4.2 The Fast CUR Algorithm

Based on the dual set sparsiﬁcation algorithm of of Lemma 1 and the adaptive sampling algorithm
of Theorem 3, we develop a randomized algorithm to solve the second stage of CUR problem. We
present the results of the algorithm in Theorem 4. Theorem 5 of [2] is a special case of the following
theorem where C = Ak .
Theorem 4 (The Fast Row Selection Algorithm). Given a matrix A ∈ Rm×n and a matrix C ∈
A) = ρ, (ρ ≤ c ≤ n), and a target rank k (≤ ρ), the
Rm×c such that rank(C) = rank(CC
†
ϵ (1 + o(1)) rows of A to construct R ∈ Rr×n , such
proposed randomized algorithm selects r = 2ρ
that
R∥2
E∥A − CC
≤ ∥A − CC
A∥2
F + ϵ∥A − Ak ∥2
†
†
†
F ,
AR
where the expectation is taken w.r.t. R. Furthermore, the matrix R can be computed in O((mnk +
F
−2/3 ) time.
mk3 )ϵ
Based on Lemma 2 and Theorem 4, here we present the main theorem for the fast CUR algorithm.

4

Table 1: A summary of the datasets.
Dataset
Source
size
Type
Redrock natural image 18000 × 4000
http://www.agarwala.org/efficient gdc/
10000 × 900 http://archive.ics.uci.edu/ml/datasets/Arcene
Arcene
biology
Dexter bag of words 20000 × 2600 http://archive.ics.uci.edu/ml/datasets/Dexter

Theorem 5 (The Fast CUR Algorithm). Given a matrix A ∈ Rm×n and a positive integer k ≪
min{m, n}, the fast CUR algorithm (described in Algorithm 1) randomly selects c = 2k
ϵ (1 + o(1))
columns of A to construct C ∈ Rm×c with the near-optimal column selection algorithm of Lemma 2,
ϵ (1 + o(1)) rows of A to construct R ∈ Rr×n with the fast row selection
and then selects r = 2c
)
(
algorithm of Theorem 4. Then we have
)R∥F ≤ (1 + ϵ)∥A − Ak ∥F .
E∥A − CUR∥F = E∥A − C(C
†
†
AR
Moreover, the algorithm runs in time O
−2 + nk2 ϵ
−2/3 + (m + n)k3 ϵ
−2/3 + mk2 ϵ
−4
.
mnkϵ
Since k , c, r ≪ min{m, n} by the assumptions, so the time complexity of the fast CUR algorithm
is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm.
Another advantage of this algorithm is avoiding loading the whole m × n data matrix A into main
memory. None of three steps — the randomized SVD, the dual set sparsiﬁcation algorithm, and the
adaptive sampling algorithm — requires loading the whole of A into memory. The most memory-
expensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverse
of C and R, which requires maintaining an m × c matrix or an r × n matrix in memory.
In
comparison, the subspace sampling algorithm requires loading the whole matrix into memory to
compute its truncated SVD.

5 Empirical Comparisons

In this section we provide empirical comparisons among the relative-error CUR algorithms on sev-
eral datasets. We report the relative-error ratio and the running time of each algorithm on each data
set. The relative-error ratio is deﬁned by
∥A − CUR∥F
∥A − Ak ∥F

Relative-error ratio =

,

where k is a speciﬁed target rank.
We conduct experiments on three datasets, including natural image, biology data, and bags of words.
Table 1 brieﬂy summarizes some information of the datasets. Redrock is a large size natural image.
Arcene and Dexter are both from the UCI datasets [11]. Arcene is a biology dataset with 900
instances and 10000 attributes. Dexter is a bag of words dataset with a 20000-vocabulary and 2600
documents. Each dataset is actually represented as a data matrix, upon which we apply the CUR
algorithms.
We implement all the algorithms in MATLAB 7.10.0. We conduct experiments on a workstation
with 12 Intel Xeon 3.47GHz CPUs, 12GB memory, and Ubuntu 10.04 system. According to the
analysis in [10] and this paper, k , c, and r should be integers far less than m and n. For each data
set and each algorithm, we set k = 10, 20, or 50, and c = αk , r = αc, where α ranges in each set of
experiments. We repeat each set of experiments for 20 times and report the average and the standard
deviation of the error ratios. The results are depicted in Figures 1, 2, 3.
The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace
sampling algorithm. The experimental results well match our theoretical analyses in Section 4. As
for the running time, the fast CUR algorithm is more efﬁcient when c and r are small. When c and
r become large, the fast CUR algorithm becomes less efﬁcient. This is because the time complexity
−4 and large c and r imply small ϵ. However, the purpose
of the fast CUR algorithm is linear in ϵ
of CUR is to select a small number of columns and rows from the data matrix, that is, c ≪ n and
r ≪ m. So we are not interested in the cases where c and r are large compared with n and m, say
k = 20 and α = 10.

5

(a) k = 10, c = αk , and r = αc.

(b) k = 20, c = αk , and r = αc.

(c) k = 50, c = αk , and r = αc.

Figure 1: Empirical results on the Redrock data set.

(a) k = 10, c = αk , and r = αc.

(b) k = 20, c = αk , and r = αc.

(c) k = 50, c = αk , and r = αc.

Figure 2: Empirical results on the Arcene data set.

6

246810121416182022242628303234360100200300400500600700Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022240100200300400500600700Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416180100200300400500600700Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022242628303234360.60.70.80.911.11.21.3Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022240.50.60.70.80.911.11.21.31.4Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416180.40.50.60.70.80.911.11.21.3Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR24681012141618202224262830024681012Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182002468101214Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR2468101214468101214161820Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022242628300.60.70.80.911.11.21.31.4Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR24681012141618200.60.70.80.911.11.21.3Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR24681012140.40.50.60.70.80.911.11.21.3Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR(a) k = 10, c = αk , and r = αc.

(b) k = 20, c = αk , and r = αc.

(c) k = 50, c = αk , and r = αc.

Figure 3: Empirical results on the Dexter data set.

6 Conclusions

In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition
problem. This algorithm is faster, more scalable, and more accurate than the state-of-the-art algo-
−1 (1 + o(1))
rithm, i.e., the subspace sampling algorithm. Our algorithm requires only c = 2kϵ
−1 (1 + o(1)) rows to achieve (1+ϵ) relative-error ratio. To achieve the same
columns and r = 2cϵ
relative-error bound, the subspace sampling algorithm requires c = O(kϵ
−2 log k) columns and
r = O(cϵ
−2 log c) rows selected from the original matrix. Our algorithm also beats the subspace
sampling algorithm in time-complexity. Our algorithm costs O(mnkϵ
−2/3 + (m + n)k3 ϵ
−2/3 +
−4 ) time, which is lower than O(min{mn2 , m2n}) of the subspace sampling algo-
−2 + nk2 ϵ
mk2 ϵ
rithm when k is small. Moreover, our algorithm enjoys another advantage of avoiding loading the
whole data matrix into main memory, which also makes our algorithm more scalable. Finally, the
empirical comparisons have also demonstrated the effectiveness and efﬁciency of our algorithm.

A The Dual Set Sparsiﬁcation Algorithm

For the sake of completeness, we attach the dual set sparsiﬁcation algorithm here and describe
some implementation details. The dual set sparsiﬁcation algorithms are deterministic algorithms
established in [2]. The fast CUR algorithm calls the dual set spectral-Frobenius sparsiﬁcation al-
gorithm [2, Lemma 13] in both stages. We show this algorithm in Algorithm 2 and its bounds in
Lemma 6.
∑
Lemma 6 (Dual Set Spectral-Frobenius Sparsiﬁcation). Let U = {x1 , · · · , xn} ⊂ Rl , (l < n),
contains the columns of an arbitrary matrix X ∈ Rl×n . Let V = {v1 , · · · , vn} ⊂ Rk , (k < n),
n
be a decompositions of the identity, i.e.
i = Ik . Given an integer r with k < r < n,
i=1 vivT
√
Algorithm 2 deterministically computes a set of weights si ≥ 0 (i = 1, · · · , n) at most r of which
)2
( n∑
)
( n∑
(
)
are non-zero, such that
tr
i=1
i=1

≤ ∥X∥2
F .

sivivT
i

sixixT
i

≥

1 −

k
r

λk

and

7

24681012141618202224262830050100150200250Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR24681012141618202224050100150200250300Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR24681012141618050100150200250300350400Time (s)αRunning Time  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022242628300.90.9511.051.11.151.21.25Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416182022240.850.90.9511.051.11.151.21.25Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR246810121416180.750.80.850.90.9511.051.11.151.2Relative Error RatioαConstruction Error (Frobenius Norm)  Subspace Sampling (Exactly)Subspace Sampling (Expected)Fast CUR∑
Algorithm 2 Deterministic Dual Set Spectral-Frobenius Sparsiﬁcation Algorithm.
1: Input: U = {xi }n
i=1 ⊂ Rl , (l < n); V = {vi }n
i=1 ⊂ Rk , with
n
i = Ik (k < n); k < r < n;
i=1 vivT
∑
2: Initialize: s0 = 0m×1 , A0 = 0k×k ;
√
3: Compute ∥xi ∥2
2 for i = 1, · · · , n, and then compute δU =
∥xi ∥2
n
i=1
2
1−
4: for τ = 0 to r − 1 do
k/r
(
)−2
Compute the eigenvalue decomposition of Aτ ;
5:
Find an index j in {1, · · · , n} and compute a weight t > 0 such that
(
6:
Aτ − (Lτ + 1)Ik
−1 ≤ vT
vj
j
ϕ(Lτ + 1, Aτ ) − ϕ(Lτ , Aτ )
(
)−1
k∑
λi (A) − L
i=1

Aτ − (Lτ + 1)Ik

∥xj ∥2
2 ≤ t

Lτ = τ −

)−1

ϕ(L, A) =

√

rk ;

−1
U

δ

where

− vT
j

vj ;

;

,

Update the j -th component of sτ and Aτ :
7:
√
8: end for
1−
9: return s =

sr .

k/r

r

sτ +1 [j ] = sτ [j ] + t, Aτ +1 = Aτ + tvj vT
j ;

The weights si can be computed deterministically in O(rnk2 + nl) time.
Here we would like to mention the implementation of Algorithm 2, which is not described in detailed
(
)q
(
)
by [2]. In each iteration the algorithm performs once eigenvalue decomposition: Aτ = W(cid:3)WT .
(Aτ is guaranteed to be positive semi-deﬁnite in each iteration). Since
Aτ − αIk
(λ1 − α)q , · · · , (λk − α)q
WT ,
= WDiag
we can efﬁciently compute (Aτ − (Lτ + 1)Ik )q based on the eigenvalue decomposition of Aτ . With
the eigenvalues at hand, ϕ(L, Aτ ) can also be computed directly.

Acknowledgments

This work has been supported in part by the Natural Science Foundations of China (No. 61070239),
the Google visiting faculty program, and the Scholarship Award for Excellent Doctoral Student
granted by Ministry of Education.

References
[1] Adi Ben-Israel and Thomas N.E. Greville. Generalized Inverses: Theory and Applications.
Second Edition. Springer, 2003.
[2] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based
matrix reconstruction. CoRR, abs/1103.0995, 2011.
[3] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based
matrix reconstruction. In Proceedings of the 2011 IEEE 52nd Annual Symposium on Founda-
tions of Computer Science, FOCS ’11, pages 305–314, 2011.
[4] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard
Harshman. Indexing by latent semantic analysis. Journal of The American Society for Infor-
mation Science, 41(6):391–407, 1990.
[5] Amit Deshpande and Luis Rademacher. Efﬁcient volume sampling for row/column subset se-
lection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer
Science, FOCS ’10, pages 329–338, 2010.
[6] Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation
and projective clustering via volume sampling. Theory of Computing, 2(2006):225–247, 2006.
[7] Petros Drineas. Pass-efﬁcient algorithms for approximating large matrices. In In Proceeding
of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 223–232, 2003.

8

[8] Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for
matrices iii: Computing a compressed approximate matrix decomposition. SIAM Journal on
Computing, 36(1):184–206, 2006.
[9] Petros Drineas and Michael W. Mahoney. On the Nystr ¨om method for approximating a gram
matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153–
2175, 2005.
[10] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix de-
compositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, September
2008.
[11] A. Frank and A. Asuncion. UCI machine learning repository, 2010.
[12] S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton
approximations. Linear Algebra and Its Applications, 261:1–21, 1997.
[13] S. A. Goreinov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. Pseudo-skeleton approximations
by matrices of maximal volume. Mathematical Notes, 62(4):619–623, 1997.
[14] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix re-
construction. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ’12, pages 1207–1214. SIAM, 2012.
[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011.
[16] John Hopcroft and Ravi Kannan. Computer Science Theory for the Information Age. 2012.
[17] Finny G. Kuruvilla, Peter J. Park, and Stuart L. Schreiber. Vector algebra in the analysis of
genome-wide expression data. Genome Biology, 3:research0011–research0011.1, 2002.
[18] Lester Mackey, Ameet Talwalkar, and Michael I. Jordan. Divide-and-conquer matrix factor-
ization. In Advances in Neural Information Processing Systems 24. 2011.
[19] Michael W. Mahoney and Petros Drineas. CUR matrix decompositions for improved data
analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009.
[20] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces.
Journal of the Optical Society of America A, 4(3):519–524, Mar 1987.
[21] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of Cognitive Neuro-
science, 3(1):71–86, 1991.
[22] Eugene E. Tyrtyshnikov.
Incomplete cross approximation in the mosaic-skeleton method.
Computing, 64:367–380, 2000.

9


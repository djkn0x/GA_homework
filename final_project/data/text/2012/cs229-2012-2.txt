CS229 Course Pro ject: A new rival to Predator and ALIEN

Martin Raison
Stanford University
mraison@stanford.edu

Botao Hu
Stanford University
botaohu@stanford.edu

Abstract

2.1 TLD detector operation

This report documents how we improved the TLD
framework for real-time object tracking [1] by using a
new set of features and modifying the learning algo-
rithm.

1 Introduction

The problem of real-time ob ject tracking in a sequence
of frames has gained much recognition in the Computer
Vision community in recent years. The TLD frame-
work (Kalal et.al. [1]), marketed as Predator, and the
ALIEN tracker (Pernici et.al. [2]) are recent successful
attempts to solve the problem. The TLD framework [1]
improves the tracking performance by combining a de-
tector and an optical-ﬂow tracker. The purpose of the
detector is to prevent the tracker from drifting away
from the ob ject, and recover tracking after an occlu-
sion. Since the only prior knowledge about the ob ject
is a bounding box in the initial frame, the detector is
trained online via semi-supervised learning. In order to
build such a system, two challenges must be addressed:

1) ﬁnding a good set of features to be used by the
detector for classifying image patches

2) using an eﬃcient learning algorithm to train the
detector with examples from previous frames

The solutions to these two problems are dependent on
each other, and as such, they must be designed so as
to ﬁt into a single system.
Our goal was to investigate new approaches for 1)
and 2) and try to ﬁnd improvements in terms of ro-
bustness of tracking (good performance with a wide
range of ob jects, tolerance to fast movements, camera
blur, clutter, low resolution, etc) and eﬃciency (time
and space complexity).

The main focus of this work is the detection component
of the framework, and the associated learning process.
The detector used in the TLD framework uses the fol-
lowing workﬂow:

1) Each frame is scanned using a sliding window, at
multiple scales. About a hundred thousand win-
dows are considered, depending on the size of the
image and the size of the original ob ject bounding
box. The part of the image contained in a window
is called a patch.

2) Each patch is ﬂagged as positive or negative using
a 3-step detection cascade:
• Variance ﬁlter: If the variance of the patch is less
than half the variance of the ob ject in its initial
bounding box, the window is rejected
• Ensemble classiﬁer: a conﬁdence measure is obtained
for the patch using random ferns. Several groups of
features are extracted from the patch, and for each
group, a probability is computed, based on the num-
ber of times the same combination of features ap-
peared in previous frames as positive or negative ex-
amples. The ﬁnal conﬁdence measure is the average
of the probabilities of each group of features.
• Nearest-Neighbor classiﬁer: the Normalized Correla-
tion Coeﬃcient is used to evaluate the distance be-
tween the considered patch and two sets of patches:
one set of positive patches, one set of negative
patches (built from previous frames). These two sets
of patches represent the ob ject template, and are
maintained by a P/N learning algorithm, introduced
in [1].

Background substraction can also be used as a pre-
liminary step to ﬁlter out windows.

2 Motivation & Background Work

This section details the mode of operation of the TLD
detector [1], and motivates our use of features based on
compressive sensing for improving the system.

2.2 Compressive sensing for image patch
descriptors

The ensemble classiﬁer is a critical part of the detection
cascade. With the dataset used for the experiments, we

1

noticed that on average, it selects about 50 patches out
of 25,000 patches on each frame. One of the main diﬃ-
culties of training the ensemble classiﬁer is that the size
of the training set is small. The TLD framework tack-
les this issue by using random ferns, but in reality the
independence assumption between groups of features is
not veriﬁed. During the pro ject, we tried to improve
this part of the detection cascade by using alternative
descriptors (i.e. sets of features) for the patches and
modifying the classiﬁcation algorithm accordingly.
Many descriptor extraction algorithms have been de-
veloped in the last few years, such as FREAK, BRISK,
BRIEF or ORB. The recent Compressive Tracking
(CT) method [3] introduces a way of computing de-
scriptors based on compressive sensing. Given an im-
age patch X of size w × h, the patches Xi,j , 1 ≤ i ≤ w,
1 ≤ j ≤ h, are obtained by ﬁltering X with a rectangle
ﬁlter of size i × j whose elements are all equal to 1.
Each pixel of a patch Xi,j represents a rectangle fea-
ture (sum of all the elements in a rectangle). All the
Xi,j are then considered as column vectors, and con-
catenated to form a big column vector X of size (wh)2 .
The many features contained in X are meant to ac-
count for a variety of scales and horizontal/vertical de-
formations of the ob ject. The descriptor x of the patch
is then obtained from X with a random pro jection. A
very sparse random matrix R of size l × (wh)2 is used
for the pro jection, where l is the size chosen for the
−1 with probability 1
descriptor. The matrix R is randomly deﬁned by:
2s
with probability 1
1
2s
with probability 1 − 1
0
s
where s is a constant. Li et.al. showed in [4] that for
s up to wh/ log (wh), the matrix R is asymptotically
normal (up to a constant). A consequence is that with
high probability, X can be reconstructed from x with
minimal error (Achlioptas [5]).
This algorithm was used in [3] for the purpose of
building a real-time ob ject tracker - the “Compressive
Tracker” (CT). This approach led to successful results,
but we observed that the system itself has limitations.
The CT method considers much fewer windows on each
frame than TLD. As a consequence, while the compu-
tation time is signiﬁcantly reduced, the CT tracker is
not very robust to fast movements and occlusion. In
addition, although the rectangle ﬁlters are intended to
account for “scales” of the ob ject, the scale itself is
never explicitly determined. The size of the current
bounding box always remains equal to the size of the
initial bounding box, which is a signiﬁcant drawback
in scenes where the distance between the ob ject of in-

Ri,j =

terest and the camera varies.

3 Methodology

We focused on transposing the CT method to the TLD
framework, to improve the TLD detector while over-
coming the limitations of the original CT approach.

3.1 Descriptor Computation

The main challenge for the descriptor computation was
to scale up the algorithm. CT considers only windows
near the current ob ject location, whereas TLD scans
the whole frame. So we needed to build descriptors that
were easier to compute. Fortunately, another charac-
teristic of CT descriptors is that they were intended to
work for all scales of the ob ject. This is not necessary
in the case of the TLD framework, since each frame
is scanned at multiple scales. Based on these obser-
vations, we slightly modiﬁed the descriptors, and de-
signed an algorithm to compute them eﬃciently. This
procedure was the ob ject of the CS231A part of the
pro ject.

3.2 Online Na¨ıve Bayes

Given the sparsity of the training data, and the ne-
cessity to train the classiﬁer online, the Na¨ıve Bayes
algorithm was a natural choice.
In addition, ob ject
tracking is speciﬁcally challenging because of appear-
ance changes of the ob ject of interest over the course
of the video. In order to introduce decay in the model,
we used a learning rate λ to do the relative weighting
between past and present examples. During tracking,
one model update is performed for each frame.
The input features are image patch descriptors. Each
descriptor is denoted by x(i) = (x(i)
2 , ..., x(i)
1 , x(i)
n ), with
a label y (i) = 1 if the patch corresponds to the ob ject
of interest, y (i) = 0 otherwise. The x(i)
j ’s for j = 1, .., n
are supposed to be independent given y (i) , with

| y (i) = 1 ∼ N (µ(1)
x(i)
, σ (1)
j )
j
j
| y (i) = 0 ∼ N (µ(0)
x(i)
, σ (0)
j )
j
j
Also, for k ∈ {0, 1}, we denote by µ(k)∗ , σ (k)∗ the
mean and variance of the positive (k = 1) and nega-
tive (k = 0) training examples drawn from the current
frame. If we give a weight λ to the training examples
from the previous frames, and a weight 1 − λ to the
training examples from the current frame, we can de-
rive the mean and variance of the resulting descriptor

2

distribution, and obtain the following update formulas:
(cid:118)(cid:117)(cid:117)(cid:116) λ(σ (k) )2 + (1 − λ)(σ (k)∗ )2
µ(k) := λµ(k) + (1 − λ)µ(k)∗
+ λ(1 − λ)(µ(k) − µ(k)∗ )2
Similar updates are used in [3].

σ (k) :=

4 Experiments

This section details how we tested the performance of
our approach.

4.1 Dataset and evaluation

For evaluating our system, we used the videos from the
TLD dataset [1] and other videos commonly used for
evaluating trackers (Zhong et.al. [6]).
A typical measure for the performance of a track-
ing system is the PASCAL overlap measure [7]. A pre-
diction is considered valid if the overlap ratio of the
bounding box with the ground truth is greater than a
threshold τ :

|Bprediction ∩ Bground truth |
|Bprediction ∪ Bground truth | > τ

The value of τ chosen in [1] for comparing the TLD
framework with other trackers is 0.25. We used this
same threshold for our experiments.
We measured the performance of our system at two
levels: we evaluated the performance of our new clas-
siﬁer alone (Section 4.3), and we evaluated the perfor-
mance impact for the entire pipeline (Section 4.4). In
both cases, we measured the performance in terms of
precision, recall and f-score.
Finally, since the goal was to build a real-time sys-
tem, we evaluated the speed of our algorithm in terms
of frames per second.

4.2 Preliminary experiments

Before adopting the approach detailed in Section 3, we
did some early experiments with popular keypoint de-
tectors and descriptor extraction algorithms (FREAK,
BRISK, SURF, ORB etc). The pipeline was:

1. On each frame, run the keypoint detector

2. Compute a descriptor for each keypoint

3. Classify the descriptors with a Na¨ıve Bayes algo-
rithm

Figure 1: Early experiments. Keypoints are detected on each
frame, and then classiﬁed. The red points are negative, the blue
points are positive. The green box is the ground truth bounding
box for the ob ject.

This method allowed us to perform detection with-
out using a sliding window mechanism. However, there
were limitations. First, keypoint detectors do not uni-
formly detect keypoints on the frame, and sometimes
do not even detect any keypoint on the ob ject of inter-
est, making further detection impossible. Then, using
available implementation of descriptors, we could not
easily tune parameters such as the number of features.
Finally, these descriptors are more appropriate for de-
tecting characteristic points on an ob ject, rather than
full ob jects. For these reasons, we moved on to the pre-
viously described method, keeping the sliding-window
mechanism but using CT-like descriptors for classiﬁca-
tion.

4.3 Classiﬁer performance

To compare our new classiﬁer with the original ensem-
ble classiﬁer, we modiﬁed the TLD pipeline. On each
frame, before the learning step, we replaced the pre-
dicted bounding box with the ground truth. This en-
sured that the training examples selected afterwards
were similar. Otherwise the training sets for diﬀerent
classiﬁers would diverge, and the comparison would be-
come irrelevant.
On each frame, we assigned a label to all the image
patch descriptors using the PASCAL measure (Section
4.1), and compared it with the output of the classiﬁer.
Figure 2 shows an example of precision, recall and f-
score as a function of time, using a descriptor length
n = 100 and a learning rate λ = 0.75. With these set-
tings, we achieved an f-score of 36.9% on average on the
whole dataset (precision=47.8%, recall=42.2%), while
the TLD framework achieved 30.8% (precision=48.2%,
recall=25.8%). Our system signiﬁcantly improves the
classiﬁcation recall, for an almost equivalent precision.
The introduction of decay (through the parameter λ)
makes the classiﬁcation performance much more sta-
ble over time. The example on Figure 2 shows that
the original ensemble classiﬁer fails to adapt quickly
when the appearance of the ob ject changes, because

3

(a) Original TLD

(b) Improved TLD

Figure 2: Precision (blue), recall (green) and f-score (red) as a
function of time for the “panda” video from the TLD dataset (the
frame number is shown on the x-axis). Our classiﬁer produces a
more stable f-score.

the weight of the ﬁrst training examples remains too
high. On the other hand, the f-score of our system is
almost always above 0.2. This is important because
the detector is most useful when the ob ject is diﬃcult
to track, i.e. when the optical-ﬂow tracker tra jectory is
most likely to drift from the ob ject.
We tested several values of λ (Figure 3). Increasing
λ reduces the vulnerability to temporary appearance
changes of the ob ject (blur, occlusion, etc), but the
system adapts more slowly to long-term appearance
changes (orientation change, shape change, etc). We
obtained the best performance with λ = 0.75.

Figure 3: Top: Average precision/recall/f-score on the dataset as
a function of the learning rate λ (number of features n = 100).
Bottom: Average precision/recall/f-score as a function of n (λ =
0.75).

Finally, we observed how the performance varied
with the number of features (Figure 3).
If n is too
low, the model is highly biased, and the descriptors
don’t capture enough information about the patches.
We couldn’t observe any clear sign of overﬁtting when
increasing the number of features. However, high val-
ues of n require more computation, which is critical for
a real-time system. We found n = 100 to be a good
compromise between accuracy and speed.

4.4 Overall system performance

For evaluating the complete pipeline, we measured the
precision, recall and f-score of the bounding box predic-
tion for each video. This is diﬀerent from the classiﬁer
evaluation, where we measured the precision, recall and
f-score of the image patch classiﬁcation for each frame.
The precision P is the rate of valid bounding boxes
among all the predictions, the recall R is the rate of
predicted bounding boxes among all those that should
have been predicted, and as usual, the f-score is deﬁned
as F = 2P R
P +R . We obtained one value of P , R, and F for
each video. A comparison of the two trackers is shown
on ﬁgure 4.
Our system did slightly better overall than the orig-
inal TLD framework, but since the average numbers
are very close, tests on a more extensive dataset would
be required for conﬁrming the progress. In addition,
both systems have their strengths and weaknesses. To
understand the results, we did a qualitative analysis of
the performance for each video. We observed that our
system is more resistant to image blur, clutter and oc-
clusion, whereas the original TLD framework is better
for discriminating between ob jects with small variation
of intensity, and more robust to illumination changes.
Examples are given on Figure 5. A general observation

Sequence
david
jumping
pedestrian1
pedestrian2
pedestrian3
car
panda
animal
board
car11
caviar
faceocc2
girl
panda2
shaking
stone
singer1
mean

TLD (Measured)
1.00 / 1.00 / 1.00
1.00 / 0.87 / 0.93
1.00 / 0.64 / 0.78
0.77 / 0.70 / 0.73
0.88 / 1.00 / 0.94
0.95 / 1.00 / 0.97
0.52 / 0.46 / 0.49
1.00 / 0.79 / 0.88
0.83 / 0.84 / 0.84
0.93 / 0.94 / 0.94
0.27 / 0.27 / 0.27
0.99 / 0.99 / 0.99
1.00 / 0.91 / 0.95
1.00 / 0.43 / 0.60
1.00 / 0.15 / 0.27
0.99 / 0.99 / 0.99
1.00 / 1.00 / 1.00
0.79 / 0.72 / 0.74

CT-TLD
1.00 / 1.00 / 1.00
1.00 / 0.98 / 0.99
1.00 / 0.83 / 0.91
0.73 / 0.88 / 0.80
0.83 / 0.97 / 0.90
0.93 / 0.98 / 0.95
0.49 / 0.49 / 0.49
1.00 / 0.82 / 0.90
0.99 / 0.86 / 0.92
0.99 / 1.00 / 1.00
0.71 / 0.16 / 0.26
1.00 / 1.00 / 1.00
0.96 / 0.95 / 0.95
1.00 / 0.59 / 0.74
0.82 / 0.35 / 0.49
1.00 / 0.88 / 0.94
1.00 / 1.00 / 1.00
0.81 / 0.74 / 0.76

Figure 4: Comparison of our system (CT-TLD) with the original
TLD. The numbers in each column are the precision, recall and
f-score.

4

05010015020025030000.20.40.60.81TLD  precisionrecallfscore05010015020025030000.20.40.60.81CTTLD  precisionrecallfscore0.40.50.60.70.80.910.20.250.30.350.40.45Learning rate  PrecisionRecallF−Score05010015020000.10.20.30.40.5Number of features  PrecisionRecallF−Scoreis that our system can deal with a wider range of ap-
pearance changes and movements, whereas the original
TLD framework tends to be more precise (higher over-
lap when the tracking is successful). This can probably
be explained by the nature of the descriptors: our de-
scriptor corresponds to a summation of intensities over
rectangles, whereas TLD uses more localized features
(intensity diﬀerence between pairs of pixels).

Finally, we measured the speed of our system. Ini-
tially, the frame rate was low, but writing C code in-
stead of MATLAB code increased the average frame
rate to 11 fps on a 2.7 Ghz Intel i7 processor on a
640x480 video, with an initial bounding box of size
100x70. The initial TLD framework is faster: during
our tests, we achieved 20 fps on average. Our system’s
performance still has the right order of magnitude for
real-time operation, and we plan on optimizing it to
get smoother tracking.

5 Conclusion

Our classiﬁer improves the tracking performance of the
TLD framework in a large range of common situations.
In some cases, such as when the intensity variance over
the ob ject is low, the original TLD system still remains
better. Our learning algorithm does not seem to be
the bottleneck. The introduction of a decay parameter
makes the classiﬁcation performance signiﬁcantly more
stable over time. On the other hand, our features do
not capture the same kind of information as the small
scale features used in the original TLD system. In the
future, we intend to improve our descriptors by using
a retinal topology inspired from FREAK keypoint de-
scriptors [8], in order to capture both local and larger
scale information about the ob ject.

6 Acknowledgements

We would like to express our gratitude to Alexandre
Alahi (Post-doc at the Stanford Computer Vision lab),
who accepted to mentor our pro ject.

7 Appendix

(a) animal

This pro ject is done jointly with the CS231A class
pro ject for all members of the team.

(b) pedestrian1

(c) pedestrian2

(d) stone

Figure 5: Superposition of the bounding boxes output by the
optical-ﬂow tracker alone (LK), the original CT system, the orig-
inal TLD system, and our system (CTTLD). On (a) (blur exam-
ple) and (b) (clutter example), our system recovers faster. On
(c), TLD never recovers after the occlusion. On (d), our tracker
jumps to another similar ob ject.

5

References

[1] Z. Kalal, K. Mikola jczyk, and J. Matas. “Tracking-learning-
detection”. In: Pattern Analysis and Machine Intel ligence,
IEEE Transactions on 34.7 (2012), pp. 1409–1422.

[2] F. Pernici. “FaceHugger: The ALIEN Tracker Applied
to Faces”. In: European Conference on Computer Vision
(ECCV) (2012).

[3] K. Zhang, L. Zhang, and M.H. Yang. “Real-time Compres-
sive Tracking”. In: ECCV (2012).

[4] P. Li, T.J. Hastie, and K.W. Church. “Very sparse random
pro jections”. In: Proceedings of the 12th ACM SIGKDD
international conference on Know ledge discovery and data
mining. ACM. 2006, pp. 287–296.

[5] D. Achlioptas. “Database-friendly random pro jections:
Johnson-Lindenstrauss with binary coins”. In: Journal of
computer and System Sciences 66.4 (2003), pp. 671–687.

[6] W. Zhong, H. Lu, and M.H. Yang. “Robust ob ject track-
ing via sparsity-based collaborative model”. In: Computer
Vision and Pattern Recognition (CVPR), 2012 IEEE Con-
ference on. IEEE. 2012, pp. 1838–1845.

[7] M. Everingham et al. “The pascal visual ob ject classes (voc)
challenge”. In: International journal of computer vision 88.2
(2010), pp. 303–338.

[8] P. Vandergheynst, R. Ortiz, and A. Alahi. “FREAK: Fast
Retina Keypoint”. In: 2012 IEEE Conference on Computer
Vision and Pattern Recognition. IEEE. 2012, pp. 510–517.

1234567891012345678910  Ground TruthLKCTTLDCTTLD
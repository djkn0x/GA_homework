Spectral Learning of General Weighted Automata
via Constrained Matrix Completion

Borja Balle
Universitat Polit `ecnica de Catalunya
bballe@lsi.upc.edu

Mehryar Mohri
Courant Institute and Google Research
mohri@cims.nyu.edu

Abstract

Many tasks in text and speech processing and computational biology require es-
timating functions mapping strings to real numbers. A broad class of such func-
tions can be deﬁned by weighted automata. Spectral methods based on the sin-
gular value decomposition of a Hankel matrix have been recently proposed for
learning a probability distribution represented by a weighted automaton from a
training sample drawn according to this same target distribution. In this paper, we
show how spectral methods can be extended to the problem of learning a general
weighted automaton from a sample generated by an arbitrary distribution. The
main obstruction to this approach is that, in general, some entries of the Hankel
matrix may be missing. We present a solution to this problem based on solving a
constrained matrix completion problem. Combining these two ingredients, matrix
completion and spectral method, a whole new family of algorithms for learning
general weighted automata is obtained. We present generalization bounds for a
particular algorithm in this family. The proofs rely on a joint stability analysis of
matrix completion and spectral learning.

1

Introduction

Many tasks in text and speech processing, computational biology, or learning models of the environ-
ment in reinforcement learning, require estimating a function mapping variable-length sequences to
real numbers. A broad class of such functions can be deﬁned by weighted automata. The mathe-
matical and algorithmic properties of weighted automata have been extensively studied in the most
general setting where they are deﬁned in terms of an arbitrary semiring [28, 9, 23]. Weighted au-
tomata are widely used in applications ranging from natural text and speech processing [24] to
optical character recognition [12] and image processing [1]. This paper addresses the problem of
learning weighted automata from a ﬁnite set of labeled examples.
The particular instance of this problem where the objective is to learn a probabilistic automaton
from examples drawn from this same distribution has recently drawn much attention: starting with
the seminal work of Hsu et al. [19], the so-called spectral method has proven to be a valuable tool in
developing novel and theoretically-sound algorithms for learning HMMs and other related classes
of distributions [5, 30, 31, 10, 6, 4]. Spectral methods have also been applied to other probabilistic
models of practical interest, including probabilistic context-free grammars and graphical models
with hidden variables [26, 22, 16, 3, 2]. The main idea behind these algorithms is that, under
an identiﬁability assumption, the method of moments can be used to formulate a set of equations
relating the parameters deﬁning the target to observable statistics. Given enough training data, these
statistics can be accurately estimated. Then, solving the corresponding approximate equations yields
a model that closely estimates the target distribution. The spectral term takes its origin from the use
of a singular value decomposition in solving those equations.

1

This paper tackles a signiﬁcantly more general and more challenging problem than the speciﬁc
instance just mentioned. Indeed, in general, there seems to be a large gap separating the scenario
of learning a probabilistic automaton using data drawn according to the distribution it generates,
from that of learning an arbitrary weighted automaton from labeled data drawn from some unknown
distribution. For a start, in the former setting there is only one object to care about because the
distribution from which examples are drawn is the target machine. In contrast, the latter involves two
distinct objects: a distribution according to which strings are drawn, and a target weighted automaton
assigning labels to these strings. It is not difﬁcult in this setting to conceive that, for a particular
target, an adversary could ﬁnd a distribution over strings making the learner’s task insurmountably
difﬁcult. In fact, this is the core idea behind the cryptography-based hardness results for learning
deterministic ﬁnite automata given by Kearns and Valiant [20] – these same results apply to our
setting as well.
But, even in cases where the distribution “cooperates,” there is still an obstruction in leveraging the
spectral method for learning general weighted automata. The statistics used by the spectral method
are essentially the probabilities assigned by the target distribution to each string in some ﬁxed ﬁnite
set B .
In the case where the target is a distribution, increasingly large samples yield uniformly
convergent estimates for these probabilities. Thus, it can be safely assumed that the probability of
any string from B not present in the sample is zero. When learning arbitrary weighted automata,
however, the value assigned by the target to an unseen string is unknown. Furthermore, one cannot
expect that a sample would contain the values of the target function for all the strings in B . This
observation raises the question of whether it is possible at all to apply the spectral method in a
setting with missing data, or, alternatively, whether there is a principled way to “estimate” this
missing information and then apply the spectral method.
As it turns out, the latter approach can be naturally formulated as a constrained matrix completion
problem. When applying the spectral method, the (approximate) values of the target on B are ar-
ranged in a matrix H. Thus, the main difference between the two settings can be restated as follows:
when learning a weighted automaton representing a distribution, unknown entries of H can be ﬁlled
in with zeros, while in the general setting there is a priori no straightforward method to ﬁll in the
missing values. We propose to use a matrix completion algorithm for solving this last problem. In
particular, since H is a Hankel matrix whose entries must satisfy some equality constraints, it turns
out that the problem of learning weighted automata under an arbitrary distribution leads to what
we call the Hankel matrix completion problem. This is essentially a constrained matrix completion
problem where entries of valid hypotheses need to satisfy a set of equalities. We give an algorithm
for solving this problem via convex optimization. Many existing approaches to matrix completion,
e.g., [14, 13, 27, 18], are also based on convex optimization. Since the set of valid hypotheses for our
constrained matrix completion problem is convex, many of these algorithms could also be modiﬁed
to deal with the Hankel matrix completion problem.
In summary, our approach leverages two recent techniques for learning a general weighted automa-
ton: matrix completion and spectral learning. It consists of ﬁrst predicting the missing entries in
H and then applying the spectral method to the resulting matrix. Altogether, this yields a family
of algorithms parametrized by the choice of the speciﬁc Hankel matrix completion algorithm used.
These algorithms are designed for learning an arbitrary weighted automaton from samples generated
by an unknown distribution over strings and labels.
We study a special instance of this family of algorithms and prove generalization guarantees for
its performance based on a stability analysis, under mild conditions on the distribution. The proof
contains two main novel ingredients: a stability analysis of an algorithm for constrained matrix
completion, and an extension of the analysis of spectral learning to an agnostic setting where data
is generated by an arbitrary distribution and labeled by a process not necessarily modeled by a
weighted automaton.
The rest of the paper is organized as follows. Section 2 introduces the main notation and deﬁnitions
used in subsequent sections. In Section 3, we describe a family of algorithms for learning general
weighted automata by combining constrained matrix completion and spectral methods.
In Sec-
tion 4, we give a detailed analysis of one particular algorithm in this family, including generalization
bounds.

2

2 Preliminaries
This section introduces the main notation used in this paper. Bold letters will be used for vec-
tors v and matrices M. For vectors, (cid:107)v(cid:107) denotes the standard euclidean norm. For matri-
(cid:107)M(cid:107)p = ((cid:80)
ces, (cid:107)M(cid:107) denotes the operator norm. For p ∈ [1, +∞], (cid:107)M(cid:107)p denotes the Schatten p-norm:
n (M))1/p , where σn (M) is the nth singular value of M. The special case
n≥1 σp
p = 2 coincides with the Frobenius norm which will be sometimes also written as (cid:107)M(cid:107)F . The
Moore–Penrose pseudo-inverse of a matrix M is denoted by M+ .

2.1 Functions over Strings and Hankel Matrices
We denote by Σ = {a1 , . . . , ak } a ﬁnite alphabet of size k ≥ 1 and by  the empty string. We also
write Σ(cid:48) = {} ∪ Σ. The set of all strings over Σ is denoted by Σ(cid:63) and the length of a string x
denoted by |x|. For any n ≥ 0, Σ≤n denotes the set of all strings of length at most n. Given two
sets of strings P , S ⊆ Σ(cid:63) we denote by P S the set of all strings uv obtained by concatenation of a
string u ∈ P and a string v ∈ S . A set of strings P is called Σ-complete when P = P (cid:48)Σ(cid:48) for some
set P (cid:48) . P (cid:48) is then called the root of P . A pair (P , S ) with P , S ⊆ Σ(cid:63) is said to form a basis of Σ(cid:63)
if  ∈ P ∩ S and P is Σ-complete. We deﬁne the dimension of a basis (P , S ) as the cardinality of
P S , that is |P S |.
For any basis B = (P , S ), we denote by HB the vector space of functions RP S whose dimension
is the dimension of B . We will simply write H instead of HB when the basis B is clear from the
context. The Hankel matrix H ∈ RP×S associated to a function h ∈ H is the matrix whose entries
are deﬁned by H(u, v) = h(uv) for all u ∈ P and v ∈ S . Note that the mapping h (cid:55)→ H is linear.
In fact, H is isomorphic to the vector space formed by all |P | × |S | real Hankel matrices and we can
H = (cid:8)H ∈ RP×S
: ∀u1 , u2 ∈ P , ∀v1 , v2 ∈ S , u1 v1 = u2 v2 ⇒ H(u1 , v1 ) = H(u2 , v2 )(cid:9) .
thus write by identiﬁcation
It is clear from this characterization that H is a convex set because it is a subset of a convex
space deﬁned by equality constraints. In particular, a matrix in H contains |P ||S | coefﬁcients with
|P S | degrees of freedom, and the dependencies can be speciﬁed as a set of equalities of the form
H(u1 , v1 ) = H(u2 , v2 ) when u1 v1 = u2 v2 . We will use both characterizations of H indistinctly for
the rest of the paper. Also, note that different orderings of P and S may result in different sets of
matrices. For convenience, we will assume for all that follows an arbitrary ﬁxed ordering, since the
choice of that order has no effect on any of our results.
Matrix norms extend naturally to norms in H. For any p ∈ [1, +∞], the Hankel–Schatten p-norm
on H is deﬁned as (cid:107)h(cid:107)p = (cid:107)H(cid:107)p . It is straightforward to verify that (cid:107)h(cid:107)p is a norm by the linearity
of h (cid:55)→ H. In particular, this implies that the function (cid:107) · (cid:107)p : H → R is convex. In the case p = 2,
(cid:88)
it can be seen that (cid:107)h(cid:107)2
2 = (cid:104)h, h(cid:105)H , with the inner product on H deﬁned by
(cid:104)h, h
(cid:48) (cid:105)H =
(cid:48)
cxh(x)h
x∈P S
where cx = |{(u, v) ∈ P × S : x = uv}| is the number of possible decompositions of x into a preﬁx
in P and a sufﬁx in S .

(x) ,

2.2 Weighted ﬁnite automata

A widely used class of functions mapping strings to real numbers is that of functions deﬁned by
weighted ﬁnite automata (WFA) or in short weighted automata [23]. These functions are also known
as rational power series [28, 9]. A WFA over Σ with n states can be deﬁned as a tuple A =
(cid:104)α, β , {Aa}a∈Σ (cid:105), where α, β ∈ Rn are the initial and ﬁnal weight vectors, and Aa ∈ Rn×n the
transition matrix associated to each alphabet symbol a ∈ Σ. The function fA realized by a WFA A
is deﬁned by
fA (x) = α(cid:62)Ax1 · · · Axt β ,
for any string x = x1 · · · xt ∈ Σ∗ with t = |x| and xi ∈ Σ for all i ∈ [1, t]. We will say that a WFA
A = (cid:104)α, β , {Aa}(cid:105) is γ -bounded if (cid:107)α(cid:107), (cid:107)β(cid:107), (cid:107)Aa (cid:107) ≤ γ for all a ∈ Σ. This property is convenient
to bound the maximum value assigned by a WFA to any string of a given length.

3

α(cid:62)

Aa =

(cid:20)3/4
= [1/2
0

(cid:21)
1/2]

0
1/3

(cid:62)

β

Ab =

(cid:20)6/5
(cid:21)
= [1 −1]
2/3
1
3/4

(b)
(a)
Figure 1: Example of a weighted automaton over Σ = {a, b} with 2 states: (a) graph representation;
(b) algebraic representation.

WFAs can be more generally deﬁned over an arbitrary semiring instead of the ﬁeld of real numbers
and are also known as multiplicity automata (e.g., [8]). To any function f : Σ(cid:63) → R, we can
associate its Hankel matrix Hf ∈ RΣ(cid:63)×Σ(cid:63) with entries deﬁned by Hf (u, v) = f (uv). These are
just the bi-inﬁnite versions of the Hankel matrices we introduced in the case P = S = Σ(cid:63) . Carlyle
and Paz [15] and Fliess [17] gave the following characterization of the set of functions f in RΣ(cid:63)
deﬁned by a WFA in terms of the rank of their Hankel matrix rank(Hf ).1
Theorem 1 ([15, 17]) A function f : Σ(cid:63) → R can be deﬁned by a WFA iff rank(Hf ) is ﬁnite and in
that case rank(Hf ) is the minimal number of states of any WFA A such that f = fA .

Thus, WFAs can be viewed as those functions whose Hankel matrix can be ﬁnitely “compressed”.
Since ﬁnite sub-blocks of a Hankel matrix cannot have a larger rank than its bi-inﬁnite extension,
this justiﬁes the use of a low-rank-enforcing regularization in the deﬁnition of a Hankel matrix
completion.
Note that deterministic ﬁnite automata (DFA) with n states can be represented by a WFA with at
most n states. Thus, the results we present here can be directly applied to classiﬁcation problems in
Σ(cid:63) . However, specializing our results to this particular setting may yield several improvements.

2.2.1 Example
Figure 1 shows an example of a weighted automaton A = (cid:104)α, β , {Aa}(cid:105) with two states deﬁned
over the alphabet Σ = {a, b}, with both its algebraic representation (Figure 1(b)) in terms of vectors
and matrices and the equivalent graph representation (Figure 1(a)) useful for a variety of WFA
algorithms [23]. Let W = {, a, b}, then B = (W Σ(cid:48) , W ) is a Σ-complete basis. The following is

 .
the Hankel matrix of A on this basis shown with three-digit precision entries:

a
b
aa
ab
ba
bb
0.00 0.20

0.14
0.22
0.15
0.45
0.31
a 0.20 0.22
0.45
0.19
0.29
0.45
0.85
b
0.14
0.15
0.31
0.13
0.20
0.32
0.58
By Theorem 1, the Hankel matrix of A has rank at most 2. Given HB , the spectral method described
in [19] can be used to recover a WFA ˆA equivalent to A, in the sense that A and ˆA compute the
same function. In general, one may be given a sample of strings labeled using some WFA that does
not contain enough information to fully specify a Hankel matrix over a complete basis. In that case,
Theorem 1 motivates the use of a low-rank matrix completion algorithm to ﬁll in the missing entries
in HB prior to the application of the spectral method. This is the basis of the algorithm we describe
in the following section.

H(cid:62)
B =

3 The HMC+ SM Algorithm

In this section we describe our algorithm HMC+SM for learning weighted automata. As input,
the algorithm takes a sample Z = (z1 , . . . , zm ) containing m examples zi = (xi , yi ) ∈ Σ(cid:63) × R,
1The construction of an equivalent WFA with the minimal number of states from a given WFA was ﬁrst
given by Sch ¨utzenberger [29].

4

1-1a,0b,2~3a,0b,3~4a,1~3b,1a,3~4b,6~51/21/21 ≤ i ≤ m, drawn i.i.d. from some distribution D over Σ(cid:63) × R. There are three parameters a user
can specify to control the behavior of the algorithm: a basis B = (P , S ) of Σ(cid:63) , a regularization
parameter τ > 0, and the desired number of states n in the hypothesis. The output returned by
HMC+SM is a WFA AZ with n states that computes a function fAZ : Σ(cid:63) → R.
The algorithm works in two stages. In the ﬁrst stage, a constrained matrix completion algorithm
with input Z and regularization parameter τ is used to return a Hankel matrix HZ ∈ HB . In the
second stage, the spectral method is applied to HZ to compute a WFA AZ with n states. These two
steps will be described in detail in the following sections.
As will soon become apparent, HMC+SM deﬁnes in fact a whole family of algorithms. In particular,
by combining the spectral method with any algorithm for solving the Hankel matrix completion
problem, one can derive a new algorithm for learning WFAs. For concreteness, in the following,
we will only consider the Hankel matrix completion algorithm described in Section 3.1. Through
its parametrization by a number 1 ≤ p ≤ ∞ and a convex loss (cid:96) : R × R → R+ , this completion
algorithm already gives rise to a family of learning algorithms that we denote by HMCp,(cid:96) +SM.
However, it is important to keep in mind that for each existing matrix completion algorithm that can
be modiﬁed to solve the Hankel matrix completion problem, a new algorithm for learning WFAs
can be obtained via the general scheme we describe below.

3.1 Hankel Matrix Completion
We now describe our Hankel matrix completion algorithm. Given a basis B = (P , S ) of Σ(cid:63) and a
sample Z over Σ(cid:63) × R, the algorithm solves a convex optimization problem and returns a matrix
HZ ∈ HB . We give two equivalent descriptions of this optimization, one in terms of functions
h : P S → R, and another in terms of Hankel matrices H ∈ RP×S . While the former is perhaps
conceptually simpler, the latter is easier to implement within the existing frameworks of convex
We will denote by (cid:101)Z the subsample of Z formed by examples z = (x, y) with x ∈ P S and by (cid:101)m
optimization.
its size | (cid:101)Z |. For any p ∈ [1, +∞] and a convex loss function (cid:96) : R × R → R+ , we consider the
(cid:88)
objective function FZ deﬁned for any h ∈ H by
FZ (h) = τ N (h) + (cid:98)R (cid:101)Z (h) = τ (cid:107)h(cid:107)2
1(cid:101)m
(x,y)∈ (cid:101)Z
p +
(cid:96)(h(x), y) ,
where τ > 0 is a regularization parameter. FZ is a convex function, by the convexity of (cid:107) · (cid:107)p and (cid:96).
Our algorithm seeks to minimize this loss function over the ﬁnite-dimensional vector space H and
returns a function hZ satisfying
hZ ∈ argmin
(HMC-h)
FZ (h) .
h∈H
To deﬁne an equivalent optimization over the matrix version of H, we introduce the following no-
tation. For each string x ∈ P S , ﬁx a pair of coordinate vectors (ux , vx ) ∈ RP × RS such that
x Hvx = H(x) for any H ∈ H. That is, ux and vx are coordinate vectors corresponding respec-
u(cid:62)
tively to a preﬁx u ∈ P and a sufﬁx v ∈ S , and such that uv = x. Now, abusing our previous
(cid:88)
FZ (H) = τ N (H) + (cid:98)R (cid:101)Z (H) = τ (cid:107)H(cid:107)2
notation, we deﬁne the following loss function over matrices:
1(cid:101)m
(cid:96)(u(cid:62)
(x,y)∈ (cid:101)Z
x Hvx , y) .
p +
This is a convex function deﬁned over the space of all |P | × |S | matrices. Optimizing FZ over the
convex set of Hankel matrices H leads to an algorithm equivalent to (HMC-h):
HZ ∈ argmin
FZ (H ) .
H∈H
We note here that our approach shares some common aspects with some previous work in matrix
completion. The fact that there may not be a true underlying Hankel matrix makes it somewhat close
to the agnostic setting in [18], where matrix completion is also applied under arbitrary distributions.
Nonetheless, it is also possible to consider other learning frameworks for WFAs where algorithms
for exact matrix completion [14, 27] or noisy matrix completion [13] may be useful. Furthermore,
since most algorithms in the literature of matrix completion are based on convex optimization prob-
lems, it is likely that most of them can be adapted to solve constrained matrix completions problems
such as the one we discuss here.

(HMC-H)

5

3.2 Spectral Method for General WFA

Here, we describe how the spectral method can be applied to HZ to obtain a WFA. We use the
same notation as in [7] and a version of the spectral method working with an arbitrary basis (as in
[5, 4, 7]), in contrast to versions restricted to P = Σ≤2 and S = Σ like [19].
We ﬁrst need to partition HZ into k + 1 blocks as follows. Since B is a basis, P is Σ-complete
and admits a root P (cid:48) . We deﬁne a block Ha ∈ RP (cid:48)×S for each a ∈ Σ(cid:48) , whose entries are given by
Ha (u, v) = HZ (ua, v), for any u ∈ P (cid:48) and v ∈ S . Thus, after suitably permuting the rows of HZ ,
we can write H(cid:62)
Z = [H(cid:62)
 , H(cid:62)
a1 , . . . , H(cid:62)
ak ]. We will use the following speciﬁc notation to refer to the
rows and columns of H corresponding to  ∈ P (cid:48) ∩ S : h,S ∈ RS with h,S (v) = H (, v) and
hP (cid:48) , (u) ∈ RP (cid:48)
with hP (cid:48) , (u) = H (u, ).
Using this notation, the spectral method can be described as follows. Given the desired number
of states n, it consists of ﬁrst computing the truncated SVD of H corresponding to the n largest
singular values: UnDnV(cid:62)
n . Thus, matrix UnDnV(cid:62)
n is the best rank n approximation to H with
respect to the Frobenius norm. Then, using the right singular vectors Vn of H , the next step
consists of computing a weighted automaton AZ = (cid:104)α, β , {Aa}(cid:105) as follows:
= h(cid:62)
α(cid:62)
Aa = (HVn )+ HaVn .
β = (HVn )+ hP (cid:48) ,
,S Vn

(SM)

The fact that the spectral method is based on a singular value decomposition justiﬁes in part the use
of a Schatten p-norm as a regularizer in (HMC-H). In particular, two very natural choices are p = 1
and p = 2. The ﬁrst one corresponds to a nuclear norm regularized optimization, which is known
to enforce a low rank constraint on HZ . In a sense, this choice can be justiﬁed in view of Theorem
1 when the target is known to be generated by some WFA. On the other hand, choosing p = 2 also
has some effect on the spread of singular values, while at the same time enforcing the coefﬁcients
in HZ – especially those that are completely unknown – to be small. As our analysis suggests, this
last property is important for preventing errors from accumulating on the values assigned by AZ to
long strings.

4 Generalization Bound

In this section, we study the generalization properties of HMCp,(cid:96) +SM. We give a stability analysis
for a special instance of this family of algorithms and use it to derive a generalization bound. We
study the speciﬁc case where p = 2 and (cid:96)(y , y (cid:48) ) = |y − y (cid:48) | for all (y , y (cid:48) ). But, much of our analysis
can be used to derive similar bounds for other instances of HMCp,(cid:96) +SM. The proofs of the technical
results presented are given in the Appendix.
We ﬁrst introduce some notation needed for the presentation of our main result. For any ν > 0, let
tν be the function deﬁned by tν (x) = x for |x| ≤ ν and tν (x) = ν sign(x) for |x| > ν . For any
distribution D over Σ(cid:63) × R, we denote by DΣ its marginal distribution over Σ(cid:63) . The probability that
a string x ∼ DΣ belongs to P S is denoted by π = DΣ (P S ).
We assume that the parameters B , n, and τ are ﬁxed. Two parameters that depend on D will appear
in our bound. In order to deﬁne these parameters, we need to consider the output HZ of (HMC-H)
as a random variable that depends on the sample Z . Writing H(cid:62)
Z = [H(cid:62)
 , H(cid:62)
a1 , . . . , H(cid:62)
(cid:2)σn (H )2 − σn+1 (H )2 (cid:3) ,
ak ], as in
Section 3.2, we deﬁne:
ρ = E
σ = E
Z∼Dm
Z∼Dm
where σn (M) denotes the nth singular value of matrix M. Note that these parameters may vary
with m, n, τ and B .
In contrast to previous learning results based on the spectral method, our bound holds in an agnostic
setting. That is, we do not require that the data was generated from some (probabilistic) unknown
WFA. However, in order to prove our results we do need to make two assumptions about the tails of
the distribution. First, we need to assume that there exists a bound on the magnitude of the labels
generated by the distribution.
Assumption 1 There exists a constant ν > 0 such that if (x, y) ∼ D , then |y | ≤ ν almost surely.

[σn (H )]

6

Second, we assume that the strings generated by the distribution will not be too long. In particular,
that the length of the strings generated by DΣ follows a distribution whose tail is slightly lighter than
sub-exponential.
Assumption 2 There exist constants c, η > 0 such that Px∼DΣ [|x| ≥ t] ≤ exp(−ct1+η ) holds for
all t ≥ 0.

ln

1
δ

.

ln m
m1/3

We note that in the present context both assumptions are quite reasonable. Assumption 1 is equiva-
lent to assumptions made in other contexts where a stability analysis is pursued, e.g., in the analysis
of support vector regression in [11]. Furthermore, in our context, this assumption can be relaxed
to require only that the distribution over labels be sub-Gaussian, at the expense of a more complex
proof.
Assumption 2 is required by the fact already pointed out in [19] that errors in the estimation of
operator models accumulate exponentially with the length of the string. Moreover, it is well known
that the tail of any probability distribution generated by a WFA is sub-exponential. Thus, though we
do not require DΣ to be generated by a WFA, we do need its distribution over lengths to have a tail
behavior similar to that of a distribution generated by a WFA. This seems to be a limitation common
to all known learnability proofs based on the spectral method.
in terms of the empirical loss (cid:98)RZ (f ) = |Z |−1 (cid:80)
We can now state our main result, which is a bound on the average loss R(f ) = Ez∼D [(cid:96)(f (x), y)]
z∈Z (cid:96)(f (x), y).
Theorem 2 Let Z be a sample formed by m i.i.d. examples generated from some distribution D
satisfying Assumptions 1 and 2. Let AZ be the WFA returned by algorithm HMCp,(cid:96) + SM with p = 2
and loss function (cid:96)(y , y (cid:48) ) = |y − y (cid:48) |. Then, for any δ > 0, the following holds with probability at
(cid:114)
(cid:18) ν 4 |P |2 |S |3/2
(cid:19)
least 1 − δ for fZ = tν ◦ fAZ :
R(fZ ) ≤ (cid:98)RZ (fZ ) + O
τ σ3ρπ
The proof of this theorem is based on an algorithmic stability analysis. Thus, we will consider
two samples of size m, Z ∼ Dm consisting of m i.i.d. examples drawn from D , and Z (cid:48) differing
m in Z (cid:48) = (z1 , . . . , zm−1 , z (cid:48)
from Z by just one point: say zm in Z = (z1 , . . . , zm ) and z (cid:48)
m ). The
m is an arbitrary point the support of D . Throughout the analysis we use the shorter
new example z (cid:48)
notation H = HZ and H(cid:48) = HZ (cid:48) for the Hankel matrices obtained from (HMC-H) based on
samples Z and Z (cid:48) respectively.
The ﬁrst step in the analysis is to bound the stability of the matrix completion algorithm. This is
done in the following lemma, that gives a sample-dependent and a sample-independent bound for
the stability of H.
(cid:26)
(cid:27)
Lemma 3 Suppose D satisﬁes Assumption 1. Then, the following holds:
(cid:112)|P ||S |,
τ min{ (cid:101)m, (cid:101)m(cid:48)}
(cid:107)H − H(cid:48) (cid:107)F ≤ min
1
The standard method for deriving generalization bounds from algorithmic stability results could be
applied here to obtain a generalization bound for our Hankel matrix completion algorithm. However,
our goal is to give a generalization bound for the full HMC+ SM algorithm.
Using the bound on the Frobenius norm (cid:107)H − H(cid:48)(cid:107)F , we are able to analyze the stability of σn (H ),
σn (H )2 − σn+1 (H )2 , and Vn using well-known results on the stability of singular values and
singular vectors. These results are used to bound the difference between the operators of WFA AZ
and AZ (cid:48) . The following lemma can be proven by modifying and extending some of the arguments
of [19, 4], which were given in the speciﬁc case of WFAs representing a probability distribution.
Lemma 4 Let ε = (cid:107)H−H(cid:48)(cid:107)F , (cid:98)σ = min{σn (H ), σn (H(cid:48)
 )}, and (cid:98)ρ = σn (H )2 −σn+1 (H )2 . Sup-
pose ε ≤ (cid:112) (cid:98)ρ/4. Then, there exists some constant C > 0 such that the following three inequalities
7

2ν

.

hold:

a(cid:107) ≤ C εν 3 |P |3/2 |S |1/2 / (cid:98)ρ(cid:98)σ2 ;
∀a ∈ Σ : (cid:107)Aa − A(cid:48)
(cid:107)α − α(cid:48)(cid:107) ≤ C εν 2 |P |1/2 |S |/ (cid:98)ρ;
(cid:48)(cid:107) ≤ C εν 3 |P |3/2 |S |1/2 / (cid:98)ρ(cid:98)σ2 .
(cid:107)β − β
The other half of the proof results from combining Lemmas 3 and 4 to obtain a bound for
|fZ (x) − fZ (cid:48) (x)|. This is a delicate step, because some of the bounds given above involve quantities
that are deﬁned in terms of Z . Therefore, all these parameters need to be controlled in order to
ensure that the bounds do not grow too large. Furthermore, to obtain the desired bounds we need to
extend the usual tools for analyzing spectral methods to the current setting. In particular, these tools
need to be adapted to the agnostic settings where there is no underlying true WFA. The analysis is
further complicated by the fact that now the functions we are trying to learn and the distribution that
generates the data are not necessarily related.
Once all this is achieved, it remains to combine these new tools to show an algorithmic stability
result for HMCp,(cid:96) +SM. In the following lemma, we ﬁrst deﬁne “bad” samples Z and show that bad
samples have a very low probability.
Lemma 5 Suppose D satisﬁes Assumptions 1 and 2.
If Z is a large enough i.i.d. sample from
((1/c) ln(4m4 ))1/1+η for all i, ε ≤ 4/(τ πm), (cid:98)σ ≥ σ/2, and (cid:98)ρ ≥ ρ/2.
D , then with probability at least 1 − 1/m3 the following inequalities hold simultaneously: |xi | ≤
After that we give two upper bounds for |fZ (x) − fZ (cid:48) (x)|: a tighter bound that holds for “good”
samples Z and Z (cid:48) and a another one that holds for all samples. These bounds are combined using
a variant of McDiarmid’s inequality for dealing with functions that do not satisfy the bounded dif-
ferences assumption almost surely [21]. The rest of the proof then follows the same scheme as the
standard one for deriving generalization bounds for stable algorithms [11, 25].

5 Conclusion

We described a new algorithmic solution for learning arbitrary weighted automata from a sam-
ple of labeled strings drawn from an unknown distribution. Our approach combines an algorithm
for constrained matrix completion with the recently developed spectral learning methods for learn-
ing probabilistic automata. Using our general scheme, a broad family of algorithms for learning
weighted automata can be obtained. We gave a stability analysis of a particular algorithm in that
family and used it to prove generalization bounds that hold for all distributions satisfying two rea-
sonable assumptions. The particular case of Schatten p-norm with p = 1, which corresponds to
a regularization with the nuclear norm, can be analyzed using similar techniques. Our results can
be further extended by deriving generalization guarantees for all algorithms in the family we intro-
duced. An extensive and rigorous empirical comparison of all these algorithms will be an important
complement to the research we presented. Finally, learning DFAs under an arbitrary distribution
using the algorithms we presented deserves a speciﬁc study since the problem is of interest in many
applications and since it may beneﬁt from improved learning guarantees.

Acknowledgments

Borja Balle is partially supported by an FPU fellowship (AP2008-02064) and project TIN2011-
27479-C04-03 (BASMATI) of the Spanish Ministry of Education and Science, the EU PASCAL2
NoE (FP7-ICT-216886), and by the Generalitat de Catalunya (2009-SGR-1428). The work of
Mehryar Mohri was partly funded by the NSF grant IIS-1117591.

8

References
[1] J. Albert and J. Kari. Digital image compression. In Handbook of Weighted Automata. Springer, 2009.
[2] A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y-K. Liu. Two SVDs sufﬁce: Spectral decom-
positions for probabilistic topic modeling and latent dirichlet allocation. CoRR, abs/1204.6703, 2012.
[3] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden
Markov models. COLT, 2012.
[4] R. Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maximization. ACML, 2011.
[5] R. Bailly, F. Denis, and L. Ralaivola. Grammatical inference as a principal component analysis problem.
ICML, 2009.
[6] B. Balle, A. Quattoni, and X. Carreras. A spectral learning algorithm for ﬁnite state transducers. ECML–
PKDD, 2011.
[7] B. Balle, A. Quattoni, and X. Carreras. Local loss optimization in operator models: A new insight into
spectral learning. ICML, 2012.
[8] A. Beimel, F. Bergadano, N.H. Bshouty, E. Kushilevitz, and S. Varricchio. Learning functions represented
as multiplicity automata. JACM, 2000.
[9] J. Berstel and C. Reutenauer. Rational Series and Their Languages. Springer, 1988.
[10] B. Boots, S. Siddiqi, and G. Gordon. Closing the learning planning loop with predictive state representa-
tions. I. J. Robotic Research, 2011.
[11] O. Bousquet and A. Elisseeff. Stability and generalization. JMLR, 2002.
[12] T. M. Breuel. The OCRopus open source OCR system. IS&T/SPIE Annual Symposium, 2008.
[13] E.J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010.
[14] E.J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans-
actions on Information Theory, 2010.
[15] Jack W. Carlyle and Azaria Paz. Realizations by stochastic ﬁnite automata. J. Comput. Syst. Sci., 5(1):26–
40, 1971.
[16] S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. Spectral learning of latent-variable
PCFGs. ACL, 2012.
[17] M. Fliess. Matrices de Hankel. Journal de Math ´ematiques Pures et Appliqu ´ees, 53:197–222, 1974.
[18] R. Foygel, R. Salakhutdinov, O. Shamir, and N. Srebro. Learning with the weighted trace-norm under
arbitrary sampling distributions. NIPS, 2011.
[19] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. COLT,
2009.
[20] M. Kearns and L. Valiant. Cryptographic limitations on learning boolean formulae and ﬁnite automata.
JACM, 1994.
[21] S. Kutin. Extensions to McDiarmid’s inequality when differences are bounded with high probability.
Technical report, TR-2002-04, University of Chicago, 2002.
[22] F.M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning in non-deterministic dependency
parsing. EACL, 2012.
[23] M. Mohri. Weighted automata algorithms. In Handbook of Weighted Automata. Springer, 2009.
[24] M. Mohri, F. C. N. Pereira, and M. Riley. Speech recognition with weighted ﬁnite-state transducers. In
Handbook on Speech Processing and Speech Communication. Springer, 2008.
[25] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press, 2012.
[26] A.P. Parikh, L. Song, and E.P. Xing. A spectral algorithm for latent tree graphical models. ICML, 2011.
[27] B. Recht. A simpler approach to matrix completion. JMLR, 2011.
[28] Arto Salomaa and Matti Soittola. Automata-Theoretic Aspects of Formal Power Series. Springer-Verlag:
New York, 1978.
[29] M.P. Sch ¨utzenberger. On the deﬁnition of a family of automata. Information and Control, 1961.
[30] S. M. Siddiqi, B. Boots, and G. J. Gordon. Reduced-rank hidden Markov models. AISTATS, 2010.
[31] L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola. Hilbert space embeddings of hidden Markov
models. ICML, 2010.

9


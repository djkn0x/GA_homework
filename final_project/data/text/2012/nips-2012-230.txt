Bayesian nonparametric models for bipartite graphs

Franc¸ ois Caron
INRIA
IMB - University of Bordeaux
Talence, France
Francois.Caron@inria.fr

Abstract

We develop a novel Bayesian nonparametric model for random bipartite graphs.
The model is based on the theory of completely random measures and is able
to handle a potentially inﬁnite number of nodes. We show that the model has
appealing properties and in particular it may exhibit a power-law behavior. We
derive a posterior characterization, a generative process for network growth, and
a simple Gibbs sampler for posterior simulation. Our model is shown to be well
ﬁtted to several real-world social networks.

1

Introduction

The last few years have seen a tremendous interest in the study, understanding and statistical mod-
eling of complex networks [14, 6]. A network is a set if items, called vertices, with connections
between them, called edges. In this article, we shall focus on bipartite networks, also known as two-
mode, afﬁliation or collaboration networks [16, 17]. In bipartite networks, items are divided into two
different types A and B , and only connections between items of different types are allowed. Exam-
ples of this kind can be found in movie actors co-starring the same movie, scientists co-authoring a
scientiﬁc paper, internet users posting a message on the same forum, people reading the same book
or listening to the same song, members of the boards of company directors sitting on the same board,
etc. Following the readers-books example, we will refer to items of type A as readers and items of
type B as books. An example of bipartite graph is shown on Figure 1(b). An important summariz-
ing quantity of a bipartite graph is the degree distribution of readers (resp. books) [14]. The degree
of a vertex in a network is the number of edges connected to that vertex. Degree distributions of
real-world networks are often strongly non-Poissonian and exhibit a power-law behavior [15].
A bipartite graph can be represented by a set of binary variables (zij ) where zij = 1 if reader i has
read book j , 0 otherwise. In many situations, the number of available books may be very large and
potentially unknown. In this case, a Bayesian nonparametric (BNP) approach can be sensible, by
assuming that the pool of books is inﬁnite. To formalize this framework, it will then be convenient
∞(cid:88)
to represent the bipartite graph by a collection of atomic measures Zi , i = 1, . . . , n with
j=1
where {θj } is the set of books and typically Zi only has a ﬁnite set of non-zero zij corresponding to
books reader i has read. Grifﬁths and Ghahramani [8, 9] have proposed a BNP model for such binary
random measures. The so-called Indian Buffet Process (IBP) is a simple generative process for the
conditional distribution of Zi given Z1 , . . . , Zi−1 . Such process can be constructed by considering
that the binary measures Zi are i.i.d. from some random measure drawn from a beta process [19, 10].
It has found several applications for inferring hidden causes [20], choices [7] or features [5]. Teh
and Gor ¨ur [18] proposed a three-parameter extension of the IBP, named stable IBP, that enables to

zij δθj

Zi =

(1)

1

model a power-law behavior for the degree distribution of books. Although more ﬂexible, the stable
IBP still induces a Poissonian distribution for the degree of readers.
In this paper, we propose a novel Bayesian nonparametric model for bipartite graphs that addresses
some of the limitations of the stable IBP, while retaining computational tractability. We assume
that each book j is assigned a positive popularity parameter wj > 0. This parameter measures the
popularity of the book, larger weights indicating larger probability to be read. Similarly, each reader
i is assigned a positive parameter γi which represents its ability to read books. The higher γi , the
more books the reader i is willing to read. Given the weights wj and γi , reader i reads book j with
probability 1 − exp(−γiwj ). We will consider that the weights wj and/or γi are the points of a
Poisson process with a given L ´evy measure. We show that depending on the choice of the L ´evy
measure, a power-law behavior can be obtained for the degree distribution of books and/or readers.
Moreover, using a set of suitably chosen latent variables, we can derive a generative process for
network growth, and an efﬁcient Gibbs sampler for approximate inference. We provide illustrations
of the ﬁt of the proposed model on several real-world bipartite social networks. Finally, we discuss
some potentially useful extensions of our work, in particular to latent factor models.

2 Statistical Model

2.1 Completely Random Measures

We ﬁrst provide a brief overview of completely random measures (CRM) [12, 13] before describing
the BNP model for bipartite graphs in Section 2.2. Let Θ be a measurable space. A CRM is a
random measure G such that for any collection of disjoint measurable subsets A1 , . . . , An of Θ,
the random masses of the subsets G(A1 ), . . . , G(An ) are independent. CRM can be decomposed
into a sum of three independent parts: a non-random measure, a countable collection of atoms with
i.e. G = (cid:80)∞
ﬁxed locations, and a countable collection of atoms with randoms masses at random locations. In this
paper, we will be concerned with models deﬁned by CRMs with random masses at random locations,
j=1 wj δθj . The law of G can be characterized in terms of a Poisson process over the point
set {(wj , θj ), j = 1, . . . , ∞} ⊂ R+ × Θ. The mean measure Λ of this Poisson process is known as
h : Θ → [0, +∞) and (cid:82)
the L ´evy measure. We will assume in the following that the L ´evy measure decomposes as a product
of two non-atomic densities, i.e. that G is a homogeneous CRM Λ(dw, dθ) = λ(w)h(θ)dwdθ with
over R+ with mean intensity λ. We will further assume that the total mass G(Θ) = (cid:80)∞
Θ h(θ)dθ = 1. It implies that the locations of the atoms in G are independent
of the masses, and are i.i.d. from h, while the masses are distributed according to a Poisson process
j=1 wj is
(cid:90) ∞
(cid:90) ∞
positive and ﬁnite with probability one, which is guaranteed if the following conditions are satisﬁed
0
0

(1 − exp(−w))λ(w)dw < ∞

λ(w)dw = ∞ and

(2)

and note g(x) its probability density function evaluated at x. We will refer to λ as the L ´evy intensity
(cid:90) ∞
in the following, and to h as the base density of G, and write G ∼ CRM(λ, h). We will also note
(cid:90) ∞
ψλ (t) = − log E [exp(−tG(Θ))] =
(cid:101)ψλ (t, b) =
0
(cid:90) ∞
(1 − exp(−tw))λ(w) exp(−bw)dw
0
κ(n, z ) =
0

(1 − exp(−tw))λ(w)dw

λ(w)wn e−zw dw

(3)

(4)

(5)

As a notable particular example of CRM, we can mention the generalized gamma process (GGP) [1],
whose L ´evy intensity is given by
w−σ−1 e−wτ
α
Γ(1 − σ)
GGP encompasses the gamma process (σ = 0), the inverse Gaussian process (σ = 0.5) and the
stable process (τ = 0) as special cases. Table ?? in supplementary material provides the expressions
of λ, ψ and κ for these processes.

λ(w) =

2

G =

2.2 A Bayesian nonparametric model for bipartite graphs
∞(cid:88)
Let G ∼ CRM(λ, h) where λ satisﬁes conditions (2). A draw G takes the form
(6)
wj δθj
j=1
where {θj } is the set of books and {wj } the set of popularity parameters of books. For i = 1, . . . , n,
∞(cid:88)
let consider the latent exponential process
(7)
Vi =
vij δθj
j=1
deﬁned for j = 1, . . . , ∞ by vij |wj ∼ Exp(wj γi ) where Exp(a) denotes the exponential distri-
(cid:26) zij = 1 if vij < 1
bution of rate a. The higher wj and/or γi , the lower vij . We then deﬁne the binary process Zi
∞(cid:88)
conditionally on Vi by
(8)
zij δθj with
Zi =
zij = 0 otherwise
j=1
By integrating out the latent variables vij we clearly have p(zij = 1|wj , γi ) = 1 − exp(−γiwj ).
1, . . . , ∞} ⊂ Θ, of intensity measure ψλ (γi )h(θ∗ ). Hence, the total mass Zi (Θ) = (cid:80)∞
Proposition 1 Zi is marginally characterized by a Poisson process over the point set {(θ∗
j ), j =
j=1 zij ,
which corresponds to the total number of books read by reader i is ﬁnite with probability one and
admits a Poisson(ψλ (γi )) distribution, where ψλ (z ) is deﬁned in Equation (3), while the locations
θ∗
j are i.i.d. from h.
tary material. As an example, for the gamma process we have Zi (Θ) ∼ Poisson (cid:0)α log (cid:0)1 + γi
(cid:1)(cid:1).
The proof, which makes use of Campbell’s theorem for point processes [13] is given in supplemen-
τ
∞(cid:88)
It will be useful in the following to introduce a censored version of the latent process Vi , deﬁned by
(9)
Ui =
uij δθj
j=1
where uij = min(vij , 1), for i = 1, . . . , n and j = 1, . . . , ∞. Note that Zi can be obtained
deterministically from Ui .

2.3 Characterization of the conditional distributions

The conditional distribution of G given Z1 , . . . , Zn cannot be obtained in closed form1 . We will
make use of the latent process Ui . In this section, we derive the formula for the conditional laws
P (U1 , . . . , Un |G), P (U1 , . . . , Un ) and P (G|U1 , . . . , Un ) . Based on these results, we derive in Sec-
Assume that K books {θ1 , . . . , θK } have appeared. We write Ki = Zi (Θ) = (cid:80)∞
tion 2.4 a generative process and in Section 2.5 a Gibbs sampler for our model, that both rely on the
introduction of these latent variables.
i=1 Zi ({θj }) = (cid:80)n
of reader i (number of books read by reader i) and mj = (cid:80)n
j=1 zij the degree
i=1 zij the degree of


 K(cid:89)
 exp (−γiG(Θ\{θ1 , . . . , θK }))
book j (number of people having read book j ). The conditional likelihood of U1 , . . . Un given G is
n(cid:89)
given by
P (U1 , . . . Un |G) =
exp (−γiwj uij )
(cid:33)  K(cid:89)
(cid:33) exp
γ zij
i wzij
(cid:32)
(cid:32) n(cid:89)
(cid:32)
(cid:32) n(cid:88)
(cid:33)
(cid:33)
j
n(cid:88)
i=1
j=1
−
wmj
(10)
exp
j
i=1
i=1
i=1
j=1
1 In the case where γi = γ , it is possible to derive P (Z1 , . . . , Zn ) and P (Zn+1 |Z1 , . . . , Zn ) where the
random measure G and the latent variables U are marginalized out. This particular case is described in supple-
mentary material.

γi (uij − 1)

−wj

γi

G(Θ)

=

γKi
i

3

wj δθj

(12)

(cid:33)

(cid:32)

n(cid:88)
i=1

−w

γi

(cid:33)

γi

h(θj )κ

P (U1 , . . . Un ) =

γKi
i

exp

(13)

(14)

n(cid:88)
i=1

γiuij

(11)

and the masses are

(cid:32)
mj ,

(cid:32) n(cid:89)
(cid:33)
(cid:34)
(cid:32) n(cid:88)
(cid:33)(cid:35) K(cid:89)
Proposition 2 The marginal distribution P (U1 , . . . Un ) is given by
−ψλ
i=1
i=1
j=1
where ψλ and κ are resp. deﬁned by Eq. (3) and (5).
Proof. The proof, detailed in supplementary material, is obtained by an application of the Palm
formula for CRMs [3, 11], and is the same as that of Theorem 1 in [2].
Proposition 3 The conditional distribution of G given the latent processes U1 , . . . Un can be ex-
K(cid:88)
pressed as
G = G∗ +
j=1
where G∗ and (wj ) are mutually independent with
G∗ ∼ CRM(λ∗ , h)
λ∗ (w) = λ(w) exp
(cid:80)n
κ(mj , (cid:80)n
exp (−wj
λ(wj )wmj
i=1 γiUij )
P (wj |rest) =
j
i=1 γiuij )
Proof. The proof, based on the application of the Palm formula and detailed in supplementary
In the case of the GGP, G∗ is still a GGP of parameters (α∗ = α, σ∗ = σ, τ ∗ = τ + (cid:80)n
material, is the same as that of Theorem 2 in [2].
(cid:32)
(cid:33)
i=1 γi ), while
n(cid:88)
the wj ’s are conditionally gamma distributed, i.e.
wj |rest ∼ Gamma
mj − σ, τ +
i=1
K(cid:88)
Corollary 4 The predictive distribution of Zn+1 given the latent processes U1 , . . . , Un is given by
Zn+1 = Z ∗
zn+1,j δθj
n+1 +
j=1
1 − κ(mj , τ + γn+1 + (cid:80)n
(cid:18)
where the zn+1,j are independent of Z ∗
n+1 with
κ(mj , τ + (cid:80)n
zn+1,j |U ∼ Ber
i=1 γiuij )
i=1 γiuij )
where Ber is the Bernoulli distribution and Z ∗
n+1 is a homogeneous Poisson process over Θ of
intensity measure ψλ∗ (γn+1 ) h(θ).
 Poisson
i=1 γi )σ (cid:105)(cid:17)
(cid:17)σ − (τ + (cid:80)n
(cid:104)(cid:16)
(cid:16) α
τ + (cid:80)n+1
(cid:16)
(cid:16)
(cid:17)(cid:17)
i=1 γi
τ +(cid:80)n
(cid:19)−mj +σ(cid:33)
(cid:32)
σ
(cid:18)
1 + γn+1
α log
Poisson
τ + (cid:80)n
i=1 γi
1 −
zn+1,j |U ∼ Ber
γn+1
.
i=1 γiuij
n(cid:88)
Finally, we consider the distribution of un+1,j |zn+1,j = 1, u1:n,j . This is given by
p(un+1,j |zn+1,j = 1, u1:n,j ) ∝ κ(mj + 1, un+1,j γn+1 +
i=1

For the GGP, we have
n+1 (Θ) ∼
Z ∗

if σ (cid:54)= 0
if σ = 0

γiuij )1un+1,j ∈[0,1]

(15)

and

1 +

γiuij

(cid:19)

In supplementary material, we show how to sample from this distribution by the inverse cdf method
for the GGP.

4

Reader 1

Reader 2

Reader 3

18
4
0
12
16 10

14
8
0

Books

13
0

4
14

9

6

(a)

...
...
...

A1

A2

A3

B1

B2

B3

B4

B5

B6

B7

(b)

Figure 1: Illustration of the generative process described in Section 2.4.

2.4 A generative process

In this section we describe the generative process for Zi given (U1 , . . . , Ui−1 ), G being integrated
out. This reinforcement process, where popular books will be more likely to be picked, is remi-
niscent of the generative process for the beta-Bernoulli process, popularized under the name of the
Indian buffet process [8]. Let xij = − log(uij ) ≥ 0 be latent positive scores.
Consider a set of n readers who successively enter into a library with an inﬁnite number of books.
Each reader i = 1, . . . n, has some interest in reading quantiﬁed by a positive parameter γi > 0.
The ﬁrst reader picks a number K1 ∼ Poisson(ψλ (γ1 )) books. Then he assigns a positive score
x1j = − log(u1j ) > 0 to each of these books, where u1j is drawn from distribution (15).
Now consider that reader i enters into the library, and knows about the books read by previous
readers and their scores. Let K be the total number of books chosen by the previous i − 1 readers,
and mj the number of times each of the K books has been read. Then for each book j = 1, . . . , K ,
1 − κ(mj , τ + γi + (cid:80)i−1
reader i will choose this book with probability
κ(mj , τ + (cid:80)i−1
k=1 γk ukj )
k=1 γk ukj )
(cid:32) (cid:101)ψλ
(cid:32)
(cid:33)(cid:33)
i−1(cid:88)
and then will choose an additional number of K +
i books where
i ∼ Poisson
K +
γi ,
γk
k=1
Reader i will then assign a score xij = − log uij > 0 to each book j he has read, where uij is drawn
from (15). Otherwise he will set the default score xij = 0. This generative process is illustrated in
Figure 1 together with the underlying bipartite graph . In Figure 2 are represented draws from this
generative process with a GGP with parameters γi = 2 for all i, τ = 1, and different values for α
and σ .

2.5 Gibbs sampling

From the results derived in Proposition 3, a Gibbs sampler can be easily derived to approximate
the posterior distribution P (G, U |Z ). The sampler successively updates U given (w, G∗ (Θ)) then
(w, G∗ (Θ)) given U . We present here the conditional distributions in the GGP case. For i =
1, . . . , n, j = 1, . . . , K , set uij = 1 if zij = 0, otherwise sample
uij |zij , wj , γi ∼ rExp(γiwj , 1)
where rExp(λ, a) is the right-truncated exponential distribution of pdf λ exp(−λx)/(1 −
(cid:32)
(cid:33)
exp(−λa))1x∈[0,a] from which we can sample exactly. For j = 1, . . . , K , sample
n(cid:88)
wj |U, γi ∼ Gamma
mj − σ, τ +
and the total mass G∗ (Θ) follows a distribution g∗ (w) ∝ g(w) exp (−w (cid:80)n
γiuij
i=1
i=1 γi ) where g(w) is
update G∗ (Θ) ∼ Gamma (α, τ + (cid:80)n
the distribution of G(Θ). In the case of the GGP, g∗ (w) is an exponentially tilted stable distribution
for which exact samplers exist [4]. In the particular case of the gamma process, we have the simple
i=1 γi ) .

5

(a) α = 1, σ = 0

(b) α = 5, σ = 0

(c) α = 10, σ = 0

(d) α = 2, σ = 0.1

(e) α = 2, σ = 0.5

(f) α = 2, σ = 0.9

Figure 2: Realisations from the generative process of Section 2.4 with a GGP of parameters γ = 2,
τ = 1 and various values of α and σ .

zij , bγ +

wj uij + G∗ (Θ)

γi |G, U ∼ Gamma

3 Update of γi and other hyperparameters
We may also consider the weight parameters γi to be unknown and estimate them from the graph.
We can assign a gamma prior γi ∼ Gamma(aγ , bγ ) with parameters (aγ > 0, bγ > 0) and update
aγ +

K(cid:88)
K(cid:88)
it conditionally on other variables with
j=1
j=1
In this case, the marginal distribution of Zi (Θ), hence the degree distribution of books, follows a
continuous mixture of Poisson distributions, which offers more ﬂexibility in the modelling.
associated to a given CRM Γ ∼ CRM(λγ , hγ ) and a measurable space of readers (cid:101)Θ. We then have
Γ = (cid:80)∞
We may also go a step further and consider that there is an inﬁnite number of readers with weights γi
i=1 γi δ(cid:101)θi
. This provides a lot of ﬂexibility in the modelling of the distribution of the degree
simplicity. Conditionally on (w, G∗ (Θ), U ), we have Γ = Γ∗ + (cid:80)n
of readers, allowing in particular to obtain a power-law behavior, as shown in Section 5. We focus
here on the case where Γ is drawn from a generalized gamma process of parameters (αγ , σγ , τγ ) for
 K(cid:88)

i=1 γi δ(cid:101)θi
K(cid:88)
where for i = 1, . . . , n,
zij − σγ , τ +
γi |G, U ∼ Gamma
wj uij + G∗ (Θ)
(cid:17)(cid:17)
(cid:16)(cid:80)K
(cid:16)−γ
j=1
j=1
update for (w, G∗ ) conditional on (U, γ , Γ( (cid:101)Θ)) is now for j = 1, . . . , K
and Γ∗ ∼ CRM(λ∗
j=1 wj + G∗ (Θ)
γ , hγ ) with λ∗
γ (γ ) = λγ (γ ) exp
(cid:32)
(cid:33)
n(cid:88)
γiuij + Γ∗ ( (cid:101)Θ)
mj − σ, τ +
wj |U, Γ ∼ Gamma
(cid:17)(cid:17)
(cid:16)−w
(cid:16)(cid:80)n
i=1 γi + Γ∗ ( (cid:101)Θ)
i=1
and G∗ ∼ CRM(λ∗ , h) with λ∗ (w) = λ(w) exp
. Note that
(cid:16)
(cid:16)(cid:80)n
(cid:17)(cid:17)
For the scale parameter α of
there is now symmetry in the treatment of books/readers.
i=1 γi + Γ∗ ( (cid:101)Θ)
the GGP, we can assign a gamma prior α ∼ Gamma(aα , bα ) and update it with α|γ ∼
. Other parameters of the GGP can be updated
Gamma
aα + K, bα + ψλ
using a Metropolis-Hastings step.

. In this case, the

6

BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders20406080510152025304 Discussion

Power-law behavior. We now discuss some of the properties of the model, in the case of the
GGP. The total number of books read by n readers is O(nσ ). Moreover, for σ > 0, the degree
distribution follows a power-law distribution: asymptotically, the proportion of books read by m
readers is O(m−1−σ ) (details in supplementary material). These results are similar to those of the
stable IBP [18]. However, in our case, a similar behavior can be obtained for the degree distribution
of readers when assigning a GGP to it, while it will always be Poisson for the stable IBP.
Connection to IBP. The stable beta process [18] is a particular case of our construction, obtained
by setting weights γi = γ and L ´evy measure

λ(w) = α

γ (1 − e−γw )−σ−1 e−γw(c+σ)

Γ(1 + c)
Γ(1 − σ)Γ(c + σ)
The proof is obtained by a change of variable from the L ´evy measure of the stable beta process.
Extensions to latent factor models. So far, we have assumed that the binary matrix Z was observed.
The proposed model can also be used as a prior for latent factor models, similarly to the IBP. As
an example of the potential usefulness of our model compared to IBP, consider the extraction of
features from time series of different lengths. Longer time series are more likely to exhibit more
features than shorter ones, and it is sensible in this case to assume different weights γi . In a more
general setting, we may want γi to depend on a set of metadata associated to reader i. Inference for
latent factor models is described in supplementary material.

(16)

5

Illustrations on real-world social networks

We now consider estimating the parameters of our model and evaluating its predictive performance
on six bipartite social networks of various sizes. We ﬁrst provide a short description of these net-
works. The dataset ‘Boards’ contains information about members of the boards of Norwegian com-
panies sitting at the same board in August 20112 . ‘Forum’ is a forum network about web users
contributing to the same forums3 . ‘Books’ concerns data collected from the Book-Crossing com-
munity about users providing ratings on books4 where we extracted the bipartite network from the
ratings. ‘Citations’ is the co-authorship network based on preprints posted to Condensed Matter
section of ArXiv between 1995 and 1999 [15]. ‘Movielens100k’ contains information about users
rating particular movies5 from which we extracted the bipartite network. Finally, ‘IMDB’ contains
information about actors co-starring a movie6 . The sizes of the different networks are given in
Table 1.

Dataset
Board

K
5766

n
355

899
5064
16726
943
28088

Edges
1746

7089
49997
58595
100000
341313

S-IBP
9.82
(29.8)
-6.7e3
83.1
-3.7e4
-6.7e4
-1.5e5

Forum
Books
Citations
Movielens100k
IMDB

IG
GGP
-145.1
-68.6
(81.9)
(31.9)
-5.5e3
-5.6e3
4.6e4
4.4e4
-3.1e4
-3.4e4
-5.5e4
-5.5e4
-1.1e5
-1.1e5
Table 1: Size of the different datasets and test log-likelihood of four different models.
We evaluate the ﬁt of four different models on these datasets. First, the stable IBP [18] with param-
eters (αIBP , τIBP , σIBP ) (S-IBP). Second, our model where the parameter γ is the same over dif-
ferent readers, and is assigned a ﬂat prior (SG). Third our model where each γi ∼ Gamma(aγ , bγ )
where (aγ , bγ ) are unknown parameters with ﬂat improper prior (IG). Finally, our model with a
GGP model for γi , with parameters (αγ , σγ , τγ ) (GGP). We divide each dataset between a training

SG
8.3
(30.8)
-6.7e3
214
-3.7e4
-6.7e4
-1.5e5

552
36275
22016
1682
178074

2Data can be downloaded from http://www.boardsandgender.com/data.php
3Data for the forum and citation datasets can be downloaded from http://toreopsahl.com/datasets/
4 http://www.informatik.uni-freiburg.de/ cziegler/BX/
5The dataset can be downloaded from http://www.grouplens.org
6The dataset can be downloaded from http://www.cise.uﬂ.edu/research/sparse/matrices/Pajek/IMDB.html

7

(a) S-IBP

(b) GS

(c) IG

(d) GGP

(h) GGP
(g) IG
(f) GS
(e) S-IBP
Figure 3: Degree distributions for movies (a-d) and actors (e-h) for the IMDB movie-actor dataset
with four different models. Data are represented by red plus and samples from the model by blue
crosses.

(a) S-IBP

(b) GS

(c) IG

(d) GGP

(h) GGP
(g) IG
(f) GS
(e) S-IBP
Figure 4: Degree distributions for readers (a-d) and books (e-h) for the BX books dataset with four
different models. Data are represented by red plus and samples from the model by blue crosses.

set containing 3/4 of the readers and a test set with the remaining. For each model, we approximate
the posterior mean of the unknown parameters (respectively (αIBP , τIBP , σIBP ), γ , (aγ , bγ ) and
(αγ , σγ , τγ ) for S-IBP, SG, IG and GGP) given the training network with a Gibbs sampler with
γ = (cid:98)αγ /3 to take into account the different sample
10000 burn-in iterations then 10000 samples; then we evaluate the log-likelihood of the estimated
model on the test data. For GGP, we use αtest
sizes. For ‘Boards’, we do 10 replications with random permutations given the small sample size
and report standard deviation together with mean value. Table 1 shows the results over the different
networks for the different models. Typically, S-IBP and SG give very similar results. This is not
surprising, as they share the same properties, i.e. Poissonian degree distribution for readers and
power-law degree distribution for books. Both methods perform better solely on the Board dataset,
where the Poisson assumption on the number of people sitting on the same board makes sense. On
all the other datasets, IG and GGP perform better and similarly, with slightly better performances for
IG. These two models are better able to capture the power-law distribution of the degrees of readers.
These properties are shown on Figures 3 and 4 which resp. give the empirical degree distributions
of the test network and a draw from the estimated models, for the IMDB dataset and the Books
dataset. It is clearly seen that the four models are able to capture the power-law behavior of the
degree distribution of actors (Figure 3(e-h)) or books (Figure 4(e-h)). However, only IG and GGP
are able to capture the power-law behavior of the degree distribution of movies (Figure 3(a-d)) or
readers (Figure 4(a-d)).

8

100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103104Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelDataReferences
[1] A. Brix. Generalized gamma measures and shot-noise Cox processes. Advances in Applied
Probability, 31(4):929–953, 1999.
[2] F. Caron and Y. W. Teh. Bayesian nonparametric models for ranked data. In Neural Information
Processing Systems (NIPS), 2012.
[3] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes. Springer
Verlag, 2008.
[4] L. Devroye. Random variate generation for exponentially and polynomially tilted stable dis-
tributions. ACM Transactions on Modeling and Computer Simulation (TOMACS), 19(4):18,
2009.
[5] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. Sharing features among dynamical
In Advances in Neural Information Processing Systems, vol-
systems with beta processes.
ume 22, pages 549–557, 2009.
[6] A. Goldenberg, A.X. Zheng, S.E. Fienberg, and E.M. Airoldi. A survey of statistical network
models. Foundations and Trends in Machine Learning, 2(2):129–233, 2010.
[7] D. G ¨or ¨ur, F. J ¨akel, and C.E. Rasmussen. A choice model with inﬁnitely many latent features. In
Proceedings of the 23rd international conference on Machine learning, pages 361–368. ACM,
2006.
[8] T Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In
NIPS, 2005.
[9] T. Grifﬁths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal
of Machine Learning Research, 12(April):1185–1224, 2011.
[10] N.L. Hjort. Nonparametric bayes estimators based on beta processes in models for life history
data. The Annals of Statistics, 18(3):1259–1294, 1990.
[11] L.F. James, A. Lijoi, and I. Pr ¨unster. Posterior analysis for normalized random measures with
independent increments. Scandinavian Journal of Statistics, 36(1):76–97, 2009.
[12] J.F.C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics, 21(1):59–78,
1967.
[13] J.F.C. Kingman. Poisson processes, volume 3. Oxford University Press, USA, 1993.
[14] M.E.J. Newman. The structure and function of complex networks. SIAM review, pages 167–
256, 2003.
[15] M.E.J. Newman, S.H. Strogatz, and D.J. Watts. Random graphs with arbitrary degree distribu-
tions and their applications. Physical Review E, 64(2):26118, 2001.
[16] M.E.J. Newman, D.J. Watts, and S.H. Strogatz. Random graph models of social networks.
Proceedings of the National Academy of Sciences, 99:2566, 2002.
[17] J.J. Ramasco, S.N. Dorogovtsev, and R. Pastor-Satorras. Self-organization of collaboration
networks. Physical review E, 70(3):036106, 2004.
[18] Y.W. Teh and D. G ¨or ¨ur. Indian buffet processes with power-law behavior. In NIPS, 2009.
[19] R. Thibaux and M. Jordan. Hierarchical beta processes and the Indian buffet process.
In
International Conference on Artiﬁcial Intelligence and Statistics, volume 11, pages 564–571,
2007.
[20] F. Wood, T.L. Grifﬁths, and Z. Ghahramani. A non-parametric Bayesian method for inferring
In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence,
hidden causes.
volume 22, 2006.

9


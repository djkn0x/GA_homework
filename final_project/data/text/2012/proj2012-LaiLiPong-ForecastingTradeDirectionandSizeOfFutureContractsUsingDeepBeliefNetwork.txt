Forecasting Trade Direction and Size of Future Contracts Using Deep
Belief Network
Anthony Lai (aslai), MK Li (l ilemon), Foon Wang  Pong  (ppong )

Abstract

A lgorithmic trad ing , high frequency trad ing  (HFT) in particular, has drawn a lot of interests within the computer
science community.  Market making  is one of the most common type of high frequency trad ing  strateg ies.
Market making  strateg ies prov ide l iquid ity to  financial  markets, and  in return, profit by capturing  the b id-ask
spread .  The profitab il ity of a market making  strategy is dependent on its ab il ity to  adapt to  demand  fluctuations
of the underlying  asset.  In this paper, we attempt to  use deep  bel ief network to  forecast trade d irection and  size
of future contracts.  The ab il ity to  pred ict trade d irection and  size enables automated  market maker to  manage
inventories more intel l igently.

Introduction

Market mak ing in  a nutshell
Accord ing  to  Garman (1976), buyers and  sel lers arrive randomly accord ing  to  a po isson process. [1] Assume that
A l ice wants to  buy 100 shares of Goog le.  There might not be someone who  wants to  sel l  100 shares of Goog le
right now.  Market maker plays the role of an intermed iary, quoting  at the b id  and  ask price simultaneously.
When a buyer or sel ler wants to  make a trade, the market maker act as a counterparty, taking  the other side of
the trade.  In return, the market maker profit from the b id-ask spread  for prov id ing  l iquid ity to  the market.

Figure  1. Il lustration of Center Limit Order Book

Figure  2.  Bid  Ask Price and  Arrival  Rates of Buyers
and  Sel lers.

Automated market mak ing strategy
An automated  market maker is connected  to  an exchange; usual ly v ia the FIX protocol .  The algorithm
strateg ical ly places l imit orders in the order book and  constantly read just the open orders based  on market
cond itions and  its inventory.  As we have mentioned  earl ier, the market maker profits from the b id-ask spread .
Let us formal ize the average profit of a market maker over time.  Let λBu y(Ask) and  λSell(Bid) be the arrival  rate of
the buyer and  sel ler at the ask and  b id  price respectively.  In the ideal  world , λBu y(Ask) is equal  to  λSell(Bid) and  the
average profit is g iven as:

However, in practice, this is rarely the case: buyers and  sel lers arrive randomly.  Figure 2 il lustrates how λBu y(Ask)
and  λSell(Bid) is related  to  the b id  and  ask prices. In theory, assuming  that there is only one market maker, he could
control  λBu y(Ask) and  λSell(Bid) by ad justing  the b id  and  ask prices.  In practice, market maker could  also  adapt by
strateg ical ly ad just their open orders in the order book. The ab il ity to  accurately pred ict the future d irections and

quantities of trades al low market maker to  properly gauge the supply and  demand  in the marketplace and
thereby to  mod ify their open orders.
Deep Belief Network  (DBN)

Figure  3.  Structure of our deep  bel ief network.
Consists of an input layer, 3 hidden layers of RBM and
an output layer of RBM.

Figure  4. Diagram of a restricted  Boltzmann machine.

Deep  bel ief network is a probab il istic generative model  proposed  by Hinton [2].  It consists of an input layer, an
output layer, and  is composed  of multiple layers of hidden variables stacked  on top  of each other.  Using
multiple hidden layers al lows the network to  learn higher order features.  Hinton has developed  an efficient,
greedy layer-by-layer approach for learning  the network weights.  Each layer consists of a Restricted  Boltzmann
Machine (RBM).  The deep  bel ief network is trained  in two  steps, namely, the unsuperv ised  pre-training  and  the
superv ised  fine tuning .

Unsupervised pre-training
Trad itional ly, deep  architectures is d ifficult to  train because of their non-convex  ob jective function.  As a result,
the parameters tend  to  converge at local  optima. Performing  unsuperv ised  pre-training  helps initial ize the
weights to  better values and  helps us find  better local  optima. [4] Unsuperv ised  pre-training  is performed  by
hav ing  each hidden layer greed ily learns the identity function one at a time, using  the output of the prev ious layer
as the input.

Supervised fine  tuning
Superv ised  fine tuning  is performed  after the unsuperv ised  pre-training .  The network is initial ized  using  the
weights from the pre-training .  By performing  unsuperv ised  pre-training , we are able to  find  better local  optima
in the fine tuning  phase than using  randomly initial ized  weights.

Restricted Boltzmann  Machine
Restricted  Boltzmann Machines (RBMs) consist of v isible and  hidden units that form a b ipartite graph (Figure 3).
The v isible and  hidden units are cond itional ly independent g iven the other layer.  The RBMs are then trained  by
max imizing  the l ikel ihood  g iven by the fol lowing  equation (Figure 5).  RBM is trained  using  a method  cal led
contrastive d ivergence that is developed  by Hinton. Further information can be found  in Hinton’s paper. [3]

Figure  5. Log  l ikel ihood  of  the restricted  Boltzmann machine. In this case, x(i) refers to  the v isible units.

Dataset

To  ensure the relevancy of our results, we acquired  tick-by-tick data of S&P equity index  Futures (ES) from the

Chicago  Mercantile Exchange (CME).  Tick-by-tick data includes al l  trade and  order book events occurred  at the
exchange.  It contains significantly more information than the typ ical  information obtained  from Goog le or
Yahoo  finance - which are summary of the tick-by-tick data.  The data we obtained  from CME is encoded  in FIX
format (Figure 5).  We implemented  a parser to  reconstruct  trade messages and  order book data.

1128=9|9=361|35=X|49=CME|34=2279356|52=20120917185520880|75=20120917|268=3|279=1|22=8|48=10113|83=2476017|107=ES
Z2|269=0|270=145200|271=1607|273=185520000|336=2|346=240|1023=6|279=1|22=8|48=10113|83=2476018|107=ESZ2|269=0|270=
145325|271=421|273=185520000|336=2|346=95|1023=1|279=1|22=8|48=10113|83=2476019|107=ESZ2|269=0|270=145325|271=416|
273=185520000|336=2|346=94|1023=1|10=141|
Figure  6. A  sing le FIX message encod ing  an incremental  market data update message.

Normalization  and data embedding
Our input vector is detailed  in Figure 7.  Prices and  time values are first converted  to  relative values by
subtracting  the values of the trade_price and  trade_time at t=0 respectively.  For instance, if the trade price at
t=0 and  t=-9 are 12,575 and  12,550 respectively, then trade_price0  = 0 and  trade_price9  = -25.

The neural  network requires input to  be in the range [0, 1]. We have therefore created  an embedd ing  by
converting  numerical  values into  b inary form.  For example, the final  result for trade size would  look as fol lows:
Trade size=500 → round(log2(500)) = 8 → [0, 0, 0, 0, 0, 0, 0, 0, 1, 0].   After these data transformations, our input
features becomes a 700x1 feature vector.

Results

Figure  7.  Il lustration of an Input vector.

Baseline  Approaches
For the purpose of this pro ject, we evaluated  several  machine learning  algorithms, in particular, log istic
regression and  support vector machines.  Log istic regression was a natural  cho ice because we wanted  to  make
b inary pred ictions about the d irections of future trades.  We also  attempted  to  use SVM for both the trade
d irection and  quantity pred iction tasks.  We d id  some field  trials comparing  the various learning  algorithms.  DBN
was the most accurate out of the three, and  therefore we decided  to  focus on DBN for the purpose of this
pro ject.

DBN
The structure of our deep  bel ief network is shown in Figure 3.  There are a total  of 3 hidden layers, each with 100
nodes.  This is the best configuration out of various other configurations of network topolog ies we explored .

Trade  D irection  Prediction
We want to  pred ict the next 10 trades g iven the prev ious 10 trades.  For this task, we trained  10 DBNs:  the ith
DBN is trained  to  pred ict the d irection of the trade at ti.  The results are shown in Figure 8.  For comparison, we
used  2 d ifferent sets of input:  one using  only information from the past 10 trades and  another using  both trade
and  order book information.

Figure  7. Accuracies of pred icting  trade d irection.  The pred ictions in the first row only uses trade information.
The pred ictions in the second  row uses both trade and  top  of the book information (namely the b id  price, b id
size, ask price, ask size).

Trade  Quantity Prediction
Similar to  the prev ious pred iction task, we trained  10 DBNs to  pred ict the sizes of the next 10 trades.  Our earl ier
attempt uses a log istic regression in the output layer to  pred ict a normal ized  value between [0, 1]. The pred iction
range is very narrow, between 0.2 and  0.3.  We attribute this to  the steepness of the sigmo id  function.  Posing  this
as a classification problem would  perhaps be more appropriate.  We therefore transformed  the trade quantity
into  10 d iscrete b ins.  The d istribution of the trade quantity is extremely skewed  as shown in  Figure 8.  As a
preprocessing  step , we log  normal ized  the trade quantity, reducing  the kurtosis from 80.52 to  2.9.  Do ing  so
created  10 d iscrete b ins, each corresponded  to  a power of 2.  Results are shown in Figure 10.

Figure  8. Histogram of trade quantities.

Figure  9. Histogram of log  normal ized  trade
quantities.

Figure  10. RMSE of pred icting  trade quantities. The pred ictions in the first row only uses trade information. The
pred ictions in the second  row uses both trade and  top  of the book information (namely the b id  price, b id  size,
ask price, ask size).

Error Analysis

Trade  direction  error analysis
Given the past 10 trades, our model  yields an accuracy of over 60%.  It appears that add itional  information, such
as the order book data, d id  not improve the result.  A lthough the unprocessed  order book data does not seems
very pred ictive, it could  potential ly be useful  if we manual ly eng ineer features that is not currently captured  by
the DBN.

We are interested  to  understand  why trade d irection can be pred icted .  The graph of trade d irections are shown

in figure 11.  By inspection, we d iscovered  that the trades tend  to  be of consecutive buys and  sel ls, which explains
it far from random and  al low our algorithm to  perform some pred ictions.

Figure  11.  Time series of trade d irections.  Buy and  sel l  orders are shown at the top  and  bottom
respectively.

Trade  quantity error analysis
From the result above, we can see that the pred iction for the trade quantities are fairly inconsistent.  It is unclear
if there ex ists any correlations between our input data and  the trade quantities.  Perhaps, we may need
add itional  information, other than the last 10 trades, to  improve the pred iction.  We also  tried  to  gain add itional
insights about the DBN by v isual izing  its weights, but it turned  out not to  be too  informative either.

The RMSE of the results are on average over 2 b ins d ifferent from the actual  labels.   When we look at the output
of the DBN, it pred icts the majority class most of the time - in this case it is b in 0.  We attribute this to  the
skewness of the class labels.  One way to  deal  with skewed  class d istribution is to  resample from the underlying
dataset to  obtain a more evenly d istributed  set of training  examples.

There are potential ly multiple factors contributing  to  such unpred ictab il ity.  First, when trad ing  large quantities,
hedge funds and  institutional  traders use several  ways to  minimize market impact. The simplest way is to  d iv ide
large trades into  smal ler trades.  More complex  approaches involve further obscuring  the trad ing  patterns by
introducing  no ise using  multiple buying  and  sel l ing  transactions. [5]  Moreover, other market makers may use
similar algorithms to  pred ict the demand  and  supply. As a result, any arb itrage opportunities would  d isappear
quickly, lead ing  to  mixed  signals ming led  with each other.

Perhaps, the more interesting  problem is the pred iction of outl iers -- trades with large quantities.  The ab il ity to
pred ict these outl iers is more important as these outl iers would  cause the market maker to  over-buy or over-sel l
the asset, lead ing  to  increased  risks and  unbalance inventories.

Future  Work
We would  l ike to  investigate further using  other complementary tools and  information and  see if we can see
improveme our current results.  One possib il ity is to  include data from highly correlated  financial  product such as
Nasdaq  futures market(NQ) as add itional  features.  The rationale behind  this is similar to  pairs trad ing . If a large
quantity is traded  in Nasdaq , it might inform us about trades related  to  ES (S&P) futures.  Another possib il ity is to
include other handcrafted  statistical  information such as mov ing  averages as input features.  There could  be
information not captured  by the fundamental  trade information that can help  achieve better pred ictions.  It may
also  be useful  to  create an automated  process to  experiment more comprehensively with our DBNs using
d ifferent hyper parameters, such as the number of hidden layers, the d imension of d ifferent hidden layers,
number of training  iterations, learning  rate and  etc. A lthough we have done various tests and  decide to  settle on
our current configurations, it would  be beneficial  if we could  further fine tune our hyper parameters.

[1]Joel  Hasbrouck.  Emp irical  Market Microstructure: The Institutions, Economics, and  Econometrics of Securities
Trad ing .
[ 2]  G. E. Hinton, S. Osindero , and  Y. W. Teh, “A  fast learning  algorithm for deep  bel ief nets,” Neural  Computation,
vol . 18, no . 7, pp . 1527–1554, 2006
[3] Geoffrey Hinton (2002). Training  products of experts by minimizing  contrastive d ivergence. Neural
Computation 14:1771–1800.
[4] http ://ufldl .stanford .edu/wiki/index .php/Deep_Networks:_Overv iew
[5] http ://en.wikiped ia.org/wiki/A lgorithmic_trad ing


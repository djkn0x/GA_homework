Learning Mixtures of Tree Graphical Models

Animashree Anandkumar
UC Irvine
a.anandkumar@uci.edu

Daniel Hsu
Microsoft Research New England
dahsu@microsoft.com

Furong Huang
UC Irvine
furongh@uci.edu

Sham M. Kakade
Microsoft Research New England
skakade@microsoft.com

Abstract

We consider unsupervised estimation of mixtures of discrete graphical models,
where the class variable is hidden and each mixture component can have a poten-
tially different Markov graph structure and parameters over the observed variables.
We propose a novel method for estimating the mixture components with provable
guarantees. Our output is a tree-mixture model which serves as a good approxi-
mation to the underlying graphical model mixture. The sample and computational
requirements for our method scale as poly(p, r), for an r-component mixture of p-
variate graphical models, for a wide class of models which includes tree mixtures
and mixtures over bounded degree graphs.

Keywords: Graphical models, mixture models, spectral methods, tree approximation.

1 Introduction

The framework of graphical models allows for parsimonious representation of high-dimensional
data by encoding statistical relationships among the given set of variables through a graph, known
as the Markov graph. Recent works have shown that a wide class of graphical models can be
estimated efﬁciently in high dimensions [1 –3]. However, fr
equently, graphical models may not
sufﬁce to explain all the characteristics of the observed da ta. For instance, there may be latent or
hidden variables, which can in ﬂuence the observed data in my riad ways.

In this paper, we consider latent variable models, where a latent variable can alter the relationships
(both structural and parametric) among the observed variables. In other words, we posit the observed
data as being generated from a mixture of graphical models, where each mixture component has a
potentially different Markov graph structure and parameters. The choice variable corresponding
to the selection of the mixture component is hidden. Such a class of graphical model mixtures
can incorporate context-speciﬁc dependencies , and employs multiple graph structures to model the
observed data. This leads to a signiﬁcantly richer class of m odels, compared to graphical models.

Learning graphical model mixtures is however far more challenging than learning graphical mod-
els. State-of-art theoretical guarantees are mostly limited to mixtures of product distributions, also
known as latent class models or na¨ıve Bayes models . These models are restrictive since they do not
allow for dependencies to exist among the observed variables in each mixture component. Our work
signiﬁcantly generalizes this class and allows for general Markov dependencies among the observed
variables in each mixture component.

The output of our method is a tree mixture model, which is a good approximation for the underlying
graphical model mixture. The motivation behind ﬁtting the o bserved data to a tree mixture is clear:
inference can be performed efﬁciently via belief propagati on in each of the mixture components.

1

See [4] for a detailed discussion. Thus, a tree mixture model offers a good tradeoff between using
single-tree models, which are too simplistic, and general graphical model mixtures, where inference
is not tractable.

1.1 Summary of Results

We propose a novel method with provable guarantees for unsupervised estimation of discrete graph-
ical model mixtures. Our method has mainly three stages: graph structure estimation, parameter
estimation, and tree approximation. The ﬁrst stage involve s estimation of the union graph structure
G∪ := ∪hGh , which is the union of the Markov graphs {Gh} of the respective mixture components.
Our method is based on a series of rank tests, and can be viewed as a generalization of conditional-
independence tests for graphical model selection (e.g. [1, 5, 6]). We establish that our method is
efﬁcient (in terms of computational and sample complexitie s), when the underlying union graph has
sparse vertex separators. This includes tree mixtures and mixtures with bounded degree graphs. The
second stage of our algorithm involves parameter estimation of the mixture components. In general,
this problem is NP-hard. We provide conditions for tractable estimation of pairwise marginals of the
mixture components. Roughly, we exploit the conditional-independence relationships to convert the
given model to a series of mixtures of product distributions. Parameter estimation for product dis-
tribution mixture has been well studied (e.g. [7 –9]), and is based on spectral decompositions of the
observed moments. We leverage on these techniques to obtain estimates of the pairwise marginals
for each mixture component. The ﬁnal stage for obtaining tre e approximations involves running the
standard Chow-Liu algorithm [10] on each component using the estimated pairwise marginals of the
mixture components.

We prove that our method correctly recovers the union graph structure and the tree structures cor-
responding to maximum-likelihood tree approximations of the mixture components. Note that if
the underlying model is a tree mixture, we correctly recover the tree structures of the mixture com-
ponents. The sample and computational complexities of our method scale as poly(p, r), for an
r-component mixture of p-variate graphical models, when the union graph has sparse vertex separa-
tors between any node pair. This includes tree mixtures and mixtures with bounded degree graphs.
To the best of our knowledge, this is the ﬁrst work to provide p rovable learning guarantees for
graphical model mixtures. Our algorithm is also efﬁcient fo r practical implementation and some
preliminary experiments suggest an advantage over EM with respect to running times and accuracy
of structure estimation of the mixture components. Thus, our approach for learning graphical model
mixtures has both theoretical and practical implications.

1.2 Related Work

Graphical Model Selection: Graphical model selection is a well studied problem starting from
the seminal work of Chow and Liu [10] for ﬁnding the maximum-l ikelihood tree approximation of a
graphical model. Works on high-dimensional loopy graphical model selection are more recent. They
can be classiﬁed into mainly two groups: non-convex local ap proaches [1, 2, 6] and those based on
convex optimization [3, 11]. However, these works are not directly applicable for learning mixtures
of graphical models. Moreover, our proposed method also provides a new approach for graphical
model selection, in the special case when there is only one mixture component.

Learning Mixture Models: Mixture models have been extensively studied, and there are a num-
ber of recent works on learning high-dimensional mixtures, e.g. [12, 13]. These works provide guar-
antees on recovery under various separation constraints between the mixture components and/or
have computational and sample complexities growing exponentially in the number of mixture com-
ponents r. In contrast, the so-called spectral methods have both computational and sample complex-
ities scaling only polynomially in the number of components, and do not impose stringent separation
constraints. Spectral methods are applicable for parameter estimation in mixtures of discrete product
distributions [7] and more generally for latent trees [8] and general linear multiview mixtures [9].
We leverage on these techniques for parameter estimation in models beyond product distribution
mixtures.

2

2 Graphical Models and their Mixtures

A graphical model is a family of multivariate distributions Markov on a given undirected graph [14].
In a discrete graphical model, each node in the graph v ∈ V is associated with a random variable Yv
taking value in a ﬁnite set Y . Let d := |Y | denote the cardinality of the set and p := |V | denote the
number of variables. A vector of random variables Y := (Y1 , . . . , Yp ) with a joint probability mass
function (pmf) P is Markov on the graph G if P satisﬁes the global Markov property for all disjoint
sets A, B ⊂ V
∀A, B ⊂ V : N [A] ∩ N [B ] = ∅.
P (yA , yB |yS (A,B ;G) ) = P (yA |yS (A,B ;G) )P (yB |yS (A,B ;G) ),
where the set S (A, B ; G) is a node separator1between A and B , and N [A] denotes the closed
neighborhood of A (i.e., including A).
Mixtures of discrete graphical models is considered. Let H denote the discrete hidden choice vari-
able corresponding to selection of a different mixture components, taking values in [r] := {1, . . . , r}
h as the probability vec-
and let Y denote the observed random vector. Denote πH := [P (H = h)]>
tor of the mixing weights and Gh as the Markov graph of the distribution P (y|H = h) of each
mixture component. Given n i.i.d. samples yn = [y1 , . . . , yn ]> from P (y), our goal is to ﬁnd a
tree approximation for each mixture component {P (y|H = h)}h . We do not assume any knowl-
edge of the mixing weights πH or Markov graphs {Gh}h or parameters of the mixture components
{P (y|H = h)}h . Moreover, since the variable H is latent, we do not a priori know the mixture com-
ponent from which a sample is drawn. Thus, a major challenge is in decomposition of the observed
statistics into the component models, and we tackle this in three main stages. First, we estimate the
union graph G∪ := ∪r
h=1Gh , which is the union of the Markov graphs of the components. We then
use this graph estimate bG∪ to obtain the pairwise marginals of the respective mixture components
{P (y|H = h)}h . Finally, Chow-Liu algorithm provides tree approximations {Th}h of each mixture
components.
3 Estimation of the Union of Component Graphs

Fact 1 (Markov Property of G∪ ) For any two nodes u, v ∈ V such that (u, v) /∈ G∪ ,
Yu ⊥⊥ Yv |YS , H, S := S (u, v ; G∪ ).

We propose a novel method for learning graphical model mixtures by ﬁrst estimating the union
graph G∪ = ∪r
h=1Gh , which is the union of the graphs of the components. In the special case when
Gh ≡ G∪ , this gives the graph estimate of the components. However, the union graph G∪ appears
to have no direct relationship with the marginalized model P (y). We ﬁrst provide intuitions on how
G∪ relates to the observed statistics.
2
Intuitions: We ﬁrst establish the simple result that the union graph G∪ satisﬁes Markov property
in each mixture component. Recall that S (u, v ; G∪ ) denotes a vertex separator between nodes u and
v in G∪ .
(1)
The separator set in G∪ , denoted by S := S (u, v ; G∪ ), is also a vertex separator for u and
Proof:
v in each of the component graphs Gh . This is because removal of S disconnects u and v in each
Gh . Thus, we have Markov property in each component: Yu ⊥⊥ Yv |YS , {H = h}, for each h ∈ [r],
and the above result follows.
The above result can be exploited to obtain union graph estimate as follows: two nodes u, v are
not neighbors in G∪ if a separator set S can be found which results in conditional independence,
as in (1). The main challenge is indeed that the variable H is not observed and thus, conditional
independence cannot be directly inferred via observed statistics. However, the effect of H on the
observed statistics can be quantiﬁed as follows:

Lemma 1 (Rank Property) Given an r-component mixture of graphical models with G∪ =
∪r
h=1Gh , for any u, v ∈ V such that (u, v) /∈ G∪ and S := S (u, v ; G∪ ), the probability matrix
Mu,v,{S ;k} := [P [Yu = i, Yv = j, YS = k ]]i,j has rank at most r for any k ∈ Y |S | .
1A set S (A, B ; G) ⊂ V is a separator of sets A and B if the removal of nodes in S (A, B ; G) separates A
and B into distinct components.

3

The proof is given in [15]. Thus, the effect of marginalizing the choice variable H is seen in the
rank of the observed probability matrices Mu,v,{S ;k} . When u and v are non-neighbors in G∪ , a
separator set S can be found such that the rank of Mu,v,{S ;k} is at most r. In order to use this result
as a criterion for inferring neighbors in G∪ , we require that the rank of Mu,v,{S ;k} for any neighbors
(u, v) ∈ G∪ be strictly larger than r. This requires the dimension of each node variable d > r. We
discuss in detail the set of sufﬁcient conditions for correc tly recovering G∪ in Section 3.1.

Tractable Graph Families: Another obstacle in using Lemma 1 to estimate graph G∪ is computa-
tional: the search for separators S for any node pair u, v ∈ V is exponential in |V | := p if no further
constraints are imposed. We consider graph families where a vertex separator can be found for any
(u, v) /∈ G∪ with size at most η . Under our framework, the hardness of learning a union graph is
parameterized by η . Similar observations have been made before for graphical model selection [1].
There are many natural families where η is small:

1. If G∪ is trivial (i.e., no edges) then η = 0, we have a mixture of product distributions.

2. When G∪ is a tree, i.e., we have a mixture model Markov on the same tree, then η = 1,
since there is a unique path between any two nodes on a tree.

3. For an arbitrary r-component tree mixture, G∪ = ∪h Th where each component is a tree
distribution, we have that η ≤ r (since for any node pair, there is a unique path in each of
the r trees {Th}, and separating the node pair in each Th also separates them on G∪ ).
4. For an arbitrary mixture of bounded degree graphs, we have η ≤ Ph∈[r ] ∆h , where ∆h is
the maximum degree in Gh , i.e., the Markov graph corresponding to component {H = h}.
In general, η depends on the respective bounds ηh for the component graphs Gh , as well as the extent
of their overlap. In the worst case, η can be as high as Ph∈[r ] ηh , while in the special case when
Gh ≡ G∪ , the bound remains the same ηh ≡ η . Note that for a general graph G∪ with treewidth
tw(G∪ ) and maximum degree ∆(G∪ ), we have that η ≤ min(∆(G∪ ), tw(G∪ )).
Algorithm 1 bGn
h=1Gh of an r-component
∪ = RankTest(yn ; ξn,p , η , r) for estimating G∪ := ∪r
mixture using yn samples, where η is the bound on size of vertex separators between any node pair
in G∪ and ξn,p is a threshold on the singular values.
Rank(A; ξ ) denotes the effective rank of matrix A, i.e., number of singular values more than ξ .
cM n
u,v,{S ;k} := [ bP n (Yu = i, Yv = j, YS = k)]i,j is the empirical estimate computed using n i.i.d.
samples yn . Initialize bGn
∪ = (V , ∅). For each u, v ∈ V , estimate cM n
u,v,{S ;k} from yn for some
con ﬁguration k ∈ Y |S | , if
Rank( cM n
(2)
min
u,v,{S ;k} ; ξn,p ) > r,
S⊂V \{u,v}
|S |≤η

then add (u, v) to bGn
∪ .
Rank Test: Based on the above observations, we propose a rank test to estimate G∪ := ∪h∈[r ]Gh ,
the union graph in Algorithm 1. The method is based on a search for potential separators S between
any two given nodes u, v ∈ V , based on the effective rank of cM n
u,v,{S ;k} : if the effective rank is r
or less, then u and v are declared as non-neighbors (and set S as their separator). If no such sets are
found, they are declared as neighbors. Thus, the method involves searching for separators for each
node pair u, v ∈ V , by considering all sets S ⊂ V \ {u, v} satisfying |S | ≤ η . The computational
complexity of this procedure is O(pη+2 d3 ), where d is the dimension of each node variable Yi , for
i ∈ V and p is the number of nodes. This is because the number of rank tests performed is O(pη+2 )
over all node pairs and conditioning sets; each rank tests has O(d3 ) complexity since it involves
performing singular value decomposition (SVD) of a d × d matrix.

4

3.1 Analysis of the Rank Test

We now provide guarantees for the success of rank tests in estimating G∪ . As noted before, we
require that the number of components r and the dimension d of each node variable satisfy d > r.
Moreover, we assume bounds on the size of separator sets, η = O(1). This includes tree mixtures
and mixtures over bounded degree graphs.
In addition, the following parameters determine the
success of the rank tests.

ρmin :=

max
k∈Y |S |

(3)

min
(u,v)∈G∪ ,|S |≤η
S⊂V \{u,v}

(A1) Rank condition for neighbors: Let Mu,v,{S ;k} := [P (Yu = i, Yv = j, YS = k)]i,j and
σr+1 (cid:0)Mu,v,{S ;k} (cid:1) > 0,
where σr+1 (·) denotes the (r + 1)th singular value, when the singular values are arranged in
the descending order σ1 (·) ≥ σ2 (·) ≥ . . . σd (·). This ensures that the probability matrices
for neighbors (u, v) ∈ G∪ have (effective) rank of at least r + 1, and thus, the rank test can
correctly distinguish neighbors from non-neighbors. It rules out the presence of spurious
low rank matrices between neighboring nodes in G∪ (for instance, when the nodes are
marginally independent or when the distribution is degenerate).
(A2) Choice of threshold ξ : The threshold ξ on singular values is chosen as ξ := ρmin
2 .
(A3) Number of Samples: Given δ ∈ (0, 1), the number of samples n satisﬁes
ρmin − t (cid:19)2! ,
n > nRank(δ ; p) := max   1
t2 (cid:0)2 log p + log δ−1 + log 2(cid:1) , (cid:18)
2
for some t ∈ (0, ρmin ) (e.g. t = ρmin/2,) where p is the number of nodes.
We now provide the result on the success of recovering the union graph G∪ := ∪r
h=1Gh .

(4)

Theorem 1 (Success of Rank Tests) The RankTest(yn ; ξ , η , r) recovers the correct graph G∪ ,
which is the union of the component Markov graphs, under (A1) –(A3) with probability at least
1 − δ .

A special case of the above result is graphical model selection, where there is a single graphical
model (r = 1) and we are interested in estimating its graph structure.

Corollary 1 (Application to Graphical Model Selection) Given n i.i.d.
samples yn ,
RankTest(yn ; ξ , η , 1) is structurally consistent under (A1) –(A3) with probabili ty at least 1 − δ .

the

Remarks: Thus, the rank test is also applicable for graphical model selection. Previous works (see
Section 1.2) have proposed tests based on conditional independence, using either conditional mutual
information or conditional variation distances, see [1, 6]. The rank test above is thus an alternative
test for conditional independence in graphical models, resulting in graph structure estimation. In
addition, it extends naturally to estimation of union graph structure of mixture components. Our
above result establishes that our method is also efﬁcient in high dimensions, since it only requires
logarithmic samples for structural consistency (n = Ω(log p)).

4 Parameter Estimation of Mixture Components

Having obtained an estimate of the union graph G∪ , we now describe a procedure for estimating
parameters of the mixture components {P (y|H = h)}. Our method is based on spectral decom-
position, proposed previously for mixtures of product distributions [7 –9]. We recap it brie ﬂy below
and then describe how it can be adapted to the more general setting of graphical model mixtures.

Recap of Spectral Decomposition in Mixtures of Product Distributions: Consider the case
where V = {u, v , w}, and Yu ⊥⊥ Yv ⊥⊥ Yw |H . For simplicity assume that d = r, i.e., the hidden
and observed variables have the same dimension. This assumption will be removed subsequently.
Denote Mu|H := [P (Yu = i|H = j )]i,j , and similarly for Mv|H , Mw |H and assume that they are

5

full rank. Denote the probability matrices Mu,v := [P (Yu = i, Yv = j )]i,j and Mu,v,{w ;k} :=
[P (Yu = i, Yv = j, Yw = k)]i,j . The parameters (i.e., matrices Mu|H , Mv|H , Mw |H ) can be
estimated as:

Lemma 2 (Mixture of Product Distributions) Given
the
above model,
1 , . . . , λ(k)
[λ(k)
d ]> be the column vector with the d eigenvalues given by
λ(k) := Eigenvalues (cid:0)Mu,v,{w ;k}M −1
u,v (cid:1) ,
k ∈ Y .
Let Λ := [λ(1) |λ(2) | . . . |λ(d) ] be a matrix where the k th column corresponds to λ(k) . We have
Mw |H := [P (Yw = i|H = j )]i,j = Λ> .

let λ(k)

=

(5)

(6)

For the proof of the above result and for the general algorithm (when d ≥ r), see [9]. Thus, if
we have a general product distribution mixture over nodes in V , we can learn the parameters by
performing the above spectral decomposition over different triplets {u, v , w}. However, an obstacle
remains: spectral decomposition over different triplets {u, v , w} results in different permutations
of the labels of the hidden variable H . To overcome this, note that any two triplets (u, v , w) and
(u, v 0 , w0 ) share the same set of eigenvectors in (5) when the “left” node u is the same. Thus, if we
consider a ﬁxed node u∗ ∈ V as the “left” node and use a ﬁxed matrix to diagonalize (5) for
all
triplets, we obtain a consistent ordering of the hidden labels over all triplet decompositions.

Parameter Estimation in Graphical Model Mixtures: We now adapt the above procedure for
estimating components of a general graphical model mixture. We ﬁrst make a simple observation on
how to obtain mixtures of product distributions by considering separators on the union graph G∪ .
For any three nodes u, v , w ∈ V , which are not neighbors on G∪ , let Suvw denote a multiway vertex
separator, i.e., the removal of nodes in Suvw disconnects u, v and w in G∪ . On lines of Fact 1,
(7)
∀u, v , w : (u, v), (v , w), (w, u) /∈ G∪ .
Yu ⊥⊥ Yv ⊥⊥ Yw |YSuvw , H,
Thus, by ﬁxing the con ﬁguration of nodes in
Suvw , we obtain a product distribution mixture over
{u, v , w}. If the previously proposed rank test is successful in estimating G∪ , then we possess cor-
rect knowledge of the separators Suvw . In this case, we can obtain estimates {P (Yw |YSuvw =
k , H = h)}h by ﬁxing the nodes in Suvw and using the spectral decomposition described in
Lemma 2, and the procedure can be repeated over different triplets {u, v , w}.
An obstacle remains, viz., the permutation of hidden labels over different triplet decompositions
{u, v , w}. In case of product distribution mixture, as discussed previously, this is resolved by ﬁxing
the “left” node in the triplet to some
u∗ ∈ V and using the same matrix for diagonalization over
different triplets. However, an additional complication arises when we consider graphical model
mixtures, where conditioning over separators is required. We require that the permutation of the
hidden labels be unchanged upon conditioning over different values of variables in the separator set
Su∗ vw . This holds when the separator set Su∗ vw has no effect on node u∗ , i.e., we require that
∃u∗ ∈ V , s.t. Yu∗ ⊥⊥ YV \u∗ |H,
which implies that u∗ is isolated from all other nodes in graph G∪ .
Condition (8) is required for identiﬁability if we only oper ate on statistics over different triplets
(along with their separator sets). In other words, if we resort to operations over only low order
statistics, we require additional conditions such as (8) for identiﬁability. However, our setting is a
signiﬁcant generalization over the mixtures of product dis tributions, where (8) is required to hold
for all nodes.

(8)

Finally, since our goal is to estimate pairwise marginals of the mixture components, in place of node
w in the triplet {u, v , w} in Lemma 2, we need to consider a node pair a, b ∈ V . The general algo-
rithm allows the variables in the triplet to have different dimensions, see [9] for details. Thus, we
obtain estimates of the pairwise marginals of the mixture components. For details on implementa-
tion, refer to [15].

4.1 Analysis and Guarantees

In addition to (A1) –(A3) in Section 3.1 to guarantee correct
recovery of G∪ and the conditions
discussed above, the success of parameter estimation depends on the following quantities:

6

(A4) Non-degeneracy: For each node pair a, b ∈ V , and any subset S ⊂ V \ {a, b} with
|S | ≤ 2η and k ∈ Y |S | , the probability matrix M(a,b)|H,{S ;k} := [P (Ya,b = i|H =
j, YS = k)]i,j ∈ Rd2×r has rank r.
(A5) Spectral Bounds and Number of Samples: Refer to various spectral bounds used to
obtain K (δ ; p, d, r) in (??) in [15], where δ ∈ (0, 1) is ﬁxed. Given any ﬁxed
 ∈ (0, 1),
assume that the number of samples satisﬁes

n > nspect(δ, ; p, d, r) :=

4K 2 (δ ; p, d, r)
2

.

(9)

Note that (A4) is a natural condition required for success of spectral decomposition and has been
previously imposed for learning product distribution mixtures [7 –9]. Moreover, when (A4) does not
hold, i.e., when the matrices are not full rank, parameter estimation is computationally at least as
hard as learning parity with noise, which is conjectured to be computationally hard [8]. Condition
(A5) is required for learning product distribution mixtures [9], and we inherit it here.

We now provide guarantees for estimation of pairwise marginals of the mixture components. Let
k · k2 on a vector denote the `2 norm.

Theorem 2 (Parameter Estimation of Mixture Components) Under the assumptions (A1) –(A5),
the spectral decomposition method outputs bP spect(Ya , Yb |H = h), for each a, b ∈ V , such that for
all h ∈ [r], there exists a permutation τ (h) ∈ [r] with
k bP spect(Ya , Yb |H = h) − P (Ya , Yb |H = τ (h))k2 ≤ ,
(10)
with probability at least 1 − 4δ .
Remark: Recall
that p denotes the number of variables, r is the number of mixture
components, d is the dimension of each node variable and η is the bound on separa-
tor sets between any node pair in the union graph. We establish that K (δ ; p, d, r) is
O (cid:0)p2η+2d2η r5 δ−1 poly log(p, d, r, δ−1 )(cid:1) in [15]. Thus, we require the number of samples in (9)
scaling as n = Ω (cid:0)p4η+4d4η r10 δ−2 −2 poly log(p, d, r, δ−1 )(cid:1). Since we consider models where
η = O(1) is a small constant, this implies that we have a polynomial sample complexity in p, d, r.
Tree Approximation of Mixture Components: The ﬁnal step involves using the estimated pair-
wise marginals of each component { bP spect(Ya , Yb |H = h)} to obtain tree approximation of the
component via Chow-Liu algorithm [10]. We now impose a standard condition of non-degeneracy
on each mixture component to guarantee the existence of a unique tree structure corresponding to
the maximum-likelihood tree approximation to the mixture component.
(A6) Separation of Mutual Information: Let Th denote the maximum-likelihood tree approx-
imation corresponding to the model P (y|H = h) when exact statistics are input and let
(11)

(I (Yu , Yv |H = h) − I (Ya , Yb |H = h)) ,

min
(u,v)∈Path(a,b;Th )

ϑ := min
h∈[r ]

min
(a,b) /∈Th

where Path(a, b; Th) denotes the edges along the path connecting a and b in Th . Intuitively
ϑ denotes the “bottleneck ” where errors are most likely to occ ur in tree structure estimation.
See [16] for a detailed discussion.
(A7) Number of Samples: Given tree de ﬁned in [15], we require
n > nspect(δ, tree ; p, d, r),
where nspect is given by (9).
Intuitively, tree provides the bound on distortion of the
estimated pairwise marginals of the mixture components, required for correct estimation of
tree approximations, and depends on ϑ in (11).

(12)

Theorem 3 (Tree Approximations of Mixture Components) Under (A1) –(A7), the Chow-Liu al-
gorithm outputs the correct tree structures corresponding to maximum-likelihood tree approxima-
tions of the mixture components {P (y|H = h)} with probability at least 1 − 4δ , when the estimates
of pairwise marginals { bP spect(Ya , Yb |H = h)} from spectral decomposition method are input.
7

EM
Proposed
Proposed+EM

 

−5.5

−6

−25

−26

d
−27
o
o
h
i
−28
l
e
k
i
−29
l
-
g
o
L
−30

 

3000

4000
5000
Sample Size n
(a) Overall likelihood of the mixture

7000

6000

8000

 

−18

−20

 

d
o
o
−6.5
h
i
l
e
k
i
l
-
g
o
−7.5
L

 

−7

3000

EM
Proposed
Proposed+EM
5000
4000
6000
Sample Size n
(b) Conditional likelihood of strong
component

8000

7000

d
−22
o
o
−24
h
i
l
e
−26
k
i
l
-
−28
g
o
L
−30

 

3000

EM
Proposed
Proposed+EM
5000
4000
6000
Sample Size n
(c) Conditional likelihood of weak
component

7000

8000

Figure 1: Performance of the proposed method, EM and EM initialized with the proposed method
output on a tree mixture with two components.

0.2

 

1.5

0.1

r
0.15
o
r
r
e
n
o
i
t
a
c
0.05
i
s
s
a
l
C

0

EM
Proposed
Proposed+EM

 

3000
5000
4000
Sample Size n
(a) Classi ﬁcation error

6000

7000

8000

1

s
e
c
n
a
t
0.5
s
i
d
t
i
d
E

0

 

EM
Proposed
Proposed+EM

 

2

1.5

 

EM
Proposed
Proposed+EM

1

s
e
c
n
a
t
s
i
0.5
d
t
i
d
E

0

 

3000

4000
5000
Sample Size n
(b) Strong component edit distance

7000

6000

8000

3000

4000
5000
Sample Size n
(c) Weak component edit distance

7000

6000

8000

Figure 2: Classiﬁcation error and normalized edit distance s of the proposed method, EM and EM
initialized with the proposed method output on the tree mixture.
5 Experiments
Experimental results are presented on synthetic data. We estimate the graph using proposed algo-
rithm and compare the performance of our method with EM [4]. Comprehensive results based on
the normalized edit distances and log-likelihood scores between the estimated and the true graphs
are presented. We generate samples from a mixture over two different trees (r = 2) with mixing
weights π = [0.7, 0.3] using Gibbs sampling. Each mixture component is generated from the stan-
dard Potts model on p = 60 nodes, where the node variables are ternary (d = 3), and the number of
samples n ∈ [2.5 × 103, 104 ]. The joint distribution of nodes in each mixture component is given by
P (X |H = h) ∝ exp  X(i,j)∈G
Ki;hYi , 
Ji,j ;h (I(Yi = Yj ) − 1) + Xi∈V
where I is the indicator function and Jh := {Ji,j ;h } are the edge potentials in the model. For the
ﬁrst component ( H = 1), the edge potentials J1 are chosen uniformly from [5, 5.05], while for the
second component (H = 2), J2 are chosen from [0.5, 0.55]. We refer to the ﬁrst component as
strong and the second as weak since the correlations vary widely between the two models due to the
choice of parameters. The node potentials are all set to zero (Ki;h = 0) except at the isolated node
u∗ in the union graph. The performance of the proposed method is compared with EM. We consider
10 random initializations of EM and run it to convergence. We also evaluated EM by utilizing
proposed result as the initial point (referred to as Proposed+EM in the ﬁgures). We observe in Fig 1a
that the overall likelihood under our method is comparable with EM. Intuitively this is because EM
attempts to maximize the overall likelihood. However, our algorithm has signiﬁcantly superior
performance with respect to the edit distance which is the error in estimating the tree structure in
the two components, as seen in Fig 2. In fact, EM never manages to recover the structure of the
weak components(i.e., the component with weak correlations). Intuitively, this is because EM uses
the overall likelihood as criterion for tree selection. Under the above choice of parameters, the weak
component has a much lower contribution to the overall likelihood, and thus, EM is unable to recover
it. We also observe in Fig 1b and Fig 1c, that our proposed method has superior performance in terms
of conditional likelihood for both the components. Classiﬁ cation error is evaluated in Fig 2a. We
could get smaller classiﬁcation errors than EM method.

The above experimental results con ﬁrm our theoretical anal ysis and suggest the advantages of our
basic technique over more common approaches. Our method provides a point of tractability in the
spectrum of probabilistic models, and extending beyond the class we consider here is a promising
direction of future research.
Acknowledgements: The ﬁrst author is supported in part by the NSF Award CCF-1219 234, AFOSR Award
FA9550-10-1-0310, ARO Award W911NF-12-1-0404, and setup funds at UCI. The third author is supported
by the NSF Award 1028394 and AFOSR Award FA9550-10-1-0310.

8

ﬁ
References

[1] A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-Dimensional Structure Learning of Ising
Models: Local Separation Criterion. Accepted to Annals of Statistics, Jan. 2012.
[2] A. Jalali, C. Johnson, and P. Ravikumar. On learning discrete graphical models using greedy methods. In
Proc. of NIPS, 2011.
[3] P. Ravikumar, M.J. Wainwright, and J. Lafferty. High-dimensional Ising Model Selection Using l1-
Regularized Logistic Regression. Annals of Statistics, 2008.
[4] M. Meila and M.I. Jordan. Learning with mixtures of trees. J. of Machine Learning Research, 1:1–48,
2001.
[5] P. Spirtes and C. Meek. Learning bayesian networks with discrete variables from data. In Proc. of Intl.
Conf. on Knowledge Discovery and Data Mining, pages 294–299, 1995.
[6] G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov Random Fields from Samples: Some Obser-
vations and Algorithms. In Intl. workshop APPROX Approximation, Randomization and Combinatorial
Optimization, pages 343–356. Springer, 2008.
[7] J.T. Chang. Full reconstruction of markov models on evolutionary trees: identi ﬁability and consistency.
Mathematical Biosciences, 137(1):51–73, 1996.
[8] E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. The Annals of
Applied Probability, 16(2):583–614, 2006.
[9] A. Anandkumar, D. Hsu, and S.M. Kakade. A Method of Moments for Mixture Models and Hidden
Markov Models. In Proc. of Conf. on Learning Theory, June 2012.
[10] C. Chow and C. Liu. Approximating Discrete Probability Distributions with Dependence Trees. IEEE
Tran. on Information Theory, 14(3):462–467, 1968.
[11] N. Meinshausen and P. B ¨uhlmann. High Dimensional Graphs and Variable Selection With the Lasso.
Annals of Statistics, 34(3):1436–1462, 2006.
[12] M. Belkin and K. Sinha. Polynomial learning of distribution families. In IEEE Annual Symposium on
Foundations of Computer Science, pages 103–112, 2010.
[13] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In IEEE Annual
Symposium on Foundations of Computer Science, 2010.
[14] S.L. Lauritzen. Graphical models: Clarendon Press. Clarendon Press, 1996.
[15] A. Anandkumar, D. Hsu, and S.M. Kakade. Learning High-Dimensional Mixtures of Graphical Models.
Preprint. Available on ArXiv:1203.0697, Feb. 2012.
[16] V.Y.F. Tan, A. Anandkumar, and A. Willsky. A Large-Deviation Analysis for the Maximum Likelihood
Learning of Tree Structures. IEEE Tran. on Information Theory, 57(3):1714–1735, March 2011.

9


Proximal Newton-type methods for convex
optimization

Jason D. Lee∗ and Yuekai Sun∗
Institute for Computational and Mathematical Engineering
Stanford University, Stanford, CA
{jdl17,yuekai}@stanford.edu

Michael A. Saunders
Department of Management Science and Engineering
Stanford University, Stanford, CA
saunders@stanford.edu

Abstract

We seek to solve convex optimization problems in composite form:

minimize
f (x) := g(x) + h(x),
x∈Rn
where g is convex and continuously differentiable and h : Rn → R is a convex
but not necessarily differentiable function whose proximal mapping can be eval-
uated efﬁciently. We derive a generalization of Newton-type methods to handle
such convex but nonsmooth objective functions. We prove such methods are glob-
ally convergent and achieve superlinear rates of convergence in the vicinity of an
optimal solution. We also demonstrate the performance of these methods using
problems of relevance in machine learning and statistics.

1

Introduction

Many problems of relevance in machine learning, signal processing, and high dimensional statistics
can be posed in composite form:

(1)
minimize
f (x) := g(x) + h(x),
x∈Rn
where g : Rn → R is a convex, continuously differentiable loss function, and h : Rn → R is a
convex, continuous, but not necessarily differentiable penalty function. Such problems include: (i)
the lasso [23] (ii) multitask learning [14] and (iii) trace-norm matrix completion [6].
We describe a family of Newton-type methods tailored to these problems that achieve superlinear
rates of convergence subject to standard assumptions. These methods can be interpreted as general-
izations of the classic proximal gradient method that use the curvature of the objective function to
select a search direction.

1.1 First-order methods

The most popular methods for solving convex optimization problems in composite form are ﬁrst-
order methods that use proximal mappings to handle the nonsmooth part. SpaRSA is a generalized
spectral projected gradient method that uses a spectral step length together with a nonmonotone line
∗Equal contributors

1

search to improve convergence [24]. TRIP by Kim et al. also uses a spectral step length but selects
search directions using a trust-region strategy [12]. TRIP performs comparably with SpaRSA and
the projected Newton-type methods we describe later.
√
A closely related family of methods is the set of optimal ﬁrst-order methods, also called acceler-
ated ﬁrst-order methods, which achieve -suboptimality within O(1/
) iterations [22]. The two
most popular methods in this family are Auslender and Teboulle’s method [1] and Fast Iterative
Shrinkage-Thresholding Algorithm (FISTA), by Beck and Teboulle [2]. These methods have been
implemented in the software package TFOCS and used to solve problems that commonly arise in
statistics, machine learning, and signal processing [3].

1.2 Newton-type methods

There are three classes of methods that generalize Newton-type methods to handle nonsmooth objec-
tive functions. The ﬁrst are projected Newton-type methods for constrained optimization [20]. Such
methods cannot handle nonsmooth objective functions; they tackle problems in composite form via
constraints of the form h(x) ≤ τ . PQN is an implementation that uses a limited-memory quasi-
Newton update and has both excellent empirical performance and theoretical properties [19, 18].
The second class of these methods by Yu et al. [25] uses a local quadratic approximation to the
smooth part of the form

Q(x) := f (x) + sup
g∈∂ f (x)

gT d +

1
2

dT H d,

where ∂ f (x) denotes the subdifferential of f at x. These methods achieve state-of-the-art perfor-
mance on many problems of relevance, such as (cid:96)1 -regularized logistic regression and (cid:96)2 -regularized
support vector machines.
This paper focuses on proximal Newton-type methods that were previously studied in [16, 18] and
are closely related to the methods of Fukushima and Mine [10] and Tseng and Yun [21]. Both use
search directions ∆x that are solutions to subproblems of the form
∇g(x)T d +
1
dT H d + h(x + d),
minimize
2
d
where H is a positive deﬁnite matrix that approximates the Hessian ∇2 g(x). Fukushima and Mine
choose H to be a multiple of the identity, while Tseng and Yun set some components of the search
direction ∆x to be zero to obtain a (block) coordinate descent direction. Proximal Newton-type
methods were ﬁrst studied empirically by Mark Schmidt in his Ph.D. thesis [18].
The methods GLMNET [9] ((cid:96)1 -regularized regression), LIBLINEAR [26] ((cid:96)1 -regularized classiﬁ-
cation), QUIC and recent work by Olsen et al. [11, 15] (sparse inverse covariance estimation) are
special cases of proximal Newton-type methods. These methods are considered state-of-the-art for
their speciﬁc applications, often outperforming generic methods by orders of magnitude. QUIC and
LIBLINEAR also achieve a quadratic rate of convergence, although these results rely crucially on
the structure of the (cid:96)1 norm and do not generalize to generic nonsmooth regularizers.
The quasi-Newton splitting method developed by Becker and Fadili is equivalent to a proximal
quasi-Newton method with rank-one Hessian approxiamtion [4]. In this case, they can solve the
subproblem via the solution of a single variable root ﬁnding problem, making their method signiﬁ-
cantly more efﬁcient than a generic proximal Newton-type method.
The methods described in this paper are a special case of cost approximation (CA), a class of meth-
ods developed by Patriksson [16]. CA requires a CA function ϕ and selects search directions via
subproblems of the form

minimize
d

g(x) + ϕ(x + d) − ϕ(x) + h(x + d) − ∇g(x)T d.

Cost approximation attains a linear convergence rate. Our methods are equivalent to using the CA
2 xT H x. We refer to [16] for details about cost approximation and its convergence
function ϕ(x) := 1
analysis.

2

2 Proximal Newton-type methods

We seek to solve convex optimization problems in composite form:
(2)
minimize
f (x) := g(x) + h(x).
x∈Rn
We assume g : Rn → R is a closed, proper convex, continuously differentiable function, and its
gradient ∇g is Lipschitz continuous with constant L1 ; i.e.
(cid:107)∇g(x) − ∇g(y)(cid:107) ≤ L1 (cid:107)x − y(cid:107)
for all x and y in Rn . h : Rn → R is a closed and proper convex but not necessarily everywhere
differentiable function whose proximal mapping can be evaluated efﬁciently. We also assume the
optimal value, f (cid:63) , is attained at some optimal solution x(cid:63) , not necessarily unique.

2.1 The proximal gradient method

The proximal mapping of a convex function h at x is

h(y) +

(cid:107)y − x(cid:107)2 .

1
proxh (x) = arg min
2
y
Proximal mappings can be interpreted as generalized projections because if h is the indicator func-
tion of a convex set, then proxh (x) is the projection of x onto the set.
The classic proximal gradient method for composite optimization uses proximal mappings to handle
the nonsmooth part of the objective function and can be interpreted as minimizing the nonsmooth
function h plus a simple quadratic approximation to the smooth function g during every iteration:
xk+1 = proxtk h (xk − tk∇g(xk ))
∇g(xk )T (y − xk ) +
(cid:107)y − xk (cid:107)2 + h(y),
1
= arg min
2tk
y
where tk denotes the k-th step length. We can also interpret the proximal gradient step as a general-
ized gradient step
Gf (x) = proxh (x − ∇g(x)) − x.
(3)
Gf (x) = 0 if and only if x minimizes f so (cid:107)Gf (x)(cid:107) generalizes the smooth ﬁrst-order measure of
optimality (cid:107)∇f (x)(cid:107).
Many state-of-the-art methods for problems in composite form, such as SpaRSA and the optimal
ﬁrst-order methods, are variants of this method. Our method uses a Newton-type approximation in
lieu of the simple quadratic to achieve faster convergence.

2.2 The proximal Newton iteration

Deﬁnition 2.1 (Scaled proximal mappings). Let h be a convex function and H , a positive deﬁnite
matrix. Then the scaled proximal mapping of h at x is deﬁned to be
(cid:107)y − x(cid:107)2
1
proxH
H .
h (x) := arg min
h(y) +
2
y
Proximal Newton-type methods use the iteration
(cid:0)xk − H −1
k ∇g(xk )(cid:1) − xk ,
(5)
xk+1 = xk + tk∆xk ,
∆xk := proxHk
(6)
h
where tk > 0 is the k-th step length, usually determined using a line search procedure and Hk is an
k ∇g(xk )(cid:1)
(cid:0)xk − H −1
approximation to the Hessian of g at xk . We can interpret the search direction ∆xk as a step to the
minimizer of the nonsmooth function h plus a local quadratic approximation to g because
proxHk
h
k ∇g(xk )(cid:107)2
(cid:107)(y − xk ) + H −1
1
h(y) +
= arg min
Hk
2
y
∇g(xk )T (y − xk ) +
(y − xk )T Hk (y − xk ) + h(y).

(4)

(7)

= arg min
y

1
2

3

Hence, the search direction solves the subproblem
∇g(xk )T d +

dT Hk d + h(xk + d)

∆xk = arg min
d
= arg min
d

1
2
Qk (d) + h(xk + d).

To simplify notation, we shall drop the subscripts and say x+ = x + t∆x in lieu of xk+1 =
xk + tk∆xk when discussing a single iteration.
f (x+ ) ≤ f (x) + t (cid:0)∇g(x)T ∆x + h(x + ∆x) − h(x)(cid:1) + O(t2 ),
Lemma 2.2 (Search direction properties). If H is a positive deﬁnite matrix, then the search direction
∆x = arg mind Q(d) + h(x + d) satisﬁes:
∇g(x)T ∆x + h(x + ∆x) − h(x) ≤ −∆xT H∆x.

(8)
(9)

Lemma 2.2 implies the search direction is a descent direction for f because we can substitute (9)
into (8) to obtain
f (x+ ) ≤ f (x) − t∆xT H∆x + O(t2 ).
We use a quasi-Newton approximation to the Hessian and a ﬁrst-order method to solve the subprob-
lem for a search direction, although the user is free to use a method of his or her choice. Empirically,
we ﬁnd that inexact solutions to the subproblem yield viable descent directions.
We use a backtracking line search to select a step length t that satisﬁes a sufﬁcient descent condition:
f (x+ ) ≤ f (x) + αt∆
(10)
∆ := ∇g(x)T ∆x + h(x + ∆x) − h(x),
(11)
where α ∈ (0, 0.5). This sufﬁcient descent condition is motivated by our convergence analysis but
it also seems to perform well in practice.
Lemma 2.3 (Step length conditions). Suppose H (cid:23) mI for some m > 0 and ∇g is the Lipschitz
(cid:26)
(cid:27)
continuous with constant L1 . Then the step lengths
t ≤ min
2m
L1

(1 − α)

(12)

1,

.

satisﬁes the sufﬁcient descent condition (10).

Algorithm 1 A generic proximal Newton-type method
Require: x0 in dom f
(cid:0)xk − H −1
k ∇g(xk )(cid:1)
1: repeat
Update Hk using a quasi-Newton update rule
2:
zk ← proxHk
3:
∆xk ← zk − xk
h
4:
Conduct backtracking line search to select tk
5:
xk+1 ← xk + tk∆xk
6:
7: until stopping conditions are satisﬁed

3 Convergence analysis

3.1 Global convergence
We assume our Hessian approximations are sufﬁciently positive deﬁnite; i.e. Hk (cid:23) mI , k =
1, 2, . . . for some m > 0. This assumption guarantees the existence of step lengths that satisfy the
sufﬁcient decrease condition.
Lemma 3.1 (First-order optimality conditions). Suppose H is a positive deﬁnite matrix. Then x is
a minimizer of f if and only if the search direction is zero at x; i.e.
Q(d) + h(x + d).
0 = arg min
d

4

The global convergence of proximal Newton-type methods results from the fact that the search
directions are descent directions and if our Hessian approximations are sufﬁciently positive deﬁnite,
then the step lengths are bounded away from zero.
Theorem 3.2 (Global convergence). Suppose Hk (cid:23) mI , k = 1, 2, . . . for some m > 0. Then the
sequence {xk } generated by a proximal Newton-type method converges to a minimizer of f .

3.2 Convergence rate

If g is twice-continuously differentiable and we use the second order Taylor approximation as our
local quadratic approximation to g , then we can prove {xk } converges Q-quadratically to the optimal
solution x(cid:63) . We assume in a neighborhood of x(cid:63) : (i) g is strongly convex with constant m; i.e.
∇2 g(x) (cid:23) mI , x ∈ N (x(cid:63) )
where N (x(cid:63) ) := {x | (cid:107)x − x(cid:63) (cid:107) ≤ }; and (ii) ∇2 g is Lipschitz continuous with constant L2 .
This convergence analysis is similar to that of Fukushima and Min ´e [10] and Patriksson [16]. First,
we state two lemmas: (i) that says step lengths of unity satisfy the sufﬁcient descent condition after
sufﬁciently many iterations and (ii) that the backward step is nonexpansive.
Lemma 3.3. Suppose (i) ∇2 g (cid:23) mI and (ii) ∇2 g is Lipschitz continuous with constant L2 . If we let
Hk = ∇2 g(xk ), k = 1, 2, . . . , then the step length tk = 1 satisﬁes the sufﬁcient decrease condition
(10) for k sufﬁciently large.
(cid:0)x − H −1∇g(x)(cid:1), then
We can characterize the solution of the subproblem using the ﬁrst-order optimality conditions for
(4). Let y denote proxH
h
H (x − H −1∇g(x) − y) ∈ ∂h(u).
Let R(x) and S (x) denote (cid:2) 1
m (H + ∂h)(cid:3)−1
(x) and (cid:2) 1
m (H − ∇g)(cid:3) (x) respectively, where m is
or equivalently
[H − ∇g ] (x) ∈ [H + ∂h] (y)
the smallest eigenvalue of H . Then
−1 [H − ∇g ] (x) = R ◦ S (x).
y = [H + ∂h]
Lemma 3.4. Suppose R(x) = (cid:2) 1
m H + ∂h(cid:3)−1
.
(x), where H is positive deﬁnite. Then R is ﬁrmly-
nonexpansive; i.e. for x and y in dom f , R satisﬁes
(R(x) − R(y))T (x − y) ≥ (cid:107)R(x) − R(y)(cid:107)2 .
We note that x(cid:63) is a ﬁxed point of R ◦ S ; i.e. R ◦ S (x(cid:63) ) = x(cid:63) , so we can express (cid:107)y − x(cid:63) (cid:107) as
(cid:107)y − x(cid:63) (cid:107) = (cid:107)R ◦ S (x) − R ◦ S (x(cid:63) )(cid:107) ≤ (cid:107)S (x) − S (x(cid:63) )(cid:107).
Theorem 3.5. Suppose (i) ∇2 g (cid:23) mI and (ii) ∇2 g is Lipschitz continuous with constant L2 . If we
let Hk = ∇2 g(xk ), k = 1, 2, . . . , then {xk } converges to x(cid:63) Q-quadratically; i.e.
(cid:107)xk+1 − x(cid:63) (cid:107)
(cid:107)xk − x(cid:63) (cid:107)2 → c.
We can also use the fact that the proximal Newton method converges quadratically to prove a prox-
(cid:13)(cid:13)(cid:0)Hk − ∇2 g(x(cid:63) )(cid:1) (xk+1 − xk )(cid:13)(cid:13)
imal quasi-Newton method converges superlinearly. We assume the quasi-Newton Hessian approx-
imations satisfy the Dennis-Mor ´e criterion [7]:
(cid:107)xk+1 − xk (cid:107)
We ﬁrst prove two lemmas: (i) step lengths of unity satisfy the sufﬁcient descent condition after
sufﬁciently many iterations and (ii) the proximal quasi-Newton step is close to the proximal Newton
step.

→ 0.

(13)

5

∆x = proxH
h
∆ ˆx = prox ˆH
h
(cid:115)
Then these two search directions satisfy

Lemma 3.6. Suppose g is twice-continuously differentiable and the eigenvalues of Hk , k = 1, 2, . . .
there exist M ≥ m > 0 such that mI (cid:22) H (cid:22) M I .
If {Hk } satisfy the
are bounded; i.e.
Dennis-Mor ´e criterion, then the unit step length satisﬁes the sufﬁcient descent condition (10) after
sufﬁciently many iterations.
Lemma 3.7. Suppose H and ˆH are positive deﬁnite matrices with bounded eigenvalues; i.e. mI (cid:22)
H (cid:22) M I and ˆmI (cid:22) ˆH (cid:22) ˆM I . Let ∆x and ∆ ˆx denote the search directions generated using H
(cid:0)x − H −1∇g(x)(cid:1) − x,
and ˆH respectively; i.e.
(cid:17) − x.
(cid:16)
x − ˆH −1∇g(x)
(cid:13)(cid:13)(cid:13)1/2 (cid:107)∆x(cid:107)1/2 ,
(cid:13)(cid:13)(cid:13)( ˆH − H )∆x
1 + c(H, ˆH )
m
where c is a constant that depends on H and ˆH .
Theorem 3.8. Suppose g is twice-continuously differentiable and the eigenvalues of Hk , k =
1, 2, . . . are bounded. If {Hk } satisfy the Dennis-Mor ´e criterion, then the sequence {xk } converges
to x(cid:63) Q-superlinearly; i.e.
(cid:107)xk+1 − x(cid:63) (cid:107)
(cid:107)xk − x(cid:63) (cid:107) → 0.

(cid:107)∆x − ∆ ˆx(cid:107) ≤

4 Computational experiments

4.1 PNO PT: Proximal Newton OPTimizer

PNO PT1 is a MAT LAB package that uses proximal Newton-type methods to minimize convex objec-
tive functions in composite form. PNO PT can build BFGS and L-BFGS approximation to the Hes-
sian (the user can also supply a Hessian approximation) and uses our implementation of SpaRSA or
an optimal ﬁrst order method to solve the subproblem for a search direction.
PNO PT uses an early stopping condition for the subproblem solver based on two ideas: (i) the
subproblem should be solved to a higher accuracy if Qk is a good approximation to g and (ii) near a
solution, the subproblem should be solved almost exactly to achieve fast convergence.
We thus require that the solution to the k-th subproblem (7) y(cid:63)
k satisfy
(cid:107)GQ+h (y(cid:63)
k )(cid:107) ≤ ηk (cid:107)Gf (y(cid:63)
k )(cid:107),
(14)
where Gf (x) denotes the generalized gradient step at x (3) and ηk is a forcing term. We choose
(cid:27)
(cid:26)
forcing terms based on the agreement between g and the previous quadratic approximation to g
Qk−1 . We set η1 := 0.5 and
(cid:107)∇g(xk ) − ∇Qk−1 (xk )(cid:107)
(15)
(cid:107)∇g(xk )(cid:107)
, k = 2, 3, . . .
ηk := min
0.5,
This choice measures the agreement between ∇g(xk ) and ∇Qk−1 (xk ) and is borrowed from a
choice of forcing terms for inexact Newton methods described by Eisenstat and Walker [8]. Empiri-
cally, we ﬁnd that this choice avoids “oversolving” the subproblem and yields desirable convergence
behavior.
We compare the performance of PNO PT, our implementation of SpaRSA, and the TFOCS imple-
mentations of Auslender and Teboulle’s method (AT) and FISTA on (cid:96)1 -regularized logistic regres-
sion and Markov random ﬁeld structure learning. We used the following settings:

1. PNO PT: We use an L-BFGS approximation to the Hessian with L = 50 and set the suf-
ﬁcient decrease parameter to α = 0.0001. To solve the subproblem, we use the TFOCS
implementation of FISTA.

1 PNO PT is available at www.stanford.edu/group/SOL/software/pnopt.html.

6

(a)

(b)

Figure 1: Figure 1a and 1b compare two variants of proximal Newton-type methods with SpaRSA
and TFOCS on on the MRF structure learning problem.

2. SpaRSA: We use a nonmonotone line search with a 10 iteration memory and also set the
sufﬁcient decrease parameter to α = 0.0001. Our implementation of SpaRSA is included
in PNO PT as the default solver for the subproblem.

3. AT/FISTA: We set tfocsOpts.restart = -inf to turn on adaptive restarting and
use default values for the rest of the settings.

These experiments were conducted on a machine running the 64-bit version of Ubuntu 12.04 with
an Intel Core i7 870 CPU and 8 GB RAM.

.

(16)

minimize
θ

θrj (xr , xj ) + log Z (θ) +

4.2 Markov random ﬁeld structure learning
We seek the maximum likelihood estimates of the parameters of a Markov random ﬁeld (MRF)
(cid:17)
(cid:16)
(cid:88)
− (cid:88)
subject to a group elastic-net penalty on the estimates. The objective function is given by
λ1 (cid:107)θrj (cid:107)2 + λ2 (cid:107)θrj (cid:107)2
F
(r,j )∈E
(r,j )∈E
xr is a k state variable; xj is a l state variable, and each parameter block θrj is a k × l matrix that
is associated with an edge in the MRF. We randomly generate a graphical model with |V | = 12 and
n = 300. The edges are sample uniformly with p = 0.3. The parameters of the non-zero edges are
sampled from a N (0, 1) distribution.
set to λ1 = (cid:112)n log |V | and λ2 = .1λ1 . These parameter settings are shown to be model selection
The group elastic-net penalty regularizes the solution and promotes solutions with a few non-zero
groups θrj corresponding to edges of the graphical model [27]. The regularization parameters were
consistent under certain irrepresentable conditions [17].
The algorithms for solving (16) require evaluating the value and gradient of the smooth part. For a
discrete graphical model without special structure, the smooth part requires O(k |V | ) operations to
evaluate, where k is the number of states per variable. Thus even for our small example, where k = 3
and |V | = 12, function and gradient evaluations dominate the computational expense required to
solve (16).
We see that for maximum likelihood learning in graphical models, it is important to minimize the
number of function evaluations. Proximal Newton-type methods are well-suited to solve such prob-
lems because the main computational expense is shifted to solving the subproblems that do not
require function evaluations.

7

010020030010−5100Iterationlog(f−f*)  FistaATPN100PN15SpaRSA02040608010−5100Time (sec)log(f−f*)  FistaATPN100PN15SpaRSA(a)

(b)

Figure 2: Figure 2 compares proximal Newton-type methods with SpaRSA and TFOCS on (cid:96)1 -
regularized logistic regression.

4.3

(cid:96)1 -regularized logistic regression

minimize
w∈Rp

(17)

log(1 + exp(−yiwT xi )) + λ(cid:107)w(cid:107)1 .

Given training data (xi , yi ), i = 1, 2, . . . , n, (cid:96)1 -regularized logistic regression trains a classiﬁer via
the solution of the convex optimization problem
n(cid:88)
1
n
i=1
for a set of parameters w in Rp . The regularization term (cid:107)w(cid:107)1 avoids overﬁtting the training data
and promotes sparse solutions. λ is trades-off between goodness-of-ﬁt and model complexity.
We use the dataset gisette, a handwritten digits dataset from the NIPS 2003 feature se-
lection challenge. The dataset is available at http://www.csie.ntu.edu.tw/∼cjlin/
libsvmtools/datasets. We train our classiﬁer using the original training set consisting of
6000 examples starting at w = 0. λ was chosen to match the value reported in [26], where it was
chosen by ﬁve-fold cross validation on the training set.
The gisette dataset is quite dense (3 million nonzeros in the 6000 × 5000 design matrix) and the
evaluation of the log-likelihood requires many expensive exp/log operations. We see in ﬁgure 2 that
PNO PT outperforms the other methods because the computational expense is shifted to solving the
subproblems, whose objective functions are cheap to evaluate.

5 Conclusion

Proximal Newton-type methods are natural generalizations of ﬁrst-order methods that account for
curvature of the objective function. They share many of the desirable characteristics of traditional
ﬁrst-order methods for convex optimization problems in composite form and achieve superlinear
rates of convergence subject to standard assumptions. These methods are especially suited to prob-
lems with expensive function evaluations because the main computational expense is shifted to solv-
ing subproblems that do not require function evaluations.

6 Acknowledgements

We wish to thank Trevor Hastie, Nick Henderson, Ernest Ryu, Ed Schmerling, Carlos Sing-Long,
and Walter Murray for their insightful comments.

8

01000200030004000500010−610−410−2100Function evaluationsRelative suboptimality  ATFISTASpaRSAPN010020030040050010−610−410−2100Time (sec)Relative suboptimality  ATFISTASpaRSAPNReferences
[1] A. Auslender and M. Teboulle, Interior gradient and proximal methods for convex and conic optimization,
SIAM J. Optim., 16 (2006), pp. 697–725.
[2] A. Beck and M. Teboulle , A fast iterative shrinkage-thresholding algorithm for linear inverse problems,
SIAM J. Imaging Sci., 2 (2009), pp. 183–202.
[3] S. R. Becker, M. J. Cand `es, and M. C. Grant, Templates for convex cone problems with applications to
sparse signal recovery, Math. Program. Comput., 3 (2011), pp. 1–54.
[4] S. Becker and J. Fadili, A quasi-Newton proximal splitting method, NIPS, Lake Tahoe, California, 2012.
[5] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cambridge, 2004.
[6] E. J. Cand `es and B. Recht, Exact matrix completion via convex optimization, Found. Comput. Math, 9
(2009), pp. 717–772.
[7] J. E. Dennis, Jr. and J. J. Mor ´e, A characterization of superlinear convergence and its application to
quasi-Newton methods, Math. Comp., 28, (1974), pp. 549–560.
[8] S. C. Eisenstat and H. F. Walker, Choosing the forcing terms in an inexact Newton method, SIAM J. Sci.
Comput., 17 (1996), pp. 16–32.
[9] J. Friedman, T. Hastie, H. Holﬁng, and R. Tibshirani, Pathwise coordinate optimization, Ann. Appl. Stat.
(2007), pp. 302–332
[10] M. Fukushima and H. Mine, A generalized proximal point algorithm for certain non-convex minimization
problems, Internat. J. Systems Sci., 12 (1981), pp. 989–1000.
[11] C. J. Hsieh, M. A. Sustik, P. Ravikumar, and I. S. Dhillon, Sparse inverse covariance matrix estimation
using quadratic approximation, NIPS, Granada, Spain, 2011.
[12] D. Kim, S. Sra, and I. S. Dhillon, A scalable trust-region algorithm with applications to mixed-norm
regression, ICML, Haifa, Israel, 2010.
[13] Y. Nesterov, Gradient methods for minimizing composite objective function, CORE discussion paper,
2007.
[14] G. Obozinski, B. Taskar, and M. I. Jordan, Joint covariate selection and joint subspace selection for
multiple classiﬁcation problems, Stat. Comput. (2010), pp. 231–252
[15] P. Olsen, F. Oztoprak, J. Nocedal, S. Rennie, Newton-like methods for sparse inverse covariance estima-
tion, NIPS, Lake Tahoe, California, 2012.
[16] M. Patriksson, Nonlinear Programming and Variational Inequality Problems, Kluwer Academic Publish-
ers, The Netherlands, 1999.
[17] P. Ravikumar, M. J. Wainwright and J. D. Lafferty, High-dimensional Ising model selection using (cid:96)1-
regularized logistic regression, Ann. Statist. (2010), pp. 1287-1319.
[18] M. Schmidt, Graphical Model Structure Learning with l1-Regularization, Ph.D. Thesis (2010), Univer-
sity of British Columbia
[19] M. Schmidt, E. van den Berg, M. P. Friedlander, and K. Murphy, Optimizing costly functions with simple
constraints: a limited-memory projected quasi-Newton algorithm, AISTATS, Clearwater Beach, Florida,
2009.
[20] M. Schmidt, D. Kim, and S. Sra, Projected Newton-type methods in machine learning, in S. Sra, S.
Nowozin, and S. Wright, editors, Optimization for Machine Learning, MIT Press (2011).
[21] P. Tseng and S. Yun, A coordinate gradient descent method for nonsmooth separable minimization, Math.
Prog. Ser. B, 117 (2009), pp. 387–423.
[22] P. Tseng, On accelerated proximal gradient methods for convex-concave optimization, submitted to
SIAM J. Optim. (2008).
[23] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R. Stat. Soc. Ser. B Stat. Methodol., 58
(1996), pp. 267–288.
[24] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, Sparse reconstruction by separable approximation,
IEEE Trans. Signal Process., 57 (2009), pp. 2479–2493.
[25] J. Yu, S. V. N. Vishwanathan, S. G ¨unter, and N. N. Schraudolph, A Quasi-Newton Approach to Nonsmooth
Convex Optimization, ICML, Helsinki, Finland, 2008.
[26] G. X. Yuan, C. H. Ho and C. J. Lin, An improved GLMNET for (cid:96)1-regularized logistic regression and
support vector machines, National Taiwan University, Tech. Report 2011.
[27] R. H. Zou and T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B
Stat. Methodol., 67 (2005), pp. 301–320.

9


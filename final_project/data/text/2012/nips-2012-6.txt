Coupling Nonparametric Mixtures via
Latent Dirichlet Processes

Dahua Lin
MIT CSAIL
dhlin@mit.edu

John Fisher
MIT CSAIL
fisher@csail.mit.edu

Abstract

Mixture distributions are often used to model complex data. In this paper, we de-
velop a new method that jointly estimates mixture models over multiple data sets
by exploiting the statistical dependencies between them. Speciﬁcally, we intro-
duce a set of latent Dirichlet processes as sources of component models (atoms),
and for each data set, we construct a nonparametric mixture model by combining
sub-sampled versions of the latent DPs. Each mixture model may acquire atoms
from different latent DPs, while each atom may be shared by multiple mixtures.
This multi-to-multi association distinguishes the proposed method from previous
ones that require the model structure to be a tree or a chain, allowing more ﬂexible
designs. We also derive a sampling algorithm that jointly infers the model param-
eters and present experiments on both document analysis and image modeling.

1

Introduction

Mixture distributions have been widely used for statistical modeling of complex data. Classical
formulations specify the number of components a priori, leading to difﬁculties in situations where
the number is either unknown or hard to estimate in advance. Bayesian nonparametric models,
notably those based on Dirichlet processes (DPs) [14, 16], have emerged as an important method to
address this issue. The basic idea of DP mixture models is to use a sample of a DP, which is itself a
distribution over a countably inﬁnite set, as the prior for component parameters.
One signiﬁcant assumption underlying a DP mixture model is that observations are inﬁnitely ex-
changeable. This assumption does not hold in the cases with multiple groups of data, where sam-
ples in different groups are generally not exchangeable. Among various approaches to this issue,
hierarchical Dirichlet processes (HDPs) [20], which organize DPs into a tree with parents acting as
the base measure for children, is one of the most popular. HDPs have been extended in a variety of
ways. Kim and Smyth [9] incorporated group-speciﬁc random perturbations, allowing component
parameters to vary across different groups. Ren et al. [17] proposed dynamic HDPs, which combine
the DP at a previous time step with a new one at the current time step.
Other methods have also been developed. MacEachern [13] proposed a DDP model that allows pa-
rameters to vary following a stochastic process. Grifﬁn and Steel [6] proposed the order-based DDP,
where atoms can be weighted differently via the permutation of the Beta variables for stick-breaking.
Chung and Dunson [3] carried this approach further, using local predictors to select subsets of atoms.
Recently, the connections between Poisson, Gamma, and Dirichlet processes have been exploited.
Rao and Teh [15] proposed the spatially normalized Gamma process, where a set of dependent DPs
can be derived by normalizing restricted projections of an auxiliary Gamma process over overlap-
ping sub-regions. Lin et al [12] proposed a new construction of dependent DPs, which supports
dynamic evolution of a DP through operations on the underlying Poisson processes.
Our primary goal here is to describe multiple groups of data through coupled mixture models. Shar-
ing statistical properties across different groups allows for more reliable model estimation, especially

1

when the observed samples in each group are limited or noisy. From a probabilistic standpoint, this
framework can be obtained by devising a joint stochastic process that generates DPs with mutual
dependency. Particularly, it is desirable to have a design that satisﬁes three properties: (1) Sharing
of mixture components (atoms) between groups. (2) The marginal distribution of atoms for each
group remains a DP. (3) Flexible conﬁguration of inter-group dependencies. For example, the prior
weight of a common atom can vary across groups.
Achieving these goals simultaneously is nontrivial. Whereas several existing constructions [3, 6, 12,
15] meet the ﬁrst two properties, they impose restrictions on the model structure (e.g. the groups need
to be arranged into a tree or a chain). We present a new framework to address this issue. Speciﬁcally,
we express mixture models for each group as a stochastic combination over a set of latent DPs. The
multi-to-multi association between data groups and latent DPs provides much greater ﬂexibility to
model conﬁgurations, as opposed to prior work (we provide a detailed comparison in section 3.2).
We also derive an MCMC sampling method to infer model parameters from grouped observations.

2 Background

(1)

D =

πk δφk , with πk = vk

We provide a review of Dirichlet processes in order to lay the theoretical foundations of the method
described herein. We also discuss the related construction of dependent DPs proposed by [12], which
exploits the connection between Poisson and Dirichlet processes to support various operations.
A Dirichlet process, denoted by DP(αB ), is a distribution over probability measures, which is
characterized by a concentration parameter α and a base measure B over an underlying space Ω.
Each sample path D ∼ DP(αB ) is itself a distribution over Ω. Sethuraman [18] showed that D is
∞(cid:88)
k−1(cid:89)
almost surely discrete (with countably inﬁnite support), and can be expressed as
(1 − vl ), vk ∼ Beta(1, α).
k=1
l=1
This is known as the stick breaking representation of a DP. This discrete nature makes a DP partic-
ularly suited to serve as a prior for component parameters in mixture models.
Generally, in a DP mixture model, each data sample xi is considered to be generated from a compo-
nent model with parameter θi , denoted by G (θi ). The component parameters are samples from D ,
which is itself a realization of a DP. The formulation is given below
(2)
for i = 1, . . . , n.
D ∼ DP(αB ),
θi ∼ D , xi ∼ G (θi ),
As D is an inﬁnite series, it is infeasible to instantiate D . As such, the Chinese restaurant process,
K/i(cid:88)
given by Eq. 3, is often used to directly sample the component parameters, with D integrated out.
m/i (k)
α
p(θi |θ/i ) =
α + (n − 1)
α + (n − 1)
k=1
Here, θ/i denotes all component parameters except θi , K/i denotes the number of distinct atoms
among them, and m/i (k) denotes the number of occurrences of the atom φk . When xi is given, the
likelihood to generate xi conditioned on θi can be incorporated, resulting in an modulated sampling
scheme described below. Let f (xi ; φ) denote the likelihood to generate xi w.r.t. G (φ), and f (xi ; B )
denote the marginal likelihood w.r.t. the parameter prior B . Then, with a probability proportional
to m/i (k)f (xi ; φk ), we set θi = φk , and with a probability proportional to αf (xi ; B ), we draw an
new atom from B (·|xi ), which is the posterior parameter distribution given xi .
Recently, Lin et al. [12] proposed a new construction of DPs based on the connections between
Poisson, Gamma, and Dirichlet processes. The construction provides three operations to derive new
DPs depending on existing ones, which we will use to develop the coupled DP model. Here, we
provide a brief review of these operations.
(1) Superposition. Let Dk ∼ DP(αkBk ) for k = 1, . . . , K be independent DPs and (c1 , . . . , cK ) ∼
Dir(α1 , . . . , αK ). Then the stochastic convex combination of these DPs as below remains a DP:
(4)
c1D1 + · · · + cK DK ∼ DP(α1B1 + · · · + αK BK ).

δφk +

B .

(3)

2

Figure 2: The reformulated model for Gibbs
sampling contains latent DPs, groups of data, and
atoms. Each sample xti is attached a label zti that
assigns it an atom φzti . To generate zti , we draw
a latent DP (from Mult(ct )) and choose a label
therefrom. In sampling, Hs is integrated out, re-
sulting in mutual dependency between zti , as in
the Chinese restaurant process.

Figure 1: This shows the graphical model
of the coupled DP formulation on a case
with four groups and two latent DPs. Each
mixture model Dt inherits atoms from Hs
with a probability qts , resulting in Eq.(7).
(2) Sub-sampling. Let D = (cid:80)∞
k=1 πk δφk ∼ DP (αB ). One obtains a new DP by sub-sampling D
via independent Bernoulli trials. Given a sub-sampling probability q , one draws a binary value rk
Sq (D) (cid:44) (cid:88)
with Pr(rk = 1) = q for each atom φk to decide whether to retain it, resulting in a DP as
(cid:48)
k δφk ∼ DP(αqB ).
k = πk / (cid:80)
k:rk=1
Here, Sq denotes the sub-sampling operation (with probability q ), and π (cid:48)
k is the re-normalized coef-
(3) Transition. Given D = (cid:80)∞
ﬁcient for φk , which is given by π (cid:48)
k rk πk .
lowing a probabilistic transition kernel T also yields a new DP, given by T (D) (cid:44) (cid:80)∞
k=1 πk δφk ∼ DP (αB ), perturbing the locations of each atom fol-
k=1 πk δT (φk ) .
While these operations were originally developed to evolve a DP along a Markov chain, we show in
the next section that they can also be utilized to construct models with different structures.

(5)

π

3 Coupled Nonparametric Mixture Models

Our primary goal is to develop a joint formulation over group-wise DP mixture models where com-
ponents are shared across different groups and the weights and parameters of shared components
vary across groups. We propose a new construction illustrated in Figure 1. Suppose there are M
groups of data, each with a mixture model. They are coupled by ML latent DPs. The generative
formulation is then described as follows: First, generate ML latent DPs independently, as
for s = 1, . . . , ML .
Hs ∼ DP (αsB ),
Second, generate M dependent DPs, each for a group of data, by combining the sub-sampled ver-
ML(cid:88)
sions of the latent DPs through stochastic convex combination. For each t = 1, . . . , M ,
ctsSqts (Hs ), with (ct1 , . . . , ctML ) ∼ Dir(α1 qt1 , . . . , αML qtML ).
Dt =
s=1
inherited by Dt . Note that this formulation can be further extended into Dt = (cid:80)
Intuitively, for each group of data (say the t-th), we choose a subset of atoms from each latent source
and bring them together to generate Dt . Here, qts is the prior probability that an atom in Hs will be
s ctsTt (Sqts (Hs )).
Here, Tt is a probabilistic transition kernel. Using the transition operation, this extension allows
parameters to vary across different groups. Particularly, the atom parameter would be an adapted
version from Tt (φk , ·) instead of φk itself, when the atom φk is inherited by Dt .

(7)

(6)

3

n4n3n2n1H1H2D1D2D3D4q31q21q11q22q32q42✓1ix1i✓2i✓3i✓4ix2ix3ix4i↵sHsMLctztixtintQBM1 k1rtkLatent DPsGroupsAtomsThird, generate the component parameters and data samples in the standard way, as
θt,i |Dt ∼ Dt , and xt,i |θt,i ∼ G (θt,i ),
for i = 1, . . . , nt , t = 1, . . . , M .
Here, xt,i is the i-th data sample in the t-th group, and θt,i is the associated atom parameter.

(8)

3.1 Theoretical Analysis
Theorem 1. The stochastic process Dt given by Eq.(7) has Dt ∼ DP(βtB ), with βt = (cid:80)ML
The following theorems (proofs provided in supplementary material) demonstrate that, as a result of
the construction above, the marginal distribution of Dt is a DP:
s=1 αs qts .
We also show that they are dependent, with the covariance given by the theorem below.
ML(cid:88)
Theorem 2. Let t1 (cid:54)= t2 and U be a measurable subset of Ω, then
(αs qt1 s qt2 s )2
1
αs qt1 s qt2 s + 1
βt1 βt2
s=1

B (U )(1 − B (U )).

Cov(Dt1 (U ), Dt2 (U )) =

(9)

It can be seen that the hyper-parameters inﬂuence the model characteristics in different ways. The
inheritance probabilities (i.e. the q -values) control how closely the models are coupled. Two models
are strongly coupled, if there exists a subset of latent DPs, from which both inherit atoms with high
probabilities, while their coupling is much weaker if the associated q -values are set differently. The
latent concentration parameters (i.e. the values of αs ) control how frequently new atoms are created.
Generally, higher values of αs lead to more atoms being associated with the data, resulting in ﬁner
clusters. Another important factor is ML , the number of latent DPs. A large number of latent DPs
provides ﬁne-grained control of the model conﬁguration at the cost of increased complexity.

3.2 Comparison with Other Models

We review related approaches and discuss their differences with the one proposed here. Similar
to this work, HDPs [20] model grouped data. Such models must be arranged into a tree, i.e. each
child can only have one parent. Our model allows the mixture model for each group to inherit from
multiple sources, making it applicable to more general contexts.
It is worth emphasizing that enabling inheritance from multiple parents is not just a straightfor-
ward extension, as it entails both theoretical and practical challenges: First, to combine atoms from
multiple DPs while guaranteeing that the resultant process remains a DP requires careful design of
the formulation (e.g. the combination coefﬁcients should be from a Dirichlet distribution, and each
parent DP should be properly sub-sampled). Second, the sampling procedure has to determine the
source of each atom, which, again, is nontrivial and needs special algorithmic design (see section 4)
to maintain the detailed balance.
SNΓP [15] deﬁnes a gamma process G over an extended space. For each group t, a DP Dt is
formulation in the form Dt = (cid:80)
derived through normalized restriction of G into a measurable subset. The DPs derived on over-
lapped subsets are dependent. Though motivated differently, this construction can be reduced to a
j∈Rt ctj Hj , where Rt is the subset of latent DPs used for Dt . Com-
pared to Eq.(7), we can see that it is essentially a special case of the present construction without
sub-sampling (i.e. all q -values equal 1). Consequently, the combination coefﬁcients have to satisfy
(ctj )j∈Rt ∼ Dir((αj )j∈Rt ), implying that the relative weights of two latent sources are restricted to
be the same in all groups that inherit from both. In contrast, the approach here allows the weights of
latent DPs to vary across groups. Also, SNΓP doesn’t allow atom parameters to vary across groups.

4 Sampling Algorithm

This section introduces a Gibbs sampling algorithm to jointly estimate the mixture models of mul-
tiple groups. Overall, this algorithm is an extension to the Chinese restaurant process, with several
new aspects: (1) The conditional probability of labels depend on the total number of samples asso-
ciated with it over the entire corpus (instead of that within a speciﬁc group). Note that it also differs

4

from HDP, where such probabilities depend on the number of associated tables. (2) Each group
maintains a distribution over the latent DPs to choose from, which reﬂects the different contribu-
tions of these sources. (3) It leverages the sub-sampling operation to explicitly control the model
complexity. In particular, each group maintains indicators of whether particular atoms are inherited,
and as a consequence, the ones that are deemed irrelevant are put out of scope. (4) As there are mul-
tiple latent DPs, for each atom, there is uncertainty about where it comes from. We have a speciﬁc
step that takes this into account, which allows reassigning an atom to different sources.
We ﬁrst set up the notations. Recall that there are M groups of data, and ML latent DPs to link
between them. The observations in the t-th group are xt1 , . . . , xtnt . We use φk to denote an atom.
Note here that the index k is a globally unique identiﬁer of the atom, which would not be changed
during atom relocation. Since an atom may correspond to multiple data samples. Instead of instan-
tiating the parameter θti for each data sample xti , we attach to xti an indicator zti that associates
the sample to a particular atom. This is equivalent to setting θti = φzti . To facilitate the sampling
process, for each atom φk , we maintain an indicator sk specifying the latent DP that contains it, and
a set of counters {mtk }, where mtk equals the number of associated data samples in t-th group. We
also maintain a set Is for Hs (the s-th latent DP), which contains the indices of all atoms therein.
The model in Eq.(7) and (8) can then be reformulated, as shown in Fig 2. It consists of four steps:
(1) Generate latent DPs: for each s = 1, . . . , ML , we draw Hs ∼ DP(αsB ). (2) Generate the
combination coefﬁcients: for each group t, we draw (ct1 , . . . , ctML ) ∼ Dir(α1 qt1 , . . . , αML qtML ),
which gives the group-speciﬁc prior over the sources for the t-th group. (3) Decide inheritance:
for each atom φk , we draw a binary variable rtk with Pr(rtk = 1) = qtsk to indicate whether φk is
inherited by the t-th group. Here sk is the index of the latent DP which φk is from. (4) Generate
data: to generate xti , we ﬁrst choose a latent DP by drawing u ∼ Mult(ct1 , . . . , ctML ), then draw
an atom from Hu , using it to produce xti . Based on this formulation, we derive the following Gibbs
sampling steps to update the atom parameters and other hidden variables.
(1) Update labels. Recall that each data sample xti is associated with a label variable zti that
indicates the atom accounting for xti . To draw zti , we ﬁrst have to choose a particular latent DP
(cid:32) (cid:88)
(cid:33)
as the source (we denote the index of this DP by uti ). Let z/ti denote all labels except zti , and rt
denote the inheritance indicators. Then, we get the likelihood of xti (with Hs integrated out) as
1
m∗k/ti f (xti ; φk ) + qtsαs f (xti ; B )
p(xti |uti = s, rt , z/ti ) =
wst/i + qtsαs
k∈Is :rtk=1
k∈Is :rtk=1 m∗k/ti , f (xti ; φk ) is the pdf at xti w.r.t. φk , and f (xti ; B ) = (cid:82)
(cid:80)
Here, m∗k/ti is the total number of samples associated with φk in all groups (except for xti ), wst/i =
θ f (xti ; θ)B (θ)dθ .
Derivations of this and other formulas for sampling are in the supplemental document. Hence,
(11)
p(uti = s|others) ∝ p(uti = s|ct )p(xti |uti = s, z/ti ) = ctsp(xti |uti = s, z/ti ).
Here, ct = (ct1 , . . . , ctML ) are the group-speciﬁc prior over latent sources. Once a latent DP is
chosen (using the formula above), we can then draw a particular atom. This is similar to the Chinese
restaurant process: with a probability proportional to m∗k/ti f (xti ; φk ), we set zti = k , and with a
probability proportional to qtsαs f (xti ; B ), we draw a new atom from B (·|xi ). Only the atoms that
is contained in Hs and has rtk = 1 (inherited by Dt ) can be drawn at this step.
We have to modify relevant quantities accordingly, such as mtk , ws , and Is , when a label zti is
changed. Moreover, when a new atom φk is created, it will be initially assigned to the latent DP that
generates it (i.e. setting sk = uti ).
(2) Update inheritance indicators. If an atom φk is associated with some data in the t-th group,
then we know for sure that it is inherited by Dt , and thus we can set rtk = 1. However, if φk is not
observed, it doesn’t imply rtk = 0. For such an atom (suppose it is from Hs ), we have
γ (τs/t , nt )
Pr(rtk = 1|others)
qts · p(zt |rtk = 1, others)
qts
Here, τs/t = qtsαs + (cid:80)
=
γ (τs/t + m∗k/t , nt )
Pr(rtk = 0|others)
(1 − qts ) · p(zt |rtk = 0, others)
1 − qts
(cid:81)n−1
k(cid:48)∈Is−{k} m∗k/t and m∗k(cid:48) /t is the number of samples associated with
k (cid:48) in all other groups (excluding the ones in the t-th group). γ is a function deﬁned by γ (τ , n) =
i=0 (τ + i) = Γ(τ + n)/Γ(τ ). Intuitively, when m∗k is large (indicating that φk appears frequently

. (10)

=

.

(12)

5

Figure 3: model
structures.

Figure 4: The results on NIPS data ob-
tained with training sets of different sizes.

Figure 5: The results on NIPS data us-
ing M-LDP, with different σ values.

mtk

(13)

ct |zt ∼ Dir

mtk , . . . , αML qtML +

in other groups) or nt is large, φk is likely to appear in the t-th group if it is inherited. Under such
circumstances, if φk not seen, then it is probably not inherited.
(3) Update combination coefﬁcients. The coefﬁcients ct = (ct1 , . . . , ctML ) reﬂect the relative
 .
α1 qt1 +
contribution of each latent DP to the t-th group. ct follows a Dirichlet distribution a priori (see
(cid:88)
(cid:88)
Eq.(7). Given zt , the labels of all samples in the t-th group, we have
Here, (cid:80)
k∈IML
k∈I1
k∈Is mtk is the total number of samples in the t-th group that are associated with Hs .
(4) Update atom parameters. Given all the labels, we can update the atoms, by re-drawing their
conditioned on Xk , with the pdf given by B (φ|Xk ) ∝ B (φ) (cid:81)
parameters from the posterior distributions. Let Xk denote the set of all data samples associated with
the k-th atom, then we can draw φk ∼ B (·|Xk ), where B (·|Xk ) denotes the posterior distribution
f (xk ; φ).
x∈Xk
(5) Reassign atoms. In this model, each atom is almost surely from a unique latent DP (i.e. it never
comes from two distinct sources). This leads to an important question: How to we assign atoms to
latent DPs? Initially, an atom is assigned to the latent DP from which it is generated. This is not
necessarily optimal. Here, we treat the assignment of each atom as a variable. Consider an atom φk ,
(cid:89)
(cid:89)
with sk indicating its corresponding source DP. Then, we have
(1 − qts ).
p(sk = j |others) =
t:rtk=0
t:rtk=1
When an atom φk that was in Hs is reassigned to Hs(cid:48) , we have to move the index k from Is to Is(cid:48) .

qts

(14)

5 Experiments

The framework developed in this paper provides a generic tool to model grouped data. In this sec-
tion, we present experiments on two applications: document analysis and scene modeling. The
primary goal is to demonstrate the key distinctions between the proposed approach and other non-
parametric methods, and study how the new design inﬂuences empirical performance.

5.1 Document Analysis

Topic models [1, 2, 7, 20] have been widely used for statistical analysis of documents. In general, a
topic model comprises a set of topics, each associated with a multinomial distribution, from which
words can be independently generated. Here, we formulate a Coupled Topic Model by extending
LDA [2] to model multiple groups of documents. Speciﬁcally, it associates the t-th group with a
mixture of topics, characterized by a DP sample Dt . With this given, the words in a document are
generated independently, each from a topic drawn from Dt . To exploit the statistical dependency
between groups, we further introduce a set of latent DPs to link between these mixtures, as described

6

HDP / S-LDPSNΓPM-LDP0200400600800100012001500200025003000350040004500# training docsperplexity  HDP (train)HDP (test)S−LDP (train)S−LDP (test)SNGP (train)SNGP (test)M−LDP (train)M−LDP (test)05101520180020002200240026002800σperplexity  # train docs = 400# train docs = 800# train docs = 1200above. The NIPS (1-17) database [5], which contains 2484 papers published from 1987 to 2003, is
used in our experiments. We clean the data by removing the words that occur fewer than 10 times
over the corpus and those that appear in more than 2000 papers, resulting in a reduced vocabulary
comprised of 11729 words. The data are divided into 17 groups, one for each year.
We perform experiments on several conﬁgurations, with different ways to connect between latent
sources and data groups, as illustrated in Figure 3. (1) Single Latent DP (S-LDP): there is only one
latent DP connecting to all groups, with q -values set to 0.5. Though with a structure similar to HDP,
the formulation is actually different: HDP generates group-speciﬁc mixtures by using the latent DP
as the base measure, while our model involves explicit sub-sampling. (2) Multi Latent DP (M-LDP):
there are two types of latent DPs – local and global ones. The local latent DPs are introduced to help
sharing statistical strength among the groups close to each other, so as to capture the intuition that
papers published in consecutive years are more likely to share topics than those published in distant
years. The inheritance probability from a local latent DP Hs to Dt is set as qts = exp(−|t − s|/σ).
Also, recognizing that some topics may be shared across the entire corpus, we also introduce a
global latent DP, from which every group inherit atoms with the same probability, which allows
distant groups to be connected. This design illustrates the ﬂexibility of the proposed framework and
how one can leverage this ﬂexibility to address practical needs.
For comparison, we also consider another setting of q -values under the M-LDP structure: to set
qts = I(|t − s| ≤ σ), that is to connect Dt and Hs only when |t − s| ≤ σ , with qts = 1. Under
this special setting, the formulation reduces to SNΓP [15]. We also test HDP following exactly the
settings given in [20]: α0 ∼ Gamma(0.1, 0.1) and γ ∼ Gamma(5, 0.1). Other design parameters
are set as below. We place a weak prior over αs for each latent DP, as αs ∼ Gamma(0.1, 0.1), and
periodically update its value. The base distribution B is assumed to be Dir(1), which is actually a
uniform distribution over the probability simplex.
The ﬁrst experiment is to compare different methods on training sets of various sizes. We divide
all papers into two disjoint halves, respectively for training and testing. In each test, models are
estimated upon a subset of speciﬁc size randomly chosen from the training corpus. The learned
models are then respectively tested on the training subset and the held-out testing set, so as to study
the gap between empirical and generalized performance, which is measured in terms of perplexity.
From Figure 4, we observe: (1) In general, as the training set size increases, the perplexity evaluated
on the training set increases and that on the testing set decreases. However, such convergence is
faster when local coupling is used (e.g. in SNΓP and M-LDP). This suggests that the sharing of
statistical strength through local latent DPs improves the reliability of the estimation, especially
when the training data are limited. (2) Even when the training set size is increased to 1200, the
methods using local coupling still yield lower perplexity than others. This is partly ascribed to the
model structure. For example, the papers published in consecutive years tend to share lots of topics,
however, the topics may not be as similar when you compare papers published recently to those
a decade ago. A set of local latent DPs may capture such relations more effectively than a single
global one. (3) The proposed method under M-LDP setting outperforms other methods, including
SNΓP. In M-LDP, the contribution of Hs to Dt decreases gracefully as |t − s| increases. This
way encourages each latent DP to be locally focused, while allowing the atoms therein to be shared
across the entire corpus. This is enabled through the use of explicit sub-sampling. The SNΓP,
instead, provides no mechanism to vary the contributions of the latent DPs, and has to make a hard
limit of their spans to achieve locality. Whereas this issue could be addressed through multiple level
of latent nodes with different spans, it will increase the complexity, and thus the risk of overﬁtting.
For M-LDP, recall that we set qts = exp(−|t − s|/σ). Here, σ is an important design parameter
that controls the range of local coupling. The results acquired with different σ values are shown in
Figure 5. Optimal performance is attained when the choice of σ balances the need to share atoms
and the desire to keep the latent DPs locally focused. Generally, the optimum of σ depends on data.
When the training set is limited, one may increase its value to enlarge the coupling range.

5.2 Scene Modeling

Scene modeling is an important task in computer vision. Among various approaches, topic models
that build upon bag-of-features image representation [4, 11, 21] have become increasingly popular

7

Figure 6: This ﬁgure shows example images in all eight categories
selected for the experiment.

Figure 7: The results on SUN data,
with training sets of different sizes.

and are widely used for statistical modeling of visual scenes. Along this trend, Dirichlet processes
have also been employed to discover visual topics from observed scenes [10, 19].
We apply the proposed method to jointly model the topics in multiple scene categories. Rather than
pursuing the optimal scene model, here we primarily aimed at comparing different nonparametric
methods in mixture model estimation, under a reasonable setting. We choose a subset from the
SUN database [22]. The selected set comprises eight outdoor categories: mountain snowy, hill,
boardwalk, swamp, water cascade, ocean, coast and sky. The number of images in each category
ranges from 50 to 100. Figure 6 shows some example images. We can see that some categories
are similar (e.g. ocean and coast, boardwalk and swamp, etc), while others are largely different. To
derive the image representation, PCA-SIFT [8] descriptors are densely extracted from each training
image, and then pooled together and quantized using K-means into 512 visual words. In this way,
each image can be represented as a histogram of 512 bins.
All methods mentioned above are compared. For M-LDP, we introduce a global latent DP to capture
common topics, with q -values set uniformly to 0.5, and a set of local latent DPs, each for a category.
The prior probability of inheriting from the corresponding latent DP is 1.0, and that from other local
DPs is 0.2. Whereas no prior knowledge about the similarity between categories is assumed, the
latent DPs incorporated in this way still provide a mechanism for local coupling. For SNΓP, we use
28 latent DPs, each connected to a pair of categories. Again, we divide the data into two disjoint
halves, respectively for training and testing, and evaluate the performance in terms of perplexity. The
results are shown in Figure 7, where we can observe trends similar to those that we have seen on the
NIPS data: local coupling helps model estimation, and our model under the M-LDP setting further
reduces the perplexity (from 37 to 31, as compared to SNΓP). This is due to the more ﬂexible way
to conﬁgure local coupling that allows the weights of latent DPs to vary.

6 Conclusion

We have presented a principled approach to modeling grouped data, where mixture models for dif-
ferent groups are coupled via a set of latent DPs. The proposed framework allows each mixture
model to inherit from multiple latent DPs, and each latent DP to contribute differently to differ-
ent groups, thus providing great ﬂexibility for model design. The experiments on both document
analysis and image modeling has clearly demonstrated the utility of such ﬂexibility. Particularly,
the proposed method makes it possible to make various modeling choices, e.g. the use of latent
DPs with different connection patterns, substantially improving the effectiveness of the estimated
models. While q -values are treated as design parameters, it should be possible to extend this frame-
work to incorporate prior models over these and other parameters. Such extensions should lead to
constructions with richer structure capable of addressing more complex problems.

Acknowledgements

This research was partially supported by the Ofﬁce of Naval Research Multidisciplinary Research
Initiative (MURI) program, award N000141110688 and by DARPA award FA8650-11-1-7154.

8

hillmountainsnowyboardwalkwatercascadeoceancoastskyswamp501001502002503003504004505002030405060708090100# training imagesperplexity  HDP (train)HDP (test)S−LDP (train)S−LDP (test)SNGP (train)SNGP (test)M−LDP (train)M−LDP (test)References
[1] David Blei and John Lafferty. Correlated topic models. In Proc. of NIPS’06, 2006.
[2] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3:993–1022, 2003.
[3] Yeonseung Chung and David B. Dunson. The local Dirichlet Process. Annals of the Inst. of
Stat. Math., 63(1):59–80, 2009.
[4] Li Fei-fei. A bayesian hierarchical model for learning natural scene categories. In Proc. of
CVPR’05, 2005.
[5] Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. Euclidean embedding of
co-occurrence data. JMLR, 8, 2007.
[6] J. E Grifﬁn and M. F. J Steel. Order-Based Dependent Dirichlet Processes. Journal of the
American Statistical Association, 101(473):179–194, March 2006.
[7] Thomas Hofmann. Probabilistic latent semantic indexing. In Proc. of ACM SIGIR’99, 1999.
[8] Yan Ke and Rahul Sukthankar. Pca-sift: A more distinctive representation for local image
descriptors. In Proc. of CVPR’04, 2004.
[9] Seyoung Kim and Padhraic Smyth. Hierarchical dirichlet processes with random effects. In
Proc. of NIPS’06, 2006.
[10] Jyri J. Kivinen, Erik B. Sudderth, and Michael I. Jordan. Learning multiscale representations
of natural scenes using dirichlet processes. In Proc. of CVPR’07, 2007.
[11] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for
recognizing natural scene categories. In Proc. of CVPR’06, 2006.
[12] Dahua Lin, Eric Grimson, and John Fisher. Construction of dependent dirichlet processes
based on poisson processes. In Advances of NIPS’10, 2010.
[13] Steven N. MacEachern. Dependent Nonparametric Processes. In Proceedings of the Section
on Bayesian Statistical Science, 1999.
[14] Radford M. Neal. Markov Chain Sampling Methods for Dirichlet Process Mixture Models.
Journal of computational and graphical statistics, 9(2):249–265, 2000.
[15] Vinayak Rao and Yee Whye Teh. Spatial Normalized Gamma Processes. In Proc. of NIPS’09,
2009.
[16] Carl Edward Rasmussen. The Inﬁnite Gaussian Mixture Model. In Proc. of NIPS’00, 2000.
[17] Lu Ren, David B. Dunson, and Lawrence Carin. The Dynamic Hierarchical Dirichlet Process.
In Proc. of ICML’08, New York, New York, USA, 2008. ACM Press.
[18] J. Sethuraman. A Constructive Deﬁnition of Dirichlet Priors. Statistica Sinica, 4(2):639–650,
1994.
[19] Erik B. Sudderth, Antonio Torralba, William Freeman, and Alan Willsky. Describing visual
scenes using transformed dirichlet processes. In Proc. of NIPS’05, 2005.
[20] Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. Hierarchical Dirichlet
Processes. Journal of the American Statistical Association, 101(476):1566–1581, 2006.
[21] Chang Wang, David Blei, and Fei-Fei Li. Simultaneous image classiﬁcation and annotation.
In Proc. of CVPR’09, 2009.
[22] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In Proc. of CVPR’10, 2010.

9


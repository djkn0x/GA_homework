On the (Non-)existence of Convex, Calibrated
Surrogate Losses for Ranking

Cl ´ement Calauz `enes, Nicolas Usunier, Patrick Gallinari
LIP6 - UPMC
4 place Jussieu, 75005 Paris, France
firstname.lastname@lip6.fr

Abstract

We study surrogate losses for learning to rank, in a framework where the rankings
are induced by scores and the task is to learn the scoring function. We focus on the
calibration of surrogate losses with respect to a ranking evaluation metric, where
the calibration is equivalent to the guarantee that near-optimal values of the sur-
rogate risk imply near-optimal values of the risk deﬁned by the evaluation metric.
We prove that if a surrogate loss is a convex function of the scores, then it is not
calibrated with respect to two evaluation metrics widely used for search engine
evaluation, namely the Average Precision and the Expected Reciprocal Rank. We
also show that such convex surrogate losses cannot be calibrated with respect to
the Pairwise Disagreement, an evaluation metric used when learning from pair-
wise preferences. Our results cast lights on the intrinsic difﬁculty of some ranking
problems, as well as on the limitations of learning-to-rank algorithms based on the
minimization of a convex surrogate risk.

1

Introduction

A surrogate loss is a loss function used as a substitute for the true quality measure during training in
order to ease the optimization of the empirical risk. The hinge loss or the exponential loss, which are
used in Support Vector Machines or AdaBoost as convex upper bounds of the classiﬁcation error, are
well-known examples of surrogate losses for binary classiﬁcation. In this paper, we study surrogate
losses for learning to rank, in a context where a set of items should be ranked given an input query
and where the ranking is obtained by sorting the items according to predicted numerical scores. This
work is motivated by the intensive research that has recently been carried out on machine learning
approaches to improve the quality of search engine results, and more speciﬁcally on the design of
surrogate losses that lead to high quality rankings (see [16] for a review).
Considering algorithms for learning to rank on the axis of scalability, there are ﬁrst algorithms that
are designed for small-scale datasets only and that directly solve the NP-hard problem [5] with-
out using any surrogate loss; after them come algorithms that use a surrogate loss chosen as a
non-convex but continuous and (almost everywhere) differentiable approximation of the evaluation
metric [3, 21, 10], and ﬁnally algorithms that use a convex surrogate loss. Most algorithms for
learning to rank fall into the latter category, including the reference algorithms RankBoost [12] and
Ranking SVMs [14, 4] or the regression approach of [8], because convex surrogate losses lead to
optimization problems that can be solved efﬁciently while non-convex approaches may require in-
tensive computations to ﬁnd a good local optimum. The disadvantage of convex surrogate losses is
that they cannot closely approximate the evaluation metrics on the whole prediction space. However,
as more examples are available and smaller values of the surrogate risk are achieved, the only region
of interest becomes that of near-optimal predictions. It is thus possible that the minimization of the
surrogate risk provably leads to optimal predictions according to the risk deﬁned by the evaluation
measure. In that case, the surrogate loss is said to be calibrated with respect to the evaluation metric.

1

The calibration of surrogate losses has been extensively studied for various classiﬁcation settings
[1, 26, 27, 18, 19] and for AUC optimization [7, 15]. For each of these tasks, many usual convex
losses are calibrated with respect to the natural evaluation metric. In the context of learning to rank
for search engines, several families of convex losses are calibrated with respect to the Discounted
Cumulative Gain (DCG) and its variants [8, 2, 17]. However, other metrics than the DCG are often
used as reference for the evaluation of ranked results, such as the Average Precision (AP), used in
past TREC competitions [22], the Expected Reciprocal Rank (ERR), used the Yahoo! Learning to
Rank Challenge [6], or the Pairwise Disagreement (PD), used when learning from pairwise pref-
erences. And despite the multiplicity of convex losses that have been proposed for ranking, none
of them was proved to be calibrated with respect to any of these three metrics. This lead us to the
question of whether convex losses can be calibrated with respect to the AP, the ERR, or the PD.
Our main contribution is a deﬁnitive and negative answer to that question. We prove that if a sur-
rogate loss is convex, then it cannot be calibrated with respect to any of the AP, the ERR or the
PD. Thus, if one of these metrics should be optimized, the price to pay for the computational ad-
vantage of convex losses is an inconsistent learning procedure, which may converge to non-optimal
predictions as the number of examples increases.
Our result generalizes previous works on non-calibration. First, Duchi et al. [11] showed that many
convex losses based on pairwise comparisons, such as those of RankBoost [12] or Ranking SVMs
[14, 4], are not calibrated with respect to the PD. Secondly, Buffoni et al. [2] showed that speciﬁc
convex losses, called order-preserving, are not calibrated with respect to the AP or the ERR, even
though these losses are calibrated with respect to (any variant of) the DCG. Our result is stronger
than those because we do not make any assumption on the exact structure of the loss; our approach as
a whole is also more general because it directly applies to the three evaluation metrics (AP, ERR and
PD). Finally, Duchi et al. conjectured that no convex loss can be calibrated with the PD in general
[11, Section 2.1] because it would provide a polynomial algorithm to solve an NP-hard problem.
Our approach thus leads to a direct proof of this conjecture.
In the next section, we describe our framework for learning to rank. We then present in Section 3
the general framework of calibration of [20], and give a new characterization of calibration for the
evaluation metrics we consider (Theorem 2), and the implications of the convexity of a surrogate
loss. Our main result is proved in Section 4. Section 5 concludes the paper, and Section 6 is a
technical part containing the full proof of Theorem 2.
Notation Let V , W be two sets. A set-valued function g from V to W maps all v ∈ V to a subset
V , i.e. g(V ) = (cid:83)
of W (set-valued functions appear in the paper as the result of arg min operations). Given a subset
V of V , the image of V by g , denoted by g(V ), is the union of the images by g of all members of
v∈V g(v). If n is a positive integer, [n] is the set {1, ..., n}, and Sn is the set of
permutations of [n]. Boldface characters are used for vectors of Rn . If x ∈ Rn , the i-th component
of x is denoted by xi (normal font and subscript). The cardinal of a ﬁnite set V is denoted by |V |.

2 Ranking Framework

We describe in this section the formal framework of ranking we consider. We ﬁrst present the
prediction problem we address, and then deﬁne the two main objects of our study: evaluation metrics
for ranking and surrogate losses. We end the section with an outline of our technical contributions.

2.1 Framework and Deﬁnitions
We consider a framework similar to label ranking [9] or subset ranking [8]. Let X be a measurable
space (the instance space). An instance x ∈ X represents a query and its associated n items to rank,
for an integer n ≥ 3. The items are indexed from 1 to n, and the goal is to order the set of item
indexes [n] = {1, ..., n} given x. The ordering (or ranking) is predicted by a scoring function, which
is a measurable function f : X → Rn . For any input instance x, the scoring function f predicts a
vector of n relevance scores (one score for each item) and the ranking is predicted by sorting the
item indexes by decreasing scores. We use permutations over [n] to represent rankings, with the
following conventions. First, given a permutation σ in Sn , k in [n] is the rank of the item σ(k);
second, low ranks are better, so that σ(1) is the top-ranked item.

2

Discounted Cumulative Gain
(higher values mean better performances)

Expected Reciprocal Rank
(higher values mean better performances)

n(cid:80)
k=1

y ∈ Y = {0, ..., Y }n ,
Y ∈ N, Y ≥ 1

TYPE OF FEEDBACK

METRIC

y ∈ Y = {0, 1}n
y ∈ Y = all DAGs over [n]

Average Precision
(higher values mean better performances)
Pairwise Disagreement
(lower values mean better performances)

Table 1: Formulas of r(y , σ) for some common ranking evaluation metrics
FORMULA
n(cid:80)
2yσ(k) −1
k−1(cid:81)
log(1+k)
k=1
(1 − Rq ), Rk = 2yσ(k) –1
σ -1 (i)(cid:80)
(cid:80)
Rk
2Y
k
q=1
I (cid:0)σ -1 (i) > σ -1 (j )(cid:1)
(cid:80)
yσ(k)
1
|{i:yi=1}|
σ -1 (i)
i:yi=1
k=1
i→j∈y
The quality of a ranking is measured by a ranking evaluation metric, relatively to a feedback. The
feedback space, denoted by Y , is a ﬁnite set, and an evaluation metric is a function r : Y × Sn → R.
We use the convention that lower values of r are preferable, and thus when we discuss existing
metrics for which higher values are better (e.g. the DCG, the AP or the ERR), we implicitly consider
their opposite. Table 1 gives the formula and feedback spaces of the evaluation metrics that we
discuss in the paper. The ﬁrst three metrics – the DCG, the ERR and the AP – are commonly used
for search engine evaluation. The feedback they consider is a vector of relevance judgments (one
judgment per item). The last measure we consider is the PD, which is widely used when learning
from pairwise preferences. For the feedback space of the PD, we follow [11] and take Y as the set
of all directed acyclic graph (DAG) over [n]. For a DAG y ∈ Y , there is an edge from item i to j
(denoted i → j ∈ y ) when i is preferred to j , or, equivalently when i should have better rank than j .
In general, using a sorting algorithm, any ranking evaluation metric r induces a quality measure on
vectors of scores instead of rankings, considering that the sorting algorithm break ties randomly.
Thus, using the following set-valued function from Rn to Sn , called arg sort, which gives the set
(cid:9) ,
arg sort(s) = (cid:8)σ ∈ Sn |∀k ∈ [n − 1] , sσ(k) ≥ sσ(k+1)
of rankings induced by a vector of scores:
∀s = (s1 , ..., sn ) ∈ Rn ,
(cid:88)
the evaluation metric on vectors of scores induced by r is deﬁned by:
∀y ∈ Y , ∀s ∈ Rn ,
r (cid:48) (y , s) =
r (y , σ)
| arg sort(s)| .
is to ﬁnd a scoring function f with low ranking risk R(cid:48)(D , f ) = (cid:82)
σ∈arg sort(s)
For a ﬁxed, but unknown, probability measure D on X × Y , the objective of a learning algorithm
X ×Y r (cid:48) (y , f (x))dD(x, y) using a
training set of (instance, feedback) pairs (e.g. drawn i.i.d. according to D).
The optimization of the empirical ranking risk is usually intractable because the ranking loss is
discontinuous. To address this issue, algorithms optimize the empirical risk associated to a surrogate
loss instead. Throughout the paper, we assume that this loss is bounded below, so that all the inﬁma
surrogate risk of a scoring function f is then deﬁned by L(D , f ) = (cid:82)
we take are well-deﬁned. Without loss of generality, we assume that the surrogate loss has non-
negative values, and we deﬁne a surrogate loss as a measurable function (cid:96) : Y × Rn → R+ . The
X ×Y (cid:96) (y , f (x))dD(x, y).
2.2 Outline of the Analysis

Any learning algorithm that performs empirical or structural risk minimization on the surrogate risk
can, at most, be expected to reach low values of the surrogate risk. The question we address in
this paper is whether such an algorithm provably solves the real learning task, which is to achieve
low values of the ranking risk. More formally, the criterion under study is whether the following
implication holds for every sequence of scoring functions (fk )k≥0 and every data distribution D :
R(cid:48)(D , f )
L(D , f ) ⇒ R(cid:48)(D , fk ) −→
L(D , fk ) −→
(1)
k→∞ inf
k→∞ inf
f
f
where the inﬁma are taken over all scoring functions. In particular, we show that if a surrogate loss
is convex in the sense that (cid:96)(y , .) is convex for every y ∈ Y , and if the evaluation metric is the AP,

3

the ERR or the PD, then there are distributions and sequences of scoring functions for which (1)
does not hold. In other words, we show that learning-to-rank algorithms that deﬁne their objective
through a convex surrogate loss cannot provably optimize any of these evaluation metrics.
In order to perform a general analysis for all the three evaluation metrics, we consider Assumption
(A) below, which formalizes the common property of these metrics that is relevant to our study.
Intuitively, it means that for any given item, there is a feedback for which the performance only
depends on the rank of this item, with a strict improvement of performances when one improves the
rank of the item:
(A) ∃β1 < β2 < ... < βn such that ∀i ∈ [n] , ∃y ∈ Y : ∀σ ∈ Sn , r(y , σ) = βσ -1 (i) .
Note that in the assumption, the values of βk (i.e. the performance when item i is predicted at rank
k) are the same for all items. This is not a strong requirement because the metrics we consider do
not depend on how we index the elements. The DCG, the AP and the ERR satisfy (A): for each i,
we take the vector of relevance with a 1 for item i and 0 for all other items so that the values of the
metrics only depends on the rank of i (which should be ranked ﬁrst). The PD satisﬁes Assumption
(A) as well: for each i, take y as the DAG containing the edges i → j, ∀j ∈ [n] \ {i} and only those
edges. For this feedback, i is preferred to all other items (and no preference is speciﬁed regarding
the other items) and thus the quality of a ranking only depends on the rank of i.
Our analysis is organized as follows. In the next section, we introduce the notion of a calibrated
surrogate loss deﬁned by Steinwart [20], which is a criterion equivalent to (1). We then obtain a
new condition that is equivalent to calibration when (A) holds, and ﬁnally we restrict our attention
to evaluation metrics satisfying (A) and to convex surrogate losses. In that context, using our new
condition for calibration, we show that evaluation metrics with a calibrated surrogate loss necessarily
satisfy a speciﬁc property. Then, in Section 4, we prove that the AP, the ERR and the PD do not
satisfy this property. Since Assumption (A) holds for these three metrics, this latter result implies
that they do not have any convex and calibrated surrogate loss. Equivalently, it implies that (1) does
not hold in general for these metrics if the surrogate loss is convex.

3 A New Characterization of Calibration

We present in this section the notion of calibration as studied in [20], which is the basis of our
work. Then, we provide a characterization of calibration more speciﬁc to the evaluation metrics
we consider, that relates more closely calibrated surrogate losses and evaluation metrics. This more
speciﬁc characterization of calibration is the starting point of the analysis of convex and calibrated
surrogate losses carried out in the last subsection and that allows us to state the results of Section 4.

3.1 The Framework of Calibration
Applying the general results of [20] to our setting, the criterion deﬁned by (1) can be studied by
noting the set of probability distributions over Y by P = (cid:8)p : Y → [0, 1]| (cid:80)
y∈Y p(y) = 1(cid:9), the
restricting our attention to the contributions of a single instance to the surrogate and ranking risk.
These contributions are called the inner surrogate risk and the inner ranking risk respectively. De-
(cid:88)
inner risks are respectively deﬁned for all p ∈ P and all s ∈ Rn by:
(cid:88)
(cid:88)
y∈Y
| arg sort(s)| , where ∀σ ∈ Sn , R (p, σ) =
and R(cid:48) (p, s) =
R (p, σ)
y∈Y
σ∈arg sort(s)
L (p, s) and R(cid:48) (p) = R (p) = min
Their optimal values are denoted by L (p) = inf
s∈Rn
σ∈Sn
More precisely, [20, Theorem 2.8] shows that (1) holds for any distribution D and any sequence of
scoring functions if and only if the surrogate loss is r-calibrated according to the deﬁnition below.
Similarly to (1), the calibration is an implication of two limits, but it involves the inner risks L and
R(cid:48) instead of the risks L and R(cid:48) . For convenience in the rest of the work, we write the implication

p(y)r (y , σ) .

R (p, σ).

L (p, s) =

p(y)(cid:96) (y , s)

4

between the two limits of L and R(cid:48) as an inclusion of the sets of near-optimal vectors of scores. For
any ε > 0 and δ > 0, the latter sets are respectively denoted by
M(cid:96) (p, δ) = {s ∈ Rn |L (p, s) − L (p) < δ} and Mr (p, ε) = {s ∈ Rn |R(cid:48) (p, s) − R(cid:48) (p) < ε} ,
so that the deﬁnition of an r-calibrated loss is the following:
Deﬁnition 1. [20, Deﬁnition 2.7] The surrogate loss (cid:96) is r-calibrated if
∀p ∈ P , ∀ε > 0, ∃δ > 0 : M(cid:96) (p, δ) ⊆ Mr (p, ε) .

3.2 Calibration through Optimal Rankings

Deﬁnition 1 is the starting point of our analysis, and our goal is to show that if the evaluation metric
is the AP, the ERR or the PD, then no convex surrogate loss can satisfy it. The goal of this subsection
is to give a stronger characterization of r-calibrated surrogate losses when Assumption (A) holds.
The starting point of this characterization is to rewrite Deﬁnition 1 in terms of rankings induced by
the sets of near-optimal scores, from which we can deduce that (cid:96) is r-calibrated if and only if1 :
∀p ∈ P , ∀ε > 0, ∃δ > 0 : arg sort(M(cid:96) (p, δ)) ⊆ arg sort(Mr (p, ε)) .
In contrast to this characterization of calibration, our result (Theorem 2 below), which is speciﬁc to
metrics that satisfy (A), replaces the inclusion (which can be strict in general) of sets of ranking by
an equality when ε tends to 0. More speciﬁcally, we deﬁne the set of optimal rankings for the inner
ranking risk with the following set-valued function from P to Sn :
∀p ∈ P , Ar (p) = arg min
R (p, σ) ,
σ∈Sn
so that when Assumption (A) holds, the set of optimal rankings is equal to a set of rankings induced
by near-optimal scores of the inner surrogate risk:
Theorem 2. If Assumption (A) holds, then (cid:96) is r-calibrated if and only if
∀p ∈ P , ∃δ > 0 s.t. arg sort(M(cid:96) (p, δ)) = Ar (p) .
The proof of Theorem 2 is deferred to Section 6 at the end of the paper. This theorem enables us to
relate the surrogate loss and the evaluation metric so that the convexity of (cid:96) induces some constraints
on r that are not satisﬁed by all evaluation metrics.

3.3 The implication of Convexity on Sets of Optimal Rankings
If (cid:96)(y , .) is convex for all y ∈ P , then the inner risk L(p, .) is also convex for every distribution
p ∈ P . This implies that M(cid:96) (p, δ) is a convex subset of Rn . Thus, if (cid:96) is r-calibrated, then Theorem
2 implies that Ar (p) = arg sort(M(cid:96) (p, δ)) is a set of rankings induced by a convex set of Rn .
The following theorem presents a condition that the set Ar (p) must satisfy if it is generated by a
convex set of scores: if there exists at least one pair of items (i, j ) which are inverted in two rankings
of Ar (p), then i and j are “indifferent” in Ar (p):
Theorem 3. Assume that for all y ∈ Y , the function s (cid:55)→ (cid:96) (y , s) is convex. If Assumption (A) holds
and (cid:96) is r-calibrated, then r satisﬁes: ∀p ∈ P , ∀i, j ∈ [n] , ∀σ, σ (cid:48) ∈ Ar (p),
σ -1 (i) < σ -1 (j ) and σ (cid:48)-1 (i) > σ (cid:48)-1 (j ) ⇒ ∃s ∈ Rn : si = sj and arg sort(s) ⊆ Ar (p) .
(2)
Proof of Theorem 3. Assume that the conditions of the theorem are satisﬁed. From now on, we ﬁx
arg sort(cid:0)M(cid:96) (p, δ) (cid:1) by Theorem 2. Thus, there are two score vectors u and v in M(cid:96) (p, δ) such
some p ∈ P and two i and j in [n]. Take σ and σ (cid:48) in Ar (p) and assume that σ -1 (i) < σ -1 (j )
and σ (cid:48)-1 (i) > σ (cid:48)-1 (j ). Since Assumption (A) holds, there is a δ > 0 such that Ar (p) =
that ui ≥ uj (u induces the ranking σ ) and vi ≤ vj (v induces the ranking σ (cid:48) ).
Moreover, since (cid:96) is convex, the function L (p, .) is convex for every p ∈ P , and thus M(cid:96) (p, δ) is
convex. Consequently, for all t ∈ [0, 1], the vector γ (t) = (1 − t)u + tv belongs to M(cid:96) (p, δ). We
deﬁne g : t (cid:55)→ γi (t) − γj (t) for t ∈ [0, 1]. Then, g is continuous, with g(0) = ui − uj ≥ 0 and
g(1) = vi − vj ≤ 0. By the intermediate value theorem, there is t0 ∈ [0, 1] such that g(t0 ) = 0. The
1We remind to the reader the notation arg sort(M(cid:96) (p, δ)) = (cid:83)
consequence is that the score vector s, deﬁned by s = γ (t0 ), satisﬁes s ∈ M(cid:96) (p, δ) and si = sj .
s∈M(cid:96)(p,δ) arg sort(s).
5

(1 − α)p110 + αp001

Table 2: Examples for Corollary 4. There are three elements to rank. i (cid:31) j (cid:31) k represents the
permutation that ranks item i ﬁrst, j second and k last. For the ERR and the AP, we consider binary
relevance judgments. p110 denotes a Dirac distribution at the feedback vector y = [1, 1, 0]. p001
is deﬁned similarly. For the Pairwise Disagreement, p1(cid:31)2(cid:31)3 is the Dirac distribution at the DAG
containing the edges 1 → 2, 2 → 3 and 1 → 3, i.e. the DAG corresponding to 1 (cid:31) 2 (cid:31) 3. The
Dirac distribution at the DAG containing only the edge 3 → 1 is denoted by p3(cid:31)1 . In all cases, ˜p(α)
is a mixture between two Dirac distributions. The sets Ar ( ˜p(α)) are obtained by direct calculations.
The set Ar ( ˜p(α)) is the same for all αs in the range given in the third column.
α ∈ (cid:0) 1
(cid:1)
Ar ( ˜p(α))
DISTRIBUTION ˜p(α) METRIC RANGE OF α
{(1 (cid:31) 3 (cid:31) 2), (2 (cid:31) 3 (cid:31) 1)}
3 , 1
{(1 (cid:31) 2 (cid:31) 3), (3 (cid:31) 1 (cid:31) 2),
2
α ∈ (cid:0) 2
3 , 1(cid:1)
(2 (cid:31) 1 (cid:31) 3), (3 (cid:31) 2 (cid:31) 1)}
α = 5
13
{(2 (cid:31) 3 (cid:31) 1), (3 (cid:31) 1 (cid:31) 2)}
The contrapositive of Theorem 3 is our technical tool to prove the nonexistence of convex and
calibrated losses. Indeed, for a given evaluation metric r , if we are able to exhibit a distribution
p ∈ P such that (2) is not satisﬁed, this evaluation metric cannot have a surrogate loss both convex
and calibrated. In the next subsection, we apply this argument to the AP, the ERR and the PD.
Remark 1. It has been proved by several authors that there exist convex surrogate losses that are
i (p) = (cid:80)
DCG-calibrated [8, 2, 17]. Thus, the DCG satisﬁes (2). It can be seen by observing that the optimal
rankings for the DCG are exactly those generated by sorting the items according to the vector of
y∈Y p(y)2yi , i.e. Ar (p) = arg sort(s∗ (p)).
score s∗ (p) deﬁned by s∗
4 Nonexistence Results

(1 − α)p1(cid:31)2(cid:31)3 + αp3(cid:31)1

ERR
AP

PD

We now present the main result of the nonexistence of convex, calibrated surrogate losses:
Corollary 4. No convex surrogate loss is calibrated with respect to the AP, the ERR or the PD.

Proof. We consider the case where there are three elements to rank, and we use the examples and
the notations of Table 2. Since all three metrics satisfy (A), Theorem 3 implies that if r (taken as
either the AP, the ERR or the PD) has a calibrated, convex surrogate loss, then, for any distribution
˜p(α), we have: if item i is preferred to j according to a ranking in Ar ( ˜p(α)), and j is preferred to i
(a) (cid:8)(i (cid:31) j (cid:31) k), (j (cid:31) i (cid:31) k)(cid:9) ⊆ Ar ( ˜p(α)) ,
(b) (cid:8)(k (cid:31) i (cid:31) j ), (k (cid:31) j (cid:31) i)(cid:9) ⊆ Ar ( ˜p(α))
according to another ranking in Ar ( ˜p(α)), then one of the two assertions below must hold:
(cid:1), we
si = sj ≥ sk . Now, let us consider the case of the ERR. Taking an arbitrary α ∈ (cid:0) 1
because there exists s ∈ R3 such that arg sort(s) ⊆ Ar ( ˜p(α)) for which either si = sj ≤ sk or
3 , 1
see on the last column of Table 2 that Ar ( ˜p(α)) contains two rankings: one of them ranks item 1
2
before item 2, and the other one ranks 2 before 1. If the ERR had a convex calibrated surrogate loss,
then either (a) or (b) should hold. However, we see that neither (a) nor (b) holds. Thus their is no
convex, ERR-calibrated surrogate loss. For the AP, a similar argument with items 1 and 3 leads to
the conclusion. For the PD, taking any two items leads to the result.

A ﬁrst consequence of Corollary 4 is that for ranking problems evaluated in terms of AP, ERR or
PD, surrogate losses deﬁned as convex upper bounds on an evaluation metric as discussed in [24],
as well as convex surrogate losses proposed in the structured output framework such as SVMmap
[25] are not calibrated with respect to the evaluation metric they are designed for. The convex
surrogate losses used by most participants of the recent Yahoo! Learning to Rank Challenge [6] are
also not calibrated with respect to the ERR, the ofﬁcial evaluation metric of the challenge. The fact
that the minimization of a non-calibrated surrogate risk leads to suboptimal prediction functions on
some data distributions suggests that convex losses are not a deﬁnitive solution to learning to rank.
Signiﬁcant improvements in performances may then be obtained by switching to other approaches
than the optimization of a convex risk.

6

5 Conclusion

We proved that convex surrogate losses cannot be calibrated with three major ranking evaluation
metrics. The result cast light on the intrinsic limitations of all algorithms based on (empirical)
convex risk minimization for ranking, even though most existing algorithms for learning to rank
follow this approach. A possible direction for future work is to study whether the calibration of
convex losses can be obtained under low noise conditions. Such studies was carried out for the
PD [11], and calibrated, convex surrogate losses were found for special cases of practical interest.
Nonetheless, in order to obtain algorithms that do not rely on low noise assumptions, our results
suggest to explore whether alternatives to convex surrogate approaches can lead to improvements in
terms of performances. A ﬁrst possibility is to turn to non-convex losses for ranking as in [10, 3],
and to study the calibration of such losses. Another alternative is to use another surrogate approach
than scoring, such as directly learning pairwise preferences [13], even though the reconstruction of
an optimal ranking, given the pairwise predictions, that is optimal for evaluation metrics such as the
AP, the ERR or the PD is still mostly an open issue.

6 Proof of Theorem 2
p ∈ P , there exists δ > 0 such that Ar (p) = arg sort(M(cid:96) (p, δ) (cid:1). We prove the result using the
We remind the statement of Theorem 2: if r satisﬁes (A), then (cid:96) is r-calibrated if and only if for all
(cid:101)L (p, σ) where (cid:101)L (p, σ) = inf (cid:8)L (p, s) | s ∈ Rn s.t. σ ∈ arg sort(s) (cid:9) .
following set-valued function which deﬁnes the set of optimal rankings for the inner surrogate risk:
A(cid:96) (p) = arg min
σ∈Sn
(a) the assertion ∀p ∈ P , ∃δ > 0, arg sort(M(cid:96) (p, δ) (cid:1) = A(cid:96) (p) is true in general;
Then, Theorem 2 is a direct implication of the two following claims that we prove in this section:
(b) if Assumption (A) holds, then (cid:96) is r-calibrated if and only if ∀p ∈ P , A(cid:96) (p) = Ar (p).
The proof of these two claims is based on three lemmas that we present before the ﬁnal proof. The
ﬁrst lemma, which does not need any assumption on the evaluation metric, both proves equality
(a) and provides a general characterization of calibration in terms of optimal rankings. The second
lemma concerns the surrogate loss; it states that a slight perturbation in p does not affect “too much”
A(cid:96) (p). The third lemma concerns evaluation metrics and gives a simple consequence of Assumption
(A). The ﬁnal proof of Theorem 2 connects all these pieces together to prove (b).
(i) ∀p ∈ P , ∀δ > 0, A(cid:96) (p) ⊆ arg sort(cid:0)M(cid:96) (p, δ) (cid:1).
Lemma 5. The following claims are true:
(ii) ∀p ∈ P , ∃δ0 > 0 : A(cid:96) (p) = arg sort(cid:0)M(cid:96) (p, δ0 ) (cid:1).
(iii) (cid:96) is r-calibrated if and only if: ∀p ∈ P , A(cid:96) (p) ⊆ Ar (p).
Proof. (i) Fix p ∈ P and δ > 0. Let σ ∈ A(cid:96) (p). By the deﬁnition of (cid:101)L, there is an s ∈ Rn such
that σ ∈ arg sort(s) and L (p, s) − (cid:101)L (p, σ) < δ . Since (cid:101)L (p, σ) = minσ (cid:48)∈Sn (cid:101)L (p, σ (cid:48) ) = L (p), we
have L (p, s) − L (p) < δ . This proves s ∈ M(cid:96) (p, δ) and thus σ ∈ arg sort(cid:0)M(cid:96) (p, δ) (cid:1).
(ii) Fix p ∈ P and take δ0 = minσ (cid:54)∈A(cid:96)(p) (cid:101)L (p, σ) − L (p) > 0, with the convention min ∅ = +∞.
equivalent to arg sort(cid:0)M(cid:96) (p, δ0 ) (cid:1) ⊆ A(cid:96) (p). The reverse inclusion is given by the ﬁrst point.
The choice of δ0 guarantees that ∀s ∈ Rn , L (p, s) − L (p) < δ0 ⇒ arg sort(s) ⊆ A(cid:96) (p), which is
(iii) Since r can only take a ﬁnite set of values, we can prove that (cid:96) is r-calibrated if and only if:
∀p ∈ P , ∃δ > 0 : ∀s ∈ Rn , L (p, s) − L (p) < δ ⇒ R(cid:48) (p, s) = R(cid:48) (p). Moreover, we
that arg sort(cid:0)M(cid:96) (p, δ) (cid:1) ⊆ Ar (p). This characterization and the ﬁrst two points give the result.
have R(cid:48) (p, s) = R(cid:48) (p) ⇔ arg sort(s) ⊆ Ar (p) since R(cid:48) (p, s) is the mean of R (p, σ) for
σ ∈ arg sort(s). Thus, (cid:96) is r-calibrated if and only if for every p ∈ P , there exists δ > 0 such
7

We now present a more technical result on A(cid:96) , which shows the set of optimal rankings cannot
dramatically change under a slight perturbation in the distribution over the feedback space. From
now on, for any p ∈ P and any η > 0, we denote by B (p, η) the open ball of P (with respect to
(cid:107).(cid:107)1 ) of radius η centered at p, i.e. B (p, η) = {p(cid:48) ∈ P |(cid:107)p − p(cid:48)(cid:107)1 < η}.
Lemma 6. ∀p ∈ P , ∃η > 0 such that A(cid:96) (B (p, η)) = A(cid:96) (p).
the main argument is that (cid:101)L (., σ) is continuous for every σ because Y is ﬁnite [23, Theorem 2].
Proof. Note that A(cid:96) (p) ⊆ A(cid:96) (B (p, η)) since p ∈ B (p, η). We now prove A(cid:96) (B (p, η)) ⊆ A(cid:96) (p);
(cid:0) minσ (cid:48) (cid:54)∈A(cid:96)(p) (cid:101)L (p, σ (cid:48) ) − L (p)(cid:1). For each σ ∈ Sn , since
(cid:101)L (., σ) is continuous, there exists ησ > 0 such that ∀p(cid:48) ∈ B (p, ησ ), | (cid:101)L (p(cid:48) , σ) − (cid:101)L (p, σ)| < ε.
Indeed, let us ﬁx p ∈ P and deﬁne ε = 1
2
Let η = minσ∈Sn ησ , and let p(cid:48) be an arbitrary member of B (p, η). By the deﬁnition of ε, we have:
∀σ (cid:48) (cid:54)∈ A(cid:96) (p) , (cid:101)L (p(cid:48) , σ (cid:48) ) = (cid:101)L (p(cid:48) , σ (cid:48) ) − (cid:101)L (p, σ (cid:48) ) + (cid:101)L (p, σ (cid:48) ) − L (p) + L (p) > −ε + 2ε + L (p) .
(cid:54)∈ A(cid:96) (p) , (cid:101)L (p(cid:48) , σ (cid:48) ) > L (p) + ε. Additionally, the deﬁnition of η gives ∀σ ∈
A(cid:96) (p) , (cid:101)L (p(cid:48) , σ) < L (p) + ε. Thus, we have minσ (cid:48) (cid:54)∈A(cid:96)(p) (cid:101)L (p(cid:48) , σ (cid:48) ) > minσ∈A(cid:96)(p) (cid:101)L (p(cid:48) , σ).
Thus, ∀σ (cid:48)
This proves that a ranking that is not optimal for (cid:101)L (p, .) cannot be optimal for (cid:101)L (p(cid:48) , .). Thus
A(cid:96) (p(cid:48) ) ⊆ A(cid:96) (p) from which we conclude A(cid:96) (B (p, η)) ⊆ A(cid:96) (p).
Now that we have studied the properties of A(cid:96) , we analyze in more depth the evaluation metrics. We
prove the following consequence of Assumption (A): for each possible ranking there is a distribution
over the feedback space for which this ranking is the unique optimal ranking.
Lemma 7. If Assumption (A) holds, then ∀σ ∈ Sn , ∃pσ ∈ P such that Ar (pσ ) = {σ}.
with α1 > ... > αn > 0 and (cid:80)n
Proof. Assume (A) holds, and, for each item k , let us denote by yk the feedback corresponding
to item k in Assumption (A). Now, let us take some σ ∈ Sn and deﬁne pσ as pσ (yk ) = ασ -1 (k)
k=1 ασ -1 (k) r (cid:0)yk , σ (cid:48) (cid:1) = (cid:80)n
R (pσ , σ (cid:48) ) = (cid:80)n
k=1 αk = 1. Then, for any σ (cid:48) ∈ Sn , we have the equality
k=1 ασ -1 (k)βσ (cid:48)-1 (k) . Since the αs are non-negative, and
since there are ties neither the αs nor in the β s, the rearrangement inequality implies that the mini-
mum value of R (pσ , σ (cid:48) ) is obtained for the single permutation σ (cid:48) for which the βσ (cid:48)-1 (k) are in reverse
σ (cid:48) (cid:55)→ R (pσ , σ (cid:48) ) = (cid:80)n
order relatively to the ασ -1 (k) (i.e. smaller values βσ (cid:48)-1 (k) should be associated to greater values of
ασ -1 (k) ). Since the αk s are decreasing with k and the βk s are increasing, the minimum value of
k=1 ασ -1 (k)βσ (cid:48)-1 (k) is obtained if and only if σ -1 = σ (cid:48)-1 (i.e. σ (cid:48) = σ ).
there is δ > 0 such that A(cid:96) (p) = arg sort(M(cid:96) (p, δ) (cid:1). What remains to show is that if Assumption
Proof of Theorem 2. We remind to the reader that by the second point of Lemma 5, for any p ∈ P ,
(A) holds, then (cid:96) is r-calibrated if and only if ∀p ∈ P , A(cid:96) (p) = Ar (p).
(“if ” direction) If ∀p ∈ P , A(cid:96) (p) = Ar (p) then (cid:96) is r-calibrated by Lemma 5.
(“only if ” direction) Assume that (A) holds and that (cid:96) is r-calibrated. Let p ∈ P . By Point (iii) of
Lemma 5, we know that A(cid:96) (p) ⊆ Ar (p). We now prove the reverse inclusion A(cid:96) (p(cid:48) ) ⊆ Ar (p(cid:48) ).
By Lemma 6, there exists some η > 0 such that A(cid:96) (B (p, η)) = A(cid:96) (p). Let σ ∈ Ar (p). The idea is
to use Lemma 7 to ﬁnd some p(cid:48) ∈ B (p, η) such that A(cid:96) (p(cid:48) ) = {σ} which would prove σ ∈ A(cid:96) (p)
and thus the result. The rest of the proof consists in building p(cid:48) .
Using Lemma 7, let pσ ∈ P such that Ar (pσ ) = {σ}. Now, let p(cid:48) = (1 − η
4 )p + η
4 pσ . Then, we have
(cid:107)p − p(cid:48)(cid:107)1 = η
4 (cid:107)p − pσ (cid:107)1 ≤ η/2 and thus p(cid:48) ∈ B (p, η). Moreover, Ar (p(cid:48) ) = {σ} since σ is optimal
for both p and pσ , and any other permutation is suboptimal for pσ . We also have A(cid:96) (p(cid:48) ) = {σ}
because A(cid:96) has non-empty values and calibration implies that A(cid:96) (p(cid:48) ) ⊆ Ar (p(cid:48) ) by Lemma 5.

Acknowledgements

This work was partially funded by the French DGA. The authors thank M. R. Amini, D. Buffoni, S.
Cl ´emenc¸ on, L. Denoyer and G. Wisniewski for their comments and suggestions.

8

References
[1] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. J. of the
American Stat. Assoc., pages 1–36, 2006.
[2] D. Buffoni, C. Calauz `enes, P. Gallinari, and N. Usunier. Learning scoring functions with order-preserving
losses and standardized supervision. In Proc. of the Intl. Conf. on Mach. Learn., pages 825–832, 2011.
[3] C. J. Burges, R. Ragno, and Q. V. Le. Learning to rank with nonsmooth cost functions. In Proc. of Adv.
in Neural Info. Proc. Syst., pages 193–200, 2007.
[4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking svm to document retrieval.
In Proc. of the ACM SIGIR Conf. on Res. and Dev. in Info. Retr., pages 186–193, 2006.
[5] A. Chang, C. Rudin, M. Cavaretta, R. Thomas, and G. Chou. How to reverse-engineer quality rankings.
Mach. Learn., 88(3):369–398, Sept. 2012.
[6] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. J. of Mach. Learn. Res., 14:1–24,
2011.
[7] S. Cl ´emenc¸ on, G. Lugosi, and N. Vayatis. Ranking and scoring using empirical risk minimization. In
Proc. of the 18th Conf. on Learning Theory, COLT’05, pages 1–15, 2005.
[8] D. Cossock and T. Zhang. Statistical analysis of bayes optimal subset ranking. IEEE Trans. Info. Theory,
54:5140–5154, 2008.
[9] O. Dekel, C. D. Manning, and Y. Singer. Log-linear models for label ranking. In Proc. of Advances in
Neural Information Processing Systems (NIPS), 2003.
[10] C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola. Tighter bounds for structured estimation. In Proc.
of Adv. in Neural Inf. Processing Syst., pages 281–288, 2008.
[11] J. Duchi, L. W. Mackey, and M. I. Jordan. On the consistency of ranking algorithms. In Proc. of the Int.
Conf. on Mach. Learn., pages 327–334, 2010.
[12] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining prefer-
ences. J. of Mach. Learn. Res., 4:933–969, 2003.
[13] E. Hullermeier, J. Furnkranz, W. Cheng, and K. Brinker. Label ranking by learning pairwise preferences.
Artiﬁcial Intelligence, 172(16-17):1897–1916, Nov. 2008.
[14] T. Joachims. Optimizing search engines using clickthrough data. In Proc. of Know. Disc. and Dat. Mining
(SIGKDD), pages 133–142, 2002.
[15] W. Kotlowski, K. Dembczynski, and E. Huellermeier. Bipartite ranking through minimization of univari-
ate loss. In Proc. of the Intl. Conf. on Mach. Learn., pages 1113–1120, 2011.
[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval,
3:225–331, March 2009.
[17] P. D. Ravikumar, A. Tewari, and E. Yang. On ndcg consistency of listwise ranking methods. J. of Mach.
Learn. Res. - Proc. Track, 15:618–626, 2011.
[18] M. D. Reid and R. C. Williamson. Surrogate Regret Bounds for Proper Losses. In Proc. of the Intl. Conf.
on Mach. Learn., pages 897–904, 2009.
[19] C. Scott. Surrogate losses and regret bounds for cost-sensitive classiﬁcation with example-dependent
costs. Proc. of the Intl. Conf. on Mach. Learn., pages 153–160, 2011.
[20] I. Steinwart. How to compare different loss functions and their risks. Constructive Approximation,
26(2):225–287, 2007.
[21] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In
Proceedings of the international conference on Web search and web data mining, WSDM ’08, pages
77–86, 2008.
[22] E. Voorhees, D. Harman, N. I. of Standards, and T. (U.S.). TREC: experiment and evaluation in informa-
tion retrieval. Digital libraries and electronic publishing. MIT Press, 2005.
[23] R. A. Wijsman. Continuity of the bayes risk. The Annals of Math. Stat., 41(3):pp. 1083–1085, 1970.
[24] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. Directly optimizing evaluation measures in learning to rank.
In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in
information retrieval, SIGIR ’08, pages 107–114, 2008.
[25] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average preci-
sion. In Proc. of the ACM SIGIR Intl. Conf. on Res. and Dev. in Info. Retr., pages 271–278, 2007.
[26] T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods. J. of Mach.
Learn. Res., 5:1225–1251, 2004.
[27] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimiza-
tion. The Annals of Stat., 32(1):pp. 56–85, 2004.

9


Smooth-projected Neighborhood Pursuit for
High-dimensional Nonparanormal Graph Estimation

Tuo Zhao
Department of Computer Science
Johns Hopkins University

Kathryn Roeder
Department of Statistics
Carnegie Mellon University

Han Liu
Department of Operations Research and Financial Engineering
Princeton University

Abstract

We introduce a new learning algorithm, named smooth-projected neighborhood
pursuit, for estimating high dimensional undirected graphs. In particularly, we
focus on the nonparanormal graphical model and provide theoretical guarantees
for graph estimation consistency. In addition to new computational and theoretical
analysis, we also provide an alternative view to analyze the tradeoff between com-
putational efﬁciency and statistical error under a smoothing optimization frame-
work. Numerical results on both synthetic and real datasets are provided to support
our theory.

1
Introduction
We consider the undirected graph estimation problem for a d-dimensional random vector X =
(X1 , ..., Xd )T (Lauritzen, 1996; Wille et al., 2004; Blei and Lafferty, 2007; Honorio et al., 2009).
More speciﬁcally, let V be the set that contains nodes representing the d variables in X , and E be the
set that contains edges representing the conditional independence relationship among X1 , ..., Xd , we
say that the distribution of X is Markov to G = (V , E ) if Xi is independent of Xj given X\{i,j} for
all (i, j ) /∈ E , where X\{i,j} = {Xk : k (cid:54)= i, j }. Our goal is to recover G based on n independent
observations of X .
Most existing methods for high dimensional graph estimation assume that the random vector X
follows a Gaussian distribution, i.e., X ∼ N (µ, Σ). Under this parametric assumption, the graph
estimation problem can be solved by estimating the sparsity pattern of the precision matrix Ω =
Σ−1 , i.e., the nodes i and j are connected if and only if Ωij (cid:54)= 0. The problem of estimating the
sparsity pattern of Ω is also called covariance selection in Dempster (1972). There are two major
approaches for learning high dimensional Gaussian graphical models: (i) graphical lasso (Yuan and
Lin, 2007; Friedman et al., 2007; Banerjee et al., 2008) and (ii) neighborhood pursuit (Meinshausen
and B ¨uhlmann, 2006). The graphical lasso maximizes the (cid:96)1 -penalized Gaussian likelihood and
simultaneously estimates the precision matrix Ω and graph G . In contrast, the neighborhood pursuit
method maximizes the (cid:96)1 -penalized pseudo-likelihood and can only estimate the graph G . Scalable
software packages such as glasso and huge have been developed to implement these algorithms
(Friedman et al., 2007; Zhao et al., 2012). Theoretically, both methods are consistent in graph
recovery for Gaussian models under certain regularity conditions. However, Ravikumar et al. (2011)
suspect that the neighborhood pursuit approach has a better sample complexity in graph recovery
than the graphical lasso. Moreover, these two methods are often observed to behave differently on
real datasets in practical applications.
In Liu et al. (2009), an semiparametric nonparanormal model is proposed to relax the restrictive nor-
mality assumption. More speciﬁcally, they assume that there exists a set of strictly monotone trans-

1

j=1 , such that the transformed random vector f (X ) = (f1 (X1 ), . . . , fd (Xd ))T
formations f = (fj )d
follows a Gaussian distribution, i.e., f (X ) ∼ N (0, Ω−1 ). Liu et al. (2009) show that for the non-
paranormal distribution, the graph G can also be estimated by examining the sparsity pattern of Ω.
Different methods have been proposed to infer the nonparanormal model in high dimensions. In Liu
et al. (2012), a rank-based estimator named nonparanormal SKE PT IC is proposed to directly esti-
mate Ω. Their main idea is to calculate a rank-correlation matrix (either based on the Spearman’s
rho or Kendall’s tau correlation) and plug the estimated correlation matrix into the graphical lasso to
estimate Ω and graph G . Such a procedure has been proven to be robust and achieve the same para-
metric rates of convergence as the graphical lasso (Liu et al., 2012). However, how to combine the
nonparanormal SKE PT IC estimator with the neighborhood pursuit approach is still an open problem.
The main challenge is that the possible indeﬁniteness of the rank-based correlation matrix estimates
could lead to a non-convex computational formulation. Such potential non-convexity challenges
both computational and theoretical analysis.
In this paper, we bridge this gap by proposing a novel smooth-projected neighborhood pursuit
method. The main idea is to project the possibly indeﬁnite nonparanormal SK E PT IC correlation
matrix estimator into the cone of all positive semi-deﬁnite matrices with respect to a smoothed el-
ementwise (cid:96)∞ -norm. Such a projection step is closely related to the dual smoothing approach in
Nesterov (2005). We provide both computational and theoretical analysis of the derived algorithm.
√
Computationally, our proposed smoothed elementwise (cid:96)∞ -norm has nice structure so that we can
develop an efﬁcient fast proximal gradient solver with a provable convergence rate O(1/
) ( is
the desired accuracy of the objective value, Nesterov (1988)). Theoretically, we provide sufﬁcient
conditions to guarantee that the proposed smooth-projected neighborhood pursuit approach is graph
estimation consistent.
In addition to new computational and statistical analysis, we further provide an alternative view
to analyze the fundamental tradeoff between computational efﬁciency and statistical error under the
smoothing optimization framework. Existing literature (Nesterov, 2005; Chen et al., 2012) considers
the dual smoothing approach as a tradeoff between computational efﬁciency and approximation
√
error. To avoid a large approximation error, they need to restrict the smoothness and obtain a slower
)). However, we directly consider the statistical error introduced by the
rate (O(1/) vs. O(1/
smoothing approach, and show that the obtained estimator preserves the good statistical properties
without losing the computational efﬁciency. Thus we get the good sides of both worlds.
The rest of this paper is organized as follows: The next section reviews the nonparanormal SK E PT IC
in Liu et al. (2012); Section 3 introduces the smooth-projected neighborhood pursuit and derives
the fast proximal gradient algorithm; Section 4 explores the statistical properties of the procedure;
Section 5 and 6 present results on on both simulated and real datasets. Due to the space limit,
most of technical details are put in a signiﬁcantly extended version of this paper (Zhao et al., 2013).
In addition, Zhao et al. (2013) also contains more thorough numerical experiments and detailed
comparison with other competitors.
2 Background
2 = (cid:80)
||v ||1 = (cid:80)
We ﬁrst introduce some notation. Let v = (v1 , . . . , vd )T ∈ Rd , we deﬁne the vector norms:
j , and ||v ||∞ = maxj |vi |. Let A = [Ajk ] ∈ Rd×d and
j |vj |,
||v ||2
(cid:80)
(cid:80)
j v2
B = [Bjk ] ∈ Rd×d be two symmetric matrices, we deﬁne the matrix operator norms: ||A||1 =
|||A|||1 = (cid:80)
F = (cid:80)
k |Ajk |, ||A||2 = max||v ||2=1 ||Av ||2 and elementwise norms
j |Ajk |, ||A||∞ = maxj
maxk
j,k |Ajk |, |||A|||∞ = maxj,k |Ajk |, ||A||2
j,k |Ajk |2 . We denote Λmin (A) and
(cid:10)A, B(cid:11) = tr(AT B), where tr(·) is the trace operator.
Λmax (A) as the smallest and largest eigenvalues of A. The inner product of A and B is denoted by
We denote the subvector of v with the j th entry removed by v\j = (v1 , . . . , vj−1 , vj+1 , . . . , vd )T ∈
Rd−1 . In a similar notion, we denote the ith row of A with its j th entry removed by Ai,\j . If I is a
set of indices, then the sub-matrix of A with both column and row indices in I is denoted by AI I .
We then introduce the nonparanormal graphical model. The nonparanormal (nonparametric normal)
distribution was initially motivated by the sparse additive models (Ravikumar et al., 2009). It aims
at separately modeling the marginal distribution and conditional independence structure. The formal
deﬁnition is as follows,

2

Deﬁnition 2.1 (Nonparanormal Distribution Liu et al. (2009)). Let f = {f1 , ..., fd } be a collection
of non-decreasing univariate functions and Σ∗ ∈ Rd×d be a correlation matrix with diag(Σ∗ ) = 1.
We say a d-dimensional random variable X = (X1 , ..., Xd )T follows a nonparanormal distribution,
denoted by X ∼ N P Nd (f , Σ∗ ), if
f (X ) = (f1 (X1 ), ..., fd (Xd ))T ∼ N (0, Σ∗ ).

(2.1)

(2.2)

(2.3)

,

,

The nonparanormal family is equivalent to the Gaussian copula family for continuous distributions
(Klaassen and Wellner, 1997; Tsukahara, 2005; Liu et al., 2009). Similar to the Gaussian graphical
model, the nonparanormal graphical model also encodes the conditional independence graph by the
sparsity pattern of the precision matrix Ω∗ = (Σ∗ )−1 . More details can be found in (Liu et al.,
2009).
Recently, Liu et al. (2012) propose a rank-based procedure, named nonparanormal SKE PT IC, for
learning nonparanromal graphical models. More speciﬁcally, let x1 , ..., xn with xi = (xi
1 , ..., xi
d )T
(cid:80)n
be n independent observations of X , we deﬁne the Spearman’s rho and Kendall’s tau correlation
(cid:113)(cid:80)n
Spearman’s rho : (cid:98)ρjk =
j − ¯rj )2 · (cid:80)n
coefﬁcients as
j − ¯rj )(ri
k − ¯rk )
i=1 (ri
(cid:17)
(cid:16)
(cid:17) (cid:16)
(cid:88)
k − ¯rk )2
i=1 (ri
i=1 (ri
: (cid:98)τjk =
k − xi(cid:48)
j − xi(cid:48)
2
Kendall’s tau
xi
xi
(cid:80)n
n(n − 1)
sign
j
k
i<i(cid:48)
transformations. The nonparanormal SK E PT IC estimators are deﬁned as (cid:98)Sρ = [(cid:98)Sρ
where ri
j and ¯rj = 1
j among x1
j denotes the rank of xi
j = (n + 1)/2. Both the
j , . . . , xn
i=1 ri
n
Spearman’s rho and Kendall’s tau correlations are rank-based and invariant to univariate monotone
(cid:98)Sτ = [(cid:98)Sτ
jk ] ∈ Rd×d and
(cid:17)
(cid:16) π
(cid:17)
(cid:16) π
jk ] ∈ Rd×d calculated from
and (cid:98)Sτ
(cid:98)Sρ
6 (cid:98)ρjk
2 (cid:98)τjk
(2.4)
Here the sin(·) transformations correct the population bias. (cid:98)Sρ and (cid:98)Sτ avoid explicitly calculating
.
jk = sin
jk = 2 sin
ric rates of convergence (Liu et al., 2012). Since Liu et al. (2012) suggest that (cid:98)Sρ and (cid:98)Sτ have very
the marginal transformation functions {fj }d
j=1 and has been shown to achieve the optimal paramet-
similar performance, for notational simplicity, we simply omit the superscript (τ and ρ), and use (cid:98)S
instead. Theoretically, Liu et al. (2012) establish the following concentration bound of the nonpara-
normal SKE PT IC estimator, which is a sufﬁcient condition to achieve graph estimation consistency
in high dimensions.
mator (cid:98)S, for large enough n, we have (cid:98)S satisfying
Lemma 2.2 (Nonparanormal SK E PT IC, Liu et al. (2012)). Given the nonparanormal SK E PT IC esti-
P (cid:16)|||(cid:98)S − Σ∗ |||∞ ≤ 8πϕ
(cid:17) ≥ 1 − d2 exp(−nϕ2 ).
In the next section we will introduce our new smooth-projected neighborhood pursuit method and
show that it also admits a similar concentration bound.
3 Smooth-Projected Neighborhood Pursuit
Similar to the neighborhood pursuit, our smooth-projected neighborhood pursuit also solves a col-
(cid:98)B\j,j = argmin
BT\j,j (cid:101)S\j,\j B\j,j − 2(cid:101)ST\j,j B\j,j + λ(cid:107)B\j,j (cid:107)1 for all j = 1, ..., d,
lection of (cid:96)1 -penalized least square problems as follows,
(3.1)
where (cid:101)S is a positive semi-deﬁnite replacement of the nonparanormal SK E PT IC estimator (cid:98)S. (3.1)
Bj,j =0
can be efﬁciently solved by existing solvers such as the coordinate descent algorithm (Friedman
of vertices, that are not, then we obtain (cid:98)Ij = {k : (cid:98)Bjk (cid:54)= 0} and (cid:98)Jj = {k : (cid:98)Bjk = 0}. Thus we can
et al., 2007). Let Ij denote a set of vertices, that are the neighbors of of node j , and Jj denote a set
eventually get the graph estimator (cid:98)G by combining all (cid:98)Ij ’s.
3

(2.5)

3.1 Smoothed Elementwise (cid:96)∞ -norm

(3.2)

|||U|||2
F ,

|||(cid:98)S − S|||∞ s.t. S (cid:23) 0.
Our proposed method starts with the following projection problem,
S = argmin
S
From the triangle inequality and the fact that Σ∗ is a feasible solution to (3.2), we have
|||Σ∗ − (cid:98)S + (cid:98)S − S|||∞ ≤ |||(cid:98)S − S|||∞ + |||(cid:98)S − Σ∗ |||∞ ≤ 2|||(cid:98)S − Σ∗ |||∞ .
(3.3)
Then by combining Lemma 2.2 and (3.3), we can show that S concentrates to Σ∗ with a rate similar
to Lemma 2.2. However, (3.2) is computationally expensive due to the non-smooth elementwise
(cid:96)∞ -norm. To overcome this challenge, we apply the dual smoothing approach in Nesterov (2005)
to efﬁciently solve (3.2) with a controllable loss in accuracy. More speciﬁcally, for any matrix
A ∈ Rd×d , we exploit the Fenchel’s dual representation of the elementwise (cid:96)∞ -norm to obtain its
(cid:10)U, A(cid:11) − µ
smooth surrogate as follows,
|||A|||µ∞ = max
|||U|||1≤1
2
where µ > 0 is the smoothing parameter, and the second term is the proximity function of U. We
call |||A|||µ∞ smoothed elementwise (cid:96)∞ -norm. A closed form solution to (3.4) is characterized in the
Lemma 3.1. Equation (3.4) has a closed form solution, (cid:101)U with
following lemma.
(cid:26)(cid:12)(cid:12)(cid:12)(cid:12) Ajk
(cid:12)(cid:12)(cid:12)(cid:12) − γ , 0
(cid:27)
(cid:101)Ujk = sign (Ajk ) · max
where γ is the minimum non-negative constant such that ||| (cid:101)U|||1 ≤ 1.
µ
By utilizing a suitable pivotal quantity, we can efﬁciently obtain γ with the expected computational
elementwise (cid:96)∞ -norm is a smooth convex function. Let A = (cid:98)S − S, and we can evaluate its gradient
complexity O(d2 ). More details of the algorithm can be found in Zhao et al. (2013). The smoothed
∂ |||(cid:98)S − S|||µ∞
· ∂ ((cid:98)S − S)
using (3.5) as follows,
∇|||(cid:98)S − S|||µ∞ =
= − (cid:101)U.
∂ ((cid:98)S − S)
(3.6)
Since (cid:101)U is essentially a soft thresholding function, therefore it is continuous in S with the Lip-
∂S
chitz constant µ−1 . In the next section, we will show that by considering the following alternative
|||(cid:98)S − S|||µ∞ s.t. S (cid:23) 0,
(cid:101)S = argmin
optimization problem
S
we can also obtain a good correlation estimator without losing computational efﬁciency.
3.2 Fast Proximal Gradient Algorithm
Equation (3.7) has a minimum eigenvalue constraint, regarding which, we exploit Nesterov (1988)
and derive the following fast proximal gradient algorithm. The main idea is to utilize the gradients in
previous iterations to help ﬁnd the descent direction for the current iteration, and eventually achieves
a faster convergence rate than the ordinary projected gradient algorithm. In this algorithm, we need
two sequences of auxiliary variables M(t) and W(t) with M(0) = W(0) = S(0) , and a sequence of
weights θt = 2/(1 + t) where t = 0, 1, 2, ....
Before we proceed with the proposed algorithm, we describe Lemma 3.2, which can solve the fol-
lowing projection problem

,

(3.5)

(3.4)

(3.7)

Π+ (A) = argmin
B
where A ∈ Rd×d is a symmetric matrix.

||B − A||2
F s.t. B (cid:23) 0,

(3.8)

4

t =

.

(3.9)

(3.11)

G(t)

,

(3.12)

(3.10)

Lemma 3.2. Suppose A has the eigenvalue decomposition as A = (cid:80)d
the eigenvalues and vj ’s are corresponding eigenvectors. Let (cid:101)σj = max{σj , 0} for j = 1, ..., d,
then we have Π+ (A) = (cid:80)d
j , where σj ’s are
j=1 σj vj vT
j=1 (cid:101)σj vj vT
j .
Now we start with the t-th iteration. We ﬁrst calculate the auxiliary variable M(t) as
M(t) = (1 − θt )S(t−1) + θtW(t−1) .
∂ |||(cid:98)S − M(t) |||µ∞
We then evaluate the gradient according to (3.6),
G(t) =
.
∂M(t)
Q(W, W(t−1) , µ) = |||(cid:98)S − W(t−1) |||µ∞ + (cid:10)G(t) , W − W(t−1) (cid:11) +
We consider the following quadratic approximation
||W − W(t) ||2
1
F .
(cid:18)
(cid:19)
2µθt
By simple manipulations and Lemma 3.2, the fast proximal gradient algorithm takes
W(t−1) − µ
Q(W, W(t−1) , µ) = Π+
W(t) = argmin
W(cid:23)0
θt
where µ works as a step-size here. We further calculate S(t) for the t-th iteration as follow,
Theorem 3.3. Given the desired accuracy  such that |||(cid:98)S − S(t) |||µ∞ − |||(cid:98)S − (cid:101)S|||µ∞ < , we need the
S(t) = (1 − θt )S(t−1) + θtW(t) .
(3.13)
(cid:113)
(cid:16)(cid:112)1/(µ)
(cid:17)
2||S(0) − (cid:101)S||2
number of iterations to be at most
F/(µ) − 1 = O
(3.14)
The detailed proof can be found in the extended draft Zhao et al. (2013) due to the space limit.
Theorem 3.3 guarantees that our derived algorithm achieves the optimal rate of convergence for
show that choosing a suitable smooth parameter µ allows (cid:101)S concentrate to Σ∗ with a rate similar
minimizing (3.7) over the class of all gradient-based computational algorithms. In next section, by
directly analyzing the tradeoff between the computational efﬁciency and statistical error, we will
to Lemma 2.2 in high dimensions, though (3.7) is not the same as the original projection problem
(3.2).
4 Statistical Properties
theorem establishes the concentration property of (cid:101)S under the elementwise (cid:96)∞ norm. This result
In this section we present the statistical properties of the proposed method. Due to space limit, all
the proofs of following theorems can be found in the extended draft Zhao et al. (2013). The next
Theorem 4.1. Given the nonparanormal SKE PT IC estimator (cid:98)S, for any large enough n, under the
will be useful to prove later main theorem.
conditions that µ ≤ 4πϕ and ϕ > 0, we have the optimum to (3.7), denoted as (cid:101)S, satisfying
P (cid:16)|||(cid:101)S − Σ∗ |||∞ ≤ 18πϕ
(cid:17) ≥ 1 − d2 exp(−nϕ2 ).
(4.1)
Theorem 4.1 is non-asymptotic. It implies that we can gain the computational efﬁciency without
losing statistical rate in terms of elementwise sup-norm as long as µ is reasonably large. We now
show that our proposed smooth-projected neighborhood approach recovers the true neighborhood
for each node with high probability under the following irrepresentable condition (Zhao and Yu,
2006; Zou, 2006; Wainwright, 2009).
Assumption 1 (Irrepresentable Condition). Recall that Ij and Jj denote the true neighborhood and
non-neighborhood of node j respectively. There exist α ∈ (0, 1), δmin > 0 and δ−1 ≤ ψ < ∞ such
that for ∀j = 1, .., d, the following conditions hold,
(C.1) ||Σ∗
)−1 ||∞ ≤ α;
(Σ∗
Ij Ij
Jj Ij
) ≥ δ, (cid:107)(Σ∗
(C.2) Λmin (Σ∗
Ij Ij
Ij Ij

)−1(cid:107)∞ ≤ ψ .

(4.2)
(4.3)

5

The proposed projection approach can be also combined with other graph estimation method such
as Zhou et al. (2009), in which the conditions above can be relaxed. Here we use this condition for
an illustrative purpose to show that the proposed method has a theoretical guarantee.
Theorem 4.2 (Graph Recovery Performance). Let τ = min |B∗
jk | for all (j, k)’s such that G ∗
jk (cid:54)= 0,
where B∗ ∈ Rd×d with B∗
\j,j = (Σ∗
\j,\j )−1Σ∗
\j,j and B∗
j,j = 0. We assume that Σ∗ satisﬁes
Conditions C.1 and C.2. Let sj = |Ij | < n and choose the λ such that λ ≤ min {τ /ψ , 2}, then
(cid:32)
(cid:33)
(cid:33)
(cid:32) −c1nδ2
(cid:17) ≥ 1 − s2
P (cid:16) (cid:98)Jj = Jj , (cid:98)Ij = Ij
there exist positive universal constants c0 and c1 , such that
− c1nϕ2
− s2
(cid:32)
(cid:33)
j exp
j exp
4s2
s2
j
j
− c1nϕ2
− (d − sj )sj exp
(cid:113) log d
(cid:110)
s2
j
26ψ , λ(1−α)
2ψ , λ(1−α)
n ≤ ϕ ≤ min
1, 1
26(α+1) ,
Theorem 4.2 is also non-asymptotic. It guarantees that for each individual node, we can correctly re-
cover its neighborhood with high probability. Consequently, the following corollary can be implied
so that we can asymptotically recover the underlying graph structure under given conditions.
P( (cid:98)G = G ) → 1 if the following conditions hold:
Corollary 4.3. Let s = max1≤j≤d sj , then under the same conditions as in Theorem 4.2, we have
(C.3) α, δ and ψ are constants, which do not scale with the triplet (n, d, s);
(C.4) The triplet (n, d, s) scales as s2 (log d + log s)/n → 0 and s2 log d/(τ 2n) → 0;
(C.5) λ scales with (n, d, s) as λ/τ → 0 and s2 log d/(λ2n) → 0.

− d exp(−c1nϕ2 ),
(cid:111)
.

(4.4)

where ϕ satisﬁes that c0

14ψ2 , τ
τ
14ψ

5 Numerical Simulations
Liu et al. (2012) recommend to use the Kendall’s tau for nonparanormal graph estimation because
of its superior robustness property compared to the Spearman’s rho. In this section, we use the
Kendall’s tau in our smooth-projected neighborhood pursuit method. For synthetic data, we use
the following four different graphs with 200 nodes (d = 200): (1) Erd ¨os-R ´enyi graph; (ii) Cluster
graph; (iii) Chain graph; (4) Scale-free graph. We simulate data from the Gaussian distributions that
Markov to the above graphs. We adopt the power function g(t) = sign(t)|t|4 to convert the Gaussian
data to the nonparanormal data. More details about the data simulation can be found in Zhao et al.
(2013). We use the ROC curve to evaluate the graph estimation performance. Since d > n, the full
solution paths cannot be obtained, therefore we restrict the range of false positive edge discovery
rates to be from 0 to 0.3 for computational convenience.
5.1 Our proposed method vs. Nonparanormal SKEPTIC Estimator
We ﬁrst evaluate the proposed smoothed elementwise (cid:96)∞ -norm projection algorithm. For this, we
sampled 100 data points from a 200-dimensional standard normal distribution N (0, I200 ). We study
the empirical performance of the proposed fast proximal gradient algorithm using different smooth-
the original objective value |||(cid:98)S − S(t) |||∞ v.s.
ing parameters (µ = 1, 0.5, 0.25, 0.1). The optimization and statistical error curves for different
smoothing parameters (averaged over 50 replications) are presented in Figure 1. Figure 1(a) shows
the number of iterations. Compared with smaller
µ’s, we see that choosing µ = 1 reduces the computational burden but increases the approxima-
tion error w.r.t the problem in (3.2). However, Figure 1(b) shows that, in terms of the statistical
error |||Σ∗ − S(t) |||∞ , µ = 1 performs similarly to the other smaller µ’s. Therefore, we show that
signiﬁcant computational efﬁciency can be gained with little loss of statistical error.
We further compare the graph recovery performance of our proposed method with the naive in-
deﬁnite nonparanormal SKE PT IC estimator as in Liu et al. (2012). The averaged ROC curves over
100 replications are presented in Figure 2. We see that directly plugging the indeﬁnite nonpara-
normal SKE PT IC estimator into the neighborhood pursuit results in the worst performance. The
ROC performance drops dramatically due to the non-convexity of the objective function. While

6

(a) |||(cid:98)S − S(t) |||∞
Figure 1: The empirical performance using different smoothing parameters. µ = 1 has a similar
performance to the smaller µ’s in terms of the estimation error.

(b) |||Σ∗ − S(t) |||∞

our smoothed-projected neighborhood pursuit method signiﬁcantly outperforms the naive indeﬁnite
nonparanormal SK E PT IC estimator.

(a) Erd ¨os-R ´enyi

(b) Cluster

(c) Chain

(d) Scale-free

Figure 2: The averaged ROC curves of the neighborhood pursuit when combined with different
correlation estimators. “SKEPTIC” represents the indeﬁnite nonparanormal SK E PT IC estimator,
and “Projection” represents our proposed projection approach.
5.2 Our Proposed Method vs. Naive Neighborhood Pursuit
In this subsection, we conduct similar numerical studies as in Liu et al. (2012) to compare our pro-
posed method with the naive neighborhood pursuit method. The naive neighborhood pursuit directly
exploits the Pearson correlation estimator under the neighborhood pursuit framework. Choosing
n = 100 and d = 200, we use the same experimental setup as in the previous subsection. The
averaged ROC curves over 100 replications are presented in Figure 3. As can be seen, our proposed
projection method outperforms the naive neighborhood pursuit throughout all four different graphs.

(a) Erd ¨os-R ´enyi

(b) Cluster

(c) Chain

(d) Scale-free

Figure 3: The averaged ROC curves of the neighborhood pursuit when combined with different
correlation estimators. “SNP” represents our proposed estimator and “NNP” represents the Pearson
estimator. The SNP uniformly outperforms the NNP for all four graphs.

6 Real Data Analysis
In this section we present a real data experiment to compare the nonparanormal graphical model to
Gaussian graphical model. For model selection, we use the stability graph procedure (Meinshausen

7

010203040500.0100.0150.0200.0250.0300.035# of IterationsObjective Valuesµ=1µ=0.5µ=0.25µ=0.1010203040500.4360.4380.4400.4420.444# of IterationsStatistical Errorµ=1µ=0.5µ=0.25µ=0.10.00.10.20.30.40.40.50.60.70.80.91.0False Positive RateTrue Positive RateSKEPTICProjection0.00.10.20.30.40.30.40.50.60.70.80.9False Positive RateTrue Positive RateSKEPTICProjection0.00.10.20.30.40.20.40.60.81.0False Positive RateTrue Positive Rate0.00.10.20.30.40.20.30.40.50.60.70.8False Positive RateTrue Positive RateSKEPTICProjection0.000.050.100.150.200.250.300.00.20.40.60.81.0False Positive RateTrue Positive RateNNPSNP0.000.050.100.150.200.250.300.00.20.40.60.81.0False Positive RateTrue Positive RateNNPSNP0.000.050.100.150.200.250.300.00.20.40.60.81.0False Positive RateTrue Positive RateNNPSNP0.000.050.100.150.200.250.300.00.20.40.60.81.0False Positive RateTrue Positive RateNNPSNPand B ¨uhlmann, 2010; Liu et al., 2010), which has the following steps: (1) Calculate the solution
path using all the samples, and choose the regularization parameter at the sparsity level 4%; (2)
Randomly choose 10% of all the samples without replacement using the regularization parameter
chosen in (1); (3) Repeat the step (2) 500 times and retain the edges that appear with frequencies no
less than 95%.
The topic graph is ﬁrst used in Blei and Lafferty (2007) to illustrate the idea of correlated topic
modeling. The correlated topic model, is a hierarchical Bayesian model for abstracting K “topics”
that occur in a collection of documents (corpus). By applying the variational EM-algorithm, we can
estimate the topic proportion for each document and represent it in a K -dimensional simplex (mixed-
membership). Blei and Lafferty (2007) assume that the topic proportion approximately follows a
normal distribution after the logarithmic-transformation. Here we are interested in visualizing the
relationship among the topics using an undirected graph:
the nodes represent individual topics,
and edges connecting different nodes represent highly related topics. The corpus used in Blei and
Lafferty (2007) contains 16,351 documents with 19,088 unique terms. Similar to Blei and Lafferty
(2007), we choose K = 100 and ﬁt a topic model to the articles published in Science from 1990 to
1999.
Evaluated by the Kolmogorov-Smirnov test, we ﬁnd many topic data highly violate the normal-
ity assumption (More details can be found in Zhao et al. (2013)). This motivates our choice of
the smooth-projected neighborhood pursuit approach. The estimated topic graphs are provided in
Figure 4. The smooth-projected neighborhood pursuit generates 6 mid-size modules and 6 small
modules, while the naive neighborhood pursuit generated 1 large module, 2 mid-size modules and
6 small modules. The nonparanormal approach discovers more reﬁned structures and improves the
interpretability of the obtained graph. For example: (1) Topics closely related to climate change in
Antarctica are clustered in the same module such as “ice-68”, “ozone-23” and “carbon-64”; (2) Top-
ics closely related to environmental ecology are clustered in the same module such as “monkey-21”,
“science-4”, “environmental-67”, “species-86”, etc.; (3) Topics closely related to modern physics
are clustered in the same module such as “quantum-29”, “magnetic-55”, ”pressure-92”, “solar-62”,
etc.. In contrast, the naive neighborhood pursuit mixes all these different topics in a large module.

(a) Our Proposed Method

(b) Naive Neighborhood Pursuit

Figure 4: Two topic graphs illustrating the difference of the estimated topic graphs. The smooth-
projected neighborhood pursuit (subﬁgure (a)) generates 6 mid-size modules and 6 small modules
while the naive neighborhood pursuit (subﬁgure (b)) generates 1 large module, 2 mid-size modules
and 6 small modules.
7 Conclusion and Acknowledgement
In this paper, we study how to estimate the nonparanormal graph using the neighborhood pursuit
in conjunction with the possible indeﬁnite nonparanormal skeptic estimator. Using our proposed
smoothed projection approach, the resulting estimator can be used as a positive semi-deﬁnite reﬁne-
ment of the nonparanormal skeptic estimator. Our estimator has better graph estimation performance
with theoretical guarantee. Our results suggest that it is possible to gain estimation robustness and
modeling ﬂexibility without losing two important computational structures: convexity and smooth-
ness. The topic modeling experiment demonstrates that our proposed method may lead to more
reﬁned scientiﬁc discovery. Han Liu and Tuo Zhao are supported by NSF award IIS-11167308, and
Kathryn Roeder is supported by National Institute of Mental Health grant MH057881.

8

References
BANER J EE , O ., GHAOU I , L . E . and D ’A S PR EMON T, A . (2008). Model selection through sparse maximum
likelihood estimation. Journal of Machine Learning Research 9 485–516.
B LE I , D . and LA FFERTY, J . (2007). A correlated topic model of science. Annals of Applied Statistics 1 17–35.
CH EN , X ., L IN , Q ., K IM , S ., CARBON EL L , J . and X ING , E . (2012). A smoothing proximal gradient method
for general structured sparse regression. Annals of Applied Statistics To appear.
D EM P S TER , A . (1972). Covariance selection. Biometrics 28 157–175.
FR I EDMAN , J ., T. HA ST I E , H . H . and T IB SH IRAN I , R . (2007). Pathwise coordinate optimization. Annals of
Applied Statistics 1 302–332.
HONOR IO , J ., ORT I Z , L ., SAMARA S , D ., PARAG IO S , N ., and GO LD ST E IN , R . (2009). Sparse and locally
constant gaussian graphical models. Advances in Neural Information Processing Systems 745–753.
K LAA S S EN , C . and W E LLN ER , J . (1997). Efﬁcient estimation in the bivariate normal copula model: Normal
margins are least-favorable. Bernoulli 3 55–77.
LAUR I T ZEN , S . (1996). Graphical models, vol. 17. Oxford University Press, USA.
L IU , H ., HAN , F., YUAN , M ., LA FFERTY, J . and WA S S ERMAN , L . (2012). High dimensional semiparametric
gaussian copula graphical models. Annals of Statistics To appear.
L IU , H ., LA FFERTY, J . and WA S S ERMAN , L . (2009). The nonparanormal: Semiparametric estimation of high
dimensional undirected graphs. Journal of Machine Learning Research 10 2295–2328.
L IU , H ., RO ED ER , K . and WA S S ERMAN , L . (2010). Stability approach to regularization selection for high
dimensional graphical models. Advances in Neural Information Processing Systems .
M E IN SHAU SEN , N . and B ¨UH LMANN , P. (2006). High dimensional graphs and variable selection with the
lasso. Annals of Statistics 34 1436–1462.
M E IN SHAU SEN , N . and B ¨UHLMANN , P. (2010). Stability selection. Journal of the Royal Statistical Society,
Series B 72 417–473.
N E S TEROV, Y. (1988). On an approach to the construction of optimal methods of smooth convex functions.
´Ekonom. i. Mat. Metody 24 509–517.
N E S TEROV, Y. (2005). Smooth minimization of non-smooth functions. Mathematical Programming 103 127–
152.
RAV IKUMAR , P., LA FFERTY, J ., L IU , H . and WA S S ERMAN , L . (2009). Sparse additive models. Journal of
the Royal Statistical Society, Series B 71 1009–1030.
RAV IKUMAR , P., WA INWR IGH T, M ., RA SKU TT I , G . and YU , B . (2011). High-dimensional covariance esti-
mation by minimizing (cid:96)1 -penalized log-determinant divergence. Electronic Journal of Statistics 5 935–980.
T SUKAHARA , H . (2005). Semiparametric estimation in copula models. Canadian Journal of Statistics 33
357–375.
WA INWR IGH T, M . (2009).
Sharp thresholds for highdimensional and noisy sparsity recovery using
(cid:96)1 constrained quadratic programming. IEEE Transactions on Information Theory 55 2183–2201.
W I LL E , A ., Z IMM ERMANN , P., VRANOVA , E ., FRHO L Z , A . , LAU LE , O . , B L EU LER , S ., H ENN IG , L .,
PR EL IC , A ., VON ROHR , P., TH I EL E , L ., Z I TZ LER , E ., GRU I S SEM , W. and B ¨UHLMANN , P. (2004). Sparse
graphical gaussian modeling of the isoprenoid gene network in arabidopsis thaliana. Genome Biology 5 R92.
YUAN , M . and L IN , Y. (2007). Model selection and estimation in the gaussian graphical model. Biometrika
94 19–35.
ZHAO , P. and YU , B . (2006). On model selection consistency of lasso. Journal of Machine Learning Research
7 2541–2563.
ZHAO , T., L IU , H ., RO ED ER , K ., LA FFERTY, J . and WA S S ERMAN , L . (2012). The huge package for high-
dimensional undirected graph estimation in r. Journal of Machine Learning Research To appear.
ZHAO , T., RO ED ER , K . and L IU , H . (2013). A smoothing projection approach for high dimensional nonpara-
normal graph estimation. Tech. rep., Johns Hopkins University.
ZHOU , S ., VAN D E G E ER , S . and B ¨UHLMANN , P. (2009). Adaptive lasso for high dimensional regression and
gaussian graphical modeling. Tech. rep., ETH Zurich.
ZOU , H . (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association
101 1418–1429.

9


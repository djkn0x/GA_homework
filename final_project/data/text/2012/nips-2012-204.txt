Persistent Homology for Learning Densities with
Bounded Support

Danica Kragic ∗
Hedvig Kjellstr ¨om
Florian T. Pokorny
Carl Henrik Ek
Computer Vision and Active Perception Lab, Centre for Autonomous Systems
School of Computer Science and Communication
KTH Royal Institute of Technology, Stockholm, Sweden
{fpokorny, chek, hedvig, danik}@csc.kth.se

Abstract

We present a novel method for learning densities with bounded support which
enables us to incorporate ‘hard’ topological constraints. In particular, we show
how emerging techniques from computational algebraic topology and the notion
of persistent homology can be combined with kernel-based methods from machine
learning for the purpose of density estimation. The proposed formalism facilitates
learning of models with bounded support in a principled way, and – by incorpo-
rating persistent homology techniques in our approach – we are able to encode
algebraic-topological constraints which are not addressed in current state of the
art probabilistic models. We study the behaviour of our method on two synthetic
examples for various sample sizes and exemplify the beneﬁts of the proposed ap-
proach on a real-world dataset by learning a motion model for a race car. We show
how to learn a model which respects the underlying topological structure of the
racetrack, constraining the trajectories of the car.

1

Introduction

Probabilistic methods based on Gaussian densities have celebrated successes throughout machine
learning. They are the crucial ingredient in Gaussian mixture models (GMM) [1], Gaussian pro-
cesses [2] and Gaussian mixture regression (GMR) [3] which have found applications in ﬁelds such
as robotics, speech recognition and computer vision [1, 4, 5] to name just a few. While Gaussian
distributions are convenient to work with for several theoretical and practical reasons (the central
limit theorem, easy computation of means and marginals, etc.) they do fall into the class of densities
on Rd for which supp f = Rd ; i.e. they assign a non-zero probability to every subset with non-zero
volume in Rd . This property of Gaussians can be problematic if an application dictates that certain
subsets of space should constitute a ‘forbidden’ region having zero probability mass. A simple ex-
ample would be a probabilistic model of admissible positions of a robot in an indoor environment,
where one wants to assign zero – rather than just ‘low’ – probability to positions corresponding to
collisions with the environment. Encoding such constraints using e.g. a Gaussian mixture model is
not natural since it assigns potentially low, but non-zero probability mass to every portion of space.
In contrast to the above Gaussian models, we consider non-parametric density estimators based on
spherical kernels with bounded support. As we shall explain, this enables us to study topological
properties of the support region Ωε for such estimators. Kernel-based density estimators are well-
established in the statistical literature [6] with the basic idea being that one should put a rescaled
version of a given model density over each observed data-point to obtain an estimate for the prob-
ability density from which the data was sampled. The choice of rescaling – or ‘bandwidth’ – ε
has been studied with respect to the standard L1 and L2 error and is still an active area of research
[7]. We focus particularly on spherical truncated Gaussian kernels here which have been some-
∗This work was supported by the EU projects FLEXBOT (FP7-ERC-279933) and TOMSY (IST-FP7-
270436) and the Swedish Foundation for Strategic Research

1

what overlooked as a tool for probabilistic modelling. An important aspect of these kernels is that
their associated conditional and marginal distributions can be computed analytically, enabling us to
efﬁciently work with them in the context of probabilistic inference.
A different interpretation of a density with support in an ε-ball can be given using the notion of
bounded noise. There, one assumes that observations are distorted by noise following a density with
bounded support (instead of e.g. Gaussian noise). Bounded noise models are used in the signal
processing community for robust ﬁltering and estimation [8, 9], but to our knowledge, we are the
ε-ball’ naturally leads one to consider the space Ωε (S ) = (cid:83)
ﬁrst to combine densities with bounded support and topology to model the underlying structure
of data. Thinking of a set of observations S = {X1 , ..., Xn} ⊂ Rn as ‘fuzzy up to noise in an
Bε (Xi ) of balls of size ε around
i
the data points. Persistent homology is a novel tool for studying topological properties of spaces
such as Ωε (S ) which has emerged from the ﬁeld of computational algebraic topology in recent
years [10, 11]. Using persistent homology, it becomes possible to study clustering, periodicity and
more generally the existence of ‘holes’ of various dimensions in Ωε (S ) for ε lying in an interval.
Starting from the basic observation that one can construct a kernel-based density estimator ˆfε whose
region of support is exactly Ωε (S ), this paper investigates the interplay between the topological
information contained in Ωε (S ) and a corresponding density estimate. Speciﬁcally, we make the
following contributions:
• Given prior topological information about supp f = Ω, we deﬁne a topologically admis-
sible bandwidth interval [εmin , εmax ] and propose and evaluate a topological bandwidth
selector εtop ∈ [εmin , εmax ].
• Given no prior topological information, we explain how persistent homology can be of use
to determine a topologically admissible bandwidth interval.
• We describe how additional constraints deﬁning a forbidden subset F ⊂ Rn of the
parameter-space can be incorporated into our topological bandwidth estimation framework.
• We provide quantitative results on synthetic data in 1D and 2D evaluating the ex-
pected L2 errors for density estimators with topologically chosen bandwidth values ε ∈
{εmin , εmid , εmax , εtop }. We carry out this evaluation for various spherical kernels and
compare our results to an asymptotically optimal bandwidth choice.
• We use our method in a learning by demonstration [12] context and compare our results
with a current state of the art Gaussian mixture regression method.
2 Background
2.1 Kernel-based density estimation
Let S = {X1 , ..., Xn} ⊂ Rd be an i.i.d. sample arising from a probability density f : Rd → R.
(cid:1), where the kernel function K : Rd → R
i=1 K (cid:0) x−Xi
(cid:80)n
Kernel-based density estimation [13, 14, 15] is an approach for reconstructing f from the sample
by means of an estimator ˆfε,n (x) = 1
nεd
ε
is a suitably chosen probability density. In this context, ε > 0 is called the bandwidth. If one is
only interested in an estimator that minimizes the expected L2 norm of ˆfε,n − f , the choice of ε is
crucial, while the particular choice of kernel K is generally less important [7, 6]. Let {εn}∞
n=1 be
a sequence of positive bandwidth values depending on the sample size n. It follows from classical
results [14, 15] that for any sufﬁciently well-behaved density K , limn→∞ E[( ˆfεn ,n (x) − f (x))2 ] = 0
n = ∞. Despite this encouraging result, the question
provided that limn→∞ εn = 0 and limn→∞ nεd
minimize the Mean Integrated Squared Error, M I SE (εn ) = E (cid:104)(cid:82) ( ˆfεn ,n (x) − f (x))2dx
(cid:105)
of determining the best bandwidth for a given sample is an ongoing research topic and the interested
reader is referred to the review [7] for an in-depth discussion. One branch of methods [6] tries to
.
An asymptotic analysis reveals that, under mild conditions on K and f [6], M I SE (εn ) can be ap-
proximated asymptotically by AM I SE (εn ) as n → ∞ if limn→∞ εn = 0 and limn→∞ nεd
n = ∞.
Here, AMISE denotes the Asymptotic Mean Integrated Squared Error. If we consider only spher-
ical kernels that are symmetric functions of the norm (cid:107)x(cid:107) of their input variable x, an asymptotic
(cid:90)
(cid:90)
analysis [6] shows that, in dimension d,
1
nεd
n

{tr(Hess f (x))}2 dx,

AM I SE (εn ) =

K (x)2 dx +

µ2 (K )2

ε4
n
4

2

εamise (n) =

where µ2 (K ) = (cid:82) x2
j K (x)dx is independent of the choice of j ∈ {1, . . . , d} by the spherical
symmetry and tr(Hess f (x)) denotes the trace of the Hessian of f at x. Due to the availability of a
relatively simple explicit formula for AMISE, a large class of bandwidth selection methods attempt
to estimate and minimize AMISE instead of working with MISE directly. One ﬁnds that AMISE is
(cid:32)
(cid:33) 1
d (cid:82) K (x)2 dx
minimized for
µ2 (K )2 (cid:82) {tr(Hess f (x))}2 dx
4+d
Since f is assumed unknown in real world examples, so called plug-in methods can be used to
approximate εamise [7]. In this paper, we will work with two synthetic examples of densities for
which we can compute εamise numerically in order to benchmark our topological bandwidth se-
lection procedure. For our experiments, we choose three spherical kernels K : Rd → R that are
(cid:18)
(cid:19)−1
(cid:17)
(cid:16) d
deﬁned to be zero outside the unit ball B1 (0) and are deﬁned by Ku = Vol(B1 (0))−1 (uniform),
− (cid:107)x(cid:107)2
1 − Γ
(1 − (cid:107)x(cid:107)) (conic) and Kt (x) = (2πσ2 )− d
2 , 1
Kc (x) = d(d+1)Γ( d
2 )
(truncated
2σ2
e
2σ2
2
d
Γ( d
2 )
Gaussian) respectively for (cid:107)x(cid:107) (cid:54) 1. These kernels can be deﬁned in any dimension d > 0 and are
2π
2
spherical, i.e. they are functions of the radial distance to the origin only which enables us to efﬁ-
ciently evaluate them and to sample from the corresponding estimator ˆfε,n even when the dimension
− (cid:107)x(cid:107)2
d is very large. We will denote the standard spherical Gaussian by Ke (x) = (2πσ2 )− d
2σ2 .
2 e

1
n

.

(a) Ku

(b) Kc

(c) Kt , σ2 = 1
4

Figure 1: 1
4 ) for the indicated kernels and a corresponding estimator ˆf4,3 for three sample points.
42 K ( x

2.2 Persistent homology

Consider the point cloud S shown in Figure 2(a). For a human observer, it is noticeable that S looks
‘circular’. One can reformulate the existence of the ‘hole’ in Figure 2(a) in a mathematically precise
way using persistent homology [16] which has recently gained increasing traction as a tool for the
analysis of structure in point-cloud data [10].

(a) Ω0

(b) Ω0.25

(c) Ω0.5

(d) b0

(e) b1

Figure 2: Noisy data concentrated around a circle (a) and corresponding barcodes in dimension zero (d) and
one (e). In (b) and (c), we display Ωε for ε = 0.25, 0.5 respectively together with the corresponding Vietoris-
Rips complex V2ε which we use for approximating the topology of Ωε . While the vertical axis in the ith
barcode has no special meaning, the horizontal axis displays the ε parameter of V2ε . At any ﬁxed ε value,
the number of bars lying above and containing ε is equal to the ith Betti number of V2ε . The shaded region
highlights the ε-interval for which V2ε has one connected component (i.e. b0 (V2ε ) = 1) in (d) and for which a
single ‘circle’ (i.e. b1 (V2ε ) = 1) is detected in (e).
In the approach of [10], one starts with a subset Ω ⊂ Rd and assumes that there exists some proba-
bility density f on Rd that is concentrated near Ω. Given an i.i.d. sample S = {X1 , · · · , Xn} from
the corresponding probability distribution, one of the aims of persistent homology in this setting is
to recover some of the topological structure of Ω – the homology groups Hi (Ω, Z2 ), for i = 1, . . . , d
– from the sample S . Each Hi (Ω, Z2 ) is a vector space over Z2 and its dimension bi (Ω) is called

3

the ith Betti number. One of the properties of homology is that homology groups are invariant under
a large class of deformations (i.e. homotopies) of the underlying topological space. A popular ex-
ample of such a deformation is to consider a teacup that is continuously deformed into a doughnut.
One can think of b0 (Ω) as measuring the number of connected components while, roughly, bi (Ω),
for i > 0 describes the number i-dimensional holes of Ω. A closed curve in Rd that does not self-
intersect can for example be classiﬁed by b0 = 1 (it has one connected component) and b1 = 1
Given a discrete sample S and a distance parameter ε > 0, consider the set Ωε (S ) = (cid:83)n
(it is topologically a circle). The reader is encouraged to consult [17] for a rigorous introduction to
homotopies and related concepts.
Bε (Xi ),
for ε ∈ [0, ∞), where Bε (p) = {x ∈ Rd : (cid:107)x − p(cid:107) (cid:54) ε}. In Figure 2(b) and 2(c) this set is displayed
i=1
for increasing ε values. Ωε (S ) is a topological space and, in the case where Ω is a smooth compact
submanifold in Rd and f is in a very restrictive class of densities with support in a small tubular
neighbourhood around Ω, [18, 11] have proven results showing that Ωε (S ) is homotopy equivalent
to Ω with high probability for certain large sample sizes. The key insight of persistent homology is
that we should study not just the homology of Ωε (S ) for a ﬁxed value of ε but for all ε ∈ [0, ∞)
simultaneously. The idea is then to study how the homology groups Hi (Ωε (S ), Z2 ) change with ε
and one records the changes in Betti number using a barcode [10] (see e.g. ﬁgure 2(d) and 2(e)).
Computing the barcode corresponding to Hi (Ωε (S ), Z2 ) directly (via the ˇCech complex given by
our covering of balls Bε (X1 ), . . . , Bε (Xn ) [10]) is computationally very expensive and one hence
computes the barcode corresponding to the homology groups of the Vietoris-Rips complex V2ε (S ).
This complex is an abstract complex with vertices given by the elements of S and where we insert
a k-simplex for every set of k + 1 distinct elements of S such that any two are within distance less
than 2ε of each other (see [10]). The homology groups of V2ε (S ) are not necessarily isomorphic to
the homology groups of Ωε (S ), but can serve as an approximation due to the interleaving property
of the Vietoris-Rips and ˇCech complex, see e.g. Prop 2.6 [10]. For the computation of barcodes, we
use the javaPlex software [19]. The computed ith barcode then records the birth and death times of
topological features of V2ε in dimension i as we increase ε from zero to some maximal value M ,
where M is called the maximal ﬁltration value.

3 Our framework
Given a dataset S = {X1 , . . . , Xn} ⊂ Rd , sampled in an i.i.d. fashion from an underlying prob-
ability distribution with density f : Rd → R with bounded support Ω, we propose to recover f
using a kernel density estimator ˆfε,n in a way that respects the algebraic topology of Ω. For this, we
consider only ˆfε,n based on kernels K with supp K = B1 (0), and in particular, we experiment with
Kt , Ku and Kc . For such kernels, supp ˆfε,n = Ωε (S ) = ∪n
Bε (Xi ) whose topological features
we can approximate by computing the barcodes for V2ε .
i=1
If no prior information on the topological features of Ω is given, we can then inspect these barcodes
and search for large intervals in which the Betti numbers do not change. This approach is used in
[10], who demonstrated that topological features of data can be discovered in this way. Alternatively,
one might be given prior information on the Betti numbers (e.g. using knowledge of periodicity,
number of clusters, inequalities involving Betti numbers) that one can incorporate by searching for ε-
intervals on which such constraints are satisﬁed. Geometric constraints on the data can additionally
be incorporated by restricting to allowable ε-intervals to values for which Ωε (S ) does not contain
‘forbidden regions’. In the robotics setting, frequently encountered examples for such forbidden
regions are singular points in the joint space of a robot, or positions in space corresponding to
collisions with the environment.
Let us now assume that we are given constraints on some of the Betti numbers of Ω. For a given
sample S , we then compute the barcodes for V2ε in each dimension i ∈ {1, . . . , d} up to a large
maximal value M using javaPlex [19] and determine the set A of admissible ε values. If A is empty,
we consider the topological reconstruction to have failed. This will happen, for example, if our
assumptions about the data are incorrect, or if we do not have enough samples to reconstruct Ω.
If A is non-empty, we attempt to determine a ﬁnite union of disjoint intervals on which the Betti
numbers constraints are satisﬁed. Since, in our experiments, the interval I = [εmin (n), εmax (n)]
(determined up to some ﬁxed precision) with smallest possible εmin (n) among those coincided
with the largest such interval in most cases (indicating stable topological features), we decided to

4

investigate this I ⊂ A for further analysis. For ε ∈ [εmin (n), εmax (n)], the resulting density ˆfε,n
then has a support region Ωε (S ) with the correct Betti numbers – as approximated by V2ε . We note
the following elementary observation:
Lemma 3.1. Let d ∈ N and εmin (n), εmax (n) ∈ R for all n ∈ N. Suppose that limn→∞ εmin (n) = 0
and that there exists a, b ∈ R such that 0 < a < εmax (n) < b and 0 (cid:54) εmin (n) < εmax (n) for all
4+d satisﬁes i) εtop (1) = εmid (1) and εtop (n) ∈
n ∈ N. Then εtop (n) = εmin (n) + εmax (n)−εmin (n)
− 1
n
2
[εmin (n), εmid (n)] for all n ∈ N, where we deﬁne εmid (n) = εmax (n)+εmin (n)
ii) limn→∞ εtop (n) = 0
and iii) limn→∞ nεtop (n)d = ∞.
2

It is our intuition that, for a large class of constraints on the Betti numbers and for tame densities
f : Rd → R (such as densities concentrated on a neighbourhood of a compact submanifold of Rd
[11]), εmin (n) and εmax (n) exist for all large enough sample sizes n with high probability and
that the conditions of Lemma 3.1 are satisﬁed. In that case, Lemma 3.1 provides a motivation for
choosing {εtop (n)}∞
n=1 as a topological bandwidth selector since – while it is difﬁcult to analyse
εmin (n) asymptotically – at least the second summand of εtop (n) has the same asymptotics in
n as the optimal AMISE solution. Furthermore, this choice of bandwidth then corresponds to a
support region Ωεtop (n) (S ) with the correct Betti numbers (as approximated by the Vietoris-Rips
complex) since εtop (n) ∈ [εmin (n), εmax (n)]. Finally, ii) and iii) then imply that, point-wise,
limn→∞ E[( ˆfεtop (n),n (x) − f (x))2 ] = 0 due to the results of [14, 15].
We note here that many different methods for choosing ε(n) ∈ [εmin (n), εmax (n)] can be con-
sidered. If the topologically admissible interval [εmin (n), εmax (n)] is for example determined by
the constraint of having three connected components of supp f as in 3(a), εmax (n) will increase
if we shift the connected components of supp f further apart. εtop (n) hence also increases and
might not yield good L2 error results for small sample sizes anymore.
In that case, an estima-
tor ˆεtop (n) ∈ [εmin (n), εmax (n)] closer to εmin (n) might be a better choice. To give an initial
overview, we hence also display results for εmin (n), εmid (n), εmax (n) in our experiments. Note
however also that the L2 error might not be the right quality measure for applications where the
topological features of supp f are most important – we illustrate an example of this situation in our
racetrack data experiment. We will show that – in the absence of further problem-speciﬁc knowl-
edge – εtop (n) does yields a good bandwidth estimate with respect to the L2 error in our examples.

4 Experiments
Results in 1D We consider the probability density f : R → R displayed in grey in each of the
graphs in Figure 3. To benchmark the performance of our topological bandwidth estimators, we then
compute the AMISE-optimal bandwidth parameter εamise numerically from the analytic formula for
f and for Kt , Ku , Kc and Ke . Here, we include the Gaussian kernel Ke for comparison purposes
only.

(a) fεtop ,10 using Kt .

(b) fεamise ,10 using Ke

(c) fεtop ,2500 using Kt

Figure 3: Density f (grey) and reconstructions (black) for the indicated sample size, bandwidth and kernel.

In order to topologically reconstruct f , we then assume only the knowledge of some points sampled
from f and that b0 (supp f ) = 3 and no further information about f , i.e. we assume to know a
sample and that the support region of f has three components. We then ﬁnd εtop (n) by computing
a topologically admissible interval [εmin (n), εmax (n)] from the barcode corresponding to the given
sample. To evaluate the quality of bandwidth parameters chosen inside [εmin (n), εmax (n)], we then
sample at various sampling sizes and compute the mean L2 errors for the resulting density estimator
2 (εmax + εmin ) for each of the spherical kernels that
fε,n for ε = εtop , εmin , εmax and εmid = 1
we have described and compare our results to εamise . We set σ2 = 1
4 for Ke and Kt . The results,
summarized in Figure 4, show that εtop performs at a level comparable to εamise in our experiments.
Note here that εamise can only be computed if the true density f is known, while, for εtop , we only

5

05101520253000.10.205101520253000.10.205101520253000.10.2(a) bandwidth values

(b) Kt , σ2 = 1
4

(c) Ke , σ2 = 1
4

(d) Ku

(e) Kc

Figure 4: We generate samples from our 1D density using rejection sampling and consider sample sizes n from
10 to 100 in increments of 10 (small scale) and from 250 to 5000 in increments of 250 (larger scale), resulting
in 30 increasing sample sizes n1 , . . . , n30 . In order to obtain stable results, we perform the sampling for each
sampling size 1000 times (small scale), 100 times (for 250, 500, 750, 1000) and 10 times (for n > 1000)
respectively. We then compute the corresponding kernel density estimators ˆfε,n and the mean L2 norm of
f − ˆfεn ,n . Figures (b)-(e) display these mean L2 errors (vertical axis) for the indicated kernel function and
bandwidth selectors. Figure (a) displays the bandwidth values (vertical axis) for the given bandwidth selectors.
In all the above plots, a horizontal coordinate of i ∈ {1, . . . , 30} corresponds to a sample size of ni .

(a) density f

(b) 100 samples and
Ωεtop in grey.

(c) ˆfεtop ,100 using just
100 samples as in
5(b)

(d) barcode
for b0

(e) barcode
for b1

Figure 5: 2D density, samples with inferred support region Ωεtop , topological reconstruction (using Kt ,
4 ) and barcodes with [εmin , εmax ] highlighted.
σ2 = 1

required the information that b0 (supp f ) = 3. In our experiments (sample sizes n (cid:62) 10), we were
able to determine a valid interval [εmin (n), εmax (n)] in all cases and did not encounter a case where
the topological reconstruction was impossible.

Results in 2D Here, we consider the density f displayed in Figure 5(a). We chose this exam-
ple to be representative for problems also arising in robotics, where the localization of a robot can
be modelled as depending on a probability prior which encodes space occupied by objects by zero
probability. In such scenarios, we might be able to obtain topological information about the un-
obstructed space X , such as knowing the number of components or holes in X . Such information
could be particularly valuable in the case of deformable obstacles since their homology stays invari-
ant under continuous deformations by homotopies. We set up the current experiment in a fashion
similar to our 1D experiments, i.e. we iterate sampling from the given density for various sample
sizes and compute the resulting mean L2 errors to evaluate our results. As we can see from Figure
6, our results indicate that bandwidths ε ∈ [εmin , εmax ] yield errors comparable with the AMISE
optimal bandwidth choice. While εtop does not perform as well as in the previous experiment, we
can observe that the corresponding L2 errors nonetheless follow a decreasing trend. Note also that
both in 1D and 2D, εtop also yields good L2 error results for the standard spherical Gaussian ker-
nel here. In applications such as probabilistic motion planning, the inferred structure of supp f is
however of importance as well (e.g. since path-connectedness of supp f is important), making a
bounded support kernel a preferable choice (see also our racetrack example).

6

11020300.00.51.01.52.02.53.03.5Εamise,KcΕamise,KuΕamise,KeΕamise,KtΕmaxΕmidΕminΕtop11020300.000.050.100.150.200.250.30maxmidminamisetop11020300.000.050.100.150.200.250.30maxmidminamisetop11020300.000.050.100.150.200.250.30maxmidminamisetop11020300.000.050.100.150.200.250.30maxmidminamisetop(cid:45)10010(cid:45)100100303(a) bandwidth values

(b) Kt , σ2 = 1
4

(c) Ke , σ2 = 1
4

(d) Ku

(e) Kc

Figure 6: We generate samples from our 2D density using rejection sampling and consider sample sizes from
100 to 1500 in increments of 100. We perform sampling 10 times for each sample size and compute the
corresponding kernel-based density estimator ˆfε,n and the mean L2 norm of f − ˆfεn ,n . Figures (b)-(e) display
these mean L2 errors (vertical axis) for the indicated sample size (horizontal axis) and kernel function. Figure
(a) displays the indicated bandwidth values (vertical axis) and sample size (horizontal axis).

(a) Position component of our
racetrack data

(b) Projection of inferred sup-
port region, generated vector
ﬁeld and sample trajectories

(c) Inferred vector ﬁeld, position
likelihood and sample trajec-
tories using GMR.

Figure 7: Figure (a) shows the positions of a race car driving 10 laps around a racetrack. In (b), the results
of our proposed method are displayed while Figure (c) shows the standard GMR approach. We exploit the
topological information that a racetrack should be connected and ‘circular’ when learning the density. As can
be seen, our model correctly infers the region of support as the track (grey). Using GMR, on the other hand, a
non-zero probability is assigned to each location. We observe that the most probable regions are also lying over
the track (black being more likely). However, when sampling new trajectories using the learned density, we can
see that, whereas the trajectories using our method are conﬁned to the track, the GMM results in undesirable
trajectories.

Application to regression We now consider how our framework can be applied to learn complex
dynamics given a topological constraint. We consider GPS/timestamp data from 10 laps of a race car
driving around a racetrack which was provided to us by [20]. For this dataset (see Figure 7(a)), we
are given no information on what the boundaries of the racetrack are. One state of the art approach to
modelling data like this is to employ a learning by demonstration [12] technique which is prominent
especially in the context of robotics, where one attempts to learn motion patterns by observing a
few demonstrations. There, one uses data points S = {(Pk , Vk ) ∈ R2n , k = 1 . . . n}, where Pk
describes the position and Vk ∈ Rn the associated velocity at the given position. In order to model
the dynamics, one can then employ a Gaussian mixture model [12] in R2n to learn a probability
density ˆf for the dataset (usually using the EM-algorithm). To every position x ∈ Rn , one can
then associate the velocity vector given by E(V |P = x) with respect to the learned density ˆf –
this uses the idea of Gaussian mixture regression (GMR). The resulting vector ﬁeld can then be
numerically integrated to yield new trajectories. Since E(V |P = x) for a Gaussian mixture model
can be computed easily, this method can be applied even in high-dimensional spaces. While it can
be considered as a strength of the GMR approach that it is able to infer – from just a few examples –

7

100500100015000.00.51.01.52.0Εamise,KcΕamise,KuΕamise,KeΕamise,KtΕmaxΕmidΕminΕtop100500100015000.000.020.040.060.08maxmidminamisetop100500100015000.000.020.040.060.08maxmidminamisetop100500100015000.000.020.040.060.08maxmidminamisetop100500100015000.000.020.040.060.08maxmidminamisetop............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................050100150200050100150200050100150200050100150200a vector ﬁeld that is non-zero on a dense subset of Rn , this can also be problematic since geometric
and topological constraints are not naturally part of this approach and we cannot easily encode the
fact that the vector-ﬁeld should be non-zero only on the racetrack.
From our GPS/timestamp data, we now compute velocity vectors for each data-point and embed
the data in the manner just described in R4 . We then experimented with the software [21] to model
our racetrack data with a mixture of a varying number of Gaussians. While the model brakes down
completely for a low number of Gaussians, some interesting behaviour can be observed in the case of
a mixture model with 50 Gaussians displayed in Figure 7(c). We display the resulting velocity vector
ﬁeld together with several newly synthesized trajectories. We observe both an undesired periodic
trajectory as well as a trajectory that almost completely traverses the racetrack before converging
towards an attractor. The likelihood of a given position is additionally displayed in 7(c) with black
being the most likely. While the most likely positions do occur over the racetrack, the mixture
model does not provide a natural way of determining where the boundaries of the track should lie.
The topmost trajectory in 7(c), for example, starts at a highly unlikely position.
Let us now consider how we can apply the density estimation techniques
we have described in this paper in this case. Given that we know that
the racetrack is a closed curve, we assume that the data should be mod-
elled by a probability density f : R4 → R whose support region Ω has
a single component (b0 (Ω) = 1) and Ω should topologically be a circle
(b1 (Ω) = 1). In order for the velocities of differing laps around the track
not to lie too far apart , and so that the topology of the racetrack dominates
in R4 , we rescale the velocity components of our data to lie inside the in-
terval [−0.6, 0.6]. Figure 8 displays the barcode for our data. Using our
procedure, we compute that [εmin , εmax ] (cid:117) [3.25, 3.97] is the bandwidth
interval for which the topological constraints that we just deﬁned are sat-
Figure 8:
Barcodes
isﬁed. Using the kernel Kt with σ2 = 1
4 and the corresponding density
in dimension zero (a)
estimator ˆfεtop , we obtain Ωεtop ⊂ R4 with the correct topological prop-
and one (b) and shaded
erties. Figure 7(b) displays the projection of Ωεtop onto R2 . As a next
[εmin , εmax ] interval for
our racetrack.
step, we suggest to follow the idea of the GMR approach to compute the
posterior expectation E(V |P = x), but this time for our density ˆfεtop . It
follows from the deﬁnition of our kernel-based estimator that, for x such that (x, y) ∈ Ωεtop for
(cid:19)
(cid:18) x−Xi
(cid:82) Kt
(cid:80)n
(cid:18) x−Xi
(cid:19)
(cid:82) Kt
(cid:80)n
some y ∈ Rn , we have E(V |P = x) =
,z
dz
i=1 Yi
εtop
. While we were not able to ﬁnd a ref-
dz
,z
i=1
εtop
erence for the use or computation of these marginals for spherical truncated Gaussians, a reasonably
simple calculation shows that these can in fact be computed analytically in arbitrary dimension:
Lemma 4.1. Consider d, k ∈ N, d > k and x ∈ Rk . Let Kt : Rd → R denote the spherical truncated
(cid:90)
Gaussian with parameter σ2 > 0. Then

2 , 1−(cid:107)x(cid:107)2
P ( d−k
1
2σ2
2 , 1
P ( d
(2πσ2 )k/2
Rd−k
2σ2 )
for (cid:107)x(cid:107) (cid:54) 1 and 0 otherwise. Here, P (a, b) = 1 − Γ(a,b)
Γ(a) denotes the normalized Gamma P function.

Kt (x, y)dy =

(b) b1

(a) b0

− (cid:107)x(cid:107)2
2σ2

)

e

For every point in the projection of Ωεtop onto the position coordinates, we can hence compute a
velocity E(V |P = x) to generate new motion trajectories. For points outside the support region, we
postulate zero velocity. Figure 7(c) displays the resulting vector-ﬁeld and a few sample trajectories.
As we can see, these follow the trajectory of the data points in Figure 7(a) very well. At the same
time, the displayed support region looks like a sensible choice for the position of the racetrack.
5 Conclusion

In this paper, we have presented a novel method for learning density models with bounded sup-
port. The proposed topological bandwidth selection approach allows to incorporate topological
constraints within a probabilistic modelling framework by combining algebraic-topological infor-
mation obtained in terms of persistent homology with tools from kernel-based density estimation.
We have provided a ﬁrst thorough evaluation of the L2 errors for synthetic data and have exempliﬁed
the practical use of our approach through application in a learning by demonstration scenario.

8

02.5502.55References
[1] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker veriﬁcation using adapted Gaussian
mixture models,” Digital Signal Processing, vol. 10, no. 1–3, pp. 19–41, 2000.
[2] C. E. Rasmussen and C. Williams, Gaussian Processes for Machine Learning. MIT Press,
2006.
[3] D. A. Cohn, Z. Ghahramani, and M. I. Jordan, “Active learning with statistical models,” Jour-
nal of Artiﬁcial Intelligence Research, no. 4, pp. 129–145, 1996.
[4] S. Calinon and A. Billard, “Incremental learning of gestures by imitation in a humanoid robot,”
in ACM/IEEE International Conference on Human-Robot Interaction, pp. 255–262, 2007.
[5] D.-S. Lee, “Effective Gaussian mixture learning for video background subtraction,” PAMI,
vol. 27, no. 5, pp. 827–832, 2005.
[6] M. P. Wand and M. C. Jones, Kernel Smoothing, vol. 60 of Monographs on Statistics and
Applied Probability. Chapman and Hall/CRC, 1995.
[7] B. A. Turlach, “Bandwidth selection in kernel density estimation: A review,” in CORE and
Institut de Statistique, pp. 23–493, 1993.
[8] L. El Ghaoui and G. Calaﬁore, “Robust ﬁltering for discrete-time systems with bounded noise
and parametric uncertainty,” IEEE Transactions on Automatic Control, vol. 46, no. 7, pp. 1084–
1089, 2001.
[9] Y. C. Eldar, A. Ben-Tal, and A. Nemirovski, “Linear minimax regret estimation of determin-
istic parameters with bounded data uncertainties,” IEEE Transactions on Signal Processing,
vol. 52, no. 8, pp. 2177–2188, 2008.
[10] G. Carlsson, “Topology and data,” Bull. Amer. Math. Soc. (N.S.), vol. 46, no. 2, pp. 255–308,
2009.
[11] P. Niyogi, S. Smale, and S. Weinberger, “A topological view of unsupervised learning from
noisy data,” SIAM Journal of Computing, vol. 40, no. 3, pp. 646–663, 2011.
[12] S. M. Khansari-Zadeh and A. Billard, “Learning stable non-linear dynamical systems with
Gaussian mixture models,” IEEE Transaction on Robotics, vol. 27, no. 5, pp. 943–957, 2011.
[13] M. Rosenblatt, “Remarks on some nonparametric estimates of a density function,” The Annals
of Mathematical Statistics, vol. 27, no. 3, pp. 832–837, 1956.
[14] E. Parzen, “On estimation of a probability density function and mode,” Annals of Mathematical
Statistics, vol. 33, pp. 1065–1076, 1962.
[15] T. Cacoullos, “Estimation of a multivariate density,” Annals of the Institute of Statistical Math-
ematics, vol. 18, pp. 179–189, 1966.
[16] H. Edelsbrunner, D. Letscher, and A. Zomorodian, “Topological persistence and simpliﬁca-
tion,” Discrete Comput. Geom., vol. 28, no. 4, pp. 511–533, 2002.
[17] A. Hatcher, Algebraic Topology. Cambridge University Press, 2002.
[18] P. Niyogi, S. Smale, and S. Weinberger, “Finding the homology of submanifolds with high
conﬁdence from random samples,” Discrete Comput. Geom., vol. 39, no. 1-3, pp. 419–441,
2008.
[19] A. Tausz, M. Vejdemo-Johansson, and H. Adams, “JavaPlex: A software package for comput-
ing persistent topological invariants.” Software, 2011.
[20] KTH Racing, Formula Student Team, KTH Royal Institute of Technology, Stockholm, Swe-
den.
[21] A. Billard, “GMM/GMR 2.0.” Software.

9


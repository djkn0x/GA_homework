Classifying applications based on API consumption.
CS229, Autumn 2012

Arne Roomann-Kurrik

December 14, 2012

Is it possible to classify applications based oﬀ of
records of their calls to an API?

Providers of public APIs take on the burden of
supporting the ecosystem of applications which
depend upon the API to function.
Insight into
the types of applications issuing queries into the
system is valuable to the maintainers of the plat-
form.

This paper is concerned with two use cases ad-
dressed by classifying applications into categories.

The ﬁrst addresses identifying applications which
are considered to be abusive users of the platform.
This can be modeled through a binary “abusive”
and “not abusive” classiﬁcation.

The second is categorization of applications into
distinct groups for the purposes of identifying ap-
plication verticals. Such verticals of similar ap-
plications can be used to tune rate limiting, plan
new features, and gain insight into the ways the
platform is being used which may not be totally
intuitive from an initial inspection.

Examining this problem required collecting and
identifying characteristics of application usage
logs,
to clean the
experimenting with ways
dataset, and training classiﬁers to determine the
potential success rates of applying machine learn-
ing toward application classiﬁcation.

1. Data collection

Twitter’s API supports an ecosystem of millions
of applications. This volume complicates even

1

simple analysis. Such analysis may beneﬁt from
machine learning to expose inherent categories of
application and highlight candidates which should
be reviewed by policy teams.

As an employee of Twitter, I was able to run
distributed queries against the API logs using
a Hadoop cluster. The scale of the raw log
data required a 30 minute job to pull records for
a single hour. The ﬁnal training dataset con-
tained 124,916 applications which made requests
to api.twitter.com during a period of October
8th, 2012. The ﬁnal test dataset contained 89,166
applications from a period of October 15th, 2012-
a week later.

A log line
to a sin-
contains a request
gle path (e.g /1.1/statuses/update) on the
server.
Some normalization is performed (e.g.
/1/statuses/destroy/:id/) but the set still
contained 2,232 unique paths.

To assign a score to the entries in the test and
train dataset, I used up-to-date lists of suspended
applications. There were 147 applications which
had made calls in October which had been sus-
pended since.

The process which collected the log lines out-
put a matrix where the columns corresponded to
individual endpoint paths, and the rows repre-
sented applications. The Mi,j th element of the
matrix represented the number of times applica-
tion i made calls to endpoint j .

2. Properties of the dataset

A suspended application is the result of a man-
ual review, so the list of suspended applications
is a strong dataset for training a classiﬁer to look
for abusive applications. However, not being sus-
pended is not an indicator that an application is
non-abusive, just that it hasn’t been reviewed and
ﬂagged. So while there are no false positives, there
are potentially many false negatives.

Reviewing an application requires a non-trivial
amount of manual work.
It was not very feasi-
ble to build a large, accurate dataset by hand. I
realized that a successful classiﬁer would identify
as many suspended applications as possible while
keeping the false positive rate low enough as to be
realistic to review by hand.

The ratio of suspended to non-suspended applica-
tions is very unbalanced. It is possible to achieve a
99.882% success rate simply by writing a classiﬁer
which says that no applications are abusive.

3. Clustering applications

My intuition was that an application of k -means
clustering would separate applications into logical
groupings which could be individually analyzed.
However, running the k -means algorithm to con-
vergence did not result in even groupings of appli-
cations - one cluster always gained the vast ma-
jority of applications, as shown in this grouping:

k Size
0
31
2
1
38
2
256
3
4
3
30
5
6
12

k
7
8
9
10
11
12
13

Size
9
15
4
2
4
5
11

k
14
15
16
17
18
19

Size
4
6
9
124380
19
76

Table 1: Cluster size for 20 k -means groups

There were some patterns in the groupings. For
example, all of Twitter’s oﬃcial clients were clus-
tered into various smaller groups. Many popular

2

mobile clients also fell into the smaller clusters.
Mobile clients typically display the same data and
diﬀerentiate on presentation, so this seems intu-
itve.

Cluster 3 also showed some patterns. As the sec-
ond largest cluster, it contained sequences of ap-
plications with almost-contiguous IDs and very
similar naming patterns:

XXXXX3869 XXXXXXXXXXAPP
XXXXX3876 XXXXXXXXXXAPP
XXXXX3898 XXXXXXXXXX’s App
XXXXX3913 XXXXXXXXXX’s tweetApp
XXXXX3964 XXXXXXXXXX’s tweetApplication
XXXXX3970 XXXXXXXXXX’s testApp
XXXXX3990 XXXXXXXXXX’sSP app tweet awsome
XXXXX4007 XXXXXXXXXX’s app
XXXXX4017 XXXXXXXXXX’s app test
XXXXX4023 XXXXXXXXXX’s app
XXXXX4028 XXXXXXXXXX’sapplication
XXXXX4050 XXXXXXXXXX’s app
XXXXX4055 XXXXXXXXXX application
XXXXX4058 XXXXXXXXXX’s app
XXXXX4099 XXXXXXXXXX’s app
XXXXX4112 XXXXXXXXXX’s application
XXXXX4117 XXXXXXXXXX’s app
XXXXX4123 XXXXXXXXXX’s app
XXXXX4145 XXXXXXXXXX’s app
XXXXX4147 XXXXXXXXXX’sapp
XXXXX4241 XXXXXXXXXX’s APP

Even some relatively distant apps (by ID) were
obviously similarly registered:

XXX599713 FOO App 23 P
XXX599734 FOO App 23 Q
XXX600451 FOO App 23 B
XXX600463 FOO App 23 A
XXX731254 FOO App 33 T45
XXX754732 FOO App 33 N63

This would seem to indicate that these apps were
registered and operated by some sort of auto-
mated system, and warrant investigation. That
the clusterings were created by analyzing us-
age without regard to ID or application name is

promising. The sheer size of cluster 17 means a
diﬀerent categorization approach would be needed
for the ma jority of applications, however.

4. Logistic regression

Most of the work for this analysis was spent try-
ing to classify abusive applications. To get an in-
tuition about where to spend my time, I wrote a
‘quick and dirty’ logistic regression classiﬁer using
the following stochastic gradient ascent rule:
θj := θj + α (cid:0)y (i) − hθ (x(i) )(cid:1) x(i)
j
1
1+e−θT x , the sigmoid function.
Where hθ (x) =
Graphing the performance of this classiﬁer us-
ing increasingly large subsets of the training set
showed a slow convergence between the error rates
for the test and training sets:

Figure 1: SGA performance

This was an indicator of high variance. General
strategies to address this are to increase the data
set size and to reduce the number of features used
in the model. I pulled more data for my datasets,
and planned strategies for reducing the dimen-
sionality of the data.

5. Principal component analysis

When collecting the dataset, I really had no idea
whether the suspended applications would be lo-
cated in proximity to each other, or if they would
be spread out throughout the dataset. I wanted
a way to determine whether a classiﬁer was even
likely to ﬁnd a separation between the two cate-
gories of application. Principal component analy-
sis seemed like a good way to cast the data into

two dimensional space, which could be used for a
visualization.

Figure 2: 2D plot of applications

By normalizing the data to zero out the mean
and variance, I was able to ﬁnd the corresponding
eigenvectors and generate Image 2.

By plotting suspended applications in red,
it
seems that types of application fall into speciﬁc
linear combinations of the two principal compo-
nents. Applications which have been marked as
abusive tend to vary along the second principal
component (y-axis in the above graph) with lit-
tle variation along the ﬁrst principal component
(x-axis).

6. Endpoint collection

A look at the features in the dataset made it
obvious that many were redundant. During the
time that the data was pulled there were two
available versions of the API, and calls were dis-
tributed across each. A simple way to reduce
features seemed to be to collect the endpoints
which accomplished the same task (e.g. posting a
Tweet) with diﬀerent semantics (e.g. update vs.
update with media) and combine the numbers of
calls into a single number.

3

1002003004005006000.100.16SamplesErrorTrain vs. test error − stochastic gradient ascentltest errortrain error−4−20246x 10−3−20246810x 10−3Endpoint
/1/statuses/destroy/
/1/statuses/destroy/:id/
/1.1/statuses/destroy/:id/
/1.1/statuses/update/
/1/statuses/update/
...

Mapping
statuses destroy
statuses destroy
statuses destroy
statuses write
statuses write
...

Table 2: Logically grouping endpoints

This was best accomplished via a manual process.
Reviewing 2000 endpoints by hand seems like a
lot of work, but many contained IDs which had
not been collected properly by the logs processor,
many contained mistakes or typos made by the
application, and some were otherwise restricted or
internal endpoints made by oﬃcial clients. This
process reduced the number of features to 61.

7. Feature selection and forward search

Another approach for reducing the number of fea-
tures was to identify which endpoints contributed
most toward reducing the error of a classiﬁer. To
implement forward search, I generated one dataset
per feature (at this point having culled out ty-
pos, unparsed IDs, and restricted endpoints) and
saw which endpoint produced the best logistic re-
gression score. Keeping that endpoint, I created
datasets with 2 features, and ran the classiﬁer
again.

i
1
2
3
4
5
6
7
8
9
10

Endpoint
/1/friendships/create/
/1/followers/ids/
/1/statuses/friends timeline/
/1/statuses/home timeline/
/1/users/show/
/1/account/totals/
/1/statuses/friends/
/1/statuses/update/
/1/account/rate limit status/
/1/statuses/update with media/

Error
0.088608
0.075949
0.072333
0.065099
0.059675
0.057866
0.056058
0.054250
0.052441
0.050633

Table 3: Forward search for API features

The results, listed in Table 3 indicate the 10 end-
points which contributed the most to classiﬁer
accuracy.
Intuitively, /1/friendships/create/
is used for follower spam and appears to be the
best single-endpoint feature for classifying abusive
apps. The /1/followers/ids endpoint would
also be useful for identifying possible targets for
spammy follow requests.

8. Scoring

As mentioned earlier, it would be possible to write
a classiﬁer with 99.882% accuracy simply by as-
serting that every app is not abusive. My exper-
iments with logistic regression were yielding 90%
accuracy, so I investigated diﬀerent scoring mech-
anisms which may give better insight into how
well a given classiﬁer was doing.

Precision and recall are useful for the unbalanced
data set case:

Precision =

Recall =

TP
TP + FP
TP
TP + FN

I desired a single value metric, though, so looked
into F1 score:
F1 = 2 × Precision × Recall
Precision + Recall

C =

When investigating F1 ,
I also came across
Matthew’s correlation coeﬃcient (C ), which was
supposed to be useful for ranking the performance
of binary classiﬁers even in cases where classes
were of very diﬀerent sizes.
(cid:112)(TP + FP)(TP + FN)(TN + FP)(TN + FN)
TP × TN − FP × FN
In practice, I found Matthew’s correlation co-
eﬃcient to be most in line with my intuitions
about well-performing classiﬁers, in that classi-
ﬁers which appeared to be doing a good job had
higher coeﬃcients than ones which appeared to be
doing poorly.

4

9. Support vector machines

Using LIBSVM, classiﬁers were trained against each
type of feature reduction.

I paid special attention to the number of false
positives each classiﬁer returned. By adjust-
ing weights given to each category, the SVM-
generated model could be tweaked to return more
or fewer false positives as needed. It was impor-
tant to develop a model which identiﬁed as many
abusive applications as possible, while keeping the
false positive rate below a quantity which would
be appropriate for manual review.
I estimated
that a review might take 10 minutes for an ex-
perienced reviewer with appropriate tools. 500
reviews would cost 83 reviewer-hours, so over two
weeks of work for a single person. I rejected mod-
els which returned signiﬁcantly more false posi-
tives than this.

I was also sometimes able to tune the amount of
returned results by changing the threshold of the
SVM score past which a positive score would be
awarded. Surprisingly the only model which re-
ally diﬀered much here was the endpoint collec-
tion method:

Figure 3: Threshold vs MCC score for SVM

Ultimately the endpoint collection method pro-
duced the best results out of all of the models.
The result was a SVM classiﬁer which identiﬁed
34% of the suspended applications in the test set
of 89,166 applications, with 576 false positives.

FP C
Model
0.165301030367
Endpoint collection 576
Feature selection
370
0.0229517324541
0.0200499603763
220
2d PCA
All endpoints
42
-0.000882154435157

Table 4: Comparison of models for SVM

Threshold TP
53
0.1
53
0.2
0.3
53
...
0.9
1.0

52
51

TN
87888
87970
88023

FP FN
94
1131
94
1049
996
94

88272
88443

747
576

95
96

Table 5: Endpoint collection counts

Threshold
0.1
0.2
0.3
...
0.9
1.0

P
0.044
0.048
0.050

R
0.000602
0.000602
0.000601

F1
0.001189
0.001189
0.001189

MCC
0.123
0.128
0.131

0.065
0.081

0.000588
0.000576

0.001166
0.001144

0.148
0.165

Table 6: Endpoint collection scores

10. Conclusion

Machine learning appears to have merit for clas-
sifying applications, particularly in the context of
surfacing candidates for eventual human review.

While not perfect, the current SVM results repre-
sent a pool of applications realistically sized to be
able to review by hand.

There are a few steps which could be used to im-
prove classiﬁer accuracy while reducing the num-
ber of false positives returned. More compli-
cated kernels may be able to better categorize
the data, and a system which took the output of
human reviews (in particular applications which
were marked as candidates and not suspended) in
order to retrain its model would have a cleaner
dataset to work with.

5

0.10.20.30.40.50.60.70.80.91−0.0200.020.040.060.080.10.120.140.160.18Classification ThresholdMCC Score  Endpoint CollectionFeature SelectionPCAAll Endpoints
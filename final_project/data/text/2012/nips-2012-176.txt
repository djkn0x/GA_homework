Iterative Thresholding Algorithm for Sparse Inverse
Covariance Estimation

Dominique Guillot
Dept. of Statistics
Stanford University
Stanford, CA 94305
dguillot@stanford.edu

Bala Rajaratnam
Dept. of Statistics
Stanford University
Stanford, CA 94305
brajarat@stanford.edu

Benjamin T. Rolfs
ICME
Stanford University
Stanford, CA 94305
benrolfs@stanford.edu

Arian Maleki
Dept. of ECE
Rice University
Houston, TX 77005
arian.maleki@rice.edu

Ian Wong
Dept. of EE and Statistics
Stanford University
Stanford, CA 94305
ianw@stanford.edu

Abstract

The (cid:96)1 -regularized maximum likelihood estimation problem has recently become
a topic of great interest within the machine learning, statistics, and optimization
communities as a method for producing sparse inverse covariance estimators. In
this paper, a proximal gradient method (G-ISTA) for performing (cid:96)1 -regularized
covariance matrix estimation is presented. Although numerous algorithms have
been proposed for solving this problem, this simple proximal gradient method is
found to have attractive theoretical and numerical properties. G-ISTA has a linear
rate of convergence, resulting in an O(log ε) iteration complexity to reach a toler-
ance of ε. This paper gives eigenvalue bounds for the G-ISTA iterates, providing
a closed-form linear convergence rate. The rate is shown to be closely related to
the condition number of the optimal point. Numerical convergence results and
timing comparisons for the proposed method are presented. G-ISTA is shown to
perform very well, especially when the optimal point is well-conditioned.

1

Introduction

Datasets from a wide range of modern research areas are increasingly high dimensional, which
presents a number of theoretical and practical challenges. A fundamental example is the problem
of estimating the covariance matrix from a dataset of n samples {X (i)}n
i=1 , drawn i.i.d from a p-
++ , X (i) ∼ Np (0, Σ),
dimensional, zero-mean, Gaussian distribution with covariance matrix Σ ∈ Sp
(cid:80)n
++ denotes the space of p × p symmetric, positive deﬁnite matrices. When n ≥ p the maxi-
where Sp
i=1 X (i)X (i)T . A
mum likelihood covariance estimator ˆΣ is the sample covariance matrix S = 1
n
problem however arises when n < p, due to the rank-deﬁciency in S . In this sample deﬁcient case,
common throughout several modern applications such as genomics, ﬁnance, and earth sciences, the
matrix S is not invertible, and thus cannot be directly used to obtain a well-deﬁned estimator for the
inverse covariance matrix Ω := Σ−1 .
A related problem is the inference of a Gaussian graphical model ([27, 14]), that is, a sparsity
pattern in the inverse covariance matrix, Ω. Gaussian graphical models provide a powerful means
of dimensionality reduction in high-dimensional data. Moreover, such models allow for discovery
of conditional independence relations between random variables since, for multivariate Gaussian
data, sparsity in the inverse covariance matrix encodes conditional independences. Speciﬁcally, if

1

i=1 ∈ Rp is distributed as X ∼ Np (0, Σ), then (Σ−1 )ij = Ωij = 0 ⇐⇒ Xi ⊥
X = (Xi )p
⊥ Xj |{Xk }k (cid:54)=i,j , where the notation A ⊥⊥ B |C denotes the conditional independence of A and
B given the set of variables C (see [27, 14]). If a dataset, even one with n (cid:29) p is drawn from a
normal distribution with sparse inverse covariance matrix Ω, the inverse sample covariance matrix
S−1 will almost surely be a dense matrix, although the estimates for those Ωij which are equal to 0
may be very small in magnitude. As sparse estimates of Ω are more robust than S−1 , and since such
sparsity may yield easily interpretable models, there exists signiﬁcant impetus to perform sparse
inverse covariance estimation in very high dimensional low sample size settings.

Banerjee et al. [1] proposed performing such sparse inverse covariance estimation by solving the
(cid:96)1 -penalized maximum likelihood estimation problem,

− log det Θ + (cid:104)S, Θ(cid:105) + ρ (cid:107)Θ(cid:107)1 ,
Θ∗
ρ = arg min
Θ∈Sp
where ρ > 0 is a penalty parameter, (cid:104)S, Θ(cid:105) = Tr (SΘ), and (cid:107)Θ(cid:107)1 = (cid:80)
++
i,j |Θij |. For ρ > 0,
ρ , as it is the closest convex relaxation of the 0 − 1 penalty, (cid:107)Θ(cid:107)0 = (cid:80)
Problem (1) is strongly convex and hence has a unique solution, which lies in the positive deﬁnite
cone Sp
++ due to the log det term, and is hence invertible. Moreover, the (cid:96)1 penalty induces sparsity
I(Θij (cid:54)= 0), where
in Θ∗
i,j
I(·) is the indicator function [5]. The unique optimal point of problem (1), Θ∗
ρ , is both invertible
(for ρ > 0) and sparse (for sufﬁciently large ρ), and can be used as an inverse covariance matrix
estimator.

(1)

In this paper, a proximal gradient method for solving Problem (1) is proposed. The resulting “graph-
ical iterative shrinkage thresholding algorithm”, or G-ISTA, is shown to converge at a linear rate to
(cid:13)(cid:13)F
≤ s (cid:13)(cid:13)Θt − Θ∗
(cid:13)(cid:13)F
(cid:13)(cid:13)Θt+1 − Θ∗
Θ∗
ρ , that is, its iterates Θt are proven to satisfy
(2)
,
ρ
ρ
for a ﬁxed worst-case contraction constant s ∈ (0, 1), where (cid:107)·(cid:107)F denotes the Frobenius norm.
The convergence rate s is provided explicitly in terms of S and ρ, and importantly, is related to the
condition number of Θ∗
ρ .
The paper is organized as follows. Section 2 describes prior work related to solution of Problem (1).
The G-ISTA algorithm is formulated in Section 3. Section 4 contains the convergence proofs of
this algorithm, which constitutes the primary mathematical result of this paper. Numerical results
are presented in Section 5, and concluding remarks are made in Section 6.

2 Prior Work

While several excellent general convex solvers exist (for example, [11] and [4]), these are not al-
ways adept at handling high dimensional problems (i.e., p > 1000). As many modern datasets
have several thousands of variables, numerous authors have proposed efﬁcient algorithms designed
speciﬁcally to solve the (cid:96)1 -penalized sparse maximum likelihood covariance estimation problem (1).

These can be broadly categorized as either primal or dual methods. Following the literature, we refer
to primal methods as those which directly solve Problem (1), yielding a concentration estimate. Dual
methods [1] yield a covariance matrix by solving the constrained problem,
− log det(S + U ) − p
minimize
U ∈Rp×p
subject to (cid:107)U (cid:107)∞ ≤ ρ,
where the primal and dual variables are related by Θ = (S + U )−1 . Both the primal and dual prob-
lems can be solved using block methods (also known as “row by row” methods), which sequentially
optimize one row/column of the argument at each step until convergence. The primal and dual block
problems both reduce to (cid:96)1 -penalized regressions, which can be solved very efﬁciently.

(3)

2

2.1 Dual Methods

A number of dual methods for solving Problem (1) have been proposed in the literature. Banerjee
et al. [1] consider a block coordinate descent algorithm to solve the block dual problem, which
reduces each optimization step to solving a box-constrained quadratic program. Each of these
quadratic programs is equivalent to performing a “lasso” ((cid:96)1 -regularized) regression. Friedman et al.
[10] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise de-
scent. Their widely used solver, known as the graphical lasso (glasso) is implemented on CRAN.
Global convergence rates of these block coordinate methods are unknown. D’Aspremont et al. [9]
use Nesterov’s smooth approximation scheme, which produces an ε-optimal solution in O(1/ε) it-
√
erations. A variant of Nesterov’s smooth method is shown to have a O(1/
ε) iteration complexity
in [15, 16].

2.2 Primal Methods

Interest in primal methods for solving Problem (1) has been growing for many reasons. One impor-
tant reason stems from the fact that convergence within a certain tolerance for the dual problem does
not necessarily imply convergence within the same tolerance for the primal.
Yuan and Lin [30] use interior point methods based on the max-det problem studied in [26]. Yuan
[31] use an alternating-direction method, while Scheinberg et al. [24] proposes a similar method and
show a sublinear convergence rate. Mazumder and Hastie [18] consider block-coordinate descent
approaches for the primal problem, similar to the dual approach taken in [10]. Mazumder and
Agarwal [17] also solve the primal problem with block-coordinate descent, but at each iteration
perform a partial as opposed to complete block optimization, resulting in a decreased computational
complexity per iteration. Convergence rates of these primal methods have not been considered in the
literature and hence theoretical guarantees are not available. Hsieh et al. [13] propose a second-order
proximal point algorithm, called QUIC, which converges superlinearly locally around the optimum.

3 Methodology

In this section, the graphical iterative shrinkage thresholding algorithm (G-ISTA) for solving the
primal problem (1) is presented. A rich body of mathematical and numerical work exists for general
iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 19, 20, 21, 25]. A brief
description is provided here.

3.1 General Iterative Shrinkage Thresholding (ISTA)

Iterative shrinkage thresholding algorithms (ISTA) are general ﬁrst-order techniques for solving
problems of the form

(4)
minimize
F (x) := f (x) + g(x),
x∈X
where X is a Hilbert space with inner product (cid:104)·, ·(cid:105) and associated norm (cid:107)·(cid:107), f : X → R is a
continuously differentiable, convex function, and g : X → R is a lower semi-continuous, convex
function, not necessarily smooth. The function f is also often assumed to have Lipschitz-continuous
gradient ∇f , that is, there exists some constant L > 0 such that
(cid:107)∇f (x1 ) − ∇f (x2 )(cid:107) ≤ L (cid:107)x1 − x2(cid:107)

for any x1 , x2 ∈ X .
For a given lower semi-continuous convex function g , the proximity operator of g , denoted by
(cid:26)
(cid:27)
proxg : X → X , is given by
(cid:107)x − y(cid:107)2
1
proxg (x) = arg min
(6)
g(y) +
,
y∈X
2
It is well known (for example, [8]) that x∗ ∈ X is an optimal solution of problem (4) if and only if
x∗ = proxζ g (x∗ − ζ∇f (x∗ ))
(7)

(5)

3

for any ζ > 0. The above characterization suggests a method for optimizing problem (4) based on
the iteration

xt+1 = proxζt g (xt − ζt∇f (xt ))
(8)
for some choice of step size, ζt . This simple method is referred to as an iterative shrinkage thresh-
(cid:18) 1
(cid:19)
olding algorithm (ISTA). For a step size ζt ≤ 1
L , the ISTA iterates xt are known to satisfy
F (xt ) − F (x∗ ) (cid:39) O
, ∀t,
(9)
t
where x∗ is some optimal point, which is to say, they converge to the space of optimal points at a
sublinear rate. If no Lipschitz constant L for ∇f is known, the same convergence result still holds
for ζt chosen such that

f (xt+1 ) ≤ Qζt (xt+1 , xt ),
where Qζ (·, ·) : X × X → R is a quadratic approximation to f , deﬁned by
Qζ (x, y) = f (y) + (cid:104)x − y , ∇f (y)(cid:105) +
(cid:107)x − y(cid:107)2 .
1
2ζ

See [3] for more details.

3.2 Graphical Iterative Shrinkage Thresholding (G-ISTA)

(10)

(11)

and

γ =

,

(cid:40)

The general method described in Section 3.1 can be adapted to the sparse inverse covariance es-
++ → R by
timation Problem (1). Using the notation introduced in Problem (4), deﬁne f , g : Sp
f (X ) = − log det(X ) + (cid:104)S, X (cid:105) and g(X ) = ρ (cid:107)X (cid:107)1 . Both are continuous convex functions de-
++ . Although the function ∇f (X ) = S − X −1 is not Lipschitz continuous over Sp
ﬁned on Sp
++ ,
it is Lipschitz continuous within any compact subset of Sp
++ (See Lemma 2 of the Supplemental
section).
(cid:26) p − α Tr(S )
(cid:27)
ρ , satisﬁes αI (cid:22) Θ∗
ρ (cid:22) β I , for
Lemma 1 ([1, 15]). The solution of Problem (1), Θ∗
1
(cid:107)S (cid:107)2 + pρ
α =
,
β= min
, γ
ρ
min{1T (cid:12)(cid:12)S−1 (cid:12)(cid:12) 1, (p − ρ
pα) (cid:13)(cid:13)S−1(cid:13)(cid:13)2 − (p − 1)α}
21T (cid:12)(cid:12)(S + ρ
2 I )−1 (cid:12)(cid:12) 1 − Tr((S + ρ
√
if S ∈ Sp
++
2 I )−1 )
otherwise,
where I denotes the p × p dimensional identity matrix and 1 denotes the p-dimensional vector of
ones.
Note that f + g as deﬁned is a continuous, strongly convex function on Sp
++ . Moreover, by Lemma
2 of the supplemental section, f has a Lipschitz continuous gradient when restricted to the compact
domain aI (cid:22) Θ (cid:22) bI . Hence, f and g as deﬁned meet the conditions described in Section 3.1.
The proximity operator of ρ (cid:107)X (cid:107)1 for ρ > 0 is the soft-thresholding operator, ηρ : Rp×p → Rp×p ,
deﬁned entrywise by
[ηρ (X )]i,j = sgn(Xi,j ) (|Xi,j | − ρ)+ ,
(14)
where for some x ∈ R, (x)+ := max(x, 0) (see [8]). Finally, the quadratic approximation Qζt of f ,
as in equation (11), is given by
Qζt (Θt+1 , Θt ) = − log det(Θt ) + (cid:104)S, Θt (cid:105) + (cid:104)Θt+1 − Θt , S − Θ−1
t (cid:105) +

(12)

(13)

1
2ζt

(cid:107)Θt+1 − Θt(cid:107)2
F .
(15)

The G-ISTA algorithm for solving Problem (1) is given in Algorithm 1. As in [3], the algorithm
uses a backtracking line search for the choice of step size. The procedure terminates when a pre-
speciﬁed duality gap is attained. The authors found that an initial estimate of Θ0 satisfying [Θ0 ]ii =

4

(Sii + ρ)−1 works well in practice. Note also that the positive deﬁnite check of Θt+1 during Step
(1) of Algorithm 1 is accomplished using a Cholesky decomposition, and the inverse of Θt+1 is
computed using that Cholesky factor.

Algorithm 1: G-ISTA for Problem (1)
input : Sample covariance matrix S , penalty parameter ρ, tolerance ε, backtracking constant
c ∈ (0, 1), initial step size ζ1,0 , initial iterate Θ0 . Set ∆ := 2ε.
(cid:0)Θt − ζt (S − Θ−1
t )(cid:1), the following are satisﬁed:
while ∆ > ε do
(1) Line search: Let ζt be the largest element of {cj ζt,0}j=0,1,... so that for
Θt+1 = ηζt ρ
Θt+1 (cid:31) 0
f (Θt+1 ) ≤ Qζt (Θt+1 , Θt ),
and
(cid:0)Θt − ζt (S − Θ−1
t )(cid:1)
for Qζt as deﬁned in (15).
(2) Update iterate: Θt+1 = ηζt ρ
(3) Set next initial step, ζt+1,0 . See Section 3.2.1.
(4) Compute duality gap:
∆ = − log det(S + Ut+1 ) − p − log det Θt+1 + (cid:104)S, Θ(cid:105) + ρ (cid:107)Θt+1(cid:107)1 ,
where (Ut+1 )i,j = min{max{([Θ−1
t+1 ]i,j − Si,j ), −ρ}, ρ}.
end
output: ε-optimal solution to problem (1), Θ∗
ρ = Θt+1 .

3.2.1 Choice of initial step size, ζ0
Each iteration of Algorithm 1 requires an initial step size, ζ0 . The results of Section 4 guarantee
that any ζ0 ≤ λmin (Θt )2 will be accepted by the line search criteria of Step 1 in the next iteration.
However, in practice this choice of step is overly cautious; a much larger step can often be taken.
Our implementation of Algorithm 1 chooses the Barzilai-Borwein step [2]. This step, given by
Tr ((Θt+1 − Θt )(Θt+1 − Θt ))
t − Θ−1
Tr ((Θt+1 − Θt )(Θ−1
t+1 ))
is also used in the SpaRSA algorithm [29], and approximates the Hessian around Θt+1 . If a cer-
tain number of maximum backtracks do not result in an accepted step, G-ISTA takes the safe step,
λmin (Θt )2 . Such a safe step can be obtained from λmax (Θ−1
t ), which in turn can be quickly ap-
proximated using power iteration.

ζt+1,0 =

(16)

,

4 Convergence Analysis

In this section, linear convergence of Algorithm 1 is discussed. Throughout the section, Θt (t =
1, 2, . . . ) denote the iterates of Algorithm 1, and Θ∗
ρ the optimal solution to Problem (1) for ρ > 0.
The minimum and maximum eigenvalues of a symmetric matrix A are denoted by λmin (A) and
λmax (A), respectively.
Theorem 1. Assume that the iterates Θt of Algorithm 1 satisfy aI (cid:22) Θt (cid:22) bI , ∀t for some ﬁxed
(cid:12)(cid:12)(cid:12)(cid:12)(cid:27) (cid:13)(cid:13)Θt − Θ∗
(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12)1 − ζt
(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)1 − ζt
(cid:13)(cid:13)Θt+1 − Θ∗
(cid:13)(cid:13)F
(cid:13)(cid:13)F
constants 0 < a < b. If ζt ≤ a2 , ∀t, then
≤ max
ρ
ρ
b2
a2
Furthermore,
1. The step size ζt which yields an optimal worst-case contraction bound s(ζt ) is ζ =
a−2+b−2 .
2
2. The optimal worst-case contraction bound corresponding to ζ =
s(ζ ) : = 1 − 2
1 + b2
a2

a−2+b−2 is given by
2

(17)

.

5

Proof. A direct proof is given in the appendix. Note that linear convergence of proximal gradient
methods for strongly convex objective functions in general has already been proven (see Supple-
mental section).

It remains to show that there exist constants a and b which bound the eigenvalues of Θt , ∀t. The
existence of such constants follows directly from Theorem 1, as Θt lie in the bounded domain
{Θ ∈ Sp
++ : f (Θ) + g(Θ) < f (Θ0 ) + g(Θ0 )}, for all t. However, it is possible to specify the
(cid:13)(cid:13)F
+ (cid:13)(cid:13)Θ0 − Θ∗
(cid:13)(cid:13)2
Θt of Algorithm 1 satisfy αI (cid:22) Θt (cid:22) b(cid:48) I , ∀t, with b(cid:48) = (cid:13)(cid:13)Θ∗
constants a and b to yield an explicit rate; this is done in Theorem 2.
Theorem 2. Let ρ > 0, deﬁne α and β as in Lemma 1, and assume ζt ≤ α2 , ∀t. Then the iterates
√
≤ β +
p(β + α).
ρ
ρ
Proof. See the Supplementary section.

Importantly, note that the bounds of Theorem 2 depend explicitly on the bound of Θ∗
ρ , as given by
Lemma 1. These eigenvalue bounds on Θt+1 , along with Theorem 1, provide a closed form linear
convergence rate for Algorithm 1. This rate depends only on properties of the solution.
Theorem 3. Let α and β be as in Lemma 1. Then for a constant step size ζt := ζ < α2 , the iterates
of Algorithm 1 converge linearly with a rate of
2α2
s(ζ ) = 1 −
√
p(β − α))2 < 1
α2 + (β +
αI (cid:22) Θt (cid:22) (cid:16)(cid:13)(cid:13)Θ∗
(cid:17)
(cid:13)(cid:13)F
+ (cid:13)(cid:13)Θ0 − Θ∗
(cid:13)(cid:13)2
Proof. By Theorem 2, for ζ < α2 , the iterates Θt satisfy
I
ρ
ρ
for all t. Moreover, since αI (cid:22) Θ∗ (cid:22) β I , if αI (cid:22) Θ0 (cid:22) β I (for instance, by taking Θ0 =
p (cid:13)(cid:13)Θ0 − Θ∗
(cid:13)(cid:13)2
(cid:13)(cid:13)F
+ (cid:13)(cid:13)Θ0 − Θ∗
(cid:13)(cid:13)2
(cid:13)(cid:13)Θ∗
(S + ρI )−1 or some multiple of the identity) then this can be bounded as:
√
≤ β +
√
ρ
ρ
ρ
≤ β +
p(β − α).

(19)
(20)

(18)

Therefore,

αI (cid:22) Θt (cid:22) (β +
and the result follows from Theorem 1.

√

p(β − α)) I ,

(21)

Remark 1. Note that the contraction constant (equation 18) of Theorem 3 is closely related to the
condition number of Θ∗
ρ ,
λmax (Θ∗
ρ )
λmin (Θ∗
ρ )

κ(Θ∗
ρ ) =

≤ β
α

as

1 −

2α2
p(β − α))2 ≥ 1 − 2α2
α2 + β 2 ≥ 1 − 2κ(Θ∗
ρ )−2 .
√
α2 + (β +
Therefore, the worst case bound becomes close to 1 as the conditioning number of Θ∗
ρ increases.

(22)

5 Numerical Results

In this section, we provide numerical results for the G-ISTA algorithm. In Section 5.2, the theo-
retical results of Section 4 are demonstrated. Section 5.3 compares running times of the G-ISTA,
glasso [10], and QUIC [13] algorithms. All algorithms were implemented in C++, and run on an
Intel i7 − 2600k 3.40GHz × 8 core with 16 GB of RAM.

6

5.1 Synthetic Datasets

Synthetic data for this section was generated following the method used by [16, 17]. For a ﬁxed p, a
p dimensional inverse covariance matrix Ω was generated with off-diagonal entries drawn i.i.d from
a uniform(−1, 1) distribution. These entries were set to zero with some ﬁxed probability (in this
case, either 0.97 or 0.85 to simulate a very sparse and a somewhat sparse model). Finally, a multiple
of the identity was added to the resulting matrix so that the smallest eigenvalue was equal to 1. In
this way, Ω was insured to be sparse, positive deﬁnite, and well-conditioned. Datsets of n samples
were then generated by drawing i.i.d. samples from a Np (0, Ω−1 ) distribution. For each value of p
and sparsity level of Ω, n = 1.2p and n = 0.2p were tested, to represent both the n < p and n > p
cases.

problem

p = 2000
n = 400
nnz(Ω) = 3%

p = 2000
n = 2400
nnz(Ω) = 3%

p = 2000
n = 400
nnz(Ω) = 15%

p = 2000
n = 2400
nnz(Ω) = 15%

ρ
algorithm
nnz(Ω∗
ρ )/κ(Ω∗
ρ )
glasso
QUIC
G-ISTA
ρ )/κ(Ω∗
nnz(Ω∗
ρ )
glasso
QUIC
G-ISTA
nnz(Ω∗
ρ )/κ(Ω∗
ρ )
glasso
QUIC
G-ISTA
nnz(Ω∗
ρ )/κ(Ω∗
ρ )
glasso
QUIC
G-ISTA

0.03
time/iter
27.65%/48.14
1977.92/11
1481.80/21
145.60/437
14.56%/10.25
667.29/7
211.29/10
14.09/47
27.35%/64.22
2163.33/11
1496.98/21
251.51/714
19.98%/17.72
708.15/6
301.35/10
28.23/88

0.06
time/iter
15.08%/20.14
831.69/8
257.97/11
27.05/9
3.11%/2.82
490.90/6
24.98/7
3.51/13
15.20%/28.50
862.39/8
318.57/12
47.35/148
5.49%/4.03
507.04/6
491.54/17
4.08/16

0.09
time/iter
7.24%/7.25
604.42/7
68.49/8
8.05/27
0.91%/1.51
318.24/4
5.16/5
2.72/10
7.87%/11.88
616.81/7
96.25/9
7.96/28
65.47%/1.36
313.88/4
4.12/5
1.95/7

0.12
time/iter
2.39%/2.32
401.59/5
15.25/6
3.19/12
0.11%/1.18
233.94/3
1.56/4
2.20/8
2.94%/2.87
48.47/7
23.62/7
3.18/12
0.03%/1.09
233.16/3
1.34/4
1.13/4

Table 1: Timing comparisons for p = 2000 dimensional datasets, generated as in Section 5.1.
Above, nnz(A) is the percentage of nonzero elements of matrix A.

5.2 Demonstration of Convergence Rates

The linear convergence rate derived for G-ISTA in Section 4 was shown to be heavily dependent on
the conditioning of the ﬁnal estimator. To demonstrate these results, G-ISTA was run on a synthetic
dataset, as described in Section 5.1, with p = 500 and n = 300. Regularization parameters of
ρ = 0.75, 0.1, 0.125, 0.15, and 0.175 were used. Note as ρ increases, Θ∗
ρ generally becomes
better conditioned. For each value of ρ, the numerical optimum was computed to a duality gap of
10−10 using G-ISTA. These values of ρ resulted in sparsity levels of 81.80%, 89.67%, 94.97%,
97.82%, and 99.11%, respectively. G-ISTA was then run again, and the Frobenius norm argument
errors at each iteration were stored. These errors were plotted on a log scale for each value of ρ
to demonstrate the dependence of the convergence rate on condition number. See Figure 1, which
clearly demonstrates the effects of conditioning.

5.3 Timing Comparisons

The G-ISTA, glasso, and QUIC algorithms were run on synthetic datasets (real datasets are
presented in the Supplemental section) of varying p, n and with different levels of regularization, ρ.
All algorithms were run to ensure a ﬁxed duality gap, here taken to be 10−5 . This comparison used
efﬁcient C++ implementations of each of the three algorithms investigated. The implementation of
G-ISTA was adapted from the publicly available C++ implementation of QUIC Hsieh et al. [13].
Running times were recorded and are presented in Table 1. Further comparisons are presented in the
Supplementary section.
Remark 2. The three algorithms variable ability to take advantage of multiple processors is an
important detail. The times presented in Table 1 are wall times, not CPU times. The comparisons
were run on a multicore processor, and it is important to note that the Cholesky decompositions and

7

Figure 1: Semilog plot of (cid:13)(cid:13)Θt − Θ∗
(cid:13)(cid:13)F
vs. iteration number t, demonstrating linear convergence
ρ
rates of G-ISTA, and dependence of those rates on κ(Θ∗
ρ ).

inversions required by both G-ISTA and QUIC take advantage of multiple cores. On the other hand,
the p2 dimensional lasso solve of QUIC and p-dimensional lasso solve of glasso do not. For this
reason, and because Cholesky factorizations and inversions make up the bulk of the computation
required by G-ISTA, the CPU time of G-ISTA was typically greater than its wall time by a factor
of roughly 4. The CPU and wall times of QUIC were more similar; the same applies to glasso.

6 Conclusion

In this paper, a proximal gradient method was applied to the sparse inverse covariance problem.
Linear convergence was discussed, with a ﬁxed closed-form rate. Numerical results have also been
presented, comparing G-ISTA to the widely-used glasso algorithm and the newer, but very fast,
QUIC algorithm. These results indicate that G-ISTA is competitive, in particular for values of
ρ which yield sparse, well-conditioned estimators. The G-ISTA algorithm was very fast on the
synthetic examples of Section 5.3, which were generated from well-conditioned models. For poorly
conditioned models, QUIC is very competitive. The Supplemental section gives two real datasets
which demonstrate this. For many practical applications however, obtaining an estimator that is
well-conditioned is important ([23, 28]). To conclude, although second-order methods for the sparse
inverse covariance method have recently been shown to perform well, simple ﬁrst-order methods
cannot be ruled out, as they can also be very competitive in many cases.

8

5010015020025030035040045010−610−410−2100102iteration||Θt−Θ*ρ||F  ρ = 0.075, κ(Θ∗ρ) = 7.263ρ = 0.1, κ(Θ∗ρ) = 3.9637ρ = 0.125, κ(Θ∗ρ) = 2.3581ρ = 0.15, κ(Θ∗ρ) = 1.6996ρ = 0.175, κ(Θ∗ρ) = 1.3968References
[1] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivarate gaussian or binary data. Journal of Machine Learning Research, 9:485–516,
2008.
[2] Jonathan Barzilai and Jonathan M. Borwein. Two-Point Step Size Gradient Methods. IMA Journal of
Numerical Analysis, 8(1):141–148, 1988.
[3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-
lems. SIAM Journal on Imaging Sciences, 2:183–202, 2009. ISSN 1936-4954.
[4] S. Becker, E.J. Candes, and M. Grant. Templates for convex cone problems with applications to sparse
signal recovery. Mathematical Programming Computation, 3:165–218, 2010.
[5] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[6] P. Brohan, J. J. Kennedy, I. Harris, S. F. B. Tett, and P. D. Jones. Uncertainty estimates in regional and
global observed temperature changes: A new data set from 1850. Journal of Geophysical Research, 111,
2006.
[7] George H.G. Chen and R.T. Rockafellar. Convergence rates in forward-backward splitting. Siam Journal
on Optimization, 7:421–444, 1997.
[8] Patrick L. Combettes and Val ´erie R. Wajs. Signal recovery by proximal forward-backward splitting.
Multiscale Modeling & Simulation, 4(4):1168–1200, 2005.
[9] Alexandre D’Aspremont, Onureena Banerjee, and Laurent El Ghaoui. First-order methods for sparse
covariance selection. SIAM Journal on Matrix Analysis and Applications, 30(1):56–66, 2008.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9:432–441, 2008.
[11] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21.
http://cvxr.com/cvx, April 2011.
[12] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.
[13] Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon, and Pradeep K. Ravikumar. Sparse inverse covari-
ance matrix estimation using quadratic approximation. In Advances in Neural Information Processing
Systems 24, pages 2330–2338. 2011.
[14] S.L. Lauritzen. Graphical models. Oxford Science Publications. Clarendon Press, 1996.
[15] Zhaosong Lu. Smooth optimization approach for sparse covariance selection. SIAM Journal on Opti-
mization, 19(4):1807–1827, 2009. ISSN 1052-6234. doi: http://dx.doi.org/10.1137/070695915.
[16] Zhaosong Lu. Adaptive ﬁrst-order methods for general sparse inverse covariance selection. SIAM Journal
on Matrix Analysis and Applications, 31:2000–2016, 2010.
[17] Rahul Mazumder and Deepak K. Agarwal. A ﬂexible, scalable and efﬁcient algorithmic framework for
the Primal graphical lasso. Pre-print, 2011.
[18] Rahul Mazumder and Trevor Hastie. The graphical lasso: New insights and alternatives. Pre-print, 2011.
[19] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2 ).
Soviet Mathematics Doklady, 27(2):372–376, 1983.
[20] Yurii Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.
[21] Yurii Nesterov. Gradient methods for minimizing composite objective function. CORE discussion papers,
Universit ´e catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2007.
[22] Jennifer Pittman, Erich Huang, Holly Dressman, Cheng-Fang F. Horng, Skye H. Cheng, Mei-Hua H.
Tsou, Chii-Ming M. Chen, Andrea Bild, Edwin S. Iversen, Andrew T. Huang, Joseph R. Nevins, and
Mike West. Integrated modeling of clinical and gene expression information for personalized prediction
of disease outcomes. Proceedings of the National Academy of Sciences of the United States of America,
101(22):8431–8436, 2004.
[23] Benjamin T. Rolfs and Bala Rajaratnam. A note on the lack of symmetry in the graphical lasso. Compu-
tational Statistics and Data Analysis, 2012.
[24] Katya Scheinberg, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating
In Advances in Neural Information Processing Systems 23, pages 2101–2109.
linearization methods.
2010.
[25] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to
SIAM Journal on Optimization, 2008.
[26] Lieven Vandenberghe, Stephen Boyd, and Shao-Po Wu. Determinant maximization with linear matrix
inequality constraints. SIAM Journal on Matrix Analysis and Applications, 19:499–533, 1996.
[27] J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley, 1990.
[28] J. Won, J. Lim, S. Kim, and B. Rajaratnam. Condition number regularized covariance estimation. Journal
of the Royal Statistical Society Series B, 2012.
[29] Stephen J. Wright, Robert D. Nowak, and M ´ario A. T. Figueiredo. Sparse reconstruction by separable
approximation. IEE Transactions on Signal Processing, 57(7):2479–2493, 2009.
[30] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94
(1):19–35, 2007.
[31] X.M. Yuan. Alternating direction method of multipliers for covariance selection models. Journal of
Scientiﬁc Computing, pages 1–13, 2010.

9


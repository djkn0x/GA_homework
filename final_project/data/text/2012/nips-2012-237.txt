Gradient-based kernel method for feature extraction
and variable selection

Kenji Fukumizu
The Institute of Statistical Mathematics
10-3 Midori-cho, Tachikawa, Tokyo 190-8562 Japan
fukumizu@ism.ac.jp

Chenlei Leng
National University of Singapore
6 Science Drive 2, Singapore, 117546
stalc@nus.edu.sg

Abstract

We propose a novel kernel approach to dimension reduction for supervised learn-
ing: feature extraction and variable selection; the former constructs a small num-
ber of features from predictors, and the latter ﬁnds a subset of predictors. First,
a method of linear feature extraction is proposed using the gradient of regression
function, based on the recent development of the kernel method.
In compari-
son with other existing methods, the proposed one has wide applicability without
strong assumptions on the regressor or type of variables, and uses computationally
simple eigendecomposition, thus applicable to large data sets. Second, in combi-
nation of a sparse penalty, the method is extended to variable selection, following
the approach by Chen et al. [2]. Experimental results show that the proposed meth-
ods successfully ﬁnd effective features and variables with out parametric models.

1

Introduction

Dimension reduction is involved in most of modern data analysis, in which high dimensional data
must be handled. There are two categories of dimension reduction: feature extraction, in which a
linear or nonlinear mapping to a low-dimensional space is pursued, and variable selection, in which
a subset of variables is selected. This paper discusses both the methods in supervised learning.
Let (X, Y ) be a random vector such that X = (X 1 , . . . , X m ) ∈ Rm . The domain of Y can be
arbitrary, either continuous, discrete, or structured. The goal of dimension reduction in supervised
setting is to ﬁnd such features or a subset of variables X that explain Y as effectively as possible.
This paper focuses linear dimension reduction, in which linear combinations of the components of
X are used to make effective features. Although there are many methods for extracting nonlinear
features, this paper conﬁnes its attentions on linear featu res, since linear methods are more stable
than nonlinear feature extraction, which depends strongly on the choice of the nonlinearity, and after
establishing a linear method, extension to a nonlinear one would not be difﬁcult.

We ﬁrst develop a method for linear feature extraction with k ernels, and extend it to variable selec-
tion with a sparseness penalty. The most signi ﬁcant point of
the proposed methods is that we do
not assume any parametric models on the conditional probability, or make strong assumptions on
the distribution of variables. This differs from many other methods, particularly for variable selec-
tion, where a speci ﬁc parametric model is often assumed. Bey ond the classical approaches such as
Fisher Discriminant Analysis and Canonical Correlation Analysis to linear dimension reduction, the
modern approach is based on the notion of conditional independence; we assume for the distribution
p(Y |X ) = ˜p(Y |B T X )
Y ⊥⊥X | B T X,
or equivalently
(1)
where B is a projection matrix (B T B = Id ) onto a d-dimensional subspace (d < m) in Rm , and
wish to estimate B . For variable selection, we further assume that some rows of B may be zero.
The subspace spanned by the columns of B is called the effective direction for regression, or EDR
space [14]. Our goal is thus to estimate B without speci ﬁc parametric models for p(y |x).

1

First, consider the linear feature extraction based on Eq. (1). The ﬁrst method using this formula-
tion is the sliced inverse regression (SIR, [13]), which employs the fact that the inverse regression
E [X |Y ] lies in the EDR space under some assumptions. Many methods have been proposed in this
vein of inverse regression ([4, 12] among others). While the methods are computationally simple,
they often need some strong assumptions on the distribution of X such as elliptic symmetry.
There are two most relevant works to this paper. The ﬁrst one i s the dimension reduction with the
gradient of regressor E [Y |X = x] [11, 17]. As explained in Sec. 2.1, under Eq. (1) the gradient
is contained in the EDR space. One can thus estimate the space by some standard nonparametric
method. There are some limitations in this approach, however: the nonparametric gradient esti-
mation in high-dimensional spaces is challenging, and the method may not work unless the noise
is additive. The second one is the kernel dimension reduction (KDR, [8, 9, 28]), which uses the
kernel method for characterizing the conditional independence to overcome various limitations of
existing methods. While KDR applies to a wide class of problems without any strong assumptions
on the distributions or types of X or Y , and shows high estimation accuracy for small data sets, its
optimization has a problem: the gradient descent method used for KDR may have local optima, and
needs many matrix inversions, which prohibits application to high-dimensional or large data.

We propose a kernel method for linear feature extraction using the gradient-based approach, but
unlike the existing ones [11, 17], the gradient is estimated based on the recent development of the
kernel method [9, 19]. It solves the problems of existing methods: by virtue of the kernel method, Y
can be of arbitrary type, and the kernel estimator is stable without careful decrease of bandwidth. It
solves also the problem of KDR: the estimator by an eigenproblem needs no numerical optimization.
The method is thus applicable to large and high-dimensional data, as we demonstrate experimentally.

Second, by using the above feature extraction in conjunction with a sparseness penalty, we propose a
novel method for variable selection. Recently extensive studies have been done for variable selection
with a sparseness penalty such as LASSO [23] and SCAD [6]. It is also known that with appropriate
choice of regularization coefﬁcients they have oracle prop erty [6, 25, 30]. These methods, however,
use some speci ﬁc model for regression such as linear regress ion, which is a limitation of the methods.
Chen et al. [2] proposed a novel method for sparse variable selection based on the objective function
of linear feature extraction formulated as an eigenproblem such as SIR. We follow this approach to
derive our method for variable selection. Unlike the methods used in [2], the proposed one does not
require strong assumptions on the regressor or distribution, and thus provides a variable selection
method based on the conditional independence irrespective of the regression model.

2 Gradient-based kernel dimension reduction

2.1 Gradient of a regression function and dimension reduction

We review the basic idea of the gradient-based method [11, 17] for dimension reduction. Suppose
Y is an R-valued random variable. If the assumption of Eq. (1) holds, we have
∂ z ˜p(y |z )(cid:12)(cid:12)z=BT x dy ,
∂x ˜p(y |B T x)dy = B R y ∂
∂x R yp(y |x)dy = R y ∂
∂
∂x E [Y |X = x] = ∂
which implies that the gradient ∂
∂x E [Y |X = x] at any x is contained in the EDR space. Based on
this fact, the average derivative estimates (ADE, [17]) has been proposed to estimate B . In the more
recent method [11], a standard local linear least squares with a smoothing kernel (not necessarily
positive deﬁnite, [5]) is used for estimating the gradient,
and the dimensionality of the projection
is continuously reduced to the desired one in the iteration. Since the gradient estimation for high-
dimensional data is difﬁcult in general, the iterative redu ction is expected to give more accurate
estimation. We call the method in [11] iterative average derivative estimates (IADE) in the sequel.

2.2 Kernel method for estimating gradient of regression
For a set Ω, a (R-valued) positive deﬁnite kernel k on Ω is a symmetric kernel k : Ω × Ω → R
such that Pn
i,j=1 ci cj k(xi , xj ) ≥ 0 for any x1 , . . . , xn in Ω and c1 , . . . , cn ∈ R. It is known that
a positive deﬁnite kernel on Ω uniquely deﬁnes a Hilbert space H consisting of functions on Ω,
in which the reproducing property hf , k(·, x)iH = f (x) (∀f ∈ H) holds, where h·, ·iH is the inner
product of H. The Hilbert space H is called the reproducing kernel Hilbert space (RKHS) associated
with k . We assume that an RKHS is always separable.

2

.

(2)

In deriving a kernel method based on the approach in Sec. 2.1, the fundamental tool is the repro-
ducing property for the derivative of a function. It is known (e.g., [21] Sec. 4.3) that if a positive
deﬁnite kernel k(x, y) on an open set in the Euclidean space is continuously differentiable with re-
spect to x and y , every f in the corresponding RKHS H is continuously differentiable. If further
∂
∂x k(·, x) ∈ H, we have
k(·, x)EH
= Df ,
∂
∂ f
∂x
∂x
This reproducing property combined with the following kernel estimator of the conditional expec-
tation (see [8, 9, 19] for details) will provide a method for dimension reduction. Let (X, Y ) be a
random variable on X × Y with probability P . We always assume that the p.d.f. p(x, y) and the
conditional p.d.f. p(y |x) exist, and that a positive deﬁnite kernel is measurable and b ounded. Let kX
and kY be positive deﬁnite kernels on X and Y , respectively, with respective RKHS HX and HY .
The (uncentered) covariance operator CY X : HX → HY is deﬁned by the equation
hg , CY X f iHY = E [f (X )g(Y )] = E (cid:2)hf , ΦX (X )iHX hΦY (Y ), giHY (cid:3)
(3)
for all f ∈ HX , g ∈ HY , where ΦX (x) = kX (·, x) and ΦY (y) = kY (·, y). Similarly, CXX
denotes the operator on HX that satis ﬁes
hf2 , CXX f1 i = E [f2 (X )f1 (X )] for any f1 , f2 ∈ HX .
These deﬁnitions are straightforward extensions of the ord inary covariance matrices, if we con-
sider the covariance of the random vectors ΦX (X ) and ΦY (Y ) on the RKHSs. One of the advan-
tages of the kernel method is that estimation with ﬁnite data is straightforward. Given i.i.d. sample
(X1 , Y1 ), . . . , (Xn , Yn ) with law P , the covariance operator is estimated by
n Pn
n Pn
bC (n)
bC (n)
Y X f = 1
XX f = 1
i=1kX (·, Xi )hkX (·, Xi ), f iHX . (4)
i=1kY (·, Yi )hkX (·, Xi ), f iHX
It is known [8] that if E [g(Y )|X = ·] ∈ HX holds for g ∈ HY , then we have CXX E [g(Y )|X =
·] = CX Y g . If further CXX is injective1 , this relation can be expressed as
E [g(Y )|X = ·] = CXX −1CX Y g .
While the assumption E [g(Y )|X = ·] ∈ HX may not hold in general, we can nonetheless obtain an
empirical estimator based on Eq. (5), namely,
XX + εn I )−1 bC (n)
( bC (n)
X Y g ,
where εn is a regularization coefﬁcient in Tikhonov-type regulariz ation. Note that the above expres-
sion is the kernel ridge regression of g(Y ) on X . As we discuss in Supplements, we can in fact
prove rigorously that this estimator converges to E [g(Y )|X = ·].
Assume now that X = Rm , CXX is injective, kX (x, ˜x) is continuously differentiable, E [g(Y )|X =
x] ∈ HX for any g ∈ HY , and ∂
∂x kX (·, x) ∈ R(CXX ), where R denotes the range of the operator.
XX CX Y g , ∂ kX (·,x)
∂ kX (·,x)
∂x E [g(Y )|X = x] = hC −1
i = hg , CY X C −1
From Eqs. (5) and (2), ∂
i.
XX
∂x
∂x
With g = kY (·, ˜y), we obtain the gradient of regression of the feature vector ΦY (Y ) on X as
∂ kX (·, x)
∂
E [ΦY (Y )|X = x] = CY X C −1
.
XX
∂x
∂x

(5)

(6)

2.3 Gradient-based kernel method for linear feature extraction

It follows from the same argument as in Sec. 2.1 that ∂
∂x E [kY (·, y)|X = x] = Ξ(x)B with an
operator Ξ(x) from Rm to HY , where we use a slight abuse of notation by identifying the operator
Ξ(x) with a matrix. In combination with Eq. (6), we have
EHX
B T hΞ(x), Ξ(x)iHY B = D ∂ kX (·, x)
∂ kX (·, x)
XX CX Y CY X C −1
, C −1
XX
∂x
∂x
which shows that the eigenvectors for non-zero eigenvalues of m × m matrix M (x) are contained
in the EDR space. This fact is the basis of our method. In contrast to the conventional gradient-
based method described in Sec. 2.1, this method incorporates high (or inﬁnite) dimensional regressor
E [ΦY (Y )|X = x].
1Noting hCXX f , f i = E [f (X )2 ], it is easy to see that CXX is injective, if kX is a continuous kernel on a
topological space X , and PX is a Borel probability measure such that P (U ) > 0 for any open set U in X .

=: M (x),

(7)

3

Given i.i.d. sample (X1 , Y1 ), . . . , (Xn , Yn ) from the true distribution, based on the empirical covari-
ance operators Eq. (4) and regularized inversions, the matrix M (x) is estimated by
XX + εn I (cid:1)−1 bC (n)
, (cid:0) bC (n)
XX + εn I (cid:1)−1 ∂ kX (·,x)
Y X (cid:0) bC (n)
(cid:11)
cMn (x) = (cid:10) ∂ kX (·,x)
X Y bC (n)
∂x
∂x
= ∇kX (x)T (GX + nεn I )−1GY (GX + nεn I )−1∇kX (x),
(8)
where GX and GY are the Gram matrices (kX (Xi , Xj )) and (kY (Yi , Yj )), respectively, and
∇kX (x) = (∂ kX (X1 , x)/∂x, · · · , ∂ kX (Xn , x)/∂x)T ∈ Rn .
As the eigenvectors of M (x) are contained in the EDR space for any x, we propose to use the
average of M (Xi ) over all the data points Xi , and deﬁne
n Pn
n Pn
i=1 cMn (Xi ) = 1
˜Mn := 1
i=1∇kX (Xi )T (GX + nεn In )−1GY (GX + nεn In )−1∇kX (Xi ).
We call the dimension reduction with the matrix ˜Mn the gradient-based kernel dimension reduction
(gKDR). For linear feature extraction, the projection matrix B in Eq. (1) is then estimated simply
by the top d eigenvectors of ˜Mn . We call this method gKDR-FEX.
The proposed method applies to a wide class of problems; in contrast to many existing methods,
the gKDR-FEX can handle any type of data for Y including multinomial or structured variables,
and make no strong assumptions on the regressor or distribution of X . Additionally, since the
gKDR incorporates the high dimensional feature vector ΦY (Y ), it works for any regression relation
including multiplicative noise, for which many existing methods such as SIR and IADE fail.

As in all kernel methods, the results of gKDR depend on the choice of kernels. We use the cross-
validation (CV) for choosing kernels and parameters, combined with some regression or classi ﬁca-
tion method. In this paper, the k-nearest neighbor (kNN) regression / classi ﬁcation is used in CV
for its simplicity: for each candidate of a kernel or parameter, we compute the CV error by the kNN
method with (B T Xi , Yi ), where B is given by gKDR, and choose the one that gives the least error.
The time complexity of the matrix inversions and the eigendecomposition for gKDR are O(n3 ),
which is prohibitive for large data sets. We can apply, however, low-rank approximation of Gram
matrices, such as incomplete Cholesky decomposition. The space complexity may be also a problem
of gKDR, since (∇kX (Xi ))n
i=1 has n2 × m dimension.
In the case of Gaussian kernel, where
∂
∂xa kX (Xj , x)|x=Xi = 1
i ) exp(−kXj − Xi k2 /(2σ2 )), we have a way of reducing
σ2 (X a
j − X a
the necessary memory by low rank approximation. Let GX ≈ RRT and GY ≈ HH T be the
low rank approximation with rx = rkR, ry = rkH (rx , ry < n, m). With the notation F :=
i = 1
(GX + nεn In )−1H and Θas
i Ris , we have, for 1 ≤ a, b ≤ m,
σ2 X a
i (cid:0)Pn
s=1Ris (cid:0)Pn
j=1Rj sFj t (cid:1).
j Fj t (cid:1) − Prx
˜Mn,ab = Pn
i=1 Pry
ia = Prx
t=1Γt
iaΓt
ib , Γt
j=1Θas
s=1Θas
With this method, the complexity is O(nmr) in space and O(nm2 r) in time (r = max{rx , ry }),
which is much more efﬁcient in memory than straightforward i mplementation.

We introduce two variants of gKDR-FEX. First, since accurate nonparametric estimation with high-
dimensional X is not easy, we propose a method for decreasing the dimensionality iteratively. Using
gKDR-FEX, we ﬁrst ﬁnd a matrix B1 of dimensionality d1 larger than the target d, project data Xi
onto the subspace as Z (1)
1 Xi , ﬁnd the projection matrix B2 (d1 × d2 matrix) for Z (1)
onto a
i = B T
i
d2 (d2 < d1 ) dimensional subspace, and repeat this process. We call this method gKDR-FEXi.
Second, if Y takes only L points as in classi ﬁcation, the Gram matrix GY and thus ˜Mn are of rank L
at most (see Eq. (8)), which is a strong limitation of gKDR. Note that this problem is shared by many
linear dimension reduction methods including CCA and slice-based methods. To solve this problem,
we propose to use the variation of cMn (x) over the points x = Xi instead of the average ˜Mn . By
partitioning {1, . . . , n} into T1 , . . . , T` , the projection matrices bB[a] given by the eigenvectors of
cM[a] = Pi∈Ta cM (Xi ) are used to deﬁne bP = 1
` P`
a=1 bB[a] bB T
[a] . The estimator of B is then given
by the top d eigenvectors of bP . We call this method gKDR-FEXv.
2.4 Theoretical analysis of gKDR
We have derived the gKDR method based on the necessary condition of EDR space. The following
theorem shows that it is also sufﬁcient, if kY is characteristic. A positive deﬁnite kernel k on a

4

gKDR
-FEX
0.1989
0.1264
0.1500
0.0755
0.1919
0.1346

gKDR
-FEXi
0.1639
0.0995
0.1358
0.0750
0.2322
0.1372

gKDR
-FEXv
0.2002
0.1287
0.1630
0.0802
0.1930
0.1369

IADE
0.1372
0.0857
0.1690
0.0940
0.7724
0.7863

SIR II
0.2986
0.2077
0.3137
0.2129
0.7326
0.7167

KDR
0.2807
0.1175
0.2138
0.1440
0.1479
0.0897

gKDR-FEX
+KDR
0.0883
0.0501
0.1076
0.0506
0.1285
0.0893

(A) n = 100
(A) n = 200
(B) n = 100
(B) n = 200
(C) n = 200
(C) n = 400

Table 1: gKDE-FEX for synthetic data: mean discrepancies over 100 runs.

measurable space is characteristic if EP [k(·, X )] = EQ [k(·, X )] means P = Q, i.e., the mean of
feature vector uniquely determines a probability [9, 20]. Examples include Gaussian kernel.
In the following theoretical results, we assume (i) ∂ kX (·, x)/∂xa ∈ R(CXX ) (a = 1, . . . , m), (ii)
E [kY (y , X )|X = ·] ∈ HX for any y ∈ Y , and (iii) E [g(Y )|B T X = z ] is a differentiable function
of z for any g ∈ HY and the linear functional g 7→ ∂E [g(Y )|B T X = z ]/∂ z is continuous for any z .
In the sequel, the subspace spanned by the columns of B is denoted by Span(B ), and the Frobenius
norm of a matrix M by kM kF . The proofs are given in Supplements.
Theorem 1. In addition to the above assumptions (i)-(iii), assume that the kernel kY is character-
istic. If the eigenspaces for the non-zero eigenvalues of E [M (X )] are included in Span(B ), then Y
and X are conditionally independent given B T X .
We can obtain the rate of consistency for cMn (x) and ˜Mn .
Theorem 2. In addition to (i)-(iii), assume that ∂ kX (·,x)
∈ R(C β+1
XX ) (a = 1, . . . , m) for some
∂xa
β ≥ 0, and E [kY (y , Y )|X = ·] ∈ HX for every y ∈ Y . Then, for εn = n− max{ 1
1
3 ,
2β+2 } , we have
4β+4 } (cid:1)
cMn (x) − M (x) = Op (cid:0)n− min{ 1
3 , 2β+1
F ] < ∞ and ∂ kX (·,x)
If further E [kM (X )k2
for every x ∈ X as n → ∞.
∂xa
X kHX < ∞, then ˜Mn → E [M (X )] in the same order as above.
E kha
Note that, assuming that the eigenvalues of M (x) or E [M (X )] are all distinct, the convergence
of matrices implies the convergence of the eigenvectors [22], thus the estimator of gKDR-FEX is
consistent to the subspace given by the top eigenvectors of E [M (X )].

= C β+1
XX ha
x with

2.5 Experiments with gKDR-FEX
We always use the Gaussian kernel k(x, ˜x) = exp(− 1
2σ2 kx − ˜xk2 ) in the kernel method below. First
we use three synthetic data to verify the basic performance of gKDR-FEX(i,v). The data are gener-
ated by (A): Y = Z sin(√5Z )+W , Z = 1√5
(1, 2, 0, . . . , 0)T X , (B): Y = (Z 3
2 )+W ,
1 +Z2 )(Z1 −Z 3
Z1 = 1√2
(1, 1, 0, . . . , 0)T X, Z2 = 1√2
(1, −1, 0, . . . , 0)T X , where 10-dimensional X is generated
by the uniform distribution on [−1, 1]10 and W is independent noise with N (0, 10−2 ), and (C):
Y = Z 4E , Z = (1, 0, . . . , 0)T X , where each component of 10-dimensional X is independently
generated by the truncated normal distribution N (0, 1/4) ∗ I[−1,1] and E ∼ N (0, 1) is a multi-
plicative noise. The discrepancy between the estimator B and the true projector B0 is measured by
0 (Im − BB T )kF /d. For choosing the parameter σ in Gaussian kernel and the regularization
kB0B T
parameter εn , the CV in Sec. 2.3 with kNN (k = 5, manually chosen to optimize the results) is
used with 8 different values given by cσmed (0.5 ≤ c ≤ 10), where σmed is the median of pairwise
distances of data [10], and ` = 4, 5, 6, 7 for εn = 10−` (a similar strategy is used for the CV below).
We compare the results with those of IADE, SIR II [13], and KDR. The IADE has seven parameters
[11], and we tuned two of them (h1 and ρmin ) manually to optimize the performance. For SIR II,
we tried several numbers of slices, and chose the one that gave the best result. From Table 1, we see
that gKDR-FEX(i,v) show much better results than SIR II in all the cases. The IADE works better
than these methods for (A), while for (B) and (C) it works worse. Since (C) has multiplicative noise,
the IADE does not obtain meaningful estimation. The KDR attains higher accuracy for (C), but less
accurate for (A) and (B) with n = 100; this undesired result is caused by failure of optimization in

5

)
%
(
 
e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
l
C

95

90

85

80

75

70
 

95

90

85

)
%
(
 
e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
l
C

80
 

gKDR−v
KDR
All variables

 

gKDR−v
KDR
All variables

30

 

100

 

gKDR−v
KDR
All variables

85

80

75

)
%
(
 
e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
l
C

70
 

3

5

11

3 5

10

13

7
9
15
20
15
20
Dimensionality
Dimensionality
Dimensionality
(H) Heart Disease
(I) Ionoshpere
(B) Breast-cancer-Wisconsin
(m:13, ntr :129, ntest :148)
(m:34, ntr :151, ntest :200)
(m:30, ntr :200, ntest :369)
Figure 1: Classi ﬁcation accuracy with gKDR-v and KDR for bin ary classi ﬁcation problems. m, ntr
and ntest are the dimension of X , training data size, and testing data size, respectively.

3 5

34

10

10
Dim.
13.53
gKDR + kNN
gKDR-v + kNN
13.15
22.77
CCA + kNN
SIR-II + kNN
77.42
gKDR + SVM 14.43
gKDR-v + SVM 16.87
13.09
CCA + SVM

20
4.55
4.55
6.74
70.11
5.00
4.75
6.54

30

40

50

4.81

5.26

5.58

63.44

52.66

50.61

3.85

3.59

3.08

gKDR
+SVM

12.0
16.2
18.0
21.8
19.5

Corr
+SVM
(500)
15.7
30.2
29.2
35.4
41.1

Corr
+SVM
(2000)
8.3
18.0
24.0
25.0
29.0

L

10
20
30
40
50

Table 2: Left: ISOLET - classi ﬁcation errors for test data (p ercentage). Right: Amazon Reviews -
10-fold cross-validation errors (%) for classi ﬁcation

some runs (see Supplements for error bars). We also used the results of gKDR-FEX as the initial
state for KDR, which improved the accuracy signi ﬁcantly for
(A) and (B). Note however that these
data sets are very small in size and dimension, and KDR is not applicable to large data used later.

One way of evaluating dimension reduction methods in supervised learning is to consider the classi-
ﬁcation or regression accuracy after projecting data onto t he estimated subspaces. We next used three
data sets for binary classi ﬁcation, heart-disease (H), ionoshpere (I), and breast-cancer-Wisconsin
(B), from UCI repository [7], and evaluated the classi ﬁcati on rates of gKDR-FEXv with kNN clas-
si ﬁers ( k = 7). We compared them with KDR, as KDR shows high accuracy for small data sets.
From Fig. 1, we see gKDR-FEXv shows competitive accuracy with KDR: slightly worse for (I), and
slightly better for (B). The computation of gKDR-FEXv for these data sets can be much faster than
that of KDR. For each parameter set, the computational time of gKDR vs KDR was, in (H) 0.044
sec / 622 sec (d = 11), in (I) 0.l03 sec / 84.77 sec (d = 20), and in (B) 0.116 sec / 615 sec (d = 20).
The next two data sets taken from UCI repository are larger in the sample size and dimensionality,
for which the optimization of KDR is difﬁcult to apply. The ﬁr
st one is ISOLET, which provides
617 dimensional continuous features of speech signals to classify 26 alphabets. In addition to 6238
training data, 1559 test data are separately provided. We evaluate the classi ﬁcation errors with the
kNN classi ﬁer ( k = 5) and 1-vs-1 SVM to see the effectiveness of the estimated subspaces (see
Table 2). From the information on the data at the UCI repository, the best performance with neural
networks and C4.5 with ECOC are 3.27% and 6.61%, respectively. In comparison with these results,
the low dimensional subspaces found by gKDR-FEX and gKDR-FEXv maintain the information for
classi ﬁcation effectively. SIR-II does not ﬁnd meaningful
features.

The second data set is author identi ﬁcation of Amazon commer ce reviews with 10000 dimensional
linguistic features. The total number of authors is 50, and 30 reviews were collected for each author;
the total size of data is thus 1500. We varied the used number of authors (L) to make different levels
of difﬁculty for the tasks. The reduced dimensionality by gK DR-FEX is set to the same as L, and the
10-fold CV errors with data projected on the estimated EDR space are evaluated using 1-vs-1 SVM.
As comparison, the squared sum of variable-wise Pearson correlations, PL
`=1 Corr[X a , Y ` ]2 , is also
used for choosing explanatory variables (a = 1, . . . , 10000). Such variable selection methods with
Pearson correlation are popularly used for very high dimensional data. The variables with top 500
and 2000 correlations are used to make SVM classi ﬁers. As we can see fr om Table 2, the gKDR-
FEX gives much more effective subspaces for regression than the Pearson correlation method, when

6

–
–
–
–
–
–
–
–
–
–
–
–
the number of authors is large. The creator of the data set has also reported the classi ﬁcation result
with a neural network model [15]; for 50 authors, the 10-fold CV error with 2000 selected variables
is 19.51%, which is similar to the gKDR-FEX result with only 50 linear features.

3 Variable selection with gKDR

In recent years, extensive studies have been done on variable selection with a sparseness penalty
([6, 16, 18, 23 –27, 29, 30] among many others). In supervised setting, these studies often consider
some speci ﬁc model for the regression such as least square or
logistic regression. While consistency
and oracle property have been also established for many methods, the assumption that there is a true
parameter in the model may not hold in practice, and thus a strong restriction of the methods. It
is then important to consider more ﬂexible ways of variable s election without assuming any para-
metric model on the regression. The gKDR approach is appealing to this problem, since it realizes
conditional independence without strong assumptions for regression or distribution of variables.

Chen et al. [2] recently proposed the Coordinate-Independent Sparse (CIS) method, which is a semi-
parametric method for sparse variable selection. In CIS, the linear feature B T X is assumed with
some rows of B zero, but no parametric model is speci ﬁed for regression. We wish to estimate B so
that the zero-rows should be estimated as zeros. This is achieved by imposing the sparseness penalty
of the group LASSO [29] in combination with an objective function of linear feature extraction
written in the form of eigenproblem such as SIR and PFC [3].

We follow the CIS method for our variable selection with gKDR; since the gKDR is given by the
eigenproblem with matrix ˜Mn , the CIS method is applied straightforwardly. The signi ﬁca nce of our
method is that the gKDR formulates the conditional independence of Y and X given B T X , while
the existing CIS-based methods in [2] realize only weaker conditions under strong assumptions.

3.1 Sparse variable selection with gKDR

Throughout this section, it is assumed that the true probability satis ﬁes Eq. (1) with B = B0 =
0m )T , and with some 1 ≤ q ≤ m the j -th row v0j is non-zero for j ≤ q and v0j = 0
01 , . . . , vT
(vT
m )T , where bi is the i-th
for j ≥ q + 1. The projection matrix is B = (b1 , . . . , bd ) = (vT
1 , . . . , vT
column and vj is the j -th row. The proposed variable selection method, gKDR-VS, estimates B by
λikvi ki,
B :BT B=Id h−Tr[B T ˜MnB ] +
mXi=1
bBλ = arg min
where kvj k is the Euclidean norm and λ = (λ1 , . . . , λm ) ∈ Rm
+ is the regularization coefﬁcients.
To optimize Eq. (9), as in [2], we used the local quadratic approximation [6], which is simple and
fast. We used the matlab code provided at the homepage of X. Chen.

(9)

The choice of λ is crucial on the practical performance of sparse variable selection. As a theoretical
guarantee, we will show that some asymptotic condition provides model consistency. In practice, as
in the Adaptive Lasso [30], it is suitable to consider λ = λ(θ) deﬁne by
λi = θk ˜vik−r
where θ and r are positive numbers, and ˜vi is the row vector of ˜B0 , the solution to gKDR without
penalty, i.e., ˜B0 = arg minBT B=Id −Tr[B T ˜MnB ]. We used r = 1/2 for all of our experiments.
To choose the parameter θ , a BIC-based method is often used in sparse variable selection [27, 31]
with theoretical guarantee of model consistency. We use a BIC-type method for choosing θ by
minimizing
log n
˜Mn bBλ(θ) ] + Cn dfθ
BICθ = −Tr[ bB T
(10)
,
λ(θ)
n
where dfθ = d(p − d) is the degree of freedom of bBλ(θ) with p the number of non-zero rows in
bBλ(θ) , and Cn is a positive number of Op (1). We used Cn = α1 log log(m) with α1 is the largest
eigenvalue of ˜Mn . The log log(m) factor is used in [27], where increasing number of variables is
discussed, and α1 is introduced to adjust the scale of Tr[ bB T
˜Mn bBλ ]; we use CV for choosing the
λ
in which the values of Tr[ bB T
˜Mn bBλ ] is not
hyperparameters (kernel and regularization coefﬁcient),
λ
normalized well for different choices.

7

gKDR
-VS
.94/.99/75
1.0/1.0/98
.92/.84/63
.98/.89/75

CIS
-SIR
.89/1.0/65
.99/1.0/97
.19/.85/1
.18/.85/1

(A) n = 60
(A) n = 120
(B) n = 100
(B) n = 200

Table 3: gKDR-VS and CIS-SIR
with synthetic data (ratio of non-
zeros in 1 ≤ j ≤ q / ratio of ze-
ros in q + 1 ≤ j ≤ m / number of
correct models among 100 runs).

CIS-PFC
CIS-SIR
gKDR-VS
Method
0
0
0
0
0
0
CRIM
ZN
0
0
-0.008
-0.000
0
0
INDUS
0
0
0
0
0
0
0
0
0
0
0
0
CHAS
NOX
0
0
0
0
0
0
1.045
-1.390
-1.00
-1.253
0.896
0.393
RM
AGE
-0.011
-0.003
-0.022
0.005
0
0
0
0
0
0
0.022
-0.169
DIS
RAD
0
0
0
0
-0.000
0.018
TAX
-0.005
-0.001
-0.001
0.001
0
0
0.007
-0.038
0.003
0.049
0.919
-0.376
PTRATIO
B
0.001
0.005
-0.001
0.002
0
0
-0.113
-0.043
-0.114
0.043
0.017
-0.165
LSTAT
Table 4: Boston Housing Data: estimated sparse EDR.

3.2 Theoretical results on gKDR-VS

This subsection shows the model consistency of the gKDR-VS. All the proofs are shown in Supple-
ments. Let αn = max{λj | 1 ≤ j ≤ q} and βn = min{λj | q + 1 ≤ j ≤ m}. The eigenvalues of
M = E [M (X )] are η1 ≥ . . . ≥ ηm ≥ 0. For two m × d matrices Bi (i = 1, 2) with B T
i Bi = Id ,
2 k, where k · k is the operator norm.
we deﬁne D(B1 , B2 ) = kB1B T
1 − B2B T
Theorem 3. Suppose k ˜Mn − M kF = Op (n−τ ) for some τ > 0. If nτ αn → 0 as n → ∞ and
ηq > ηq+1 , then the estimator bBλ in Eq. (9) satis ﬁes D( bBλ , B0 ) = Op (n−τ ) as n → ∞.
We saw in Theorem 2 that under some conditions ˜Mn converges to M at the rate Op (n−τ ) with
1/4 ≤ τ ≤ 1/3. Thus Theorem 3 shows that bBλ is also consistent of the same rate.
Theorem 4. In addition to the assumptions in Theorem 3, assume nτ βn → ∞ as n → ∞. Then,
for all q + 1 ≤ j ≤ m, Pr(bvj = 0) → 1 as n → ∞, where bvj is the j -th row of bBλ .
3.3 Experiments with gKDR-VS
We ﬁrst apply the gKDR-VS with d = 1 to synthetic data generated by the following two models:
(A): Y = X 1 + X 2 + X 3 + W and (B): Y = (X 1 + X 2 + X 3 )4W , where the noise W follows
N (0, 1). For (A), X = (X 1 , . . . , X 24 ) is generated by N (0, Σ) with Σij = (1/2)|i−j | (1 ≤ i, j ≤
24), and for (B) X = (X 1 , . . . , X 10 ) by N (0, 4I10 ). Note that (B) includes multiplicative noise,
which cannot be handled by many dimension reduction methods. In comparison, the CIS method
with SIR is also applied to the same data. The regularization parameter of CIS-SIR is chosen by
BIC described in [2]. While both the methods work effectively for (A), only gKDR-VS can handle
the multiplicative noise of (C).

The next experiment uses Boston Housing data, which has been often used for variable selection.
The response Y is the median value of homes in each tract, and thirteen variables are used to explain
it. The detail of the variables is described in Supplements, Sec. E. The results of gKDR-VS and CIS-
SIR / CIS-PFC with d = 2 are shown in Table 4. The variables selected by gKDR-VS are RM, DIS,
RAD, PTRATIO and LSTAT, which are slightly different from the CIS methods. In a previous study
[1], the four variables RM, TAX, PTRATIO and LSTAT are considered to have major contribution.

4 Conclusions

We have proposed a gradient-based kernel approach for dimension reduction in supervised learn-
ing. The method is based on the general kernel formulation of conditional independence, and thus
has wide applicability without strong restrictions on the model or variables. The linear feature
extraction, gKDR-FEX, ﬁnds effective features with simple eigendecomposition, even when other
conventional methods are not applicable by multiplicative noise or high-dimensionality. The con-
sistency is also guaranteed. We have extended the method to variable selection (gKDR-VS) with a
sparseness penalty, and demonstrated its promising performance with synthetic and real world data.
The model consistency has been also proved.
Acknowledgements. KF has been supported in part by JSPS KAKENHI (B). 22300098.

8

References

[1] L. Breiman and J. Friedman. Estimating optimal transformations for multiple regression and correlation.
J. Amer. Stat. Assoc., 80:580–598, 1985.
[2] X. Chen, C. Zou, and R. Dennis Cook. Coordinate-independent sparse sufﬁcient dimension reduction and
variable selection. Ann. Stat., 38(6):3696–3723, 2010.
[3] R. Dennis Cook and L. Forzani. Principal ﬁtted components for dime nsion reduction in regression. Sta-
tistical Science, 23(4):485–501, 2008.
[4] R. Dennis Cook and S. Weisberg. Discussion of Li (1991). J. Amer. Stat. Assoc., 86:328–332, 1991.
[5] J. Fan and I. Gijbels. Local Polynomial Modelling and its Applications. Chapman and Hall, 1996.
[6] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Amer.
Stat. Assoc., 96(456):1348–1360, 2001.
[7] A. Frank and A. Asuncion. UCI machine learning repository, [http://archive.ics.uci.edu/ml]. Irvine, CA:
University of California, School of Information and Computer Science. 2010.
[8] K. Fukumizu, F.R. Bach, and M.I. Jordan. Dimensionality reduction for supervised learning with repro-
ducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.
[9] K. Fukumizu, F.R. Bach, and M.I. Jordan. Kernel dimension reduction in regression. Ann. Stat.,
37(4):1871–1905, 2009.
[10] A. Gretton, K. Fukumizu, C.H. Teo, L. Song, B. Sch ¨olkopf, and Alex Smola. A kernel statistical test of
independence. In Advances in NIPS 20, pages 585–592. 2008.
[11] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduc-
tion. Ann. Stat., 29(6):1537–1566, 2001.
[12] B. Li, H. Zha, and F. Chiaromonte. Contour regression: A general approach to dimension reduction. Ann.
Stat., 33(4):1580–1616, 2005.
[13] K.-C. Li. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat. Assoc.,
86:316–342, 1991.
[14] K.-C. Li. On principal Hessian directions for data visualization and dimension reduction: Another appli-
cation of Stein’s lemma. J. Amer. Stat. Assoc., 87:1025–1039, 1992.
[15] S. Liu, Z. Liu, J. Sun, and L. Liu. Application of synergetic neural network in online writeprint identiﬁ-
cation. Intern. J. Digital Content Technology and its Applications, 5(3):126–135, 2011.
[16] L. Meier, S. Van De Geer, and P. B ¨uhlmann. The group lasso for logistic regression. J. Royal Stat. Soc.:
Ser. B, 70(1):53–71, 2008.
[17] A.M. Samarov. Exploring regression structure using nonparametric functional estimation. J. Amer. Stat.
Assoc., 88(423):836–847, 1993.
[18] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm fo r gene selection using sparse logistic
regression. Bioinformatics, 19(17):2246–2253, 2003.
[19] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions
with applications to dynamical systems. In Proc. ICML2009, pages 961–968. 2009.
[20] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch ¨olkopf, and G.R.G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. JMLR, 11:1517–1561, 2010.
[21] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
[22] G.W. Stewart and J.-Q. Sun. Matrix Perturbation Theory. Academic Press, 1990.
[23] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc.: Ser. B, 58(1):pp.
267–288, 1996.
[24] H. Wang and C. Leng. Uniﬁed lasso estimation by least squares app roximation. J. Amer. Stat. Assoc., 102
(479):1039–1048, 2007.
[25] H. Wang, G. Li, and C.-L. Tsai. Regression coefﬁcient and auto regressive order shrinkage and selection
via the lasso. J. Royal Stat. Soc.: Ser. B, 69(1):63–78, 2007.
[26] H. Wang, G. Li, and C.-L. Tsai. On the consistency of SCAD tunign parameter selector. Biometrika, 94:
553–558, 2007.
[27] H. Wang, B. Li, and C. Leng. Shrinkage tuning parameter selection with a diverging number of parame-
ters. J. Royal Stat. Soc.: Ser. B, 71(3):671–683, 2009.
[28] M. Wang, F. Sha, and M. Jordan. Unsupervised kernel dimension reduction. NIPS 23, pages 2379–2387.
2010.
[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Royal Stat.
Soc.: Ser. B, 68(1):49–67, 2006.
[30] H. Zou. The adaptive lasso and its oracle properties. J. Amer. Stat. Assoc., 101:1418–1429, 2006.
[31] C. Zou and X. Chen. On the consistency of coordinate-independent sparse estimation with BIC. J. Multi-
variate Analysis, 112:248–255, 2012.

9


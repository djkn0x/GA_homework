Privacy Aware Learning

Martin J. Wainwright1,2
Michael I. Jordan1,2
John C. Duchi1
1Department of Electrical Engineering and Computer Science, 2Department of Statistics
University of California, Berkeley
Berkeley, CA USA 94720
{jduchi,jordan,wainwrig}@eecs.berkeley.edu

Abstract

We study statistical risk minimization problems under a version of privacy in
which the data is kept conﬁdential even from the learner. In this local privacy
framework, we establish sharp upper and lower bounds on the convergence rates
of statistical estimation procedures. As a consequence, we exhibit a precise trade-
off between the amount of privacy the data preserves and the utility, measured by
convergence rate, of any statistical estimator.

1

Introduction

There are natural tensions between learning and privacy that arise whenever a learner must aggregate
data across multiple individuals. The learner wishes to make optimal use of each data point, but the
providers of the data may wish to limit detailed exposure, either to the learner or to other individuals.
It is of great interest to characterize such tensions in the form of quantitative tradeoffs that can be
both part of the public discourse surrounding the design of systems that learn from data and can be
employed as controllable degrees of freedom whenever such a system is deployed.

We approach this problem from the point of view of statistical decision theory. The decision-
theoretic perspective offers a number of advantages. First, the use of loss functions and risk functions
provides a compelling formal foundation for deﬁning “learning,” one that dates back to Wald [28] in
the 1930’s, and which has seen continued development in the context of research on machine learn-
ing over the past two decades. Second, by formulating the goals of a learning system in terms of
loss functions, we make it possible for individuals to assess whether the goals of a learning system
align with their own personal utility, and thereby determine the extent to which they are willing to
sacriﬁce some privacy. Third, an appeal to decision theory permits abstraction over the details of
speciﬁc learning procedures, providing (under certain conditions) minimax lower bounds that apply
to any speciﬁc procedure. Finally, the use of loss functions, in particular convex loss functions, in
the design of a learning system allows powerful tools of optimization theory to be brought to bear.
In more formal detail, our framework is as follows. Given a compact convex set Θ ⊂ Rd , we
wish to ﬁnd a parameter value θ ∈ Θ achieving good average performance under a loss function
ℓ : X × Rd → R+ . Here the value ℓ(X, θ) measures the performance of the parameter vector θ ∈ Θ
on the sample X ∈ X , and ℓ(x, ·) : Rd → R+ is convex for x ∈ X . We measure the expected
performance of θ ∈ Θ via the risk function
R(θ) := E[ℓ(X, θ)].

(1)

In the standard formulation of statistical risk minimization, a method M is given n samples
X1 , . . . , Xn , and outputs an estimate θn approximately minimizing R(θ). Instead of allowing M
access to the samples Xi , however, we study the effect of giving only a perturbed view Zi of each
datum Xi , quantifying the rate of convergence of R(θn ) to inf θ∈Θ R(θ) as a function of both the
number of samples n and the amount of privacy Zi provides for Xi .

1

There is a long history of research at the intersection of privacy and statistics, where there is a natural
competition between maintaining the privacy of elements in a dataset {X1 , . . . , Xn } and the output
of statistical procedures. Study of this issue goes back at least to the 1960s, when Warner [29]
suggested privacy-preserving methods for survey sampling. Recently, there has been substantial
work on privacy—focusing on a measure known as differential privacy [12]—in statistics, computer
science, and other ﬁelds. We cannot hope to do justice to the large body of related work, referring
the reader to the survey by Dwork [10] and the statistical framework studied by Wasserman and
Zhou [30] for background and references.
In this paper, we study local privacy [13, 17], in which each datum Xi is kept private from the
method M. The goal of many types of privacy is to guarantee that the output bθn of the method M
based on the data cannot be used to discover information about the individual samples X1 , . . . , Xn ,
but locally private algorithms access only disguised views of each datum Xi . Local algorithms
are among the most classical approaches to privacy, tracing back to Warner’s work on randomized
response [29], and rely on communication only of some disguised view Zi of each true sample Xi .
Locally private algorithms are natural when the providers of the data—the population sampled to
give X1 , . . . , Xn—do not trust even the statistician or statistical method M, but the providers are
interested in the parameters θ∗ minimizing R(θ). For example, in medical applications, a participant
may be embarrassed about his use of drugs, but if the loss ℓ is able to measure the likelihood of
developing cancer, the participant has high utility for access to the optimal parameters θ∗ . In essence,
we would like the statistical procedure M to learn from the data X1 , . . . , Xn but not about it.
Our goal is to understand the fundamental tradeoffs between maintaining privacy while still retain-
ing the utility of the statistical inference method M. Though intuitively there must be some tradeoff,
quantifying it precisely has been difﬁcult. In the machine learning literature, Chaudhuri et al. [7]
develop differentially private empirical risk minimization algorithms, and Dwork and Lei [11] and
Smith [26] analyze similar statistical procedures, but do not show that there must be negative effects
of privacy. Rubinstein et al. [24] are able to show that it is impossible to obtain a useful parameter
vector θ that is substantially differentially private; it is unclear whether their guarantees are improv-
able. Recent work by Hall et al. [15] gives sharp minimax rates of convergence for differentially
private histogram estimation. Blum et al. [5] also give lower bounds on the closeness of certain
statistical quantities computed from the dataset, though their upper and lower bounds do not match.
Sankar et al. [25] provide rate-distortion theorems for utility models involving information-theoretic
quantities, which has some similarity to our risk-based framework, but it appears challenging to
map their setting onto ours. The work most related to ours is probably that of Kasiviswanathan et al.
[17], who show that that locally private algorithms coincide with concepts that can be learned with
polynomial sample complexity in Kearns’s statistical query (SQ) model. In contrast, our analysis
addresses sharp rates of convergence, and applies to estimators for a broad class of convex risks (1).

2 Main results and approach

Our approach to local privacy is based on a worst-case measure of mutual information, where we
view privacy preservation as a game between the providers of the data—who wish to preserve
privacy—and nature. Recalling that the method sees only the perturbed version Zi of Xi , we adopt
a uniform variant of the mutual information I (Zi ; Xi ) between the random variables Xi and Zi
as our measure for privacy. This use of mutual information is by no means original [13, 25], but
because standard mutual information has deﬁciencies as a measure of privacy [e.g. 13], we say the
distribution Q generating Z from X is private only if I (X ; Z ) is small for all possible distributions
P on X (possibly subject to constraints). This is similar to the worst-case information approach of
Evﬁmievski et al. [13], which limits privacy breaches. (In the long version of this paper [9] we also
consider differentially private algorithms.)

The central consequences of our main results are, under standard conditions on the loss functions ℓ,
sharp upper and lower bounds on the possible convergence rates for estimation procedures when we
wish to guarantee a level of privacy I (Xi ; Zi ) ≤ I ∗ . We show there are problem dependent constants
a(Θ, ℓ) and b(Θ, ℓ) such that the rates of convergence of all possible procedures are lower bounded
by a(Θ, ℓ)/√nI ∗ and that there exist procedures achieving convergence rates of b(Θ, ℓ)/√nI ∗ ,
where the ratio b(Θ, ℓ)/a(Θ, ℓ) is upper bounded by a universal constant. Thus, we establish and
quantify explicitly the tradeoff between statistical estimation and the amount of privacy.

2

We show that stochastic gradient descent is one procedure that achieves the optimal convergence
rates, which means additionally that our upper bounds apply in streaming and online settings, re-
quiring only a ﬁxed-size memory footprint. Our subsequent analysis builds on this favorable prop-
erty of gradient-based methods, whence we focus on statistical estimation procedures that access
data through the subgradients of the loss functions ∂ ℓ(X, θ). This is a natural restriction. Gradients
of the loss ℓ are asymptotically sufﬁcient [18] (in an asymptotic sense, gradients contain all of the
statistical information for risk minimization problems), stochastic gradient-based estimation proce-
dures are (sample) minimax optimal and Bahadur efﬁcient [23, 1, 27, Chapter 8], many estimation
procedures are gradient-based [20, 6], and distributed optimization procedures that send gradient
information across a network to a centralized procedure M are natural [e.g. 3]. Our mechanism
gives M access to a vector Zi that is a stochastic (sub)gradient of the loss evaluated on the sample
Xi at a parameter θ of the method’s choosing:
E[Zi | Xi , θ ] ∈ ∂ ℓ(Xi , θ),
(2)
where ∂ ℓ(Xi , θ) denotes the subgradient set of the function θ 7→ ℓ(Xi , θ). In a sense, the unbiased-
ness of the subgradient inclusion (2) is information-theoretically necessary [1].

To obtain upper and lower bound on the convergence rate of estimation procedures, we provide a
two-part analysis. One part requires studying saddle points of the mutual information I (X ; Z ) (as a
function of the distributions P of X and Q(· | X ) of Z ) under natural constraints that allow inference
of the optimal parameters θ∗ for the risk R. We show that for certain classes of loss functions ℓ and
constraints on the communicated version Zi of the data Xi , there is a unique distribution Q(· | Xi )
that attains the smallest possible mutual information I (X ; Z ) for all distributions on X . Using this
unique distribution, we can adapt information-theoretic techniques for obtaining lower bounds on
estimation [31, 1] to derive our lower bounds. The uniqueness results for the conditional distribution
Q show that no algorithm guaranteeing privacy between M and the samples Xi can do better. We
can obtain matching upper bounds by application of known convergence rates for stochastic gradient
and mirror descent algorithms [20, 21], which are computationally efﬁcient.

3 Optimal learning rates and tradeoffs

Having outlined our general approach, we turn in this section to providing statements of our main
results. Before doing so, we require some formalization of our notions of privacy and error measures,
which we now provide.

3.1 Optimal Local Privacy

We begin by describing in slightly more detail the communication protocol by which information
about the random variables X is communicated to the procedure M. We assume throughout that
there exist two d-dimensional compact sets C, D , where C ⊂ int D ⊂ Rd , and we have that
∂ ℓ(x, θ) ⊂ C for all θ ∈ Θ and x ∈ X . We wish to maximally “disguise” the random variable
X with the random variable Z satisfying Z ∈ D . Such a setting is natural; indeed, many online
optimization and stochastic approximation algorithms [34, 21, 1] assume that for any x ∈ X and
θ ∈ Θ, if g ∈ ∂ ℓ(x, θ) then kgk ≤ L for some norm k·k. We may obtain privacy by allowing a
perturbation to the subgradient g so long as the perturbation lives in a (larger) norm ball of radius
M ≥ L, so that C = {g ∈ Rd : kgk ≤ L} ⊂ D = {g ∈ Rd : kgk ≤ M }.
Now let X have distribution P , and for each x ∈ X , let Q(· | x) denote the regular conditional
probability measure of Z given that X = x. Let Q(·) denote the marginal probability deﬁned by
Q(A) = EP [Q(A | X )]. The mutual information between X and Z is the expected Kullback-
Leibler (KL) divergence between Q(· | X ) and Q(·):
I (P , Q) = I (X ; Z ) := EP [Dkl (Q(· | X )||Q(·))] .
We view the problem of privacy as a game between the adversary controlling P and the data owners,
who use Q to obscure the samples X . In particular, we say a distribution Q guarantees a level of
privacy I ∗ if and only if supP I (P , Q) ≤ I ∗ . (Evﬁmievski et al. [13, Deﬁnition 6] present a similar
condition.) Thus we seek a saddle point P ∗ , Q∗ such that
I (P ∗ , Q),
I (P , Q∗ ) ≤ I (P ∗ , Q∗ ) ≤ inf
Q

(3)

(4)

sup
P

3

where the ﬁrst supremum is taken over all distributions P on X such that ∇ℓ(X, θ) ∈ C with
P -probability 1, and the inﬁmum is taken over all regular conditional distributions Q such that if
Z ∼ Q(· | X ), then Z ∈ D and EQ [Z | X, θ ] = ∇ℓ(X, θ). Indeed, if we can ﬁnd P ∗ and Q∗
satisfying the saddle point (4), then the trivial direction of the max-min inequality yields
I (P , Q) = I (P ∗ , Q∗ ) = inf
Q

I (P , Q).

inf
Q

sup
P

sup
P

(5a)

(5b)

To fully formalize this idea and our notions of privacy, we deﬁne two collections of probability
measures and associated losses. For sets C ⊂ D ⊂ Rd , we deﬁne the source set
P (C ) := {Distributions P such that supp P ⊂ C }
and the set of regular conditional distributions (r.c.d.’s), or communicating distributions,
Q (C, D) := nr.c.d.’s Q s.t. supp Q(· | c) ⊂ D and ZD
zdQ(z | c) = c for c ∈ C o.
The deﬁnitions (5a) and (5b) formally deﬁne the sets over which we may take inﬁma and suprema
in the saddle point calculations, and they capture what may be communicated. The condi-
tional distributions Q ∈ Q (C, D) are deﬁned so that if ∇ℓ(x, θ) ∈ C then EQ [Z | X, θ ] :=
RD zdQ (z | ∇ℓ(x, θ)) = ∇ℓ(x, θ). We now make the following key deﬁnition:
Deﬁnition 1. The conditional distribution Q∗ satisﬁes optimal
local privacy for the sets
C ⊂ D ⊂ Rd at level I ∗ if
I (P , Q) = I ∗ ,
I (P , Q∗ ) = inf
sup
sup
Q
P
P
where the supremum is taken over distributions P ∈ P (C ) and the inﬁmum is taken over regular
conditional distributions Q ∈ Q (C, D).
If a distribution Q∗ satisﬁes optimal local privacy, then it guarantees that even for the worst possible
distribution on X , the information communicated about X is limited.
In a sense, Deﬁnition 1
captures the natural competition between privacy and learnability. The method M speciﬁes the
set D to which the data Z it receives must belong; the “teachers,” or owners of the data X , choose
the distribution Q to guarantee as much privacy as possible subject to this constraint. Using this
mechanism, if we can characterize a unique distribution Q∗ attaining the inﬁmum (4) for P ∗ (and
by extension, for any P ), then we may study the effects of privacy between the method M and X .
3.2 Minimax error and loss functions

Having deﬁned our privacy metric, we now turn to our original goal: quantiﬁcation of the effect
privacy has on statistical estimation rates. Let M denote any statistical procedure or method (that
uses n stochastic gradient samples) and let θn denote the output of M after receiving n such samples.
Let P denote the distribution according to which samples X are drawn. We deﬁne the (random) error
of the method M on the risk R(θ) = E[ℓ(X, θ)] after receiving n sample gradients as
ǫn (M, ℓ, Θ, P ) := R(θn ) − inf
EP [ℓ(X, θ)].
R(θ) = EP [ℓ(X, θn )] − inf
θ∈Θ
θ∈Θ

(6)

In our settings, in addition to the randomness in the sampling distribution P , there is additional
randomness from the perturbation applied to stochastic gradients of the objective ℓ(X, ·) to mask X
from the statistitician. Let Q denote the regular conditional probability—the channel distribution—
whose conditional part is deﬁned on the range of the subgradient mapping ∂ ℓ(X, ·). As the output
θn of the statistical procedure M is a random function of both P and Q, we measure the expected
sub-optimality of the risk according to both P and Q. Now, let L be a collection of loss functions,
where L(P ) denotes the losses ℓ : supp P × Θ → R belonging to L. We deﬁne the minimax error
ǫ∗
sup
n (L, Θ) := inf
EP,Q [ǫn (M, ℓ, Θ, P )],
(7)
M
ℓ∈L(P ),P
where the expectation is taken over the random samples X ∼ P and Z ∼ Q(· | X ). We characterize
the minimax error (7) for several classes of loss functions L(P ), giving sharp results when the
privacy distribution Q satisﬁes optimal local privacy.

We assume that our collection of loss functions obey certain natural smoothness conditions, which
are often (as we see presently) satisﬁed. We deﬁne the class of losses as follows.

4

(8)

Deﬁnition 2. Let L > 0 and p ≥ 1. The set of (L, p)-loss functions are those measurable functions
ℓ : X × Θ → R such that x ∈ X , the function θ 7→ ℓ(x, θ) is convex and
|ℓ(x, θ) − ℓ(x, θ ′ )| ≤ L kθ − θ ′kq
for any θ , θ ′ ∈ Θ, where q is the conjugate of p: 1/p + 1/q = 1.
A loss ℓ satisﬁes the condition (8) if and only if for all θ ∈ Θ, we have the inequality kgkp ≤ L for
any subgradient g ∈ ∂ ℓ(x, θ) (e.g. [16]). We give a few standard examples of such loss functions.
First, we consider ﬁnding a multi-dimensional median, in which case the data x ∈ Rd and ℓ(x, θ) =
L kθ − xk1 . This loss is L-Lipschitz with respect to the ℓ1 norm, so it belongs to the class of (L, ∞)
losses. A second example includes classiﬁcation problems, using either the hinge loss or logistic
regression loss. In these cases, the data comes in pairs x = (a, b), where a ∈ Rd is the set of
regressors and b ∈ {−1, 1} is the label; the losses are
ℓ(x, θ) = [1 − b ha, θi]+ or ℓ(x, θ) = log (1 + exp(−b ha, θi))
By computing (sub)gradients, we may verify that each of these belong to the class of (L, p)-losses
if and only if the data a satisﬁes kakp ≤ L, which is a common assumption [7, 24].
The privacy-guaranteeing channel distributions Q∗ we construct in Section 4 are motivated by our
concern with the (L, p) families of loss functions. In our model of computation, the learning method
M queries the loss ℓ(Xi , ·) at the point θ ; the owner of the datum Xi then computes the subgradient
∂ ℓ(Xi , θ) and returns a masked version Zi with the property that E[Zi | Xi , θ ] ∈ ∂ ℓ(Xi , θ). In
the following two theorems, we give lower bounds on ǫ∗
n for the (L, ∞) and (L, 1) families of loss
functions under the constraint that the channel distribution Q must guarantee that a limited amount
of information I (Xi ; Zi ) is communicated: the channel distribution Q satisﬁes our Deﬁnition 1 of
optimal local privacy.

3.3 Main theorems

We now state our two main theorems, deferring proofs to Appendix B. Our ﬁrst theorem applies to
the class of (L, ∞) loss functions (recall Deﬁnition 2). We assume that the set to which the perturbed
data Z must belong is [−M∞ , M∞ ]d , where M∞ ≥ L. We state two variants of the theorem, as one
gives sharper results for an important special case.
Theorem 1. Let L be the collection of (L, ∞) loss functions and assume the conditions of the
preceding paragraph. Let Q satisfy be optimally private for the collection L. Then

(a) If Θ contains the ℓ∞ ball of radius r ,

ǫ∗
n (L, Θ) ≥

1
163 ·

M∞ rd
√n

.

(b) If Θ = {θ ∈ Rd : kθk1 ≤ r},

1
163 ·

5

ǫ∗
n (L, Θ) ≥

rM∞plog(2d)
17√n
For our second theorem, we assume that the loss functions L consist of (L, 1) losses, and that the
perturbed data must belong to the ℓ1 ball of radius M1 , i.e., Z ∈ {z ∈ Rd | kzk1 ≤ M1 }. Setting
M = M1/L, we deﬁne (these constants relate to the optimal local privacy distribution for ℓ1 -balls)
γ := log   2d − 2 + p(2d − 2)2 + 4(M 2 − 1)
! ,
eγ − e−γ
eγ + e−γ + 2(d − 1)
2(M − 1)
Theorem 2. Let L be the collection of (L, 1) loss functions and assume the conditions of the pre-
ceding paragraph. Let Q be optimally locally private for the collection L. Then
rL√d
√n∆(γ )

ǫ∗
n (L, Θ) ≥

and ∆(γ ) :=

.

(9)

.

.

Remarks We make two main remarks about Theorems 1 and 2. First, we note that each result
yields a minimax rate for stochastic optimization problems when there is no random distribution Q.
Indeed, in Theorem 1, we may take M∞ = L, in which case (focusing on the second statement of
the theorem) we obtain the lower bound rLplog(2d)/17√n when Θ = {θ ∈ Rd : kθk1 ≤ r}.
Mirror descent algorithms [20, 21] attain a matching upper bound (see the long version of this
paper [9, Sec. 3.3] for more substantial explanation). Moreover, our analysis is sharper than previous
analyses [1, 20], as none (to our knowledge) recover the logarithmic dependence on the dimension
d, which is evidently necessary. Theorem 2 provides a similar result when we take M1 ↓ L, though
in this case stochastic gradient descent attains the matching upper bound.

Our second set of remarks are somewhat more striking. In these, we show that the lower bounds in
Theorems 1 and 2 give sharp tradeoffs between the statistical rate of convergence for any statistical
procedure and the desired privacy of a user. We present two corollaries establishing this tradeoff. In
each corollary, we look ahead to Section 4 and use one of Propositions 1 or 2 to derive a bijection
between the size M∞ or M1 of the perturbation set and the amount of privacy—as measured by the
worst case mutual information I ∗—provided. We then combine Theorems 1 and 2 with results on
stochastic approximation to demonstrate the tradeoffs.
Corollary 1. Let the conditions of Theorem 1(b) hold, and assume that M∞ ≥ 2L. Assume Q∗
satisﬁes optimal local privacy at information level I ∗ . For universal constants c ≤ C ,
rL√d log d
rL√d log d
≤ ǫ∗
√nI ∗
√nI ∗
c ·
n (L, Θ) ≤ C ·
.
Since Θ ⊆ {θ ∈ Rd : kθk1 ≤ r}, mirror descent [2, 21, 20, Chapter 5], using n un-
Proof
biased stochastic gradient samples whose ℓ∞ norms are bounded by M∞ , obtains convergence
rate O(M∞ r√log d/√n). This matches the second statement of Theorem 1. Now ﬁx our desired
amount of mutual information I ∗ . From the remarks following Proposition 1, if we must guarantee
that I ∗ ≥ supP I (P , Q) for any distribution P and loss function ℓ whose gradients are bounded in
ℓ∞ -norm by L, we must (by the remarks following Proposition 1) have
dL2
I ∗ ≍
.
M 2
∞
Up to higher-order terms, to guarantee a level of privacy with mutual information I ∗ , we must allow
gradient noise up to M∞ = Lpd/I ∗ . Using the bijection between M∞ and the maximal allowed
mutual information I ∗ under local privacy that we have shown, we substitute M∞ = L√d/√I ∗
into the upper and lower bounds that we have already attained.
Similar upper and lower bounds can be obtained under the conditions of part (a) of Theorem 1,
where we need not assume Θ is an ℓ1 -ball, but we lose a factor of √log d in the lower bound. Now
we turn to a parallel result, but applying Theorem 2 and Proposition 2.
Corollary 2. Let the conditions of Theorem 2 hold and assume that M1 ≥ 2L. Assume that Q∗
satisﬁes optimal local privacy at information level I ∗ . For universal constants c ≤ C ,
rLd
rLd
√nI ∗ ≤ ǫ∗
√nI ∗
n (L, Θ) ≤ C ·
c ·
.
Proof By the conditions of optimal local privacy (Proposition 2 and Corollary 3), to have I ∗ ≥
supP I (P , Q) for any loss ℓ whose gradients are bounded in ℓ1 -norm by L, we must have
dL2
I ∗ ≍
,
2M 2
1
using Corollary 3. Rewriting this, we see that we must have M1 = Lpd/2I ∗ (to higher-order
terms) to be able to guarantee an amount of privacy I ∗ . As in the ℓ∞ case, we have a bijection
between the multiplier M1 and the amount of information I ∗ and can apply similar techniques.
Indeed, stochastic gradient descent (SGD) enjoys the following convergence guarantees (e.g. [21]).
Let Θ ⊆ Rd be contained in the ℓ∞ ball of radius r and the gradients of the loss ℓ belong to the
n (L, Θ) ≤ CM1 r√d/√n. Now apply the lower bound
ℓ1 -ball of radius M1 . Then SGD has ǫ∗
provided by Theorem 2 and substitute for M1 .

6

4 Saddle points, optimal privacy, and mutual information

In this section, we explore conditions for a distribution Q∗ to satisfy optimal local privacy, as given
by Deﬁnition 1. We give characterizations of necessary and sufﬁcient conditions based on the com-
pact sets C ⊂ D for distributions P ∗ and Q∗ to achieve the saddle point (4). Our results can be
viewed as rate distortion theorems [14, 8] (with source P and channel Q) for certain compact al-
phabets, though as far as we know, they are all new. Thus we sometimes refer to the conditional
distribution Q, which is designed to maintain the privacy of the data X by communication of Z , as
the channel distribution. Since we wish to bound I (X ; Z ) for arbitrary losses ℓ, we must address the
case when ℓ(X, θ) = hθ , X i, in which case ∇ℓ(X, θ) = X ; by the data-processing inequality [14,
Chapter 5] it is thus no loss of generality to assume that X ∈ C and that E[Z | X ] = X .
We begin by deﬁning the types of sets C and D that we use in our characterization of privacy. As
we see in Section 3, such sets are reasonable for many applications. We focus on the case when the
compact sets C and D are (suitably symmetric) norm balls:
Deﬁnition 3. Let C ⊂ Rd be a compact convex set with extreme points ui ∈ Rd , i ∈ I for some
index set I . Then C is rotationally invariant through its extreme points if kuik2 = kuj k2 for each
i, j , and for any unitary matrix U such that U ui = uj for some i 6= j , then U C = C .
Some examples of convex sets rotationally invariant through their extreme points include ℓp -norm
balls for p = 1, 2, ∞, though ℓp -balls for p 6∈ {1, 2, ∞} are not. The following theorem gives
a general characterization of the minimax mutual information for rotationally invariant norm balls
with ﬁnite numbers of extreme points by providing saddle point distributions P ∗ and Q∗ . We provide
the proof of Theorem 3 in Section A.1.
Theorem 3. Let C be a compact, convex, polytope rotationally invariant through its extreme points
{ui }m
i=1 and D = (1 + α)C for some α > 0. Let Q∗ be the conditional distribution on Z | X that
maximizes the entropy H (Z | X = x) subject to the constraints that
EQ [Z | X = x] = x
for x ∈ C and that Z is supported on (1 + α)ui for i = 1, . . . , m. Then Q∗ satisﬁes Deﬁnition 1,
optimal local privacy, and Q∗ is (up to measure zero sets) unique. Moreover, the distribution P ∗
uniform on {ui }m
i=1 uniquely attains the saddle point (4).
Remarks: While in the theorem we assume that Q∗ (· | X = x) maximizes the entropy for each
x ∈ C , this is not in fact essential. In fact, we may introduce a random variable X ′ between X and
i=1 of C in any way such that E[X ′ | X ] =
Z : let X ′ be distributed among the extreme points {ui }m
X , then use the maximum entropy distribution Q∗ (· | ui ) deﬁned in the theorem when X ∈ {ui }m
i=1
to sample Z from X ′ . The information processing inequality [14, Chapter 5] guarantees the Markov
chain X → X ′ → Z satisﬁes the minimax bound I (X ; Z ) ≤ inf Q supP I (P , Q).
With Theorem 3 in place, we can explicitly characterize the distributions achieving optimal local
privacy (recall Deﬁnition 1) for ℓ1 and ℓ∞ balls. We present the propositions in turn, providing
some discussion here and deferring proofs to Appendices A.2 and A.3.
First, consider the case where X ∈ [−1, 1]d and Z ∈ [−M , M ]d . For notational convenience, we
deﬁne the binary entropy h(p) = −p log p − (1 − p) log(1 − p). We have
Proposition 1. Let X ∈ [−1, 1]d and Z ∈ [−M , M ]d be random variables with M ≥ 1 and
E[Z | X ] = X almost surely. Deﬁne Q∗ to be the conditional distribution on Z | X such that the
coordinates of Z are independent, have range {−M , M }, and
1
Xi
1
Xi
Q∗ (Zi = M | X ) =
and Q∗ (Zi = −M | X ) =
+
2 −
2
2M
2M
Then Q∗ satisﬁes Deﬁnition 1, optimal local privacy, and moreover,
2M (cid:19) .
I (P , Q∗ ) = d − d · h (cid:18) 1
1
2
Before continuing, we give a more intuitive understanding of Proposition 1. Concavity implies that
for a, b > 0, log(a) ≤ log b + b−1 (a − b), or − log(a) ≥ − log(b) + b−1 (b − a), so in particular
2M (cid:19) ≥ − (cid:18) 1
2M (cid:19) (cid:18)− log 2 −
M (cid:19)−(cid:18) 1
2M (cid:19) (cid:18)− log 2 +
M (cid:19) = log 2−
h (cid:18) 1
1
1
1
1
1
1
+
+
2 −
M 2 .
2
2
7

sup
P

.

+

That is, we have for any distribution P on X ∈ [−1, 1]d that (in natural logarithms)
d
d
I (P , Q∗ ) ≤
and I (P , Q∗ ) =
M 2 + O(M −3 ).
M 2
We now consider the case when X ∈ (cid:8)x ∈ Rd | kxk1 ≤ 1(cid:9) and Z ∈ (cid:8)z ∈ Rd | kzk1 ≤ M (cid:9). Here
the arguments are slightly more complicated, as the coordinates of the random variables are no
longer independent, but Theorem 3 still allows us to explicitly characterize the saddle point of the
mutual information.
Proposition 2. Let X ∈ {x ∈ Rd | kxk1 ≤ 1} and Z ∈ {z ∈ Rd | kzk1 ≤ M } be random
variables with M > 1. Deﬁne the parameter γ as in Eq. (9), and let Q∗ be the distribution on Z | X
such that Z is supported on {±M ei }d
i=1 , and
eγ
Q∗ (Z = M ei | X = ei ) =
eγ + e−γ + (2d − 2)
e−γ
Q∗ (Z = −M ei | X = ei ) =
eγ + e−γ + (2d − 2)
1
Q∗ (Z = ±M ej | X = ei , j 6= i) =
eγ + e−γ + (2d − 2)
(For X 6∈ {±ei }, deﬁne X ′ to be randomly selected in any way from among {±ei } such that
E[X ′ | X ] = X , then sample Z conditioned on X ′ according to (10a)–(10c).) Then Q∗ satisﬁes
Deﬁnition 1, optimal local privacy, and
e−γ
eγ
I (P , Q∗ ) = log(2d)−log (cid:0)eγ + e−γ + 2d − 2(cid:1)+γ
eγ + e−γ + 2d − 2 −γ
sup
eγ + e−γ + 2d − 2
P
We remark that the additional sampling to guarantee that X ′ ∈ {±ei } (where the conditional
distribution Q∗ is deﬁned) can be accomplished simply: deﬁne the random variable X ′ so that
X ′ = ei sign(xi ) with probability |xi |/ kxk1 . Evidently E[X ′ | X ] = x, and X → X ′ → Z
for Z distributed according to Q∗ deﬁnes a Markov chain as in our remarks following Theorem 3.
Additionally, an asymptotic expansion allows us to gain a somewhat clearer picture of the values of
the mutual information, though we do not derive upper bounds as we did for Proposition 1. We have
the following corollary, proved in Appendix E.1.
Corollary 3. Let Q∗ denote the conditional distribution in Proposition 2. Then
d (cid:27)(cid:19) .
2M 2 + Θ (cid:18)min (cid:26) d3
log4 (d)
d
M 4 ,
5 Discussion and open questions

I (P , Q∗ ) =

sup
P

(10b)

,

,

.

(10a)

(10c)

.

This study leaves a number open issues and areas for future work. We study procedures that access
each datum only once and through a perturbed view Zi of the subgradient ∂ ℓ(Xi , θ), which allows
us to use (essentially) any convex loss. A natural question is whether there are restrictions on the loss
function so that a transformed version (Z1 , . . . , Zn ) of the data are sufﬁcient for inference. Zhou
et al. [33] study one such procedure, and nonparametric data releases, such as those Hall et al. [15]
study, may also provide insights. Unfortunately, these (and other) current approaches require the
data be aggregated by a trusted curator. Our constraints on the privacy-inducing channel distribution
Q require that its support lie in some compact set. We ﬁnd this restriction useful, but perhaps it
possible to achieve faster estimation rates under other conditions. A better understanding of general
privacy-preserving channels Q for alternative constraints to those we have proposed is also desirable.

These questions do not appear to have easy answers, especially when we wish to allow each provider
of a single datum to be able to guarantee his or her own privacy. Nevertheless, we hope that our view
of privacy and the techniques we have developed herein prove fruitful, and we hope to investigate
some of the above issues in future work.

Acknowledgments We thank Cynthia Dwork, Guy Rothblum, and Kunal Talwar for feedback on
early versions of this work. This material supported in part by ONR MURI grant N00014-11-1-0688
and the U.S. Army Research Laboratory and the U.S. Army Research Ofﬁce under grant W911NF-
11-1-0391. JCD was partially supported by an NDSEG fellowship and a Facebook fellowship.

8

References

In

URL

privacy.

[1] A. Agarwal, P. Bartlett, P. Ravikumar, and M. Wainwright. Information-theoretic lower bounds on the
oracle complexity of convex optimization. IEEE Trans. on Information Theory, 58(5):3235–3249, 2012.
[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex opti-
mization. Operations Research Letters, 31:167–175, 2003.
[3] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-
Hall, Inc., 1989.
[4] P. Billingsley. Probability and Measure. Wiley, Second edition, 1986.
[5] A. Blum, K. Ligett, and A. Roth. A learning theory approach to non-interactive database privacy.
Proceedings of the Fourtieth Annual ACM Symposium on the Theory of Computing, 2008.
[6] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[7] K. Chaudhuri, C. Moneleoni, and A. D. Sarwate. Differentially private empirical risk minimization.
Journal of Machine Learning Research, 12:1069–1109, 2011.
[8] T. M. Cover and J. A. Thomas. Elements of Information Theory, Second Edition. Wiley, 2006.
[9] J. C. Duchi, M.
Privacy aware learning.
J. Wainwright.
and M.
Jordan,
I.
http://arxiv.org/abs/1210.2085, 2012.
[10] C. Dwork. Differential privacy: a survey of results. In Theory and Applications of Models of Computation,
volume 4978 of Lecture Notes in Computer Science, p. 1–19. Springer, 2008.
[11] C. Dwork and J. Lei. Differential privacy and robust statistics. In Proceedings of the Fourty-First Annual
ACM Symposium on the Theory of Computing, 2009.
[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis.
In Proceedings of the 3rd Theory of Cryptography Conference, p. 265–284, 2006.
[13] A. V. Evﬁmievski, J. Gehrke, and R. Srikant. Limiting privacy breaches in privacy preserving data mining.
In Proceedings of the Twenty-Second Symposium on Principles of Database Systems, p. 211–222, 2003.
[14] R. M. Gray. Entropy and Information Theory. Springer, 1990.
[15] R. Hall, A. Rinaldo,
and L. Wasserman.
Random differential
http://arxiv.org/abs/1112.2680, 2011.
[16] J. Hiriart-Urruty and C. Lemar ´echal. Convex Analysis and Minimization Algorithms. Springer, 1996.
[17] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn
privately? SIAM Journal on Computing, 40(3):793–826, 2011.
[18] L. Le Cam. On the asymptotic theory of estimation and hypothesis testing. Proceedings of the Third
Berkeley Symposium on Mathematical Statistics and Probability, p. 129–156, 1956.
[19] L. Le Cam. Convergence of estimates under dimensionality restrictions. Annals of Statistics, 1(1):38–53,
1973.
[20] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley, 1983.
[21] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[22] R. R. Phelps. Lectures on Choquet’s Theorem, Second Edition. Springer, 2001.
[23] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal
on Control and Optimization, 30(4):838–855, 1992.
[24] B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and N. Taft. Learning in a large function space: privacy-
preserving mechanisms for SVM learning. Journal of Privacy and Conﬁdentiality, 4(1):65–100, 2012.
[25] L. Sankar, S. R. Rajagopalan, and H. V. Poor. An information-theoretic approach to privacy. In The 48th
Allerton Conference on Communication, Control, and Computing, p. 1220–1227, 2010.
[26] A. Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the
Fourty-Third Annual ACM Symposium on the Theory of Computing, 2011.
[27] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics.
Cambridge University Press, 1998. ISBN 0-521-49603-9.
[28] A. Wald. Contributions to the theory of statistical estimation and testing hypotheses. Annals of Mathe-
matical Statistics, 10(4):299–326, 1939.
[29] S. L. Warner. Randomized response: a survey technique for eliminating evasive answer bias. Journal of
the American Statistical Association, 60(309):63–69, 1965.
[30] L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American
Statistical Association, 105(489):375–389, 2010.
[31] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of
Statistics, 27(5):1564–1599, 1999.
[32] B. Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, p. 423–435. Springer-Verlag, 1997.
[33] S. Zhou, J. Lafferty, and L. Wasserman. Compressed regression.
IEEE Transactions on Information
Theory, 55(2):846–866, 2009.
[34] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings
of the Twentieth International Conference on Machine Learning, 2003.

URL

9


Reconstructing
Broadcasts on Trees

Douglas Stanford
CS 229
December 13, 2012

Abstract

Given a branching Markov process on a tree,
the goal of the reconstruction (or “broad-
cast”) problem is to guess the initial con-
dition at the root, given the states of the
leaves. In this essay, we evaluate the perfor-
mance of some algorithms, new and old, for
solving the reconstruction problem.

1

Introduction

Branching Markov processes have a wide rel-
evance across disciplines. In phylogenetics,
they arise as a description of the mutation
dynamics of evolution. In mathematics and
physics, they are simple examples of statis-
tical mechanics systems with phase transi-
tions.
In theoretical cosmology, they are
used as a model for the ﬂuctuation dynamics
of inﬂation.
In all three of these contexts, a natu-
ral and important question is the following:
how well can one recover the initial state of
the branching Markov process from the ﬁnal
state on the leaves of the tree? In this essay,
we will apply machine learning algorithms
to this question.
First, a deﬁnition.
In this essay we will
work with two-state branching Markov sys-
tems. This is deﬁned on a regular rooted
tree of degre d as follows. Begin by spec-
ifying the state of the root, at generation

u = 0, as either white or black. Then, for
each of the (d − 1) children of the root, in-
dependently assign the child the color of the
parent with probability (1 − p) and the op-
posite color with probability p. Repeat this
process recursively to color the entire tree.

Figure 1: The time variable u in a p = 2
tree.

The reconstruction problem [1] asks the
following question: given the colors of the
leaves, with what probability can the initial
state of the root be inferred? The answer
depends on the value of u, the degree d and
the probability of color ﬂip p. It is known
[2] that, in the limit of large u, the recon-
struction probablem is unsolvable if

1 − 2p ≥

1√
d − 1

.

(1)

However,
if the reverse inequality holds,
then the leaves contain at least some infor-
mation about the state of root, even in the
u → ∞ limit [3, 4].
In this pro ject, we will ﬁx u and d and
vary p.
This gives us an environment
in which the diﬃculty of the classiﬁcation
problem can be controlled precisely. We will
study the performance of four diﬀerent al-
gorithms as a function of p: naive Bayes,
not-so-naive Bayes, parsimony and an SVM.

1

u=0u=1u=2u=3four children of the root. These principal
components have an approximate interpre-
tation as labelling whether one of the ﬁrst
children had a color change, compared to the
root.

2 The dataset

We generated training and testing sets by
running the branching Markov process for
diﬀerent values of p. Sample conﬁgurations
of the leaves for a degree ﬁve tree are shown
in ﬁgure 2. These conﬁgurations all had
white initial conditions at the root. The
value of p increases from the left panel to
the right panel to the bottom panel, and it
is intuitively obvious that the reconstruction
becomes more diﬃcult as p increases.

Figure 3: The second, third and fourth prin-
cipal components for the d = 5 system. The
ﬁrst principal component is approximately
spatially homogeneous.

Figure 2:
Sample conﬁgurations of the
leaves in the d = 5 two-state system after
six generations, for diﬀerent values of p.

To get a feel for the important variables,
we ran PCA on a set of 10000 training ex-
amples. The ﬁrst principal component ends
up being roughly constant on all
leaves.
Naively, this encodes most of the informa-
tion about whether the root was white or
black. The second, third and fourth princi-
pal components are shown in ﬁgure 2, again
for a degree ﬁve tree. The four obvious
blocks correspond to the descendants of the

Although the data shown here is for the
d = 5 tree, it is convenient to run the al-
gorithms (particularly parsimony) on a tree
of even degree, so we chose to work with
d = 4. We generated 20000 training and
testing examples for each of twelve values of
p in the range 0.01 to 0.23, with randomly
chosen white/black initial conditions. For
reference, the critical value beyond which
reconstruction is asymptotically (u → ∞)
impossible is approximately p = 0.21 for a
degree four tree.

2

’output.txt’matrix’output.txt’matrix’output.txt’matrix’3’matrix’3’matrix’3’matrix3 Naive-ish Bayes

The optimal (maximum likelihood) recon-
struction of the root would proceed as fol-
lows. We would assign a uniform prior for
P (root) and then compute
P (leaves|root)P (root)
P (leaves)

P (root|leaves) =

.

(2)
As usual P (leaves) cancels out when com-
paring probabilities for classiﬁcation. All we
need is P (leaves|root). In principle, explicit
knowledge of the Markov matrix and the
tree graph make is possible to compute this
exactly. However, it appears to be compu-
tationally intractable.
One alternative is to use a maximum like-
lihood algorithm on a related but simpler
graph. For example, choosing the graph on
the left of ﬁgure 4 is equivalent to making
(cid:89)
an ansatz
P N B (leaves|root) =
i
Here, P (leafi |root) can be computed simply
by iterating the Markov matrix. Explicitly,
it is

P (leafi |root). (3)

P (leafi |root) =

1
2

± 1
2

(1 − 2p)u ,

(4)

with the upper sign if the leaf is the same
color as the root, and the lower sign if the
leaf and root are opposite. After taking the
product over leaves, it becomes clear that
maximizing P N B over choices of the root
color reduces to ma jority vote:
if most of
the leaves are white, guess that the root was
also white. Otherwise, guess that the root
was black.
A slightly more complicated graph for
which maximum likelihood is nevertheless
tractable is shown on the right in ﬁgure 4.

3

Figure 4: Simpliﬁed tree graphs for the naive
Bayes model (left) and the NSNB model
(right)

(5)
P (leafi |{z}).

This is much like the naive Bayes graph on
the left, but it models the ﬁrst generation
after the root explicitly as latent variables:
(cid:89)
(cid:88)
P N SN B (leaves|root) =
P ({z}|root)
{z}
i
Here, {z} are latent varibles labeling the col-
ors of ﬁrst generation, P ({z}|root) is equal
to pm (1 − p)n , where n is the number of ver-
tices in the ﬁrst generation with the same
color as the root, and m is the number with
the opposite color. Finally, P (leafi |{z}) is
as in eq (4), but with the color of the root re-
placed by the color of the relevant ﬁrst gen-
eration vertex, and u replaced by u − 1.
In the text below, we refer to this model as
the not-so-naive-Bayes (NSNB) algorithm.

4 Parsimony

The parsimony reconstruction algorithm,
popular in the phylogenetic literature, recur-
sively applies ma jority vote.1 More speciﬁ-
cally, the leaves at generation u are grouped
into families of size (d − 1), each having a
single parent at generation (u − 1). The

1On a tree of odd degree, the algorithm is more
complicated. Here, we will focus on degree four, so
the above description is correct.

color of each such parent is assigned as the
ma jority vote of the children. This algo-
rithm is repeated until it assigns a color to
the root at generation u = 0. Parsimony
has the nice property that it identiﬁes a col-
oring of the tree that minimizes the num-
ber of mutations, or parent-child diﬀerences.
Note, however, that this is not necessarily
the maximum likelihood reconstruction. In-
deed, we’ll see that parsimony is a subopti-
mal algorithm for large p.

5 SVM

The ﬁnal algorithm we consider is an SVM.
We used a Gaussian kernel, with an (cid:96)1 soft
margin, implemented using libsvm [5]. We
found that feeding the data of the leaves
directly into the SVM leads to overﬁtting
and relatively poor performance.
Instead,
inspired by the principal components dis-
cussed above, we used as features the ma-
jority vote of the descendants of diﬀernt ver-
tices, starting with the root and working up-
wards. The ﬁrst feature is just the total cen-
sus Nwhite − Nblack of the leaves. The second
feature is the census of the ﬁrst 1/(d − 1)
leaves, the third is the census of the next
1/(d − 1) and so forth. We selected features
using cross validation, and found rather ro-
bustly that the optimal number of features
for the d = 4 tree was four. We also selected
the parameters γ of the Gaussian kernel and
C of the soft margin using cross validation.
The results were rather insensitive to the
choices of γ and C , for γ ≈ 0.5 and C ≈ 30.
We trained on 20000 samples. This appears
to be suﬃcient, judging from the learning
curve shown in ﬁgure 5.

Figure 5: Learning curve for the SVM, p =
0.11.

6 Comparison
methods

of

the

Finally, we tested all four of the algorithms
on 20000 samples for each of the values of p.
The performance is shown in the ﬁgure and
table below.
As expected, the performance of all al-
gorithms decreases monotonically as p in-
creases. For small values of p, the parsimony
algorithm does best, followed closely by the
SVM. However, as p increases, parsimony is
Indeed, for p ≥ 0.15, parsi-
less eﬀective.
mony is the worst of the four. The naive
Bayes algorithm is the worst for small val-
ues of p, but it ends up more or less tied
with the SVM at larger p. The SVM does
best overall, coming close to the accuracy of
parsimony for small p, but tracking the naive
Bayes performance for larger p. NSNB has
performance very similar to the SVM, but it
is slightly worse for all p.
We conclude that the SVM emerges, once
again, as having very competitive perfor-
mance across the board.

4

101102103104105Training set size8082848688909294Probability of correct classificationSVM learning curveTable 1: Probability of correct reconstruc-
tion for the ﬁve reconstruction algorithms,
as a function of p, the probability of chang-
ing color in a single generation

p
0.01
0.03
0.05
0.07
0.09
0.11
0.13
0.15
0.17
0.19
0.21
0.23

NB
0.9998
0.9960
0.9855
0.9684
0.9456
0.9150
0.8831
0.8451
0.7954
0.7444
0.6963
0.6497

parsimony NSNB SVM
0.9998
0.9998
0.9998
0.9970
0.9969
0.9972
0.9886
0.9886
0.9901
0.9780
0.9751
0.9751
0.9536
0.9511
0.9588
0.9235
0.9198
0.9272
0.8880
0.8836
0.8918
0.8487
0.8411
0.8358
0.7965
0.7875
0.7791
0.7435
0.7408
0.7158
0.6575
0.6927
0.6969
0.6492
0.6462
0.6145

nals of Applied Probability, 11(1):285–
300, 2001.

[5] Chih-Chung Chang and Chih-Jen Lin.
LIBSVM: A library for support vec-
tor machines.
ACM Transactions
on Intel ligent Systems and Technology,
2:27:1–27:27, 2011.
Software avail-
able at http://www.csie.ntu.edu.tw/
~cjlin/libsvm.

Figure 6: Performance of diﬀerent recon-
struction algorithms for various values of p.
The degree is ﬁxed at four, and simulation
is run for six generations. The NSNB curve
is omitted for clarity. See the table for more
details.

References

[1] T. Moore and J. L. Snell. A branch-
ing process showing a phase transition.
Journal of Applied Probability, 16(2):pp.
252–260, 1979.

[2] P. Bleher, J. Ruiz, and V. Zagrebnov.
On the purity of the limiting gibbs state
for the ising model on the bethe lattice.
Journal of Statistical Physics, 79:473–
482, 1995. 10.1007/BF02179399.

[3] H. Kesten and B.P. Stigum. Additional
limit theorems for indecomposable mul-
tidimensional galton-watson processes.
Ann. math. Statist., 37:1463–1481, 1966.

[4] E. Mossel. Reconstruction on trees:
beating the second eigenvalue. The An-

5

0.000.050.100.150.200.25Probably of flip per generation0.600.650.700.750.800.850.900.951.00Probability of correct classificationPerformance comparisonNBparsimonySVM
Tight Bounds on Proﬁle Redundancy and Distinguishability

Jayadev Acharya
ECE, UCSD
jacharya@ucsd.edu

Hirakendu Das
Yahoo!
hdas@yahoo-inc.com

Alon Orlitsky
ECE & CSE, UCSD
alon@ucsd.edu

Abstract
The minimax KL-divergence of any distribution from all distributions in a collection P has several
practical implications. In compression, it is called redundancy and represents the least additional
number of bits over the entropy needed to encode the output of any distribution in P . In online es-
timation and learning, it is the lowest expected log-loss regret when guessing a sequence of random
values generated by a distribution in P . In hypothesis testing, it upper bounds the largest number of
distinguishable distributions in P . Motivated by problems ranging from population estimation to text
classiﬁcation and speech recognition, several machine-learning and information-theory researchers
have recently considered label-invariant observations and properties induced by i.i.d. distributions.
A sufﬁcient statistic for all these properties is the data’s proﬁle, the multiset of the number of times
each data element appears. Improving on a sequence of previous works, we show that the redun-
dancy of the collection of distributions induced over proﬁles by length-n i.i.d. sequences is between
0.3 · n1/3 and n1/3 log2 n, in particular, establishing its exact growth power.

1
Introduction
Information theory, machine learning, and statistics, are closely related disciplines. One of their main intersection
areas is the conﬂuence of universal compression, online learning, and hypothesis testing. We consider two concepts in
this overlap. The minimax KL divergence—a fundamental measure for, among other things, how difﬁcult distributions
are to compress, predict, and classify, and proﬁles—a relatively new approach for compression, classiﬁcation, and
property testing over large alphabets. Improving on several previous results, we determine the exact growth power of
the KL-divergence minimax of proﬁles of i.i.d. distributions over any alphabet.
1.1 Minimax KL divergence

As is well known in information theory, the expected number of bits required to compress data X generated according
to a known distribution P is the distribution’s entropy, H (P ) = EP log 1/P (X ), and is achieved by encoding X using
roughly log 1/P (X ) bits. However, in many applications P is unknown, except that it belongs to a known collection
P of distributions, for example the collection of all i.i.d., or all Markov distributions. This uncertainty typically raises
the number of bits above the entropy and is studied in Universal compression [9, 13]. Any encoding corresponds to
some distribution Q over the encoded symbols. Hence the increase in the expected number of bits used to encode the
output of P is EP log 1/Q(X ) − H (P ) = D(P ||Q), the KL divergence between P and Q. Typically one is interested
in the highest increase for any distribution P ∈ P , and ﬁnds the encoding that minimizes it. The resulting quantity,
called the (expected) redundancy of P , e.g., [8, Chap. 13], is therefore the KL minimax
R(P ) def= min
P ∈P D(P ||Q).
max
Q
(cid:80)n
The same quantity arises in online-learning, e.g., [5, Ch. 9], where the probabilities of random elements X1 , . . . , Xn
are sequentially estimated. One of the most popular measures for the performance of an estimator Q is the per-symbol
i=1 log Q(Xi |X i−1 ). As in compression, for underlying distribution P ∈ P , the expected log loss is
log loss 1
EP log 1/Q(X ), and the log-loss regret is EP log 1/Q(X ) − H (P ) = D(P ||Q). The maximal expected regret for
n
any distribution in P , minimized over all estimators Q is again the KL minimax, namely, redundancy.

1

In statistics, redundancy arises in multiple hypothesis testing. Consider the largest number of distributions that can
be distinguished from their observations. For example, the largest number of topics distinguishable based on text of a
given length. Let P be a collection of distributions over a support set X . As in [18], a sub-collection S ⊆ P of the
distributions is -distinguishable if there is a mapping f : X → S such that if X is generated by a distribution S ∈ S ,
then P (f (X ) (cid:54)= S ) ≤ . Let M (P , ) be the largest number of -distinguishable distributions in P , and let h() be
the binary entropy function. In Section 4 we show that for all P ,
(1 − ) log M (P , ) ≤ R(P ) + h(),

(1)

and in many cases, like the one considered here, the inequality is close to equality.
Redundancy has many other connections to data compression [27, 28], the minimum-description-length principle [3,
16, 17], sequential prediction [21], and gambling [20]. Because of the fundamental nature of R(P ), and since tight
bounds on it often reveal the structure of P , the value of R(P ) has been studied extensively in all three communities,
e.g., the above references as well as [29, 37] and a related minimax in [6].

1.2 Redundancy of i.i.d. distributions
The most extensively studied collections are independently, identically distributed (i.i.d.). For example, for the collec-
tion I n
k of length-n i.i.d. distributions over alphabets of size k , a string of works [7, 10, 11, 28, 33, 35, 36] determined
the redundancy up to a diminishing additive term,

R(I n
k ) =

k − 1
2

log n + Ck + o(1),

(2)

where the constant Ck was determined exactly in terms of k . For compression this shows that the extra number of
bits per symbol required to encode an i.i.d. sequence when the underlying distribution is unknown diminishes to zero
as (k − 1) log n/(2n). For online learning this shows that these distributions can be learned (or approximated) and
that this approximation can be done at the above rate. In hypothesis testing this shows that there are roughly n(k−1)/2
distinguishable i.i.d. distributions of alphabet size k and length n.
Unfortunately, while R(I n
k ) increases logarithmically in the sequence length n, it grows linearly in the alphabet size k .
For sufﬁciently large k , this value even exceeds n itself, showing that general distributions over large alphabets cannot
be compressed or learned at a uniform rate over all alphabet sizes, and as the alphabet size increases, progressively
larger lengths are needed to achieve a given redundancy, learning rate, or test error.

1.3 Patterns
Partly motivated by redundancy’s fast increase with the alphabet size, a new approach was recently proposed to address
compression, estimation, classiﬁcation, and property testing over large alphabets.
The pattern [25] of a sequence represents the relative order in which its symbols appear. For example, the pattern of
abracadabra is 12314151231. A natural method to compress a sequence over a large alphabet is to compress its pattern
as well as the dictionary that maps the order to the original symbols. For example, for abracadabra, 1 → a, 2 → b,
3 → r , 4 → c, 5 → d.
It can be shown [15, 26] that for all i.i.d. distributions, over any alphabet, even inﬁnitely large, as the sequence
length increases, essentially all the entropy lies in the pattern, and practically none is in the dictionary. Hence [25]
focused on the redundancy of compressing patterns. They showed, e.g., Subsection 1.5, that the although, as in (2),
i.i.d. sequences over large alphabets have arbitrarily high per-symbol redundancy, and although as above patterns
contain essentially all the information of long sequences, the per-symbol redundancy of patterns diminishes to zero at
a uniform rate independent of the alphabet size.
In online learning, patterns correspond to estimating the probabilities of each observed symbol, and of all unseen ones
combined. For example, after observing the sequence dad, with pattern 121, we estimate the probabilities of 1, 2, and
3. The probability we assign to 1 is that of d, the probability we assign to 2 is that of a, and the probability we assign
to 3 is the probability of all remaining letters combined. The aforementioned results imply that while distributions
over large alphabets cannot be learned with uniformly diminishing per-symbol log loss, if we would like to estimate
the probability of each seen element, but combine together the probabilities of all unseen ones, then the per symbol
log loss diminishes to zero uniformly regardless of the alphabet size.

2

1.4 Proﬁles

Improving on existing pattern-redundancy bounds seems easier to accomplish via proﬁles. Since we consider i.i.d.
distributions, the order of the elements in a pattern does not affect its probability. For example, for every distribution
It is easy to see that the probability of a pattern is determined by the ﬁngerprint [4] or
P , P (112) = P (121).
proﬁle [25] of the pattern, the multiset of the number of appearances of the symbols in the pattern. For example, the
proﬁle of the pattern 121 is {1, 2} and all patterns with this proﬁle, 112, 121, 122 will have the same probability under
any distribution P . Similarly, the proﬁle of 1213 is {1, 1, 2} and all patterns with this proﬁle, 1123, 1213, 1231, 1223,
1232, and 1233, will have the same probability under any distribution.
It is easy to see that since all patterns of a given proﬁle have the same probability, the ratio between the actual and
estimated probability of a proﬁle is the same as this ratio for each of its patterns. Hence pattern redundancy is the
same as proﬁle redundancy [25]. Therefore from now on we consider only proﬁle redundancy, and begin by deﬁning
it more formally.
The multiplicity µ(a) of a symbol a in a sequence is the number of times it appears. The proﬁle ϕ(x) of a sequence
x is the multiset of multiplicities of all symbols appearing in it [24, 25]. The proﬁle of the sequence is the multiset of
multiplicities. For example, the sequence ababcde has multiplicities µ(a) = µ(b) = 2, µ(c) = µ(d) = µ(e) = 1, and
proﬁle {1, 1, 1, 2, 2}. The prevalence ϕµ of a multiplicity µ is the number of elements with multiplicity µ.
Let Φn denote the collection of all proﬁles of length-n sequences. For example, for sequences of length one there is a
single element appearing once, hence Φ1 = {{1}}, for length two, either one element appears twice, or each of two
elements appear once, hence Φ2 = {{2}, {1, 1}}, similarly Φ3 = {{3}, {2, 1}, {1, 1, 1}}, etc.
that an i.i.d. distribution P generates an n-element sequence x is P (x) def= (cid:81)n
We consider the distributions induced on Φn by all discrete i.i.d. distributions over any alphabet. The probability
ϕ ∈ Φn is the sum of the probabilities of all sequences of this proﬁle, P (ϕ) def= (cid:80)
i=1 P (xi ). The probability of a proﬁle
x:ϕ(x)=ϕ P (x). For example, if P is
B (2/3) over h and t, then for n = 3, P ({3}) = P (hhh) + P (ttt) = 1/3, P ({2, 1}) = P (hht) + P (hth) + P (thh) +
P (tth) + P (tht) + P (htt) = 2/3, and P ({1, 1, 1} = 0 as this P is binary hence at most two symbols can appear.
On the other hand, if P is a roll of a fair die, then P ({3}) = 1/36, P ({2, 1}) = 5/12, and P ({1, 1, 1} = 5/9. We let
Φ = {P (ϕ) : P is a discrete i.i.d. distribution} be the collection of all distributions on Φn induced by any discrete
I n
i.i.d. distribution over any alphabet, possibly even inﬁnite.
It is easy to see that any relabeling of the elements in an i.i.d. distribution will leave the proﬁle distribution unchanged,
for example, if instead of h and t above, we have a distribution over 0’s and 1’s. Furthermore, proﬁles are sufﬁcient
statistics for every label-invariant property. While many theoretical properties of proﬁles are known, even calculating
the proﬁle probabilities for a given distribution and a proﬁle seems hard [23, 38] in general.
Proﬁle redundancy arises in at least two other machine-learning applications, closeness-testing and classiﬁcation.In
closeness testing [4], we try to determine if two sequences are generated by same or different distributions. In clas-
siﬁcation, we try to assign a test sequence to one of two training sequences. Joint proﬁles and quantities related to
proﬁle redundancy are used to construct competitive closeness tests and classiﬁers that perform almost as well as the
best possible [1, 2].
Proﬁles also arise in statistics, in estimating symmetric or label-invariant properties of i.i.d. distributions ([34] and
references therein). For example the support size, entropy, moments, or number of heavy hitters. All these properties
depend only on the multiset of probability values in the distribution. For example, the entropy of the distribution
p(heads) = .6, p(tails) = .4, depends only on the probability multiset {.6, .4}. For all these properties, proﬁles are
a sufﬁcient statistic.
1.5 Previous Results
As patterns and proﬁles have the same redundancy, we describe the results for proﬁles.
Instead of the expected redundancy R(I n
Φ ) that reﬂects the increase in the expected number of bits, [25] bounded the
more stringent but closely-related worst-case redundancy, ˆR(I n
(cid:32)
(cid:33)
Φ ), reﬂecting the increase in the worst-case number of
(cid:114) 2
bits, namely over all sequences. Using bounds [19] on the partition function, they showed that
π
3

Ω(n1/3 ) ≤ ˆR(I n
Φ ) ≤

n1/2 .

3

These bounds do not involve the alphabet size, hence show that unlike the sequences themselves, patterns (whose re-
dundancy equals that of proﬁles), though containing essentially all the information of the sequence, can be compressed
and learned with redundancy and log-loss diminishing as n−1/2 , uniformly over all alphabet sizes.
Note however that by contrast to i.i.d. distributions, where the redundancy (2) was determined up to a diminishing
additive constant, here not even the power was known. Consequently several papers considered improvements of these
bounds, mostly for expected redundancy, the minimax KL divergence.
Since expected redundancy is at most the worst-case redundancy, the upper bound applies also for expected redun-
dancy. Subsequently [31] described a partial proof-outline that could potentially show the following tighter upper
(cid:18) n
(cid:19)1/3 ≤ R(I n
bound on expected redundancy, and [14] proved the following lower bound, strengthening one in [32],
Φ ) ≤ n0.4 .
log n

1.84

(3)

1.6 New results
In Theorem 15 we use error-correcting codes to exhibit a larger class of distinguishable distributions in I n
Φ than was
known before, thereby removing the log n factor from the lower bound in (3). In Theorem 11 we demonstrate a small
number of distributions such that every distribution in I n
Φ is within a small KL divergence from one of them, thereby
reducing the upper bound to have the same power as the lower bound. Combining these results we obtain,
Φ , ) ≤ R(I n
Φ ) ≤ n1/3 log2 n.
0.3 · n1/3 ≤ (1 − ) log M (I n

(4)

These results close the power gap between the upper and lower bounds that existed in the literature. They show that
when a pattern is compressed or a sequence is estimated (with all unseen elements combined into new), the per-symbol
redundancy and log-loss decrease to 0 uniformly over all distributions faster than log2 n/n2/3 , a rate that is optimal
up to a log2 n factor. They also show that for length-n proﬁles, the redundancy R(I n
Φ ) is essentially the logarithm
log M (I n
Φ , ) of the number of distinguishable distributions.

1.7 Outline

In the next section we describe properties of Poisson sampling and redundancy that will be used later in the paper.
In Section 3 we establish the upper bound and in Section 4, the lower bound. Most of the proofs are provided in the
Appendix.

2 Preliminaries
We describe some techniques and results used in the proofs.

2.1 Poisson sampling

When a distribution is sampled i.i.d. exactly n times, the multiplicities are dependent, complicating the analysis of
many properties. A standard approach [22] to overcome the dependence is to sample the distribution a random poi(n)
times, the Poisson distribution with parameter n, resulting in sequences of random length near close to n. We let
poi(λ, µ) def= e−λλµ/µ! denote the probability that a poi(λ) random variable attains the value µ.
The following basic properties of Poisson sampling help simplify the analysis and relate it to ﬁxed-length sampling.
Lemma 1. If a discrete i.i.d. distribution is sampled poi(n) times then: (1) the number of appearances of different
symbols are independent; (2) a symbol with probability p appears poi(np) times; (3) for any ﬁxed n0 , conditioned on
the length poi(n) ≥ n0 , the ﬁrst n0 elements are distributed identically to sampling P exactly n0 times.
We now express proﬁle probabilities and redundancy under Poisson sampling. As we saw, the probability of a proﬁle is
determined by just the multiset of probability value and the symbol labels are irrelevant. For convenience, we assume
that the distribution is over the positive integers, and we replace the distribution parameters {pi } by the Poisson
def= npi , and Λ = {λ1 , λ2 , . . .}. The proﬁle generated
parameters {npi }. For a distribution P = {p1 , p2 , . . .}, let λi

4

Λ(ϕ) =

(5)

poi(λσ(i) , µi ).

by this distribution is a multiset ϕ = {µ1 , µ2 , . . .}, where each µi generated independently according to poi(λi ). The
(cid:89)
(cid:88)
probability that Λ generates ϕ is [1, 25],
1(cid:81)∞
µ=0 ϕµ !
σ
i
where the summation is over all permutations of the support set.
For example, for Λ = {λ1 , λ2 , λ3}, the proﬁle ϕ = {2, 2, 3} can be generated by specifying which element appears
three times. This is reﬂected by the ϕ2 ! in the denominator, and each of the repeated terms in the numerator are
counted only once.
to denote the class of distributions induced on Φ∗ ∆= Φ0 ∪ Φ1 ∪ Φ2 ∪ . . . when sequences
Similar to I n
Φ , we use I poi(n)
Φ
of length poi(n) are generated i.i.d.. It is easy to see that a distribution in I poi(n)
is a collection of λi ’s summing to n.
Φ
), and -distinguishability M (I poi(n)
The redundancy R(I poi(n)
, ) are deﬁned as before. The following lemma shows
Φ
Φ
that bounding M (I poi(n)
) is sufﬁcient to bound R(I n
, ) and R(I poi(n)
Φ ).
Φ
Φ
Lemma 2. For any ﬁxed  > 0,
(1 − o(1))R(I n−√
) ≤ R(I poi(n)
n log n
)
Φ
Φ
Φ ) and M (I n
Proof Sketch. It is easy to show that R(I n
the probability that poi(n) is less than n − √
√
Φ , ) are non-decreasing in n. Combining this with the fact that
(cid:4)
n log n or greater than n +
n log n goes to 0 yields the bounds.

√
, ) ≤ M (I n+
Φ

and M (I poi(n)
Φ

n log n

, 2).

Finally, the next lemma, proved in the Appendix, provides a simple formula for cross expectations of Poisson distri-
butions.
(cid:21)
(cid:18) (λ1 − λ0 )(λ2 − λ0 )
(cid:20) poi(λ2 , µ)
(cid:19)
Lemma 3. For any λ0 , λ1 , λ2 > 0,
Eµ∼poi(λ1 )
λ0
poi(λ0 , µ)

= exp

.

2.2 Redundancy

We state some basic properties of redundancy.
For a distribution P over A and a function f : A → B , let f (P ) be the distribution over B that assigns to b ∈ B
the probability P (f −1 (b)). Similarly, for a collection P of distributions over A, let f (P ) = {f (P ) : P ∈ P }. The
convexity of KL-divergence shows that D(f (P )||f (Q)) ≤ D(P ||Q), and can be used to show
Lemma 4 (Function Redundancy). R(f (P )) ≤ R(P ).
For a collection P of distributions over A × B , let PA and PB be the collection of marginal distributions over A and B ,
respectively. In general, R(P ) can be larger or smaller than R(PA ) + R(PB ). However, when P consists of product
distributions, namely P (a, b) = PA (a) · PB (b), the redundancy of the product is at most the sum of the marginal
redundancies. The proof is given in the Appendix.
Lemma 5 (Redundancy of products). If P be a collection of product distributions over A × B , then
R(P ) ≤ R(PA ) + R(PB ).
For a preﬁx-free code C : A → {0, 1}∗ , let EP [|C |] be the expected length of C under distribution P . Redundancy is
the extra number of bits above the entropy needed to encode the output of any distribution in P . Hence,
Lemma 6. For every preﬁx-free code C , R(P ) ≤ maxP ∈P EP [|C |].
(cid:91)
Lemma 7 (Redundancy of unions). If P1 , . . . , PT are distribution collections, then
Pi ) ≤ max
R(Pi ) + log T .
1≤i≤T
1≤i≤k

R(

5

In Subsection 3.2 we show that

3 Upper bound
A distribution in Λ ∈ I poi(n)
is a multiset of λ’s adding to n. For any such distribution, let
Φ
def= {λ ∈ Λ : λ ≤ n1/3 }, Λmed
def= {λ ∈ Λ : n1/3 < λ ≤ n2/3}, Λhigh
def= {λ ∈ Λ : λ > n2/3 },
Λlow
and let ϕlow , ϕmed , ϕhigh denote the corresponding proﬁle each subset generates. Then ϕ = ϕlow ∪ ϕmed ∪ ϕhigh . Let
Iϕlow = {Λlow : Λ ∈ I poi(n)
} be the collection of all Λlow . Note that n is implicit here and in the rest of the paper.
Φ
A distribution in Iϕlow is a multiset of λ’s such that each is ≤ n1/3 and they sum to either n or to ≤ n − n1/3 . Iϕmed
and Iϕhigh are deﬁned similarly.
ϕ is determined by the triple (ϕlow , ϕmed , ϕhigh ), and by Poisson sampling, ϕlow , ϕmed and ϕhigh are independent.
Hence by Lemmas 4 and 5,
R(I n
Φ ) ≤ R(I(ϕlow ,ϕmed ,ϕhigh ) ) ≤ R(Iϕlow ) + R(Iϕmed ) + R(Iϕhigh ).
In Subsection 3.1 we show that Iϕlow < 4n1/3 log n and Iϕhigh < 4n1/3 log n.
Iϕmed < 1
2 n1/3 log2 n.
In the next two subsections we elaborate on the overview and sketch some proof details.
3.1 Bounds on R(Iϕlow ) and R(Iϕhigh )
Elias Codes [12] are preﬁx-free codes that encode a positive integer n using at most log n + log(log n + 1) + 1 bits.
We use Elias codes and design explicit coding schemes for distributions in Iϕlow and Iϕhigh , and prove the following
result.
Lemma 8. R(Iϕlow ) < 4n1/3 log n, and R(Iϕhigh ) < 2n1/3 log n.
Proof. Any distribution Λhigh ∈ Iϕhigh consists of λ’s that are > n2/3 and add to ≤ n. Hence |Λhigh | is < n1/3 , and
so is the number of multiplicities in ϕhigh . Each multiplicity is a poi(λ) random variable, and is encoded separately
using Elias code. For example, the proﬁle {100, 100, 200, 250, 500} is encoded by coding the sequence 100, 100, 200,
250, 500 all using Elias scheme. For λ > 10, the number of bits needed to encode a poi(λ) random variable using
Elias codes can be shown to be at most 2 log λ. The expected code-length is at most n1/3 · 2 log n. Applying Lemma 6
gives R(Iϕhigh ) < 2n1/3 log n.
A distribution Λlow ∈ Iϕlow consists of λ’s less that < n1/3 and sum at most n. We encode distinct multiplicities along
with their prevalences, using two integers for each distinct multiplicity. For example, ϕ = {1, 1, 1, 1, 1, 2, 2, 2, 5} is
coded as 1, 5, 2, 3, 5, 1. Using Poisson tail bounds, we bound the largest multiplicity in ϕlow , and use arguments similar
to Iϕhigh to obtain R(Iϕlow ) < 4n1/3 log n.
(cid:4)
3.2 Bound on R(cid:0)Iϕmed
(cid:1)
We partition the interval (n1/3 , n2/3 ] into B = n1/3 bins. For each distribution in Iϕmed , we divide the λ’s in it
according to these bins. We show that within each interval, there is a uniform distribution such that the KL divergence
between the underlying distribution and the induced uniform distribution is small. We then show that the number of
uniform distributions needed is at most exp(n1/3 log n). We expand on these ideas and bound R(Iϕmed ).
(cid:1). A distribution Λ = {λ1 , λ2 , . . . , λr } ∈ Iϕmed is such that λi ∈
Lemma 7 to obtain an upper bound on R(cid:0)Iϕmed
We partition Iϕmed into T ≤ exp(n1/3 log n) classes, upper bound the redundancy of each class, and then invoke
[n1/3 , n2/3 ] and (cid:80)r
i=1 λi ≤ n.
into B def= n1/3
Consider any partition of
consecutive intervals
I1 , I2 , . . . , IB of
lengths
(n1/3 , n2/3 ]
∆1 , ∆2 , . . . , ∆B .For each distribution Λ ∈ Iϕmed , let Λj
def= {λj,l : l = 1, 2, . . . , mj } def= {λ : λ ∈ Λ ∩ Ij } be
def= mj (Λ) def= |Λj | is the number of elements of Λ in Ij . Let
the set of elements of Λ in Ij where mj
τ (Λ) def= (m1 , m2 , . . . , mB )

6

be the B−tuple of the counts of λ’s in each interval.
For example, if n = 1000, then n1/3 = 10 and n2/3 = 100. For simplicity, we choose B = 3 instead of
n1/3 and ∆1 = 10, ∆2 = 30, ∆3 = 50, so the intervals are I1 = (10, 20], I2 = (20, 50], I3 = (50, 100].
Suppose, Λ = {12, 15, 25, 35, 32, 43, 46, 73}, then Λ1 = {12, 15}, Λ2 = {25, 35, 32, 43, 46}, Λ3 = {73} and
τ (Λ) = (m1 , m2 , m3 ) = (2, 5, 1).
We partition Iϕmed , such that two distributions Λ and Λ(cid:48) are in the same class if and only if τ (Λ) = τ (Λ(cid:48) ). Thus each
class of distributions is characterized by a B -tuple of integers τ = (m1 , m2 , . . . , mB ) and let Iτ denote this class. Let
We ﬁrst bound T below. Observe that for any Λ ∈ Iϕmed , and any j , we have mj < n2/3 , otherwise (cid:80)
T def= T (∆) be the set of all possible different τ (such that Iτ is non-empty), and T = |T | be the number of classes.
λ∈Λ λ >
def= n1/3 + (cid:80)j−1
mj · n1/3 = n. So, each mj in τ can take at most n2/3 < n values. So, T < (n2/3 )B < nn1/3
= exp(n1/3 log n).
For any choice of ∆, let λ−
i=1 ∆i be the left end point of the interval Ij for j = 1, 2, . . . , B . We upper
j
bound R(Iτ ) of any particular class τ = (m1 , m2 , . . . , mB ) in the following result.
Lemma 9. For all choices of ∆ = (∆1 , . . . , ∆B ), and all classes Iτ such that τ = (m1 , . . . , mB ) ∈ T (∆),
R(Iτ ) ≤ B(cid:88)
∆2
j
λ−
j
j=1
all Λ ∈ Iτ , D(Λ||Λ∗ ) ≤ (cid:80)B
Proof Sketch. For any choice of ∆, τ = (m1 , . . . , mB ) ∈ T (∆), we show a distribution Λ∗ ∈ Iτ such that for
. Recall that for Λ ∈ Iτ , Λj is the set of elements of Λ in Ij . Let ϕj
∆2
j
j=1 mj
λ∗
be the proﬁle generated by Λj . Then, ϕmed = ϕ1 ∪ . . . ∪ ϕB . The distribution Λ∗ is chosen to be of the form
j
{λ∗
1 ×m1 , λ∗
2 ×m2 , . . . , λ∗
B ×mB }, i.e., each Λ∗
j is uniform. The result follows from Lemma 3, and the details are in
(cid:4)
the Appendix .

mj

.

We now prove that R(Iϕmed ) < 1
2 n1/3 log2 n.
By Lemma 7 it sufﬁces to bound R(Iτ ). From Theorem 9 it follows that the choice of ∆ determines the bound on
B(cid:88)
B(cid:88)
R(Iτ ). A solution to the following optimization problem yields a bound :
∆2
j ≤ n.
mj λ−
j
λ−
j
j=1
j=1

, subject to

min
∆

max
τ

mj

Instead of minimizing over all partitions, we choose the endpoints of the intervals as a geometric series as a bound for
the expression. The left-end point of Ij is λ−
j , so λ−
1 = n1/3 . We let λ−
j+1 = λ−
j (1 + c). The constant c is chosen to
ensure that λ−
= n2/3 , the right end-point of IB . This yields, c < 2 log(1+c) = 2 log(n1/3 )
1 (1+c)B = n1/3 (1+c)n1/3
.
n1/3
j+1 − λ−
j = cλ−
Now, ∆j = λ−
= c2λ−
j , so ∆2
j . This translates the objective function to the constraint, and is in fact
j
−
λ
the optimal intervals for the optimization problem (details omitted). Using this, for any τ = (m1 , . . . , mB ) ∈ T (∆),
j
(cid:19)2
(cid:18) 2 log(n1/3 )
B(cid:88)
B(cid:88)
n1/3
j=1
j=1

j ≤ c2n <
mj λ−

n1/3 log2 n.

∆2
j
λ−
j

= c2

n =

mj

4
9

This, along with Lemma 7 gives the following Corollary for sufﬁciently large n.
Corollary 10. For large n, R(Iϕmed ) < 1
2 · n1/3 log2 n.
Combining Lemma 8 with this result yields,
Theorem 11. For sufﬁciently large n,

Φ ) ≤ n1/3 log2 n.
R(I n

7

4 Lower bound
We use error-correcting codes to construct a collection of 20.3n1/3 distinguishable distributions, improving by a loga-
rithmic factor the bound in [14, 31].
1/2. Then, D(P ||Q) ≥ (1 − ) log (cid:0) 1
(cid:1) − h().
The convexity of KL-divergence can be used to show
Lemma 12. Let P and Q be distributions on A. Suppose A1 ⊂ A be such that P (A1 ) ≥ 1 −  > 1/2, Q(A1 ) ≤ δ <
δ
We use this result to show that (1 − ) log M (P , ) ≤ R(P ). Recall that for P over A, M def= M (P , ) is the largest
such that Pj (Aj ) ≥ 1−. Let Q0 be the distribution such that, R(P ) = supP ∈P D(P ||Q0 ). Since (cid:80)M
number of −distinguishable distributions in P . Let P1 , P2 , . . . , PM in P and A1 , A2 , . . . , AM be a partition of A
j=1 Q0 (Aj ) = 1,
Q0 (Am ) < 1
M for some m ∈ {1, . . . , M }. Also, Pm (Am ) ≥ 1 − . Plugging in P = Pm , Q = Q0 , A1 = Am , and
δ = 1/M in the Lemma 12,
R(P ) ≥ D(Pm ||Q0 ) ≥ (1 − ) log (M (P , )) − h().

def= C i2 , K def= (cid:98)(3n/C )1/3 (cid:99), and
We now describe the class of distinguishable distributions. Fix C > 0. Let λ∗
i
i : xi = 1} ∪ (cid:110)
(cid:111)
i : 1 ≤ i ≤ K }. K is chosen so that sum of elements in S is at most n. Let x = x1x2 . . . xK be a binary
S def= {λ∗
n − (cid:88)
string and
def= {λ∗
λ∗
Λx
i xi
.
The distribution contains λ∗
i whenever xi = 1, and the last element ensures that the elements add up to n. A binary
code of length k and minimum distance dmin is a collection of k−length binary strings with Hamming distance
between any two strings is at least dmin . The size of the code is the number of elements (codewords) in it. The
following shows the existence of codes with a speciﬁed minimum distance and size.
2 > α > 0. There exists a code with dmin ≥ αk and size ≥ 2k(1−h(α)−o(1)) .
Lemma 13 ([30]). Let 1
Let C be a code satisfying Lemma 13 for k = K and let L = {Λc : c ∈ C } be the set of distributions generated by
using the strings in C . The following result shows that distributions in L are distinguishable and is proved in Appendix
.
Lemma 14. The set L is 2e−C/4
α −distinguishable.
Plugging α = 5 × 10−5 and C = 60, then Lemma 13 and Equation (1) yields,
Theorem 15. For sufﬁciently large n,

0.3 · n1/3 ≤ R(I n
Φ ).

Acknowledgments
The authors thank Ashkan Jafarpour and Ananda Theertha Suresh for many helpful discussions.
References
[1] J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing. J. of Machine Learning Research -
Proceedings Track, 19:47–68, 2011.
[2] J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, S. Pan, and A. T. Suresh. Competitive classiﬁcation and closeness testing.
Journal of Machine Learning Research - Proceedings Track, 23:22.1–22.18, 2012.
[3] A. R. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions
on Information Theory, 44(6):2743–2760, 1998.
[4] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In Annual Symposium on
Foundations of Computer Science, page 259, 2000.
[5] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
[6] K. Chaudhuri and A. McGregor. Finding metric structure in information theoretic clustering. In Conference on Learning
Theory, pages 391–402, 2008.
[7] T. Cover. Universal portfolios. Mathematical Finance, 1(1):1–29, January 1991.

8

IEEE Transactions on Information Theory,

[8] T. Cover and J. Thomas. Elements of Information Theory, 2nd Ed. Wiley Interscience, 2006.
[9] L. Davisson. Universal noiseless coding. IEEE Transactions on Information Theory, 19(6):783–795, November 1973.
[10] L. D. Davisson, R. J. McEliece, M. B. Pursley, and M. S. Wallace. Efﬁcient universal noiseless source codes. IEEE Transac-
tions on Information Theory, 27(3):269–279, 1981.
[11] M. Drmota and W. Szpankowski. Precise minimax redundancy and regret.
50(11):2686–2707, 2004.
[12] P. Elias. Universal codeword sets and representations of the integers. IEEE Transactions on Information Theory, 21(2):194–
203, Mar 1975.
[13] B. M. Fitingof. Optimal coding in the case of unknown and changing message statistics. Probl. Inform. Transm., 2(2):1–7,
1966.
[14] A. Garivier. A lower-bound for the maximin redundancy in pattern coding. Entropy, 11(4):634–642, 2009.
IEEE Transactions on Information Theory,
[15] G. M. Gemelos and T. Weissman. On the entropy rate of pattern processes.
52(9):3994–4007, 2006.
[16] P. Gr ¨unwald. A tutorial introduction to the minimum description length principle. CoRR, math.ST/0406077, 2004.
[17] P. Gr ¨unwald, J. S. Jones, J. de Winter, and ´E. Smith. Safe learning: bridging the gap between bayes, mdl and statistical
learning theory via empirical convexity. J. of Machine Learning Research - Proceedings Track, 19:397–420, 2011.
[18] P. D. Gr ¨unwald. The Minimum Description Length Principle. The MIT Press, 2007.
[19] G. Hardy and S. Ramanujan. Asymptotic formulae in combinatory analysis. Proceedings of London Mathematics Society,
17(2):75–115, 1918.
[20] J. Kelly. A new interpretation of information rate. IEEE Transactions on Information Theory, 2(3):185–189, 1956.
[21] N. Merhav and M. Feder. Universal prediction. IEEE Transactions on Information Theory, 44(6):2124–2147, October 1998.
[22] M. Mitzenmacher and E. Upfal. Probability and computing - randomized algorithms and probabilistic analysis. Cambridge
University Press, 2005.
[23] A. Orlitsky, S. Pan, Sajama, N. Santhanam, and K. Viswanathan. Pattern maximum likelihood: computation and experiments.
In preparation, 2012.
[24] A. Orlitsky, N. Santhanam, K. Viswanathan, and J. Zhang. On modeling proﬁles instead of values. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence, 2004.
[25] A. Orlitsky, N. Santhanam, and J. Zhang. Universal compression of memoryless sources over unknown alphabets. IEEE
Transactions on Information Theory, 50(7):1469– 1481, July 2004.
[26] A. Orlitsky, N. P. Santhanam, K. Viswanathan, and J. Zhang. Limit results on pattern entropy.
Information Theory, 52(7):2954–2964, 2006.
[27] J. Rissanen. Universal coding, information, prediction, and estimation. IEEE Transactions on Information Theory, 30(4):629–
636, July 1984.
[28] J. Rissanen. Fisher information and stochastic complexity. IEEE Transactions on Information Theory, 42(1):40–47, January
1996.
[29] J. Rissanen, T. P. Speed, and B. Yu. Density estimation by stochastic complexity. IEEE Transactions on Information Theory,
38(2):315–323, 1992.
[30] R. M. Roth. Introduction to coding theory. Cambridge University Press, 2006.
[31] G. Shamir. A new upper bound on the redundancy of unknown alphabets. In Proceedings of The 38th Annual Conference on
Information Sciences and Systems, Princeton, New-Jersey, 2004.
[32] G. Shamir. Universal lossless compression with unknown alphabets—the average case. IEEE Transactions on Information
Theory, 52(11):4915–4944, November 2006.
[33] W. Szpankowski. On asymptotics of certain recurrences arising in universal coding. Problems of Information Transmission,
34(2):142–146, 1998.
[34] P. Valiant. Testing symmetric properties of distributions. PhD thesis, Cambridge, MA, USA, 2008. AAI0821026.
[35] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context-tree weighting method: basic properties. IEEE Transactions
on Information Theory, 41(3):653–664, 1995.
[36] Q. Xie and A. Barron. Asymptotic minimax regret for data compression, gambling and prediction. IEEE Transactions on
Information Theory, 46(2):431–445, March 2000.
[37] B. Yu and T. P. Speed. A rate of convergence result for a universal d-semifaithful code. IEEE Transactions on Information
Theory, 39(3):813–820, 1993.
[38] J. Zhang. Universal Compression and Probability Estimation with Unknown Alphabets. PhD thesis, UCSD, 2005.

IEEE Transactions on

9


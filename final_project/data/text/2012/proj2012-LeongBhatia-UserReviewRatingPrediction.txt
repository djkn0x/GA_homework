User Review Rating Prediction

Sammy Leong, Ashish Bhatia

Dec 14, 2012

Ob jective

Given a set of user review data each of which is
associated with a user rating, the ob jective is to
build a model that is able to predict numerical
rating from textual data.

reviews are interleaved. We had to throw out
a lot of positive reviews but we're still left with
abundant training examples.

Overview

Success Metric

To solve this problem[4, 3], rst we evaluated
a number of learning algorithms such as Logis-
tic Regression, Naive Bayes, and SVM on the
original raw data as-is to get a sense of their
feasibility, capability, and speed. Then we devel-
oped our own pre-processing pipeline to trans-
form the original data to better emphasize senti-
mental features, which we evaluated with one of
the learning algorithms.
Based on our analysis, we removed several re-
dundant and noisy features to further improve
the accuracy of our pipeline.
Pro ject code (excluding Yelp data whose
distribution
is
prohibited)
is
available
https://github.com/ashishb/farsight

Data Source

We gathered our user review data from the Yelp
academic data set[1] which contains user reviews
of local businesses near Stanford University.
Each review contains a text sequence repre-
senting the user review along with a numerical
rating that ranges from 1 to 5. The rst thing we
did was relabel each review as positive or nega-
tive. Reviews with ratings 1, 2 are labeled as
negative, reviews with ratings 4, 5 are labeled as
positive, and reviews with rating 3 are randomly
labeled as positive or negative.
As it turns out, 80% of the reviews are positive
which means the data is highly skewed to begin
with. To address this, we reshued the data
such that we have equal number of positive and
negative reviews, and that positive and negative

Our goal is to nd a learning algorithm along
with data processing techniques that give us the
highest generalization accuracy. To do this, we
divided our data in half, trained with the rst
half, and then tested with the second half. The
former gives us the training accuracy while the
latter gives us the generalization accuracy.

First Attempt (Baseline)

For our rst attempt we focused on trying out
various learning algorithms on the original un-
processed data as-is to get a sense of their capa-
bility for our given problem. Another goal is to
choose the fastest algorithm which gives reason-
ably good results and use it to iterate while we
work on our pre-processing pipeline.

Results

Here is a table that shows the training and gener-
alization accuracies for each learning algorithm,
along with the time it took to run. The experi-
ments were done using 14000 training examples
and 14000 testing examples.

1

81.27

1.05

81.67
78.71

1.58
2.86

Train Test Time
Matlab
91.67
Naive Bayes (mn)
Liblinear
97.39
Logistic Regression
99.69
L2-reg SVM (linear)
Libsvm
98.99
70.26
65.11
87.78
88.80
84.78

C-SVM (linear)
C-SVM (radial)
C-SVM (sigmoid)
nu-SVM* (linear)
nu-SVM* (radial)
nu-SVM* (sigmoid)

78.86
69.68
65.43
83.75
83.91
81.49

786
356
359
277
296
291

* nu = 0.5

Discussion

Firstly, it's quite clear that liblinear runs sig-
nicantly faster than libsvm. For this reason,
we will iterate using liblinear when we evaluate
the performance of various pre-processing tech-
niques. In particular, we will use logistic regres-
sion because it has higher generalization accu-
racy than L2-regularized SVM with linear ker-
nel.
Secondly, it appears that nu-SVM with nu =
0.5 in this case improved both the training and
generalization accuracies signicantly (as com-
pared to the counterpart C-SVM results).
In
fact, the we were able to achieve the highest gen-
eralization accuracy using nu-SVM with the ra-
dial basis kernel. The only issue is that it takes
a very long time to run. For this reason we will
only revisit it after we've settled on a good pre-
processing pipeline.

Second Attempt

For our second attempt, we focused on data pro-
cessing and feature selection with the goal of re-
ducing the noise in the data as much as possi-
ble. Here we only made use of logistic regression
with 14000 training examples and 14000 testing
examples. What follows are the data processing
techniques that we used.

Spell Correction

The rst thing we did was to apply spell check-
ing on the reviews and replace mispelled words
with suggested corrections. The idea here is that
user reviews on the internet are often lled with

2

misspelled words. One positive review may con-
tain the word good while another may contain
gooodi. We want to treat both signals as rep-
resenting positive reviews.

Stemming

After spell correction, we further consolidated
words to their canonical forms by applying stem-
ming. For example, one positive review may con-
tain the word perfect while another may con-
tain perfection. Again, we want to treat both
signals as representing positive reviews. Stem-
ming converts both words to their root:
per-
fect.

Stopword Removal

In this step we removed stop words that are be-
lieved to add little relevence to reviews. Words
like the, I, was, etc. are stop words and thus
removed. One caveat is that we did not remove
the words no and not which will be explained
in the bigram generation section that follows.

Bi-gram Generation

After spell correction, stemming, and stopword
removal, we moved on to bi-gram generation.
This step is very important because it allows us
to capture semantics that are not captured in
the uni-grams or worst have opposite meaning
altogether. For example really good is a much
stronger signal for positive reviews than pretty
good or just good. Similarly, not good, if
captured inidividually contains the word good
which falsely indicates a positive review. We can
x that by adding the bigram not-good but
now we have a problem where the review con-
tains contradictory signals. To x that, we re-
move the word good and keep only not-good
which is a clear indication of a negative review.

Results

Here is a table that shows the training and
generalization accuracies. The Individual sec-
tion shows experiment results where we used
each data processing technique individually. The
Incremental section shows experiment results
where we incrementally combined data process-
ing techniques. Finally, the Optimal section
shows experiment results where we combined

only data processing techniques that we believe
to give us optimal results.

Train
Individual
97.3931
Baseline
97.1862
Spell Correction
95.5655
Stemming
97.1724
Stopword Rem
99.9241
Bi-grams
Incremental
97.3931
Baseline
97.1862
+ Spell Correction
95.5034
+ Stemming
+ Stopword Rem 95.3241
99.7379
+ Bi-grams
Optimal
99.9586
Spell + Bi-grams

Test

81.6759
82.3517
82.1586
82.6759
84.4621

81.6759
82.3517
82.0000
81.5586
83.2966

84.5103

Here is a graph that shows the number of
training examples vs training/generalization er-
ror for the case of using only spell correction and
bi-grams.

Discussion

Individually, every data processing technique im-
proved the generalization accuracy but all of
them with the exception of bi-grams, also re-
duced training accuracy. When combined incre-
mentally, however, it appears that stemming and
stopword removal actually reduces generalization
accuracy.
From the individual and incremental experi-
ment results, we gathered that the most promis-
ing data processing techniques are spell correc-
tion and bi-grams. Thus we combined only those

3

two and indeed we achieved training and general-
ization accuracies that were superior to the rest.
Looking at the training error vs generalization
error, it is as expected that by adding more train-
ing examples, the training error (bias) goes up
where as the generalization error (variance) goes
down. However the trend suggests that as we
add more training examples, training error will
continue to go up where as the generalization
error will likely at o. Judging from this we
will have to either explore other pre-processing
techniques, other learning algorithms, or other
tuning techniques.

Further enhancements

To further rene our results, we focused on data
processing and feature selection with the goal of
reducing redundant features in the data. Just
like in second attempt, we only made use of logis-
tic regression with 14000 training examples and
14000 testing examples. What follows are the
extra data processing techniques that we tried.

Using tri-grams (and beyond)

We tried using tri-grams, quad-grams and even
penta-grams. The assumption here was that the
it will catch phrases like not so good, was
hardly any better but it turns out that these
phrases have really sparse density. Therefore,
we did not get any signicant gains out of it. In
some trials, the accuracy even took a dip.

Respecting natural phrase bound-
aries for bi-grams

Earlier, we were forming bigrams across bound-
aries like full stop, question marks and semi-
colons, this produced features where the rst
word was a part of previous phrase and second
part of another. During the analysis of mis-
classied results, we realized that this was creat-
ing a lot of noise and hence, we added checks to
ensure that we do create bi-grams across phrase
boundaries.

Importance of parts of speech

A detailed analysis of results showed that proper
nouns like pizza, pasta as well as determin-
ers like which were too frequent and therefore,
ended up in features. To clean this up, we de-
cided to consider only following parts of speech.

05000100001500000.050.10.150.20.250.30.35training exampleserror (%)Training Example vs Error  traininggeneralizationThis cleanup, though, made the feature extrac-
tion phase really slow, gave us signcant boost in
accuracy. We used NLTK[2] for parts-of-speech
tagging.
POS Tag

Description

Example

CC

CD

EX

JJ

JJR

JJS

RB

RBR

RBS

VB

VBD

VBG

VBN

VBP

VBZ

WP

WP$

WRB

Conjunction

Cardinal number

existential there

adjective

adj. comparative

adj., superlative

adverb

adverb, comparative

adverb, superlative

verb, base

verb, past tense

And

1, 2

there is

green

greener

greenest

good

better

best

take

took

verb, present participle

taking

verb, past participle

verb, singular present

verb, 3-rd person

wh-pronoun

wh-possessive

wh-adverb

taken

take

takes

who

whose

when

Following parts of speech were found to be use-
less and were ignored
Description
POS Tag

Example

FW

IN

LS

MD

NN

NNS

NNP

PDT

POS

PRP

RP

TO

UH

foreign word

d'hoevre

preposition conjunction

in, of

list marker

modal

noun, singular

noun, plural

proper noun

predeterminer

1)

will

chair

chairs

Ashish

both

possessive ending

friend's

personal pronoun

particle

to

interjection

he

give

to him

hmm

which

WDT

wh-determiner

Results

Here is a table that shows the training and
generalization accuracies. The Individual sec-
tion shows experiment results where we used
each data processing technique individually. The
Incremental section shows experiment results
where we incrementally combined data process-
ing techniques. Finally, the Optimal section
shows experiment results where we combined

4

only data processing techniques that we believe
to give us optimal results.

Train

Test

Individual

Baseline(spell + Bigrams)

99.9586

84.5103

Tri-grams

Quad-grams

99.9602

84.6311

99.7413

83.4371

Phase boundaries

99.9754

85.6752

POS selection

99.9723

90.1685

Incremental

Baseline (spell + Bi-grams)

99.9586

84.5103

+ Tri-grams

+ Quad-grams

99.9602

84.6311

99.7731

83.4513

+ Phase boundaries (PB)

99.9013

85.1846

+ POS selection

99.8979

89.5943

Optimal

Baseline+Bigrams+PB+POS

99.9679

90.3476

Here is a graph that shows the number of
training examples vs training/generalization er-
ror for the case of spell correction + bi-grams +
phase-boundaries + POS (parts-of-speech) selec-
tion.

Discussion

Clearly, using k-grams for k>2 is too noisy. Ig-
noring bi-grams across phrase boundaries (like
full stop) improved the results and using Natu-
ral language tool kit to tag parts of speech and
then removing extraneous parts of speech gave a
huge boost to accuracy of our pipeline.

References

[1] Yelp's Academic Dataset. On request, 2012.

[2] Steven Bird. Nltk: The natural language
toolkit, 2002.

020004000600080001000012000024681012141618training exampleserror (%)Training Example vs Error  traininggeneralization[3] Shoushan Li, Sophia Y. Lee, Ying Chen,
Chu-Ren Huang, and Guodong Zhou. Sen-
timent Classication and Polarity Shifting.
In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COL-
ING '10, pages 635643, Stroudsburg, PA,
USA, 2010. Association for Computational
Linguistics.

[4] Lizhen Qu, Georgiana Ifrim, and Gerhard
Weikum. The bag-of-opinions method for re-
view rating prediction from sparse text pat-
terns.
In Proceedings of the 23rd Interna-
tional Conference on Computational Linguis-
tics, COLING '10, pages 913921, Strouds-
burg, PA, USA, 2010. Association for Com-
putational Linguistics.

5


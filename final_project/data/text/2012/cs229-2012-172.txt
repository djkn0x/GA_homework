Predicting the Daily Liquidity of Corporate Bonds

Louis Ben Arous
December 14, 2012

Introduction

Methods

Let us deﬁne the liquidity of a ﬁnancial asset as
its volume, or the number of transactions of that
asset during a particular time window. Due to the
lack of a ma jor electronic market for ﬁxed income
instruments, many liquidity issues may arise when
trading corporate bonds, where one security might
become highly illiquid and create huge price im-
pacts for investors, as they ﬁnd themselves unable
to close their positions.
This research will set up, test and compare statisti-
cal predictors from diﬀerent ﬁelds (time series anal-
ysis and machine learning) to accurately forecast
the daily volume of US investment grade corporate
bonds. The goal is to ﬁnd a global machine learning
algorithm that outperforms univariate time series
models out-of-sample.

Data

To implement our study, we need daily data for
investment grade US corporate bonds. The ques-
tion that arises is which bonds should we look at to
create an appropriate predictor that could be ap-
plied to other bonds. The approach taken in this
study is to look at the composition of the Barclays
US aggregate index (AGG) , and to pick the top
28 corporate bonds that have the highest weight in
this index. The AGG is the most commonly fol-
lowed index for US ﬁxed income products, which
justiﬁes its use to pick the most relevant bonds.
I have grabbed and merged the volume and price
data for each of those bonds from Bloomberg into
one dataset for a time window from November 15,
2005 to November 15, 2012. Since some of those as-
sets were created well after 2005, special care must
be taken to handle gaps and missing values in the
data when building our predictors. I normalize my
data such that the liquidity values for each bond
have mean 0 and variance 1.

1

I use as measure of performance the average abso-
lute error (AAE) to compare models. The AAE of
a given model is deﬁned as:
n(cid:88)
i=1

| ˆyi − yi |

AAE =

1
n

where ˆyi is the prediction of the model for the ith
observation. The goal of this study is to ﬁnd a
machine learning learning algorithm with a lower
AAE than the time series AAE.
As mentioned earlier, the ﬁrst part of this study
is to ﬁnd the best possible univariate time series
model, and to ﬁt this model to each one of the 28
bonds in my dataset. As a result, I will have 28
sets of parameters (one for each bond), and I will
consider my general AAE for the time series case
as the average of the AAE of each model.
Then I will ﬁt diﬀerent machine learning models to
the complete dataset (all the bonds at once), and
see how the error of those models compare to the
time series error.

ARMA-EGARCH

Let lt,k be the volume of the kth bond at time t. We
have 28 time series lk for k = 1, ..., 28. There is no
reason for the volume of a corporate bond to be sta-
tionary, since the number of transactions a day of
an asset depends highly on the overall market cap-
italization of the underlying company, which may
grow or shrink over time. Since time series model
assume stationary inputs, I diﬀerence the lk series
to get the rk series, where rt,k = lt,k − lt−1,k .
A quick look at the autocorrelation function (ACF)
plots and partial autocorrelation (PACF) function
plots for each rk tells us that the rk are indeed sta-
tionary, and that the ACF cuts oﬀ at lag 2, and the
PACF cuts oﬀ between lag 4 and lag 8. Example:

Once the model is ﬁtted, I test the time series
model out-of-sample by ﬁtting the models only on
previous data, and then making the forecast. We
can easily make new prediction ˆrt+1,k , and compute
t−1(cid:88)
ˆlt+1,k as
i=0

ˆlt+1,k = l0,k +

rt−i,k + ˆrt+1,k

Assuming we have data from t=0 to t=T and that
we want to ﬁt our model on at least 15 values, we
T(cid:88)
compute the AAE of the kth bond as
t=15

|ˆlt+1,k − lt+1,k |

AAEk =

1
T

The results are shown at the end of this paper.

OLS and PCA

Now that I have ﬁtted our time series models, I
want to ﬁt global learning algorithms to the data
to see whether they fare better. Restricting our-
selves to the case where we only use previous val-
ues as features (as in time series models), the ﬁrst
approach to take is to try to ﬁt an ordinary least
square regression to ﬁt the previous daily volume
values to the current one. Let h the autocorrela-
h(cid:88)
tion lag (the number of past values I will look at),
and let:
i=1
where t has mean 0 and variance σ2 . Notice that
I seek to get the unique values θi for all k. To
do that, I set up my data in a panel data format,
where I set up a response variable y and a matrix
X such that:

θi lt−i,k + t

lt,k =

This hints that we should ﬁt an ARMA(p, q)
model, with p between 4 and 8, and q=2. For sim-
plicity, instead of ﬁtting a diﬀerent ARMA model
to each series, I decided to ﬁt an ARMA(5, 2) to
all of them. To take into account volatility clus-
tering and asymmetric volatility responses due to
movements in rk , I also add a EGARCH compo-
nent to the model. I therefore ﬁt by MLE the fol-
lowing ARMA(5,2)-EGARCH(2,2) model to each
series by using the matlab garchﬁt function:
2(cid:88)
5(cid:88)
i=1
j=1
ut,k = σt,k t,k
2(cid:88)
2(cid:88)
βi,k log(σ2
log(σ2
t−i,k ) +
t,k ) = ωk +
(cid:26) (αj,k + γj,k )t,k − γj,kE |t,k |
j=1
i=1
(αj,k + γj,k )t,k − γj,kE |t,k |

t,k ≥ 0
t,k < 0

φi,k rt−i,k + ut,k +

rt,k = φ0,k +

ψj,k ut−j,k

fi (t−j,k )

fj (t,k ) =

Looking at qqplots, we see that the normal assump-
tion t ∼ N (0, 1) is violated. Hence, I assume t is
a standardized Student-t random variable (mean 0,
variance 1).

2

051015202530-0.50.00.51.0LagACFIBM.5.7.09.14.17051015202530-0.4-0.3-0.2-0.10.0LagPartial ACFIBM.5.7.09.14.17-4-2024-10010203040standardized resid QQplotTheoretical QuantilesSample Quantiles

lt,1
lt−1,1
...
lt,2
lt−1,2
...
lt−3,1
lt−2,1
lt−3,1
lt−4,1

y =

X =

lt−2,2
lt−3,2

· · ·
· · ·

· · ·
· · ·

lt−3,2
lt−4,2

lt−h,2
lt−h−1,2

lt−h,1
lt−h−1,1



lt−1,1
lt−2,1
...
lt−1,2
lt−2,2
...
To make my analysis, I remove any row with missing
values. I create a training sample and a test sample by
randomly selecting 30% of the total number of row in-
dices from X and y and by making the predictor matrix
Xtest and test response ytest from those selected rows.
The rest of X and y become Xtraining and ytraining . I
ﬁnd the parameter vector ˆθ by
trainingXtraining )−1X T
ˆθ = (X T
training ytraining
For a lag of 50, we see that our parameters θi for
i=1,..., 50 look as such:

This plot shows that the most relevant variables
seem to be the ﬁrst 5 lagged values in decreasing
order (the previous day is the most important, fol-
lowed by the second one, ...). Given we are looking
at a time series, this is fairly intuitive. This, cou-
pled with the fact that only the ﬁrst 5 lags seem to
have a pvalue lower than 0.05, tells us that h=50 is
too large, and that we are overﬁtting. Clearly, our
parameters have a large variance. I take two ap-
proaches to solve this issue. First, I do a stepwise

3

inclusion of my lagged variables using a bayesian
information criterion (BIC), and ﬁnd that my best
model is with h= 15. For h=15, my sample has ∼
18000 observations after removing missing values,
with the test set having 5000 observations and the
training set having 13000 observations. Xtraining is
therefore a 13000 by 15 matrix.
Secondly, I do PCA on the covariance matrix of
Xtraining , to see whether I can reduce the dimen-
sion of this problem. The next ﬁgure shows that
the ﬁrst 3 components (and mainly the ﬁrst one)
account for most of the variance.

ytraining = θ0 +

As such, let u1 , u2 and u3 be the top eigenvectors,
3(cid:88)
then I can represent the regression as:
i=1
I can then test the OLS and the OLS with PCA
factors models I get on Xtest and yset to get the
AAE of those two models. The results are listed in
the results section.

θiXtraining ui + t

Kernel Regression

Due to the nature of our kernel, I try to imple-
ment kernel regression with an exponential kernel
e−(t−ti )/τ to see whether it can ﬁlter out the noise.
Instead of ﬁtting τ by cross validation, a better
technique seems to be to do a lasso regression of
diﬀerent kernel ﬁts with diﬀerent τ . I take 10 val-
(cid:80)(h)
ues of τ , where τ1 = 1, τi = τi−1 ∗ 1.5.
(cid:88)
(cid:80)(h)
j=1 e(−j /τ )Xtraining ,i,j
j=1 e(−(j−1)/τ )
i=1
where I implement the lasso using the modiﬁed
least angle regression algorithm from the R lars

ytraining ,i = θ0 +

θi

Timemodel$coeff010203040500.000.050.100.15Comp.1Comp.3Comp.5Comp.7Comp.9PCAVariances0.00.51.01.52.02.53.03.5package, which ﬁnd the best ﬁt by k-fold cross-
validation. This ﬁt sets to 0 all of the θ except for
θ0 = −0.005, θ1 = 0.185, θ7 = 0.019, θ8 = 0.183,
and θ9 = 0.211.
We can see in the results section that the OLS re-
gression, the OLS regression on PCA factors and
the Kernel regression yield similar out-of-sample
AAE, which is higher than the average out-of-
sample AAE we get from our ARMA-EGARCH
models. This is a perfectly understandable result,
considering ARMA-EGARCH models do ﬁlter and
regress the data on its past values, as does kernel
regression, but do it one bond at a time, and by
modeling the variance as well. However, we can
ask ourselves whether there is another way to do
better than ARMA-EGARCH using Xtraining as
our predictor matrix.

Boosted Trees, SVR

The ﬁrst step is to use boosted trees using the
MART (Multiple Additive Regression Trees) algo-
rithm on our predictor matrix Xtraining . MART
computes a sum of regression trees of ﬁxed size (in
this case, only 6 leaves) which are added to the
model via a stagewise forward procedure. This im-
plies that each new regression tree is ﬁtted on the
residual of the current model. A regression tree has
J(cid:88)
the form:
i=1

γj I (x ∈ Rj )

T (x, θ) =

where θ = {Rj , γj }J
1 , and J is the number of leaves.
Another technique to try is using Support Vector
for Regression (SVR), with a radial basis kernel,
which is a modiﬁcation of Support Vector Machines
adapted to regression.
I ﬁt MART using the R
MART package and SVR using the R e1071 pack-
age.
We see in the result section that MART per-
forms better than OLS and Kernel regression, but
still does not match the ARMA-EGARCH models.
However, the diﬀerence between the SVR model
and the ARMA-EGARCH models is almost in-
signiﬁcant. As such, we have managed to match
the time series model using SVR. However, we
do not outperform, and training the SVR model
takes a much longer time than training the ARMA-
EGARCH Models, even though there are 28 of
them. As such, I try two other approaches:

4

• Using neural networks and seeing whether us-
ing additional depth can make us perform
better.
• Adding features to the model.

Will Deep Learning help?

I ﬁt a neural network with only 1 hidden layer com-
posed of 100 hidden units on the Xtraining , with
a learning rate of 0.5, a momentum of 0.2 and a
weight decay of 10−4 . I ﬁnd that this model per-
forms better than MART but not as well as SVR
and the time series model.
I then ﬁt a 4-hidden layers neural network with
100 hidden units in each layer.
I train this net-
work as a Deep Belief net, which means seeing the
network (minus the output node) as stacked re-
stricted Boltzmann machines. Those are trained
by stochastic gradient descent with Contrastive Di-
vergence of 1 iteration. The resulting parameters
are then used as initial values to train the neural
network via backpropagation.
I used the DeepLearnToolbox in matlab to do this
ﬁt. I ﬁnd that given the numbers of parameters, I
need to increase my weight decay to 10−3 for bet-
ter results. Adding (or subtracting) hidden layers
increases the AAE. As such, 4 hidden layers is my
best DBN model. We see that this model fares bet-
ter than the 1-hidden layer network, but not signif-
icantly better than SVR of the ARMA-EGARCH
models. As such, adding depth did not signiﬁcantly
improve my accuracy in this setting.

Adding Features

The only remaining way to improve our model is
to add more features. The ﬁrst feature I added was
the industry label, split between 1- Financials (JP
Morgan, Goldman Sachs), 2- Utilities (The Dow
Chemical Company, AT&T) 3- Industrials (IBM),
Consumers (Walmart). Out of the 28 bonds in my
dataset, 12 are Financials, 6 are Utilities, 5 are In-
dustrials and 5 are Consumers.
Additionally, I grabbed the price data for each
bond during the same time period (November 15
2005 to November 15 2012), and used returns and
volatility, where the returns are computed as
P ricet,k − P ricet−1,k
P ricet−1,k

Rt,k =

and the volatility is deﬁned as the standard de-
viation of the past 20 returns (since a month has
around 20 trading days).
For each volume lt,k , I then deﬁne its predictors to
be (as before) the previous 15 volume values for
this bond (lt−i,k where i=1,...,15), but also its past
4 returns values and 4 standard deviation values,
its industry label, as well as the past 2 volume,
returns and standard deviation values of all of the
other bonds in the sample. As such, I try to capture
whether returns and volatility can inﬂuence vol-
ume, whether bonds from diﬀerent industries have
diﬀerent behavior, as well as whether the volume
of a bond can be inﬂuenced by the volume, price
and volatility of other bonds.
There are a few issues with this model. First of
all, the number of missing values increases dras-
tically. If I were to remove rows with no missing
values, I would lose most of my sample. Moreover,
there are now (15+4+4+1+27*6)= 186 features in
the model, many of which may be redundant. As
such, I need a model that can perform well with
missing values, a mix of categorical and numerical
values and redundant variables. Neural networks
and SVR are of not much help here.
The best candidate in this case is MART, which
does enough regularization to remove unneeded
variables, and uses surrogate splits to accommo-
date for missing values. However, I realized that
after too many iterations, the MART algorithm is
overﬁtting the data. I therefore restricted the num-
ber of iterations to 150, and the number of leaves
in each tree to 4 instead of 6.
I ﬁnd that MART with this new extra set of
features outperforms on average the ARIMA-
EGARCH models, with a reasonable training time.

Results

Here are the summary statistics for the AAE val-
ues for the 28 ARMA(5,2)-EGARCH(2, 2) models.

Min
AAE 0.3878

Max
0.6301

Mean Median
0.5138
0.5196

Std
0.0654

And here are the sorted AAE values for the diﬀer-
ent models:

Model
OLS + PCA
Kernel
OLS
MART
Neural Net, 1 layer
SVR
ARMA(5,2)-GARCH(2,2) (Mean)
Neural Network-4 layers
MART + new features

AAE
0.6142
0.6028
0.5974
0.5442
0.5260
0.5197
0.5138
0.5119
0.4049

We see that ARMA-EGARCH models fare really
well against other learning algorithms. The error
of ARMA-EGARCH models is on average half the
standard deviation of data. We ﬁnd that using
MART with additional features such as an indus-
try label, past returns and volatility get us to an
error rate that is 40% of the standard deviation.
We see that this ﬁnal model gives us for any bond
in the sample approximately the minimum AAE we
can get with an ARMA-EGARCH model, with the
advantage of ﬁtting only one model to the data.

Going forward

I have restricted myself to study the behavior of
a limited number of investment grade corporate
bonds. This research could be extended to junk
bonds or other types of ﬁxed income products.
Similarly, we could add other features in the anal-
ysis, such as credit rating and maturity. Finally, I
have also restricted myself in this study to univari-
ate time series model. It could be valuable to test
how multivariate volatility models perform against
MART.

References
• Trevor Hastie, Robert Tibshirani, Jerome
Friedman, The Elements of Statistical Learn-
ing: Data Mining, Inference, and Prediction.
Second Edition. February 2009.
• Tze Leung Lai, Haipeng Xing, Statistical
Models and Methods for Financial Markets,
2008.
• Rasmus Berg Palm,Prediction as a candi-
date for learning deep hierarchical models of
data, http://www2.imm.dtu.dk/pubdb/views/
publication_details.php?id=6284 2012.

5


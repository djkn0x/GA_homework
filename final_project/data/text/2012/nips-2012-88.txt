Topology Constraints in Graphical Models

Marcelo Fiori
Universidad de la
Rep ´ublica, Uruguay
mfiori@fing.edu.uy

Pablo Mus ´e
Universidad de la
Rep ´ublica, Uruguay
pmuse@fing.edu.uy

Guillermo Sapiro
Duke University
Durham, NC 27708
guillermo.sapiro@duke.edu

Abstract

Graphical models are a very useful tool to describe and understand natural phe-
nomena, from gene expression to climate change and social interactions. The
topological structure of these graphs/networks is a fundamental part of the analy-
sis, and in many cases the main goal of the study. However, little work has been
done on incorporating prior topological knowledge onto the estimation of the un-
derlying graphical models from sample data. In this work we propose extensions
to the basic joint regression model for network estimation, which explicitly in-
corporate graph-topological constraints into the corresponding optimization ap-
proach. The ﬁrst proposed extension includes an eigenvector centrality constraint,
thereby promoting this important prior topological property. The second devel-
oped extension promotes the formation of certain motifs, triangle-shaped ones in
particular, which are known to exist for example in genetic regulatory networks.
The presentation of the underlying formulations, which serve as examples of the
introduction of topological constraints in network estimation, is complemented
with examples in diverse datasets demonstrating the importance of incorporating
such critical prior knowledge.

1

Introduction

The estimation of the inverse of the covariance matrix (also referred to as precision matrix or con-
centration matrix) is a very important problem with applications in a number of ﬁelds, from biology
to social sciences, and is a fundamental step in the estimation of underlying data networks. The
covariance selection problem, as introduced by Dempster (1972), consists in identifying the zero
pattern of the precision matrix. Let X = (X1 . . . Xp ) be a p-dimensional multivariate normal dis-
tributed variable, X ∼ N (0, Σ), and C = Σ−1 its concentration matrix. Then two coordinates Xi
and Xj are conditionally independent given the other variables if and only if C (i, j ) = 0 (Lauritzen,
1996). This property motivates the representation of the conditional dependency structure in terms
of a graphical model G = (V , E ), where the set of nodes V corresponds to the p coordinates and
the edges E represent conditional dependency. Note that the zero pattern of the G adjacency matrix
coincides with the zero pattern of the concentration matrix. Therefore, the estimation of this graph
G from k random samples of X is equivalent to the covariance selection problem. The estimation of
G using (cid:96)1 (sparsity promoting) optimization techniques has become very popular in recent years.
This estimation problem becomes particularly interesting and hard at the same time when the number
of samples k is smaller than p. Several real life applications lie in this “small k-large p” setting. One
of the most studied examples, and indeed with great impact, is the inference of genetic regulatory
networks (GRN) from DNA microarray data, where typically the number p of genes is much larger
than the number k of experiments. Like in the vast majority of applications, these networks have
some very well known topological properties, such as sparsity (each node is connected with only a
few other nodes), scale-free behavior, and the presence of hubs (nodes connected with many other
vertices). All these properties are shared with many other real life networks like Internet, citation
networks, and social networks (Newman, 2010).

1

Genetic regulatory networks also contain a small set of recurring patterns called motifs. The system-
atic presence of these motifs has been ﬁrst discovered in Escherichia coli (Shen-Orr et al., 2002),
where it was found that the frequency of these patterns is much higher than in random networks, and
since then they have been identiﬁed in other organisms, from bacteria to yeast, plants and animals.
The topological analysis of networks is fundamental, and often the essence of the study. For ex-
ample, the proper identiﬁcation of hubs or motifs in GRN is crucial. Thus, the agreement of the
reconstructed topology with the original or expected one is critical. Sparsity has been successfully
exploited via (cid:96)1 penalization in order to obtain consistent estimators of the precision matrix, but
little work has been done with other graph-topological properties, often resulting in the estimation
of networks that lack critical known topological structures, and therefore do not look natural. Incor-
porating such topological knowledge in network estimation is the main goal of this work.
Eigenvector centrality (see Section 3 for the precise deﬁnition) is a well-known measure of the
importance and the connectivity of each node, and typical centrality distributions are known (or can
be estimated) for several types of networks. Therefore, we ﬁrst propose to incorporate this structural
information into the optimization procedure for network estimation in order to control the topology
of the resulting network. This centrality constraint is useful when some prior information about the
graphical model is known, for example, in dynamic networks, where the topology information of
the past can be used; in networks which we know are similar to other previously studied graphs; or
in networks that model a physical phenomenon for which a certain structure is expected.
As mentioned, it has been observed that genetic regulatory networks are conformed by a few geo-
metric patterns, repeated several times. One of these motifs is the so-called feedforward loop, which
is manifested as a triangle in the graph. Although it is thought that these important motifs may help
to understand more complex organisms, no effort has been made to include this prior information in
the network estimation problem. As a second example of the introduction of topological constraints,
we propose a simple modiﬁcation to the (cid:96)1 penalty, weighting the edges according to their local
structure, in order to favor the appearance of these motifs in the estimated network.
Both developed extensions here presented are very ﬂexible, and they can be combined with each
other or with other extensions reported in literature.
To recapitulate, we propose several contributions to the network estimation problem: we show the
importance of adding topological constraints; we propose an extension to (cid:96)1 models in order to
impose the eigenvector centrality; we show how to transfer topology from one graph to another; we
show that even with the centrality estimated from the same data, the proposed extension outperforms
the basic model; we present a weighting modiﬁcation to the (cid:96)1 penalty favoring the appearance of
motifs; as illustrative examples, we show how the proposed framework improves the edge and motif
detection in the E. coli network, and how the approach is important as well in ﬁnancial applications.
The rest of this paper is organized as follows. In Section 2 we describe the basic precision matrix
estimation models used in this work. In Section 3 we introduce the eigenvector centrality and de-
scribe how to impose it in graph estimation. We propose the weighting method for motifs estimation
in Section 4. Experimental results are presented in Section 5, and we conclude in Section 6.

2 Graphical Model Estimation
Let X be a k × p matrix containing k independent observations of X , and let us denote by Xi
j in Xi = (cid:80)
the i-th column of X. Two main families of approaches use sparsity constraints when inferring the
structure of the precision matrix. The ﬁrst one is based on the fact that the (i, j ) element of Σ−1 is,
up to a constant, the regression coefﬁcient β i
l Xl + εi , where εi is uncorrelated
l (cid:54)=i β i
with {Xl |l (cid:54)= i}. Following this property, the neighborhood selection technique by Meinshausen &
B ¨uhlmann (2006) consists in solving p independent (cid:96)1 regularized problems (Tibshirani, 1996),
||Xi − Xβ i ||2 + λ||β i ||1 ,

1
arg min
k
βi :β i
i =0
j s. While this is an asymptotically consistent estimator of the Σ−1 zero
where β i is the vector of β i
j and β j
pattern, β i
i are not necessarily equal since they are estimated independently. Peng et al.
(2009) propose a joint regression model which guarantees symmetry. This regression of the form
X ≈ XB , with B sparse, symmetric, and with null diagonal, allows to control the topology of the
graph deﬁned by the non-zero pattern of B , as it will be later exploited in this work. Friedman

2

max
Θ(cid:31)0

et al. (2010) also solve a symmetric version of the model by Meinshausen & B ¨uhlmann (2006) and
incorporate some structure penalties as the grouped lasso by Yuan & Lin (2006).
Methods of the second family are based on a maximum likelihood (ML) estimator with an (cid:96)1 penalty
(cid:88)
(Yuan & Lin, 2007; Banerjee et al., 2008; Friedman et al., 2008). Speciﬁcally, if S denotes the
empirical covariance matrix, the solution is the matrix Θ which solves the optimization problem
log det Θ − tr(SΘ) − λ
|Θij | .
i,j
An example of an extension to both models (the regression and ML approaches), and the ﬁrst to
explicitly consider additional classical network properties, is the work by Liu & Ihler (2011), which
modiﬁes the (cid:96)1 penalty to derive a non-convex optimization problem that favors scale-free networks.
A completely different technique for network estimation is the use of the PC-Algorithm to infer
acyclic graphs (Kalisch & B ¨uhlmann, 2007). This method starts from a complete graph and re-
cursively deletes edges according to conditional independence decisions. In this work, we use this
technique to estimate the graph eigenvector centrality.
3 Eigenvector Centrality Model Extension
Node degree (the number of connections of a node) is the simplest algebraic property than can be
deﬁned over a graph, but it is very local as it only takes into account the neighborhood of the node.
A more global measure of the node importance is the so-called centrality, in any of its different
variants. In this work, we consider the eigenvector centrality, deﬁned as the dominant eigenvector
(the one corresponding to the largest eigenvalue) of the corresponding network connectivity matrix.
The coordinates of this vector (which are all non-negatives) are the corresponding centrality of each
node, and provide a measure of the inﬂuence of the node in the network (Google’s PageRank is a
variant of this centrality measure). Distributions of the eigenvector centrality values are well known
for a number of graphs, including scale-free networks as the Internet and GRN (Newman, 2010).
In certain situations, we may have at our disposal an estimate of the centrality vector of the network
to infer. This may happen, for instance, because we already had preliminary data, or we know a net-
work expected to be similar, or simply someone provided us with some partial information about the
graph structure. In those cases, we would like to make use of this important side information, both
to improve the overall network estimation and to guarantee that the inferred graph is consistent with
our prior topological knowledge. In what follows we propose an extension of the joint regression
model which is capable of controlling this topological property of the estimated graph.
To begin with, let us remark that as Σ is positive-semideﬁnite and symmetric, all its eigenvalues are
non-negative, and thus so are the eigenvalues of Σ−1 . By virtue of the Perron-Frobenius Theorem,
for any adjacency matrix A, the eigenvalue with largest absolute value is positive. Therefore for
precision and graph connectivity matrices it holds that max||v ||=1 |(cid:104)Av , v(cid:105)| = max||v ||=1 (cid:104)Av , v(cid:105),
and moreover, the eigenvector centrality is c = arg max||v ||=1 (cid:104)Av , v(cid:105).
Suppose that we know an estimate of the centrality c ∈ Rp , and want the inferred network to have
centrality close to it. We start from the basic joint regression model,
||X − XB ||2
F + λ1 ||B ||(cid:96)1 ,
s.t. B symmetric, Bii = 0 ∀ i,
min
B
and add the centrality penalty,
where || · ||F is the Frobenius norm and ||B ||(cid:96)1 = (cid:80)
s.t. B symmetric, Bii = 0 ∀ i
||X − XB ||2
F + λ1 ||B ||(cid:96)1 − λ2 (cid:104)B c, c(cid:105) ,
(2)
min
B
i,j |Bij |. The minus sign is due to the mini-
mization instead of maximization, and since the term (cid:104)B c, c(cid:105) is linear, the problem is still convex.
Although B is intended to be a good estimation of the precision matrix (up to constants), formu-
lations (1) or (2) do not guarantee that B will be positive-semideﬁnite, and therefore the leading
eigenvalue might not be positive. One way to address this is to add the positive-semideﬁnite con-
straint in the formulation, which keeps the problem convex. However, in all of our experiments with
model (2) the spectral radius resulted positive, so we decided to use this simpler formulation due to
the power of the available solvers.
Note that we are imposing the dominant eigenvector of the graph connectivity matrix A to a non-
binary matrix B . We have exhaustive empirical evidence that the leading eigenvector of the matrix

(1)

3

B obtained by solving (2), and the leading eigenvector corresponding to the resulting connectivity
matrix (the binarization of B ) are very similar (see Section 5.1).
In addition, based on Wolf &
Shashua (2005), these type of results can be proved theoretically (Zeitouni, 2012).
As shown in Section 5, when the correct centrality is imposed, our proposed model outperforms the
joint regression model, both in correct reconstructed edge rates and topology. This is still true when
we only have a noisy version of c. Even if we do not have prior information at all, and we estimate
the centrality from the data with a pre-run of the PC-Algorithm, we obtain improved results.
The model extension here presented is general, and the term (cid:104)B c, c(cid:105) can be included in maximum
likelihood based approaches like Banerjee et al. (2008); Friedman et al. (2008); Yuan & Lin (2007).

3.1

Implementation

Following Peng et al. (2009), the matrix optimization (2) can be cast as a classical vector (cid:96)1
penalty problem. The symmetry and null diagonal constraints are handled considering only the
upper triangular sub-matrix of B (excluding the diagonal), and forming a vector θ with its entries:
θ = (B12 , B13 , . . . , B(p−1)p ). Let us consider a pk × 1 column vector y formed by concatenating all
F = ||y−Xt θ ||2
the columns of X. It is easy to ﬁnd a pk×p(p−1)/2 matrix Xt such that ||X−XB ||2
(see Peng et al. (2009) for details), and trivially ||B ||(cid:96)1 = 2||θ||1 . The new term in the cost function
2
is (cid:104)B c, c(cid:105), which is linear in B , thus it exists a matrix Ct = Ct (c) such that (cid:104)B c, c(cid:105) = (cid:104)Ct , θ(cid:105). The
construction of Ct is similar to the construction of Xt . The optimization problem (2) then becomes
||y − Xt θ ||2
2 + λ1 ||θ ||1 − λ2 (cid:104)Ct , θ(cid:105),

min
θ
which can be efﬁciently solved using any modern (cid:96)1 optimization method (Wright et al., 2009).

4 Favoring Motifs in Graphical Models

One of the biggest challenges in bioinformatics is the estimation and understanding of genetic regu-
latory networks. It has been observed that the structure of these graphs is far from being random: the
transcription networks seem to be conformed by a small set of regulation patterns that appear much
more often than in random graphs. It is believed that each one of these patterns, called motifs, are
responsible of certain speciﬁc regulatory functions. Three basic types of motifs are deﬁned (Shen-
Orr et al., 2002), the “feedforward loop” being one of the most signiﬁcant. This motif involves three
genes: a regulator X which regulates Y, and a gene Z which is regulated by both X and Y. The
representation of these regulations in the network takes the form of a triangle with vertices X, Y, Z.
Although these triangles are very frequent in GRN, the common algorithms discussed in Section
2 seem to fail at producing them. As these models do not consider any topological structure, and
the total number of reconstructed triangles is usually much lower than in transcription networks, it
seems reasonable to help in the formation of these motifs by favoring the presence of triangles.
In order to move towards a better motif detection, we propose an iterative procedure based on the
joint regression model (1). After a ﬁrst iteration of solving (1), a preliminary symmetric matrix B is
obtained. Recall that if A is a graph adjacency matrix, then A2 counts the paths of length 2 between
nodes. More speciﬁcally, the entry (i, j ) of A2 indicates how many paths of length 2 exist from node
i to node j . Back to the graphical model estimation, this means that if the entry (B 2 )ij (cid:54)= 0 (a length
2 path exists between i and j ), then by making Bij (cid:54)= 0 (if it is not already), at least one triangle
is added. This suggests that by including weights in the (cid:96)1 penalization, proportionally decreasing
with B 2 , we are favoring those edges that, when added, form a new triangle.
Given the matrix B obtained in the preliminary iteration, we consider the cost matrix M such that
Mij = e−µ(B 2 )ij , µ being a positive parameter. This way, if (B 2 )ij = 0 the weight does not affect
the penalty, and if (B 2 )ij (cid:54)= 0, it favors motifs detection. We then solve the optimization problem
||X − XB ||2
F + λ1 ||M · B ||(cid:96)1 ,
(3)
min
B
where M · B is the pointwise matrix product.
The algorithm iterates between reconstructing the matrix B and updating the weight matrix M
(initialized as the identity matrix). Usually after two or three iterations the graph stabilizes.

4

5 Experimental Results
In this section we present numerical and graphical results for the proposed models, and compare
them with the original joint regression one.
As discussed in the introduction, there is evidence that most real life networks present scale-free be-
havior. Therefore, when considering simulated results for validation, we use the model by Barab ´asi
& Albert (1999) to generate graphs with this property. Namely, we start from a random graph with 4
nodes and add one node at a time, randomly connected to one of the existing nodes. The probability
of connecting the new node to the node i is proportional to the current degree of node i.
Given a graph with adjacency matrix A, we simulate the data X as follows (Liu & Ihler, 2011): let
D be a diagonal matrix containing the degree of node i in the entry Dii , and consider the matrix
L = ηD − A with η > 1 so that L is positive deﬁnite. We then deﬁne the concentration matrix
2 , where Λ is the diagonal matrix of L−1 (used to normalize the diagonal of Σ = Θ−1 ).
Θ = Λ 1
2 LΛ 1
Gaussian data X is then simulated with distribution N (0, Σ). For each algorithm, the parameters
are set such that the resulting graph has the same number of edges as the original one. As the total
number of edges is then ﬁxed, the false positive (FP) rate can be deduced from the true positive (TP)
rate. We therefore report the TP rate only, since it is enough to compare the different performances.

Including Actual Centrality
5.1
In this ﬁrst experiment we show how our model (2) is able to correctly incorporate the prior centrality
information, resulting in a more accurate inferred graph, both in detected edges and in topology.
The graph of the example in Figure 1 contains 20 nodes. We generated 10 samples and inferred the
graph with the joint regression model and with the proposed model (2) using the correct centrality.

Figure 1: Comparison of networks estimated with the simple joint model (1) (middle) and with model (2)
(right) using the eigenvector centrality. Original graph on left.

The following more comprehensive test shows the improvement with respect to the basic joint model
(1) when the correct centrality is included. For a ﬁxed value of p = 80, and for each value of k from
30 to 50, we made 50 runs generating scale-free graphs and simulating data X. From these data
we estimated the network with the joint regression model with and without the centrality prior. The
TP edge rates in Figure 2(a) are averaged over the 50 runs, and count the correctly detected edges
over the (ﬁxed) total number of edges in the network. In addition, Figure 2(b) shows a ROC curve.
We generated 300 networks and constructed a ROC curve for each one by varying λ1 , and we then
averaged all the 300 curves. As expected, the incorporation of the known topological property helps
in the correct estimation of the graph.

0.9

0.8

0.7

0.6

e
t
a
r

e
g
d
e
P
T

1

0.9
e
t
a
R
0.8
e
v
i
t
0.7
i
s
o
P
e
0.6
u
r
T
0.5

30

60

70

0.4

0

40

50

0.01
0.02
0.015
k
False Positive Rate
(b) Edge detection ROC curve for net-
(a) True positive rates for different sam-
ple sizes on networks with 80 nodes.
works with p = 80 nodes and k = 50.
Figure 2: Performance comparison of models 2 and 1. In blue (dashed), the standard joint model (1), and in
black the proposed model with centrality (2). In thin lines, curves corresponding to 95% conﬁdence intervals.

0.005

0.025

5

Following the previous discussion, Figure 3 shows the inner product (cid:104)vB , vC (cid:105) for several runs of
model (2), where vB is the leading eigenvector of the obtained matrix B , C is the resulting connec-
tivity matrix (the binarized version of B ), and vC its leading eigenvector.
0.8
1

e
t
a
r

e
g
d
e
P
T

0.7

0.6

0.5

0.4

t
c
u
d
o
r
p

r
e
n
n
I

0.8

0.6

0.4

0.2

0

80
200
160
120
40
0
Run number
Figure 3: Inner product (cid:104)vC , vB (cid:105) for 200 runs.

5.2

Imposing Centrality Estimated from Data

20

30

40

50

60

70

k
Figure 4: True positive edge rates for different sam-
ple sizes on a network with 100 nodes. Dashed, the
joint model (1), dotted, the PC-Algorithm, and solid
the model (2) with centrality estimated from data.

The previous section shows how the performance of the joint regression model (1) can be improved
by incorporating the centrality, when this topology information is available. However, when this
vector is unknown, it can be estimated from the data, using an independent algorithm, and then
incorporated to the optimization in model (2). We use the PC-Algorithm to estimate the centrality
(by computing the dominant eigenvector of the resulting graph), and then we impose it as the vector
c in model (2). It turns out that even with a technique not specialized for centrality estimation, this
combination outperforms both the joint model (1) and the PC-Algorithm.
We compare the three mentioned models on networks with p = 100 nodes for several values of k ,
ranging from 20 to 70. For each value of k , we randomly generated ten networks and simulated
data X. We then reconstructed the graph using the three techniques and averaged the edge rate over
the ten runs. The parameter λ2 was obtained via cross validation. Figure 4 shows how the model
imposing centrality can improve the other ones without any external information.

5.3 Transferring Centrality
In several situations, one may have some information about the topology of the graph to infer,
mainly based on other data/graphs known to be similar. For instance, dynamic networks are a good
example where one may have some (maybe abundant) old data from the network at a past time
T1 , some (maybe scarce) new data at time T2 , and know that the network topology is similar at
the different times. This may be the case of ﬁnancial, climate, or any time-series data. Outside of
temporal varying networks, this topological transference may be useful when we have two graphs
of the same kind (say biological networks), which are expected to share some properties, and lots
of data is available for the ﬁrst network but very few samples for the second network are known.
We would like to transfer our inferred centrality-based topological knowledge from the ﬁrst network
into the second one, and by that improving the network estimation from limited data.
For these examples, we have an unknown graph G1 corresponding to a k1 × p data matrix X1 , which
we assume is enough to reasonably estimate G1 , and an unknown graph G2 with a k2 × p data matrix
X2 (with k2 (cid:28) k1 ). Using X2 only might not be enough to obtain a proper estimate of G2 , and
considering the whole data together (concatenation of X1 and X2 ) might be an artiﬁcial mixture or
too strong and lead to basically reconstructing G1 . What we really want to do is to transfer some
high-level structure of G1 into G2 , e.g., just the underlying centrality of G1 is transferred to G2 .
In what follows, we show the comparison of inferring the network G2 using only the data X2 in the
joint model (1); the concatenation of X1 and X2 in the joint model (1); and ﬁnally the centrality
estimated from X1 , imposed in model (2), along with data X2 . We ﬁxed the networks size to
p = 100 and the size of data for G1 to k1 = 200. Given a graph G1 , we construct G2 by randomly
changing a certain number of edges (32 and 36 edges in Figure 5). For k2 from 35 to 60, we generate
data X2 , and we then infer G2 with the methods described above. We averaged over 10 runs.
As it can be observed in Figure 5, the performance of the model including the centrality estimated
from X1 is better than the performance of the classical model, both when using just the data X2 and
the concatenated data X1 |X2 . Therefore, we can discard the old data X1 and keep only the structure
(centrality) and still be able to infer a more accurate version of G2 .

6

0.75

0.65

0.55

e
t
a
r

e
g
d
e
P
T

0.75

0.65

0.55

e
t
a
r

e
g
d
e
P
T

35
60
55
50
45
40
k2
(a) G1 /G2 differ in 32 edges.

35
60
55
50
45
40
k2
(b) G1 /G2 differ in 36 edges.

5.4 Experiments on Real Data
5.4.1 International Stock Market Data

Figure 5: True positive edge rate when es-
timating the network G2 vs amount of data.
In blue, the basic joint model using only
X2 , in red using the concatenation of X1
and X2 , and in black the model (2) using
only X2 with centrality estimated from X1
as prior.

The stock market is a very complicated system, with lots of time-dependent underlying relationships.
In this example we show how the centrality constraint can help to understand these relationships with
limited data on times of crisis and times of stability.
We use the daily closing values (πk ) of some relevant stock market indices from U.S., Canada, Aus-
tralia, Japan, Hong Kong, U.K., Germany, France, Italy, Switzerland, Netherlands, Austria, Spain,
Belgium, Finland, Portugal, Ireland, and Greece. We consider 2 time periods containing a crisis,
5/2007-5/2009 and 5/2009-5/2012, each of which was divided into a “pre-crisis” period, and two
more sets (training and testing) covering the actual crisis period. We also consider the relatively
stable period 6/1997-6/1999, where the division into these three subsets was made arbitrarily. Using
as data the return between two consecutive trading days, deﬁned as 100 log( πk
), we ﬁrst learned
πk−1
the centrality from the “pre-crisis” period, and we then learned three models with the training sets:
a classical least-squares regression (LS), the joint regression model (1), and the centrality model (2)
with the estimated eigenvector. For each learned model B we computed the “prediction” accuracy
||Xtest − XtestB ||2
F in order to evaluate whether the inclusion of the topology improves the estima-
tion. The results are presented in Table 1, illustrating how the topology helps to infer a better model,
both in stable and highly changing periods. Additionally, Figure 6 shows a graph learned with the
model (2) using the 2009-2012 training data. The discovered relationships make sense, and we can
easily identify geographic or socio-economic connections.

97-99
07-09
09-12
LS
2.7
3.5
14.4
Model (1)
2.5
0.9
4.0
Model (2)
1.9
0.6
2.4
Table 1: Mean square error (×10−3 ) for
the different models.

Figure 6: Countries network learned with the centrality model.

5.4.2 Motif Detection in Escherichia Coli

Along this section and the following one, we use as base graph the actual genetic regulation network
of the E. coli. This graph contains ≈ 400 nodes, but for practical issues we selected the sub-graph of
all nodes with degree > 1. This sub-graph GE contains 186 nodes and 40 feedforward loop motifs.
For the number of samples k varying from 30 to 120, we simulated data X from GE and recon-
structed the graph using the joint model (1) and the iterative method (3). We then compared the
resulting networks to the original one, both in true positive edge rate (recall that this analysis is suf-
ﬁcient since the total number of edges is made constant), and number of motifs correctly detected.
The numerical results are shown in Figure 7, where it can be seen that model (3) correctly detect
more motifs, with better TP vs FP motif rate, and without detriment of the true positive edge rate.

5.4.3 Centrality + Motif Detection
The simplicity of the proposed models allows to combine them with other existing network esti-
mation extensions. We now show the performance of the two models here presented combined
(centrality and motifs constraints), tested on the Escherichia coli network.

7

USCAAUJPHKUKGEFRITSWNEATSPBEFNPOIRGR0.55

0.45
e
t
a
r

e
g
d
0.35
e
P
T

0.25

0.3

0.2

0.1

e
t
a
r

f
i
t
o
m
P
T

0.22

0.14

0.06

e
u
l
a
v

.
d
e
r
p

.
s
o
P

40
60
80
100
120
40
60
80
100
120
40
60
80
100
120
k
k
k
Figure 7: Comparison of model (1) (dashed) with proposed model (3) (solid) for the E. coli network. Left:
TP edge rate. Middle: TP motif rate (motifs correctly detected over the total number of motifs in GE ). Right:
Positive predictive value (motifs correctly detected over the total number of motifs in the inferred graph).

We ﬁrst estimate the centrality from the data, as in Section 5.2. Let us assume that we know which
ones are the two most central nodes (genes).1 This information can be used to modify the centrality
value for these two nodes, by replacing them by the two highest centrality values typical of scale-
free networks (Newman, 2010). For the ﬁxed network GE , we simulated data of different sizes
k and reconstructed the graph with the model (1) and with the combination of models (2) and (3).
Again, we compared the TP edge rates, the percentage of motifs detected, and the TP/FP motifs rate.
Numerical results are shown in Figure 8, where it can be seen that, in addition to the motif detection
improvement, now the edge rate is also better. Figure 9 shows the obtained graphs for a speciﬁc run.

0.58

0.52

0.46

0.4

e
t
a
r

e
g
d
e
P
T

0.3

0.22

0.14

e
t
a
r

f
i
t
o
m
P
T

0.2

e
u
l
a
v

.
d
e
r
p

.
s
o
P

0.16

0.12

110
100
90
80
70
110
100
90
80
70
110
100
90
80
70
k
k
k
Figure 8: Comparison of model (1) (dashed) with the combination of models (2) and (3) (solid) for the E. coli
network. The combination of the proposed extensions is capable of detecting more motifs while also improving
the accuracy of the detected edges. Left: TP edge rate. Middle: TP motif rate. Right: Positive predictive value.

Figure 9: Comparison of graphs for the E. coli network with k = 80. Original network, inferred with model (1)
and with the combination of (2) and (3). Note how the combined model is able to better capture the underlying
network topology, as quantitative shown in Figure 8. Correctly detected motifs are highlighted.

6 Conclusions and Future Work
We proposed two extensions to (cid:96)1 penalized models for precision matrix (network) estimation. The
ﬁrst one incorporates topological information to the optimization, allowing to control the graph
centrality. We showed how this model is able to capture the imposed structure when the centrality
is provided as prior information, and we also showed how it can improve the performance of the
basic joint regression model even when there is no such external information. The second extension
favors the appearance of triangles, allowing to better detect motifs in genetic regulatory networks.
We combined both models for a better estimation of the Escherichia coli GRN.
There are several other graph-topological properties that may provide important information, mak-
ing it interesting to study which kind of structure can be added to the optimization problem. An
algorithm for estimating with high precision the centrality directly from the data would be a great
complement to the methods here presented. It is also important to ﬁnd a model which exploits all
the prior information about GRN, including other motifs not explored in this work. Finally, the
exploitation of the methods here developed for (cid:96)1 -graphs, is the the subject of future research.
1 In this case, it is well known that crp is the most central node, followed by fnr.

8

Acknowledgements

Work partially supported by ANII (Uruguay), ONR, NSF, NGA, DARPA, and AFOSR.

References
Banerjee, O., El Ghaoui, L., and D’Aspremont, A. Model selection through sparse maximum likeli-
hood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research,
9:485–516, 2008.
Barab ´asi, A. and Albert, R. Emergence of scaling in random networks. Science, 286(5439):509–512,
1999.
Dempster, A. Covariance selection. Biometrics, 28(1):157–175, 1972.
Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse covariance estimation with the graphical
lasso. Biostatistics, 9(3):432–41, July 2008.
Friedman, J., Hastie, T., and Tibshirani, R. Applications of the lasso and grouped lasso to the
estimation of sparse graphical models. Technical report, 2010.
Kalisch, M. and B ¨uhlmann, P. Estimating high-dimensional directed acyclic graphs with the PC-
Algorithm. Journal of Machine Learning Research, 8:613–636, 2007.
Lauritzen, S. Graphical Models. Clarendon Press, Oxford, 1996.
Liu, Q. and Ihler, A. Learning scale free networks by reweighted (cid:96)1 regularization. AI & Statistics,
15:40–48, April 2011.
Meinshausen, N. and B ¨uhlmann, P. High-dimensional graphs and variable selection with the Lasso.
The Annals of Statistics, 34(3):1436–1462, June 2006.
Newman, M. Networks: An Introduction. Oxford University Press, Inc., New York, NY, USA, 2010.
Peng, J., Wang, P., Zhou, N., and Zhu, J. Partial correlation estimation by joint sparse regression
models. Journal of the American Statistical Association, 104(486):735–746, June 2009.
Shen-Orr, S., Milo, R., Mangan, S., and Alon, U. Network motifs in the transcriptional regulation
network of Escherichia coli. Nature Genetics, 31(1):64–8, May 2002.
Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B, 58:267–288, 1996.
Wolf, L. and Shashua, A. Feature selection for unsupervised and supervised inference: The emer-
gence of sparsity in a weight-based approach. Journal of Machine Learning Research, 6:1855–
1887, 2005.
Wright, S., Nowak, R., and Figueiredo, M. Sparse reconstruction by separable approximation. IEEE
Transactions on Signal Processing, 57(7):2479–2493, 2009.
Yuan, M. and Lin, Y. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B, 68(1):49–67, 2006.
Yuan, M. and Lin, Y. Model selection and estimation in the Gaussian graphical model. Biometrika,
94(1):19–35, February 2007.
Zeitouni, O. Personal communication, 2012.

9


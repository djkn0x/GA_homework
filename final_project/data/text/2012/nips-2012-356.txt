Simultaneously Leveraging Output and Task
Structures for Multiple-Output Regression

Piyush Rai†
Dept. of Computer Science
University of Texas at Austin
Austin, TX
piyush@cs.utexas.edu

Abhishek Kumar†
Dept. of Computer Science
University of Maryland
College Park, MD
abhishek@cs.umd.edu

Hal Daum ´e III
Dept. of Computer Science
University of Maryland
College Park, MD
hal@umiacs.umd.edu

Abstract

Multiple-output regression models require estimating multiple parameters, one for
each output. Structural regularization is usually employed to improve parameter
estimation in such models. In this paper, we present a multiple-output regression
model that leverages the covariance structure of the latent model parameters as
well as the conditional covariance structure of the observed outputs. This is in
contrast with existing methods that usually take into account only one of these
structures. More importantly, unlike some of the other existing methods, none of
these structures need be known a priori in our model, and are learned from the
data. Several previously proposed structural regularization based multiple-output
regression models turn out to be special cases of our model. Moreover, in addition
to being a rich model for multiple-output regression, our model can also be used in
estimating the graphical model structure of a set of variables (multivariate outputs)
conditioned on another set of variables (inputs). Experimental results on both
synthetic and real datasets demonstrate the effectiveness of our method.

1
Introduction
Multivariate response prediction, also known as multiple-output regression [3] when the responses
are real-valued vectors, is an important problem in machine learning and statistics. The goal in
multiple-output regression is to learn a model for predicting K > 1 real-valued responses (the
output) from D predictors or features (the input), given a training dataset consisting of N input-
output pairs. Multiple-output prediction is also an instance of the problem of multitask learning [5,
10] where predicting each output is a task and all the tasks share the same input data. Multiple-
output regression problems are encountered frequently in various application domains. For example,
in computational biology [11], we often want to predict the gene-expression levels of multiple genes
based on a set of single nucleotide polymorphisms (SNPs); in econometrics [17], we often want to
predict the stock prices in the future using relevant macro-economic variables and stock prices in the
past as inputs; in geostatistics, we are often interested in jointly predicting the concentration levels
of different heavy metal pollutants [9]; and so on.

One distinguishing aspect of multiple-output regression is that the outputs are often related to each
other via some underlying (and often a priori unknown) structure. A part of this can be captured by
the imposing a relatedness structure among the regression coefﬁcients (e.g., the weight vectors in a
linear regression model) of all the outputs. We refer to the relatedness structure among the regression
coefﬁcients as
task structure. However, there can still be some structure left in the outputs that is not
explained by the regression coefﬁcients alone. This can be d ue to a limited expressive power of our
chosen hypothesis class (e.g., linear predictors considered in this paper). The residual structure that
is left out when conditioned on inputs will be referred to as output structure here. This can be also be
seen as the covariance structure in the output noise. It is therefore desirable to simultaneously learn

†Contributed equally

1

and leverage both the output structure and the task structure in multiple-output regression models
for improved parameter estimation and prediction accuracy.

Although some of the existing multiple-output regression models have attempted to incorporate such
structures [17, 11, 13], most of these models are restrictive in the sense that (1) they usually exploit
only one of the two structures (output structure or task structure, but not both), and (2) they assume
availability of prior information about such structures which may not always be available. For
example, Multivariate Regression with Covariance Estimation [17] (MRCE) is a recently proposed
method which learns the output structure (in form of the covariance matrix for correlated noise
across multiple outputs) along with the regression coefﬁci ents (i.e., the weight vector) for predicting
each output. However MRCE does not explicitly model the relationships among the regression
coefﬁcients of the multiple tasks and therefore fails to acc ount for the task structure. More recently,
[14] proposed an extension of the MRCE model by allowing weighting the individual entries of
the regression coefﬁcients and the entries of the output (in verse) covariance matrix, but otherwise
this model has essentially the same properties as MRCE. Among other works, Graph-guided Fused
Lasso [11] (GFlasso) incorporates task structure to some degree by assuming that the regression
coefﬁcients of all the outputs have similar sparsity patter ns. This amounts to assuming that all
the outputs share almost same set of relevant features. However, GFlasso assumes that output graph
structure is known which is rarely true in practice. Some other methods such as[13] take into account
the task structure by imposing structural sparsity on the regression coefﬁcients of the multiple tasks
but again assume that output structure is known a priori and/or is of a speci ﬁc form. In [22], the
authors proposed a multitask learning model by explicitly modeling the task structures as the task
covariance matrix but this model does not take into account the output structure which is important
in multiple-output regression problems.

In this paper, we present a multiple-output regression model that allows leveraging both output
structure and task structure without assuming an a priori knowledge of either. In our model, both
output structure and task structure are learned from the data, along with the regression coefﬁcients
for each task. Speci ﬁcally, we model the output structure us ing the (inverse) covariance matrix of
the correlated noise across the multiple outputs, and the task structure using the (inverse) covariance
matrix of the regression coefﬁcients of the multiple tasks b eing learned in the model. By explicitly
modeling and learning the output structure and task structure, our model also addresses the limi-
tations of the existing models that typically assume certain speci ﬁc type of output structures (e.g.,
tree [13]) or task structures (e.g., shared sparsity [11]). In particular, a model with task relatedness
structure based on shared sparsity on the task weight vectors may not be appropriate in many real
applications where all the features are important for prediction and the true task structure is at a
more higher level (e.g., weight vectors for some tasks are closer to each other compared to others).
Apart from providing a ﬂexible way of learning multiple-out put regression, our model can also be
used for the problem of conditional inverse covariance estimation of the (multivariate) outputs that
depend on another set of inputs variables, an important problem that has been gaining signi ﬁcant
attention recently [23, 15, 20, 4, 7, 6].

2 Multiple-Output Regression

In multiple-output regression, each input is associated with a vector of responses and the goal is
the learn the input-output relationship given some training data consisting of input-output pairs.
Formally, given an N × D input matrix X = [x1 , . . . , xN ]⊤ and an N × K output matrix Y =
[y1 , . . . , yN ]⊤ , the goal in multiple-output regression is to learn the functional relationship between
the inputs xn ∈ RD and the outputs yn ∈ RK . For a linear regression model, we write:
yn = W⊤xn + b + ǫn
(1)
∀n = 1, . . . , N
Here W = [w1 , . . . , wK ] denotes the D × K matrix where wk denotes the regression coefﬁcient
of the k-th output, b = [b1 , . . . , bK ]⊤ ∈ RK is a vector of bias terms for the K outputs, and
ǫn = [ǫn1 , . . . , ǫnK ]⊤ ∈ RK is a vector consisting of the noise for each of the K outputs. The noise
is typically assumed to be Gaussian with a zero mean and uncorrelated across the K outputs.
Standard parameter estimation for Equation 1 involves maximizing the (penalized) log-likelihood of
the model, or equivalently minimizing the (regularized) loss function over the training data:
tr((Y − XW − 1b⊤ )(Y − XW − 1b⊤ )⊤ ) + λR(W)

(2)

arg min
W,b

2

where tr(.) denotes matrix trace, 1 an N × 1 vector of all 1s and R(W) the regularizer on the weight
matrix W consisting of the regression weight vectors of all the outputs. For a choice of R(W) =
tr(W⊤W) (the ℓ2 -squared norm, equivalent to assuming independent, zero-mean Gaussian priors
on the weight vectors), solving Equation 2 amounts to solving K independent regression problems
and this solution ignores any correlations among the outputs or among the weight vectors.

3 Multiple-Output Regression with Output and Task Structures

To take into account both conditional output covariance and the covariance among the weight vec-
tors W = [w1 , . . . , wK ], we assume a full covariance matrix Ω of size K × K on the output
noise distribution to capture conditional output covariance, and a structured prior distribution on the
weight vector matrix W that induces structural regularization of W. We place the following prior
distribution on W

p(W) ∝

K
Y
k=1

Nor(wk |0, ID )MN D×K (W|0D×K , ID ⊗ Σ)

(3)

where MN D×K (M, A ⊗ B) denotes the matrix-variate normal distribution with M ∈ RD×K
being its mean, A ∈ RD×D its row-covariance matrix and B ∈ RK×K its column-covariance
matrix. Here ⊗ denotes the Kronecker product. In this prior distribution, the Nor(wk |0, ID ) factors
regularize the weight vectors wk individually, and the MN D×K (W|0D×K , ID ⊗ Σ) term couples
the K weight vectors, allowing them to share statistical strength.
To derive our objective function, we start by writing down the likelihood of the model, for a set of
N i.i.d. observations:

p(yn |xn , W, b) =

(4)

N
N
Y
Y
n=1
n=1
In the above, a diagonal Ω would imply that the K outputs are all conditionally independent of
each other. In this paper, we assume a full Ω which will allow us to capture the conditional output
correlations.

Nor(yn |W⊤xn + b, Ω)

Combining the prior on W and the likelihood, we can write down the posterior distribution of W:
p(W|X, Y , b, Ω, Σ) ∝ p(W) QN
n=1 p(yn |xn , W, b)
= QK
k=1 Nor(wk |0, ID ) MN D×K (W|0D×K , ID ⊗ Σ) QN
n=1 Nor(yn |W⊤xn + b, Ω)
Taking the log of the above and simplifying the resulting expression, we can then write the negative
log-posterior of W as (ignoring the constants):

tr((Y − XW − 1b⊤ )Ω−1 (Y − XW − 1b⊤ )⊤ ) + N log |Ω| + tr(WW⊤ )
+ tr(WΣ−1W⊤ ) + D log |Σ|

where 1 denotes a N × 1 vector of all 1s. Note that in the term tr(WΣ−1W⊤ ), the inverse
covariance matrix Σ−1 plays the role of coupling pairs of weight vectors, and therefore controls
the amount of sharing between any pair of tasks. The task covariance matrix Σ as well as the
conditional output covariance matrix Ω will be learned from the data. For reasons that will become
apparent later, we parameterize our model in terms of the inverse covariance matrices Ω−1 and Σ−1
instead of covariance matrices. With this parameterization, the negative log-posterior becomes:

tr((Y − XW − 1b⊤ )Ω−1 (Y − XW − 1b⊤ )⊤ ) − N log |Ω−1 | + tr(WW⊤ )
+ tr(WΣ−1W⊤ ) − D log |Σ−1 |

(5)

The objective function in Equation 5 naturally imposes positive-deﬁnite constraints on the inverse
covariance matrices Ω−1 and Σ−1 .
In addition, we will impose sparsity constraints (via an ℓ1
penalty) on Ω−1 and Σ−1 . Sparsity on these parameters is appealing in this context for two rea-
sons: (1) Sparsity leads to improved robust estimates [19, 8] of Ω−1 and Σ−1 , and (2) Sparsity
supports the notion that the output correlations and the task correlations tend to be sparse [21, 4, 8]

3

– not all pairs of outputs are related (given the inputs and ot her outputs), and likewise not all task
pairs (and therefore the corresponding weight vectors) are related. Finally, we will also introduce
regularization hyperparameters to control the trade-off between data- ﬁt and model complexity. Pa-
rameter estimation in the model involves minimizing the negative log-posterior which is equivalent
to minimizing the (regularized) loss function. The minimization problem is given as
tr((Y − XW − 1b⊤ )Ω−1 (Y − XW − 1b⊤ )⊤ ) − N log |Ω−1 | + λ tr(WW⊤ )

arg min
W,b,Σ−1 ,Ω−1

+λ1 tr(WΣ−1W⊤ ) − D log |Σ−1 | + λ2 ||Ω−1 ||1 + λ3 ||Σ−1 ||1
(6)
where ||A||1 denotes the sum of absolute values of the matrix A. Note that by replacing the regular-
izer tr(WW⊤ ) with a sparsity inducing regularizer on the individual weight vectors w1 , . . . , wK ,
one can also learn Lasso-like sparsity [19] in the regression weights. In this exposition, however,
we consider ℓ2 regularization on the regression weights and let the tr(WΣ−1W⊤ ) term capture the
similarity between the weights of two tasks by learning the task inverse covariance matrix Σ−1 . The
above cost function is not jointly convex in the variables but is individually convex in each variable
when others are ﬁxed. We adopt an alternating optimization s trategy that was empirically observed
to converge in all our experiments. More details are provided in the experiments section. Finally,
although it is not the main goal of this paper, since our model provides an estimate of the inverse
covariance structure Ω−1 of the outputs conditioned on the inputs, it can also be used for the more
general problem of estimating the conditional inverse covariance [23, 15, 20, 4, 7] of a set of vari-
ables y = {y1 , . . . , yK } conditioned on another set of variables x = {x1 , . . . , xD }, given paired
samples of the form {(x1 , y1 ), . . . , (xN , yN )}.

3.1 Special Cases

In this section, we show that our model subsumes/generalizes some previously proposed models for
multiple-output regression. Some of these include:

• Multivariate Regression with Covariance Estimation (MRCE-ℓ2 ): With the task in-
verse covariance matrix Σ−1 = IK and the bias term set to zero, our model results in
the ℓ2 regularized weights variant of the MRCE model [17] which would be equivalent to
minimizing the following objective:
tr((Y − XW)Ω−1 (Y − XW)⊤ ) + λ tr(WW⊤ ) − N log |Ω−1 | + λ2 ||Ω−1 ||1

arg min
W,Ω−1
• Multitask Relationship Learning for Regression (MTRL): With the output inverse co-
variance matrix Ω−1 = IK and the sparsity constraint on Σ−1 dropped, our model results
in the regression version of the multitask relationship learning model proposed in [22].
Speci ﬁcally, the corresponding objective function would b e:
tr((Y−XW)(Y−XW)⊤ )+λ tr(WW⊤ )+λ1 tr(WΣ−1W⊤ )−D log |Σ−1 |

arg min
W,Σ−1

In [22], the − log |Σ−1 | term is dropped since the authors solve their cost function in terms of Σ
and this term is concave in Σ. A constraint of tr(Σ) = 1 was introduced in its place to restrict the
complexity of the model. We keep the log | · | constraint in our cost function since we parameterize
our model in terms of Σ−1 , and − log |Σ−1 | is convex in Σ−1 .

3.2 Optimization

We take an alternating optimization approach to solve the optimization problem given by Equa-
tion 6. Each sub-problem in the alternating optimization steps is convex. The matrices Σ and Ω are
initialized to I in the beginning. The bias vector b is initialized to 1
N Y⊤1.

Optimization w.r.t. W when Ω−1 , Σ−1 and b are ﬁxed:
Given Ω−1 , Σ−1 , b, the matrix W consisting of the regression weight vectors of all the tasks can
be obtained by solving the following optimization problem:
tr((Y −XW − 1b⊤ )Ω−1 (Y −XW − 1b⊤ )⊤ ) + λ tr(WW⊤ ) + λ1 tr(WΣ−1W⊤ ) (7)

arg min
W

4

The estimate ˆW is given by solving the following system of linear equations w.r.t. W:
(cid:2)(cid:0)Ω−1 ⊗ X′X(cid:1) + (cid:0)(cid:0)λ1Σ−1 + λIK (cid:1) ⊗ ID (cid:1)(cid:3) vec(W) = vec(X′ (Y − 1b⊤ )Ω−1 )
(8)
It is easy to see that with Ω and Σ set to identity, the model becomes equivalent to solving K
regularized independent linear regression problems.
Optimization w.r.t. b when Ω−1 , Σ−1 and W are ﬁxed:
Given Ω−1 , Σ−1 , W, the bias vector b for all the K outputs can be obtained by solving the follow-
ing optimization problem:
arg min
b

tr((Y − XW − 1b⊤ )Ω−1 (Y − XW − 1b⊤ )⊤ )

(9)

N PN
The estimate ˆb is given by ˆb = 1
n=1 (Y − XW)⊤1
Optimization w.r.t. Σ−1 when Ω−1 , W and b are ﬁxed:
Given Ω−1 , W, b, the task inverse covariance matrix Σ−1 can be estimated by solving the following
optimization problem:
arg min
Σ−1

λ1 tr(WΣ−1W⊤ ) − D log |Σ−1 | + λ3 ||Σ−1 ||1

(10)

It is easy to see that the above is an instance of the standard inverse covariance estimation problem
with sample covariance λ1
D W⊤W, and can be solve using standard tools for inverse covariance
estimation. We use the graphical Lasso procedure [8] to solve Equation 10 to estimate Σ−1 :
λ1
D
If we assume Σ−1 to be non-sparse, we can drop the ℓ1 penalty on Σ−1 from Equation 10. However,
the solution to Σ−1 will not be deﬁned (when K > D) or will over ﬁt (when K is of the same order
as D). To avoid this, we add a regularizer of the form λ tr(Σ−1 ) to Equation 10. This can be seen as
imposing a matrix variate Gaussian prior on Σ−1/2 with both row and column covariance matrices
equal to I to make the solution well deﬁned. In the previous case of spar se Σ−1 , the solution was
well deﬁned because of the sparsity prior on Σ−1 . The optimization problem for Σ−1 is then given
as

ˆΣ−1 = gLasso(

W⊤W, λ3 )

(11)

(12)

λ1 tr(WΣ−1W⊤ ) − D log |Σ−1 | + λ tr (cid:0)Σ−1 (cid:1) .
arg min
Σ−1
(cid:17)−1
Equation 12 admits a closed form solution which is given by (cid:16) λ1W⊤W+λI
. For the non-sparse
D
Σ−1 case, we keep the parameter λ same as the hyperparameter for the term tr(WW⊤ ) in Equa-
tion 6.
Optimization w.r.t. Ω−1 when Σ−1 , W and b are ﬁxed:
Given Σ−1 , W, b, the task inverse covariance matrix Ω−1 can be estimated by solving the following
optimization problem:
tr((Y − XW − 1b⊤ )Ω−1 (Y − XW − 1b⊤ )⊤ ) − N log |Ω−1 | + λ2 ||Ω−1 ||1
arg min
Ω−1
It is again easy to see that the above problem is an instance of the standard inverse covariance esti-
mation problem with sample covariance 1
N (Y −XW − 1b⊤ )′ (Y −XW − 1b⊤ ), and can be solved
using standard tools for inverse covariance estimation. We use the graphical Lasso procedure [8] to
solve Equation 10 to estimate Σ−1 :
1
N

(Y − XW − cb⊤ )⊤ (Y − XW − cb⊤ ), λ2 )

ˆΩ−1 = gLasso(

(13)

(14)

4 Experiments
In this section, we evaluate our model by comparing it with several relevant baselines on both syn-
thetic and real-world datasets. Our main set of results are on multiple-output regression problems
on which we report mean-squared errors averaged across all the outputs. However, since our model
also provides an estimate of the conditional inverse covariance structure Ω−1 of the outputs, in Sec-
tion 4.3 we provide experimental results on the structure recovery task as well. We compare our
method with following baselines:

5

• Independent regressions (RLS): This baseline learns regularized least squares (RLS) re-
gression model for each output, without assuming any structure among the weight vectors
or among the outputs. This corresponds to our model with Σ = IK and Ω = IK . The
weight vector of each individual problem is ℓ2 regularized with a hyperparameter λ.
• Curds and Whey (C&W): The predictor in Curds and Whey [3] takes the form Wcw =
WrlsUΛU− , where Wrls denotes the regularized least squares predictor, the columns
of matrix U are the projection directions for the responses Y obtained from canonical
correlation analysis (CCA) of X and Y , and U− denotes Moore-Penrose pseudoinverse of
U. The diagonal matrix Λ contains the shrinkage factors for each CCA projection direction.
• Multi-task Relationship Learning (MTRL): This method leverages task relationships
by assuming a matrix-variate prior on the weight matrix W [22]. We chose this baseline
because of its ﬂexibility in modeling the task relationship s by “discovering” how the weight
vectors are related (via Σ−1 ), rather than assuming a speci ﬁc structure on them such as
shared sparsity [16], low-rank assumption [2], etc. However MTRL in the multiple-output
regression setting cannot take into account the output structure. It is therefor a special case
of our model if we assume the output inverse covariance matrix Ω−1 = I. The MTRL
approach proposed in [22] does not have sparse penalty on Σ−1 . We experimented with
both sparse and non-sparse variants of MTRL and report the better of the two results here.
• Multivariate Regression with Covariance Estimation (MRCE-ℓ2 ): This baseline is the
ℓ2 regularized variant of the MRCE model [17]. MRCE leverages output structure by
assuming a full noise covariance in multiple-output regression and learning it along with the
weight matrix W from the data. MRCE however cannot take into account the task structure
because it cannot capture the relationships among the columns of W.
It is therefore a
special case of our model if we assume the task inverse covariance matrix Σ−1 = I. We
do not compare with the original ℓ1 regularized MRCE [17] to ensure a fair comparison by
keeping all the models non-sparse in weight vectors.

In the experiments, we refer to our model as MROTS (Multiple-output Regression with Output
and Task Structures). We experiment with two variants of our proposed model, one without a
sparsity inducing penalty on the task coupling matrix Σ−1 (called MROTS-I), and the other with
the sparse penalty on Σ−1 (called MROTS-II). The hyperparameters are selected using four-fold
cross-validation. Both MTRL and MRCE-ℓ2 have two hyperparameters each and these are selected
by searching on a two-dimensional grid. For the proposed model with non-sparse Σ−1 , we ﬁx the
hyperparameter λ in Equations 6 and 12 as 0.001 for all the experiments. This is used to ensure
that the task inverse covariance matrix estimate ˆΣ−1 exists and is robust when number of response
variables K is of the same order or larger than the input dimension D . The other two parameters
λ1 and λ2 are selected using cross-validation. For sparse Σ−1 case, we use the same values of λ1
and λ2 that were selected for non-sparse case, and only the third parameter λ3 is selected by cross-
validation. This procedure avoids a potentially expensive search over a three dimensional grid. The
hyperparameter λ in Equation 6 is again ﬁxed at 0.001.

4.1 Synthetic data

We describe the process for synthetic data generation here. First, we generate a random positive
deﬁnite matrix Σ−1 which will act as the task inverse covariance matrix. Next, a matrix V of size
D×K is generated with each entry sampled from a zero mean and 1/D variance normal distribution.
We compute the square-root S of Σ (= SS, where S is also a symmetric positive deﬁnite matrix),
and S is used to generate the ﬁnal weight matrix W as W = VS. It is clear that for a W generated
in this fashion, we will have E [WT W] = SS = Σ. This process generates W such that its
columns (and therefore the weight vectors for different outputs) are correlated. A bias vector b of
size K is generated randomly from a zero mean unit variance normal distribution. Then we generate
a sparse random positive deﬁnite matrix Ω−1 that acts as the conditional inverse covariance matrix
on output noise making the outputs correlated (given the inputs). Next, input samples are generated
i.i.d. from a normal distribution and the corresponding multivariate output variables are generated
as yi = Wxi + b + ǫi , ∀i = 1, 2, . . . , N , where ǫi is the correlated noise vector randomly sampled
from a zero mean normal distribution with covariance matrix Ω.

We generate three sets of synthetic data using the above process to gauge the effectiveness of the
proposed model under varying circumstances: (i) D = 20, K = 10 and non-sparse Σ−1 , (ii)

6

Method
RLS
C&W
MTRL
MRCE-ℓ2
MROTS-I
MROTS-II

Synth data I
37.29
37.14
34.45
29.84
26.65
25.90

Synth data II
3.22
21.88
3.12
3.08
2.61
2.60

Synth data III
3.94
7.06
3.86
3.92
3.75
3.55

Paper I
1.08
1.08
1.07
1.36
0.90
0.90

Paper II Gene data
1.92
1.04
1.51
1.08
1.24
1.03
1.03
1.55
1.18
1.03
1.03
1.20

Table 1: Prediction error (MSE) on synthetic and real datasets. RLS: Independent regression, C&W: Curds
and Whey model [3], MTRL: Multi-task relationship learning [22], MRCE-ℓ2 : The ℓ2 -regularized version of
MRCE [17], MROTS-I: our model without sparse penalty on Σ−1 , MROTS-II: our model with sparse penalty
on Σ−1 . Best results are highlighted in bold fonts.

D = 10, K = 20 and non-sparse Σ−1 , and (iii) D = 10, K = 20 and sparse Σ−1 . We also
experiment with varying number of training samples (N = 20, 30, 40 and 50).

300

250

200

150

100

50

r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

 

RLS
C&W
MTRL
MRCE−l 2
MROTS−I
MROTS−II

5

4.5

4

3.5

3

r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

 

RLS
MTRL
MRCE−l 2
MROTS−I
MROTS−II

60

50

40

30

e
u
l
a
v
 
.
j
b
O
 
r
o
 
E
S
M

 

MSE
OBJ. VALUE

1.4

1.2

1

0.8

e
u
l
a
v
 
.
j
b
O
 
r
o
 
E
S
M

 

MSE
OBJ. VALUE

0
 
10

20
30
40
50
Number of training samples
(a) Synthetic data I

60

2.5
 
10

20
30
40
50
Number of training samples
(b) Synthetic data II

60

20
 
0

10
20
Iterations
(c) Synthetic data I

30

 
0

10
20
Iterations
(d) Paper data I

30

Figure 1: (a) and (b): Mean Square Error with varying number of training samples, (c) and (d): Mean Square
Error and the value of the Objective function with increasing iterations for the proposed method.
4.2 Real data
We also evaluate our model on the following real-world multiple-output regression datasets:

• Paper datasets: These are two multivariate multiple-response regression datasets from
paper industry [1]. The ﬁrst dataset has 30 samples with each sample having 9 features and
32 outputs. The second dataset has 29 samples (after ignoring one sample with missing
response variables), each having 9 features and 13 outputs. We take 15 samples for training
and the remaining samples for test.
• Genotype dataset: This dataset has genotypes as input variables and phenotypes or ob-
served traits as output variables [12]. The number of genotypes (features) is 25 and the
number of phenotypes (outputs) is 30. We have a total of 100 samples in this dataset and
we split it equally into training and test data.

The results on synthetic and real-world datasets are shown in Table 1. For synthetic datasets, the
reported results are with 50 training samples. Independent linear regression performs the worst on all
synthetic datasets. MRCE-ℓ2 performs better than MTRL on ﬁrst and second synthetic data w hile
MTRL is better on the third dataset. This mixed behavior of MRCE-ℓ2 and MTRL supports our
motivation that both task structure (i.e., relationships among weight vectors) and output structure are
important in multiple-output regression. Both MTRL and MRCE-ℓ2 are special cases of our model
with former ignoring the output structure (captured by Ω−1 ) and the latter ignoring the weight vector
relationships (captured by Σ−1 ). Both variants of our model (MROTS-I and MROTS-II) perform
signi ﬁcantly better than the compared baselines. The impro vement with sparse Σ−1 variant is more
prominent on the third dataset which is generated with sparse Σ−1 (5.33% relative reduction in
MSE), than on the ﬁrst two datasets ( 2.81% and 0.3% relative reduction in MSE). However, in our
experiments, the sparse Σ−1 variant (MROTS-II) always performed better or as good as the non-
sparse variant on all synthetic and real datasets, which suggests that explicitly encouraging zero
entries in Σ−1 leads to better estimates of task relationships (by avoiding spurious correlations
between weight vectors). This can potentially improve the prediction performance. Finally, we also
note that the Curds & Whey method [3] performs signi ﬁcantly wo rse than RLS for Synthetic data II
and III. C&W uses CCA to project the response matrix Y to a lower min(D , K )-dimensional space
learning min(D , K ) predictors there and then projecting them back to the original K-dimensional

7

space. This procedure may end up throwing away relevant information from responses if K is
much higher than D . These empirical results suggest that C&W may adversely affect the prediction
performance when the number of response variables K is higher than the number of explanatory
variables D (D = 2K in these cases).
On the real-world datasets too, our model performs better than or on par with the compared base-
lines. Both MROTS-I and MROTS-II perform signi ﬁcantly bett er than the other baselines on the
ﬁrst Paper dataset ( 9 features and 32 outputs per sample). All models perform almost similarly on
the second Paper dataset (9 features and 13 outputs per sample), which could be due to the absence
of a strong task or output structure in this data. C&W does not preform well on both Paper datasets
which might be due to the reason discussed earlier. On the genotype-phenotype prediction task
too, both our models achieve better average mean squared errors than the other baselines, with both
variants performing roughly comparably.

We also evaluate our model’s performance with varying number of training examples and compare
with the other baselines. Figures 1(a) and 1(b) show the plots of mean square error vs. number of
training examples for ﬁrst two synthetic datasets. We do not plot C&W for Synthetic data II since
it performs worse than RLS. On the ﬁrst synthetic data, the pe rformance gain of our model is more
pronounced when number of training examples is small. For the second synthetic data, we retain
similar performance gain over other models when number of training examples are increased from
20. The MSE numbers for the ﬁrst synthetic data are higher than t he ones obtained for the second
synthetic data because of a difference in the magnitude of error covariances used in the generation
of datasets.

We also experiment with the convergence properties of our method. Figures 1(c) and 1(d) show that
plots of average MSE and the value of the objective function (given by Equation 6) with increasing
number of iterations on the ﬁrst synthetic dataset and the ﬁr
st Paper dataset. The plots show that our
alternating optimization procedure converges in roughly 10–15 iterations.

4.3 Covariance structure recovery
Although not the main goal of the paper, we experiment with learned inverse covariance
matrix of the outputs (given the inputs) as a sanity check on the proposed model. To
better visualize, we generate a dataset with 5 responses and 3 predictors using the same
process as described in Sec. 4.1. Figure on the right shows the true conditional inverse
covariance matrix Ω−1 (Top), the matrix learned with MROTS ˆΩ−1 (Middle), and the
precision matrix learned with graphical lasso ignoring the predictors (Bottom). Taking
into account the regression weights results in better estimate of the true covariance
matrix. We got similar results for MRCE-ℓ2 which also takes into account the predictors
while learning the inverse covariance, although MROTS estimates were closer to the
ground truth in terms of the Frobenius norm.
5 Related Work
Apart from the prior works discussed in Section 1, our work has connections to some other works
which we discuss in this section. Recently, Sohn & Kim [18] proposed a model for jointly esti-
mating the weight vector for each output and the covariance structure of the outputs. However, they
assume a shared sparsity structure on the weight vectors. This assumption may be restrictive in some
problems. Some other works on conditional graphical model estimation [20, 4] are based on similar
structural sparsity assumptions. In contrast, our model does not assume any speci ﬁc structure on the
weight vectors, and by explicitly modeling the covariance structure of the weight vectors, learns the
appropriate underlying structure from the data.

6 Future Work and Conclusion

We have presented a ﬂexible model for multiple-output regre ssion taking into account the covariance
structure of the outputs and the covariance structure of the underlying prediction tasks. Our model
does not require a priori knowledge of these structures and learns these from the data. Our model
leads to improved accuracies on multiple-output regression tasks. Our model can be extended in
several ways. For example, one possibility is to model nonlinear input-output relationships by ker-
nelizing the model along the lines of [22].

8

References
[1] M. Aldrin. Moderate projection pursuit regression for multivariate response data. Computa-
tional Statistics and Data Analysis, 21, 1996.
[2] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.
In NIPS, 2007.
[3] L. Breiman and J.H. Friedman. Predicting multivariate responses in multiple linear regression.
Journal of the Royal Statistical Society. Series B (Methodological), pages 3–54, 1997.
[4] T. Cai, H. Li, W. Liu, and J. Xie. Covariate adjusted precision matrix estimation with an
application in genetical genomics. Biometrika, 2011.
[5] Rich Caruana. Multitask Learning. Machine Learning, 28, 1997.
[6] J. Cheng, E. Levina, P. Wang, and J. Zhu.
Sparse ising models with covariates.
arXiv:1209.6342v1, 2012.
[7] S. Ding, G. Wahba, and J. X. Zhu. Learning higher-order graph structure with features by
structure penalty. In NIPS, 2011.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graph-
ical lasso. Biostatistics, 9(3):432–441, 2008.
[9] P. Goovaerts. Geostatistics For Natural Resources Evaluation. Oxford University Press, 1997.
[10] T. Heskes. Empirical Bayes for learning to learn. ICML, 2000.
[11] S. Kim, K. Sohn, and E. P. Xing. A multivariate regression approach to association analysis of
a quantitative trait network.
[12] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative
trait network. PLoS Genetics, 2009.
[13] S. Kim and E. P. Xing. Tree-guided group lasso for multi-response regression with structured
sparsity, with an application to eQTL mapping. Annals of Applied Statistics, 2012.
[14] W. Lee and Y. Liu. Simultaneous multiple response regression and inverse covariance ma-
trix estimation via penalized gaussian maximum likelihood. Journal of Multivariate Analysis,
2012.
[15] H. Liu, X. Chen, J. Lafferty, and L. Wasserman. Graph-valued regression. In NIPS, 2010.
[16] G. Obozinskiy, M. J. Wainwright, and M. I. Jordan. Union support recovery in high-
dimensional multivariate regression. In NIPS, 2010.
[17] A. J. Rothman, E. Levina, and J. Zhu. Sparse multivariate regression with covariance estima-
tion. Journal of Computational and Graphical Statistics, 2010.
[18] K.A. Sohn and S. Kim. Joint estimation of structured sparsity and output structure in multiple-
output regression via inverse-covariance regularization. In AISTATS, 2012.
[19] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical
Society, 1996.
[20] J. Yin and H. Li. A sparse conditional gaussian graphical model for analysis of genetical
genomics data. The Annals of Applied Statistics, 2011.
[21] Y. Zhang and J. Schneider. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty. In
NIPS, 2010.
[22] Y. Zhang and D. Yeung. A convex formulation for learning task relationships in multi-task
learning. In UAI, 2010.
[23] S. Zhou, J. Lafferty, and L. Wasserman. Time varying undirected graphs. Machine Learning
Journal, 2010.

9


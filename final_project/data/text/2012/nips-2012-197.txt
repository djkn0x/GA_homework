Online Regret Bounds for Undiscounted Continuous
Reinforcement Learning

Ronald Ortner∗ †
∗Montanuniversitaet Leoben
8700 Leoben, Austria
rortner@unileoben.ac.at

Daniil Ryabko†
† INRIA Lille-Nord Europe, ´equipe SequeL
59650 Villeneuve d’Ascq, France
daniil@ryabko.net

Abstract

We derive sublinear regret bounds for undiscounted reinforcement learning in con-
tinuous state space. The proposed algorithm combines state aggregation with the
use of upper conﬁdence bounds for implementing optimism in the face of uncer-
tainty. Beside the existence of an optimal policy which satisﬁes the Poisson equa-
tion, the only assumptions made are H ¨older continuity of rewards and transition
probabilities.

1

Introduction

Real world problems usually demand continuous state or action spaces, and one of the challenges for
reinforcement learning is to deal with such continuous domains. In many problems there is a natural
metric on the state space such that close states exhibit similar behavior. Often such similarities can
be formalized as Lipschitz or more generally H ¨older continuity of reward and transition functions.
The simplest continuous reinforcement learning problem is the 1-dimensional continuum-armed
bandit, where the learner has to choose arms from a bounded interval. Bounds on the regret with
respect to an optimal policy under the assumption that the reward function is H ¨older continuous have
been given in [15, 4]. The proposed algorithms apply the UCB algorithm [2] to a discretization of the
problem. That way, the regret suffered by the algorithm consists of the loss by aggregation (which
can be bounded using H ¨older continuity) plus the regret the algorithm incurs in the discretized set-
ting. More recently, algorithms that adapt the used discretization (making it ﬁner in more promising
regions) have been proposed and analyzed [16, 8].
While the continuous bandit case has been investigated in detail, in the general case of continuous
state Markov decision processes (MDPs) a lot of work is conﬁned to rather particular settings, pri-
marily with respect to the considered transition model. In the simplest case, the transition function
is considered to be deterministic as in [19], and mistake bounds for the respective discounted set-
ting have been derived in [6]. Another common assumption is that transition functions are linear
√
functions of state and action plus some noise. For such settings sample complexity bounds have
been given in [23, 7], while ˜O(
T ) bounds for the regret after T steps are shown in [1]. However,
there is also some research considering more general transition dynamics under the assumption that
close states behave similarly, as will be considered here. While most of this work is purely experi-
mental [12, 24], there are also some contributions with theoretical guarantees. Thus, [13] considers
PAC-learning for continuous reinforcement learning in metric state spaces, when generative sam-
pling is possible. The proposed algorithm is a generalization of the E3 algorithm [14] to continuous
domains. A respective adaptive discretization approach is suggested in [20]. The PAC-like bounds
derived there however depend on the (random) behavior of the proposed algorithm.

Here we suggest a learning algorithm for undiscounted reinforcement learning in continuous state
space. The proposed algorithm is in the tradition of algorithms like UCRL2 [11] in that it implements

1

the “optimism in the face of uncertainty” maxim, here combined with state aggregation. Thus, the
algorithm does not need a generative model or access to “resets:” learning is done online, that is, in
a single continual session of interactions between the environment and the learning policy.
For our algorithm we derive regret bounds of ˜O(T (2+α)/(2+2α) ) for MDPs with 1-dimensional
state space and H ¨older-continuous rewards and transition probabilities with parameter α. These
bounds also straightforwardly generalize to dimension d where the regret
is bounded by
˜O(T (2d+α)/(2d+2α) ). Thus, in particular, if rewards and transition probabilities are Lipschitz, the
regret is bounded by ˜O(T (2d+1)/(2d+2)) ) in dimension d and ˜O(T 3/4 ) in dimension 1. We also
√
present an accompanying lower bound of Ω(
T ). As far as we know, these are the ﬁrst regret
bounds for a general undiscounted continuous reinforcement learning setting.

2 Preliminaries

We consider the following setting. Given is a Markov decision process (MDP) M with state space
S = [0, 1]d and ﬁnite action space A. For the sake of simplicity, in the following we assume d = 1.
However, proofs and results generalize straightforwardly to arbitrary dimension, cf. Remark 5 below.
The random rewards in state s under action a are assumed to be bounded in [0, 1] with mean r(s, a).
The transition probability distribution in state s under action a is denoted by p(·|s, a).
We will make the natural assumption that rewards and transition probabilities are similar in close
states. More precisely, we assume that rewards and transition probabilities are H ¨older continuous.
Assumption 1. There are L, α > 0 such that for any two states s, s(cid:48) and all actions a,
|r(s, a) − r(s(cid:48) , a)| ≤ L|s − s(cid:48) |α .
(cid:13)(cid:13)p(·|s, a) − p(·|s(cid:48) , a)(cid:13)(cid:13)1 ≤ L|s − s(cid:48) |α .
Assumption 2. There are L, α > 0 such that for any two states s, s(cid:48) and all actions a,
For the sake of simplicity we will assume that α and L in Assumptions 1 and 2 are the same.
We also assume existence of an optimal policy π∗ : S → A which gives optimal average reward
ρ∗ = ρ∗ (M ) on M independent of the initial state. A sufﬁcient condition for state-independent
optimal reward is geometric convergence of π∗ to an invariant probability measure. This is a natural
condition which e.g. holds for any communicating ﬁnite state MDP. It also ensures (cf. Chapter 10
(cid:90)
of [10]) that the Poisson equation holds for the optimal policy. In general, under suitable technical
conditions (like geometric convergence to an invariant probability measure µπ ) the Poisson equation
p(ds(cid:48) |s, π(s)) · λπ (s(cid:48) )
ρπ + λπ (s) = r(s, π(s)) +
S
relates the rewards and transition probabilities under any measurable policy π to its average re-
and the normalizing equation (cid:82)
ward ρπ and the bias function λπ : S → R of π . Intuitively, the bias is the difference in accumulated
rewards when starting in a different state. Formally, the bias is deﬁned by the Poisson equation (1)
S λπ dµπ = 0 (cf. e.g. [9]). The following result follows from the
bias deﬁnition and Assumptions 1 and 2 (together with results from Chapter 10 of [10]).
Proposition 3. Under Assumptions 1 and 2, the bias of the optimal policy is bounded.

(1)

Consequently, it makes sense to deﬁne the bias span H (M ) of a continuous state MDP M satisfying
Assumptions 1 and 2 to be H (M ) := sups λπ∗ (s) − inf s λπ∗ (s). Note that since inf s λπ∗ (s) ≤ 0
by deﬁnition of the bias, the bias function λπ∗ is upper bounded by H (M ).
performance by the regret (after T steps) deﬁned as T ρ∗ (M ) − (cid:80)T
We are interested in algorithms which can compete with the optimal policy π∗ and measure their
t=1 rt , where rt is the random
reward obtained by the algorithm at step t. Indeed, within T steps no canonical or even bias optimal
optimal policy (cf. Chapter 10 of [10]) can obtain higher accumulated reward than T ρ∗ + H (M ).

3 Algorithm

Our algorithm UCCR L, shown in detail in Figure 1, implements the “optimism in the face of uncer-
tainty maxim” just like UCRL2 [11] or REGAL [5]. It maintains a set of plausible MDPs M and

2

Algorithm 1 The UCCR L algorithm
Input: State space S = [0, 1], action space A, conﬁdence parameter δ > 0, aggregation parame-
ter n ∈ N, upper bound H on the bias span, Lipschitz parameters L, α.
(cid:66) Let I1 := (cid:2)0, 1
(cid:3) for j = 2, 3, . . . , n.
(cid:3), Ij := (cid:0) j−1
Initialization:
n , j
(cid:66) Set t := 1, and observe the initial state s1 and interval I (s1 ).
n
n
for episodes k = 1, 2, . . . do
(cid:66) Let Nk (Ij , a) be the number of times action a has been chosen in a state ∈ Ij prior to
episode k , and vk (Ij , a) the respective counts in episode k .
Initialize episode k :
(cid:66) Set the start time of episode k , tk := t.
k (Ii |s, a) for rewards and transition probabilities, using
(cid:66) Compute estimates ˆrk (s, a) and ˆpagg
all samples from states in the same interval I (s), respectively.
Compute policy ˜πk :
(cid:66) Let Mk be the set of plausible MDPs ˜M with H ( ˜M ) ≤ H and rewards ˜r(s, a) and
(cid:113) 7 log(2nAtk /δ)
(cid:12)(cid:12)˜r(s, a) − ˆrk (s, a)(cid:12)(cid:12) ≤ Ln−α +
transition probabilities ˜p(·|s, a) satisfying
(cid:113) 56n log(2Atk /δ)
(cid:13)(cid:13)(cid:13) ˜pagg (·|s, a) − ˆpagg
(cid:13)(cid:13)(cid:13)1
2 max{1,Nk (I (s),a)} ,
k (·|s, a)
≤ Ln−α +
max{1,Nk (I (s),a)} .
(cid:66) Choose policy ˜πk and ˜Mk ∈ Mk such that
ρ ˜πk ( ˜Mk ) = arg max{ρ∗ (M ) | M ∈ Mk }.

(4)

(2)

(3)

Execute policy ˜πk :
while vk (I (st ), ˜πk (st )) < max{1, Nk (I (st ), ˜πk (st ))} do
(cid:66) Choose action at = ˜πk (st ), obtain reward rt , and observe next state st+1 .
(cid:66) Set t := t + 1.
end while
end for

chooses optimistically an MDP ˜M ∈ M and a policy ˜π such that the average reward ρ ˜π ( ˜M ) is max-
imized, cf. (4). Whereas for UCRL2 and REGAL the set of plausible MDPs is deﬁned by conﬁdence
intervals for rewards and transition probabilities for each individual state-action pair, for UCCR L
we assume an MDP to be plausible if its aggregated rewards and transition probabilities are within
a certain range. This range is deﬁned by the aggregation error (determined by the assumed H ¨older
continuity) and respective conﬁdence intervals, cf. (2), (3). Correspondingly, the estimates for re-
(cid:3),
More precisely, for the aggregation UCCR L partitions the state space into intervals I1 := (cid:2)0, 1
wards and transition probabilities for some state action-pair (s, a) are calculated from all sampled
values of action a in states close to s.
(cid:3) for k = 2, 3, . . . , n. The corresponding aggregated transition probabilities are
Ik := (cid:0) k−1
n
(cid:90)
n , k
n
deﬁned by
p(ds(cid:48) |s, a).
pagg (Ij |s, a) :=
(5)
Ij
Generally, for a (transition) probability distribution p(·) over S we write pagg (·) for the aggre-
gated probability distribution with respect to {I1 , I2 . . . , In}. Now, given the aggregated state space
{I1 , I2 . . . , In}, estimates ˆr(s, a) and ˆpagg (·|s, a) are calculated from all samples of action a in
states in I (s), the interval Ij containing s. (Consequently, the estimates are the same for states in
the same interval.)
As UCRL2 and REGAL, UCCR L proceeds in episodes in which the chosen policy remains ﬁxed.
Episodes are terminated when the number of times an action has been sampled from some interval Ij
has been doubled. Only then estimates are updated and a new policy is calculated.

3

Since all states in the same interval Ij have the same conﬁdence intervals, ﬁnding the optimal
pair ˜Mk , ˜πk in (4) is equivalent to ﬁnding the respective optimistic discretized MDP ˜M agg
and
k
to S , that is,
on ˜M agg
an optimal policy ˜πagg
. Then ˜πk can be set to be the extension of ˜πagg
k
k
k
˜πk (s) := ˜πagg
(I (s)) for all s. However, due to the constraint on the bias even in this ﬁnite case
k
efﬁcient computation of ˜M agg
and ˜πagg
is still an open problem. We note that the REGAL.C algo-
k
k
rithm [5] selects optimistic MDP and optimal policy in the same way as UCCR L.
While the algorithm presented here is the ﬁrst modiﬁcation of UCRL2 to continuous reinforcement
learning problems, there are similar adaptations to online aggregation [21] and learning in ﬁnite state
MDPs with some additional similarity structure known to the learner [22].

4 Regret Bounds

For UCCR L we can derive the following bounds on the regret.
Theorem 4. Let M be an MDP with continuous state space [0, 1], A actions, rewards and transi-
tion probabilities satisfying Assumptions 1 and 2, and bias span upper bounded by H . Then with
(cid:113)
probability 1 − δ , the regret of UCCR L (run with input parameters n and H ) after T steps is upper
(cid:1) + const(cid:48) · H Ln−αT .
AT log (cid:0) T
bounded by
const · nH
(cid:113)
(cid:1) · T (2+α)/(2+2α) .
A log (cid:0) T
δ
Therefore, setting n = T 1/(2+2α) gives regret upper bounded by
const · H L
δ
With no known upper bound on the bias span, guessing H by log T one still obtains an upper bound
on the regret of ˜O(T (2+α)/(2+2α) ).

(6)

Intuitively, the second term in the regret bound of (6) is the discretization error, while the ﬁrst term
corresponds to the regret on the discretized MDP. A detailed proof of Theorem 4 can be found in
Section 5 below.
Remark 5 (d-dimensional case). The general d-dimensional case can be handled as described for
dimension 1, with the only difference being that the discretization now has nd states, so that one
has nd instead of n in the ﬁrst term of (6). Then choosing n = T 1/(2d+2α) bounds the regret by
˜O(T (2d+α)/(2d+2α) ).
Remark 6 (unknown horizon). If the horizon T is unknown then the doubling trick (executing the
algorithm in rounds i = 1, 2, . . . guessing T = 2i and setting the conﬁdence parameter to δ/2i )
gives the same bounds.
Remark 7 (unknown H ¨older parameters). The UCCR L algorithm receives (bounds on) the
H ¨older parameters L as α as inputs. If these parameters are not known, then one can still obtain
sublinear regret bounds albeit with worse dependence on T . Speciﬁcally, we can use the model-
selection technique introduced in [17]. To do this, ﬁx a certain number J of values for the constants
L and α; each of these values will be considered as a model. The model selection consists in running
UCCR L with each of these parameter values for a certain period of τ0 time steps (exploration). Then
one selects the model with the highest reward and uses it for a period of τ (cid:48)
0 time steps (exploitation),
while checking that its average reward stays within (6) of what was obtained in the exploitation
phase. If the average reward does not pass this test, then the model with the second-best average
reward is selected, and so on. Then one switches to exploration with longer periods τ1 , etc. Since
there are no guarantees on the behavior of UCCR L when the H ¨older parameters are wrong, none
of the models can be discarded at any stage. Optimizing over the parameters τi and τ (cid:48)
i as done
in [17], and increasing the number J of considered parameter values, one can obtain regret bounds
of ˜O(T (2+2α)/(2+3α) ), or ˜O(T 4/5 ) in the Lipschitz case. For details see [17]. Since in this model-
selection process UCCR L is used in a “black-box” fashion, the exploration is rather wasteful, and
thus we think that this bound is suboptimal. Recently, the results of [17] have been improved [18],
and it seems that similar analysis gives improved regret bounds for the case of unknown H ¨older
parameters as well.

The following is a complementing lower bound on the regret for continuous state reinforcement
learning.

4

Theorem 8. For any A, H > 1 and any reinforcement learning algorithm there is a continuous
√
state reinforcement learning problem with A actions and bias span H satisfying Assumption 1 such
that the algorithm suffers regret of Ω(
H AT ).

Proof. Consider the following reinforcement learning problem with state space [0, 1]. The state
space is partitioned into n intervals Ij of equal size. The transition probabilities for each action a
are on each of the intervals Ij concentrated and equally distributed on the same interval Ij . The
rewards on each interval Ij are also constant for each a and are chosen as in the lower bounds for a
√
multi-armed bandit problem [3] with nA arms. That is, giving only one arm slightly higher reward,
it is known [3] that regret of Ω(
nAT ) can be forced upon any algorithm on the respective bandit
√
problem. Adding another action giving no reward and equally distributing over the whole state
space, the bias span of the problem is n and the regret Ω(
H AT ).

Remark 9. Note that Assumption 2 does not hold in the example used in the proof of Theorem 8.
However, the transition probabilities are piecewise constant (and hence Lipschitz) and known to
the learner. Actually, it is straightforward to deal with piecewise H ¨older continuous rewards and
transition probabilities where the ﬁnitely many points of discontinuity are known to the learner. If
one makes sure that the intervals of the discretized state space do not contain any discontinuities, it
is easy to adapt UCCR L and Theorem 4 accordingly.
Remark 10 (comparison to bandits). The bounds of Theorems 4 and 8 cannot be directly com-
pared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no
special case of learning MDPs with continuous state space (and rather corresponds to a continuous
action space). Thus, in particular one cannot freely sample an arbitrary state of the state space as
assumed in continuous-armed bandits.

5 Proof of Theorem 4

For the proof of the main theorem we adapt the proof of the regret bounds for ﬁnite MDPs in [11]
and [5]. Although the state space is now continuous, due to the ﬁnite horizon T , we can reuse
some arguments, so that we keep the structure of the original proof of Theorem 2 in [11]. Some of
the necessary adaptations made are similar to techniques used for showing regret bounds for other
modiﬁcations of the original UCRL2 algorithm [21, 22], which however only considered ﬁnite-state
MDPs.
5.1 Splitting into Episodes
denote the total number of episodes by m. Then setting ∆k := (cid:80)
Let vk (s, a) be the number of times action a has been chosen in episode k when being in state s, and
s,a vk (s, a)(ρ∗ − r(s, a)), with
(cid:113) 5
probability at least 1 − δ
(cid:1) + (cid:80)m
8 T log (cid:0) 8T
12T 5/4 the regret of UCCR L after T steps is upper bounded by (cf. Section
4.1 of [11]),
k=1 ∆k .
δ

(7)

5.2 Failing Conﬁdence Intervals

Next, we consider the regret incurred when the true MDP M is not contained in the set of plausi-
ble MDPs Mk . Thus, ﬁx a state-action pair (s, a), and recall that ˆr(s, a) and ˆpagg (·|s, a) are the
estimates for rewards and transition probabilities calculated from all samples of state-action pairs
contained in the same interval I (s). Now assume that at step t there have been N > 0 samples of
action a in states in I (s) and that in the i-th sample a transition from state si ∈ I (s) to state s(cid:48)
i has
been observed (i = 1, . . . , N ).
First, concerning the rewards one obtains as in the proof of Lemma 17 in Appendix C.1 of [11] — but
now using Hoeffding for independent and not necessarily identically distributed random variables
(cid:110)(cid:12)(cid:12)ˆr(s, a) − E[ˆr(s, a)](cid:12)(cid:12) ≥ (cid:113) 7
(cid:1)(cid:111) ≤
2N log (cid:0) 2nAt
— that
δ
5

δ
60nAt7 .

Pr

(8)

(10)

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) ˆpagg (Ij |s, a) − E[ ˆpagg (Ij |s, a)]
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13) ˆpagg (·|s, a) − E[ ˆpagg (·|s, a)]
n(cid:88)
Concerning the transition probabilities, we have for a suitable x ∈ {−1, 1}n
(cid:16)
(cid:17)
n(cid:88)
j=1
ˆpagg (Ij |s, a) − E[ ˆpagg (Ij |s, a)]
=
x(Ij )
(cid:90)
(cid:16)
(cid:17)
N(cid:88)
j=1
i )) −
p(ds(cid:48) |si , a) · x(I (s(cid:48) ))
x(I (s(cid:48)
i ))−(cid:82)
= 1
(9)
.
N
S
with |Xi | ≤ 2, so that by Azuma-Hoeffding inequality (e.g., Lemma 10 in [11]), Pr{ (cid:80)N
i=1
S p(ds(cid:48) |si , a)·x(I (s(cid:48) )) is a martingale difference sequence
For any x ∈ {−1, 1}n , Xi := x(I (s(cid:48)
i=1 Xi ≥
i=1 Xi ≥ (cid:113)
(cid:17)7n ≤
(cid:1)(cid:111) ≤ (cid:16) δ
(cid:110) (cid:80)N
56nN log (cid:0) 2At
θ} ≤ exp(−θ2/8N ) and in particular
δ
Pr
2n20nAt7 .
≥ (cid:113) 56n
(cid:110)(cid:13)(cid:13)(cid:13) ˆpagg (·|s, a) − E[ ˆpagg (·|s, a)]
(cid:13)(cid:13)(cid:13)1
(cid:1)(cid:111) ≤
δ
2At
N log (cid:0) 2At
A union bound over all sequences x ∈ {−1, 1}n then yields from (9) that
δ
Pr
20nAt7 .
δ
Another union bound over all t possible values for N , all n intervals and all actions shows that the
conﬁdence intervals in (8) and (10) hold at time t with probability at least 1 − δ
15t6 for the actual
counts N (I (s), a) and all state-action pairs (s, a). (Note that the equations (8) and (10) are the same
(cid:80)N
for state-action pairs with states in the same interval.)
(cid:13)(cid:13)E[ ˆpagg (·|s, a)] − pagg (·|s, a)(cid:13)(cid:13)1 < Ln−α . Together with (8) and (10) this shows that with proba-
Now, by linearity of expectation E[ˆr(s, a)] can be written as 1
i=1 r(si , a). Since the si are as-
sumed to be in the same interval I (s), it follows that |E[ˆr(s, a)] − r(s, a)| < Ln−α . Similarly,
N
(cid:113) 7 log(2nAt/δ)
(cid:12)(cid:12)ˆr(s, a) − r(s, a)(cid:12)(cid:12) < Ln−α +
bility at least 1 − δ
15t6 for all state-action pairs (s, a)
(cid:113) 56n log(2At/δ)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13) ˆpagg (·|s, a) − pagg (·|s, a)
(11)
2 max{1,N (I (s),a)} ,
< Ln−α +
(12)
max{1,N (I (s),a)} .
This shows that the true MDP is contained in the set of plausible MDPs M(t) at step t with proba-
m(cid:88)
bility at least 1 − δ
15t6 , just as in Lemma 17 of [11]. The argument that
√
∆k 1M (cid:54)∈Mk ≤
k=1
with probability at least 1 − δ
12T 5/4 then can be taken without any changes from Section 4.2 of [11].
5.3 Regret in Episodes with M ∈ Mk
∆k = (cid:88)
vk (s, ˜πk (s))(cid:0)ρ∗ − r(s, ˜πk (s))(cid:1)
Now for episodes with M ∈ Mk , by the optimistic choice of ˜Mk and ˜πk in (4) we can bound
≤ (cid:88)
vk (s, ˜πk (s))(cid:0) ˜ρ∗
k − r(s, ˜πk (s))(cid:1)
s
k − ˜rk (s, ˜πk (s))(cid:1) + (cid:88)
= (cid:88)
vk (s, ˜πk (s))(cid:0)˜rk (s, ˜πk (s)) − r(s, ˜πk (s))(cid:1).
vk (s, ˜πk (s))(cid:0) ˜ρ∗
s
s
s
Any term ˜rk (s, a) − r(s, a) ≤ |˜rk (s, a) − ˆrk (s, a)| + |ˆrk (s, a) − r(s, a)| is bounded according to
(cid:19)
(cid:18)
(cid:113) 7 log(2nAtk /δ)
(cid:88)
n(cid:88)
∆k ≤ (cid:88)
vk (s, ˜πk (s))(cid:0) ˜ρ∗
k − ˜rk (s, ˜πk (s))(cid:1) + 2
(2) and (11), as we assume that ˜Mk , M ∈ Mk , so that summarizing states in the same interval Ij
Ln−α +
2 max{1,Nk (Ij ,a)}
a∈A
s
j=1

vk (Ij , a)

T

(13)

=

.

6

(14)

(15)

∆k ≤ (cid:88)
vk (s, ˜πk (s))(cid:0) ˜ρ∗
k − ˜rk (s, ˜πk (s))(cid:1)
Since max{1, Nk (Ij , a)} ≤ tk ≤ T , setting τk := tk+1 − tk to be the length of episode k we have
(cid:113)
(cid:1) n(cid:88)
(cid:88)
14 log (cid:0) 2nAT
(cid:112)max{1, Nk (Ij , a)} .
s
vk (Ij , a)
+ 2Ln−α τk +
δ
a∈A
j=1
(cid:88)
We continue analyzing the ﬁrst term on the right hand side of (14). By the Poisson equation (1) for
k − ˜rk (s, ˜πk (s))(cid:1)
vk (s, ˜πk (s))(cid:0) ˜ρ∗
˜πk on ˜Mk , denoting the respective bias by ˜λk := ˜λ ˜πk we can write
(cid:16) (cid:90)
(cid:17)
= (cid:88)
s
˜pk (ds(cid:48) |s, ˜πk (s)) · ˜λk (s(cid:48) ) − ˜λk (s)
(cid:16) (cid:90)
vk (s, ˜πk (s))
(cid:17)
= (cid:88)
S
s
p(ds(cid:48) |s, ˜πk (s)) · ˜λk (s(cid:48) ) − ˜λk (s)
(cid:90)
vk (s, ˜πk (s))
(cid:16)
(cid:17) · ˜λk (s(cid:48) ).
n(cid:88)
+ (cid:88)
S
s
˜pk (ds(cid:48) |s, ˜πk (s)) − p(ds(cid:48) |s, ˜πk (s))
vk (s, ˜πk (s))
(16)
Ij
s
j=1
Now (cid:13)(cid:13) ˜pagg
k (·|s, a)(cid:13)(cid:13)1 + (cid:13)(cid:13) ˆpagg
k (·|s, a) − pagg (·|s, a)(cid:13)(cid:13)1 ≤ (cid:13)(cid:13) ˜pagg
k (·|s, a) − pagg (·|s, a)(cid:13)(cid:13)1
5.4 The True Transition Functions
k (·|s, a) − ˆpagg
can be bounded by (3) and (12), because we assume ˜Mk , M ∈ Mk . Hence, since by deﬁnition of
(cid:90)
(cid:16)
(cid:17)
n(cid:88)
(cid:88)
the algorithm H bounds the bias function ˜λk , the term in (16) is bounded by
˜pk (ds(cid:48) |s, ˜πk (s)) − p(ds(cid:48) |s, ˜πk (s))
˜λk (s(cid:48) )
vk (s, ˜πk (s))
(cid:16)
vk (s, ˜πk (s)) · H · n(cid:88)
≤ (cid:88)
Ij
s
j=1
k (Ij |s, ˜πk (s)) − pagg (Ij |s, ˜πk (s))
(cid:113) 56n log(2AT /δ)
(cid:17)
(cid:16)
˜pagg
≤ (cid:88)
s
j=1
vk (s, ˜πk (s)) · H · 2
Ln−α +
(cid:113)
(cid:1) n(cid:88)
(cid:88)
max{1,Nk (I (s),at )}
14n log (cid:0) 2AT
(cid:112)max{1, Nk (Ij , a)} ,
s
vk (Ij , a)
= 2H Ln−α τk + 4H
δ
(cid:16) (cid:90)
(cid:17)
a∈A
(cid:88)
j=1
while for the term in (15)
p(ds(cid:48) |s, ˜πk (s)) · ˜λk (s(cid:48) ) − ˜λk (s)
vk (s, ˜πk (s))
(cid:16) (cid:90)
(cid:17)
tk+1−1(cid:88)
S
s
p(ds(cid:48) |st , at ) · ˜λk (s(cid:48) ) − ˜λk (st )
(cid:16) (cid:90)
tk+1−1(cid:88)
S
t=tk
S
t=tk

(cid:82)
Let k(t) be the index of the episode time step t belongs to. Then the sequence Xt
:=
S p(ds(cid:48) |st , at ) · ˜λk(t) (s(cid:48) ) − ˜λk(t) (st+1 ) is a sequence of martingale differences so that Azuma-
(cid:18) tk+1−1(cid:88)
(cid:19)
Hoeffding inequality shows (cf. Section 4.3.2 and in particular eq. (18) in [11]) that after summing
(cid:16) (cid:90)
(cid:17)
m(cid:88)
over all episodes we have
+ ˜λk (stk+1 ) − ˜λk (stk )
p(ds(cid:48) |st , at ) · ˜λk (s(cid:48) ) − ˜λk (st+1 )
(cid:113)
(cid:1),
(cid:0) 8T
2 T log (cid:0) 8T
(cid:1) + H nA log2
S
t=tk
k=1
5
δ
nA

p(ds(cid:48) |st , at ) · ˜λk (s(cid:48) ) − ˜λk (st+1 )

+ ˜λk (stk+1 ) − ˜λk (stk ).

=

=

(17)

(18)

(cid:17)

(cid:17)

≤ H

7

+ H

(19)

where the second term comes from an upper bound on the number of episodes, which can be derived
analogously to Appendix C.2 of [11].
5.5 Summing over Episodes with M ∈ Mk
To conclude, we sum (14) over all the episodes with M ∈ Mk , using (15), (17), and (18). This
(cid:113)
m(cid:88)
(cid:1) · m(cid:88)
n(cid:88)
(cid:88)
yields that with probability at least 1 − δ
14n log (cid:0) 2AT
(cid:112)max{1, Nk (Ij , a)}
12T 5/4
vk (Ij , a)
∆k 1M ∈Mk ≤ 2H Ln−αT + 4H
(cid:113)
(cid:1)
(cid:0) 8T
(cid:1) + H nA log2
2 T log (cid:0) 8T
δ
a∈A
j=1
k=1
k=1
(cid:113)
n(cid:88)
(cid:88)
(cid:1) m(cid:88)
14 log (cid:0) 2nAT
5
(cid:112)max{1, Nk (Ij , a)} .
δ
nA
vk (Ij , a)
+ 2Ln−αT +
δ
a∈A
j=1
k=1
n(cid:88)
(cid:88)
(cid:88)
(cid:112)max{1, Nk (Ij , a)} ≤ (cid:0)√
2 + 1(cid:1)√
Analogously to Section 4.3.3 and Appendix C.3 of [11], one can show that
vk (Ij , a)
a∈A
j=1
k
(cid:113)
m(cid:88)
and we get from (19) after some simpliﬁcations that with probability ≥ 1 − δ
(cid:0) 8T
2 T log (cid:0) 8T
(cid:1) + H nA log2
(cid:1)
12T 5/4
∆k 1M ∈Mk ≤ H
(cid:113)
(cid:1)(cid:17)(cid:0)√
(cid:16)
5
2 + 1(cid:1)√
14n log (cid:0) 2AT
δ
nA
k=1
nAT + 2(H + 1)Ln−αT .
(4H + 1)
+
(20)
δ
Finally, evaluating (7) by summing ∆k over all episodes, by (13) and (20) we have with probability
(cid:113)
m(cid:88)
m(cid:88)
≥ 1 − δ
(cid:1) +
8 T log (cid:0) 8T
4T 5/4 an upper bound on the regret of
(cid:113)
≤ (cid:113)
∆k 1M ∈Mk
∆k 1M /∈Mk +
5
(cid:1) + H nA log2
2 T log (cid:0) 8T
(cid:1) +
8 T log (cid:0) 8T
(cid:1)
(cid:0) 8T
δ
√
k=1
k=1
(cid:113)
(cid:16)
(cid:1)(cid:17)(cid:0)√
14n log (cid:0) 2AT
2 + 1(cid:1)√
T + H
5
5
nA
δ
δ
nAT + 2(H + 1)Ln−αT .
(4H + 1)
+
δ
A union bound over all possible values of T and further simpliﬁcations as in Appendix C.4 of [11]
ﬁnish the proof.

nAT ,

6 Outlook

We think that a generalization of our results to continuous action space should not pose any major
problems.
In order to improve over the given bounds, it may be promising to investigate more
sophisticated discretization patterns.
The assumption of H ¨older continuity is an obvious, yet not the only possible assumption one can
make about the transition probabilities and reward functions. A more general problem is to assume
a set F of functions, ﬁnd a way to measure the “size” of F , and derive regret bounds depending on
this size of F .

Acknowledgments

The authors would like to thank the three anonymous reviewers for their helpful suggestions and
R ´emi Munos for useful discussion which helped to improve the bounds. This research was funded by
the Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and FEDER
(Contrat de Projets Etat Region CPER 2007-2013), ANR projects EXPLO-RA (ANR-08-COSI-
004), Lampada (ANR-09-EMER-007) and CoAdapt, and by the European Community’s FP7 Pro-
gram under grant agreements n◦ 216886 (PASCAL2) and n◦ 270327 (CompLACS). The ﬁrst author
is currently funded by the Austrian Science Fund (FWF): J 3259-N13.

8

References
[1] Yasin Abbasi-Yadkori and Csaba Szepesv ´ari. Regret bounds for the adaptive control of linear quadratic
systems. COLT 2011, JMLR Proceedings Track, 19:1–26, 2011.
[2] Peter Auer, Nicol `o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multi-armed bandit prob-
lem. Mach. Learn., 47:235–256, 2002.
[3] Peter Auer, Nicol `o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM J. Comput., 32:48–77, 2002.
[4] Peter Auer, Ronald Ortner, and Csaba Szepesv ´ari. Improved rates for the stochastic continuum-armed
bandit problem. In Learning Theory, 20th Annual Conference on Learning Theory, COLT 2007, pages
454–468, 2007.
[5] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning
in weakly communicating MDPs. In Proc. 25th Conference on Uncertainty in Artiﬁcial Intelligence, UAI
2009, pages 25–42, 2009.
[6] Andrey Bernstein and Nahum Shimkin. Adaptive-resolution reinforcement learning with polynomial
exploration in deterministic domains. Mach. Learn., 81(3):359–397, 2010.
[7] Emma Brunskill, Bethany R. Lefﬂer, Lihong Li, Michael L. Littman, and Nicholas Roy. Provably efﬁcient
learning with typed parametric models. J. Mach. Learn. Res., 10:1955–1988, 2009.
[8] S ´ebastien Bubeck, R ´emi Munos, Gilles Stoltz, and Csaba Szepesv ´ari. Online optimization of χ-armed
bandits. In Advances in Neural Information Processing Systems 22, NIPS 2009, pages 201–208, 2010.
[9] On ´esimo Hern ´andez-Lerma and Jean Bernard Lasserre. Discrete-time Markov control processes, vol-
ume 30 of Applications of mathematics. Springer, 1996.
[10] On ´esimo Hern ´andez-Lerma and Jean Bernard Lasserre. Further topics on discrete-time Markov control
processes, volume 42 of Applications of mathematics. Springer, 1999.
[11] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
J. Mach. Learn. Res., 11:1563–1600, 2010.
[12] Nicholas K. Jong and Peter Stone. Model-based exploration in continuous state spaces. In Abstraction,
Reformulation, and Approximation, 7th International Symposium, SARA 2007, pages 258–272. Springer,
2007.
[13] Sham Kakade, Michael J. Kearns, and John Langford. Exploration in metric state spaces. In Machine
Learning, Proc. 20th International Conference, ICML 2003, pages 306–312, 2003.
[14] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Mach.
Learn., 49:209–232, 2002.
[15] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances Neural
Information Processing Systems 17, NIPS 2004, pages 697–704, 2005.
[16] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proc.
40th Annual ACM Symposium on Theory of Computing, STOC 2008, pages 681–690, 2008.
[17] Odalric-Ambrym Maillard, R ´emi Munos, and Daniil Ryabko. Selecting the state-representation in rein-
forcement learning. In Advances Neural Processing Systems 24, NIPS 2011, pages 2627–2635, 2012.
[18] Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, and Daniil Ryabko. Optimal regret bounds
for selecting the state representation in reinforcement learning. accepted for ICML 2013.
[19] Gerhard Neumann, Michael Pfeiffer, and Wolfgang Maass. Efﬁcient continuous-time reinforcement learn-
ing with adaptive state graphs. In Machine Learning: ECML 2007, 18th European Conference on Machine
Learning, pages 250–261, 2007.
[20] Ali Nouri and Michael L. Littman. Multi-resolution exploration in continuous spaces. In Advances in
Neural Information Processing Systems 21, NIPS 2008, pages 1209–1216, 2009.
[21] Ronald Ortner. Adaptive aggregation for reinforcement learning in average reward Markov decision
processes. Ann. Oper. Res., 2012. doi:10.1007/s10479-12-1064-y, to appear.
[22] Ronald Ortner, Daniil Ryabko, Peter Auer, and R ´emi Munos. Regret bounds for restless Markov bandits.
In Proc. 23rd Conference on Algorithmic Learning Theory, ALT 2012, pages 214–228, 2012.
[23] Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to model-based
reinforcement learning. In Advances Neural Information Processing Systems 20, NIPS 2007, pages 1417–
1424, 2008.
[24] William T. B. Uther and Manuela M. Veloso. Tree based discretization for continuous state space re-
inforcement learning. In Proc. 15th National Conference on Artiﬁcial Intelligence and 10th Innovative
Applications of Artiﬁcial Intelligence Conference, AAAI 98, IAAI 98, pages 769–774, 1998.

9


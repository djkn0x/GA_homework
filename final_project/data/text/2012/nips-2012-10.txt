Local Supervised Learning through Space
Partitioning

Joseph Wang
Dept. of Electrical and Computer Engineering
Boston University
Boston, MA 02116
joewang@bu.edu

Venkatesh Saligrama
Dept. of Electrical and Computer Engineering
Boston University
Boston, MA 02116
srv@bu.edu

Abstract

We develop a novel approach for supervised learning based on adaptively parti-
tioning the feature space into different regions and learning local region-speciﬁc
classiﬁers. We formulate an empirical risk minimization pr oblem that incorpo-
rates both partitioning and classiﬁcation in to a single glo bal objective. We show
that space partitioning can be equivalently reformulated as a supervised learning
problem and consequently any discriminative learning method can be utilized in
conjunction with our approach. Nevertheless, we consider locally linear schemes
by learning linear partitions and linear region classiﬁers . Locally linear schemes
can not only approximate complex decision boundaries and ensure low training
error but also provide tight control on over- ﬁtting and gene ralization error. We
train locally linear classiﬁers by using LDA, logistic regr ession and perceptrons,
and so our scheme is scalable to large data sizes and high-dimensions. We present
experimental results demonstrating improved performance over state of the art
classiﬁcation techniques on benchmark datasets. We also sh ow improved robust-
ness to label noise.

1 Introduction

We develop a novel approach for supervised learning based on adaptively partitioning the feature
space into different regions and learning local region classiﬁers. Fig. 1 (left) presents one possible
architecture of our scheme (others are also possible). Here each example passes through a cascade
of reject classiﬁers ( gj ’s). Each reject classiﬁer, gj , makes a binary decision and the observation is
either classiﬁed by the associated region classiﬁer,
fj , or passed to the next reject classiﬁer. Each
reject classiﬁer, gj , thus partitions the feature space into regions. The region classiﬁer fj operates
only on examples within the local region that is consistent with the reject classiﬁer partitions.

We incorporate both feature space partitioning (reject classiﬁers) and region-speciﬁc classiﬁers into
a single global empirical risk/loss function. We then optimize this global objective by means of coor-
dinate descent, namely, by optimizing over one classiﬁer at a time. In this context we show that each
step of the coordinate descent can be reformulated as a supervised learning problem that seeks to op-
timize a 0/1 empirical loss function. This result is somewhat surprising in the context of partitioning
and has broader implications. First, we can now solve feature space partitioning through empirical
risk function minimization(ERM) and so powerful existing methods including boosting, decision
trees and kernel methods can be used in conjunction for training ﬂexible partitioning classiﬁers.

Second, because data is usually locally “well-behaved,” si mpler region-classiﬁers, such as linear
classiﬁers, often sufﬁce for controlling local empirical e
rror. Furthermore, since complex boundaries
for partitions can be approximated by piecewise linear functions, feature spaces can be partitioned
to arbitrary degree of precision using linear boundaries (reject classiﬁers). Thus the combination
of piecewise linear partitions along with linear region classiﬁers has the ability to adapt to complex
data sets leading to low training error. Yet we can prevent over ﬁtting/overtraining by optimizing the

1

Local 
Perceptron 

Local 
Logistic 
Regression 

g1(x) 

g2(x) 

AdaBoost 

Decision Tree 

Figure 1: Left: Architecture of our system. Reject Classi ﬁers, gj (x), partition space and region classi ﬁers,
fj (x), are applied locally within the partitioned region. Right: Comparison of our approach (upper panel)
against Adaboost and Decision tree (lower panel) on the banana dataset[1]. We use linear perceptrons and
logistic regression for training partitioning classi ﬁer a nd region classi ﬁers. Our scheme splits with 3 regions
and does not overtrain unlike Adaboost.

number of linear partitions and linear region classiﬁers, s ince the VC dimension of such a structure
is reasonably small. In addition this also ensures signiﬁca nt robustness to label noise. Fig. 1 (right)
demonstrates the substantial bene ﬁts of our approach on the banana dataset[1] over competing meth-
ods such as boosting and decision trees, both of which evidently overtrain.

Limiting reject and region classiﬁers to linear methods has computational advantages as well. Since
the datasets are locally well-behaved we can locally train with linear discriminant analysis (LDA),
logistic regression and variants of perceptrons. These methods are computationally efﬁcient in that
they scale linearly with data size and data dimension. So we can train on large high-dimensional
datasets with possible applications to online scenarios.

Our approach naturally applies to multi-class datasets. Indeed, we present some evidence that shows
that the partitioning step can adaptively cluster the dataset into groups and letting region classiﬁers
to operate on simpler problems. Additionally linear methods such as LDA, Logistic regression, and
perceptron naturally extend to multi-class problems leading to computationally efﬁcient and statisti-
cally meaningful results as evidenced on challenging datasets with performance improvements over
state of the art techniques.

1.1 Related Work

Our approach ﬁts within the general framework of combining s imple classiﬁers for learning complex
structures. Boosting algorithms [2] learn complex decision boundaries characterized as a weighted
linear combination of weak classiﬁers. In contrast our meth od takes unions and intersections of
simpler decision regions to learn more complex decision boundaries. In this context our approach is
closely related to decision trees. Decision trees are built by greedily partitioning the feature space
[3]. One main difference is that decision trees typically attempt to greedily minimize some loss or a
heuristic, such as region purity or entropy, at each split/partition of the feature space. In contrast our
method attempts to minimize global classiﬁcation loss. Als o decision trees typically split/partition
a single feature/component resulting in unions of rectangularly shaped decision regions; in contrast
we allow arbitrary partitions leading to complex decision regions.

Our work is loosely related to so called coding techniques that have been used in multi-class classi-
ﬁcation [4, 5]. In these methods a multiclass problem is deco mposed into several binary problems
using a code matrix and the predicted outcomes of these binary problems are fused to obtain multi-
class labels. Jointly optimizing for the code matrix and binary classiﬁcation is known to be NP
hard [6] and iterative techniques have been proposed [7, 8]. There is some evidence (see Sec. 3)
that suggests that our space partitioning classiﬁer groups /clusters multiple classes into different re-
gions; nevertheless our formulation is different in that we do not explicitly code classes into different
regions and our method does not require fusion of intermediate outcomes.

Despite all these similarities, at a fundamental level, our work can also be thought of as a somewhat
complementary method to existing supervised learning algorithms. This is because we show that
space partitioning itself can be re-formulated as a supervised learning problem. Consequently, any

2

existing method, including boosting and decision trees, could be used as a method of choice for
learning space partitioning and region-speciﬁc decision f unctions.

R(f ) =

We use simple linear classiﬁers for partitioning and region -classiﬁers in many of our experiments.
Using piecewise combinations of simple functions to model a complex global boundary is a well
studied problem. Mixture Discriminant Analysis (MDA), proposed by Hastie et al. [9], models
each class as a mixture of gaussians, with linear discriminant analysis used to build classiﬁers be-
tween estimated gaussian distributions. MDA relies upon the structure of the data, assuming that
the true distribution is well approximated by a mixture of Gaussians. Local Linear Discriminant
Analysis (LLDA) , proposed by Kim et al. [10], clusters the data and performs LDA within each
cluster. Both of these approaches partition the data then attempt to classify locally. Partitioning of
the data is independent of the performance of the local classiﬁers, and instead based upon the spatial
structure of the data. In contrast, our proposed approach partitions the data based on the performance
of classiﬁers in each region. A recently proposed alternati ve approach is to build a global classiﬁer
ignoring clusters of errors, and building separate classiﬁ ers in each error cluster region [11]. This
proposed approach greedily approximates a piecewise linear classiﬁer in this manner, however fails
to take into account the performance of the classiﬁers in the
error cluster regions. While piece-
wise linear techniques have been proposed in the past [12, 13], we are unaware of techniques that
1
learn piecewise linear classiﬁers based on minimizing glob al ERM and allows any discriminative
approach to be used for partitioning and local classiﬁcatio n, and also extends to multiclass learning
problems.
2 Learning Space Partitioning Classi ﬁers
The goal of supervised classiﬁcation is to learn a function, f (x), that maps features, x ∈ X , to a
discrete label, y ∈ {1, 2, . . . , c}, based on training data, (xi , yi ), i = 1, 2, . . . , n. The empirical
risk/loss of classiﬁer f is:
n
Xi=1
Our goal is empirical risk minimization(ERM), namely, to minimize R(f ) over all classiﬁers, f (·),
belonging to some class F . It is well known that the complexity of the family F dictates general-
ization errors. If F is too simple, it often leads to large bias errors; if the family F is too rich, it
often leads to large variance errors. With this perspective we consider a family of classiﬁers (see
Fig. 1 that adaptively partitions data into regions and ﬁts s imple classiﬁers within each region. We
predict the output for a test sample, x, based on the output of the trained simple classiﬁer associa ted
with the region x belongs to. The complexity of our family of classiﬁers depen ds on the number
of local regions, the complexity of the simple classiﬁers in each region, and the complexity of the
partitioning. In the sequel we formulate space partitioning and region-classiﬁcation into a single
1
1
1
1
objective and show that space partitioning is equivalent to solving a binary classiﬁcation problem
with 0/1 empirical loss.
2.1 Binary Space Partitioning as Supervised Learning
In this section we consider learning binary space partitioning for ease of exposition. The function
g (·) partitions the space by mapping features, x ∈ X , to a binary label, z ∈ {0, 1}. Region classi-
ﬁers f0 (x), f1(x) operate on the respective regions generated by g (x) (see Fig. 1). The empirical
risk/loss associated with the binary space partitioned classiﬁers is given by:
i = 1
i = 1
n
n
1
1
Xi=1
Xi=1
R(g , f0 , f1 ) =
{f0 (xi ) 6=yi } +
{g(xi )=0}
n
n
Our goal is to minimize the empirical error jointly over the family of functions g (·) ∈ G and fi (·) ∈
F . From the above equation, when the partitioning function g (·) is ﬁxed, it is clear how one can
view choice of classiﬁers f0 (·) and f1 (·) as ERM problems. In contrast, even when f0 , f1 are ﬁxed,
it is unclear how to view minimization over g ∈ G as an ERM. To this end let, `(i)
0 , `(i)
indicate
1
whether or not classiﬁer f0 , f1 makes an error on example x(i) and let S denote the set of instances
where the classiﬁer f0 makes errors, namely,
`0
{f0 (xi ) 6=yi } , `1

{f1 (xi ) 6=yi } , S = {i | `0
i = 1}

{f (xi ) 6=yi }

{g(xi )=1}

{f1 (xi ) 6=yi }

(1)

1
n

3

(2)

=

=

=

1
1
1
1
1
i (1 − 1
1
1
We can then rewrite Eq. 1 as follows:
i )1
1
n
n
1
1
Xi=1
Xi=1
`1
`0
{g(xi )=0} +
R(g , f0 , f1 ) =
{g(xi )=1}
i
i
n
n
1
1
1
n Xi∈S
n Xi∈S
n Xi6∈S
`1
`1
{g(xi )=1} +
{g(xi )=0} +
{g(xi )=1}
i
i
1
1
1
n Xi∈S
n Xi∈S
n Xi6∈S
i )1
i )1
1
1
`1
{g(xi )=0} +
{g(xi )=0} ) +
{g(xi )=1}
1
1
n Xi∈S
n Xi∈S
(1 − `1
`1
`1
{g(xi )=0} +
+
{g(xi )=1}
i
i
| {z }
indep. of g
Note that for optimizing g ∈ G for ﬁxed f0 , f1 , the second term above is constant. Furthermore, by
wi1
consequence of Eq. 2 we see that the ﬁrst and third terms can be further simpliﬁed as follows:
1
1
1
1
n Xi∈S
n Xi6∈S
n Xi6∈S
n Xi∈S
(1 − `1
(1 − `1
Putting all this together we have the following lemma:
Lemma 2.1. For a ﬁxed f0 , f1 the problem of choosing the best binary space partitions, g (·) in
Eq. 1 is equivalent to choosing a binary classiﬁer g that optimizes following 0/1 (since wi ∈ {0, 1})
empirical loss function:
i } , where wi = (cid:26) 1, `0
i 6= `1
˜R(g ) =
i
{g(xi ) 6=`0
0, otherwise
The composite classiﬁer F (x) based on the reject and region classiﬁers can be written comp actly as
F (x) = fg(x)(x). We observe several aspects of our proposed scheme:
(1) Binary partitioning is a binary classiﬁcation problem on th e training set, (xi , `0
i ), i =
1, 2, . . . , n.
(2) The 0/1 weight, wi = 1, is non-zero if and only if the classiﬁers disagree on xi , i.e.,
f0 (xi ) 6= f1 (xi ).
(3) The partitioning error is zero on a training example xi with weight wi = 1 if we choose g (xi ) = 0
on examples where f0 (xi ) = yi . In contrast if f0 (xi ) 6= yi the partitioning error can be reduced by
choosing g (xi ) = 1, and thus rejecting the example from consideration by f0 .

1
n Xi6∈S

i } ;
{g(xi ) 6=`0

1
n

n
Xi=1

{g(xi )=0} =

`1
i

{g(xi )=1} =

`1
i

`1
i

{g(xi ) 6=`0
i }

2.2 Surrogate Loss Functions, Algorithms and Convergence

An important implication of Lemma 2.1 is that we can now use powerful learning techniques such
as decision trees, boosting and SVMs for learning space partitioning classiﬁers. Our method is a
coordinate descent scheme which optimizes over a single variable at a time. Each step is an ERM
and so any learning method can be used at each step.
Convergence Issues: It is well known that that indicator losses are hard to minimize, even when
the class of classiﬁers, F , is nicely parameterized. Many schemes are based on minimizing sur-
satisfy 1z≤0 ≤ φ(z ). Many such surrogates can be constructed using sigmoids, exponentials etc.
rogate losses. These surrogate losses are upper bounds for indicator losses and usually attempt
to obtain large margins. Our coordinate descent scheme in this context is equivalent to describ-
ing surrogates for each step and minimizing these surrogates. This means that our scheme may
not converge, let alone converge to a global minima, even when surrogates at each step are nice
and convex. This is because even though each surrogate upper bounds indicator loss functions
at each step, when put together they do not upper bound the global objective of Eq. 1. Conse-
quently, we need a global surrogate to ensure that the solution does converge. Loss functions are
most conveniently thought of in terms of margins. For notational convenience, in this section we
will consider the case where the partition classiﬁer, g , maps to labels ` ∈ {−1, 1}, where a la-
bel of −1 and 1 indicates classiﬁcation by f0 and f1 , respectively. We seek functions φ(z ) that
Consider the classiﬁcation function g (x) = sign (h(x) > 0). The empirical error can be upper
4

bounded: 1
`g(x)=1 = 1
−`h(x)≤0 ≤ φ(−`h(x)) We then form a global surrogate for the empir-
ical loss function. Approximating the indicator functions of the empirical risk/loss in Eq. 1 with
surrogate functions, the global surrogate is given by:
n
n
Xi=1
Xi=1
ˆR(g , f0 , f1 ) =
which is an upper bound on Eq. 1. Optimizing the partitioning function g (·) can be posed as a
supervised learning problem, resulting in the following lemma (see Supplementary for a proof):
Lemma 2.2. For a ﬁxed f0 , f1 the problem of choosing the best binary space partitions, g (·) in
Eq. 3 is equivalent to choosing a binary classiﬁer h that optimizes a surrogate function φ(·):
2n
wiφ (h(xi )ri ) , ri = (cid:26) 1,
otherwise ,wi = (cid:26) φ(f0 (xi )yi ),
i < n + 1
Xi=1
−1,
φ(f1 (xi )yi ),
Theorem 2.3. For any continuous surrogate φ(·, ·), performing alternating minimization on the
classiﬁers f0 , f1 , and g converges to a local minima of Eq. 3, with a loss upper-bounding the
empirical loss de ﬁned by Eq. 1.

φ (−h(xi )) φ (yi f1 (xi )) ,

φ (h(xi )) φ (yif0 (xi )) +

i < n + 1
otherwise .

ˆR(g ) =

1
2n

1
n

1
n

(3)

Proof. This follows directly, as this is coordinate descent on a smooth cost function.

2.3 Multi-Region Partitioning

Lemma 2.1 can be used to also reduce multi-region space partitioning to supervised learning. We
can obtain this reduction in one of several ways. One approach is to use pairwise comparisons,
training classiﬁers to decide between pairs of regions. Unf ortunately, the number of different reject
classiﬁers scales quadratically, so we instead employ a gre edy partitioning scheme using a cascade
classiﬁer.
1
1
1
Fig 1 illustrates a recursively learnt three region space partitioning classiﬁer. In general the regions
1
gk (x), k ∈ {1, 2, . . . , r − 1}, where r is the
are de ﬁned by a cascade of binary reject classiﬁers,
number of classiﬁcation regions. Region classiﬁers,
fk (x), k ∈ {1, 2, . . . , r}, map observations in
the associated region to labels. At stage k , if gk (x) = 0, an observation is classiﬁed by the region
classiﬁer, fk (x), otherwise the observation is passed to the next stage of the cascade. At the last
reject classiﬁer in the cascade, if gr−1 (x) = 1, the observation is passed to the ﬁnal region classiﬁer,
fr (x). This ensures that only r reject classiﬁers have to be trained for r regions.
Now de ﬁne for an arbitrary instance (x, y ) and ﬁxed {gj }, {fj }, the 0/1 loss function at each stage
k ,
Lk (x, y ) = (cid:26)(cid:0)
{gk (x)=0} (cid:1)
{gk (x)=1} (cid:1) Lk+1 (x, y )
{fk (x) 6=y} + (cid:0)
{fk+1 (x) 6=y}
We observe that Lk (x, y ) ∈ {0, 1} and is equal to zero if the example is classiﬁed correctly at
current or future stages and one otherwise. Consequently, the aggregate 0/1 empirical risk/loss is
the average loss over all training points at stage 1, namely,
Ck (xi )1
n
Xi=1
L1(xi , yi )
where, Cj (x) = 1
In the expression above we have made the dependence on reject classiﬁers and region-classiﬁers
explicit. We minimize Eq. 5 over all gj , fj by means of coordinate descent, namely, to optimize gk
we hold fj , ∀j and gj , j 6= k ﬁxed. Based on the expressions derived above the coordinate descent
steps for gk and fk reduces respectively to:
n
1
Xi=1
gk (·) = argmin
n
g∈G
(6)
i=1 {gi (x)=1}} , denotes whether or not an example makes it to the jth stage. The
{Vj−1
optimization problem for fk (·) is exactly the standard 0/1 empirical loss minimization over training

Ck (xi )Lk (xi , yi ), fk (·) = argmin
f ∈F

R (g1 , g2 , . . . , gr−1 , f1 , f2 , . . . , fr ) =

if k < r
if k = r

{fk (xi ) 6=yi } V{gk (xi )=0}

n
Xi=1

1
n

1
n

(5)

,

(4)

5

Algorithm 1 Space Partitioning Classiﬁer
Input: Training data, {(xi , yi )}n
i=1 , number of classiﬁcation regions, r
Output: Composite classiﬁer, F (·)
Initialize: Assign points randomly to r regions
while F not converged do
for j = 1, 2, . . . , r do
Train region classiﬁer fj (x) to optimize 0/1 empirical loss of Eq. (6).
wi1
end for
for k = r − 1, r − 2, . . . , 2, 1 do
Train reject classiﬁer gk (x) to optimize 0/1 empirical loss of Eq. (7).
end for
end while
data that survived upto stage k . On the other hand, the optimization problem for gk is exactly in
the form where Lemma 2.1 applies. Consequently, we can also reduce this problem to a supervised
learning problem:
n
1
Xi=1
gk (·) = argmin
n
g∈G
and wi = (cid:26) 1,
`i = (cid:26)0 if fk (xi ) = yi
`i 6= Lk+1 (xi , yi ), Ck (x) 6= 0
otherwise
0,
1 if fk (xi ) 6= yi
The composite classiﬁer F (x) based on the reject and region classiﬁers can be written comp actly as
follows:

{g(xi ) 6=`i } ,

where

(7)

.

(8)
F (x) = fs (x), s = min{j | gj (x) = 0} ∪ {r}
Observe that if the k th region classiﬁer correctly classiﬁes the example
xi , i.e., fk (xi ) = yi then
this would encourage gk (xi ) = 0. This is because gk (xi ) = 1 would induce an increased cost in
terms of increasing Lk+1 (xi , yi ). Similarly, if the k th region classiﬁer incorrectly classiﬁes, namely,
fk (xi ) 6= yi , the optimization would prefer gk (xi ) = 1. Also note that if the kth region classiﬁer
loss as well as the subsequent stages are incorrect on an example are incorrect then the weight on
that example is zero. This is not surprising since reject/no-reject does not impact the global cost.
We can deal with minimizing indicator losses and resulting convergence issues by deriving a global
surrogate as we did in Sec. 2.2. A pseudo-code for the proposed scheme is described in Algorithm 1.

2.4 Local Linear Classiﬁcation

Linear classiﬁcation is a natural method for learning lo-
cal decision boundaries, with the global decision regions
approximated by piecewise linear functions. In local lin-
ear classiﬁcation, local classiﬁers,
f1 , f2 , . . . , fr , and re-
ject classiﬁers,
g1 , g2 , . . . , gr−1 , are optimized over the
set of linear functions. Local linear rules can effectively
tradeoff bias and variance error. Bias error (empirical er-
ror) can be made arbitrarily small by approximating the
decision boundary by many local linear classiﬁers. Vari-
ance error (classiﬁer complexity) can be made small by
restricting the number of local linear classiﬁers used to
construct the global classiﬁer. This idea is based on the
relatively small VC-dimension of a binary local linear classiﬁer, namely,
Theorem 2.4. The VC-dimension of the class composed (Eq. 8) with r − 1 linear classiﬁers gj and
r linear classiﬁers fj in a d-dimensional space is bounded by 2(2r − 1) log(e(2r − 1))(d + 1).

Figure 2: Local LDA classi ﬁcation regions
for XOR data, the black line is reject classi-
ﬁer boundary.

The VC-dimension of local linear classiﬁers grows linearly with dimension and nearly linearly with
respect to the number of regions. This is seen from Fig. 1. In practice, few regions are necessary to
achieve low training error as highly non-linear decision boundaries can be approximated well locally
with linear boundaries. For example, consider 2-D XOR data. Learning the local linear classiﬁer
with 2 regions using LDA produces a classiﬁer with small empi rical error. In fact our empirical
observation can be translated to a theorem (see Supplementary for details):

6

Theorem 2.5. Consider an idealized XOR, namely, samples are concentrated into four equal clus-
ters at coordinates (−1, 1), (1, 1), (1, −1), (−1, −1) in a 2D space. Then with high probability
(where probability is wrt initial sampling of reject region) a two region composite classiﬁer trained
locally using LDA converges to zero training error.

In general, training linear classiﬁers on the indicator los s is impractical. Optimization on the non-
convex problem is difﬁcult and usually leads to non-unique o ptimal solutions. Although margin
based methods such as SVMs can be used, we primarily use relatively simple schemes such as
LDA, logistic regression, and average voted perceptron in our experiments. We use each of these
schemes for learning both reject and region-classiﬁers. Th ese schemes enjoy signiﬁcant computa-
tional advantages over other schemes.
Computational Costs of LDA, Logistic Regression and Perceptron: Each LDA classiﬁer is
trained in O(nd2 ) computations, where n is the number of training observations and d is the di-
mension of the training data. As a result, the total computation cost per iteration of the local lin-
ear classiﬁer with LDA scales linearly with respect to the nu mber of training samples, requiring
O(nd2 r) computations per iteration, where r is the number of classiﬁcation regions. Similarly, the
computational cost of training a single linear classiﬁer by logistic regression scales O(ncd2 ) for a
ﬁxed number of iterations, with the local linear classiﬁer t
raining time scaling O(rncd2 ) computa-
tions per iteration, where c is the number of classes. A linear variant of the voted perceptron was
implemented by taking the average of the weights generated by the unnormalized voted perceptron
[15]. Training each perceptron for a ﬁxed number of epochs is extremely efﬁcient, requiring only
O(ndc) computations to train. Therefore, training local linear perceptron scales linearly with data
size and dimensions, with O(ndcr) computations, per iteration.

3 Experimental Results

Multiclass Classiﬁcation: Experimental results on six datasets from the UCI repository [16] were
performed using the benchmark training and test splits associated with each data set, as shown in
Table 1. Con ﬁdence intervals are not possible with the resul ts, as the prede ﬁned training and test
splits were used. Although con ﬁdence intervals cannot be co mputed by multiple training/test splits,
test set error bounds [17] show that with test data sets of these sizes, the difference between true error
and empirical error is small with high probability. The six datasets tested were: Isolet (d=617, c= 26,
n=6238, T=1559), Landsat (d=36, c=7, n=4435, T=2000), Letter (d=16, c=26, n=16000, T=4000),
Optdigit (d=64, c=10, n=3823, T=1797), Pendigit (d=16, n=10, n=7494, T=3498), and Shuttle (d=9,
c=7, n=43500, T=14500), where d is the dimensions, c the number of classes, n training data size
and T the number of test samples.

Local linear classiﬁers were trained with LDA, logistic reg ression, and perceptron (mean of weights)
used to learn local surrogates for the rejection and local classiﬁcation problems. The classiﬁers were
initialized with 5 classiﬁcation regions ( r = 5), with the trained classiﬁers often reducing to fewer
classiﬁcation regions due to empty rejection region. Termi nation of the algorithm occurred when the
rejection outputs, gk (x), and classiﬁcation labels, F (x), remained consistent on the training data for
two iterations. Each classiﬁer was randomly initialized 15 times, and the classiﬁer with the minimum
training error was chosen. Results were compared with Mixture Discriminant Analysis (MDA)

g1(x)

g2(x)

g3(x)

g4(x)

g5(x)

Figure 3: Histogram of classes over test data for the Optdigit dataset in different partitions generated by our
approach using the linear voted perceptron .
[9] and classiﬁcation trees trained using the Gini diversit y index (GDI) [3]. These classiﬁcation
algorithms were chosen for comparison as both train global classiﬁers modeled as simple local
classiﬁers, and both are computationally efﬁcient.

7

For comparison to globally complex classiﬁcation techniqu es, previous state of the art boosting re-
sults of Saberian and Vasconcelos [18] and Jhu et al. [19] were listed. Although the multiclass
boosted classiﬁers were terminated early, we consider the c omparison appropriate, as early termi-
nation limits the complexity of the classiﬁers. The improve d performance of local linear learning
of comparable complexity justiﬁes approximating these bou ndaries by piecewise linear functions.
Comparison with kernelized SVM was omitted, as SVM is rarely applied to multiclass learning
on large datasets. Training each binary kernelized classiﬁ er is computationally intensive, and on
weakly learnable data, boosting also allows for modeling of complex boundaries with arbitrarily
small empirical error.

Table 1: Multiclass learning algorithm test errors on six UCI datasets using benchmark training and test sets.
Bold indicates best test error among listed algorithms. One vs All AdaBoostis trained using decision stumps as
weak learners. AdaBoost-SAMME and GD-MCBoost are trained using depth-2 decision trees as weak learners.

Algorithm
One vs All AdaBoost [2]
GDI Tree [3]
MDA [9]
AdaBoost-SAMME [19]
GD-MCBoost [18]
Local Classi ﬁers
LDA
Logistic Regression
Perceptron

Isolet
11.10%
20.59%
35.98%
39.00%
15.72%

5.58%
19.95%
5.71%

Landsat
16.10%
14.45%
36.45%
20.20%
13.35%

13.95%
14.00%
20.15%

Letter
37.37%
14.37%
22.73%
44.35%
40.35%

24.45%
13.08%
20.40%

Optdigit
12.24%
14.58%
9.79%
22.47%
7.68%

5.78%
7.74%
4.23%

Pendigit
11.29%
8.78%
7.75%
16.18%
7.06%

6.60%
4.75%
4.32%

Shuttle
0.11%
0.04%
9.59%
0.30%
0.27%

2.67%
1.19%
0.32%

In 4 of the 6 datasets, local linear classiﬁcation produced t he lowest classiﬁcation error on test
datasets, with optimal test errors within 0.6% of the minimal test error methods for the remaining
two datasets. Also there is evidence that suggests that our scheme partitions multiclass problems
into simpler subproblems. We plotted histogram output of class labels for Optdigit dataset across
different regions using local perceptrons (Fig. 3). The histogram is not uniform across regions,
implying that the reject classiﬁers partition easily disti nguishable classes. We may interpret our
approach as implicitly learning data-dependent codes for multiclass problems. This can contrasted
with many state of the art boosting techniques, such as [18], which attempt to optimize both the
codewords for each class as well as the binary classiﬁcation problems de ﬁning the codewords.

Figure 4: Test error for different values of label noise. Left: Wisconsin Breast Cancer data, Middle: Vertebrae
data, and Right: Wine data.
Robustness to Label Noise: Local linear classiﬁcation trained using LDA, logistic reg ression, and
averaged voted perceptron was tested in the presence of random label noise. A randomly selected
fraction of all training observations were given incorrect labels, and trained as described for the
multiclass experiments. Three datasets were chosen from the UCI repository [16]: Wisconsin Breast
Cancer data, Vertebrae data, and Wine data. A training set of 100 randomly selected observations
was used, with the remainder of the data used as test. For each label noise fraction, 100 randomly
drawn training and test sets were used, and the average test error is shown in Fig. 4.

For comparison, results are shown for classiﬁcation trees t rained according to Gini’s diversity index
(GDI) [3], AdaBoost trained with stumps [2], and support vector machines trained on Gaussian ra-
dial basis function kernels. Local linear classiﬁcation, n otably when trained using LDA, is extremely
robust to label noise. In comparison, boosting and classiﬁc ation trees show sensitivity to label noise,
with the test error increasing at a faster rate than LDA-trained local linear classiﬁcation on both the
Wisconsin Breast Cancer data and Vertebrae data.

Acknowledgments

This research was partially supported by NSF Grant 0932114.

8

References

[1] G. R ¨atsch, T. Onoda, and K.-R. M ¨uller. Soft margins forAdaBoost. Technical Report NC-TR-
1998-021, Department of Computer Science, Royal Holloway, University of London, Egham,
UK, August 1998. Submitted to Machine Learning.
[2] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55(1):119 – 139,
1997.
[3] Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression
Trees. Wadsworth, 1984.
[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-
correcting output codes. Journal of Artiﬁcial Intelligence Research , 2:263 –286, 1995.
[5] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: a
unifying approach for margin classiﬁers.
J. Mach. Learn. Res., 1:113 –141, September 2001.
[6] Koby Crammer and Yoram Singer. On the learnability and design of output codes for multiclass
problems. In In Proceedings of the Thirteenth Annual Conference on Computational Learning
Theory, pages 35 –46, 2000.
[7] Venkatesan Guruswami and Amit Sahai. Multiclass learning, boosting, and error-correcting
codes.
In Proceedings of the twelfth annual conference on Computational learning theory,
COLT ’99, pages 145 –155, New York, NY, USA, 1999. ACM.
[8] Yijun Sun, Sinisa Todorovic, Jian Li, and Dapeng Wu. Unifying the error-correcting and
output-code adaboost within the margin framework. In Proceedings of the 22nd international
conference on Machine learning, ICML ’05, pages 872 –879, New York, NY, USA, 2005.
ACM.
[9] Trevor Hastie and Robert Tibshirani. Discriminant analysis by gaussian mixtures. Journal of
the Royal Statistical Society, Series B, 58:155 –176, 1996.
[10] Tae-Kyun Kim and Josef Kittler. Locally linear discriminant analysis for multimodally dis-
tributed classes for face recognition with a single model image. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 27:318 –327, 2005.
[11] Ofer Dekel and Ohad Shamir. There’s a hole in my data space: Piecewise predictors for
heterogeneous learning problems. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics, volume 15, 2012.
[12] Juan Dai, Shuicheng Yan, Xiaoou Tang, and James T. Kwok. Locally adaptive classiﬁcation pi-
loted by uncertainty. In Proceedings of the 23rd international conference on Machine learning,
ICML ’06, pages 225 –232, New York, NY, USA, 2006. ACM.
[13] Marc Toussaint and Sethu Vijayakumar. Learning discontinuities with products-of-sigmoids
for switching between local models. In Proceedings of the 22nd international conference on
Machine Learning, pages 904 –911. ACM Press, 2005.
[14] Eduardo D. Sontag. Vc dimension of neural networks.
Learning, pages 69 –95. Springer, 1998.
[15] Yoav Freund and Robert E. Schapire. Large margin classiﬁcation using the perceptron algo-
rithm. Machine Learning, 37:277 –296, 1999. 10.1023/A:1007662407062.
[16] A. Frank and A. Asuncion. UCI machine learning repository, 2010.
[17] J. Langford. Tutorial on practical prediction theory for classiﬁcation.
Learning Research, 6(1):273, 2006.
[18] Mohammad J. Saberian and Nuno Vasconcelos. Multiclass boosting: Theory and algorithms.
In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,
Advances in Neural Information Processing Systems 24, pages 2124 –2132. 2011.
[19] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class adaboost, 2009.

In Neural Networks and Machine

Journal of Machine

9


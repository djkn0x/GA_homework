Accelerated Training for Matrix-norm
Regularization: A Boosting Approach

Xinhua Zhang∗, Yaoliang Yu and Dale Schuurmans
Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada
{xinhua2,yaoliang,dale}@cs.ualberta.ca

Abstract

Sparse learning models typically combine a smooth loss with a nonsmooth
penalty, such as trace norm. Although recent developments in sparse approxi-
mation have offered promising solution methods, current approaches either apply
only to matrix-norm constrained problems or provide suboptimal convergence
rates. In this paper, we propose a boosting method for regularized learning that
guarantees  accuracy within O(1/) iterations. Performance is further accelerated
by interlacing boosting with ﬁxed-rank local optimization—exploiting a simpler
local objective than previous work. The proposed method yields state-of-the-art
performance on large-scale problems. We also demonstrate an application to la-
tent multiview learning for which we provide the ﬁrst efﬁcient weak-oracle.

Introduction
1
Our focus in this paper is on unsupervised learning problems such as matrix factorization or latent
subspace identiﬁcation. Automatically uncovering latent factors that reveal important structure in
data is a longstanding goal of machine learning research. Such an analysis not only provides un-
derstanding, it can also facilitate subsequent data storage, retrieval and processing. We focus in
particular on coding or dictionary learning problems, where one seeks to decompose a data matrix
X into an approximate factorization ˆX = U V that minimizes reconstruction error while satisfying
other properties like low rank or sparsity in the factors. Since imposing a bound on the rank or
number of non-zero elements generally makes the problem intractable, such constraints are usually
replaced by carefully designed regularizers that promote low rank or sparse solutions [1–3].
Interestingly, for a variety of dictionary constraints and regularizers, the problem is equivalent to
a matrix-norm regularized problem on the reconstruction matrix ˆX [1, 4]. One intensively studied
example is the trace norm, which corresponds to bounding the Euclidean norm of the code vectors in
U while penalizing V via its (cid:96)21 norm. To solve trace norm regularized problems, variational meth-
√
ods that optimize over U and V only guarantee local optimality, while proximal gradient algorithms
that operate on ˆX [5, 6] can achieve an  accurate (global) solutions in O(1/
) iterations, but these
require singular value thresholding [7] at each iteration, preventing application to large problems.
Recently, remarkable promise has been demonstrated for sparse approximation methods. [8] con-
verts the trace norm problem into an optimization over positive semideﬁnite (PSD) matrices, then
solves the problem via greedy sparse approximation [9, 10]. [11] further generalizes the algorithm
from trace norm to gauge functions [12], dispensing with the PSD conversion. However, these
schemes turn the regularization into a constraint. Despite their theoretical equivalence, many practi-
cal applications require the solution to the regularized problem, e.g. when nested in another problem.
In this paper, we optimize the regularized objective directly by reformulating the problem in the
framework of (cid:96)1 penalized boosting [13, 14], allowing it to be solved with a general procedure de-
veloped in Section 2. Each iteration of this procedure calls an oracle to ﬁnd a weak hypothesis
∗Xinhua Zhang is now at the National ICT Australia (NICTA), Machine Learning Group.

1

(typically a rank-one matrix) yielding the steepest local reduction of the (unregularized) loss. The
associated weight is then determined by accounting for the (cid:96)1 regularization. Our ﬁrst key contri-
bution is to establish that, when the loss is convex and smooth, the procedure ﬁnds an  accurate
solution within O(1/) iterations. To the best of our knowledge, this is the ﬁrst O(1/) objective
value rate that has been rigorously established for (cid:96)1 regularized boosting. [15] considered a similar
boosting approach, but required totally corrective updates. In addition, their rate characterizes the
diminishment of the gradient, and is O(1/2 ) as opposed to O(1/) established here. [9–11, 16–18]
establish similar rates, but only for the constrained version of the problem.
We also show in Section 3 how the empirical performance of (cid:96)1 penalized boosting can be greatly im-
proved by introducing an auxiliary rank-constrained local-optimization within each iteration. Inter-
lacing rank constrained optimization with sparse updates has been shown effective in semi-deﬁnite
programming [19–21]. [22] applied the idea to trace norm optimization by factoring the reconstruc-
tion matrix into two orthonormal matrices and a positive semi-deﬁnite matrix. Unfortunately, this
strategy creates a very difﬁcult constrained optimization problem, compelling [22] to resort to man-
ifold techniques. Instead, we use a simpler variational representation of matrix norms that leads to
a new local objective that is both unconstrained and smooth. This allows the application of much
simpler and much more efﬁcient solvers to greatly accelerate the overall optimization.
Underlying standard sparse approximation methods is an oracle that efﬁciently selects a weak hy-
pothesis (using boosting terminology). Unfortunately these oracle problems are extremely challeng-
ing except in limited cases [3, 11]. Our next major contribution, in Section 4, is to formulate an
efﬁcient oracle for latent multiview factorization models [2, 4], based on a positive semi-deﬁnite
relaxation that we prove incurs no gap.
Finally, we point out that our focus in this paper is on the optimization of convex problems that relax
the “hard” rank constraint. We do not explicitly minimize the rank, which is different from [23].
Notation We use γK to denote the gauge induced by set K; (cid:107)·(cid:107)∗ to denote the dual norm of (cid:107)·(cid:107);
(cid:107)X (cid:107)R,1 denotes the row-wise norm (cid:80)
and (cid:107)·(cid:107)F , (cid:107)·(cid:107)tr and (cid:107)·(cid:107)sp to denote the Frobenius norm, trace norm and spectral norm respectively.
i (cid:107)Xi: (cid:107)R , while (cid:104)X, Y (cid:105) := tr(X (cid:48)Y ) denotes the inner prod-
uct. The notation X (cid:60) 0 will denote positive semi-deﬁnite; X:i and Xi: stands for the i-th column
and i-th row of matrix X ; and diag {ci } denotes a diagonal matrix with the (i, i)-th entry ci .
2 The Boosting Framework with (cid:96)1 Regularization
Consider a coding problem where one is presented an n×m matrix Z , whose columns correspond to
m training examples. Our goal is to learn an n × k dictionary matrix U , consisting of k basis vectors,
and a k × m coefﬁcient matrix V , such that U V approximates Z under some loss L(U V ). We sup-
press the dependence on the data Z throughout the paper. To remove the scaling invariance between
U and V , it is customary to restrict the bases, i.e. columns of U , to the unit ball of some norm (cid:107)·(cid:107)C .
Unfortunately, for a ﬁxed k , this coding problem is known to be computationally tractable only for
the squared loss. To retain tractability for a variety of convex losses, a popular and successful recent
approach has been to avoid any “hard” constraint on the number of bases, i.e. k , and instead impose
regularizers on the matrix V that encourage a low rank or sparse solution.
To be more speciﬁc, the following optimization problem lies at the heart of many sparse learning
models [e.g. 1, 3, 4, 24, 25]:
L(U ˜V ) + λ(cid:107) ˜V (cid:107)R,1 ,
(1)
min
min
U :(cid:107)U:i (cid:107)C ≤1
˜V
where λ ≥ 0 speciﬁes the tradeoff between loss and regularization. The (cid:107)·(cid:107)R norm in the block R-1
norm provides the ﬂexibility of promoting useful structures in the solution, e.g. (cid:96)1 norm for sparse
solutions, (cid:96)2 norm for low rank solutions, and block structured norms for group sparsity. To solve
(1), we ﬁrst reparameterize the rows of ˜V by ˜Vi: = σiVi: , where σi ≥ 0 and (cid:107)Vi: (cid:107)R ≤ 1. Now (1)
(cid:88)
can be reformulated by introducing the reconstruction matrix X := U ˜V :
(cid:107) ˜V (cid:107)R,1 = min
(1) = min
(2)
min
min
L(X ) + λ
L(X ) + λ
σi ,
σ,U,V :σ≥0,U ΣV =X
U, ˜V :(cid:107)U:i (cid:107)C ≤1,U ˜V =X
X
X
i
where Σ = diag{σi }, and U and V in the last minimization also carry norm constraints. (2) is
illuminating in two respects. First it reveals that the regularizer essentially seeks a rank-one decom-
position of the reconstruction matrix X , and penalizes the (cid:96)1 norm of the combination coefﬁcients
as a proxy of the “rank”. Second, the regularizer in (2) is now expressed precisely in the form of the

2

Algorithm 1: The vanilla boosting algorithm.
Require: The weak hypothesis set A in (3).
1: Set X0 = 0, s0 = 0.
2: for k = 1, 2, . . . do
(cid:104)∇L(Xk−1 ), H (cid:105).
3: Hk ← argmin
H∈A
(ak , bk ) ←
4:
L(aXk−1 + bHk ) + λ(ask + b).
argmin
a≥0,b≥0
i ← ak σ (k−1)
i ← A(k−1)
, ∀ i < k
σ (k)
, A(k)
5:
6: Xk ← (cid:80)k
i
i
k ← bk , A(k)
k ← Hk .
σ (k)
sk ← (cid:80)k
i=1 σ (k)
i A(k)
i = akXk−1+ bkHk ,
i=1 σ (k)
i = ak sk−1 + bk .
7: end for

4:

Algorithm 2: Boosting with local search.
Require: A set of weak hypotheses A.
1: Set X0 = 0, U0 = V0 = Λ0 = [ ], s0 = 0.
2: for k = 1, 2, . . . do
(cid:104)∇L(Xk−1 ), uv(cid:48) (cid:105).
(uk , vk ) ← argmin
3:
uv(cid:48)∈A
(ak , bk ) ←
(cid:112)akΛk−1 ,
L(aXk−1+ b uk v(cid:48)
k )+λ(ask+b).
argmin
Vinit ← ((cid:112)akΛk−1 ˆVk−1 ,
a≥0,b≥0
√
Uinit ← ( ˆUk−1
bk uk ),
√
bk vk )(cid:48) .
Locally optimize g(U, V ) with initial
6:
(cid:80)k
value (Uinit , Vinit ). Get a solution (Uk ,Vk ).
7: Xk ← Uk Vk , Λk ← diag{(cid:107)U:i (cid:107)C (cid:107)Vi:(cid:107)R },
sk ← 1
i=1 ((cid:107)U:i (cid:107)2
C + (cid:107)Vi: (cid:107)2
R ).
2
8: end for

5:

gauge function γK induced by the convex hull K of the set1
A = {uv(cid:48) : (cid:107)u(cid:107)C ≤ 1, (cid:107)v(cid:107)R ≤ 1}.
(3)
Since K is convex and symmetric (−K = K), the gauge function γK is in fact a norm, hence the
support function of A deﬁnes the dual norm ||| · ||| (see e.g. [26, Proposition V.3.2.1]):
(cid:107)Λ(cid:48)u(cid:107)∗
(cid:107)Λv(cid:107)∗
|||Λ||| := max
u(cid:48)Λv = max
X ∈A tr(X (cid:48)Λ) =
(4)
max
R = max
C ,
u,v:(cid:107)u(cid:107)C ≤1,(cid:107)v(cid:107)R≤1
u:(cid:107)u(cid:107)C ≤1
v:(cid:107)v(cid:107)R≤1
and the gauge function γK is simply its dual norm ||| · |||∗ . For example, when (cid:107) · (cid:107)R = (cid:107) · (cid:107)C = (cid:107) · (cid:107)2 ,
we have ||| · ||| = (cid:107) · (cid:107)sp , so the regularizer (as the dual norm) becomes (cid:107) · (cid:107)tr . Another special
case of this result was found in [4, Theorem 1], where again (cid:107) · (cid:107)R = (cid:107) · (cid:107)2 but (cid:107) · (cid:107)C is more
complicated than (cid:107) · (cid:107)2 . Note that the original proofs in [1, 4] are somewhat involved. Moreover,
this gauge function framework is ﬂexible enough to subsume a number of structurally regularized
problems [11, 12], and it is certainly possible to devise other (cid:107) · (cid:107)R and (cid:107) · (cid:107)C norms that would
induce interesting matrix norms.
(cid:16) (cid:88)
(cid:17)
(cid:88)
The gauge function framework also allows us to develop an efﬁcient boosting algorithm for (2), by
resorting to the following equivalent problem:
f ({σi , Ai }) := L
{σ∗
f ({σi , Ai }), where
i } := argmin
i , A∗
The optimal solution X ∗ of (2) can be easily recovered as (cid:80)
σi≥0,Ai∈A
i
i
iσ∗
i A∗
i . Note that in the boosting
terminology, A corresponds to the set of weak hypotheses.
2.1 The boosting algorithm
To solve (5) we propose the boosting strategy presented in Algorithm 1. At each iteration, a weak
hypothesis Hk that yields the most rapid local decrease of the loss L is selected. Then Hk is com-
bined with the previous ensemble by tuning its weights to optimize the regularized objective. Note
that in Step 5 all the weak hypotheses selected in the previous steps are scaled by the same value.
As the (cid:96)1 regularizer requires the sum of all the weights, we introduce a variable sk that recursively
updates this sum in Step 6. In addition, Xk is used only in Step 3 and 4, which do not require its
explicit expansion in terms of the elements of A. Therefore this expansion of Xk does not need to
be explicitly maintained and Step 5 is included only for conceptual clarity.

σiAi

+ λ

σi .

(5)

2.2 Rate of convergence
We prove the convergence rate of Algorithm 1, under the standard assumption:
Assumption 1 L is bounded from below and has bounded sub-level sets. The problem (5) admits
at least one minimizer X ∗ . L is differentiable and satisﬁes the following inequality for all η ∈
1Recall that the gauge function γK is deﬁned as γK (X ) := inf {(cid:80)
i σi : (cid:80)
i σiAi = X, Ai ∈ K, σi ≥ 0}.
3

[0, 1] and A, B in the (smallest) convex set that contains both X ∗ and the sub-level set of f (0):
L((1 − η)A + ηB ) ≤ L(A) + η (cid:104)B − A, ∇L(A)(cid:105) + CL η2
. Here CL > 0 is a ﬁnite constant that
depends only on L and X ∗ .
2

Theorem 1 (Rate of convergence) Under Assumption 1, Algorithm 1 ﬁnds an  accurate solution
to (5) in O(1/) steps. More precisely, denoting f ∗ as the minimum of (5), then
i }) − f ∗ ≤ 4CL
f ({σ (k)
, A(k)
i
k + 2

(6)

.

σi .

(7)

min
σi≥0

L

+ λ

σiA(k)
i

The proof is given in Appendix A. Note that the rate is independent of the regularization constant λ.
k+2 ; it should be clear that
In the proof we ﬁx the variable a in Step 4 of Algorithm 1 to be simply 2
setting a by line search will only accelerate the convergence. An even more aggressive scheme is
(cid:32) k(cid:88)
(cid:33)
k(cid:88)
the totally corrective update [15], which in Step 4 ﬁnds the weights for all A(k)
’s selected so far:
i
i=1
i=1
But in this case we will have to explicitly maintain the expansion of Xt in terms of the A(k)
’s.
i
For boosting without regularization, the 1/ rate of convergence is known to be optimal [27]. We
where h is a convex non-decreasing function over [0, ∞). In (5), this replaces (cid:80)
i σi with h((cid:80)
conjecture that 1/ is also a lower bound for regularized boosting.
Extensions Our proof technique allows the regularizer to be generalized to the form h(γK (X )),
i σi ).
By taking h(x) as the indicator h(x) = 0 if x ≤ 1; ∞ otherwise, our rate can be straightforwardly
translated into the constrained setting.
3 Local Optimization with Fixed Rank
In Algorithm 1, Xk is determined by searching in the conic hull of Xk−1 and Hk .2 Suppose there
exists some auxiliary procedure that allows Xk to be further improved somehow to Yk (e.g. by local
greedy search), then the overall optimization can beneﬁt from it. The only challenge, nevertheless,
is how to restore the “context” from Yk , especially the bases Ai and their weights σi .
In particular, suppose we have an auxiliary function g and the following procedure is feasible:
1. Initialization: given an ensemble {σi , Ai}, there exists a S such that g(S ) ≤ f ({σi , Ai }).
2. Local optimization: some (local) optimizer can ﬁnd a T such that g(T ) ≤ g(S ).
3. Recovery: one can recover an ensemble {βi , Bi : βi ≥ 0, Bi ∈ A} such that f ({βi , Bi }) ≤ g(T ).
Then obviously the new ensemble {βi , Bi } improves upon {σi , Ai }. This local search scheme can
local optimization and recover {βi , Bi }. Then replace Step 6 by Xk = (cid:80)
i βiBi and sk = (cid:80)
i }. Perform
be easily embedded into Algorithm 1 as follows. After Step 5, initialize S by {σ (k)
, A(k)
i
i βi .
The rate of convergence will directly carry over. However, the major challenge here is the potentially
expensive step of recovery because little assumption or constraint is made on T .
Fortunately, a careful examination of Algorithm 1 reveals that a complete recovery of {βi , Bi } is not
required. Indeed, only two “sufﬁcient statistics” are needed: Xk and sk , and therefore it sufﬁces to
recover them only. Next we will show how this can be accomplished efﬁciently in (2) . Two simple
propositions will play a key role. Both proofs can be found in Appendix C.
(cid:18)
(cid:19)
Proposition 1 For the gauge γK induced by K, the convex hull of A in (3), we have
(cid:88)
(cid:107)U:i (cid:107)2
C + (cid:107)Vi: (cid:107)2
1
R
2
i

γK (X ) = min
U,V :U V =X

.

(8)

2 This does not mean Xk is a minimizer of L(X ) + λγK (X ) in that cone, because the bases are not
optimized simultaneously. Incidentally, this also shows why working with (5) turns out to be more convenient.

4

Xk =

σi =

σi =

(10)

U V =

.

(12)

and sk =

If (cid:107)·(cid:107)R = (cid:107)·(cid:107)C = (cid:107) · (cid:107)2 , then γK becomes the trace norm (as we saw before), and (cid:80)
i ((cid:107)U:i (cid:107)2
C +
(cid:19)
(cid:18)
F + (cid:107)V (cid:107)2
R ) is simply (cid:107)U (cid:107)2
(cid:107)Vi:(cid:107)2
(cid:88)
F . Then Proposition 1 is a well-known variational form of the trace
norm [28]. This motivates us to choose the auxiliary function as
(cid:107)U:i (cid:107)2
C + (cid:107)Vi:(cid:107)2
λ
(9)
g(U, V ) = L(U V ) +
.
R
2
(cid:18)
(cid:19)
k(cid:88)
k(cid:88)
k(cid:88)
i
Proposition 2 For any U ∈ Rm×k and V ∈ Rk×n, there exist σi ≥ 0, ui ∈ Rm , and vi ∈ Rn such that
C + (cid:107)Vi:(cid:107)2
(cid:107)U:i(cid:107)2
(cid:107)ui (cid:107)C ≤ 1,
(cid:107)vi (cid:107)R ≤ 1,
σiuiv(cid:48)
1
i ,
.
R
2
i=1
i=1
i=1
Now we can specify concrete details for local optimization in the context of matrix norms:
i=1 , set (Uinit , Vinit ) to satisfy g(Uinit , Vinit ) = f ({σi , uiv(cid:48)
1. Initialize: given {σi ≥ 0, uiv(cid:48)
i ∈ A}k
i }):
√
√
√
√
σk vk )(cid:48) .
and
(11)
σ1u1 , . . . ,
σk uk ),
Vinit = (
σ1v1 , . . . ,
Uinit = (
2. Locally optimize g(U, V ) with initialization (Uinit , Vinit ), to obtain a solution (U ∗ , V ∗ ).
3. Recovery: use Proposition 2 to (conceptually) recover {βi , ˆui , ˆvi } from (U ∗ , V ∗ ).
(cid:19)
(cid:18)
The key advantage of this procedure is that Proposition 2 allows Xk and sk to be computed directly
k(cid:88)
k(cid:88)
k(cid:88)
from (U ∗ , V ∗ ), keeping the recovery completely implicit:
i: (cid:107)2
C + (cid:107)V ∗
:i (cid:107)2
(cid:107)U ∗
βi ˆui ˆv(cid:48)
i = U ∗V ∗ ,
1
R
2
i=1
i=1
i=1
In addition, Proposition 2 ensures that locally improving the solution does not incur an increment
in the number of weak hypotheses. Using the same trick, the (Uinit , Vinit ) in (11) for the (k + 1)-th
iteration can also be formulated in terms of (U ∗ , V ∗ ). Different from the local optimization for
trace norm in [21] which naturally works on the original objective, our scheme requires a nontrivial
(variational) reformulation of the objective based on Propositions 1 and 2.
The ﬁnal algorithm is summarized in Algorithm 2, where ˆU and ˆV in Step 5 denote the column-wise
and row-wise normalized versions of U and V , respectively. Compared to the local optimization in
[22], which is hampered by orthogonal and PSD constraints, our (local) objective in (9) is uncon-
strained and smooth for many instances of (cid:107)·(cid:107)C and (cid:107)·(cid:107)R . This is plausible because no other con-
straints (besides the norm constraint), such as orthogonality, are imposed on U and V in Proposition
2. Thus the local optimization we face, albeit non-convex in general, is more amenable to efﬁcient
solvers such as L-BFGS.
Remark Consider if one performs totally corrective update as in (7). Then all of the coefﬁcients
and weak hypotheses from (U ∗ , V ∗ ) have to be considered, which can be computationally expen-
sive. For example, in the case of trace norm, this leads to a full SVD on U ∗V ∗ . Although U ∗ and V ∗
usually have low rank, which can be exploited to ameliorate the complexity, it is clearly preferable
to completely eliminate the recovery step, as in Algorithm 2.
4 Latent Generative Model with Multiple Views
Underlying most boosting algorithms is an oracle that identiﬁes the steepest descent weak hypothesis
(Step 3 of Algorithm 1). Approximate solutions often sufﬁce [8, 9]. When (cid:107)·(cid:107)R and (cid:107)·(cid:107)C are both
Euclidean norms, this oracle can be efﬁciently computed via the leading left and right singular vector
pair. However, for most other interesting cases like low rank tensors, such an oracle is intractable
[29]. In this section we discover that for an important problem of multiview learning, the oracle can
be surprisingly solved in polynomial time, yielding an efﬁcient computational strategy.
Multiview learning analyzes multi-modal data, such as heterogeneous descriptions of text, image and
video, by exploiting the implicit conditional independence structure. In this case, beyond a single
dictionary U and coefﬁcient matrix V that model a single view Z (1) , multiple dictionaries U (k) are
needed to reconstruct multiple views Z (k) , while keeping the latent representation V shared across
k(cid:88)
all views. Formally the problem in multiview factorization is to optimize [2, 4]:
Lt (U (t)V ) + λ (cid:107)V (cid:107)R,1 .
t=1

min
U (k) :(cid:107)U (k)
:i (cid:107)C ≤1

min
U (1) :(cid:107)U (1)
:i (cid:107)C ≤1

. . .

min
V

5

(13)

= L(U V )+

Λ(cid:48)
tut

.

(14)

g(U, V ) = L(U V )+

max
(cid:107)u(cid:107)C ≤1,(cid:107)v(cid:107)≤1

(15)
(cid:33)
(16)

(cid:107)Λ(cid:48)u(cid:107)2 = max
u:∀t,(cid:107)ut (cid:107)≤1

We can easily re-express the problem as an equivalent “single” view formulation (1) by stacking all
{U (t)} into the rows of a big matrix U , with a new column norm (cid:107)U:i (cid:107)C := max
(cid:107)U (t)
:i (cid:107)C . Then
t=1...k
the constraints on U (t) in (13) can be equivalently written as (cid:107)U:i (cid:107)C ≤ 1, and Algorithm 2 can be
(cid:19)2
(cid:18)(cid:18)
(cid:19)
(cid:18)
(cid:19)
(cid:88)
(cid:88)
directly applied with two specializations. First the auxiliary function g(U, V ) in (9) becomes
C + (cid:107)Vi:(cid:107)2
:i (cid:107)2
(cid:107)U (t)
+ (cid:107)Vi:(cid:107)2
:i (cid:107)C
(cid:107)U (t)
λ
λ
max
max
R
R
2
2
t=1...k
t=1...k
i
i
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) (cid:88)
which can be locally optimized. The only challenge left is the oracle problem in (4), which takes the
following form when all norms are Euclidean:
u(cid:48)Λv = max
(cid:107)u(cid:107)C ≤1
t
[4, 24] considered the case where k = 2 and showed that exact solutions to (14) can be found efﬁ-
ciently. But their derivation does not seem to extend to k > 2. Fortunately there is still an interesting
and tractable scenario. Consider multilabel classiﬁcation with a small number of classes, and U (1)
and U (2) are two views of features (e.g. image and text). Then each class label corresponds to a
view and the corresponding ut is univariate. Since there must be an optimal solution on the extreme
points of the feasible region, we can enumerate {−1, 1} for ut (t ≥ 3) and for each assignment solve
a subproblem of the following form that instantiates (14) (c is a constant vector)
(cid:107)Λ(cid:48)
2u2 + c(cid:107)2 , s.t. (cid:107)u1 (cid:107) ≤ 1, (cid:107)u2(cid:107) ≤ 1.
1u1 + Λ(cid:48)
(QP ) max
u1 ,u2
Due to inhomogeneity, the technique in [4] is not applicable. Rewrite (15) in matrix form
(cid:32) r
(cid:33)
(cid:32) 0
(cid:33)
(cid:32) −1
(cid:33)
(cid:32) −1
(cid:104)M1 , zz(cid:48) (cid:105) ≤ 0
(cid:104)M0 , zz(cid:48) (cid:105)
(cid:104)M2 , zz(cid:48) (cid:105) ≤ 0
(cid:104)I00 , zz(cid:48) (cid:105) = 1,
s.t.
(QP ) min
z
c(cid:48)Λ(cid:48)
c(cid:48)Λ(cid:48)
, M0 = −
1 Λ1Λ(cid:48)
Λ1c Λ1Λ(cid:48)
1
2
where z=
, M1 =
, M2 =
,
u1
I
0
1 Λ2Λ(cid:48)
Λ2c Λ2Λ(cid:48)
2
u2
I
0
2
and I00 is a zero matrix with only the (1, 1)-th entry being 1. Let X = zz(cid:48) , a semi-deﬁnite program-
ming relaxation for (QP ) can be obtained by dropping the rank-one constraint:
(cid:104)M0 , X (cid:105) ,
(cid:104)M1 , X (cid:105) ≤ 0,
(cid:104)M2 , X (cid:105) ≤ 0,
(cid:104)I00 , X (cid:105) = 1, X (cid:23) 0.
(SP ) min
s.t.
X
Its dual problem, which is also the Lagrange dual of (QP ), can be written as
y2 ≥ 0.
s.t. Z := M0 − y0 I00 + y1M1 + y2M2 (cid:23) 0,
y1 ≥ 0,
max
(SD)
y0 ,
y0 ,y1 ,y2
(SD) is a convex problem that can be solved efﬁciently by, e.g., cutting plane methods. (SP ) is
also a convex semideﬁnite program (SDP) amenable for standard SDP solvers. However further
recovering the solution to (QP ) is not straightforward, because there may be a gap between the
optimal values of (SP ) and (QP ). The gap is zero (i.e. strong duality between (QP ) and (SD))
only if the rank-one constraint that (SP ) dropped from (QP ) is automatically satisﬁed, i.e. if (SP )
has a rank-one optimal solution.
Fortunately, as one of our main results, we prove that strong duality always holds for the particular
problem originating from (15). Our proof utilizes some recent development in optimization [30],
and is relegated to Appendix D.
5 Experimental Results
We compared our Algorithm 2 with three state-of-the-art solvers for trace norm regularized objec-
tives: MMBS3 [22], DHM [15], and JS [8]. JS was proposed for solving the constrained problem:
minX L(X ) s.t. (cid:107)X (cid:107)tr ≤ ζ , which makes it hard to compare with solvers for the penalized prob-
lem: minX L(X ) + λ (cid:107)X (cid:107)tr . As a workaround, we ﬁrst chose a λ, and found the optimal solution
X ∗ for the penalized problem. Then we set ζ = (cid:107)X ∗ (cid:107)tr and ﬁnally solved the constrained problem
by JS. In this case, it is only fair to compare how fast L(X ) (loss) is decreased by various solvers,
rather than L(X ) + λ (cid:107)X (cid:107)∗ (objective). DHM is sensitive to the estimate of the Lipschitz constant
of the gradient of L, which we manually tuned for a small value such that DHM still converges.
Since the code for MMBS is specialized to matrix completion, it was used only in this comparison.
Traditional solvers such as proximal methods [6] were not included because they are much slower.
3 http://www.montefiore.ulg.ac.be/˜mishra/softwares/traceNorm.html

(17)

(18)

6

(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(b) Test NMAE vs time (semilogx)
Figure 1: MovieLens100k.

(b) Test NMAE vs time (semilogx)
Figure 2: MovieLens1M.

(b) Test NMAE vs time (semilogx)
Figure 3: MovieLens10M.

Comparison 1: Matrix completion We ﬁrst compared all methods on a matrix completion prob-
lem, using the standard datasets MovieLens100k, MovieLens1M, and MovieLens10M [6, 8, 21],
which are sized 943 × 1682, 6040 × 3706, and 69878 × 10677 respectively (#user × #movie). They
contain 105 , 106 and 107 movie ratings valued from 1 to 5, and the task is to predict the rating for
a user on a movie. The training set was constructed by randomly selecting 50% ratings for each
user, and the prediction is made on the rest 50% ratings. In Figure 1 to 3, we show how fast various
algorithms drive down the training objective, training loss L (squared Euclidean distance), and the
normalized mean absolute error (NMAE) on the test data [see, e.g., 6, 8]. We tuned the λ to optimize
the test NMAE.
From Figure 1(a), 2(a), 3(a), it is clear that it takes much less amount of CPU time for our method to
reduce the objective value (solid line) and the loss L (dashed line). This implies that local search and
partially corrective updates in our method are very effective. Not surprisingly MMBS is the closest
to ours in terms of performance because it also adopts local optimization. However it is still slower
because their local search is conducted on a constrained manifold. In contrast, our local search
objective is entirely unconstrained and smooth, which we manage to solve efﬁciently by L-BFGS.4
JS, though applied indirectly, is faster than DHM in reducing the loss. We observed that DHM kept
running coordinate descent with a constant step size, while the totally corrective update was rarely
taken. We tried accelerating it by using a smaller value of the estimate of the Lipschitz constant of
the gradient of L, but it leads to divergence after a rapid decrease of the objective for the ﬁrst few
iterations. A hybrid approach might be useful.
We also studied the evolution of the NMAE performance on the test data. For this we compared the
matrix reconstruction after each iteration against the ground truth. As plotted in Figure 1(b), 2(b),
3(b), our approach achieves comparable (or better) NMAE in much less time than all other methods.
Comparison 2: multitask and multiclass learning Secondly, we tested on a multiclass classiﬁ-
cation problem with synthetic dataset. Following [15], we generated a dataset of D = 250 features
and C = 100 classes. Each class c has 10 training examples and 10 test examples drawn inde-
pendently and identically from a class-speciﬁc multivariate Gaussian N (µc , Σc ). µc ∈ R250 has
the last 200 coordinates being 0, and the top 50 coordinates were chosen uniformly random from
{−1, 1}. The (i, j )-th element of Σc is 22 (0.5)|i−j | . The task is to predict the class membership of
a given example. We used the logistic loss for a model matrix W ∈ RD×C . In particular, for each
4 http://www.cs.ubc.ca/˜pcarbo/lbfgsb-for-matlab.html

7

10−2100102105106MovieLens−100k, λ = 20Running time (seconds)Objective and loss (training)  Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss10−21001020.20.30.40.50.60.70.80.9MovieLens−100k, λ = 20Running time (seconds)Test error  OursMMBSDHMJS100102104106107MovieLens−1m, λ = 50Running time (seconds)Objective and loss (training)  Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss1001021040.20.30.40.50.60.70.80.9MovieLens−1m, λ = 50Running time (seconds)Test error  OursMMBSDHMJS100102104106106107108109MovieLens−10m, λ = 50Running time (seconds)Objective and loss (training)  Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss1001021041060.10.20.30.40.50.60.70.80.9MovieLens−10m, λ = 50Running time (seconds)Test error  OursMMBSDHMJS(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(b) Test error vs time (semilogx)
Figure 5: Multitask learning for
school dataset.

training example xi with label
yi ∈ {1, .., C }, we deﬁned an
individual loss Li (W ) as
Li (W ) = − log p(yi |xi ; W ),
where for any class c,
(cid:88)
p(c|xi ;W ) = Z −1
i exp(W (cid:48)
:cxi ),
exp(W (cid:48)
Zi =
:cxi ).
c
Then L(W ) is deﬁned as the
average of Li (W ) over the
whole training set. We found
that λ = 0.01 yielded the
lowest test classiﬁcation er-
ror; the corresponding results
are given in Figure 4. Clearly,
the intermediate models out-
put by our scheme achieve
comparable (or better) train-
ing objective and test error in
(b) Test error vs time (semilogx)
orders of magnitude less time
Figure 4: Multiclass classiﬁca-
than those generated by DHM
tion with synthetic datset.
and JS.
We also applied the solvers to a multitask learning problem with
the school dataset [25]. The task is to predict the score of
15362 students from 139 secondary schools based on a number
of school-speciﬁc and student-speciﬁc attributes. Each school is
considered as a task for which a predictor is learned. We used the
ﬁrst random split of training and testing data provided by [25] 5 ,
and set λ so as to achieve the lowest test squared error. Again,
as shown in Figure 5 our approach is much faster than DHM and
JS in ﬁnding the optimal solution for training objective and test
error. As the problem requires a large λ, the trace norm penalty
Figure 6: Multiview training.
is small, making the loss close to the objective.
Comparison 3: Multiview learning Finally we perform an initial test on our global optimization
technique for learning latent models with multiple views. We used the Flickr dataset from NUS-
WIDE [31]. Its ﬁrst view is a 634 dimensional low-level feature, and the second view consists of
1000 dimensional tags. The class labels correspond to the type of animals and we randomly chose 5
types with 20 examples in each type. The task is to train the model in (13) with λ = 10−3 . We used
squared loss for the ﬁrst view, and logistic loss for the other views.
We compared our method with a local optimization approach to solving (13). The local method ﬁrst
ﬁxes all U (t) and minimizes V , which is a convex problem that can be solved by FISTA [32]. Then
it ﬁxes V and optimizes U (t) , which is again convex. We let Alt refer to the scheme that alternates
these updates to convergence. From Figure 6 it is clear that Alt is trapped by a locally optimal
solution, which is inferior to a globally optimal solution that our method is guaranteed to ﬁnd. Our
method also reduces both the objective and the loss slightly faster than Alt.
6 Conclusion and Outlook
We have proposed a new boosting algorithm for a wide range of matrix norm regularized problems.
It is closely related to generalized conditional gradient method [33]. We established the O(1/)
convergence rate, and showed its empirical advantage over state-of-the-art solvers on large scale
problems. We also applied the method to a novel problem, latent multiview learning, for which we
designed a new efﬁcient oracle. We plan to study randomized boosting with (cid:96)1 regularization [34] ,
and to extend the framework to more general nonlinear regularization [3].

5http://ttic.uchicago.edu/˜argyriou/code/mtl_feat/school_splits.tar

8

10010210−1100101102Synthetic multiclass, λ = 0.01Running time (seconds)Objective and loss (training)  Ours−objOurs−lossDHM−objDHM−lossJS−loss1001020.650.70.750.80.850.90.95Multiclass, λ = 0.01Running time (seconds)Test error  OursDHMJS100102101102103104School multitask, λ = 0.1Running time (seconds)Objective and loss (training)  Ours−objOurs−lossDHM−objDHM−lossJS−loss10010202004006008001000School Multitask, λ = 0.1Running time (seconds)Test regression error  OursDHMJS102103100101102Multiview Flickr, λ=0.001Running time (seconds)Objective and loss (training)  Ours−objOurs−lossAlt−objAlt−lossReferences
[1] F. Bach, J. Mairal, and J. Ponce. Convex sparse matrix factorizations. arXiv:0812.1869v1, 2008.
[2] H. Lee, R. Raina, A. Teichman, and A. Ng. Exponential family sparse coding with application to self-
taught learning. In IJCAI, 2009.
[3] D. Bradley and J. Bagnell. Convex coding. In UAI, 2009.
[4] X. Zhang, Y-L Yu, M. White, R. Huang, and D. Schuurmans. Convex sparse coding, subspace learning,
and semi-supervised extensions. In AAAI, 2011.
[5] T. K. Pong, P. Tseng, S. Ji, and J. Ye. Trace norm regularization: Reformulations, algorithms, and multi-
task learning. SIAM Journal on Optimization, 20(6):3465–3489, 2010.
[6] K-C Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least
squares problems. Paciﬁc Journal of Optimization, 6:615–640, 2010.
[7] J-F Cai, E. J. Cand ´es, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2010.
[8] M. Jaggi and M. Sulovsky. A simple algorithm for nuclear norm regularized problems. In ICML, 2010.
[9] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In LATIN, 2008.
[10] K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. In SODA, 2008.
[11] A. Tewari, P. Ravikumar, and I. S. Dhillon. Greedy algorithms for structurally constrained high dimen-
sional problems. In NIPS, 2011.
[12] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse
problems. Foundations of Computational Mathematics, 12(6):805–849, 2012.
[13] Y. Bengio, N.L. Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural networks. In NIPS, 2005.
[14] L. Mason, J. Baxter, P. L. Bartlett, and M. Frean. Functional gradient techniques for combining hypothe-
ses. In Advances in Large Margin Classiﬁers, pages 221–246, Cambridge, MA, 2000. MIT Press.
[15] M. Dudik, Z. Harchaoui, and J. Malick. Lifted coordinate descent for learning with trace-norm regular-
izations. In AISTATS, 2012.
[16] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity in optimization problems with
sparsity constraints. SIAM Journal on Optimization, 20:2807–2832, 2010.
[17] X. Yuan and S. Yan. Forward basis selection for sparse approximation over dictionary. In AISTATS, 2012.
IEEE Trans.
[18] T. Zhang. Sequential greedy approximation for certain convex optimization problems.
Information Theory, 49(3):682–691, 2003.
[19] S. Burer and R. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Math-
ematical Programming, 103(3):427–444, 2005.
[20] M. Journee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive
semideﬁnite matrices. SIAM Journal on Optimization, 20:2327C–2351, 2010.
[21] S. Laue. A hybrid algorithm for convex semideﬁnite optimization. In ICML, 2012.
[22] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre. Low-rank optimization with trace norm penalty. Tech-
nical report, 2011. http://arxiv.org/abs/1112.2318.
[23] S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization with a low-rank con-
straint. In ICML, 2011.
[24] M. White, Y. Yu, X. Zhang, and D. Schuurmans. Convex multi-view subspace learning. In NIPS, 2012.
[25] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):
243–272, 2008.
[26] J-B Hiriart-Urruty and C. Lemar ´echal. Convex Analysis and Minimization Algorithms, I and II, volume
305 and 306. Springer-Verlag, 1993.
[27] I. Mukherjee, C. Rudin, and R. Schapire. The rate of convergence of Adaboost. In COLT, 2011.
[28] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In NIPS, 2005.
[29] C. Hillar and L-H Lim. Most tensor problems are NP-hard. arXiv:0911.1393v3, 2012.
[30] W. Ai and S. Zhang. Strong duality for the CDT subproblem: A necessary and sufﬁcient condition. SIAM
Journal on Optimization, 19:1735–1756, 2009.
[31] T.S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.T. Zhang. A real-world web image database from
national university of singapore. In International Conference on Image and Video Retrieval, 2009.
[32] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
[33] K. Bredies, D. Lorenz, and P. Maass. A generalized conditional gradient method and its connection to an
iterative shrinkage method. Computational Optimization and Applications, 42:173–193, 2009.
[34] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341–362, 2012.

9


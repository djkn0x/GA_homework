Structure and Trends in a Selection of Academic Literature

CS 229, Autumn 2012

Hannah Hironaka
hannahh@stanford.edu

Rahul Suri
rahul.suri@stanford.edu

ABSTRACT
We investigate the use of unsupervised machine learning
methods for discovery of latent structure in unstructured,
unlabeled text data. We brieﬂy present background mate-
rial on two such methods, k-means clustering and Latent
Dirichlet Allocation for topic modeling. We then describe
a dataset of 677 text documents that we assembled and re-
sults from evaluating the described methods on this dataset.
Lastly, we conclude with some general observations.

1.
INTRODUCTION
Supervised learning is a primary focus in much of machine
learning; in the context of text document input, supervised
methods have been brought to bear on a wide variety of
tasks including sentiment analysis, spam classiﬁcation, and
authorship attribution.

However, it is not always the case that categorization of
input documents into predetermined buckets is truly the
task of interest. Indeed, the unsupervised setting is often a
more appropriate representation for problems whose input
consists of unstructured text documents. Here, the goal is
instead to cluster together similar documents in the hope
that the grouping of documents reveals some latent structure
underlying the data.

Moreover, unstructured, unlabeled text data are ubiquitous;
books, magazines, and newspapers are traditional sources.
The explosion of data in recent years has made many more
sources readily available: webpages, blog posts, emails, tweets,
transcripts of audio recordings, etc. While each of these
sources could be shoehorned into a supervised learning prob-
lem in one way or another, a direct reckoning of the data
with unsupervised methods can be equally satisfying, and
the resultant insights can be used to summarize the data or
to inform further exploration of it. To the extent that the
volume of data now available to us is overwhelming, such
automated methods for summarizing large text corpora are
invaluable.

K-means is a fairly straightforward algorithm for clustering
points in a vector space that can be applied to text docu-
ments. Latent Dirichlet Allocation (LDA) [5] is a Bayesian
hierarchical model for discovering topic models from collec-
tions of text documents, with numerous extensions of the
standard LDA framework appearing over the past decade.
In the next sections, we brieﬂy discuss k-means and LDA.

2. ALGORITHMS
2.1 K-means Clustering
K-means clustering groups a set of input vectors into k clus-
ters based on similarity.
It starts by initializing k cluster
mean vectors with random values; at each iteration, some
distance measure is used to group each input vector with the
cluster whose mean it is closest to. Cluster mean vectors are
then recalculated based on their newly-assigned vectors, and
the algorithm loops until convergence.

In our analysis we made several decisions about the con-
struction of the algorithm and its inputs. Each input vec-
tor represented one document. The vectors are indexed by
words in the vocabulary, with the index’s corresponding
value representing the frequency of that word in the doc-
ument. We initialized the cluster means with random input
vectors. We chose cosine distance as a similarity measure be-
cause the input document vectors were not normalized; this
addresses the scenario where two input vectors have simi-
lar directions (i.e. similar relative frequencies of words) but
not magnitude (i.e. diﬀerent document lengths). Finally,
we adapted the algorithm to handle the case of zero vectors
grouped with a mean after an iteration:
in this event, the
algorithm reuses the mean from the previous iteration.

2.2 Latent Dirichlet Allocation
Intuition. LDA assumes that the data exhibit underly-
ing topics. Each topic is a multinomial1 distribution over
the vocabulary; the term “topic” reﬂects the fact that the
estimated multinomials tend to place probability mass on
words with thematic coherence (atom, mass, and particle
for a topic such as “physics,” e.g.).

Given these topics, the words in a single document are as-
sumed to be drawn from a mixture over the topics. This
gives a generative view of the set of all documents, with
each document having its own mixing proportions.

Notation. Let βi for i = 1, . . . , K be a set of K topics.
Let θd for d = 1, . . . , D be a set of D documents. Let wd,n
denote the nth word in the dth document, and let zd,n = k be
an indicator that wd,n was generated by the kth topic. For
convenience, assume that each document contains N words.

1Technically, all “multinomial” distributions referred to in
this document are better described as “categorical” distribu-
tions. However, we follow the relevant literature’s standard
convention of calling them multinomials.

Lastly, let α and η be Dirichlet-distributed priors for θd and
βk , respectively.

The results of these preprocessing steps produced document-
term matrices, which were then used as input to our analy-
ses.

Joint PDF. The joint density of the variables under the
model is given by
(cid:16)(cid:81)N
i=1 p(βi |η) (cid:81)D
(cid:81)K
p(β1:K , θ1:D , z1:D,1:N , w1:D,1:N |α, η) =
n=1 p(zd,n |θd )p(wd,n |β1:K , zd,n )
d=1 p(θd |α)
Posterior. The posterior of the parameters given the data
is

p(β1:K , θ1:D , z1:D,1:N |w1:D,1:N , α, η) =
p(β1:K ,θ1:D ,z1:D,1:N ,w1:D,1:N |α,η)
,
p(w1:D,1:N |α,η)
where the numerator is the joint PDF and the denominator
is the probability of the evidence.

The two main approaches for numerically approximating the
posterior are Gibbs sampling [7, 12] and variational methods
[10, 14].

Dynamic Topic Modeling and Extensions to LDA.
The LDA hierarchical model as presented here has often
been used as a building block for constructing more sophis-
ticated approaches by relaxing its modeling assumptions [2,
8, 15, 13].
In particular, dynamic topic models [3] take
inputs which include discrete-valued temporal data; intu-
itively, a topic model is learned for each timestep, and the
topics themselves evolve from one timestep to the next as a
random walk. Speciﬁcally, if we let βt,i denote the ith topic
at time t, then βt,i |βt−1,i ∼ N (βt−1,i , σ2 I ). Dynamic topic
models thus allow us to track the evolution of topics over
time.

3. DATA AND METHODOLOGY
3.1 Data
The text corpus for this pro ject was downloaded from the CS
229 website containing students’ ﬁnal pro ject reports [6]. It
contains 677 documents from 2005 to 2011. After removing
stopwords, numbers, and punctuation, the corpus contains
occurrences of 47121 unique tokens. The median number of
(non-unique) word occurrences per document is 1058, with
an interquartile range of 515.

3.2 Methodology
To perform topic modeling analyses on the corpus described
above, we used open-source topic modeling software [9] to
stem the tokens and to shrink the vocabulary size from
47121 to 13562 by retaining only the highest-scoring tokens,
as measured by their term frequency-inverse document fre-
quency (tf.idf ) scores [11]. The size of the shrunken vocabu-
lary was selected via informal experimentation; the selected
settings are those which appeared to reveal the most inter-
esting patterns and which made execution of dynamic topic
modeling manageable.

For k-means clustering, we used the open-source Natural
Language Toolkit (NLTK) [1] for preprocessing: we stan-
dardized to lowercase; removed punctuation, non-alphanumeric
characters, and stopwords; and lemmatized the remaining
words. The resulting vocabulary size was 37059.

(cid:17)

.

4. RESULTS
4.1 K-means
After informal experimentation, we chose k = 9 as the pa-
rameter for the k-means algorithm; the clusters it produced
seemed most coherent, and no clusters were empty. We ran
k-means using each year as a separate dataset, in addition
to running it once with the entire corpus as its input.

Table 1 displays results from the single-dataset analysis. De-
spite the small sample of titles from each cluster and the
minimal detail about each one’s full content, several pat-
terns are evident. For example, cluster 1 groups papers
related to game play, and cluster 4 groups ob ject manip-
ulation papers. Some clusters, such as the third one, remain
incoherent, though.

Table 2 shows a few results from clustering within an indi-
vidual year; several characteristics of the evolution of pro ject
topics are evident in the output. Ob ject manipulation and
robotics were common topics throughout all years, but were
more prevalent in earlier years, as was image processing and
detection. A cohesive computational biology cluster appears
in 2010. News recommendation and Twitter sentiment anal-
ysis ﬁrst appear in 2010 and 2011.

Both variations of k-means clustering suﬀer from several
limitations. The small size of the corpus attenuated the
“closeness” of the clusters around their means. This was
most evident for 2005, 2006, and 2007 which had 69, 68,
and 66 pro ject papers, respectively – the smallest num-
bers. Clusters from these years were often incoherent or
contained many outliers. Furthermore, running k-means
multiple times produced noticeably diﬀerent clusters, indi-
cating that this dataset was particularly likely to cause the
algorithm to converge to local optima.

4.2 LDA
The results shown below are for a model with 15 topics. As
on a rule of thumb suggesting roughly (cid:112)N/2 clusters for a
with choosing the exact size of the vocabulary, the number
of topics was chosen based on informal experimentation (and
dataset with N points).

Table 3 shows results produced by LDA. Each topic is illus-
trated by a list of the ten most-probable words from that
topic. We observe that topic 2 has mostly grouped terms
relating to natural language processing together, topic 10
has grouped together terms about news recommendation
pro jects, and topic 15 has grouped terms concerning pro jects
about image processing. Similar conclusions can be stated
for the remaining topics; audio processing, image process-
ing, game play, robotics, bioinformatics, and stock market
prediction emerge as some of the ma jor underlying themes
of CS 229 class pro jects. So, LDA has provided a way for
automatically discovering latent thematic structure, group-
ing words from relevant sub jects using only the unstructured
source documents.

Cluster 1
Beat CAL: Machine Learning in Football Play-calling
MLB Prediction and Analysis
A Machine Learning Approach to Opponent Modeling in
General Game Playing
Cluster 2
Applying Synthetic Images to Learning Grasping
Orientation from Single Monocular Images
Image Processing for Bubble Detection in Microﬂuidics
Value Iteration and DDP for an Inverted Pendulum
Cluster 3
RoboChef: Automatic Recipe Generation “byte-sized
recipes”
Person Following On STAIR
Travel Time Estimation Using Floating Car Data
Cluster 4
STAIR Subcomponent: Learning to Manipulate Ob jects
from Simulated Images
Door Handle Detection for the Stanford AI Robot
(STAIR)
Learning To Pick Up a Novel Ob ject
Cluster 7
Supervised Learning - Stock Trend Classiﬁer
Finding Optimal Hardware Conﬁgurations For C Code
Statistical Analysis and Application of Ensemble
Method on the Netﬂix Challenge
Cluster 8
Clustering WordNet Senses Utilizing Modiﬁed and Novel
Similarity Metrics
Classiﬁcation of Amazon Reviews
Predicting Dow Jones Movement with Twitter
Cluster 9
CRF Based Point Cloud Segmentation
Clustering Autism Cases on Social Functioning
Whos in Charge Here? : Using Clustering Algorithms
to Infer Association of Putative Regulatory
Elements and Genes

Table 1: Representative documents for a few clus-
ters, for a 9-cluster model.

4.3 Dynamic Topic Modeling
Figures 1 through 3 depict results from running dynamic
topic modeling on our dataset. Each ﬁgure corresponds to
a single topic from a 15-topic dynamic model. Lines are
shown for the ﬁve tokens whose probability of appearing in
that topic displayed the highest variance across timesteps;
the lines plot each token’s probability of occurring in that
topic against time.

For topic 3 (Figure 1), the 10 tokens which appear with high-
est probability (marginalizing across timesteps) are user,
cluster, movi, articl, recommend, feed, rmse, read, stori, and
kmean. This information can be used to inform directed ex-
ploration of the original dataset to conclude that this topic
has grouped together terms relating to recommender sys-
tem pro jects. From the ﬁgure, we see signs of the dimin-
ishing popularity of Netﬂix-inspired movie recommendation
pro jects (which were scored using rmse and often included
some form of clustering, typically kmeans). Moreover, we see
an increase in tokens related to news article recommendation

Figure 1: Word probabilities in topic 3 across time.

pro jects starting in 2009, in which students used data from
Pulse to suggest news stories to users in their news feeds.

For topic 4 (Figure 2), the 10 tokens which appear with
highest probability are stock, price, day, market, trade, tweet,
portfolio, forecast, twitter, and network; the thematic content
of this topic is rather obvious. Encouragingly, the ﬁgure
clearly depicts the sharp increase in mentions of tweets and
twitter beginning in 2009, as many pro jects have used Twit-
ter data as a springboard for stock prediction. Moreover, the
ﬁgure also reﬂects an increase in terms like stock and price.
If we were looking at these graphs to try to understand la-
tent structure in the dataset without any prior knowledge,
Figure 2 would have alerted us to one of the ma jor trending
themes in an entirely automated manner.

Lastly, we consider topic 9 (Figure 3), whose top 10 tokens
are layer, imag, network, video, deep, frame, ﬁg, roi, templat,
and reconstruct. It appears that this topic groups together
terms related to pro jects on neural networks. The ﬁgure
seems to indicate an upward trend in use of neural networks,
which is consonant with the recent increase in popularity of
deep learning with neural networks for image analysis.

5. DISCUSSION
One of the unexpected challenges that arose with these anal-
yses was that of simple data manipulation; scraping text
from PDF documents often introduced strange encoding er-
rors that negatively impacted results and took time to hunt
down.

Additionally, identical tokens sometimes appeared in dis-
tinct contexts: light appeared both in the context of traﬃc
light control policies and also in image processing; k-means
clustering was particularly susceptible to conﬂating the two
meanings. Moreover, the stemming we used to preprocess
words at times caused otherwise distinct tokens to be con-

0.000.050.100.150.000.050.100.150.000.050.100.150.000.050.100.150.000.050.100.15clustermoviuserfeedstori2005200620072008200920102011Figure 2: Word probabilities in topic 4 across time.

Figure 3: Word probabilities in topic 9 across time.

ﬂated – movie and moving were both stemmed to movi even
though the former concerned ﬁlm recommendations while
the latter concerned video analysis.

Lastly, we note that most algorithmic parameters (k for k-
means, tf.idf cutoﬀs) were chosen via informal experimen-
tation. While more rigorous approaches to selecting these
values (cross validation, e.g.) could have been used, we de-
cided to focus our eﬀorts elsewhere, as we suspected that the
marginal gains to be achieved from optimizing these values
would be relatively modest, particularly when the ultimate
evaluation of the results was largely qualitative.

6. CONCLUSIONS
We have presented k-means clustering, latent Dirichlet al-
location, and dynamic topic modeling, which are three ap-
proaches to unsupervised learning from unlabeled data. We
applied these methods to a novel collection of text docu-
ments and showed how the automatic discovery of latent
structure from free text can be used to highlight interesting
patterns and trends in the data.

7. REFERENCES
[1] S. Bird. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive
presentation sessions, COLING-ACL ’06, pages 69–72,
Stroudsburg, PA, USA, 2006. Association for
Computational Linguistics.
[2] D. M. Blei. Introduction to probabilistic topic models.
Communications of the ACM, 2011.
[3] D. M. Blei and J. D. Laﬀerty. Dynamic topic models.
In Proceedings of the 23rd international conference on
Machine learning, ICML ’06, pages 113–120, New
York, NY, USA, 2006. ACM.
[4] D. M. Blei and J. D. Laﬀerty. Topic Models. CRC
Press, 2009.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,
Mar. 2003.
[6] CS 229 ﬁnal pro ject reports.
http://cs229.stanford.edu, 2005-2011.
[7] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc
topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April 2004.
[8] T. L. Griﬃths, M. Steyvers, D. M. Blei, and J. B.
Tenenbaum. Integrating topics and syntax. In In
Advances in Neural Information Processing Systems
17, pages 537–544. MIT Press, 2005.
[9] B. Grun and K. Hornik. topicmodels: An r package
for ﬁtting topic models. Journal of Statistical
Software, 40(13):1–30, 5 2011.
[10] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and
L. K. Saul. An introduction to variational methods for
graphical models. Machine Learning, 37:183–233,
1999. 10.1023/A:1007665907178.
[11] C. Manning and H. Sch¨utze. Foundations of statistical
natural language processing. MIT Press, Cambridge,
MA, 1999.
[12] D. Mimno, H. Wallach, and A. McCallum. Gibbs
Sampling for Logistic Normal Topic Models with
Graph-Based Priors. In NIPS Workshop on Analyzing
Graphs, 2008, 2008.
[13] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
Hierarchical dirichlet processes. Journal of the
American Statistical Association, 101(476):1566–1581,
2006.
[14] M. J. Wainwright and M. I. Jordan. Graphical Models,
Exponential Families, and Variational Inference. Now
Publishers Inc., Hanover, MA, USA, 2008.
[15] H. M. Wallach. Topic modeling: beyond bag-of-words.
In Proceedings of the 23rd international conference on
Machine learning, ICML ’06, pages 977–984, New
York, NY, USA, 2006. ACM.

0.000.010.020.030.040.000.010.020.030.040.000.010.020.030.040.000.010.020.030.040.000.010.020.030.04tweetdaytwitterpricestock20052006200720082009201020110.000.020.040.060.080.000.020.040.060.080.000.020.040.060.080.000.020.040.060.080.000.020.040.060.08networkimaglayervideodeep20052006200720082009201020112005 – Cluster 1
STAIR Subcomponent: Learning to Manipulate Ob jects
from Simulated Images
Door Handle Detection for the Stanford AI Robot
(STAIR)
Re-Learning to Walk: Adding Force Feedback Control to
the Quadruped Robot
2005 – Cluster 2
Learning Depth in Light Field Images
Explicit Image Filter
Learning Traﬃc Light Control Policies
2005 – Cluster 3
Splice Site Prediction using Multiple Sequence Alignment
CS229 Pro ject: Musical Alignment Discovery
Prostate Detection Using Principal Component Analysis
2009 – Cluster 1
Automatic graph classiﬁcation
Automatic Fatigue Detection
Automatic Beat Alignment
2009 – Cluster 2
Discovering Visual Hierarchy through Unsupervised
Learning
Spoken Language Identiﬁcation With Hierarchical
Temporal Memories
2009 – Cluster 3
Learning to splash
Stock Forecasting using Hidden Markov Processes
Recognizing Informed Option Trading
2011 – Cluster 1
Complex Sentiment Analysis using Recursive Autoencoders
Predicting Rating with Sentiment Analysis
Sentiment Based Model for Reputation Systems in Amazon
2011 – Cluster 2
Pulse News Preference Prediction
Predicting Preferences: Analyzing Reading Behavior and
News Preferences
Reddit Recommendation System
Pulse Pro ject: User-Interest-based News Prediction
2011 – Cluster 3
Attentional Based Multiple-Ob ject Tracking
Learning Unsupervised Features from Ob jects
Unsupervised Learning Of Temporally Coherent Features
For Action Recognition

Table 2: Selected documents and clusters for clus-
tering by years.

Topic 1
music
instrument
signal
ﬁg
mixtur
genr
facial
autoencod
ica
spectral
Topic 6
imag
brain
patient
roi
voxel
student
fmri
pixel
neuron
tumor
Topic 11
cluster
gene
cell
kmean
signal
transcript
protein
rna
damag
genes

Topic 2
word
document
queri
cluster
review
sentenc
corpus
topic
charact
semant
Topic 7
user
movi
cluster
student
rmse
friend
social
emot
kmean
recommend
Topic 12
tag
price
portfolio
trade
stock
market
econom
compani
asset
risk

Topic 3
game
player
team
win
agent
oppon
games
action
outcom
bet
Topic 8
network
beat
signal
neuron
onset
snps
ﬁg
music
patent
diseas
Topic 13
network
layer
node
car
sensor
lane
robot
signal
cascad
polici

Topic 4
song
audio
music
pitch
speech
mfcc
energi
network
accent
ppca
Topic 9
robot
gestur
action
sensor
aircraft
ﬁnger
reward
mous
ﬂight
movement
Topic 14
robot
pca
cluster
topic
entiti
terrain
motion
lda
channel
gpr

Topic 5
day
stock
price
tweet
market
trade
twitter
node
word
network
Topic 10
user
cluster
articl
feed
stori
read
word
recommend
day
news
Topic 15
imag
pixel
edg
video
depth
segment
frame
cluster
camera
patch

Table 3: Most probable words for each topic, for a
15-topic topic model.


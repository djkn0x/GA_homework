Multiclass Learning Approaches:
A Theoretical Comparison with Implications

Amit Daniely
Department of Mathematics
The Hebrew University
Jerusalem, Israel

Sivan Sabato
Microsoft Research
1 Memorial Drive
Cambridge, MA 02142, USA

Shai Shalev-Shwartz
School of CS and Eng.
The Hebrew University
Jerusalem, Israel

Abstract

We theoretically analyze and compare the following ﬁve popular multiclass clas-
siﬁcation methods: One vs. All, All Pairs, Tree-based classiﬁers, Error Correcting
Output Codes (ECOC) with randomly generated code matrices, and Multiclass
SVM. In the ﬁrst four methods, the classiﬁcation is based on a reduction to binary
classiﬁcation. We consider the case where the binary classiﬁer comes from a class
of VC dimension d, and in particular from the class of halfspaces over Rd . We
analyze both the estimation error and the approximation error of these methods.
Our analysis reveals interesting conclusions of practical relevance, regarding the
success of the different approaches under various conditions. Our proof technique
employs tools from VC theory to analyze the approximation error of hypothesis
classes. This is in contrast to most previous uses of VC theory, which only deal
with estimation error.

1

Introduction

In this work we consider multiclass prediction: The problem of classifying objects into one of
several possible target classes. Applications include, for example, categorizing documents according
to topic, and determining which object appears in a given image. We assume that objects (a.k.a.
instances) are vectors in X = Rd and the class labels come from the set Y = [k ] = {1, . . . , k}.
Following the standard PAC model, the learner receives a training set of m examples, drawn i.i.d.
from some unknown distribution, and should output a classiﬁer which maps X to Y .
The centrality of the multiclass learning problem has spurred the development of various approaches
for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass
classiﬁcation to binary classiﬁcation. For example, the One-vs-All (OvA) method is based on a
reduction of the multiclass problem into k binary problems, each of which discriminates between
one class to all the rest of the classes (e.g. Rumelhart et al. [1986]). A different reduction is the All-
Pairs (AP) approach in which all pairs of classes are compared to each other [Hastie and Tibshirani,
1998]. These two approaches have been uniﬁed under the framework of Error Correction Output
Codes (ECOC) [Dietterich and Bakiri, 1995, Allwein et al., 2000]. A tree-based classiﬁer (TC) is
another reduction in which the prediction is obtained by traversing a binary tree, where at each node
of the tree a binary classiﬁer is used to decide on the rest of the path (see for example Beygelzimer
et al. [2007]).
All of the above methods are based on reductions to binary classiﬁcation. We pay special attention
to the case where the underlying binary classiﬁers are linear separators (halfspaces). Formally, each
w ∈ Rd+1 deﬁnes the linear separator hw (x) = sign((cid:104)w, ¯x(cid:105)), where ¯x = (x, 1) ∈ Rd+1 is the
concatenation of the vector x and the scalar 1. While halfspaces are our primary focus, many of our
results hold for any underlying binary hypothesis class of VC dimension d + 1.

1

Other, more direct approaches to multiclass classiﬁcation over Rd have also been proposed (e.g.
Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]). In this paper we analyze
the Multiclass SVM (MSVM) formulation of Crammer and Singer [2001], in which each hypothesis
is of the form hW (x) = argmaxi∈[k] (W ¯x)i , where W is a k × (d + 1) matrix and (W ¯x)i is the i’th
element of the vector W ¯x ∈ Rk .
We theoretically analyze the prediction performance of the aforementioned methods, namely, OvA,
AP, ECOC, TC, and MSVM. The error of a multiclass predictor h : Rd → [k ] is deﬁned to be the
probability that h(x) (cid:54)= y , where (x, y) is sampled from the underlying distribution D over Rd × [k ],
namely, Err(h) = P(x,y)∼D [h(x) (cid:54)= y ]. Our main goal is to understand which method is preferable
in terms of the error it will achieve, based on easy-to-verify properties of the problem at hand.
Our analysis pertains to the type of classiﬁers each method can potentially ﬁnd, and does not depend
on the speciﬁc training algorithm. More precisely, each method corresponds to a hypothesis class,
H, which contains the multiclass predictors that may be returned by the method. For example, the
hypothesis class of MSVM is H = {x (cid:55)→ argmaxi∈[k] (W ¯x)i : W ∈ Rk×(d+1)}.
A learning algorithm, A, receives a training set, S = {(xi , yi )}m
i=1 , sampled i.i.d. according to
D , and returns a multiclass predictor which we denote by A(S ) ∈ H. A learning algorithm
is called an Empirical Risk Minimizer (ERM) if it returns a hypothesis in H that minimizes the
empirical error on the sample. We denote by h(cid:63) a hypothesis in H with minimal error,1 that is,
h(cid:63) ∈ argminh∈H Err(h).
When analyzing the error of A(S ), it is convenient to decompose this error as a sum of approxima-
tion error and estimation error:
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
+ Err(A(S )) − Err(h(cid:63) )
Err(A(S )) = Err(h(cid:63) )
estimation
approximation
• The approximation error is the minimum error achievable by a predictor in the hypothesis
class, H. The approximation error does not depend on the sample size, and is determined
solely by the allowed hypothesis class2 .
• The estimation error of an algorithm is the difference between the approximation error,
and the error of the classiﬁer the algorithm chose based on the sample. This error exists
both for statistical reasons, since the sample may not be large enough to determine the best
be bounded from above by order of (cid:112)C (H)/m where C (H) is a complexity measure of
hypothesis, and for algorithmic reasons, since the learning algorithm may not output the
best possible hypothesis given the sample. For the ERM algorithm, the estimation error can
H (analogous to the VC dimension) and m is the sample size. A similar term also bounds
the estimation error from below for any algorithm. Thus C (H) is an estimate of the best
achievable estimation error for the class.

(1)

.

When studying the estimation error of different methods, we follow the standard distribution-free
analysis. Namely, we will compare the algorithms based on the worst-case estimation error, where
worst-case is over all possible distributions D . Such an analysis can lead us to the following type
of conclusion: If two hypothesis classes have roughly the same complexity, C (H1 ) ≈ C (H2 ),
and the number of available training examples is signiﬁcantly larger than this value of complexity,
then for both hypothesis classes we are going to have a small estimation error. Hence, in this
case the difference in prediction performance between the two methods will be dominated by the
approximation error and by the success of the learning algorithm in approaching the best possible
estimation error. In our discussion below we disregard possible differences in optimality which stem
from algorithmic aspects and implementation details. A rigorous comparison of training heuristics
would certainly be of interest and is left to future work.
For the approximation error we will provide even stronger results, by comparing the approximation
error of classes for any distribution. We rely on the following deﬁnition.

1For simplicity, we assume that the minimum is attainable.
2Note that, when comparing different hypothesis classes over the same distribution, the Bayes error is
constant. Thus, in the deﬁnition of approximation error, we do not subtract the Bayes error.

2

Deﬁnition 1.1. Given two hypothesis classes, H, H(cid:48) , we say that H essentially contains H(cid:48) if for
any distribution, the approximation error of H is at most the approximation error of H(cid:48) . H strictly
contains H(cid:48) if, in addition, there is a distribution for which the approximation error of H is strictly
smaller than that of H(cid:48) .
Our main ﬁndings are as follows (see a full comparison in Table 1). The formal statements are given
in Section 3.
• The estimation errors of OvA, MSVM, and TC are all roughly the same, in the sense that
C (H) = ˜Θ(dk) for all of the corresponding hypothesis classes. The complexity of AP is
˜Θ(dk2 ). The complexity of ECOC with a code of length l and code-distance δ is at most
˜O(dl) and at least dδ/2. It follows that for randomly generated codes, C (H) = ˜Θ(dl).
Note that this analysis shows that a larger code-distance yields a larger estimation error and
might therefore hurt performance. This contrasts with previous “reduction-based” analyses
of ECOC, which concluded that a larger code distance improves performance.
• We prove that the hypothesis class of MSVM essentially contains the hypothesis classes
of both OvA and TC. Moreover, these inclusions are strict. Since the estimation errors of
these three methods are roughly the same, it follows that the MSVM method dominates
both OvA and TC in terms of achievable prediction performance.
• In the TC method, one needs to associate each leaf of the tree to a label.
If no prior
knowledge on how to break the symmetry is known, it is suggested in Beygelzimer et al.
[2007] to break symmetry by choosing a random permutation of the labels. We show that
whenever d (cid:28) k , for any distribution D , with high probability over the choice of a random
permutation, the approximation error of the resulting tree would be close to 1/2. It follows
that a random choice of a permutation is likely to yield a poor predictor.
• We show that if d (cid:28) k , for any distribution D , the approximation error of ECOC with a
randomly generated code matrix is likely to be close to 1/2.
• We show that the hypothesis class of AP essentially contains the hypothesis class of MSVM
(hence also that of OvA and TC), and that there can be a substantial gap in the containment.
Therefore, as expected, the relative performance of AP and MSVM depends on the well-
known trade-off between estimation error and approximation error.

Estimation error
Approximation
error
Testing run-time

TC
dk
≥ MSVM
≈ 1/2 when d (cid:28) k
d log(k)

OvA
MSVM
dk
dk
≥ MSVM ≥ AP

AP
dk2
smallest

dk

dk

dk2

random ECOC
dl
incomparable
≈ 1/2 when d (cid:28) k
dl

Table 1: Summary of comparison

The above ﬁndings suggest that in terms of performance, it may be wiser to choose MSVM over OvA
and TC, and especially so when d (cid:28) k . We note, however, that in some situations (e.g. d = k) the
prediction success of these methods can be similar, while TC has the advantage of having a testing
run-time of d log(k), compared to the testing run-time of dk for OvA and MSVM. In addition, TC
and ECOC may be a good choice when there is additional prior knowledge on the distribution or on
how to break symmetry between the different labels.

1.1 Related work

Allwein et al. [2000] analyzed the multiclass error of ECOC as a function of the binary error. The
problem with such a “reduction-based” analysis is that such analysis becomes problematic if the
underlying binary problems are very hard. Indeed, our analysis reveals that the underlying binary
problems would be too hard if d (cid:28) k and the code is randomly generated. The experiments in All-
wein et al. [2000] show that when using kernel-based SVM or AdaBoost as the underlying classiﬁer,
OvA is inferior to random ECOC. However, in their experiments, the number of classes is small rel-
ative to the dimension of the feature space, especially if working with kernels or with combinations
of weak learners.

3

Crammer and Singer [2001] presented experiments demonstrating that MSVM outperforms OvA
on several data sets. Rifkin and Klautau [2004] criticized the experiments of Crammer and Singer
[2001], Allwein et al. [2000], and presented another set of experiments demonstrating that all meth-
ods perform roughly the same when the underlying binary classiﬁer is very strong (SVM with a
Guassian kernel). As our analysis shows, it is not surprising that with enough data and powerful
binary classiﬁers, all methods should perform well. However, in many practical applications, we
will prefer not to employ kernels (either because of shortage of examples, which might lead to a
large estimation error, or due to computational constraint), and in such cases we expect to see a
large difference between the methods.
Beygelzimer et al. [2007] analyzed the regret of a speciﬁc training method for trees, called Filter
Tree, as a function of the regret of the binary classiﬁer. The regret is deﬁned to be the difference
between the learned classiﬁer and the Bayes-optimal classiﬁer for the problem. Here again we show
that the regret values of the underlying binary classiﬁers are likely to be very large whenever d (cid:28) k
and the leaves of the tree are associated to labels in a random way. Thus in this case the regret
analysis is problematic. Several authors presented ways to learn better splits, which corresponds to
learning the association of leaves to labels (see for example Bengio et al. [2011] and the references
therein). Some of our negative results do not hold for such methods, as these do not randomly attach
labels to tree leaves.
Daniely et al. [2011] analyzed the properties of multiclass learning with various ERM learners, and
have also provided some bounds on the estimation error of multiclass SVM and of trees. In this
paper we both improve these bounds, derive new bounds for other classes, and also analyze the
approximation error of the classes.

2 Deﬁnitions and Preliminaries

We ﬁrst formally deﬁne the hypothesis classes that we analyze in this paper.
Multiclass SVM (MSVM): For W ∈ Rk×(d+1) deﬁne hW : Rd → [k ] by hW (x) =
argmaxi∈[k] (W ¯x)i and let L = {hW : W ∈ Rk×(d+1) }. Though NP-hard in general, solving
the ERM problem with respect to L can be done efﬁciently in the realizable case (namely, whenever
exists a hypothesis with zero empirical error on the sample).

Tree-based classiﬁers (TC): A tree-based multiclass classiﬁer is a full binary tree whose leaves
are associated with class labels and whose internal nodes are associated with binary classiﬁers. To
classify an instance, we start with the root node and apply the binary classiﬁer associated with
it. If the prediction is 1 we traverse to the right child. Otherwise, we traverse to the left child.
This process continues until we reach a leaf, and then we output the label associated with the leaf.
Formally, a tree for k classes is a full binary tree T together with a bijection λ : leaf(T ) → [k ],
which associates a label to each of the leaves. We usually identify T with the pair (T , λ). The set
of internal nodes of T is denoted by N (T ). Let H ⊂ {±1}X be a binary hypothesis class. Given a
mapping C : N (T ) → H, deﬁne a multiclass predictor, hC : X → [k ], by setting hC (x) = λ(v)
where v is the last node of the root-to-leaf path v1 , . . . vm = v such that vi+1 is the left (resp. right)
child of vi if C (vi )(x) = −1 (resp. C (vi )(x) = 1). Let HT = {hC | C : N (T ) → H}. Also, let
Htrees = ∪T is a tree for k classes HT . If H is the class of linear separators over Rd , then for any tree T
the ERM problem with respect to HT can be solved efﬁciently in the realizable case. However, the
ERM problem is NP-hard in the non-realizable case.
Error Correcting Output Codes (ECOC): An ECOC is a code M ∈ Rk×l along with a bijection
(cid:16)
(cid:17)
λ : [k ] → [k ]. We sometimes identify λ with the identity function and M with (M , λ)3 . Given a
(cid:80)l
code M , and the result of l binary classiﬁers represented by a vector u ∈ {−1, 1}l , the code selects
a label via ˜M : {−1, 1}l → [k ], deﬁned by ˜M (u) = λ
. Given
arg maxi∈[k]
j=1 Mij uj
binary classiﬁers h1 , . . . , hl for each column in the code matrix, the code assigns to the instance
x ∈ X the label ˜M (h1 (x), . . . , hl (x)). Let H ⊂ {±1}X be a binary hypothesis class. Denote by

3The use of λ here allows us to later consider codes with random association of rows to labels.

4

HM ⊆ [k ]X the hypotheses class HM = {h : X → [k ] | ∃(h1 , . . . , hl ) ∈ Hl s.t. ∀x ∈ X , h(x) =
˜M (h1 (x), . . . , hl (x))}.
The distance of a binary code, denoted by δ(M ) for M ∈ {±1}k×l , is the minimal hamming
distance between any two pairs of rows in the code matrix. Formally, the hamming distance between
u, v ∈ {−1, +1}l is ∆h (u, v) = |{r : u[r ] (cid:54)= v [r]}|, and δ(M ) = min1≤i<j≤k ∆h (M [i], M [j ]).
The ECOC paradigm described in [Dietterich and Bakiri, 1995] proposes to choose a code with a
large distance.
One vs. All (OvA) and All Pairs (AP): Let H ⊂ {±1}X and k ≥ 2. In the OvA method we train
k binary problems, each of which discriminates between one class and the rest of the classes. In the
AP approach all pairs of classes are compared to each other. This is formally deﬁned as two ECOCs.
Deﬁne M OvA ∈ Rk×k to be the matrix whose (i, j ) elements is 1 if i = j and −1 if i (cid:54)= j . Then,
the hypothesis class of OvA is HOvA = HM OvA . For the AP method, let M AP ∈ Rk×(k
2) be such that
for all i ∈ [k ] and 1 ≤ j < l ≤ k , the coordinate corresponding to row i and column (j, l) is deﬁned
to be −1 if i = j , 1 if i = l, and 0 otherwise. Then, the hypothesis class of AP is HAP = HM AP .
Our analysis of the estimation error is based on results that bound the sample complexity of multi-
class learning. The sample complexity of an algorithm A is the function mA deﬁned as follows: For
, δ > 0, mA (, δ) is the smallest integer such that for every m ≥ mA (, δ) and every distribution
D on X × Y , with probability of > 1 − δ over the choice of an i.i.d. sample S of size m,
Err(A(Sm )) ≤ min
(2)
h∈H Err(h) +  .
The ﬁrst term on the right-hand side is the approximation error of H. Therefore, the sample com-
plexity is the number of examples required to ensure that the estimation error of A is at most  (with
high probability). We denote the sample complexity of a class H by mH (, δ) = inf A mA (, δ),
where the inﬁmum is taken over all learning algorithms.
To bound the sample complexity of a hypothesis class we rely on upper and lower bounds on the
sample complexity in terms of two generalizations of the VC dimension for multiclass problems,
called the Graph dimension and the Natarajan dimension and denoted dG (H) and dN (H). For
(cid:18) dN (H) + ln( 1
(cid:19)
(cid:18) min{dN (H) ln(|Y |), dG (H)} + ln( 1
(cid:19)
completeness, these dimensions are formally deﬁned in the appendix.
Theorem 2.1. Daniely et al. [2011] For every hypothesis class H, and for every ERM rule,
≤ mH (, δ) ≤ mERM (, δ) ≤ O
δ )
δ )
2
2
We note that the constants in the O , Ω notations are universal.

Ω

3 Main Results

In Section 3.1 we analyze the sample complexity of the different hypothesis classes. We provide
lower bounds on the Natarajan dimensions of the various hypothesis classes, thus concluding, in
light of Theorem 2.1, a lower bound on the sample complexity of any algorithm. We also provide
upper bounds on the graph dimensions of these hypothesis classes, yielding, by the same theorem,
an upper bound on the estimation error of ERM. In Section 3.2 we analyze the approximation error
of the different hypothesis classes.

3.1 Sample Complexity

Together with Theorem 2.1, the following theorems estimate, up to logarithmic factors, the sample
complexity of the classes under consideration. We note that these theorems support the rule of thumb
that the Natarajan and Graph dimensions are of the same order of the number of parameters. The
ﬁrst theorem shows that the sample complexity of MSVM depends on ˜Θ(dk).
Theorem 3.1. d(k − 1) ≤ dN (L) ≤ dG (L) ≤ O(dk log(dk)).
Next, we analyze the sample complexities of TC and ECOC. These methods rely on an underlying
hypothesis class of binary classiﬁers. While our main focus is the case in which the binary hypoth-
esis class is halfspaces over Rd , the upper bounds on the sample complexity we derive below holds
for any binary hypothesis class of VC dimension d + 1.

5

Theorem 3.2. For every binary hypothesis class of VC dimension d + 1, and for any tree T ,
dG (HT ) ≤ dG (Htrees ) ≤ O(dk log(dk)). If the underlying hypothesis class is halfspaces over
Rd , then also

d(k − 1) ≤ dN (HT ) ≤ dG (HT ) ≤ dG (Htrees ) ≤ O(dk log(dk)).
(cid:17) ≤ dN (HT ).
(cid:16) dk
Theorems 3.1 and 3.2 improve results from Daniely et al. [2011] where it was shown that (cid:98) d
2 (cid:99)(cid:98) k
2 (cid:99) ≤
dN (L) ≤ O(dk log(dk)), and for every tree dG (HT ) ≤ O(dk log(dk)). Further it was shown that
if H is the set of halfspaces over Rd , then Ω
log(k)
We next turn to results for ECOC, and its special cases OvA and AP.
Theorem 3.3. For every M ∈ Rk×l and every binary hypothesis class of VC dimension d,
dG (HM ) ≤ O(dl log(dl)). Moreover, if M ∈ {±1}k×l and the underlying hypothesis class is
halfspaces over Rd , then
d · δ(M )/2 ≤ dN (HM ) ≤ dG (HM ) ≤ O(dl log(dl)) .

We note if the code has a large distance, which is the case, for instance, in random codes, then
δ(M ) = Ω(l). In this case, the bound is tight up to logarithmic factors.
Theorem 3.4. For any binary hypothesis class of VC dimension d, dG (HOvA ) ≤ O(dk log(dk)) and
dG (HAP ) ≤ O(dk2 log(dk)). If the underlying hypothesis class is halfspaces over Rd we also have:
d(cid:0)k−1
(cid:1) ≤ dN (HAP ) ≤ dG (HAP ) ≤ O(dk2 log(dk)).
d(k − 1) ≤ dN (HOvA ) ≤ dG (HOvA ) ≤ O(dk log(dk))
and
2
3.2 Approximation error
We ﬁrst show that the class L essentially contains HOvA and HT for any tree T , assuming, of
course, that H is the class of halfspaces in Rd . We ﬁnd this result quite surprising, since the sample
complexity of all of these classes is of the same order.
Theorem 3.5. L essentially contains Htrees and HOvA . These inclusions are strict for d ≥ 2 and
k ≥ 3.
One might suggest that a small increase in the dimension would perhaps allow us to embed L in HT
for some tree T or for OvA. The next result shows that this is not the case.
Theorem 3.6. Any embedding into a higher dimension that allows HOvA or HT (for some tree T
for k classes) to essentially contain L, necessarily embeds into a dimension of at least ˜Ω(dk).

The next theorem shows that the approximation error of AP is better than that of MSVM (and hence
also better than OvA and TC). This is expected as the sample complexity of AP is considerably
higher, and therefore we face the usual trade-off between approximation and estimation error.
Theorem 3.7. HAP essentially contains L. Moreover, there is a constant k∗ > 0, independent of d,
such that the inclusion is strict for all k ≥ k∗ .

For a random ECOC of length o(k), it is easy to see that it does not contain MSVM, as MSVM has
higher complexity. It is also not contained in MSVM, as it generates non-convex regions of labels.
We next derive absolute lower bounds on the approximation errors of ECOC and TC when d (cid:28) k .
Recall that both methods are built upon binary classiﬁers that should predict h(x) = 1 if the label
of x is in L, for some L ⊂ [k ], and should predict h(x) = −1 if the label of x is not in L. As the
following lemma shows, when the partition of [k ] into the two sets L and [k ] \ L is arbitrary and
balanced, and k (cid:29) d, such binary classiﬁers will almost always perform very poorly.
Lemma 3.8. There exists a constant C > 0 for which the following holds. Let H ⊆ {±1}X be any
hypothesis class of VC-dimension d, let µ ∈ (0, 1/2], and let D be any distribution over X × [k ]
k . Let φ : [k ] → {±1} be a randomly chosen function which
such that ∀i P(x,y)∼D (y = i) ≤ 10
is sampled according to one of the following rules: (1) For each i ∈ [k ], each coordinate φ(i)
is chosen independently from the other coordinates and P(φ(i) = −1) = µ; or (2) φ is chosen
uniformly among all functions satisfying |{i ∈ [k ] : φ(i) = −1}| = µk .

6

it with (x, φ(y)). Then, for any ν > 0, if k ≥ C · (cid:16) d+ln( 1
(cid:17)
Let Dφ be the distribution over X × {±1} obtained by drawing (x, y) according to D and replacing
, then with probability of at least 1 − δ
δ )
ν 2
over the choice of φ, the approximation error of H with respect to Dφ will be at least µ − ν .
As the corollaries below show, Lemma 3.8 entails that when k (cid:29) d, both random ECOCs with a
small code length, and balanced trees with a random labeling of the leaves, are expected to perform
very poorly.
Corollary 3.9. There is a constant C > 0 for which the following holds. Let (T , λ) be a tree
for k classes such that λ : leaf (T ) → [k ] is chosen uniformly at random. Denote by kL and kR
k . Then, for k ≥ C · (cid:16) d+ln( 1
(cid:17)
the number of leaves of the left and right sub-trees (respectively) that descend from root, and let
µ = min{ k1
k }. Let H ⊆ {±1}X be a hypothesis class of VC-dimension d, let ν > 0, and let D
k , k2
be any distribution over X × [k ] such that ∀i P(x,y)∼D (y = i) ≤ 10
δ )
,
ν 2
with probability of at least 1 − δ over the choice of λ, the approximation error of HT with respect
to D is at least µ − ν .
Corollary 3.10. There is a constant C > 0 for which the following holds. Let (M , λ) be an ECOC
k . Then, for k ≥ C · (cid:16) dl log(dl)+ln( 1
(cid:17)
where M ∈ Rk×l , and assume that the bijection λ : [k ] → [k ] is chosen uniformly at random. Let
H ⊆ {±1}X be a hypothesis class of VC-dimension d, let ν > 0, and let D be any distribution over
X × [k ] such that ∀i P(x,y)∼D (y = i) ≤ 10
δ )
, with probability
ν 2
of at least 1 − δ over the choice of λ, the approximation error of HM with respect to D is at least
1/2 − ν .

Note that the ﬁrst corollary holds even if only the top level of the binary tree is balanced and splits
the labels randomly to the left and the right sub-trees. The second corollary holds even if the code
itself is not random (nor does it have to be binary), and only the association of rows with labels is
random. In particular, if the length of the code is O(log(k)), as suggested in Allwein et al. [2000],
and the number of classes is ˜Ω(d), then the code is expected to perform poorly.
For an ECOC with a matrix of length Ω(k) and d = o(k), we do not have such a negative result as
stated in Corollary 3.10. Nonetheless, Lemma 3.8 implies that the prediction of the binary classiﬁers
when d = o(k) is just slightly better than a random guess, thus it seems to indicate that the ECOC
method will still perform poorly. Moreover, most current theoretical analyses of ECOC estimate the
error of the learned multiclass hypothesis in terms of the average error of the binary classiﬁers. Alas,
2 .
when the number of classes is large, Lemma 3.8 shows that this average will be close to 1
Finally, let us brieﬂy discuss the tightness of Lemma 3.8. Let x1 , . . . , xd+1 ∈ Rd be afﬁnely inde-
pendent and let D be the distribution over Rd × [d + 1] deﬁned by P(x,y)∼D ((x, y) = (xi , i)) = 1
d+1 .
Is is not hard to see that for every φ : [d + 1] → {±1}, the approximation error of the class of half-
spaces with respect to Dφ is zero. Thus, in order to ensure a large approximation error for every
distribution, the number of classes must be at least linear in the dimension, so in this sense, the
lemma is tight. Yet, this example is very simple, since each class is concentrated on a single point
and the points are linearly independent. It is possible that in real-world distributions, a large approx-
imation error will be exhibited even when k < d.
We note that the phenomenon of a large approximation error, described in Corollaries 3.9 and 3.10,
does not reproduce in the classes L, HOvA and HAP , since these classes are symmetric.

4 Proof Techniques

Due to lack of space, the proofs for all the results stated above are provided in the appendix. In this
section we give a brief description of our main proof techniques.
Most of our proofs for the estimation error results, stated in Section 3.1, are based on a similar
method which we now describe. Let L : {±1}l → [k ] be a multiclass-to-binary reduction (e.g., a
tree), and for H ⊆ {±1}X , denote L(H) = {x (cid:55)→ L(h1 (x), . . . , hl (x)) | h1 , . . . , hl ∈ H}. Our
upper bounds for dG (L(H)) are mostly based on the following simple lemma.
Lemma 4.1. If VC(H) = d then dG (L(H)) = O(ld ln(ld)).

7

The technique for the lower bound on dN (L(W )) when W is the class of halfspaces in Rd is more
involved, and quite general. We consider a binary hypothesis class G ⊆ {±1}[d]×[l] which consists
of functions having an arbitrary behaviour over [d] × {i}, and a very uniform behaviour on other
inputs (such as mapping all other inputs to a constant). We show that L(G ) N -shatters the set [d]× [l].
Since G is quite simple, this is usually not very hard to show. Finally, we show that the class of
halfspaces is richer than G , in the sense that the inputs to G can be mapped to points in Rd such that
the functions of G can be mapped to halfspaces. We conclude that dN (L(W )) ≥ dN (L(G )).
To prove the approximation error lower bounds stated in Section 3.2, we use the techniques of VC
theory in an unconventional way. The idea of this proof is as follows: Using a uniform convergence
argument based on the VC dimension of the binary hypothesis class, we show that there exists a
small labeled sample S whose approximation error for the hypothesis class is close to the approx-
imation error for the distribution, for all possible label mappings. This allows us to restrict our
attention to a ﬁnite set of hypotheses, by their restriction to the sample. For these hypotheses, we
show that with high probability over the choice of label mapping, the approximation error on the
sample is high. A union bound on the ﬁnite set of possible hypotheses shows that the approximation
error on the distribution will be high, with high probability over the choice of the label mapping.

5

Implications

The ﬁrst immediate implication of our results is that whenever the number of examples in the training
set is ˜Ω(dk), MSVM should be preferred to OvA and TC. This is certainly true if the hypothesis
class of MSVM, L, has a zero approximation error (the realizable case), since the ERM is then
solvable with respect to L. Note that since the inclusions given in Theorem 3.5 are strict, there are
cases where the data is realizable with MSVM but not with HOvA or with respect to any tree.
In the non-realizable case, implementing the ERM is intractable for all of these methods. Nonethe-
less, for each method there are reasonable heuristics to approximate the ERM, which should work
well when the approximation error is small. Therefore, we believe that MSVM should be the method
of choice in this case as well due to its lower approximation error. However, variations in the opti-
mality of algorithms for different hypothesis classes should also be taken into account in this anal-
ysis. We leave this detailed analysis of speciﬁc training heuristics for future work. Our analysis
also implies that it is highly unrecommended to use TC with a randomly selected λ or ECOC with a
random code whenever k > d. Finally, when the number of examples is much larger than dk2 , the
analysis implies that it is better to choose the AP approach.
To conclude this section, we illustrate the relative performance of MSVM, OvA, TC, and ECOC, by
considering the simplistic case where d = 2, and each class is concentrated on a single point in R2 .
In the leftmost graph below, there are two classes in R2 , and the approximation error of all algorithms
is zero. In the middle graph, there are 9 classes ordered on the unit circle of R2 . Here, both MSVM
and OvA have a zero approximation error, but the error of TC and of ECOC with a random code will
most likely be large. In the rightmost graph, we chose random points in R2 . MSVM still has a zero
approximation error. However, OvA cannot learn the binary problem of distinguishing between the
middle point and the rest of the points and hence has a larger approximation error.

MSVM
OvA
TC/ECOC













Acknowledgements: Shai Shalev-Shwartz was supported by the John S. Cohen Senior Lecture-
ship in Computer Science. Amit Daniely is a recipient of the Google Europe Fellowship in Learning
Theory, and this research is supported in part by this Google Fellowship.

8

References
E. L. Allwein, R.E. Schapire, and Y. Singer. Reducing multiclass to binary: A unifying approach
for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.
S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for
classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50:74–86,
1995.
S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In NIPS,
2011.
A. Beygelzimer, J. Langford, and P. Ravikumar. Multiclass classiﬁcation with ﬁlter trees. Preprint,
June, 2007.
K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265–292, 2001.
A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the erm
principle. In COLT, 2011.
T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output
codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, January 1995.
Trevor Hastie and Robert Tibshirani. Classiﬁcation by pairwise coupling. The Annals of Statistics,
26(1):451–471, 1998.
In defense of one-vs-all classiﬁcation. Journal of Machine
Ryan Rifkin and Aldebaro Klautau.
Learning Research, 5:101–141, 2004.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations
by error propagation. In David E. Rumelhart and James L. McClelland, editors, Parallel Dis-
tributed Processing – Explorations in the Microstructure of Cognition, chapter 8, pages 318–362.
MIT Press, 1986.
G. Takacs. Convex polyhedron learning and its applications. PhD thesis, Budapest University of
Technology and Economics, 2009.
V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.
J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. In Proceed-
ings of the Seventh European Symposium on Artiﬁcial Neural Networks, April 1999.

9


Kinect Gesture Recognition for Interactive System
Hao Zhang, WenXiao Du, and Haoran Li

1

Abstract— Gaming systems like Kinect and XBox always
have to tackle the problem of extracting features from
video data sets and classifying the body movement. In
this study, reasonable features like human joints positions,
joints velocities, joint angles and joint angular veloc-
ities are extracted. We used several machine learning
methods including Naive Bayes, Support Vector Machine
and Random Forest to learn and classify the human
gestures. Simulation results and the ﬁnal confusion matrix
show that by combining delicately preprocessed data sets
and Random Forest methods, the F-scores of the correct
predictions can be maximized. Such methods can also be
applied in real-time scenarios.
Index Terms— Kinect gesture recognition, SVM, random
forest, naive bayes

I . INTRODUCT ION
The last few years witnessed a great increase of the
prevalence of body-movement based interface. Among
all the modes, touchless body movement interface has
obviously caught more attentions since it can offer
more friendly user experience. In the application area,
traditional video cameras can be used to caption the
body movements to enable interactive system. However,
due to the limitation of the usage on applications,
such technology did not own a large user set. On the
other hand, some gaming systems like Microsoft Kinect,
Nitendo Wii and Sony Playstation, have made such
touchless body-movement based interface more popular
and convenient to use by introducing a depth sensor to
capture video data in 3D.
In the paper, we focus on gesture recognition of
such interactive system. In this sense, by analyzing
and training the video data from Microsoft Kinect, we
would design a machine learning method to classify the
actual movement captured from the Kinect video with
high accuracy. When a movement is ﬁnished, it would
automatically classify the movement
into one of the
6 gestures asshowninF ig .1 to further implement the
built-in functions in Kinect.
The remainder of the paper is organized as follows.
Section II describes the collection of data sets. The

The authors are with the Department of Electronic Engineering at
Stanford University, CA, 94305, United States. Email: {hzhang22,
wxdu, aimeeli}@stanford.edu.

Fig. 1.

Input Gestures.

method of extracting features from the data sets is
detailed in Section III. Section IV describes the learning
process by SVM. And in Section V, the learning process
of random forest
is explained. Section VI provides
simulation performance to compare different learning
methods and analysis the learning results. Finally, we
conclude the paper in Section VII.

I I . THE K INEC T 3D DATA SE T S

The data sets we are using in this paper are pro-
vided by Microsoft Research Cambridge-12 (MSRC-
12). Microsoft has also provided a programming toolkit
of Kinect for Windows Software Development Kit Beta.
The SDK offers the capability to track the skeleton 3D
model and obtain the data of joints positions [5]. The
data sets are collected from 30 people performing 6
different gestures (F ig . 1) with approximately 3 hour
34 minutes. More speciﬁcally, with a sample rate of
30Hz, it is composed of 30 sequences, 385732 frames,
with a total of 3054 gesture instances. Mathematically
speaking, there are 6 gestures with about 50 sequences
each. And each sequence is composed of about 800
frames constituting approximately 10 gesture instances.
Labeling of the data is automatically done by the related
’tagstream’ ﬁles.
There are two kinds of gesture types in the data sets:
Iconic gestures - those imbue a correspondence between
the gesture and the reference, and Metaphor gestures -
those represent an abstract concept. A table of the gesture
is given on the top of the next page.

TABLE I
G E STURE C LA S S I FICAT ION

Iconic Gestures
Crouch or Hide
Throw an Object
Kick

Number of Instance
500
515
502

Metaphoric Gestures
Start Music/Raise Volume
Navigate to Next Menu
Take a Bow to End Music

Number of Instance
508
522
507

2

I I I . F EATURE EXTRAC T ION
In the original data set, each frame of gestures is
recorded as the absolute position of 20 joints of hu-
man body in xyz-coordinates, 60 data total per frame.
Meanwhile, in each sequence, a single gesture has been
repeated for several times. Therefore, some preprocess
should be applied to the raw data sets in order to form
the proper training examples and informative feature
vectors. For time t, we derive a feature vector of
φt = φ(xt:t−l )from the last l observations xt to xt−l .
Indicated by the paper [2] that when l = 35, the real-
time performance can be achieved. Therefore we deﬁne
every 35 frames as a training example. Even though we
cannot precisely subsume a gesture instance into every
35 frames, the relative displacement of adjacent frames
within a training example can also provide enough
information to make the classiﬁcation. For each pair of
adjacent frames, 4 kinds of factors can be considered as
the possible components of a feature vector, which are:
• 3 xyz -coordinates per joint, 60 total
• 3 xyz -velocities per joint, 60 total
• 35 joint angles
• 35 joint angular velocities
The skeletal structure of human body is shown in Fig. 2.

between each pair of adjacent frames. The joint angle is
simply the angle between the two segments on either
side of the joint. For the joint
like shoulder-center,
which has multiple segments around it, we compute the
angles pairwise. Besides only extracting angles between
adjacent segments, we put an imaginary joint in (0, 0, 0)
in world coordinates, which is the location of camera.
This is helpful because all other angles are rotation
invariant, but this one allows you to distinguish changes
with respect to the camera, for example, when we want
to recognize whether a person throws an object to the left
or to the right of the camera. The joint angular velocity
is the rate of change of joint angle, which is computed
by the difference of the corresponding joint angle in each
pair of adjacent frames.

IV. SVM C LA S S I FICAT ION
After preprocessing the feature vectors, we ﬁrst use
a SVM tool [7] to train our data. The link to the SVM
package is from [3]. We randomly divided the given data
into 70% for training and 30% for testing. The following
ﬁve steps are gone through in the process of training and
testing:

A. Deﬁne Feature Vector
In this step, we would determine the composition of
feature vector. As described in the previous section, four
classes of features are considered. We run the forward
search on these feature classes and obtain the following
results. The feature selection is run on the data sets with
60 sequences for training and 12 sequences for test.
The result of feature selection is shown in the table
on the next page:
We choose xyz-velocity, joint angle and joint angular
velocity as the feature vector for each frame, and the
dimension is 60 + 35 + 35 = 130. As described in the
previous section, 35 frames are included in each training
example. Thus the dimension of a feature vector of a
training example is 4420.

Fig. 2. Skeletal Structure of Human Body.
Here are some details about each components. The
xyz -velocities are straightforwardly deﬁned as the dif-
ference between xyz coordinates of corresponding joints

B. Data Scaling
Scaling before applying SVM is of great signiﬁcance.
By scaling we can avoid the scenario that attributes in

−0.200.20.40.6−1−0.8−0.6−0.4−0.200.20.40.622.53TABLE II
F EATURE C LA S S S EL EC T ION

Feature Fixed Accuracy Real Proportion
Feature in Use
149/369
40.3794%
1
φ
23.5772%
87/369
2
φ
241/369
65.3117%
3
φ
87/369
23.5772%
4
φ
209/369
56.6396%
3
3+1
73.9837%
273/369
3
3+2
212/369
57.4526%
3
3+4
293/369
79.4038%
3+2
3+2+4
234/369
63.4146%
3+2
3+2+1
63.1436%
233/369
3+2+4
3+2+4+1
* 1 is xyz position, 2 is xyz velocity, 3 is joint angle,4 is joint anglular
velocity

larger numeric range dominate the ones in small numeric
range. And it can alleviate the mathematical calculation
workload.
In this paper, the features are linearly scaled into a
range of [−1, +1]. By libsvm, the improvement on the
accuracy can be seen:

TABLE III
DATA SCA L ING

Mode
Before Scaling
After Scaling

Accuracy Real Proportion
1938/3101
62.496%
63.1732%
1959/3101

From the table, it can be observed that by preprocessing
of scaling on the data, the accuracy can improved by
about 3%. Therefore, in the afterwards experiment, we
will use the scaled data in order to achieve a better
prediction.

C. Kernel Selection
We tried three kernels in our process of kernel
selection:
linear kernel, polynomial kernel and radial
basis function (RBF) kernel. The accuracy of prediction
using the three kernels after scaling is in the table below:

TABLE IV
K ERN EL S ELEC T ION

Kernel
Linear
Polynomial
RBF

Accuracy Real Proportion
61.0448%
1893/3101
1025/3101
33.0539%
63.1723%
1959/3101

Note that RBF kernel achieves the highest accuracy.
This kernel nonlinearly maps samples into a higher

3

dimensional space and it has fewer hyperparameters
than polynomial kernel which inﬂuences the complexity
of model selection. From the result, we can see that
the polynomial kernel is overﬁtting. Linear kernel also
provides us with a comparable accuracy due to the large
number of features. But since RBF kernel gives us a
higher accuracy, we determined to use RBF kernel.

D. Parameter Selection
There are two parameters for RBF kernel: C and γ ,
which is not known beforehand, thus some kinds of
parameter search must be done. The goal is to identify
good (C, γ ) so that the classiﬁer can accurately classify
unknown data. A common strategy is to use n-fold cross-
validation, which divides the training set into n subsets of
equal size and sequentially one subset is tested using the
classiﬁer trained on the remaining n-1 subsets. We use
a grid-search on C and γ by cross-validation. Various
pairs of (C,γ ) values are tried and the one with best
cross-validation accuracy is picked. After running the
parameter selection script, we got the parameter C = 32
and γ = 0.0078125 with an accuracy of 90.2439%. We
used this parameter in later training.

E. Final Result
After deﬁning the feature vectors, scaling the data,
choosing the most accurate kernel and got the param-
eters, we used svm-train and svm-predict again with
the chosen kernel and parameters, we ﬁnally got an
accuracy of prediction of 67.3331% (2088/3101), which
is acceptable.

V. RANDOM FOR E S T L EARN ING M ETHOD
As in [6], Random Forest works as described below.
After given a set of training examples, a random forest
is created with H random decision trees. And for the
k − th tree in the random forest, a random vector φk
is generated independently of the past random vectors
φ1 , ..., φk−1 . This vector φk is then used to grow the
trees resulting in a classiﬁer hk (x, φk ) where x is the
feature vector. For each tree, a decision function splits
the training data that reach a node at a given level in
the tree [4].Then each tree gives a classiﬁcation, and we
say the tree ”votes” for that class. The forest chooses the
classiﬁcation having the most votes (over all the trees in
the forest).
The resulting forest classiﬁer H is used to classify
a given feature vector by taking the mode of all the
classiﬁcations made by the tree classiﬁcation h ∈ H
for all the forest.

A. Growing Trees
The following approach is similar to that of [1]. At
test time t, we derive a vector φt = φ(xt:(t−l+1) ) ∈ Rd
from the last l observations xt to xt−l+1 . According to
what we have described in the SVM method, the training
examples are set to l = 35 frames, which obtains d =
4420 features. The feature vector φt is evaluated by a set
of M decision trees in the random forest, where simple
test
fω : Rd → {left,right}
(1)
are performed recursively at each node until a leaf node
is reached. In our experiment, the number of random
decision trees is set to be M = 300. The parameters
ω ∈ Ω of each tests are determined separately during
the training phase, and the determination process is
described below.
For each tree m = 1, ..., M , it produces one class
M(cid:88)
t and the posterior class distribution
decision ym
1
p(yt = a|xt−l+1:t ) :=
I (ym
t = a)
(2)
M
m=1
over gesture class A. At the same time, we have to add
an extra class ”None” to indicates whether a gesture has
been recognized. If for a gesture class a ∈ A we have
p(yt = a|xt−l+1:t ) ≥ δ , we can then determined the
gesture being detected at current time t. We used a ﬁxed
value δ = 0.16 [2] for all the random forest experiments.

f(i,h) (φt ) =

B. Random Forest Training and Predicting
For the training, we use approximately 70% of all the
observations together with the action point annotations
for a set of N sequences, where the n − th sequence is
an ordered list (xn
t )t=1,...Tn . Our goas is to learn a
t , yn
set of M decision trees that classify the action points in
these sequences correctly by means of (2). Then for the
(cid:40)
decision parts, we use simple ”decision stump” tests [6]
with ω = (i, h), 1 ≤ i ≤ d, h ∈ R,
if[φt ]i ≤ h
left
otherwise
right
Standard information gain criterion and training proce-
dure are used in the method. We greedily select a split
function f(i,j ) for each node in the decision tree from a
set of randomly generated proposal split functions. The
tree is then grown until the node is pure. In a sense, all
training examples assigned to that node have the same
label.
After all the decision trees are ﬁnally formed, the
random forest is well set. And we can use the random
forest model to make classiﬁcations by simply putting
the test examples into the random forest.

(3)

4

Fig. 3. Fscore of three method.

V I . S IMU LAT ION R E SU LT S AND P ER FORMANCE
A S S E S SM ENT

The accuracy of the three algorithms are summarized
in table below:

TABLE V
ACCURACY COM PAR I SON

Algorithm
Naive Bayes
SVM
Random Forest

Accuracy
56.33%
67.33%
80.69%

In the ﬁnal performance assessment, the whole data
set is randomly split into two parts, 210 sequences for
training and 70 sequences for testing. We use F-score
and confusion matrix to evaluate the performance of each
algorithm. The F-score of the three methods is Fig. 3 and
the confusion matrix of the three methods is Fig. 4.
The accuracy of the prediction by SVM is 67.33%.
From the confusion matrix, we can see that the perfor-
mance on recognizing gesture 1, 2, 5 is relatively better
than on other gestures. However, since other gestures can
also easily be misclassiﬁed into gesture 1, the recall of
gesture 1 is low, which makes its Fscore great lower than
its accuracy.
We can see that SVM preforms worse than Random
Forest Algorithm, probably because there are too many
features in each training example. Although the feature
class selection has been conducted on the data set, the
over-ﬁtting still slightly exists. Such guess can also be
conﬁrmed by the fact that the ﬁnal accuracy on the whole
data set is poorer than the accuracy when the algorithm
is conducted on the small data set in the feature selection
step.

hide      throw     kick      startMusicnextMenu  endMusic  00.20.40.60.81FScores of three method  Naive BayesianSVMRandom Forest5

Fig. 4. Confusion Matrix.
We also implemented Naive Bayes as a benchmark
to compare with the results got by SVM and Random
Forest. At ﬁrst we used normal distribution to model
the data and created class variable for training taking
1000 distinct
levels. Then we have a train category
vector that deﬁnes which class the corresponding row
of training belongs to. We used Naive Bayes classiﬁer
with the multinomial event model and Laplace transform
to classify each gesture. Then we compared with the
actual category and got the confusion matrix and F-
score. The accuracy of the prediction by Naive Bayes
is 56.33%. From the confusion matrix, we can see
that the performance of predicting gesture 1, 2, 5 is
better compared to the other gestures. But overall, the
performance is worse than that of SVM and Random
Forest. Since Naive Bayes discretizes the feature values
and instead uses a class variable, it loses some accuracy
in the process of discretization, which is reasonable.
The performance of the random forest is the best
among three algorithms. The accuracy by random forest
can reaches as high as 80.69%. Meanwhile the F-scores
of all six gestures are higher than other methods.

V I I . CONC LU S ION
In this report, we have studied the methods to pre-
processing the given data sets to ﬁnd the best features.
And then, in SVM process, after feature class is selected,
scaling, Kernel selection, RBF kernel parameter selec-
tion, we have decided the ﬁnal SVM model. And the
F-scores of every classs in the SVM model can be seen
on Fig. 3. Then, we have tried random forest method,
after growing a forest with 300 decision trees. The F-
score of every class in the model has increased a lot. As
a benchmark, a naive bayesian model was also simulated.
By comparing all three models, it can be found that by
combining delicately preprocessed data sets and Random
Forest methods, the F-scores of the correct predictions
can be maximized. In a sense, the Kinect system can thus

differentiate one human gesture from the other trained
gestures with high accuracy.
In the future work, a more accurate feature selection
on the data sets can be conducted. If we are given
more powerful computation resources, we would like
to experiment with a larger data sets, and are capable
of conducting the large computation required delicate
feature selection. Meanwhile, another improvement in
the future can be focused on the vision part, which is
the method to extract joints data sets from the kinect
video. It is indeed a challenging task.

R E F ER ENC E S
[1] Gabriele Fanelli Angela Yao, Juergen Gall and Luc Van Gool.
Does human action recognition beneﬁt from pose estimation?,
2011. http://dx.doi.org/10.5244/C.25.67.
[2] Simon Fothergill, Helena Mentis, Pushmeet Kohli, and Sebastian
Nowozin.
Instructing people for training gestural interactive
systems. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ’12, pages 1737–1746, New
York, NY, USA, 2012. ACM.
[3] Simon Fothergill, Helena M. Mentis, Pushmeet Kohli, and Sebas-
tian Nowozin. Instructing people for training gestural interactive
systems. In Joseph A. Konstan, Ed H. Chi, and Kristina H ¨o ¨ok,
editors, CHI, pages 1737–1746. ACM, 2012.
[4] G. Rogez, J. Rihan, S. Ramalingam, C. Orrite, and P.H.S. Torr.
Randomized trees for human pose detection. In Computer Vision
and Pattern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 –8, june 2008.
[5] Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark
Finocchio, Richard Moore, Alex Kipman, and Andrew Blake.
Real-time human pose recognition in parts from single depth
images. In In In CVPR, 2011. 3.
[6] Leo Breiman Statistics and Leo Breiman. Random forests. In
Machine Learning, pages 5–32, 2001.
[7] www.csie.ntu.edu.tw/∼cjlin/libsvm/.


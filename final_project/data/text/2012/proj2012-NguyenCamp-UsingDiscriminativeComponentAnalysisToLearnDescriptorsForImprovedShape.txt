Using Discriminative Component Analysis to Learn Descriptors for Improved Shape
Mapping

Andy Nguyen, Charlie Camp

Shape matching is an important problem in computational geometry with applications in many
areas ranging from animation to medical imaging. The ob jective of the shape matching problem
is to ﬁnd a correspondence between all points on two given shapes. One general approach to this
problem is to compute geometric descriptors on every point on both shapes, and then match the
points based on the descriptors, sub ject to additional continuity constraints. Unfortunately, there
are many diﬀerent descriptors that have been proposed, and all of them work well in some cases but
poorly in others. In this pro ject we will obtain training data of shapes with known correspondences,
and then compute the descriptors on these sets. We then use metric learning to determine the
appropriate weight given to each descriptor, based on how well they match up with the known
correspondences. Once the weights have been learned, we can feed them into an existing general
framework for shape matching and evaluate our results there. For this last step, we compare the
shapes using the eigenbasis of the Laplace-Beltrami operator by requiring that the map be function-
preserving for each descriptor function fed in. While the Laplace-Beltrami basis is not a particularly
good choice for excessively diﬀerent shapes, it is a popular choice due to its performance on near-
isometric shapes, as well as its ability to be truncated with limited signal loss, much like the Fourier
basis.

I. GENERAL PROCEDURE FOR SHAPE MATCHING

For this problem, we have many similar 3-dimensional shapes that are essentially the same ob ject in diﬀerent
positions. The goal of shape mapping is to ﬁnd which points on one mesh correspond to which points on another
mesh. The shapes are described by 3-d triangle meshes, which consist of a set of points in 3-dimensions, as well as
a set of edges that connect the points. Where necessary, a direction around each triangle can be determined by the
order in which the edges are listed.3 These meshes tend to contain about 12,000 points.
for each point in the mesh, we wish to create a vector containing all descriptors for that point. The most important
descriptors we use are intrinsic descriptors such as the Wave Kernel Signature (WKS)4 , Heat Kernel Signature (HKS)2 ,
Gaussian Curvature, and mean curvature. The Wave Kernel Signature and Heat Kernel Signature are measured by
initializing a delta function signal at the point, then measuring the value at that point while the signal propagates
along the mesh surface according to the Wave Equation and Heat Equation, respectively. WKS and HKS tend to
make up the ma jority of our descriptors, since we use the values at many diﬀerent time-steps, where each time-step
is treated as a distinct descriptor.
To match two meshes, we create a m x n descriptor matrix, where m is the number of points in the mesh and n
is the number of descriptors. The next step would be to match the rows of two such matrices using a least- squares
method. However, since the meshes in use tend to contain on the order of 12,000 points, we ﬁrst change to a much
smaller Laplace - Beltrami basis and perform least-squares matching with respect to the new basis functions.

II. USING DISCRIMINATIVE COMPONENT ANALYSIS (DCA) TO IMPROVE MAPPING

The main contribution of our pro ject is to apply Discriminative Component Analysis to the above matching process.
When the meshes are matched using their descriptor vectors, each descriptor is given an equal weight regardless of
its eﬀectiveness. We can achieve a much better matching by applying metric learning through the use of DCA. The
point of DCA is to create optimal descriptor vectors by weighting and mixing the existing descriptors. We are also
able to decrease the number of resulting descriptor dimensionality when using DCA. Below we describe the process
used:
We begin by choosing a small subset of points called salient points, which mark the same landmark features in each
mesh. For example we might choose the salient point 37 for each mesh to be the tip of the nose, while point 2 might
be chosen to be the left knee. In our pro ject we choose 40 such points. Our set of training data will then be the
descriptor vectors of all of these points from all meshes, where we refer to all such vectors from a particular salient
point as a chunklet.
Some chunklets are said to be discriminated from each other by negative constraints. We will aim to transform
descriptor vectors such that those from discriminated chunklets are as distinct as possible and those from the same
chunklet are as similar as possible. Note that not all chunklets are negatively constrained, in particular features that

2

a)

0

1

2

3

11996

11997

11998

11999

b)

0

1

2

3

4

196

197

198

199

200

0

1200 x 132
Descriptor Matrix

2

3

4

69 70 71

129 130 131 132

.

0

1

2

3

4

128

129

130

131

132

0

1

2

3

11996

11997

11998

11999

132 x 27
DCA
Descriptor Weigting

=

0

1

2

3

24 25 26 27

12000 x 27
Descriptor Matrix
(Weighted)

0

1

2

3

4

23 24 25 26 27

Laplace−Beltrami
Basis Change

.

0

1

2

3

11996

11997

11998

11999

11999

12000 x 27
Descriptor Matrix
(Weighted)

0

1

2

3

4

23 24 25 26 27

=

0

1

2

3

197

198

199

200

200 x 27
Descriptor Matrix
(L−B Basis)

0

1

2

3

4

23 24 25 26 27

FIG. 1: a) Begin with a m x n descriptor matrix, with m points in the mesh and n descriptors per point. The teal columns
correspond to WKS descriptors at diﬀerent time-steps, the green columns correspond to similar HKS descriptors, and the
last two columns correspond to the two curvature descriptors. By multiplying by the calculated DCA matrix, we reduce the
descriptor space to 27 optimized descriptors. b) To ﬁnd correspondence mapping, we ﬁrst change basis to the eigenstates of
the Laplace-Beltrami operator. Once in this basis we can match the descriptor matrix to that of another mesh, then transform
back to the point basis to get a correspondence mapping.

occur symmetrically in right / left pairs.
The DCA algorithm works by creating two matrices, Cb and Cw , which represent the variance between discriminated
chunklets and the variance among descriptors of the same chunklet respectively,5

Cb =

1
nb

n(cid:2)

(cid:2)

j=1

j∈Dj

(mj − mi )(mj − mi )T

Cw =

n(cid:2)

j=1

1
n

1
nj

nj(cid:2)

j=1

(xij − mj )(xij − mj )T

(1)

(2)

The matrix in equation (1) measures the average variance between negatively constrained chunklets. mj and mi
are the mean descriptor vectors of chunklets i and j, where j sums over all chunklets and i sums over all chunklets
negatively constrained to j. The matrix in equation (2) measures the average variance among points within a chunklet.
In the DCA algorithm, we ﬁrst diagonalize (1) to end up with a new basis that consists of the largest-valued
eigenvectors of (1). This represents the basis that best discriminates the chunklets. We transform (2) to this basis,
then diagonalize the resulting matrix. By taking the lowest-value eigenvectors we obtain a transformation that
maximizes variance across discriminated chunklets, and minimizes variance within a given chunklet. In this process
it is common to choose only a subset of the eigenvectors at each step, resulting in a descriptor space with much lower
dimensionality. In our pro ject, we are able to reduce the dimensionality of the descriptor space from roughly 130 to
about 25.

a)

b)

c)

3

FIG. 2: Visual comparison of mapping between two meshes. a) Reference mesh. b) Mapping calculated using raw descriptor
vectors. c) Mapping calculated using DCA weighting. The case with DCA weighting has noticeably better correspondence,
with particular attention to the pelvic region and elbows.

The end result From DCA we acquire the matrix A that transforms our raw descriptor vectors to the optimal learned
descriptor vectors. This can then be used for comparing two unknown test meshes by transforming the descriptor
vectors for the entire set of points. See FIG. 1.

III. FUNCTIONAL MAPS

Once we have two meshes with weighted descriptor matrices (in the Laplace-Beltrami basis), we can ﬁnd the
best correspondence through the use of Functional Maps. From the above methods we should know the descriptor
functions best preserved by a good map. We can use these ”probe functions” as constraints to ﬁnd a map between
basis functions on the two shapes1 . This then becomse a least squares problem:

Here F is a probe function written in the chosen basis (Laplace-Beltrami) on one mesh, and each corresponding
column of G is the same probe function written in the chosen basis on the other mesh.

minM ||F − M G||

(3)

IV. RESULTS

Mappings between test meshes were calculated both with and without DCA descriptor weighting. The average
error for each mapping was calculated as the average euclidean distance of each calculated point mapping to the
correct corresponding point. The error was consistently lower across the board when using DCA vs. using the raw
descriptors.

Average error with and without DCA with diﬀerent descriptor choices:
Using only WKS and HKS as descriptors: Training sets: error = 0.263 without DCA, 0.200 with DCA
Testing sets: error = 0.238 without DCA, 0.1915 with DCA
Using curvature with WKS and HKS: Training sets: error = 0.3123 without DCA, 0.198 with DCA
Testing sets: error = 0.3036 without DCA, 0.189 with DCA

To get an idea of the scale of the above error, the ﬁgures were approximately 2 units tall. Note that when curvature
values (both Gaussian and mean curvature) were used as descriptors, the error increased noticeably. This is not
unexpected, since adding certain descriptors has the possibility of increasing minimum variance within chunklets.

a)

5

10

15

20

25

30

b)

5

10

15

20

25

30

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

5

10

15

20

25

30

5

10

15

20

25

30

4

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

FIG. 3: Matrices containing relative error for all shape mappings amongst 30 test cases. ij th entry corresponds to (errweighted
- errraw ) value resulting from mapping mesh i to mesh j. Negative entries correspond to mappings where use DCA descriptor
weighting resulted in a better mapping. a) Error comparison where curvature is not included. b) Error comparison where
curvature is included. Note that each pixel corresponds to a comparison like in FIG. 2

Curvature in particular is known for producing this result, since the curvature of certain salient points can change
drastically with diﬀerent poses.
The important observation here is that the DCA learning algorithm deals with the curvature appropriately. The
curvature descriptors are properly incorporated into the learned descriptors in such a way to further decrease the
error, as compared even to the DCA-adjusted case without curvature. Another observation is that adding the extra
2 curvature descriptors results in 2 additional learned descriptors. This is not surprising, since curvature should
contain very diﬀerent information from the WKS and HKS descriptors, while many of the individual WKS and HKS
descriptors are similar and will be combined with each other in the DCA learning step.

We would like to thank Justin Solomon for providing us with his implementation of the functional maps framework.

Acknowledgments

[1] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, L. Guibas, Functional Maps: A Flexible Representation of Maps
Between Shapes ACM Transactions on Graphics 31, 4 (2012).
[2] Jian Sun, Maks Ovsjanikov, Leonidas Guibas. A Concise and Provably Informative Multi-scale Signature Based on Heat
Diﬀusion, Eurographics Symposium on Geometry Processing (SGP).
[3] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, J. Davis, SCAPE: shape completion and animation of people
ACM Transactions on Graphics 24, 408-416 (2005).
[4] M. Aubry, U. Schlickewey, D. Cremers. The wave kernel signature: A quantum mechanical approach to shape analysis,
Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1626-1633 (2011)
[5] Steven C. H. Hoi, Wei Liu, Michael R. Lyu, Wei-ying Ma. Learning distance metrics with contextual constraints for image
retrieva, Proc. Computer Vision and Pattern Recognition, 2072-2078 (2006)


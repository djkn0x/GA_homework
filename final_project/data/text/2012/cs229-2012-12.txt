Applying Machine Learning Algorithms to Oil Reservoir
Production Optimization

Mehrdad Gharib Shirangi
Stanford University

Abstract

Introduction

In well control optimization for an oil reservoir
described by a set of geological models, the ex-
pectation of net present value (NPV) is opti-
mized. This approach called robust optimiza-
tion, entails running the reservoir simulator for
all the reservoir models at each iteration of the
optimization algorithm. Hence, robust optimiza-
tion can be computationally demanding. One
way to reduce the computational burden, is to
select a subset of models and perform the op-
timization on the reduced set. Another popu-
lar technique, is using fast proxy models, rather
than full-physics simulators. In this work, a ker-
nel clustering technique is used to select a subset
of reservoir models that capture the range of un-
certainty in the response of the entire set.

In this work an adaptive strategy is used
to build fast proxy models for the NPV, and
then optimizing the proxy model using a pattern
search algorithm. The proxy model is generated
by training an artiﬁcial neural network (ANN)
or support vector regression model (SVR) using
some training examples. The challenge in build-
ing a proxy model is using ﬁnding good train-
ing examples. In this work, after optimizing the
proxy model, new training examples are gener-
ated around the current optimal point, and a
new proxy model is built and the procedure is
repeated. An example is presented that shows
the eﬃciency of kernel k-medoids clustering and
building proxy models for production optimiza-
tion.

Wang et al. (2012) applied retrospective opti-
mization to well placement problem. They ap-
plied k-means clustering for selecting the real-
izations at each subproblem. The focus of this
work is on well control optimization. A response
vector is introduced to well characterizes the dis-
similarity between the response of realizations.
Here, kernel k-medoids clustering is applied for
choosing a small set of statistically representa-
tive realizations.
Our ob jective is to ﬁnd an optimal well control
vector that maximizes the expectation of the life-
cycle NPV over the ensemble of reservoir models.
(cid:3)T ,
u = (cid:2)u1
The well control vector u, can be shown by
1 , · · · , u1
1 , · · · , uNc
Nw , · · · , uNc
Nw , u2
1 , u2
Nw
(1)
where, each subscript denotes the well index and
each superscript denotes the control index; Nw
denotes the number of wells and Nc denotes the
number of control steps for each well. un
j can
be either well pressure (BHP) or oil rate or total
liquid rate of the j th well at the nth simulation
time step.
In this work, the well controls are
the BHP’s. We only consider simple bound con-
straints.
In robust production optimization, we want to
maximize E [J (u, m)], where
Ne(cid:88)
j=1

E [J (u, m)] =

1
Ne

J (u, mj ),

(2)

where m denotes a realization of the reservoir
model, and J (u, mj ) denotes the corresponding
NPV.

1

Model Selection for Reducing Number
of Realizations

A simple way to reduce the computational cost,
is to select a few statistically representative mod-
els in the ensemble; these representative models
are selected based on ranking or clustering some
attributes. A random selection of the realiza-
tions will not provide a representative set for the
ensemble.
Wang et al. (2012) used k-means clustering
for selecting a set of representative realizations
from a large ensemble of models in ﬁeld develop-
ment optimization. The attributes they used for
clustering are OOIP, normalized cumulative oil
production, permeability distance, and oil-water
contact depth. They applied k-means clustering
to obtain a number of clusters, and then one re-
alization was chosen randomly from each cluster.
Scheidt and Caers (2009) developed an eﬀective
model selection method using kernel k-means
clustering in metric space. In this method, after
obtaining the clusters, the model nearest to the
centroid of that cluster (the medoid) is chosen.
In this work, kernel k-medoids clustering is
used, where the models are divided into some
clusters and the robust production optimization
is applied on the reduced ensemble which con-
tains the models corresponding to the clusters
centroids. As the center of each cluster repre-
sents all the models in that cluster, a weight is
assigned to each centroid. Finally, the weighted
expectation of NPV of the centroids of the clus-
nk(cid:88)
ters is maximized:
j=1
where nk denotes the number of clusters. With
Nj representing number of realizations in the j th
cluster, ωj = Nj is set as the weight of the j th
centroid in the ob jective function of the subprob-
lem.
In this work, the ﬁeld cumulative oil and wa-
ter production and the ﬁeld cumulative water in-
jection in time are used to construct a response
vector for each realization.
To obtain the response vectors, all realizations
are ran with the simulator using the same control

ωj J (u, mj , yj ),

(3)

maximize

1
Ne

2

strategy. At this stage, as we are only interested
in calculating a dissimilarity distance, the re-
sponses can be obtained from a rank-preserving
proxy-type simulator, e.g. the streamline simu-
lator, and need not be the exact simulator.

Building Fast Proxy Models

In this work, we use ANN and SVR to build fast
proxy models. One challenge in building a fast
proxy, is ﬁnding good training examples. Note
that the input space (well controls) is very high-
dimensional and continuous, hence, a randomly
generation of a few training examples may not
be very eﬃcient.

Support Vector Regression as a Proxy

SVR is a regression model that can be trained
to make predictions of output of a function for a
given vector of input features. The general idea
of SVR is to perform linear regression in a high
dimensional feature space by using kernel trick
(B. Scholkopf and Bartlett, 2005). In this work
we use a SVR software (Chang and Lin, 2011).
It is worthwhile to mention that for using the
SVR, all the input features should be scaled to be
between 0 and 1; same is about the output value.
Hence, here all the well controls (input features)
are scaled to be in [0, 1], while the output values
(-NPV), are also scaled to be in [0, 1]. In -SVR,
there are two important parameters that aﬀect
the eﬃciency of the algorithm, which are  and
C . In addition, the number of training examples
used also aﬀects the predictions.

Artiﬁcial Neural Network as a Proxy

Artiﬁcial neural network (ANN) is a popular ma-
chine learning algorithm inspired by biological
neural network. A neural network consists of
input nodes, output nodes and interconnected
groups of artiﬁcial neurons. In this work, we use
the neural network toolbox of MATLAB. The
idea is to build a ANN proxy of some training
examples, and then optimize the ANN model
rather than the computationally demanding full-
physics model.

3

(a) true porosity ﬁeld

(b) true ln(k) ﬁeld

(a) original space

(b) kernel space

Figure 1: True porosity and log-permeability ﬁelds.

Figure 2: MDS plot of 100 realizations with 7
clusters and their centroids.

Example

This example pertains to a two-dimensional hor-
izontal reservoir model with 28×30 uniform grid.
True porosity and log permeability ﬁelds with
well locations are shown in Fig. 1.
There are 7 producers and 2 injectors in this
reservoir. Ne = 100 realizations of both poros-
ity and log-permeability are generated using geo-
statistics and then conditioned to 300 days of
production history using truncated SVD param-
eterization (Gharib Shirangi, 2011). Production
optimization is performed for the time interval
of 300 to 3000 days.
The ob jective is ﬁrst to reduce the number
of realizations in robust optimization to signif-
icantly reduce the computational costs while ob-
taining nearly the same ﬁnal expected NPV and
characterization of uncertainty in the expected
NPV. Second ob jective is to test the eﬃciency
of using SVR and ANN in building fast proxy
models in production optimization.

Kernel Clustering

Figure 3: NPV values in time for 100 realizations
colored for diﬀerent clusters, markers
show the curves of the cluster cen-
troids.

solution, and plotted the corresponding P10, P50
and P90. As one can see, P10 and P50 values for
the exhaustive set at the ﬁnal time are in good
agreement with those from the cluster centroids.
This is of signiﬁcant importance. Fig. 3 shows
the unoptimized NPV (from initial guess) of all
realizations versus time, colored based on clus-
ters.

Fig. 2 shows the MDS plot and the points for
each of the 7 clusters. In uncertainty assessment
of production optimization, capturing P10, P50
and P90 of the uncertain NPV distribution is
very important. As one can see in Fig. 4(a), we
could achieve this goal by using only 7 clusters.
Fig. 4(b) shows the P10, P50 and P90 of the op-
timal NPV obtained by maximizing the expecta-
tion of NPV of cluster centroids. The optimiza-
tion was performed by ﬁrst generating an ANN
model and then optimizing the ANN model us-
ing pattern search algorithm (will be discussed
later). We ran all the models with the optimal

Building Fast Proxy Models

We ﬁrst perform a sensitivity study on parame-
ters of SVR ( C and  ). To do so, we generate
700 training examples, by randomly perturbing
the control vector around the mean. Out of these
training examples, we set the last 200 points as
testing set. Fig 5 shows the sensitivity of SVR
to parameters C , , and number of training ex-
amples. As can be seen, SVR has small testing
error even for small number of training examples.
In addition, a larger value of C and a smaller 
provides smaller testing error.

XY  ∅ P−1P−2 ∅P−3 ∅∅ P−4∅ P−5P−6 ∅∅ P−7⊗ I−1⊗ I−2510152025510152025300.050.10.150.20.250.3XY  ∅ P−1P−2 ∅P−3 ∅∅ P−4∅ P−5P−6 ∅∅ P−7⊗ I−1⊗ I−25101520255101520253012345678910−101x 108−505x 107−1012x 107−101−0.500.5−0.500.52468−5051015x 107NPV vs time stepstime stepsNPV $4

(a) Initial NPV

(a) sensitivity to C

(b) Optimized NPV

Figure 4: P10, P50 and P90 of all 100 realiza-
tions and those calculated using only
cluster centroids for the initial NPV,
and optimal NPV.

Next, we build a proxy model from a number
of training examples. Then we optimize the SVR
model using pattern search (PS) optimization al-
gorithm. Depending on how many training ex-
amples we use in generating the SVR model, the
solution from optimizing SVR can be diﬀerent.
Fig. 6 shows how the optimal NPV changes by in-
creasing the number of training examples. Fig. 7
shows the results of directly optimizing NPV us-
ing pattern search algorithm. While directly op-
timizing NPV takes about 800 function evalua-
tions, we could obtain the same optimal NPV
by optimizing an SVR model trained with about
100 function evaluations. Hence SVR can pro-
vide signiﬁcant computational saving in produc-
tion optimization.

Next we use ANN as a proxy. We use 1 hid-
den layer with 10 artiﬁcial neurons. Increasing
the number of hidden layers or neurons didnot
have a signiﬁcant eﬀect on the eﬃciency of ANN
model. Here, we use an adaptive strategy for
generating training examples. We ﬁrst gener-

(b) sensitivity to 

(c) number of training examples

Figure 5: Sensitivity of SVR (testing error) with re-
spect to parameters.

Figure 6: Change of optimal NPV from optimizing
SVR with number of training examples.

123456789−2024681012x 107time stepP10, P50, P90 of NPV distributionP10, P50, P90 from exhaustive set, and clsuter centroids  exhaustive setcluster centeroids12345678900.511.522.53x 108time stepP10, P50, P90 of NPV distributionP10, P50, P90 from exhaustive set, and clsuter centroids  exhaustive setcluster centeroids10−21001021041060.0080.010.0120.0140.0160.0180.02Relative Errorparameter C (cost)Testing error of epsilon−SVR for epsilon=0.1  # of TE=50# of TE=100# of TE=20010−610−410−21001020.0080.010.0120.0140.0160.0180.02Relative Errorparameter εError of epsilon−SVR for C=1  # of TE=50# of TE=100# of TE=200010020030040050000.020.040.060.080.1Relative ErrorNumber of Training ExamplesError of epsilon−SVR for ε= 0.1, C=10100200300400500−1.9−1.8−1.7−1.6−1.5−1.4−1.3x 108Number of training examples(simulation runs)−NPV ($)  optimal value from SVR modelcorresponding true value5

the examples here, we showed that optimizing
the mean NPV of only cluster centroids, would
optimize the P10 and P50 of the entire set of
models, and the P10 and P50 of the cluster cen-
troids at optimal solution are in close agreement
with those of the exhaustive set.
Water ﬂood optimization under geological un-
certainty, even over a few representative real-
izations, can be computationally expensive. In
this pro ject, support vector regression and arti-
ﬁcial neural network was applied to build proxy
models to use in production optimization. Both
SVR and ANN provided signiﬁcant computa-
tional savings, compared to optimizing without
using a proxy model.

References

B. Scholkopf, R. C. W., A. J. Smola and
P. L. Bartlett, 2005, New support vector algo-
rithms, Neural Computation, 12, 1207–1245.

Chang, C.-C. and C.-J. Lin, 2011, LIBSVM:
A library for support vector machines, ACM
Transactions on Intel ligent Systems and Tech-
nology, 2, 27:1–27:27.

Gharib Shirangi, M., 2011, History Matching
Production Data With Truncated SVD Param-
eterization, Master’s thesis, The University of
Tulsa.

Scheidt, C. and J. Caers, 2009, Representing spa-
tial uncertainty using distances and kernels,
Mathematical Geosciences, 41(4), 397–419.

Wang, H., D. Echeverra-Ciaurri, L. Durlofsky,
and A. Cominelli, 2012, Optimal well place-
ment under uncertainty using a retrospective
optimization framework, SPE Journal, 17(1),
112–121.

Figure 7: Directly optimizing NPV using pattern
search algorithm (without a proxy model).
Left axis shows -NPV while right axis
shows the number of function evaluations

Figure 8: Optimizing ANN proxy of NPV using pat-
tern search algorithm. Left axis shows -
NPV while right axis shows the number
of function evaluations

ate a few training examples, and we optimize
the ANN model. Having obtained the optimal
point, we run the simulator with the new point,
and generate new training examples around the
current point either by randomly perturbing the
current point or by polling in diﬀerent directions
around the current point. Fig. 8 shows the re-
sults of using ANN as a proxy. According to the
results of this ﬁgure, with only 60 function eval-
uations, the NPV increased to about the value
obtained by directly optimizing NPV.

Conclusions

In this work, unsupervised learning algorithm
is applied to the robust production optimiza-
tion problem to choose a set of representative
realizations. Kernel clustering technique applied
to NPV curves, provided a set of representative
models that capture the uncertainty in the re-
sponse of the entire set of models. At least for

020406080100120−2−1.5−1x 108−NPV $Patternsearch to directly optimize NPV02040608010012005001000  # of Simulations−NPVNPV increased from $ 1.203e+008to     $ 1.821e+008Cost: 850 simulation runs012345−2−1.5−1x 108Patternsearch Iteration−NPVPatternsearch to optimize ANN012345050# of Simulation RunsNPV increased from $ 1.203e+008to     $ 1.802e+008Cost: 60 simulation runs
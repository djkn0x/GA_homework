Submodular-Bregman and the Lov ´asz-Bregman
Divergences with Applications

Rishabh Iyer
Department of Electrical Engineering
University of Washington
rkiyer@u.washington.edu

Jeff Bilmes
Department of Electrical Engineering
University of Washington
bilmes@uw.edu

Abstract

We introduce a class of discrete divergences on sets (equivalently binary vectors)
that we call the submodular-Bregman divergences. We consider two kinds, deﬁned
either from tight modular upper or tight modular lower bounds of a submodular
function. We show that the properties of these divergences are analogous to the
(standard continuous) Bregman divergence. We demonstrate how they generalize
many useful divergences, including the weighted Hamming distance, squared
weighted Hamming, weighted precision, recall, conditional mutual information,
and a generalized KL-divergence on sets. We also show that the generalized
Bregman divergence on the Lov ´asz extension of a submodular function, which we
call the Lov ´asz-Bregman divergence, is a continuous extension of a submodular
Bregman divergence. We point out a number of applications, and in particular show
that a proximal algorithm deﬁned through the submodular Bregman divergence pro-
vides a framework for many mirror-descent style algorithms related to submodular
function optimization. We also show that a generalization of the k-means algorithm
using the Lov ´asz Bregman divergence is natural in clustering scenarios where
ordering is important. A unique property of this algorithm is that computing the
mean ordering is extremely efﬁcient unlike other order based distance measures.

1

Introduction

The Bregman divergence ﬁrst appeared in the context of relaxation techniques in convex programming
([4]), and has found numerous applications as a general framework in clustering ([2]), proximal
minimization ([5]) and online learning ([27]). Many of these applications are due to the nice properties
of the Bregman divergence, and the fact that they are parameterized by a single convex function.
They also generalize a large class of divergences on vectors. Recently Bregman divergences have
also been deﬁned between matrices ([26, 6]) and between functions ([8]).
In this paper we deﬁne a class of divergences between sets, where each divergence is parameterized by
a submodular function. This can alternatively and equivalently be seen as a divergence between binary
vectors in the same way that submodular functions are special cases of pseudo-Boolean functions
[3]. We call this the class of submodular Bregman divergences (or just submodular Bregman). We
show an interesting mathematical property of the submodular Bregman, namely that they can be
deﬁned based on either a tight modular (linear) upper bound or alternatively a tight modular lower
bound, unlike the traditional (continuous) Bregman deﬁnable only via a tight linear lower bound.
Let V refer to a ﬁnite ground set {1, 2, . . . , |V |}. A set function f : 2V → R is submodular if
∀S, T ⊆ V , f (S ) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ). Submodular functions have attractive properties
that make their exact or approximate optimization efﬁcient and often practical. Submodularity
can be seen as a discrete counterpart to convexity and concavity ([20]) and often the problems are
closely related ([1]). Indeed, as we shall see in this paper, the connections between submodularity

1

and convexity and concavity will help us formulate certain discrete divergences that are analogous
to the Bregman divergence. We in fact show a direct connection between a submodular Bregman
and a generalized Bregman divergence deﬁned through the Lov ´asz extension. Further background
on submodular functions may be found in the text [9].
An outline of the paper follows. We ﬁrst deﬁne the different types of submodular Bregman in
Section 2. We also deﬁne the Lov ´asz Bregman divergence, and show its relation to a version of
the submodular Bregman. Then in Section 3, we prove a number of properties of the submodular
Bregman and show how they are related to the Bregman divergence. Finally in Section 4, we show
how the submodular Bregman can be used in applications in machine learning. In particular, we show
how the proximal framework of the submodular Bregman generalizes a number of mirror-descent
style approximate submodular optimization algorithms. We also consider generalizations of the
k-means algorithm using the Lov ´asz Bregman divergence, and show how they can be used in
clustering applications where ordering or ranking is important.

2 The Bregman and Submodular Bregman divergences

Notation: We use φ to refer to a convex function, f to refer to a submodular function, and ˆf as
f ’s Lov ´asz extension. Lowercase characters x, y will refer to continuous vectors, while upper case
characters X, Y , S will refer to sets. We will also refer to the characteristic vectors of a set X as
1X ∈ {0, 1}V . Note that the characteristic vector of a set X , 1X is such that 1X (j ) = I (j ∈ X ),
where I (·) is the standard indicator function. We will refer to the ground set as V , and the cardinality
of the ground set as n = |V |. A divergence on vectors and sets is formally deﬁned as follows:
Given a domain of vectors or sets S (and if sets, S = a lattice of sets L, where L is a lattice if
∀X, Y ∈ L, X ∪ Y , X ∩ Y ∈ L), a function d : S × S → R+ is called a divergence if ∀x, y ∈ S,
d(x, y) ≥ 0 and ∀x ∈ S, d(x, x) = 0. For simplicity, we consider mostly the Boolean lattice L = 2V
but generalizations are possible as well [9].

(1)

2.1 Bregman and Generalized Bregman divergences
Recall the deﬁnition of the Bregman divergence: dφ : S × S → R+ as:
dφ (x, y) = φ(x) − φ(y) − (cid:104)∇φ(y), x − y(cid:105).
For non-differentiable convex functions we can extend equation (1) to deﬁne the generalized Bregman
divergence [13, 18]. Deﬁne a subgradient map Hφ , which for every vector y , gives a subgradient
Hφ (y) = hy ∈ ∂φ(y) [13], where ∂φ(y) is the subdifferential of φ at y .
Hφ
φ (x, y) = φ(x) − φ(y) − (cid:104)Hφ (y), x − y(cid:105), ∀x, y ∈ S.
(2)
d
When φ is differentiable, then ∂φ(x) = {∇φ(x)} and Hφ (y) = ∇φ(y). More generally, there may
be multiple distinct subgradients in the subdifferential, hence the generalized Bregman divergence is
parameterized both by φ and the subgradient-map Hφ . The generalized Bregman divergences have
also been deﬁned in terms of “extreme” subgradients [25, 18].
φ (x, y) = φ(x) − φ(y) + σ∂φ(y) (y − x), (3)
φ (x, y) = φ(x) − φ(y) − σ∂φ(y) (x − y)
d(cid:93)
d(cid:92)
and
Hφ
where, for a convex set C , σC (.) (cid:44) maxx∈C (cid:104)., x(cid:105). Clearly, we then have: d(cid:93)
φ (x, y) ≤ d
φ (x, y) ≤
φ (x, y), ∀Hφ which justiﬁes their being called the extreme generalized Bregman divergences [13].
d(cid:92)

2.2 The Submodular Bregman divergences

In a similar spirit, we deﬁne a submodular Bregman divergence parameterized by a submodular
function and deﬁned as the difference between the function and its modular (sometimes called linear)
bounds. Surprisingly, any submodular function has both a tight upper and lower modular bound
([15]), unlike strict convexity where only a tight ﬁrst-order lower bound exists. Hence, we deﬁne
two distinct forms of submodular Bregman parameterized by a submodular function and in terms
of either its tight upper or tight lower bounds.

2

2.2.1 Lower bound form of the Submodular Bregman
Given a submodular function f , the submodular polymatroid Pf , the corresponding base polytope
Bf and the subdifferential ∂f (Y ) (at a set Y ) for a submodular function f [9] are respectively:
Bf = Pf ∩ {x : x(V ) = f (V )}, and
Pf = {x : x(S ) ≤ f (S ), ∀S ⊆ V },
∂f (Y ) = {y ∈ RV : ∀X ⊆ V , f (Y ) − y(Y ) ≤ f (X ) − y(X )}.
Note that here y(S ) = (cid:80)
j∈S y(j ) is a modular function. In a manner similar to the generalized
Bregman divergence ([13]), we deﬁne a discrete subgradient map for a submodular function Hf ,
which for every set Y , picks a subgradient Hf (Y ) = hY ∈ ∂f (Y ). Then, given a submodular
function f and a subgradient-map Hf , the generalized lower bound submodular Bregman, which
Hf
we shall henceforth call d
, is deﬁned as:
f

(4)
(5)

Hf
d
f

(X, Y ) = f (X ) − f (Y ) − hY (X ) + hY (Y ) = f (X ) − f (Y ) − (cid:104)Hf (Y ), 1X − 1Y (cid:105)).

(6)

if i = 1
otherwise .

∀σY ∈ ΣY , hY ,σY (σY (i)) =

We remark here that similar to the deﬁnition of the generalized Bregman divergence, this submodular
Bregman is parameterized both by the submodular function f and the subgradient map Hf .
The subdifferential corresponding to a submodular function is an unbounded polyhedron [9], with
an uncountable number of possible subgradients. Its extreme points, however, are easy to ﬁnd
Hf
f with Hf
and characterize using the greedy algorithm [7]. Thus, we deﬁne a subclass of d
chosen so that it picks an extreme points of ∂f (Y ), which we will call the permutation based lower
bound submodular Bregman, henceforth referred to with dΣ
f . The extreme points of ∂f (Y ) can
be obtained via a greedy algorithm ([7, 9]) as follows: Let σ be a permutation of V and deﬁne
Si = {σ(1), σ(2), . . . , σ(i)} as its corresponding chain. We deﬁne ΣY as the set of permutations σY
such that their corresponding chains contain Y , meaning S|Y | = Y . Then we can deﬁne a subgradient
(cid:26)f (S1 )
hY ,σY (which is an extreme point of ∂f (Y )) where:
f (Si ) − f (Si−1 )
In the above, hY ,σY (Y ) = f (Y ). Hence deﬁne HΣ
f as a subgradient map which picks a subgradient
hY ,σY , for some Σ(Y ) = σY ∈ ΣY . Here we treat Σ as a permutation operator which, for a given set
Y , produces a permutation σY ∈ ΣY . Hence we can rewrite Eqn. (6), with the above subgradient as
f (X, Y ) = f (X ) − hY ,σY (X ) = f (X ) − (cid:104)HΣ
f (Y ), 1X (cid:105).
dΣ
Hf
As can readily be seen, the dΣ
. Similar to the extreme gen-
f are special cases of the d
f
eralized Bregman divergence above, we can deﬁne forms of the “extreme” lower bound
f (X, Y ) and d(cid:92)
submodular Bregman divergences d(cid:93)
f (X, Y ). Since in the case of a submod-
ular function ∂f (Y ) is an unbounded polyhedron, we restrict C = ∂f (Y ) ∩ Pf , and deﬁne:
f (X, Y ) = f (X ) − f (Y ) − σC (1X − 1Y ) and d(cid:92)
f (X, Y ) = f (X ) − f (Y ) + σC (1Y − 1X ) The
d(cid:93)
extreme lower bound submodular Bregman have very nice forms as shown in the theorem below:
Hf
(X, Y ) ≤ d(cid:92)
f (X, Y ) ≤ d
Theorem 2.1. For every hY ∈ ∂f (Y ) ∩ Pf , d(cid:93)
f (X, Y ). Similarly
f
f (X, Y ) ≤ d(cid:92)
f (X, Y ) ≤ dΣ
for every permutation map Σ, d(cid:93)
f (X, Y ). Further d(cid:93)
f (X, Y ) = f (X ) +
f (Y )− f (X ∩Y )− f (X ∪Y ). Similarly d(cid:92)
f (X, Y ) = f (X )− f (Y )− f (Y \X )− f (V )+ f (V \X \Y )
Hf
f . Further we see that d(cid:93)
The above theorem gives bounds for d
and dΣ
f is exactly the divergence
f
which deﬁnes the submodularity of f . Also notice that this is unlike the generalized Bregman
divergences, where the “extreme” forms may not be easy to obtain in general [13].

(7)

(8)

3

(11)

(12)

(13)

f (j |∅) − f (Y ).

f (j |V − {j }) +

f (j |X ) − f (Y ),

2.2.2 The upper bound submodular Bregman
(cid:88)
(cid:93) (X, Y ) (cid:44) f (X ) − (cid:88)
For submodular f , [23] established properties of submodular function using which we can deﬁne the
following divergences (which we call here the Nemhauser divergences):
f (j |X ∩ Y ) − f (Y )
f (j |X − {j }) +
(cid:88)
(cid:92) (X, Y ) (cid:44) f (X ) − (cid:88)
df
j∈Y \X
j∈X \Y
f (j |X ∪ Y − {j }) +
df
j∈Y \X
j∈X \Y
where f (j |X ) (cid:44) f (X ∪ j ) − f (X ). Similar to the approach in ([15]), we can relax the Nemhauser
1 (X, Y ) (cid:44) f (X ) − (cid:88)
(cid:88)
divergences to obtain three modular upper bound submodular Bregmans as:
f (j |X − {j }) +
f (j |∅) − f (Y ),
(cid:88)
2 (X, Y ) (cid:44) f (X ) − (cid:88)
df
j∈Y \X
j∈X \Y
f (j |X ) − f (Y ).
(cid:88)
3 (X, Y ) (cid:44) f (X ) − (cid:88)
df
j∈X \Y
j∈Y \X
f (j |V − {j }) +
df
j∈X \Y
j∈Y \X
We call these the Nemhauser based upper-bound submodular Bregmans of, respectively, type-I, II
2 and df
1 , df
and III. Henceforth, we shall refer to them as df
3 and when referring to them collectively,
we will use df
1:3 . The Nemhauser divergences are analogous to the extreme divergences of the
generalized Bregman divergences since they bound the Nemhauser based submodular Bregmans. Its
3 (X, Y ) ≥ df
1 (X, Y ) ≥ df
not hard to observe that for a submodular function f , df
(cid:93) (X, Y ). Similarly
3 (X, Y ) ≥ df
2 (X, Y ) ≥ df
df
(cid:92) (X, Y )
Hf
Similar to the generalized lower bound submodular Bregman d
, we deﬁne a generalized upper
f
bound submodular Bregman divergence dfG f in terms of any supergradient of f . Interestingly for
a submodular function, we can deﬁne a superdifferential ∂ f (X ) at X as follows:
∂ f (X ) = {x ∈ RV : ∀Y ⊆ V , f (X ) − x(X ) ≥ f (Y ) − x(Y )}.
Given a supergradient at X , gX ∈ ∂ f (X ), we can deﬁne a divergence dfG f , as:
dfG f (X, Y ) = f (X ) − f (Y ) − gX (X ) − gX (Y ) = f (X ) − f (Y ) − (cid:104)gX , 1X − 1Y (cid:105)
(15)
Similar to the subgradient map, we can deﬁne G f as the supergradient map, which picks a supergra-
dient from G f (X ) = gX ∈ ∂ f (X ). In fact, it can be shown that all three forms of df
1:3 are actually
special cases of dfG f , in that they form speciﬁc supergradient maps. Deﬁne three supergradients
1 , G f
X (with the corresponding maps G f
2 and G f
X (j ) = f (j |X − {j })
3 ) such that: g1
X and g3
X , g2
g1
X (j ) = f (j |V − {j }) for j ∈ X . Similarly let g1
X (j ) = f (j |∅)
and g2
X (j ) = g3
X (j ) = g3
X (j ) = f (j |X ) for j /∈ X . Then it can be shown [12] that g1
X ∈ ∂ f (X ), and
and g2
X , g3
X , g2
3 are special cases of dfG f .
2 and df
1 , df
correspondingly df
f (X ) = (cid:80)
dfG f also subsumes an interesting class of divergences for any submodular function representable as
X = (cid:80)
concave over modular. Consider any decomposable submodular function [24] f , representable as:
i λihi (mi (X )), where the hi s are (not necessarily smooth) concave functions and the mi s
are vectors in Rn . Let h(cid:48)
i λih(cid:48)
(cid:88)
i be any supergradient of hi . Then we deﬁne g cm
i (mi (X ))mi .
Further we can deﬁne a divergence deﬁned for a concave over modular function as:
λi (hi (mi (X )) − hi (mi (Y )) − hi (mi (X ))(mi (X ) − mi (Y ))
i

df
cm (X, Y ) =

(9)

(10)

(14)

(16)

cm is also a special case of dfG f with gX = g cm
Then it can be shown [12] that df
X when f is a
decomposable submodular function.

4

Hf
Table 1: Instances of weighted distances measures as special cases of d
f

Name
Hamming
Hamming
Recall
Precision
AER(Y , X ; Y )
Cond. MI
Itakura-Saito
Gen. KL

Type
Hf
d
f
df
Gf
Hf
d
f
df
Gf
Hf
d
f
d(cid:93)
f
df
Gf
df
Gf

and dfG f for w ∈ Rn
+
Hf (Y )/G f (X )
f (X )
d
2 · w (cid:12) 1Y
w(X \Y ) + w(Y \X )
w(X )
−2 · w (cid:12) 1X
−w(X )
w(X \Y ) + w(Y \X )
1 − w(X∩Y )
w(cid:12)1Y
1
w(Y )
w(Y )
− w(cid:12)1X|X |
1 − w(X∩Y )
-1
w(X )
1 − |Y |+|Y ∩X |
1Y
1
2|Y |
2|Y |
2
H (XX )
I (XX \Y ; XY \X |XX∩Y )
-
w(X ) − log w(Y )
w(X ) − 1
w(Y )
w
log w(X )
w(X )
w(X ) − w(Y ) + w(X ) −w(X ) log w(X ) −w(1 + log w(X ))
w(Y ) log w(Y )

Hf
and dfG f generalize a number of interesting distance measures like Hamming, recall,
Finally both d
f
precision, conditional mutual information, and weighted hamming. We show this in detail in [12],
and owing to lack of space brieﬂy summarize them in Table 1. The distance measures are shown in
weighted form, but cardinality based distances are special cases with w =1

2.3 The Lov ´asz Bregman divergence

The Lov ´asz extension ([20]) offers a natural connection between submodularity and convexity. The
Lov ´asz extension is a non-smooth convex function, and hence we can deﬁne a generalized Bregman
divergence ([13, 18]) which has a number of properties and applications analogous to the Bregman
divergence. Recall that the generalized Bregman divergence corresponding to a convex function
φ is parameterized by the choice of the subgradient map Hφ . The Lov ´asz extension of a submodular
function has a very interesting set of subgradients, which have a particularly nice structure in that
there is a very simple way of obtaining them [7].
Given a vector y , deﬁne a permutation σy such that y [σy (1)] ≥ y [σy (2)] ≥ · · · ≥ y [σy (n)]
(cid:80)n
and deﬁne Yk = {σy (1), · · · , σy (k)}. The Lov ´asz extension ([7, 20]) is deﬁned as: ˆf (y) =
k=1 y [σy (k)]f (σy (k)|Yk−1 ). For each point y , we can deﬁne a subdifferential ∂ ˆf (y), which has
a particularly nice form [9]: for any point y ∈ [0, 1]n , ∂ ˆf (y) = ∩{∂f (Yi )|i = 1, 2 · · · , n}. This
H ˆf
of the Lov ´asz extension, parameterized by a
naturally deﬁnes a generalized Bregman divergence d
ˆf
subgradient map H ˆf , which we can deﬁne as:
H ˆf
(x, y) = ˆf (x) − ˆf (y) − (cid:104)hy , x − y(cid:105), for some hy = H ˆf (y) ∈ ∂ ˆf (y).
ˆf

(17)

d

We can also deﬁne speciﬁc subgradients of ˆf at y as hy ,σy , with hy ,σy (σy (k)) = f (Yk )−f (Yk−1 ), ∀k .
These subgradients are really the extreme points of the submodular polyhedron. Then deﬁne the
Lov ´asz Bregman divergence d ˆf as the Bregman divergence of ˆf and the subgradient hy ,σy . Similar
f , it can be shown [12], that d ˆf (x, y) = ˆf (x) − (cid:104)hy ,σy , x(cid:105). Note that if the vector y is totally
to dΣ
ordered (no two elements are equal to each other), the subgradient of ˆf and the corresponding
permutation σy at y will actually be unique. When the vector is not totally ordered, we can consider
σy as a permutation operator which deﬁnes a valid and consistent total ordering for every vector y ,
and we can then deﬁne the Bregman divergence in terms of it. Note also that the points with no total
ordering in the interior of the hypercube is of measure zero. Hence for simplicity we just refer to the
Lov ´asz Bregman divergence as d ˆf . The Lov ´asz Bregman divergence is closely related to the lower
bound submodular Bregman, as we show below.
Theorem 2.2. The Lov ´asz Bregman divergences are an extension of the lower bound submodular
Bregman, over the interior of the hypercube. Further the Lov ´asz Bregman divergence can be expressed
as d ˆf (x, y) = (cid:104)x, hx,σx − hy ,σy (cid:105), and hence depends only x, the permutation σx and the permutation
of y(σy ), but is independent of the values of y .

5

3 Properties of the submodular Bregman and Lov ´asz Bregman divergences

In this section, we investigate some of the properties of the submodular Bregman and Lov ´asz
Bregman divergences which make these divergences interesting for Machine Learning applications.
We only state them here — for an elaborate discussion refer to [12]. All forms of the submodular
Bregman divergences are non-negative, and hence they are valid divergences. The lower bound
submodular Bregman is submodular in X for a given Y , while the upper bound submodular Bregman
is supermodular in Y for a given X . A direct consequence of this is that problems involving
optimization in X or Y (for example in ﬁnding the discrete representatives in a discrete k-means
like application which we consider in [12]), can be performed either exactly or approximately in
polynomial time. In addition to these the forms of the submodular Bregman divergence also satisfy
interesting properties like a characterization of equivalence classes, a form of set separation, a
generalized triangle inequality over sets and a form of both Fenchel and submodular duality. Finally
the generalized submodular Bregman divergence has an interesting alternate characterization, which
shows that they can potentially subsume a large number of discrete divergences. In particular, a
Hf
iff for any sets A, B ⊆ V , the set function fA (X ) = d(X, A) is
divergence d is of the form d
f
submodular in X and the set function d(X, A) − d(X, B ) is modular in X . Similarly a divergence d
is of the form dfG f iff, for any set A, B ⊆ V , the set function fA (Y ) = d(A, Y ) is supermodular in
Y and the set function d(A, Y ) − d(B , Y ) is modular in Y . These facts show that the generalized
Bregman divergences are potentially a very large class of divergences while Table 1 provides just a
few of them.
Additionally, the Lov ´asz Bregman divergence also has a number of very interesting properties.
points (e.g., f (X ) = (cid:112)|X |), d ˆf (x, y) = 0 if and only if σx = σy .
Notable amongst these is the fact that it has an interesting property related to permutations.
Theorem 3.1. [12] Given a submodular function whose polyhedron contains all possible extreme
Hence the Lov ´asz Bregman divergence can be seen as a divergence between the permutations. While
a number of distance measures capture the notion of a distance amongst orderings [17], the Lov ´asz
Bregman divergences has a unique feature not present in these distance measures. The Lov ´asz
Bregman divergences not only capture the distance between σx and σy , but also weighs it with the
value of x, thus giving preference to the values and not just the orderings. Hence it can be seen
as a divergence between a score x and a permutation σy , and hence we shall also represent it as
d ˆf (x, y) = d ˆf (x||σy ) = (cid:104)x, hx,σx − hx,σy (cid:105). Correspondingly, given a collection of scores, it also
measures how conﬁdent the scores are about the ordering. For example given two scores x and y
with the same orderings such that the values of x are nearly equal (low conﬁdence), while the values
of y have large differences, the distance to any other permutation will be more for y than x. This
property intuitively desirable in a permutation based divergence. Finally, as we shall see the Lov ´asz
Bregman divergences are easily amenable to k-means style alternating minimization algorithms for
clustering ranked data, a process that is typically difﬁcult using other permutation-based distances.

4 Applications

In this section, we show the utility of the submodular Bregman and Lov ´asz Bregman divergences by
considering some practical applications in machine learning and optimization. The ﬁrst application
is that of proximal algorithms which generalize several mirror descent algorithms. As a second
application, we motivate the use of the Lov ´asz Bregman divergence as a natural choice in clustering
where the order is important. Due to lack of space, we only concisely describe these applications,
and for a more elaborate discussion please see [12] where we also consider a third discrete clustering
application, and provide a clustering framework for the submodular Bregman with fast algorithms
for clustering sets of binary vectors

4.1 A proximal framework for the submodular Bregman divergence

The Bregman divergence has some nice properties related to a proximal method. In particular ([5]), let
ψ be a convex function that is hard to optimize, but suppose the function ψ(x) + λdφ (x, y) is easy to

6

Algorithm 1: Proximal Minimization Algorithm
X 0 = ∅
while until convergence do
X t+1 := argminX ∈S F (X ) + λd(X, X t )
t ← t + 1

optimize for a given ﬁxed y . Then a proximal algorithm, which starts with a particular x0 and updates
at every iteration xt+1 = argmaxxψ(x) + λdφ (x, xt ), is bound to converge to the global minima.
We deﬁne a similar framework for the sub-
modular Bregmans. Consider a set function
F , and an underlying combinatorial constraint
S . Optimizing this set function may not be
easy — e.g., if S is the constraint that X be a
graph-cut, then this optimization problem is NP
hard even if F is submodular ([15]). Consider
now a divergence d(X, Y ) that can be either an upper or lower bound submodular Bregman. Note,
the combinatorial constraints S are the discrete analogs of the convex set projection in the proximal
method. We offer a proximal minimization algorithm (Algorithm 1) in a spirit similar to [5].
Furthermore, Algorithm 1 is guaranteed to monotonically decrease the function value over the
iterations [12]. Interestingly, a number of approximate optimization problems considered in the past
turn out to be special cases of the proximal framework. We show this below:
Minimizing the difference between submodular (DS) functions: Consider the case where
F (X ) = f (X ) − g(X ) is a difference between two submodular functions f and g . This problem
is known to be NP hard and even NP hard to approximate [22, 11]. However there are a number
of heuristic algorithms which have been shown to perform well in practice [22, 11]. Consider ﬁrst:
g (X, X t ) (for some appropriate schedule Σt of permutations), with λ = 1 and
d(X, X t ) = dΣt
S = 2V . Then it can be shown trivially [12] that we obtain the submodular-supermodular (sub-sup)
1:3 (X t , X ), again with λ = 1 and S = 2V .
procedure ([22]). Moreover, we can deﬁne d(X, X t ) = df
Then again we can show [12] that we obtain the supermodular-submodular (sup-sub) procedure [11].
Finally deﬁning d(X, X t ) = df
g (X, X t ), we get the modular-modular (mod-mod)
1:3 (X t , X ) + dΣt
procedure [11]. Further, the sup-sub and mod-mod procedures can be used with more complicated
constraints like cardinality, matroid and knapsack constraints while the mod-mod algorithm can be
extended with even combinatorial constraints like the family of cuts, spanning trees, shortest paths,
covers, matchings, etc. [11]
Submodular function minimization: Algorithm 1 also generalizes a number of approximate
submodular minimization algorithms. If F is a submodular function and the underlying constraints
S represent the family of cuts, then we obtain the cooperative cut problem ([15], [14]) and one of
the algorithms developed in ([15]) is a special case of Algorithm 1. If S = 2V above, we get a
form of the approximate submodular minimization algorithm suggested for arbitrary (non-graph
representable) submodular functions ([16]). The proximal minimization algorithm also generalizes
three submodular function minimization algorithms IMA-I, II and III, described in detail in [12]
again with λ = 1, S = 2V and d = df
2 and df
1 , df
3 respectively. These algorithms are similar to
the greedy algorithm for submodular maximization [23]. Interestingly these algorithms provide
bounds to the lattice of minimizers of the submodular functions.
It is known [1] that the sets
A = {j : f (j |∅) < 0}, B = {j : f (j |V − {j }) > 0 are such that, for every minimizer X ∗ ,
A ⊆ X ∗ ⊆ B . Thus the lattice formed with A and B deﬁned as the join and meet respectively, gives
a bound on the minimizers, and we can restrict the submodular minimization algorithms to this lattice.
3 as a regularizer (which is IMA-III) and starting with X 0 = ∅ and X 0 = V , we
However using d = df
get the sets A and B [10, 12] respectively from Algorithm 1. With versions of algorithm 1 with d = df
1
2 , and starting respectively from X 0 = ∅ and X 0 = V , we get sets that provide a tighter
and d = df
bound on the lattice of minimizers than the one obtained with A and B . Further these algorithms
also provide improved bounds in the context of monotone submodular minimization subject to
combinatorial constraints. In particular, these algorithms provide bounds which are better than 1
ν ,
where ν is a parameter related to the curvature of the submodular function. Hence when the parameter
ν is a constant, these bounds are constant factor guarantees, which contrasts the O(n) bounds for
most of these problems. For a more elaborate and detailed discussion related to this, refer to [10]
Submodular function maximization:
If f is a submodular function, then using d(X, X v ) =
dΣv
f (X, X v ) forms an iterative algorithm for maximizing the modular lower bound of a submodular
This algorithm then generalizes a algorithms number of unconstrained submodular
function.
maximization and constrained submodular maximization, in that by an appropriate schedule of Σv
2 approximate algorithm and a 1 − 1
we can obtain these algorithms. Notable amongst them is a 1
e

7

//youtu.be/IqRhemUg14I
//youtu.be/kfEnLOmvEVc
Figure 1: Results of k-means clustering using the Lov ´asz Bregman divergence (two plots on the left)
and the Euclidean distance (two plots on the right). URLs above link to videos.

http:

http:

approximation algorithm for unconstrained and cardinality constrained submodular maximization
respectively. For a complete list of algorithms generalized by this, refer to [10].

4.2 Clustering framework with the Lov ´asz Bregman divergence

In this section we investigate a clustering framework similar to [2], using the Lov ´asz Bregman
Recall that the Lov ´asz
divergence and show how this is natural for a number of applications.
Bregman divergence in some sense measures the distance between the ordering of the vectors and can
be seen as a form of the “sorting” distance. We deﬁne the clustering problem as given a set of vectors,
ﬁnd a clustering into subsets of vectors with similar orderings. For example, given a set of voters and
their corresponding ranked preferences, we might want to ﬁnd subsets of voters who mostly agree.
Let X = {x1 , x2 , · · · , xm } represent a set of m vectors, such that ∀i, xi ∈ [0, 1]n . We ﬁrst consider
the problem of ﬁnding the representative of these vectors. Given a set of vectors X and a Lov ´asz
with minimum average distance, or in other words: σ = argminσ (cid:48) (cid:80)n
Bregman divergence d ˆf , a natural choice of a representative (in this case a permutation) is the point
i=1 d ˆf (xi ||σ (cid:48) ). Interestingly
for the Lov ´asz Bregman divergence this problem is easy and the representative permutation is exactly
the permutation of the arithmetic mean of X
argminσ (cid:48) (cid:80)n
(cid:80)n
Theorem 4.1.
the Lov ´asz Bregman representative
[12] Given a submodular function f ,
i=1 d ˆf (xi ||σ (cid:48) ) is exactly σ = σµ , µ = 1
i=1 xi
n
It may not sufﬁce to encode X using a single representative, and hence we partition X into disjoint
blocks C = {C1 , · · · , Ck } with each block having its own Lov ´asz Bregman representative, with the
this idea of clustering vectors into subsets of similar orderings: minM,C (cid:80)k
(cid:80)
set of representatives given by M = {σ1 , σ2 , · · · , σk }. Then we deﬁne an objective, which captures
d ˆf (xi , µj ).
xi∈Cj
j=1
Consider then a k-means like alternating algorithm [19, 21]. It has two stages, often called the as-
signment and the re-estimation step. In the assignment stage, for every point xi we choose its cluster
membership Cj such that j = argminl d ˆf (xi ||σl ). The re-estimation step involves ﬁnding the repre-
sentatives for every cluster Cj , which is exactly the permutation of the mean of the vectors in Cj . We
skip the algorithm here due to space constraints, and refer the reader to [12] for a complete discussion.
We remark here that a number of distance measures capture the notion of orderings, like the
bubble-sort distance [17], etc. However for these distance measures, ﬁnding the representative may
not be easy. The Lov ´asz Bregman divergence naturally captures the notion of distance between
orderings of vectors and yet, the problem of ﬁnding the representative in this case is very easy.
Finally similar to the analysis in [2, 12], we can show that the k-means algorithm using the Lov ´asz
Bregman divergence will monotonically decrease the objective at every iteration, and the algorithm
algorithm using the euclidean distance. We use the submodular function f (X ) = (cid:112)w(X ), for an
converges to a local minima. [12] To demonstrate the utility of our clustering framework, we
show some results in 2 and 3 dimensions (Fig. 1), where we compare our framework to a k-means
arbitrary vector w ensuring unique base extreme points. The results clearly show that the Lov ´asz
Bregman divergence clusters the data based on the orderings of the vectors.
Acknowledgments: We thank Stefanie Jegelka, Karthik Narayanan, Andrew Guillory, Hui Lin, John
Halloran and the rest of the submodular group at UW-EE for discussions. This material is based
upon work supported by the National Science Foundation under Grant No. (IIS-1162606), and is also
supported by a Google, a Microsoft, and an Intel research award.

8

In In Proc. ISIT, pages

References
[1] F. Bach. Learning with Submodular functions: A convex Optimization Perspective. Arxiv preprint, 2011.
[2] A. Banerjee, S. Meregu, I. S. Dhilon, and J. Ghosh. Clustering with Bregman divergences. JMLR,
6:1705–1749, Oct. 2005.
[3] E. Boros and P. L. Hammer. Pseudo-boolean optimization. Discrete Applied Math., 123(1–3):155 – 225,
2002.
[4] L. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the
solution of problems in convex programming. USSR Comput. Math and Math Physics, 7, 1967.
[5] Y. Censor and S. Zenios. Parallel optimization: Theory, algorithms, and applications. Oxford University
Press, USA, 1997.
[6] I. Dhillon and J. Tropp. Matrix nearness problems with Bregman divergences. SIAM Journal on Matrix
Analysis and Applications, 29(4):1120–1146, 2007.
[7] J. Edmonds. Submodular functions, matroids and certain polyhedra. Combinatorial structures and their
Applications, 1970.
[8] B. Frigyik, S. Srivastava, and M. Gupta. Functional Bregman divergence.
1681–1685. IEEE, 2008.
[9] S. Fujishige. Submodular functions and optimization, volume 58. Elsevier Science, 2005.
[10] R. Iyer and J. Bilmes. A framework of mirror descent algorithms for submodular optimization. To
Appear in NIPS Workshop on Discrete Optimization in Machine Learning (DISCML) 2012- Structure and
Scalability, 2012.
[11] R. Iyer and J. Bilmes. Algorithms for approximate minimization of the difference between submodular
functions, with applications. In Proc. UAI, 2012.
[12] R. Iyer and J. Bilmes. Submodular-Bregman and the Lov ´asz-Bregman Divergences with Applications:
Extended Version. 2012.
[13] R. Iyer and J. Bilmes. A uniﬁed theory on the generalized bregman divergences. Manuscript, 2012.
[14] S. Jegelka and J. Bilmes. Cooperative cuts: Graph cuts with submodular edge weights. Technical report,
Technical Report TR-189, Max Planck Institute for Biological Cybernetics, 2010.
[15] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In
Computer Vision and Pattern Recognition (CVPR), Colorado Springs, CO, June 2011.
[16] S. Jegelka, H. Lin, and J. Bilmes. Fast approximate submodular minimization. In Proc. NIPS, 2011.
[17] M. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.
[18] K. C. Kiwiel. Free-steering relaxation methods for problems with strictly convex costs and linear constraints.
Mathematics of Operations Research, 22(2):326–349, 1997.
[19] S. Lloyd. Least squares quantization in pcm. IEEE Transactions on IT, 28(2):129–137, 1982.
[20] L. Lov ´asz. Submodular functions and convexity. Mathematical Programming, 1983.
[21] J. MacQueen et al. Some methods for classiﬁcation and analysis of multivariate observations.
In
Proceedings of the ﬁfth Berkeley symposium on math. stats and probability, volume 1, pages 281–297.
California, USA, 1967.
[22] M. Narasimhan and J. Bilmes. A submodular-supermodular procedure with applications to discriminative
structure learning. In Uncertainty in Artiﬁcial Intelligence (UAI), Edinburgh, Scotland, July 2005.
[23] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of approximations for maximizing submodular set
functions—i. Mathematical Programming, 14(1):265–294, 1978.
[24] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In Proc. Neural
Information Processing Systems (NIPS), 2010.
[25] M. Telgarsky and S. Dasgupta. Agglomerative bregman clustering. In Proc. ICML, 2012.
[26] K. Tsuda, G. Ratsch, and M. Warmuth. Matrix exponentiated gradient updates for on-line learning and
Bregman projection. JMLR, 6(1):995, 2006.
[27] M. K. Warmuth. Online learning and Bregman divergences. Tutorial at the Machine Learning Summer
School, 2006.

9


Supervised Multi-Class Classiﬁcation of Tweets

Zahan Malkani
zahanm@stanford.edu

Evelyn Gillie
egillie@stanford.edu

14 December, 2012

Abstract

We present a study of a variety of supervised
learning methods used for the problem of Twit-
ter tweet classiﬁcation. This includes Support
Vector Machines (SVM), Neural Networks (NN),
Naive Bayes (NB), and Random Forests. We eva-
lutate these methods using their performance on
two sources of labeled data: an ‘attitudes’ dataset
that classiﬁes tweets with an attitude that the
tweeter might have had when composing it, and
a ‘topics’ dataset that classiﬁes tweets into one of
a (limited-domain) topic set. We ﬁnd that SVMs
out-perform the others in accuracy and when used
with feature selection have a reasonable runtime
too.
1 Introduction

The explosive growth of data being produced in the form
of short text snippets has lead to an equally voracious
growth in the methods used to analyze and decode these
snippets. Machine learning plays an important role in en-
abling such analyses, since the vastness of these datasets
vexes most hand-labeling attempts.
One of the more popular mechanisms producing this
data is Twitter. The medium is notorious for confound-
ing traditional text analysis methods, since the wealth of
context that older natural language processing techniques
depend on is simply missing in these tweets that are ar-
tiﬁcially conﬁned to being 140 characters in length. Ac-
cordingly, selecting informative features to extract from
the little context that we are given was a priority.
We explore a variety of supervised learning techniques
to use in two classiﬁcation tasks: 1), labelling tweets
with an attitude from the speaker’s perspective (this is
in contrast to most classiﬁcation systems, that catego-
rize tweets from the audience’s perspective, and usually
just has two classes, positive and negative), and 2)
labelling tweets based upon topics, where we have a pre-
deﬁned set of topics that we are looking for.
We pursue modern approaches to classifying tweets,
and explore their applications to real-world phenomena.
2 Related Work

Text classiﬁcation is a well traversed area of machine
learning, thanks to its potential for wide-reaching im-
pact. [1] and [2] present good overviews of the methods
used so far and their relative strengths and weaknesses.
In [1], Yang and Liu go over the theory behind classiﬁer

methods such as Support Vector Machines, Neural Nets,
k-Nearest Neighbor and Naive Bayes approaches. Their
results do not oﬀer much by way of quantitative outcomes
of using these methods to classify short text snippets as
we have. Lee makes some fascinating observations about
how humans perform (long form) document classiﬁcation
in [2], providing a Bayesian model that ﬁts how people
seems to reason about this problem. His approach seems
speciﬁc to the limited-domain topic classiﬁcation prob-
lem though, that will be relevant for the second dataset
in our paper.
In [3], Agarwal et. al. discuss polarity features as in-
puts for supervised learning methods, speciﬁcally SVMs,
in order to limit the feature space so that it can meaning-
fully be divided up by an SVM. These features, though
we hadn’t realized at the time, are similar in nature to the
conditional frequency features that we trained our SVMs
on eventually to capture the conditional probability of
features corresponding to classes.
O’Conner et. al. provide very useful Twitter-speciﬁc
tokenizers and emoticon analyzing algorithms in [4], and
the generalization of binary classiﬁcation trees to the
multi-class problem used by Lee and Oh in [7] is the same
as what we use for our multitree classiﬁcation algorithm.
3 Data

3.1 Data Collection

In August, we started collected Twitter data using Twit-
ter’s streaming API on an Elastic Cloud Compute in-
stance. This collected around 55-60 tweets per second
during peak hours, and 25-30 during non-peak hours.

3.1.1 Obama / Romney Dataset

We selected a sample of 800 tweets from October 28th for
Romney and 1300 tweets from November 4th for Obama.
We then had these tweets labeled with emotional atti-
tudes on Amazon Mechanical Turk using Amazon cer-
tiﬁed categorizers, getting two categorizations for each
tweet.
The emotional attitudes were chosen with consulation
from a social data analytics startup, interested in a sim-
ilar problem. Accordingly, we ended up with the set

{happy, mocking, funny, angry, sad,
hopeful, none / other}

Workers agreed on the label on only 42.3% of the sam-
ples. We manually reviewed a sample of the labels and

1

Emotion Percent of Dataset
0.45
Mocking
Funny
0.28
0.08
Sad
0.03
Angry
0.01
Happy
Hope
0.01
0.19
None

Table 1: Distribution of Classes in Emotions Dataset

found that for some tweets this is understandable, as
what one person may construe as mockery another can
readily deﬁne as anger, yet for a few tweets we would
not have agreed with either. After consulting with the
course staﬀ we ultimately decided that the data was good
enough to use as an experiment and left it at that. For
the rest of the computations, if there are two labels and
a classiﬁer predicts one of them, we consider it correct.

3.1.2 Topic Labeling Dataset

Our second dataset uses a sample stream from August
6th, in which the Olympics are beginning, the Mars Rover
had just landed, the Apple-Samsung lawsuit was in the
press, and Obama was expectedly in the news. Hence,
tweets are labeled by their topic:

{obama, olympics, mars, apple, none}

Overall, we collected around 3 million tweets, of which
we labeled 100,000 using Mechanical Turk. Of those
100,000, around 4,000 are labeled as being about any of
the above topics.
the topic distribution are shown in 2

Topic
Olympics
Apple
Mars
Obama

Percent of Dataset
0.40
0.23
0.22
0.15

Table 2: Distribution of Classes in Topical Dataset

3.2 Data Processing

We take each tweet through a pipeline of steps. First, we
lowercase the text to improve recall of unigram match-
ing. Then we tokenize it with the Twitter-speciﬁc tok-
enizer from [4], which preseves hashtags and emoticons.
Then we perform Porter stemming on the words, again to
improve recall of unigram matching. Then we construct
singleton and bigram features for the tweet. The other
features are discussed separately.

3.2.1 Twitter Speech Features

We wanted to more carefully consider the type of lan-
guage used in the these text snippets. We considered a
multitude of features that we chose ourselves, including

• Emoticons / Emo ji
• Punctuation (!!!!!)
• Capitalization (LMAOOOO)
• Dialogue (Retweets and @)
• Negation
• Sentence-level sentiment
• Cursing

We were able to see some small improvement to SVM
accuracy for certain classes through addition of these fea-
tures, as can be seen in table 3 (these tests were per-
formed on a small, high-accuracy dataset).

Bag-of-words
accuracy
emotion
angry
.9533
1
mocking
.9828
happy
.9792
sad
hopeful
.9811
1
funny
none
.9719

+ Twitter speciﬁc
precision
emotion
angry
.9659
.9699
mocking
.9710
happy
1
sad
hopeful
.9709
.9879
funny
none
.9713

Table 3: Adding Twitter Speech Features to SVM Input

It was disappointing to see that for most of the emo-
tions our engineered features hurt performance, so we’d
like to oﬀer a special shout-out to the angry tweets for
their user of upper-case and exclamation points.
3.2.2 Class Distribution Features

While exploring the performance of mocking and angry,
we decided to analyze the normalized distribution of the
sum of the weights by each token over an entire feature
set, believing that this would enable us to see where some
labels were nearly the winner only to just barely lose out
to another label. More precisely,
Let
• F = f0 ...fm be the vocabulary of features.
• S = s0 ...sn be the sample input such that si is de-
ﬁned by its features si = f0 f1 ...fk
• L = l0 ...ln be the corresponding known labels be a
multi-class vector.
(cid:80)
• W be a m × n matrix such that Wi,j =
∧ lk,j = 1), namely W counts the co-
I (fi sk
• for sample sk , vk as vk = (cid:80)
sk
occurences of features and labels.
fi sk
Upon analyzing the data we made the important real-
ization that these vectors were strongly correlated with
the labels, much more so than any element in isolation.
We quickly tried these as feature vectors in our SVM

Wi,j

2

framework and produced excellent results. We analyzed
the literature for similar feature selection, and it seems
Agarwal, et. al.
introduce a similar polarity-prior term
in [3]; our feature is an extension of this to the multi-class
case.
Ultimately, we realized that these are precisely the con-
ditional feature class counts that Naive Bayes exploits to
good eﬀect, but it provided a useful insight into the work-
ings of SVMs.
4 Algorithms

4.1 Feature Selection

Method
None
Frequency
MI
MI
MI
χ2
χ2
χ2

Threshold
N/A
2
0.01
0.1
0.2
0.1
5.0
20.0

features
6000
1093
1852
1716
1635
5989
1814
184

runtime
1m30s
25s
23s
20s
20s
20s
25s
10s

accuracy
0.92
0.93
0.92
0.93
0.96
0.87
0.91
0.89

Table 4: Results of an SVM run using feature selection.
(Using 700 examples of the topics dataset)

After stemming words and adding all words and bigrams
to the feature set, our feature space is in the thousands.
With methods such as SVM or random forests, we need
to trim down our feature set to get reasonable running
times. We tried three feature selection methods:
Our ﬁrst feature selector was a basic frequency pruner:
features that did not occur frequently at all were thrown
out (often, these were misspellings).
Our second feature selector was a mutual information
feature selector. Given a feature set F and class set C,
(cid:88)
the mutual information score is:
f ,c

P (f , c)
P (f )P (c)

(1)

I (F ; C ) =

P (f , c) log

then makes a prediction for an unlabeled point by cal-
culating the posterior probability for each class and pre-
dicting the maximizing assignment.

n(cid:89)
i

argmax
p(C = c)
c

p(F = fi |C = c)

(3)

Which translates to:

N11
N ln( N N01
N1.N .1 ) + N01
N ln( N N10
N0.N .1 ) + N10
N ln( N N11
N1.N .0 ) +
N00
N ln( N N00
N0.N .0 )
Where N is the total number of tweets, N11 is the
number of tweets of the given class containing the given
word, and N01 is the number of tweets of any other class
containing the given word.
Our third feature selector was χ2 feature selection:

(N11 + N10 + N01 + N00 ) ∗ (N11N00 − N10N01 )2
(N11 + N01 ) ∗ (N10+11 ) ∗ (N10 + N00 ) ∗ (N01 + N00 )

(2)

Mutual Information and χ2 feature selection is usually
formulated in terms of binarry classiﬁcation problems;
we extend it to our multi-class problem by throwing out
features whose maximum score with any class is below
some threshold. That is, is a feature does not score highly
with any class, we do not include it in our feature set.
Examples of high-scoring features for each feature se-
lection method are shown in 5

The independence assupmtions made in NB (as can be
seen in the second term of (3)), has the important side-
eﬀect of allowing a Naive Bayes classiﬁer handle a large
number of features with very little sacriﬁces in accuracy
due to the curse of dimensionality that often eﬀects other
learning algorithms. This is very important for text clas-
siﬁcation, where often the feature space size dwarfs the
other relevant dataset metrics.

We see this particular trait serve Naive Bayes well,
since it does rather well when using the simplest feature
set: bernoulli prescence vectors, leading to what is often
called the multivariate Bernoulli Naive Bayes classiﬁer.
The results are summarized in table 6.

Name
Romney dataset
Obama dataset
Topics labeling

size
800
1300
40,000

10-fold accuracy
54%
41%
78%

4.2 Naive Bayes

Table 6: Accuracy of Naive Bayes for each dataset

The ﬁrst learning algorithm that we tried was Naive
Bayes. Despite Naive Bayes’ assumption of feature in-
dependence (an assumption which doesn’t hold for nat-
ural language) it appears to perform resonably well on
our datasets. The algorithm works by learning the condi-
tional probability for each particular feature, conditioned
on the label of the training example being processed. It

After running Naive Bayes, we looked at the condi-
tional probabilities of each feature, and pulled out the
most informative ones for each class as shown in table
12.

3

Topical Dataset
χ2
Mutual Info
presid
googl
ipod
billionair
#teamusa
money
us
@marscurios
mar
planet
iphon
#msl
4s
money
@london2012
iphon
rover
basketball
planet
olymp

Obama

χ2
ridicul (sad)
lost (afraid)
nate (happy)
disturb (sad)
86% (happy)
lie (angry)
newspap (afraid)
cnn (sad)
die (angry)
random (afraid)

Mutual Info
vote (hopeful)
lie (angry)
elect (angry)
campaign (angry)
presid (angry)
left (angry)
tie (sad)
cnn (sad)
ﬁght (hopeful)
ﬁnal (hopeful)

Table 5: High-scoring features using feature selection

money
land,on
rais
@marscurios
app
what,you
safe
join
land
latest
busi
youtub

obama : olympi = 193.2 : 1.0
mars : olympi = 144.4 : 1.0
obama : olympi = 90.5 : 1.0
mars : apple = 77.9 : 1.0
apple : olympi = 60.0 : 1.0
mars : olympi = 53.9 : 1.0
mars : olympi = 53.9 : 1.0
obama : olympi = 50.3 : 1.0
mars : olympi = 47.3 : 1.0
mars : apple = 45.5 : 1.0
obama : olympi = 45.0 : 1.0
apple : olympi = 44.7 : 1.0

Table 7: NB Most Informative Features

4.4 Support Vector Machines

SVMs are widely known as an excellent learning model.
We used an implementation from the Milk[5] library, and
customized that as detailed below.
4.4.1 Multi-Class Classification

In order to work with with multi-class classiﬁcations, we
looked at two SVM models: one-vs-one and one-vs-rest.

Dataset
Attitudes
Attitudes
Topical
Topical

mode
one-vs-one
one-vs-rest
one-vs-one
one-vs-rest

accuracy
0.57
0.59
0.92
0.92

Table 9: SVM multiclass methods

One ﬁnal note is that Naive Bayes is also a very fast
classiﬁer – it handles a large feature space without a
huge time cost. Our implementation of NB classiﬁed all
datasets in under 5 seconds.

4.3 Neural Networks

The next machine learning algorithm we applied was an
Artiﬁcian Neural Network. We used a Tahn layer, Soft-
max output multiclass neural network. Using three hid-
den layers, we were able to achieve 86% accuracy over
the ﬁve-class topic dataset. ANNs outperformed Naive
Bayes, but with a huge time cost – running 10-fold cross-
validation in parallel over 700 examples took several min-
utes.

The one-vs-rest method appears to have slightly higher
acuracy over the attitudes dataset, so we proceed with
that.
We trained over the same features as in previous mod-
els (singletons, bigrams and twitter speech features), and
to preprocess the data, we used feature selection to re-
move linearly dependant features, and also ran stepwise
discriminant analysis (SDA). SDA functions by select-
ing the feature that is most correlated with preditions,
removing its variance from the other features and then
repeating the step to add more features.
Kernel: We experimented with two kernels: radial ba-
sis function and dot product. For the radial basis function
kernel, we performed a grid search over values of C from
2−2,−1,...3,4 and σ from -4 ... 4

Layers Feat Sel Threshold Accuracy
χ2
0.79
1.0
2
0.86
N/A
None
3
χ2
3
0.87
1.0
3
MI
0.1
0.87
0.86
0.2
MI
3
0.87
0.1
MI
3
χ2
4
0.1
0.83

Dataset mode
Attitudes RBF
Dot
Attitudes
RBF
Topical
Topical
Dot

accuracy
0.59
0.57
0.92
0.90

Table 10: A SVM Kernel Comparison

Table 8: Neural Net Accuracies Over the Topical Dataset

Again, the results are close, but the RBF kernel with
grid search appears to outperform the dot product kernel.

4

Using the feature selection techniques outlined earlier
and these optimal SVM settings, we were able to achieve
a very high accuracy over the topical dataset. Using χ2
feature selection, RBF kernel, and a one-vs-rest SVM
classiﬁer, we achieved 93% 10-fold cross-validation accu-
racy over all 4000 examples.

Figure 1: SVM accuracy and Dataset Size
Dataset)

(Topic

4.4.2 Class Distribution Features for Attitudes
Dataset

As discussed in the section on Class Distribution Fea-
tures, we had a breakthrough on the attitudes dataset
front, when we decided to use the class conditional proba-
bilities of each feature to construct our SVM input space.
We ended up settling on using just one input per class,
that was the sum of the condition probabilities of each
feature for that particular class, as seen in the training
data.
Since these features were presumably highly seperable
with the Radial Basis Function kernel, the SVM accu-
racy was greatly increased, and the runtime was brought
down considerably. Running 10-fold cross validation on
the datasets took no longer than 5 minutes. The results
for running a one-vs-rest SVM using these inputs over
the various attitudes datasets are presented in table 11.

Name
Romney dataset
Obama dataset
Romney + Obama dataset

size
800
1300
2100

10-fold accuracy
92%
90%
89%

e
g
a
t
n
e
c
r
e
P

Table 11: Accuracy of SVMs with Class Distribution Fea-
tures

4.5 Random Forest

We experimented with three random forest classiﬁers,
using one-against-one, one-against-rest, ECOC ([8], and
multi-tree learners. The multi-tree learner performed in
the 70% range on the topical dataset, using χ2 feature se-
lection, and ECOC, one-against-one and one-against-rest
yielded accuracies in the 90% range. We used stringent
feature selection (χ > 1.0) given the slowness of random
forests.

Learner
ECOC
Multi-tree
one-against-rest
one-against-one
one-against-one
one-against-one
one-against-rest

Training examples Accuracy
0.92
700
0.72
700
0.81
700
0.90
700
1000
0.90
0.90
2000
700
0.92

Table 12: Neural Net Accuracies

Despite its high accuracy, we decided not to try ECOC
with larger datasets due to its prohibitive slowness.
5 Case Studies

5.1 Fans Reacting to an Underdog Comeback

Now that we have a working model, we would like to eval-
uate it on real-world data. We collected approximately
80,000 tweets containg the keywords “Saints” or “Sea-
hawks” between 1:00 pm and 4:00 pm on the day of a
playoﬀ game between the two teams .
In this case, the 11-5 Saints were slated to dominate
the game against the 7-9 Seahawks. Surprisingly, the
Seahawks won the game, causing a ripple of emotions to
spread through the twitosphere. We captured that eﬀect
by analyzing the change in attitude distributions over
twenty minute intervals.

Tweets Expressing Mockery

Both Teams
Saints Only
Seahawks Only

0.4

0.35

0.3

0.25

0.2

0.15

We would expect this technique to do better with in-
creasing dataset size, but we suspect that internal vari-
ations in the datasets themselves (the Obama dataset
had more uninformative tweets like LOOOL!!) overpow-
ered this overall eﬀect.

1:00

1:20

1:40

2:00

2:20

2:40

3:00

3:20

3:40

We can see where the Saints took an early lead, at the
end of the ﬁrst quarter they were up 10-7. Over the next

5

Tweets Expressing Happiness

6 Conclusions

Overall, SVM using a grid search over a Radial Basis
Function Kernel outperformed Naive Bayes, neural nets,
and random forests. Random forest methods were able
to come close to SVM accuracy, but were considerably
slower. Further, our feature selection methods were able
to cut down dramatically on time without signiﬁcant ac-
curacy penalties.
Even on the ‘attitudes’ dataset, when used with our class
distribution features, the SVM classiﬁers performed well,
and seemed accurate enough to explore previously unseen
tweets about a real-world event.
7 Acknowledgements

We would like to thank Austin Gibbons for his consul-
tation on the pro ject. We would like to thank Richard
Socher for his help in guiding the pro ject. We would like
to thank the InfoLab for lending us their computational
and data resources.
References

[1] Yiming Yang and Xin Liu. A Re-examination
of
Text
Categorization
Methods
http:
//www.inf.ufes.br/~claudine/courses/ct08/
artigos/yang_sigir99.pdf

Classiﬁca-
Text
Lee.
Fast
[2] Michael
D.
Processes
Sampling
Sequential
Using
tion
http://lvk.cs.msu.su/~bruzz/articles/
classification/Fast%20Text%20Classification%
20Using%20Sequential.pdf

[3] Agarwal et al, Sentiment Analysis of Twitter Data,
2011.

[4] Brendan O’Connor, Michel Krieger, and David Ahn.
TweetMotif: Exploratory Search and Topic Summa-
rization for Twitter. ICWSM-2010.

[5] Milk. http://luispedro.org/software/milk

[6] NLTK http://nltk.org/

[7] Jin-Seon Lee and Il-Seok Oh. Binary Classiﬁ-
cation Trees for Multi-class Classiﬁcation Prob-
lems http://www.primaresearch.org/ICDAR2003/
Papers/0141_503_lee_j.pdf

[8] http://www.cs.cmu.edu/afs/cs/project/jair/
pub/volume2/dietterich95a.pdf

Both Teams
Saints Only
Seahawks Only

e
g
a
t
n
e
c
r
e
P

0.5

0.4

0.3

0.2

1:00

1:20

1:40

2:00

2:20

2:40

3:00

3:20

3:40

two hours, however, the Seahawks outscored the Saints
10-27. Then at the end of the game the Saints mounted
a brilliant 16-10 rebound, but unfortuantely it was too
little too late, and the Seahawks won the game.
Intuitively, these events align with the rise and fall
of fans’ emotions. We can even observe subtle diﬀer-
ences, for example the diﬀerence at the 3:00 mark be-
tween mocking and angry, where Saints fans are angry
at the Seahawks but not mocking them, while Saints fans
are angry at the Saints and the Seahawks fans are mock-
ing them.
We can see an interesting phenomenon with hopeful,
at the beginning of the game both sides are hopeful. Ini-
tially the Saints are expected to win and the Saints to
lose, so when the Saints take an early lead the Seahawks
lose their hope for an upset. When the tide is turned we
can see a resurgence in hope. The truly interesting part
is that henceforth both teams observe patterns related
to when they scored, yet the overall amount of hope de-
creases steadily after the ﬁrst half of the game, as fans
are hopeful for the teams at the beginning of the game
and then later pre-occupied with other emotions.

Tweets Expressing Hope

e
g
a
t
n
e
c
r
e
P

0.45

0.4

0.35

0.3

0.25

0.2

Both Teams
Saints Only
Seahawks Only

1:00

1:20

1:40

2:00

2:20

2:40

3:00

3:20

3:40

6


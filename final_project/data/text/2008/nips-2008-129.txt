Deﬂation Methods for Sparse PCA

Lester Mackey
Computer Science Division
University of California, Berkeley
Berkeley, CA 94703

Abstract

In analogy to the PCA setting, the sparse PCA problem is often solved by iter-
atively alternating between two subtasks: cardinality-constrained rank-one vari-
ance maximization and matrix deﬂation. While the former has r eceived a great
deal of attention in the literature, the latter is seldom analyzed and is typically
borrowed without justi ﬁcation from the PCA context. In this work, we demon-
strate that the standard PCA deﬂation procedure is seldom ap propriate for the
sparse PCA setting. To rectify the situation, we ﬁrst develo p several deﬂation al-
ternatives better suited to the cardinality-constrained context. We then reformulate
the sparse PCA optimization problem to explicitly reﬂect th e maximum additional
variance objective on each round. The result is a generalized deﬂation procedure
that typically outperforms more standard techniques on real-world datasets.

1

Introduction

Principal component analysis (PCA) is a popular change of variables technique used in data com-
pression, predictive modeling, and visualization. The goal of PCA is to extract several principal
components, linear combinations of input variables that together best account for the variance in a
data set. Often, PCA is formulated as an eigenvalue decomposition problem: each eigenvector of
the sample covariance matrix of a data set corresponds to the loadings or coefﬁcients of a principal
component. A common approach to solving this partial eigenvalue decomposition is to iteratively
alternate between two subproblems: rank-one variance maximization and matrix deﬂation. The ﬁrst
subproblem involves ﬁnding the maximum-variance loadings vector for a given sample covariance
matrix or, equivalently, ﬁnding the leading eigenvector of
the matrix. The second involves modifying
the covariance matrix to eliminate the inﬂuence of that eige nvector.

A primary drawback of PCA is its lack of sparsity. Each principal component is a linear combination
of all variables, and the loadings are typically non-zero. Sparsity is desirable as it often leads to
more interpretable results, reduced computation time, and improved generalization. Sparse PCA
[8, 3, 16, 17, 6, 18, 1, 2, 9, 10, 12] injects sparsity into the PCA process by searching for “pseudo-
eigenvectors ”, sparse loadings that explain a maximal amou nt variance in the data.

In analogy to the PCA setting, many authors attempt to solve the sparse PCA problem by itera-
tively alternating between two subtasks: cardinality-constrained rank-one variance maximization
and matrix deﬂation. The former is an NP-hard problem, and a v ariety of relaxations and approx-
imate solutions have been developed in the literature [1, 2, 9, 10, 12, 16, 17]. The latter subtask
has received relatively little attention and is typically borrowed without justi ﬁcation from the PCA
context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appro-
priate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several heuristic deﬂation
alternatives with more desirable properties. We then reformulate the sparse PCA optimization prob-
lem to explicitly reﬂect the maximum additional variance objective on each round. The result is a
generalized deﬂation procedure that typically outperform s more standard techniques on real-world
datasets.

1

The remainder of the paper is organized as follows. In Section 2 we discuss matrix deﬂation as it re-
lates to PCA and sparse PCA. We examine the failings of typical PCA deﬂation in the sparse setting
and develop several alternative deﬂation procedures. In Se ction 3, we present a reformulation of the
standard iterative sparse PCA optimization problem and derive a generalized deﬂation procedure
to solve the reformulation. Finally, in Section 4, we demonstrate the utility of our newly derived
deﬂation techniques on real-world datasets.

Notation

I is the identity matrix. Sp
+ is the set of all symmetric, positive semideﬁnite matrices i n Rp×p .
Card(x) represents the cardinality of or number of non-zero entries in the vector x.

2 De ﬂation methods

A matrix deﬂation modi ﬁes a matrix to eliminate the inﬂuence of a given eigenve
ctor, typically by
setting the associated eigenvalue to zero (see [14] for a more detailed discussion). We will ﬁrst
discuss deﬂation in the context of PCA and then consider its e xtension to sparse PCA.

2.1 Hotelling’s deﬂation and PCA

In the PCA setting, the goal is to extract the r leading eigenvectors of the sample covariance matrix,
A0 ∈ Sp
+ , as its eigenvectors are equivalent to the loadings of the ﬁr st r principal components.
Hotelling’s deﬂation method [11] is a simple and popular tec hnique for sequentially extracting these
eigenvectors. On the t-th iteration of the deﬂation method, we ﬁrst extract the lea
ding eigenvector
of At−1 ,

xT At−1x
xt = argmax
x:xT x=1
and we then use Hotelling’s deﬂation to annihilate xt :
t At−1xtxT
At = At−1 − xtxT
t .
The deﬂation step ensures that the t + 1-st leading eigenvector of A0 is the leading eigenvector of
At . The following proposition explains why.
Proposition 2.1. If λ1 ≥ . . . ≥ λp are the eigenvalues of A ∈ Sp
+ , x1 , . . . , xp are the corresponding
j for some j ∈ 1, . . . , p, then ˆA has eigenvectors x1 , . . . , xp
eigenvectors, and ˆA = A − xj xT
j Axj xT
with corresponding eigenvalues λ1 , . . . , λj−1 , 0, λj+1 , . . . , λp .
PROO F.

(1)

(2)

ˆAxj = Axj − xj xT
j Axj xT
j xj = Axj − xj xT
j Axj = λj xj − λj xj = 0xj .
ˆAxi = Axi − xj xT
j Axj xT
j xi = Axi − 0 = λixi , ∀i 6= j.

Thus, Hotelling’s deﬂation preserves all eigenvectors of a matrix and annihilates a selected eigen-
value while maintaining all others. Notably, this implies that Hotelling’s deﬂation preserves positive-
semideﬁniteness. In the case of our iterative deﬂation meth
od, annihilating the t-th leading eigen-
vector of A0 renders the t + 1-st leading eigenvector dominant in the next round.

2.2 Hotelling’s deﬂation and sparse PCA

In the sparse PCA setting, we seek r sparse loadings which together capture the maximum amount
of variance in the data. Most authors [1, 9, 16, 12] adopt the additional constraint that the loadings
be produced in a sequential fashion. To ﬁnd the ﬁrst such ”pse
udo-eigenvector ”, we can consider a
cardinality-constrained version of Eq. (1):

x1 =

argmax
x:xT x=1,Card(x)≤k1

xT A0x.

(3)

2

That leaves us with the question of how to best extract subsequent pseudo-eigenvectors. A common
approach in the literature [1, 9, 16, 12] is to borrow the iterative deﬂation method of the PCA setting.
Typically, Hotelling’s deﬂation is utilized by substituti ng an extracted pseudo-eigenvector for a true
eigenvector in the deﬂation step of Eq. (2). This substituti on, however, is seldom justi ﬁed, for the
properties of Hotelling’s deﬂation, discussed in Section 2 .1, depend crucially on the use of a true
eigenvector.

To see what can go wrong when Hotelling’s deﬂation is applied to a non-eigenvector, consider the
following example.
Example. Let C = (cid:18) 2 1
1 1 (cid:19), a 2 × 2 matrix. The eigenvalues of C are λ1 = 2.6180 and λ2 =
.3820. Let x = (1, 0)T , a sparse pseudo-eigenvector, and ˆC = C − xxT C xxT , the corresponding
ˆC = (cid:18) 0
1 (cid:19) with eigenvalues ˆλ1 = 1.6180 and ˆλ2 = −.6180. Thus,
1
1
Hotelling’s deﬂation does not in general preserve positive -semideﬁniteness when applied to a non-
eigenvector.

deﬂated matrix. Then

That Sp
+ is not closed under pseudo-eigenvector Hotelling’s deﬂati on is a serious failing, for most
iterative sparse PCA methods assume a positive-semideﬁnit e matrix on each iteration. A second,
related shortcoming of pseudo-eigenvector Hotelling’s deﬂation is its failure to render a pseudo-
eigenvector orthogonal to a deﬂated matrix. If A is our matrix of interest, x is our pseudo-eigenvector
with variance λ = xT Ax, and ˆA = A − xxT AxxT is our deﬂated matrix, then
ˆAx = Ax −
xxT AxxT x = Ax − λx is zero iff x is a true eigenvector. Thus, even though the “variance” of
x w.r.t. ˆA is zero (xT ˆAx = xT Ax − xT xxT AxxT x = λ − λ = 0), “covariances ” of the form
yT ˆAx for y 6= x may still be non-zero. This violation of the Cauchy-Schwarz inequality betrays a
lack of positive-semideﬁniteness and may encourage the rea ppearance of x as a component of future
pseudo-eigenvectors.

2.3 Alternative deﬂation techniques

In this section, we will attempt to rectify the failings of pseudo-eigenvector Hotelling’s deﬂation by
considering several alternative deﬂation techniques bett er suited to the sparse PCA setting. Note
that any deﬂation-based sparse PCA method (e.g. [1, 9, 16, 12 ]) can utilize any of the deﬂation
techniques discussed below.

2.3.1 Projection deﬂation

Given a data matrix Y ∈ Rn×p and an arbitrary unit vector in x ∈ Rp , an intuitive way to remove
the contribution of x from Y is to project Y onto the orthocomplement of the space spanned by x:
ˆY = Y (I − xxT ). If A is the sample covariance matrix of Y , then the sample covariance of ˆY is
given by ˆA = (I − xxT )A(I − xxT ), which leads to our formulation for projection deﬂation:
Projection deﬂation
At = At−1 − xtxT
t At−1 − At−1xtxT
t + xtxT
t At−1xtxT
t = (I − xtxT
t )At−1 (I − xtxT
(4)
t )
Note that when xt is a true eigenvector of At−1 with eigenvalue λt , projection deﬂation reduces to
Hotelling’s deﬂation:
t At−1xtxT
t + xtxT
t At−1 − At−1xtxT
At = At−1 − xtxT
t
t + λtxtxT
= At−1 − λtxtxT
t − λtxtxT
t
= At−1 − xtxT
t At−1xtxT
t .
However, in the general case, when xt is not a true eigenvector, projection deﬂation maintains th e
desirable properties that were lost to Hotelling’s deﬂatio n. For example, positive-semideﬁniteness
is preserved:

t )y = zT At−1 z
t )At−1 (I − xtxT
∀y , yT At y = yT (I − xtxT
t )y . Thus, if At−1 ∈ Sp
where z = (I − xtxT
+ , so is At . Moreover, At is rendered left and right
t )xt = xt − xt = 0 and At is symmetric. Projection deﬂation therefore
orthogonal to xt , as (I − xtxT
annihilates all covariances with xt : ∀v , vT Atxt = xT
t At v = 0.

3

2.3.2 Schur complement deﬂation

Since our goal in matrix deﬂation is to eliminate the inﬂuenc
e, as measured through variance and
covariances, of a newly discovered pseudo-eigenvector, it is reasonable to consider the conditional
variance of our data variables given a pseudo-principal component. While this conditional variance
is non-trivial to compute in general, it takes on a simple closed form when the variables are normally
distributed. Let x ∈ Rp be a unit vector and W ∈ Rp be a Gaussian random vector, representing the
joint distribution of the data variables. If W has covariance matrix Σ, then (W, W x) has covariance
matrix V = (cid:18) Σ
xT Σ xT Σx (cid:19), and V ar(W |W x) = Σ − ΣxxT Σ
Σx
xT Σx whenever xT Σx 6= 0 [15].
That is, the conditional variance is the Schur complement of the vector variance xT Σx in the full
covariance matrix V . By substituting sample covariance matrices for their population counterparts,
we arrive at a new deﬂation technique:
Schur complement deﬂation
At−1xtxT
t At−1
xT
t At−1xt
Schur complement deﬂation, like projection deﬂation, pres
erves positive-semideﬁniteness. To
+ . Then, ∀v , vT At v = vT At−1 v − vT At−1 xt xT
t At−1 v
see this, suppose At−1 ∈ Sp
≥ 0 as
xT
t At−1 xt
t At−1xt − (vT At−1xt )2 ≥ 0 by the Cauchy-Schwarz inequality and xT
vT At−1 vxT
t At−1xt ≥ 0
as At−1 ∈ Sp
+ .
Furthermore, Schur complement deﬂation renders xt left and right orthogonal to At , since At is
symmetric and Atxt = At−1xt − At−1 xt xT
t At−1 xt
= At−1xt − At−1xt = 0.
xT
t At−1 xt
Additionally, Schur complement deﬂation reduces to Hotell
ing’s deﬂation when xt is an eigenvector
of At−1 with eigenvalue λt 6= 0:

At = At−1 −

(5)

At = At−1 −

At−1xtxT
t At−1
xT
t At−1xt
λtxtxT
t λt
= At−1 −
λt
t At−1xtxT
= At−1 − xtxT
t .
While we motivated Schur complement deﬂation with a Gaussian ity assumption, the technique ad-
mits a more general interpretation as a column projection of a data matrix. Suppose Y ∈ Rn×p
is a mean-centered data matrix, x ∈ Rp has unit norm, and ˆY = (I − Y xxT Y T
||Y x||2 )Y , the projection
of the columns of Y onto the orthocomplement of the space spanned by the pseudo-principal com-
ponent, Y x. If Y has sample covariance matrix A, then the sample covariance of ˆY is given by
||Y x||2 )Y = A − AxxT A
||Y x||2 )T (I − Y xxT Y T
n Y T (I − Y xxT Y T
n Y T (I − Y xxT Y T
ˆA = 1
||Y x||2 )Y = 1
xT Ax .
2.3.3 Orthogonalized deﬂation

While projection deﬂation and Schur complement deﬂation add
ress the concerns raised by per-
forming a single deﬂation in the non-eigenvector setting, n ew difﬁculties arise when we attempt to
sequentially deﬂate a matrix with respect to a series of non-orthogonal pseudo-eigenvectors.

Whenever we deal with a sequence of non-orthogonal vectors, we must take care to distinguish
between the variance explained by a vector and the additional variance explained, given all pre-
vious vectors. These concepts are equivalent in the PCA setting, as true eigenvectors of a matrix
are orthogonal, but, in general, the vectors extracted by sparse PCA will not be orthogonal. The
additional variance explained by the t-th pseudo-eigenvector, xt , is equivalent to the variance ex-
plained by the component of xt orthogonal to the space spanned by all previous pseudo-eigenvectors,
qt = xt − Pt−1xt , where Pt−1 is the orthogonal projection onto the space spanned by x1 , . . . , xt−1 .
On each deﬂation step, therefore, we only want to eliminate t he variance associated with qt . Anni-
hilating the full vector xt will often lead to “double counting” and could re-introduce
components
parallel to previously annihilated vectors. Consider the following example:

4

√2
√2
Example. Let C0 = I .
If we apply projection deﬂation w.r.t.
2 )T , the result is
x1 = (
2 ,
2 − 1
C1 = (cid:18) 1
2 (cid:19), and x1 is orthogonal to C1 . If we next apply projection deﬂation to C1 w.r.t.
2
1
− 1
2
x2 = (1, 0)T , the result, C2 = (cid:18) 0
2 (cid:19), is no longer orthogonal to x1 .
0
1
0
The authors of [12] consider this issue of non-orthogonality in the context of Hotelling’s deﬂation.
Their modi ﬁed deﬂation procedure is equivalent to Hotellin
g’s deﬂation (Eq. (2)) for
t = 1 and can
be easily expressed in terms of a running Gram-Schmidt decomposition for t > 1:

qt =

Orthogonalized Hotelling’s deﬂation (OHD)
(I − Qt−1QT
t−1 )xt
t−1 )xt (cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(I − Qt−1QT
At = At−1 − qt qT
t At−1 qt qT
t
where q1 = x1 , and q1 , . . . , qt−1 form the columns of Qt−1 . Since q1 , . . . , qt−1 form an orthonormal
basis for the space spanned by x1 , . . . , xt−1 , we have that Qt−1QT
t−1 = Pt−1 , the aforementioned
orthogonal projection.

(6)

Since the ﬁrst round of OHD is equivalent to a standard applic ation of Hotelling’s deﬂation, OHD
inherits all of the weaknesses discussed in Section 2.2. However, the same principles may be applied
to projection deﬂation to generate an orthogonalized varia nt that inherits its desirable properties.

Schur complement deﬂation is unique in that it preserves ort hogonality in all subsequent rounds.
That is, if a vector v is orthogonal to At−1 for any t, then At v = At−1 v − At−1 xt xT
t At−1 v
= 0 as
xT
t At−1 xt
At−1 v = 0. This further implies the following proposition.
Proposition 2.2. Orthogonalized Schur complement deﬂation is equivalent to Schur complement
deﬂation.

Proof. Consider the t-th round of Schur complement deﬂation. We may write xt = ot + pt , where
pt is in the subspace spanned by all previously extracted pseudo-eigenvectors and ot is orthogonal
to this subspace. Then we know that At−1 pt = 0, as pt is a linear combination of x1 , . . . , xt−1 ,
and At−1xi = 0, ∀i < t. Thus, xT
t At ot .
t Atxt = pT
t Atpt + oT
t Atpt + pT
t At ot + oT
t At ot = oT
Further, At−1xtxT
t At−1 +At−1 otpT
t At−1 +At−1 pt oT
t At−1 = At−1 ptpT
t At−1 +At−1 ot oT
t At−1 =
t At−1 . Hence, At = At−1 − At−1 ot oT
= At−1 − At−1 qt qT
t At−1
t At−1
as qt = ot
.
At−1 ot oT
oT
qT
||ot ||
t At−1 ot
t At−1 qt

Table 1 compares the properties of the various deﬂation tech niques studied in this section.

Method
Hotelling’s
Projection
Schur complement
Orth. Hotelling’s
Orth. Projection

X

t Atxt = 0 Atxt = 0 At ∈ Sp
xT
+ Asxt = 0, ∀s > t
×
×
×
X
×
X
X
X
×
X

X
×
X

X
×
X

X

X

X

Table 1: Summary of sparse PCA deﬂation method properties

3 Reformulating sparse PCA

In the previous section, we focused on heuristic deﬂation te chniques that allowed us to reuse the
cardinality-constrained optimization problem of Eq. (3). In this section, we explore a more princi-
pled alternative: reformulating the sparse PCA optimization problem to explicitly reﬂect our maxi-
mization objective on each round.

Recall that the goal of sparse PCA is to ﬁnd r cardinality-constrained pseudo-eigenvectors which
together explain the most variance in the data. If we additionally constrain the sparse loadings to

5

be generated sequentially, as in the PCA setting and the previous section, then a greedy approach of
maximizing the additional variance of each new vector naturally suggests itself.

On round t, the additional variance of a vector x is given by qT A0 q
qT q where A0 is the data covari-
ance matrix, q = (I − Pt−1 )x, and Pt−1 is the projection onto the space spanned by previous
pseudo-eigenvectors x1 , . . . , xt−1 . As qT q = xT (I − Pt−1 )(I − Pt−1 )x = xT (I − Pt−1 )x, max-
imizing additional variance is equivalent to solving a cardinality-constrained maximum generalized
eigenvalue problem,

xT (I − Pt−1 )A0 (I − Pt−1 )x
max
x
subject to xT (I − Pt−1 )x = 1
Card(x) ≤ kt .

(7)

If we let qs = (I − Ps−1 )xs , ∀s ≤ t − 1, then q1 , . . . , qt−1 form an orthonormal basis for the space
spanned by x1 , . . . , xt−1 . Writing I − Pt−1 = I − Pt−1
s = Qt−1
s ) suggests a
s=1 qs qT
s=1 (I − qs qT
generalized deﬂation technique that leads to the solution o f Eq. (7) on each round. We imbed the
technique into the following algorithm for sparse PCA:

Algorithm 1 Generalized Deﬂation Method for Sparse PCA
Given: A0 ∈ S p
+ , r ∈ N, {k1 , . . . , kr } ⊂ N
Execute:
1. B0 ← I
2. For t := 1, . . . , r
• xt ←

xT At−1x

argmax
x:xT Bt−1 x=1,Card(x)≤kt
• qt ← Bt−1xt
t )At−1 (I − qt qT
• At ← (I − qt qT
t )
• Bt ← Bt−1 (I − qt qT
t )
• xt ← xt/ ||xt ||
Return: {x1 , . . . , xr }

Adding a cardinality constraint to a maximum eigenvalue problem renders the optimization problem
NP-hard [10], but any of several leading sparse eigenvalue methods, including GSLDA of [10],
DCPCA of [12], and DSPCA of [1] (with a modi ﬁed trace constrai nt), can be adapted to solve this
cardinality-constrained generalized eigenvalue problem.

4 Experiments

In this section, we present several experiments on real world datasets to demonstrate the value added
by our newly derived deﬂation techniques. We run our experim ents with Matlab implementations
of DCPCA [12] (with the continuity correction of [9]) and GSLDA [10], ﬁtted with each of the
following deﬂation techniques: Hotelling’s (HD), project
ion (PD), Schur complement (SCD), or-
thogonalized Hotelling’s (OHD), orthogonalized projection (OPD), and generalized (GD).

4.1 Pit props dataset

The pit props dataset [5] with 13 variables and 180 observations has become a de facto standard for
benchmarking sparse PCA methods. To demonstrate the disparate behavior of differing deﬂation
methods, we utilize each sparse PCA algorithm and deﬂation t echnique to successively extract six
sparse loadings, each constrained to have cardinality less than or equal to kt = 4. We report the
additional variances explained by each sparse vector in Table 2 and the cumulative percentage vari-
ance explained on each iteration in Table 3. For reference, the ﬁrst 6 true principal components of
the pit props dataset capture 87% of the variance.

6

HD
2.938
2.209
0.935
1.301
1.206
0.959

PD
2.938
2.209
1.464
1.464
1.057
0.980

DCPCA
OHD
SCD
2.938
2.938
2.209
2.076
0.935
1.926
1.164
0.799
0.901
1.477
0.725
0.431

OPD
2.938
2.209
1.464
1.464
1.058
0.904

GD
2.938
2.209
1.477
1.464
1.178
0.988

HD
2.938
2.107
1.988
1.352
1.067
0.557

PD
2.938
2.280
2.067
1.304
1.120
0.853

GSLDA
OHD
SCD
2.938
2.938
2.107
2.065
1.985
2.243
1.120
1.335
0.497
1.164
0.841
0.489

OPD
2.938
2.280
2.067
1.305
1.125
0.852

GD
2.938
2.280
2.072
1.360
1.127
0.908

Table 2: Additional variance explained by each of the ﬁrst 6 s parse loadings extracted from the Pit
Props dataset.

On the DCPCA run, Hotelling’s deﬂation explains 73.4% of the variance, while the best performing
methods, Schur complement deﬂation and generalized deﬂati
on, explain approximately 79% of the
variance each. Projection deﬂation and its orthogonalized variant also outperform Hotelling’s deﬂa-
tion, while orthogonalized Hotelling’s shows the worst performance with only 63.2% of the variance
explained. Similar results are obtained when the discrete method of GSLDA is used. Generalized
deﬂation and the two projection deﬂations dominate, with GD achieving the maximum cumulative
variance explained on each round. In contrast, the more standard Hotelling’s and orthogonalized
Hotelling’s underperform the remaining techniques.

HD
22.6%
39.6%
46.8%
56.8%
66.1%
73.4%

PD
22.6%
39.6%
50.9%
62.1%
70.2%
77.8%

DCPCA
OHD
SCD
22.6%
22.6%
39.6%
38.6%
46.8%
53.4%
52.9%
62.3%
59.9%
73.7%
79.3%
63.2%

OPD
22.6%
39.6%
50.9%
62.1%
70.2%
77.2%

GD
22.6%
39.6%
51.0%
62.2%
71.3%
78.9%

HD
22.6%
38.8%
54.1%
64.5%
72.7%
77.0%

PD
22.6%
40.1%
56.0%
66.1%
74.7%
81.2%

GSLDA
OHD
SCD
22.6%
22.6%
38.8%
38.5%
54.1%
55.7%
64.3%
64.4%
68.2%
73.3%
79.8%
71.9%

OPD
22.6%
40.1%
56.0%
66.1%
74.7%
81.3%

GD
22.6%
40.1%
56.1%
66.5%
75.2%
82.2%

Table 3: Cumulative percentage variance explained by the ﬁr st 6 sparse loadings extracted from the
Pit Props dataset.

4.2 Gene expression data

The Berkeley Drosophila Transcription Network Project (BDTNP) 3D gene expression data
[4] contains gene expression levels measured in each nucleus of developing Drosophila em-
bryos and averaged across many embryos and developmental stages. Here, we analyze 0-
3 1160524183713 s10436-29ap05-02.vpc, an aggregate VirtualEmbryo containing 21 genes and
5759 example nuclei. We run GSLDA for eight iterations with cardinality pattern 9,7,6,5,3,2,2,2
and report the results in Table 4.

GSLDA additional variance explained
OPD
OHD
SCD
PD
1.784
1.784
1.784
1.784
1.453
1.464
1.453
1.453
1.178
1.176
1.179
1.178
0.721
0.713
0.716
0.736
0.574
0.571
0.460
0.571
0.244
0.354
0.278
0.306
0.313
0.239
0.262
0.256
0.239
0.299
0.257
0.245

HD
1.784
1.464
1.178
0.716
0.444
0.303
0.271
0.223

GD
1.784
1.466
1.187
0.743
0.616
0.332
0.304
0.329

HD
21.0%
38.2%
52.1%
60.5%
65.7%
69.3%
72.5%
75.1%

GSLDA cumulative percentage variance
OPD
OHD
SCD
PD
21.0%
21.0%
21.0%
21.0%
38.1%
38.2%
38.1%
38.1%
51.9%
52.0%
52.0%
51.9%
60.4%
60.4%
60.4%
60.6%
67.4%
67.1%
65.9%
67.1%
70.0%
70.0%
70.4%
71.0%
73.7%
72.8%
73.4%
74.0%
76.8%
77.0%
75.9%
76.6%

GD
21.0%
38.2%
52.2%
61.0%
68.2%
72.1%
75.7%
79.6%

PC 1
PC 2
PC 3
PC 4
PC 5
PC 6
PC 7
PC 8

Table 4: Additional variance and cumulative percentage variance explained by the ﬁrst 8 sparse
loadings of GSLDA on the BDTNP VirtualEmbryo.

The results of the gene expression experiment show a clear hierarchy among the deﬂation methods.
The generalized deﬂation technique performs best, achievi ng the largest additional variance on every
round and a ﬁnal cumulative variance of 79.6%. Schur complem ent deﬂation, projection deﬂation,
and orthogonalized projection deﬂation all perform compar ably, explaining roughly 77% of the total
variance after 8 rounds. In last place are the standard Hotelling’s and orthogonalized Hotelling’s
deﬂations, both of which explain less than 76% of variance af ter 8 rounds.

7

5 Conclusion

In this work, we have exposed the theoretical and empirical shortcomings of Hotelling’s deﬂation in
the sparse PCA setting and developed several alternative methods more suitable for non-eigenvector
deﬂation. Notably, the utility of these procedures is not li mited to the sparse PCA setting. Indeed,
the methods presented can be applied to any of a number of constrained eigendecomposition-based
problems, including sparse canonical correlation analysis [13] and linear discriminant analysis [10].

Acknowledgments

This work was supported by AT&T through the AT&T Labs Fellowship Program.

References
[1] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A Direct Formulation for
Sparse PCA using Semideﬁnite Programming. In Advances in Ne ural Information Processing
Systems (NIPS). Vancouver, BC, December 2004.
[2] A. d’Aspremont, F. R. Bach, and L. E. Ghaoui. Full regularization path for sparse principal
component analysis. In Proceedings of the 24th international Conference on Machine Learn-
ing. Z. Ghahramani, Ed. ICML ’07, vol. 227. ACM, New York, NY, 177-184, 2007.
[3] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal compo-
nents. Applied Statistics, 22:203.214, 1995.
[4] C.C. Fowlkes, C.L. Luengo Hendriks, S.V. Kernen, G.H. Weber, O. Rbel, M.-Y. Huang, S.
Chatoor, A.H. DePace, L. Simirenko and C. Henriquez et al. Cell 133, pp. 364-374, 2008.
[5] J. Jeffers. Two case studies in the application of principal components. Applied Statistics, 16,
225-236, 1967.
[6] I.T. Jolliffe and M. Uddin. A Modi ﬁed Principal Componen t Technique based on the Lasso.
Journal of Computational and Graphical Statistics, 12:531.547, 2003.
[7] I.T. Jolliffe, Principal component analysis, Springer Verlag, New York, 1986.
[8] I.T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal of
Applied Statistics, 22:29-35, 1995.
[9] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: Exact and greedy
algorithms. Advances in Neural Information Processing Systems, 18, 2006.
[10] B. Moghaddam, Y. Weiss, and S. Avidan. Generalized spectral bounds for sparse LDA. In Proc.
ICML, 2006.
[11] Y. Saad, Projection and deﬂation methods for partial po le assignment in linear state feedback,
IEEE Trans. Automat. Contr., vol. 33, pp. 290-297, Mar. 1998.
[12] B.K. Sriperumbudur, D.A. Torres, and G.R.G. Lanckriet. Sparse eigen methods by DC pro-
gramming. Proceedings of the 24th International Conference on Machine learning, pp. 831-
838, 2007.
[13] D. Torres, B.K. Sriperumbudur, and G. Lanckriet. Finding Musically Meaningful Words by
Sparse CCA. Neural Information Processing Systems (NIPS) Workshop on Music, the Brain
and Cognition, 2007.
[14] P. White. The Computation of Eigenvalues and Eigenvectors of a Matrix. Journal of the Society
for Industrial and Applied Mathematics, Vol. 6, No. 4, pp. 393-437, Dec., 1958.
[15] F. Zhang (Ed.). The Schur Complement and Its Applications. Kluwer, Dordrecht, Springer,
2005.
[16] Z. Zhang, H. Zha, and H. Simon, Low-rank approximations with sparse factors I: Basic algo-
rithms and error analysis. SIAM J. Matrix Anal. Appl., 23 (2002), pp. 706-727.
[17] Z. Zhang, H. Zha, and H. Simon, Low-rank approximations with sparse factors II: Penalized
methods with discrete Newton-like iterations. SIAM J. Matrix Anal. Appl., 25 (2004), pp.
901-920.
[18] H. Zou, T. Hastie, and R. Tibshirani. Sparse Principal Component Analysis. Technical Report,
Statistics Department, Stanford University, 2004.

8


A general framework for investigating how far the
decoding process in the brain can be simpli ﬁed

Masafumi Oizumi1 , Toshiyuki Ishii2 , Kazuya Ishibashi1
Toshihiko Hosoya2 , Masato Okada1,2
oizumi@mns.k.u-tokyo.ac.jp
tishii@brain.riken.jp,kazuya@mns.k.u-tokyo.ac.jp
hosoya@brain.riken.jp, okada@k.u-tokyo.ac.jp
1 University of Tokyo, Kashiwa-shi, Chiba, JAPAN
2 RIKEN Brain Science Institute, Wako-shi, Saitama, JAPAN

Abstract

“How is information decoded in the brain?” is one of the most d ifﬁcult and im-
portant questions in neuroscience. Whether neural correlation is important or not
in decoding neural activities is of special interest. We have developed a general
framework for investigating how far the decoding process in the brain can be sim-
pli ﬁed. First, we hierarchically construct simpli ﬁed prob
abilistic models of neu-
ral responses that ignore more than K th-order correlations by using a maximum
entropy principle. Then, we compute how much information is lost when infor-
mation is decoded using the simpli ﬁed models, i.e., “mismat
ched decoders ”. We
introduce an information theoretically correct quantity for evaluating the informa-
tion obtained by mismatched decoders. We applied our proposed framework to
spike data for vertebrate retina. We used 100-ms natural movies as stimuli and
computed the information contained in neural activities about these movies. We
found that the information loss is negligibly small in population activities of gan-
glion cells even if all orders of correlation are ignored in decoding. We also found
that if we assume stationarity for long durations in the information analysis of dy-
namically changing stimuli like natural movies, pseudo correlations seem to carry
a large portion of the information.

1

Introduction

An ultimate goal of neuroscience is to elucidate how information is encoded and decoded by neural
activities. To investigate what information is encoded by neurons in certain area of the brain, the
mutual information between stimuli and neural responses is often calculated.
In the analysis of
mutual information, it is implicitly assumed that encoded information is decoded by an optimal
decoder, which exactly matches the encoder.
In other words, the brain is assumed to have full
knowledge of the encoding process. Generally, if the neural activities are correlated, the amount of
data needed for the optimal decoding scales exponentially with the number of neurons. Since a large
amount of data and many complex computations are needed for optimal decoding, the assumption
of an optimal decoder in the brain is doubtful.

The reason mutual information is widely used in neuroscience despite the doubtfulness of the opti-
mal decoder is that we are completely ignorant of how information is decoded in the brain. Thus,
we simply evaluate the maximal amount of information that can be extracted from neural activities
by calculating the mutual information. To address this lack of knowledge, we can ask a different
question: “How much information can be obtained by a decoder
that has partial knowledge of the
encoding process?” [10, 14] We call this type of a decoder “si mpli ﬁed decoder ” or a “mismatched
decoder ”. For example, an independent decoder is a simpli ﬁe d decoder; it takes only the marginal

1

distribution of the neural responses into consideration and ignores the correlations between neuronal
activities. The independent decoder is of particular importance because several studies have shown
that maximum likelihood estimation can be implemented by a biologically plausible network [2, 4].
If it is experimentally shown that a sufﬁciently large porti on of information is obtained by the in-
dependent decoder, we can say that the brain may function in a manner similar to the independent
decoder. In this context, Nirenberg et al. computed the amount of information obtained by the in-
dependent decoder in pairs of retinal ganglion cells activities [10]. They showed that no pair of
cells showed a loss of information greater than 11%. Because only pairs of cells were considered
in their analysis, it has not been still elucidated whether correlations are not important in population
activities.

To elucidate whether correlations are important or not in population activities, we have developed
a general framework for investigating the importance of correlation in decoding neural activities.
When population activities are analyzed, we have to deal with not only second-order correlations
but also higher-order correlations in general. Therefore, we need to hierarchically construct simpli-
ﬁed decoders that account of up to K th-order correlations, where K = 1, 2, ..., N . By computing
how much information is obtained by the simpli ﬁed decoders, we investigate how many orders of
correlation should be taken into account to extract enough information. To compute the information
obtained by the mismatched decoders, we introduce a information theoretically correct quantity de-
rived by Merhav et al. [8]. Information for mismatched decoders previously proposed by Nirenberg
and Latham is the lower bound on the correct information [5, 11]. Because this lower bound can be
very loose and their proposed information can be negative when many cells are analyzed as is shown
in the paper, we need to accurately evaluate the information obtained by mismatched decoders.

The plan of the paper is as follows. In Section 2, we describe a way of computing the information
that can be extracted from neural activities by mismatched decoders using the information derived
by Merhav et al.. Using analytical computation, we demonstrate how information for mismatched
decoders previously proposed by Nirenberg and Latham differs from the correct information derived
by Merhav et al., especially when many cells are analyzed. In Section 3, we apply our framework to
spike data for ganglion cells in the salamander retina. We ﬁr st describe the method of hierarchically
constructing simpli ﬁed decoders by using the maximum entro py principle [12]. We then compute the
information obtained with the simpli ﬁed decoders. We ﬁnd th
at more than 90% of the information
can be extracted from the population activities of ganglion cells even if all orders of correlations
are ignored in decoding. We also describe the problem of previous studies [10, 12] in which the
stationarity of stimuli is assumed for a duration that is too long. Using a toy model, we demonstrate
that pseudo correlations seem to carry a large portion of the information because of the stationarity
assumption.

2

Information for mismatched decoders

Let us consider how much information about stimuli can be extracted from neural responses. We
assume that we experimentally obtain the conditional probability distribution p(r|s) that neural re-
sponses r are evoked by stimulus s. We can say that the stimulus is encoded by neural response r,
which obeys the distribution p(r|s). We call p(r|s) the “encoding model ”. The maximal amount of
information obtained with the optimal decoder can be evaluated by using the mutual information:
I = − Z drp(r) log2 p(r) + Z dr Xs
p(s)p(r|s) log2 p(r|s),
where p(r) = Ps p(r|s)p(s) and p(s) is the prior probability of stimuli. In the optimal decoder, the
probability distribution q(r|s) that exactly matches the encoding model p(r|s) is used for decoding;
that is, q(r|s) = p(r|s). We call q(r|s) the “decoding model ”. We can also compute the maximal
amount of information obtained by a decoder using a decoding model q(r|s) that does not match the
encoding model p(r|s) by using an equation derived by Merhav et al. [8]:
I ∗ (β ) = − Z drp(r) log2 Xs
p(s)q(r|s)β + Z dr Xs
where β takes the value that maximizes I ∗ (β ). Thus, β is the value that satis ﬁes ∂ I ∗ /∂β = 0. We
call a decoder using the mismatched decoding model a “mismat ched decoder ”.

p(s)p(r|s) log2 q(r|s)β ,

(1)

(2)

2

(cid:14967)
(cid:14976)(cid:15000)
(cid:15035)
(cid:15036)
(cid:14972)
(cid:15045)
(cid:14975)
(cid:15040)
(cid:15032)
(cid:14967)
(cid:15049)
(cid:15051)
(cid:15036)
(cid:15033)
(cid:15035)
(cid:15046)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15034)
(cid:15046)
(cid:15036)
(cid:15040)
(cid:15035)
(cid:15051)
(cid:14967)
(cid:15032)
(cid:15035)
(cid:15044)
(cid:15036)
(cid:15049)
(cid:15039)
(cid:15046)
(cid:15034)
(cid:15037)
(cid:15051)
(cid:15045)
(cid:15032)
(cid:15040)
(cid:14967)
(cid:15044)
(cid:15037)
(cid:15046)
(cid:15050)
(cid:14967)
(cid:15040)
(cid:15044)
(cid:15051)
(cid:15045)
(cid:14967)
(cid:15052)
(cid:15056)
(cid:15046)
(cid:15033)
(cid:15044)
(cid:14967)
(cid:14967)
(cid:15000)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:23)

(cid:18)

(cid:770)(cid:18)(cid:16)(cid:23)

(cid:770)(cid:19)

(cid:20)

(cid:15001)

(cid:15008)(cid:14977)(cid:14982)(cid:15008)
(cid:15008)(cid:15013)(cid:15011)(cid:14982)(cid:15008)

(cid:23)(cid:18)
(cid:19)(cid:18)(cid:18)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:19)(cid:23)(cid:18)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:15013)(cid:15011)(cid:14982)(cid:15008)

(cid:21)

(cid:22)
(cid:24)
(cid:23)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:25)

Figure 1: Comparison between correct information I ∗ derived by Merhav et al. and Nirenberg-
Latham information I N L . A: Difference between I ∗/I (solid line) and I N L/I (dotted line) in
Gaussian model where correlations and derivatives of mean ﬁ ring rates are uniform. Correlation
1 /I (dotted line) when spike
1 /I (solid line) and I N L
parameter c = 0.01. B: Difference between I ∗
data in Figure 3A are used. For this spike data and other spike data analyzed, Nirenberg-Latham
information provides a tight lower bound on the correct information, possibly because the number
of cells is small.

Previously, Nirenberg and Latham proposed that the information obtained by mismatched decoders
can be evaluated by using [11]
I N L = − Z drp(r) log2 Xs
p(s)q(r|s) + Z dr Xs
We call their proposed information “Nirenberg-Latham info rmation”. If we set β = 1 in Eq. 2,
we obtain Nirenberg-Latham information, I ∗ (1) = I N L . Thus, Nirenberg-Latham information
does not give correct information; instead, it simply provides the lower bound on the correct infor-
mation, I ∗ (β ), which is the maximum value with respect to β [5, 8]. The lower bound provided
by Nirenberg-Latham information can be very loose and the Nirenberg-Latham information can be
negative when many cells are analyzed.

p(s)p(r|s) log2 q(r|s).

(3)

Theoretical evaluation of information I , I ∗ , and I N L

I =

,

(p′ (r|s))2
p(r|s)

We consider the problem where mutual information is computed when stimulus s, which is a single
variable, and slightly different stimulus s + ∆s are presented. We assume the prior probability of
stimuli, p(s) and p(s + ∆s), are equal: p(s) = p(s + ∆s) = 1/2. Neural responses evoked by the
stimuli are denoted by r, which is considered here to be the neuron ﬁring rate. When the difference
between two stimuli is small, the conditional probability p(r|s + ∆s) can be expanded with respect
to ∆s as p(r|s + ∆s) = p(r|s) + p′ (r|s)∆s + 1
2 p′′ (r|s)(∆s)2 + ..., where ′ represents differentiation
with respect to s. Using the expansion, to leading order of ∆s, we can write mutual information I
as
∆s2
8 Z dr
where R dr p′ (r|s)2
is the Fisher information. Thus, we can see that the mutual information is pro-
p(r|s)
portional to the Fisher information when ∆s is small. Similarly, the correct information I ∗ for the
mismatched decoders and the Nirenberg-Latham information I N L can be written as
¶−1
¶2 µZ dr
p(r|s)(q ′ (r|s))2
p′ (r|s)q ′ (r|s)
∆s2
8 µZ dr
,
q(r|s)2
q(r|s)
q(r|s) ! .
8 Ã− Z drp(r|s) µ q ′ (r|s)
q(r|s) ¶2
∆s2
p′ (r|s)q(r|s)
+ 2 Z dr
Taking into consideration the proportionality of the mutual information to the Fisher information, we
´−1
´2 ³R dr p(r|s)(q ′ (r|s))2
can interpret that ³R dr p′ (r|s)q ′ (r|s)
in Eq. 5 is a Fisher information-like
q(r|s)2
q(r|s)
quantity for mismatched decoders.
3

I N L =

I ∗ =

(4)

(5)

(6)

1
2

1
Z

p(r|s) =

Let us consider the case in which the encoding model p(r|s) obeys the Gaussian distribution
exp µ−
(r − f (s))T C−1 (r − f (s))¶ ,
where T stands for the transpose operation, f (s) is the mean ﬁring rates given stimulus s, and C is
the covariance matrix. We consider an independent decoding model q(r|s) that ignores correlations:
D (r − f (s))¶ ,
exp µ−
1
1
(r − f (s))T C−1
ZD
2
where CD is the diagonal covariance matrix obtained by setting the off-diagonal elements of C to
0. If the Gaussian integral is performed for Eqs. 4-5, I , I ∗ , and I N L can be written as

q(r|s) =

(7)

(8)

I =

(9)

,

(11)

(10)

I ∗ =

I N L =

CC−1
D

f ′T (s)C−1 f ′ (s),

f ′ (s) + 2f ′T (s)C−1
D

∆s2
8
(f ′T (s)C−1
f ′ (s))2
∆s2
D
f ′T (s)C−1
CC−1
8
f ′ (s)
D
D
∆s2
8 ¡−f ′T (s)C−1
f ′ (s)¢ .
D
The correct information obtained by the independent decoder for the Gaussian model (Eq. 10) is
inversely proportional to the decoding error of s when the independent decoder is applied, which
was computed from the generalized Cram ´er Rao bound by Wu et al. [14].
As a simple example, we consider a uniform correlation model [1, 14] in which covariance matrix C
is given by Cij = σ2 [δij + c(1 − δij )] and assume that the derivatives of the ﬁring rates are unifor m:
i = f ′ . In this case, I , I ∗ , and I N L can be computed using
that is f ′
N f ′2
∆s2
σ2 (N c + 1 − c)
8
∆s2
N f ′2
σ2 (N c + 1 − c)
8
(−c(N − 1) + 1)N f ′2
∆s2
σ2
8
where N is the number of cells. We can see that I ∗ is equal to I , which means that information is
not lost even if correlation is ignored in the decoding process. Figure 1A shows I N L/I and I ∗/I
when the degree of correlation c is 0.01. As shown in Figure 1A, the difference between the correct
information I ∗ and Nirenberg-Latham information I N L is very large when the number of cells N is
large. When N > c+1
c , I N L is negative. Analysis showed that using Nirenberg-Latham information
I N L as a lower bound on the correct information I ∗ can lead to wrong conclusions, especially when
many cells are analyzed.

I N L =

I ∗ =

(12)

(13)

(14)

,

,

,

I =

3 Analysis of information in population activities of ganglion cells

3.1 Methods

We analyzed the data obtained when N = 7 retinal ganglion cells were simultaneously recorded
using a multielectrode array. The stimulus was a natural movie, which was 200 s long and repeated
45 times. We divided the movie into many short natural movies and considered them as stimuli over
which information contained in neural activities is computed. For instance, when it was divided into
10-s-long natural movies, there were 20 stimuli. Figure 2A shows the response of the seven retinal
ganglion cells to natural movies from 0 to 10 s in length. To apply information theoretic techniques,
we ﬁrst discretized the time into small time bins ∆τ and indicated whether a spike was emitted or
not in each time bin with a binary variable: σi = 1 means that the cell i spiked and σi = 0 means that
it did not spike. We set the length of the time, ∆τ , to 5 ms so that it was short enough to avoid two
spikes falling into the same bin. In this way, the spike pattern of ganglion cells was transformed into
an N -letter binary word, σ = {σ1 , σ2 , ..., σN }, as shown in Figure 2B. Then, we determined the

4

(cid:15000)

(cid:15049)
(cid:15036)
(cid:15033)
(cid:15044)
(cid:15052)
(cid:15045)
(cid:14967)
(cid:15043)
(cid:15043)
(cid:15036)
(cid:15002)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:21)

(cid:20)

(cid:19)
(cid:18)

(cid:15001)
(cid:15049)
(cid:15036)
(cid:15033)
(cid:15044)
(cid:15052)
(cid:15045)
(cid:14967)
(cid:15043)
(cid:15043)
(cid:15036)
(cid:15002)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:21)

(cid:20)

(cid:19)
(cid:18)(cid:16)(cid:27)

(cid:23)
(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)(cid:18)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:18)(cid:16)(cid:27)(cid:23)
(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14984)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:19)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14984)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

Figure 2: A: Raster plot of seven retinal ganglion cells responding to a natural movie. B: Transfor-
mation of spike trains into binary words.

1
Z

(15)

(16)

pN (σ) =

frequency with which a particular spike pattern, σ , was observed during each stimulus and estimated
the conditional probability distribution pdata (σ |s) from experimental data. Using these conditional
probabilities, we evaluated the information contained in N -letter binary words σ .
Generally, the joint probability of N binary variables can be written as [9]
exp 
θij σiσj + · · · + θ12...N σ1σ2 ...σN 
Xi
θiσi + Xi<j
 .
This type of probability distribution is called a log-linear model. Because the number of parameters
in a log-linear model is equal to the number of all possible conﬁgurations of an N -letter binary word
σ , we can determine the values of parameters so that the log-linear model pN (σ) exactly matches
empirical probability distribution pdata (σ): that is, pN (σ) = pdata (σ).
To compute the information for mismatched decoders, we construct simpli ﬁed models of neural
responses that partially match the empirical distribution, pdata (σ). The simplest model is an “inde-
pendent model ” p1 (σ), where only the average of each σi agrees with the experimental data: that is,
hσi ip1 (σ) = hσi ipdata (σ) . There are many possible probability distributions that satisfy these con-
straints. In accordance with the maximum entropy principle [12], we choose the one that maximizes
entropy H , H = − Pσ p1 (σ) log p1 (σ). The resulting maximum entropy distribution is
i σi# .
exp "Xi
1
θ(1)
p1 (σ) =
Z1
in which model parameters θ (1) are determined so that the constraints are satis ﬁed. This mo del
corresponds to a log-linear model in which all orders of correlation parameters {θij , θijk , ..., θ12...N }
are omitted. If we perform maximum likelihood estimation of model parameters θ (1) in the log-
linear model, the result is that the average σi under the log-linear model equals the average σi
found in the data: that is, hσi ip1 (σ) = hσi ipdata (σ) . This result is identical to the constraints of
the maximum entropy model. Generally, the maximum entropy method is equivalent to maximum
likelihood ﬁtting of a log-linear model [6].
Similarly, we can consider a “second-order correlation mod el ” p2 (σ), which is consistent with not
only the averages of σi but also the averages of all products σiσj found in the data. Maximizing the
entropy with constraints hσi ip2 (σ) = hσi ipdata (σ) and hσiσj ip2 (σ) = hσiσj ipdata (σ) , we obtain
ij σiσj 
exp 
θ(2)
θ(2)
Xi
i σi + Xi,j

in which model parameters θ (2) are determined so that the constraints are satis ﬁed. The pro cedure
described above can also be used to construct a “ K th-order correlation model ” pK (σ). If we substi-
tute the simpli ﬁed models of neural responses pK (σ |s) into mismatched decoding models q(σ |s) in

(17)

p2 (σ) =

1
Z2

,

5

(cid:15000)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:15001)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:21)

(cid:22)
(cid:23)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:24)

(cid:25)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:21)

(cid:22)
(cid:24)
(cid:23)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:25)

Figure 3: Dependence of amount of information obtained by simpli ﬁed decoders on number of
ganglion cells analyzed. Same spike data obtained from retinal ganglion cells responding to a natural
movie were used to obtain analysis results shown in panels A and B. A: 10-s-long natural movie B:
100-ms-long natural movie

Eq. 2, we can compute the amount of information that can be obtained when more than K th-order
correlations are ignored in the decoding,
K (β ) = − Xσ
pN (σ) log2 Xs
p(s)pK (σ |s)β + Xs
p(s) Xσ
I ∗
By evaluating the ratio of information, I ∗
K /I , we can infer how many orders of correlation should
be taken into account to extract enough information.

pN (σ |s) log2 pK (σ |s)β .

(18)

3.2 Results

First, we investigated how the ratio of information obtained by an independent model, I ∗
1 /I , and that
2 /I , changed when the number of cells analyzed was
obtained by a second-order correlation model, I ∗
changed. We set the length of the stimulus to 10 s. We could obtain 20 kinds of stimuli from a 200-s-
long natural movie (see Methods). In previous studies, comparable length stimuli (7 s for Nirenberg
et al.’s study [10] and 20 s for Schneidman et al.’s study [12]) were used. When two neurons were
analyzed, there were 21 possible combinations for choosing 2 cells out of 7 cells, which is the total
number of cells simultaneously recorded. We computed the average value of I ∗
K /I for K = 1, 2 over
2 /I monotonically decreased
1 /I and I ∗
all possible combinations of cells. Figure 3A shows that I ∗
when the number of cells was increased. A comparison between the correct information, I ∗
1 /I , and
1 (β = 1), is shown in Figure 1B. When
1 /I where I N L
Nirenberg-Latham information, I N L
1 = I ∗
1 /I exceeded 90%, which means that ignoring correlation leads
only two cells were considered, I ∗
to only a small loss of information. This is consistent with the result obtained by Nirenberg et al.
1 /I becomes only about 60%.
[10]. However, when all cells (N = 7) were used in the analysis, I ∗
Thus, correlation seems to be much more important for decoding when population activities are
considered than when only two cells are considered. At least, we can say that qualitatively different
things occur when large populations of cells are analyzed, as Schneidman et al. pointed out [12].
We should be careful about concluding from the results shown in Figure 3A that correlation is
important for decoding. In this analysis, we considered a 10-s-long stimuli and assumed stationarity
during each stimulus. By stationarity we mean that we assumed spikes are generated by a single
process that can be described by a single conditional distribution p(σ |s). Because the natural movies
change much more rapidly and our visual system has much higher time resolution than 10 s [13],
we also considered shorter stimuli. In Figure 3B, we computed I ∗
1 /I and I ∗
2 /I over 100-ms-long
natural movies. In this case, we could obtain 2000 stimuli from the 200-s-long natural movie. When
the length of each stimulus was 100 ms, no spikes occurred while some stimuli were presented. We
removed those stimuli and used the remaining stimuli for the analysis. In this case, the amount of
1 was more than 90% even when all cells (N = 7)
information obtained by independent model I ∗
were considered. Although 100 ms may still be too long to be considered as a single process, the
result shown in Figure 3B reﬂects a situation that our brain h as to deal with, that is more realistic than
that reﬂected in Figure 3A. Figure 4A shows the dependence of
information obtained by simpli ﬁed
decoders on the length of stimulus. In this analysis, we changed the length of the stimulus from 100
ms to 10 s and computed I ∗
1 /I and I ∗
2 /I for activities of N = 7 cells. We also analyzed additional
experimental data obtained when N = 6 retinal ganglion cells were simultaneously recorded from

6

(cid:15000)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:19)(cid:18)

(cid:770)(cid:19)

(cid:15001)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:18)

(cid:19)(cid:18)
(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)

(cid:19)(cid:18)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:770)(cid:19)

(cid:19)(cid:18)

(cid:18)

(cid:19)(cid:18)
(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)

(cid:19)(cid:18)

(cid:15002)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15040)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15040)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:19)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:20)

(cid:18)

(cid:15008)(cid:14977)

(cid:14984)(cid:14982)(cid:15008)

(cid:18)(cid:16)(cid:26)
(cid:18)(cid:16)(cid:27)
(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)

Figure 4: Dependence of amount of information obtained by simpli ﬁed decoders on length of stim-
uli. Stimulus was same natural movie for both panels, but spike data obtained from retinas of
different salamander were used in panels A and B. A: Seven simultaneously recorded ganglion cells
B: Six simultaneously recorded ganglion cells C: Arti ﬁcial
spike data generated according to the
ﬁring rates shown in Figure 5A

(cid:15000)
(cid:23)(cid:18)
(cid:14970)(cid:14984)

(cid:18)
(cid:18)

(cid:23)(cid:18)
(cid:14970)(cid:14985)

(cid:18)
(cid:18)

(cid:14976)
(cid:15050)
(cid:14982)
(cid:15050)
(cid:15036)
(cid:15042)
(cid:15040)
(cid:15047)
(cid:15050)
(cid:14975)
(cid:14967)
(cid:15036)
(cid:15051)
(cid:15032)
(cid:15049)
(cid:14967)
(cid:15038)
(cid:15045)
(cid:15040)
(cid:15049)
(cid:15040)
(cid:15005)

(cid:15001)
(cid:23)(cid:18)

(cid:18)
(cid:18)

(cid:23)(cid:18)

(cid:15050)(cid:14984)

(cid:15050)(cid:14985)

(cid:23)(cid:18)

(cid:18)
(cid:19)

(cid:23)(cid:18)

(cid:18)(cid:16)(cid:23)

(cid:19)

(cid:19)(cid:16)(cid:23)

(cid:20)

(cid:18)
(cid:18)

(cid:18)(cid:16)(cid:23)

(cid:18)
(cid:19)
(cid:19)
(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)(cid:16)(cid:23)

(cid:20)

(cid:15002)
(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:15050)(cid:14984)

(cid:15050)(cid:14985)

(cid:15050)(cid:14986)

(cid:15050)(cid:14987)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:18)(cid:16)(cid:20) (cid:18)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:20) (cid:18)(cid:16)(cid:22)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:19)(cid:16)(cid:20) (cid:19)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:24) (cid:18)(cid:16)(cid:26) (cid:19)

(cid:18)

(cid:18)(cid:16)(cid:24) (cid:18)(cid:16)(cid:26) (cid:19)
(cid:19)(cid:16)(cid:20) (cid:19)(cid:16)(cid:22)
(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:19)(cid:16)(cid:24) (cid:19)(cid:16)(cid:26) (cid:20)

(cid:19)(cid:16)(cid:24) (cid:19)(cid:16)(cid:26) (cid:20)

(cid:19)

(cid:19)
(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:20)

(cid:20)

Figure 5: Firing rates of two model cells. Rate of cell #1 shown in top panel; rate of cell #2 is shown
in bottom panel. A: Firing rates from 0 to 2 s. B: Firing rates (solid line) and mean ﬁring rates
(dashed line) when stimulus was 1 s long. C: Firing rates (solid line) and mean ﬁring rates (dashed
line) when stimulus was 500 ms long.

another salamander retina. The same 200-s-long natural movie was used as a stimulus for Figure 4B
as for Figure 4A, and the activities of N = 6 cells were analyzed. Figure 4B shows the result. We
can clearly see the same tendency as shown in Figures 4A and B: the amount of information decoded
by the simpli ﬁed decoders monotonically increased as the le ngth of the stimulus was shortened.

To clarify the reason the correlation becomes less important as the stimulus is shortened, we used
the toy model shown in Figure 5. We considered the case in which two cells ﬁre independently
in accordance with a Poisson process and performed an analysis similar to the one we did for the
actual spike data. We used simulated spike data for the two cells generated in accordance with the
ﬁring rates shown in Figure 5A. The ﬁring rates with a 2-s stim ulus sinusoidally change with time.
We divided the 2-s-long stimulus into two 1-s-long stimulus, s1 and s2 , as shown in Figure 5B.
Then, we computed mutual information I and the information obtained by independent model I ∗
1
over s1 and s2 . Because the two cells ﬁred independently, there were no cor relations between two
cells essentially. However, there was pseudo correlation due to the assumption of stationarity for the
dynamically changing stimulus. The pseudo correlation was high for s1 and low for s2 . This means
that “correlation” plays an important role in discriminati ng two stimuli, s1 and s2 . In contrast, the
mean ﬁring rates of the two cells during each stimulus were eq ual for s1 and s2 . Therefore, if the
stimulus is 1 s long, we cannot discriminate two stimuli by using the independent model, that is,
1 = 0. We also considered the case in which the stimulus was 0.5 s long, as shown in Figure
I ∗
5C. In this case, pseudo correlations again appeared but there was a signi ﬁcant difference in the
mean ﬁring rates between the stimuli. Thus, the independent model can be used to extract almost all
the information. The dependence of I ∗
1 /I on the stimulus length is shown in Figure 4C. Behaviors
similar to those represented in Figure 4C were also observed in the analysis of the actual spike data
for retinal ganglion cells (Figure 4A and 4B). Even if we observe that correlation carries a signi ﬁcant
large portion of information for longer stimuli compared with the speed of change in the ﬁring rates,

7

it may simply be caused by meaningless pseudo correlation. To assess the role of correlation in
information processing, the stimuli used should be short enough to think neural responses to these
stimuli generated by a single process.

4 Summary and Discussion

We described a general framework for investigating how far the decoding process in the brain can
be simpli ﬁed. We computed the amount of information that can be extracted by using simpli ﬁed
decoders constructed using a maximum entropy model, i.e., mismatched decoders. We showed
that more than 90% of the information encoded in retinal ganglion cells activities can be decoded
by using an independent model that ignores correlation. Our results imply that the brain uses a
simpli ﬁed decoding strategy in which correlation is ignore d.

When we computed the information obtained by the independent model, we regarded a 100-ms-long
natural movie as one stimulus. However, when we considered stimuli that were long compared with
the speed of the change in the ﬁring rates as one stimulus, cor relation carried a large portion of
information. This is due to pseudo correlation, which is observed if stationarity is assumed for long
durations. The human visual system can process visual information in less than 150 ms [13]. We
should set the length of the stimulus appropriately by taking the time resolution of our visual system
into account.

Our results do not imply that any kind of correlation does not carry much information because we
dealt only with correlated spikes within a 5-ms time bin. In our analysis, we did not analyze the
correlation on a longer time scale, which can be observed in the activities of retinal ganglion cells
[7]. We also did not investigate the information carried by the relative timing of spikes [3]. Further
investigations are needed for these types of correlation. Our approach of comparing the mutual
information with the information obtained by simpli ﬁed dec oders can also be used for studying
other types of correlations.

References

[1] Abbott, L. F., & Dayan, P. (1999). Neural Comput., 11, 91-101.
[2] Deneve, S., Latham, P. E., & Pouget, A. (1999). Nature Neurosci., 2, 740-745.
[3] Gollish, S., & Meister, M. (2008). Science, 319, 1108-1111.
[4] Jazayeri, M. & Movshon, J. A. (2006). Nature Neurosci., 9, 690-696.
[5] Latham, P. E., & Nirenberg, S. (2005). J. Neurosci., 25, 5195-5206.
[6] MacKay, D. (2003). Information Theory, Inference and Learning Algorithms (Cambridge Univ.
Press, Cambridge, England).
[7] Meister, M., & Berry, M. J. II (1999). Neuron, 22, 435-450.
[8] Merhav, N., Kaplan, G., Lapidoth, A., & Shamai Shitz, S. (1994). IEEE Trans. Inform. Theory,
40, 1953-1967.
[9] Nakahara, H., & Amari, S. (2002). Neural Comput., 14, 2269-2316.
[10] Nirenberg, S., Carcieri, S. M., Jacobs, A. L., & Latham, P. E. (2001). Nature, 411, 698-701.
[11] Nirenberg, S., & Latham, P. (2003). Proc. Natl. Acad. Sci. USA, 100, 7348-7353.
[12] Schneidman, E., Berry, M. J. II, Segev, R., & Bialek. W. (2006). Nature, 440, 1007-1012.
[13] Thorpe, S., Fize, D., & Marlot, C. (1996). Nature, 381, 520-522.
[14] Wu, S., Nakahara, H., & Amari, S. (2001). Neural Comput., 13, 775-797.

8


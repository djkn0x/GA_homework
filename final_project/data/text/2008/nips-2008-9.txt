Sparse probabilistic projections

C ´edric Archambeau
Department of Computer Science
University College London, United Kingdom
c.archambeau@cs.ucl.ac.uk

Francis R. Bach
INRIA - Willow Project
Ecole Normale Sup ´erieure, Paris, France
francis.bach@mines.org

Abstract

We present a generative model for performing sparse probabilistic projections,
which includes sparse principal component analysis and sparse canonical corre-
lation analysis as special cases. Sparsity is enforced by means of automatic rele-
vance determination or by imposing appropriate prior distributions, such as gener-
alised hyperbolic distributions. We derive a variational Expectation-Maximisation
algorithm for the estimation of the hyperparameters and show that our novel prob-
abilistic approach compares favourably to existing techniques. We illustrate how
the proposed method can be applied in the context of cryptoanalysis as a pre-
processing tool for the construction of template attacks.

1

Introduction

Principal component analysis (PCA) is widely used for data pre-processing, data compression and
dimensionality reduction. However, PCA suffers from the fact that each principal component is a
linear combination of all the original variables. It is thus often difﬁcult to interpret the results. In
recent years, several methods for sparse PCA have been designed to ﬁnd projections which retain
maximal variance, while enforcing many entries of the projection matrix to be zero [20, 6]. While
most of these methods are based on convex or partially convex relaxations of the sparse PCA prob-
lem, [16] has looked at using the probabilistic PCA framework of [18] along with (cid:96)1 -regularisation.
Canonical correlation analysis (CCA) is also commonly used in the context for dimensionality re-
duction.The goal is here to capture features that are common to several views of the same data.
Recent attempts for constructing sparse CCA include [10, 19].
In this paper, we build on the probabilistic interpretation of CCA outlined by [2] and further extended
by [13]. We introduce a general probabilistic model, which allows us to infer from an arbitrary
number of views of the data, both a shared latent representation and individual low-dimensional
representations of each one of them. Hence, the probabilistic reformulations of PCA and CCA ﬁt
this probabilistic framework. Moreover, we are interested in sparse solutions, as these are important
for interpretation purposes, denoising or feature extraction. We consider a Bayesian approach to
the problem. A proper probabilistic approach allows us to treat the trade-off between the modelling
accuracy (of the high-dimensional observations by low-dimensional latent variables) and the degree
of sparsity of the projection directions in principled way. For example, we do not need to estimate
the sparse components successively, using, e.g., deﬂation, but we can estimate all sparse directions
jointly as we are taking the uncertainty of the latent variable into account.
In order to ensure sparse solutions we propose two strategies. The ﬁrst one, discussed in Ap-
pendix A, is based on automatic relevance determination (ARD) [14]. No parameter needs to be
set in advance. The entries in the projection matrix which are not well determined by the data are
automatically driven to zero. The second approach uses priors from the generalised hyperbolic fam-
ily [3], and more speciﬁcally the inverse Gamma. In this case, the degree of sparsity can be adjusted,
eventually leading to very sparse solutions if desired. For both approaches we derive a variational
EM algorithm [15].

1

(a)

(b)

Figure 1: (a) Graphical model (see text for details). Arrows denote conditional dependencies.
Shaded and unshaded nodes are respectively observed and unobserved random variables. Plates
indicate repetitions. (b) Marginal prior on the individual matrix entries (b = 1).

2 Generative model

We consider the graphical model shown in Figure 1(a). For each observation, we have P independent
measurements x1 , . . . , xP in different measurement spaces or views. The measurement xp ∈ RDp
is modelled as a mix of a common (or view independent) continuous latent vector y0 ∈ RQ0 and a
view dependent continuous latent vector yp ∈ RQp , such that
Wp ∈ RDp×Q0 , Vp ∈ RDp×Qp ,
xp = Wpy0 + Vpyp + µp + p ,
(1)
where {µp}P
p=1 are the view dependent offsets and p ∼ N (0, τ −1
p IDp ) is the residual error in view
p.
We are interested in the case where y0 and yp are low-dimensional vectors, i.e., Q0 , Qp (cid:28) Dp for
all p. We impose Gaussian priors on the latent vectors:
p ), p ∈ {1, . . . , P }.
yp ∼ N (0, Φ−1
y0 ∼ N (0, Φ−1
0 ),
(2)
The resulting generative model comprises a number of popular probabilistic projection techniques as
special cases. If there is a single view (and a single latent cause) and the prior covariance is diagonal,
we recover probabilistic factor analysis [9]. If the prior is also isotropic, then we get probabilistic
PCA [18]. If there are two views, we recover probabilistic CCA [2].
We seek a solution for which the matrices {Wp}P
p=1 and {Vp}P
p=1 are sparse, i.e. most of their en-
tries are zero. One way to achieve sparsity is by means of ARD-type priors [14]. In this framework,
a zero-mean Gaussian prior is imposed on the entries of the weight matrices:
ip ∈ {1, . . . , Dp}, j ∈ {1, . . . , Q0},
wip j ∼ N (0, 1/αip j ),
(3)
vip kp ∼ N (0, 1/βip kp ),
ip ∈ {1, . . . , Dp}, kp ∈ {1, . . . , Qp}.
(4)
Type II maximum likelihood leads then to a sparse solution when considering independent hyper-
parameters. The updates arising in the context of probabilistic projections are given in Appendix A.
Since marginalisation with respect to both the latent vectors and the weights is intractable, we apply
variational EM [15]. Unfortunately, following this route does not allow us to adjust the degree of
sparsity, which is important e.g. for interpretation purposes or for feature extraction.
Hence, we seek a more ﬂexible approach. In the remaining of this paper, we will assume that the
p=1 or {Vp}P
marginal prior on each weight λij , which is either an entry of {Wp}P
(cid:90)
p=1 and will be
deﬁned shortly, has the form of an (inﬁnite) weighted sum of scaled Gaussians:
N (0, γ−1
ij ) p(γij ) dγij .
We will chose the prior over γij in such a way that the resulting marginal prior over the correspond-
ing λij induces sparsity. A similar approach was followed in the context of sparse nonparametric
Bayesian regression in [4, 5].

p(λij ) =

(5)

2

−10−5051000.10.20.30.40.50.60.70.80.91!p(! )  a/DQ=0.1a/DQ=1a/DQ=10.

2.1 Compact reformulation of the generative model
Before discussing the approximate inference scheme, we rewrite the model in a more compact way.
xn = (cid:0)x(cid:62)
µ = (cid:0)µ(cid:62)
zn = (cid:0)y(cid:62)
(cid:1)(cid:62)
(cid:1)(cid:62)
(cid:1)(cid:62)
Let us denote the nth observation, the corresponding latent vector and the means respectively by
1 , . . . , µ(cid:62)
n1 , . . . , y(cid:62)
n0 , y(cid:62)
n1 , . . . , x(cid:62)
,
,
Φ ∈ RQ×Q , Q = Q0 + (cid:80)
nP
nP
P
The generative model can be reformulated as follows:
i ∈ {1, . . . , D}, j ∈ {1, . . . , Q}, D = (cid:80)
zn ∼ N (0, Φ−1 ),
pQp ,
λij |γij ∼ N (0, γ−1
ij ),
pDp ,
Λ ∈ RD×Q , Ψ ∈ RD×D ,
xn |zn , Λ ∼ N (Λzn + µ, Ψ−1 ),
 .
 τ1 ID1
 W1 V1
 Λ1
 =
 ,
0
0
. . .
. . .
...
...
...
...
...
...
. . .
. . .
τP IDP
. . . VP
ΛP
WP
0
0
. . .
Note that we do not assume that the latent spaces are correlated as Φ = diag{Φ0 , Φ1 , . . . , ΦP }.
This is consistent with the fact the common latent space is modelled independently through y0 .
Subsequently, we will also denote the matrix of the hyperparameters by Γ ∈ RD×Q , where we set
(and ﬁx) γij = ∞ for all λij = 0.

(6)
(7)
(8)

where

Ψ =

Λ =

2.2 Sparsity inducing prior over the individual scale variables

2bλ2
ij

DQ − 1
K a
2

p(λij ) =

for λij (cid:54)= 0, and

We impose an inverse Gamma prior on the scale variable γij :
γij ∼ I G (a/DQ, b),
(9)
for all i and j . The shape parameter a and the scale parameter b are non-negative. The marginal prior
on the weight λij is then in the class of the generalised hyperbolic distributions [3] and is deﬁned in
(cid:33) a
(cid:32)
(cid:114) 2
terms of the modiﬁed Bessel function of the third kind Kω (·):
(cid:16)(cid:113)
(cid:17)
2DQ − 1
a
4
λ2
b
DQ
ij
2b
Γ( a
DQ )
π
(cid:40) (cid:113) b
DQ − 1
Γ( a
2 )
lim
p(λij ) =
Γ( a
DQ )
2π
∞
λij→0
The function Γ(·) is the (complete) Gamma function.
The effective prior on the individual weights is shown in Figure 1(b). Intuitively, the joint distribu-
tion over the weights is sparsity inducing as it is sharply peaked around zero (and in fact inﬁnite for
sufﬁciently small a). It favours only a small number of weights to be non-zero if the scale variable
b is sufﬁciently large. For a more formal discussion in the context of regression we refer to [7].
It is interesting to note that for a/DQ = 1 we recover the popular Laplace prior, which is equivalent
to the (cid:96)1 -regulariser or the LASSO [17], and for a/DQ → 0 and b → 0 the resulting prior is
the Normal-Jeffreys prior.
In fact, the automatic thresholding method described in Appendix A
ﬁts also into the framework deﬁned by (5). However, it corresponds to imposing a ﬂat prior on
the scale variables over the log-scale, which is a limiting case of the Gamma distribution. When
imposing independent Gamma priors on the scale variables, the effective joint marginal is a product
of Student-t distributions, which again is sharply peaked around zero and sparsity inducing.

a
DQ > 1
2 ,
otherwise.

(10)

(11)

3 Variational approximation
We view {zn}N
n=1 and matrix Γ as latent variables, and optimise the parameters θ = {µ, Φ, Λ, Ψ}
by EM. In other words, we view the weight matrix Λ as a matrix of parameter and estimate the

3

entries by maximum a posteriori (MAP) learning. The other parameters are estimated by maximum
likelihood (ML).
Fq (x1 , . . . , xN , θ) = − N(cid:88)
The variational free energy is given by
(cid:104)ln p(xn , zn , Γ|θ)(cid:105)q − H[q(z1 , . . . , zN , Γ)],
(12)
n=1
where (cid:104)·(cid:105)q denotes the expectation with respect to the variational distribution q and H[·] is the differ-
ential entropy. Since the Kullback-Leibler divergence (KL) is non-negative, the negative free energy
N(cid:88)
is a lower bound to log-marginal likelihood:
ln p(xn |θ) = −Fq ({xn}, θ) + KL [q({zn}, Γ)(cid:107)p({zn}, Γ)|{xn}, θ)] (cid:62) −Fq ({xn}, θ).
n=1
(13)
Interestingly it is not required to make a factorised approximation of the the joint posterior q to ﬁnd
a tractable solution. Indeed, the posterior q factorises naturally given the data and the weights, such
that the posteriors we will obtain in the E-step are exact.
The variational EM ﬁnds maximum likelihood estimates for the parameters by cycling through the
following two steps until convergence:
1. The posterior over the latent variables are computed for ﬁxed parameters by minimising
q(z1 , . . . , zN ) ∝ N(cid:89)
the KL in (13). It can be shown that the variational posteriors are given by
e(cid:104)ln p(xn ,zn ,Γ|θ)(cid:105)q(Γ) ,
n=1
q(Γ) ∝ e(cid:104)ln p(xn ,zn |Γ,θ)(cid:105)q(z1 ,...,zN ) p(Γ).
(15)
2. The variational free energy (12) is minimised wrt the parameters for ﬁxed q . This leads
in effect to type II ML estimates for the paramteres and is equivalent to maximising the
N(cid:88)
expected complete log-likelihood:
θ ← argmax
θ
n=1

(cid:104)ln p(xn , zn , Γ|θ)(cid:105)q .

(14)

(16)

Depending on the initialisation, the variational EM algorithm converges to a local maximum of
the log-marginal likelihood. The convergence can be checked by monitoring the variational lower
bound, which monotonically increases during the optimisation. The explicit expression of the vari-
ational bound is here omitted due to a lack of space

3.1 Posterior of the latent vectors

The joint posterior of the latent vectors factorises into N posteriors due to the fact the observations
are independent. Hence, the posterior of each low-dimenstional latent vector is given by
where ¯zn = ¯SnΛ(cid:62)Ψ(xn − µ) is the mean and ¯Sn = (cid:0)Λ(cid:62)ΨΛ + Φ(cid:1)−1 is the covariance.
q(zn ) = N (¯zn , ¯Sn ),
3.2 Posterior of the scale variables

(17)

The inverse Gamma distribution is not conjugate to the exponential family. However, the posterior
over matrix Γ is tractable. It has the form of a product of generalised inverse Gaussian distributions
D(cid:89)
Q(cid:89)
D(cid:89)
Q(cid:89)
(see Appendix B for a formal deﬁnition):
p(γij |λij ) =
i=1
i=1
j=1
j=1
where ¯ωij = − a
DQ + 1
2 is the index and ¯ϕij = λ2
ij and ¯χij = 2b are the shape parameters. The
factorised form arises from the scale variable being independent conditioned on the weights.

N − ( ¯ωij , ¯ϕij , ¯χij )

q(Γ) =

(18)

4

diag{¯zn ¯z(cid:62)
n + ¯Sn},

3.3 Update for the parameters
Based on the properties of the Gaussian and the generalised inverse Gaussian, we can compute the
N(cid:88)
N(cid:88)
variational lower bound, which can then be maximised. This leads to the following updates:
µ ← 1
Φ−1 ← 1
(xn − Λ¯zn ),
λi ← (cid:16) ¯Γi + Ψ(i, i)
n (cid:105)(cid:17)−1
N(cid:88)
N(cid:88)
N
N
n=1
n=1
(cid:104)znz(cid:62)
(xn (i) − µ(i))¯zn ,
Ψ(i, i)
N(cid:88)
(cid:8)(xnp − µp )(cid:62) (xnp − µp ) − 2(xnp − µp )(cid:62)Λp ¯zn + (cid:10)(Λpzn )(cid:62)Λpzn
n=1
n=1
p ← 1
τ −1
N Dp
n=1
(cid:11) = tr{(cid:104)¯zn ¯z(cid:62)
(cid:10)(Λpzn )(cid:62)Λpzn
where the required expectations are given by
(cid:115) ¯χij
(cid:0)√
(cid:1)
n (cid:105)Λ(cid:62)
n (cid:105) = ¯Sn + ¯zn ¯z(cid:62)
(cid:104)znz(cid:62)
p Λp},
¯Γi = diag(cid:8)(cid:104)γi1 (cid:105), . . . , (cid:104)γiQ (cid:105)(cid:9),
n ,
(cid:1) .
(cid:0)√
¯χij ¯ϕij
Kω+1
(cid:104)γij (cid:105) =
¯ϕij
¯χij ¯ϕij
Kω
Note that diag{·} denotes a block-diagonal operation in (19). More importantly, since we are seek-
ing a sparse projection matrix, we do not suffer from the rotational ambiguity problem as is for
example the case standard probabilistic PCA.

(20)
(cid:11)(cid:9), (21)

(19)

(22)

(23)

4 Experiments

4.1 Synthetic denoising experiments

Because of identiﬁability issues which are subject of ongoing work, we prefer to compare various
methods for sparse PCA in a denoising experiment. That is, we assume that the data were generated
from sparse components plus some noise and we compare the various sparse PCA on the denoising
task, i.e., on the task of recovering the original data. We generated the data as follows: select
uniformly at random M = 4 unit norm sparse vectors in P = 10 dimensions with known number
S = 4 of non zero entries, then generate i.i.d. values of the random variables Z from three possible
distributions (Gaussian, Laplacian, uniform), then add isotropic noise of relative standard deviation
1/2. When the latent variables are Gaussian, our model exactly matches the data and our method
should provide a better ﬁt; however, we consider also situations where the model is misspeciﬁed in
order to study the robustness of our probabilistic model.
We consider our two models: SCA-1 (which uses automatic relevance determination type of sparsity
priors) and SCA-2 (which uses generalised hyperbolic distribution), where we used 6 latent dimen-
sions (larger than 4) and ﬁxed hyperparameters that lead to vague priors. Those two models thus
have no free parameters and we compare them to the following methods, which all have two regu-
larisation parameters (rank and regularisation): DSPCA [6], the method of Zou [20] and the recent
method of [16] which essentially considers a probabilistic PCA with (cid:96)1 -penalty on the weights.
In Table 1 we report mean-squared reconstruction error averaged over 10 replications. It can be seen
that two proposed probabilistic approaches perform similarly and signiﬁcantly outperform other
sparse PCA methods, even when the model is misspeciﬁed.

4.2 Template attacks

Power consumption and electromagnetic radiation are among the most extensively used side-
channels for analysing physically observable cryptographic devices. A common belief is that the
useful information for attacking a device is hidden at times where the traces (or time series) have
large variance. Once the relevant samples have been identiﬁed they can be used to construct tem-
plates, which can then be used to assess if a device is secure. A simple, yet very powerful approach,
recently proposed by [1], is to select time samples based on PCA. Figure 2(a) shows the weight

5

N SCA-1
39.9
100
36.5
200
400
35.5
N SCA-1
39.9
100
36.8
200
400
36.4
N SCA-1
39.3
100
36.5
200
300
35.8

SCA-2
40.8
36.8
35.5
SCA-2
40.9
37.0
36.4
SCA-2
40.3
36.7
35.8

Zou DSPCA L1-PCA
50.8
42.9
42.2
50.4
41.4
40.8
39.8
40.3
42.5
Zou DSPCA L1-PCA
49.8
43.6
42.6
48.1
41.1
40.9
40.5
40.7
46.8
Zou DSPCA L1-PCA
48.5
43.4
42.7
46.2
40.8
40.2
40.6
40.9
41.0

Table 1: Denoising experiment with sparse PCA (we report mean squared errors): (top) Gaussian
distributed latent vectors, (middle) latent vectors generated from the uniform distribution, (bottom)
latent vectors generated from the Laplace distribution.

(a) Probabilistic PCA.

(b) Sparse probabilistic PCA (SCA-2).

Figure 2: Power traces and ﬁrst three principal directions.

associated to each time sample by the ﬁrst three principal directions found by PCA. The problem
with this approach is that all time samples get a non-zero weights. As a result, the user has to deﬁne
a threshold manually in order to decide whether the information leakage at time t is relevant or not.
Figure 2(b) shows the weight associated to the time samples by SCA-2 when using a Laplace prior
(i.e. for a/DQ = 1). It can be observed that one gets a much better picture of where the relevant
information is. Clearly, sparse probabilitic PCA can be viewed as being more robust to spurious
noise and provides a more reliable and amenable solution.

5 Conclusion

In this paper we introduced a general probabilistic model for inferring sparse probabilistic projection
matrices. Sparsity was enforced by either imposing an ARD-type prior or by means of the a Normal-
Inverse Gamma prior. Although the inverse Gamma is not conjugate to the exponential family, the
posterior is tractable as it is a special case of the generalised inverse Gaussian [12], which in turn
is a conjugate prior to this family. Future work will include the validation of the method on a wide
range of applications and in particular as a feature extracting tool.

Acknowledgments

We are grateful to the PASCAL European network of excellence for partially supporting this work.

6

02040608010012014016018020000.10.2Power020406080100120140160180200−101!1020406080100120140160180200−101!2020406080100120140160180200−101!3t02040608010012014016018020000.10.2Power020406080100120140160180200−101!1020406080100120140160180200−101!2020406080100120140160180200−101!3tA Automatic thresholding the weights by ARD

(24)

(cid:104)ln p(xn , zn , Λ|θ)(cid:105)q − H[q(z1 , . . . , zN , Λ)].

In this section, we provide the updates for achieving automatic thresholding of projection matrix
entries in a probabilistic setting. We apply Tipping’s sparse Bayesian theory [8], which is closely
related to ARD [14]. More speciﬁcally, we assume the prior over the scale variables is uniform over
a log-scale, which is a limiting case of the Gamma distribution.
Let us view {zn}N
n=1 and Λ as latent variables and optimise the parameters θ = {µ, Φ, Ψ, Γ} by
Fq (x1 , . . . , xN , θ) = − N(cid:88)
variational EM. The variational free energy is given by
n=1
In order to ﬁnd a tractable solution, we further have to assume that the approximate posterior q has
a factorised form. We can then compute the posterior of the low-dimenstional latent vectors:
iΨ(i, i) ¯Σi + Φ(cid:1)−1 . And the posterior of
Ψ(xn − µ) and ¯Sn = (cid:0) ¯Λ
Ψ ¯Λ + (cid:80)
q(zn ) = N (¯zn , ¯Sn ),
(25)
(cid:62)
(cid:62)
where ¯zn = ¯Sn ¯Λ
D(cid:89)
D(cid:89)
the weights is given by
N ( ¯λi , ¯Σi ),
q(λi ) =
q(Λ) =
(26)
n (xn (i) − µ(i))¯zn and ¯Σi = (cid:0)Γi + Ψ(i, i) (cid:80)
n }(cid:1)−1 . The
where ¯λi = ¯ΣiΨ(i, i) (cid:80)
partially factorised form (cid:81)
i=1
i=1
n{ ¯Sn + ¯zn ¯z(cid:62)
i q(λi ) arises naturally. Note also that the update for the mean weights
has the same form as in (20). Finally, the updates for the parameters are found by maximising the
negative free energy, which corresponds to performing type II ML for the scaling variables. This
N(cid:88)
N(cid:88)
diag(cid:8)¯zn ¯z(cid:62)
(cid:9),
yields
(xn − ¯Λ¯zn ), Φ−1 ← 1
µ ← 1
γij ← (cid:104)λ2
ij (cid:105)−1 ,
n + ¯Sn
(27)
N(cid:88)
(cid:11)(cid:9), (28)
(cid:8)(xnp − µp )(cid:62) (xnp − µp ) − 2(xnp − µp )(cid:62) ¯Λp ¯zn + (cid:10)(Λpzn )(cid:62)Λpzn
N
N
n=1
n=1
p ← 1
τ −1
(cid:11) = tr{(¯zn ¯z(cid:62)
ij + ¯Σi (j, j ) and (cid:10)(Λpzn )(cid:62)Λpzn
n + ¯Sn ) (cid:80)
N Dp
n=1
(cid:62)
ij (cid:105) = ¯λ2
where (cid:104)λ2
+ ¯Σip )}.
( ¯λip
¯λ
ip
ip
B Generalised inverse Gaussian distribution

yω−1 e− 1
2 (χy−1+φy) ,

The Generalised inverse Gaussian distribution is in the class of generalised hyperbolic distributions.
It is deﬁned as follows [12, 11]:
√
y ∼ N − (ω , χ, φ) = χ−ω (
χφ)ω
√
χφ)
2Kω (
where y > 0 and Kω (·) is the modiﬁed Bessel function of the third kind1 with index ω .
(cid:115)
(cid:114) χ
R−ω ((cid:112)χφ),
Rω ((cid:112)χφ),
The following expectations are useful [12]:
φ
where Rω (·) ≡ Kω+1 (·)/Kω (·).
1The modiﬁed Bessel function of the third kind is known under various names. In particular, it is also known
as the modiﬁed Bessel function of the second kind (cf. E. W. Weisstein: ”Modiﬁed Bessel Function of the Sec-
ond Kind.” From MathWorld: http://mathworld.wolfram.com/ModiﬁedBesselFunctionoftheSecondKind.html).

(cid:104)ln y(cid:105) = ln ω + d ln Kω (
dω

(cid:104)y−1 (cid:105) =

(cid:104)y(cid:105) =

φ
χ

(29)

√

χφ)

,

(30)

7

Inverse Gamma distribution

When φ = 0 and ω < 0, the generalised inverse Gaussian distribution reduces to the inverse Gamma
distribution:
I G (a, b) = ba
Γ(a) x−a−1 e− b
a, b > 0.
x ,
It is straightforward to verify this result by posing a = −ω and b = χ/2, and noting that
Kω (y) = Γ(−ω)2−ω−1 yω
lim
y→0

(31)

(32)

for ω < 0.

References
[1] C. Archambeau, E. Peeters, F.-X. Standaert, and J.-J. Quisquater. Template attacks in principal subspaces.
In L. Goubin and M. Matsui, editors, 8th International Workshop on Cryptographic Hardware and Em-
bedded Systems(CHES), volume 4249 of Lecture Notes in Computer Science, pages 1–14. Springer, 2006.
[2] F. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical
Report 688, Department of Statistics, University of California, Berkeley, 2005.
[3] O. Barndorff-Nielsen and R. Stelzer. Absolute moments of generalized hyperbolic distributions and
approximate scaling of normal inverse Gaussian L ´evy processes. Scandinavian Journal of Statistics,
32(4):617–637, 2005.
[4] P. J. Brown and J. E. Grifﬁn. Bayesian adaptive lassos with non-convex penalization. Technical Report
CRiSM 07-02, Department of Statistics, University of Warwick, 2007.
[5] F. Caron and A. Doucet. Sparse bayesian nonparametric regression. In 25th International Conference on
Machine Learning (ICML). ACM, 2008.
[6] A. d’Aspremont, E. L. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA
using semideﬁnite programming. SIAM Review, 49(3):434–48, 2007.
[7] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal
of the American Statistical Association, 96:1348–1360, 2001.
[8] A. C. Faul and M. E. Tipping. Analysis of sparse Bayesian learning. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14 (NIPS), pages 383–389.
The MIT Press, 2002.
[9] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report
CRG-TR-96-1, Department of Computer Science, University of Toronto, 1996.
[10] D. Hardoon and J. Shawe-Taylor. Sparse canonical correlation analysis. Technical report, PASCAL
EPrints, 2007.
[11] Wenbo Hu. Calibration of multivariate generalized hyperbolic distributions using the EM algorithm, with
applications in risk management, portfolio optimization and portfolio credit risk. PhD thesis, Florida State
University, United States of America, 2005.
[12] B. Jørgensen. Statistical Properties of the Generalized Inverse Gaussian Distribution. Springer-Verlag,
1982.
In Z. Ghahramani, editor, 24th International
[13] A. Klami and S. Kaski. Local dependent components.
Conference on Machine Learning (ICML), pages 425–432. Omnipress, 2007.
[14] D. J. C. MacKay. Bayesian methods for backprop networks.
In E. Domany, J.L. van Hemmen, and
K. Schulten, editors, Models of Neural Networks, III, pages 211–254. 1994.
[15] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other
variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. The MIT press, 1998.
[16] C. D. Sigg and J. M. Buhmann. Expectation-maximization for sparse and non-negative PCA. In 25th
International Conference on Machine Learning (ICML). ACM, 2008.
[17] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society
B, 58:267–288, 1996.
[18] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society B, 61:611–622, 1999.
[19] D. Torres, D. Turnbull, B. K. Sriperumbudur, L. Barrington, and G.Lanckriet. Finding musically mean-
ingful words using sparse CCA. In NIPS workshop on Music, Brain and Cognition, 2007.
[20] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational and
Graphical Statistics, 15(2):265–286, 2006.

8


Risk Bounds for Randomized Sample Compressed
Classi ﬁers

Mohak Shah
Centre for Intelligent Machines
McGill University
Montreal, QC, Canada, H3A 2A7
mohak@cim.mcgill.ca

Abstract

We derive risk bounds for the randomized classiﬁers in Sampl e Compression set-
ting where the classiﬁer-speciﬁcation utilizes two source
s of information viz. the
compression set and the message string. By extending the recently proposed Oc-
cam’s Hammer principle to the data-dependent settings, we derive point-wise ver-
sions of the bounds on the stochastic sample compressed classiﬁers and also re-
cover the corresponding classical PAC-Bayes bound. We further show how these
compare favorably to the existing results.

1 Introduction
The Sample compression framework [Littlestone and Warmuth, 1986, Floyd and Warmuth, 1995]
has resulted in an important class of learning algorithms known as sample compression algorithms.
These algorithms have been shown to be competitive with the state-of-the-art algorithms such as
the SVM in practice [Marchand and Shawe-Taylor, 2002, Laviolette et al., 2005]. Moreover, the
approach has also resulted in practical realizable bounds and has shown signiﬁcant promise in using
these bounds in model selection.

On another learning theoretic front, the PAC-Bayes approach [McAllester, 1999] has shown that
stochastic classiﬁer selection can prove to be more powerfu l than outputing a deterministic classiﬁer.
With regard to the sample compression settings, this was further con ﬁrmed in the case of sample
compressed Gibbs classiﬁer by Laviolette and Marchand [200 7]. However, the speciﬁc classiﬁer
output by the algorithm (according to a selected posterior) is generally of immediate interest since
this is the classiﬁer whose future performance is of relevan ce in practice. Diluting such guarantees
in terms of the expectancy of the risk over the posterior over the classiﬁer space, although gives
tighter risk bounds, result in averaged statements over the expected true error.

A signiﬁcant result in obtaining such guarantees for the spe ciﬁc randomized classiﬁer has appeared
in the form of Occam’s Hammer [Blanchard and Fleuret, 2007]. It deals with bounding the perfor-
mance of algorithms that result in a set output when given training data. With respect to classiﬁers,
this results in a bound on the true risk of the randomized classiﬁer output by the algorithm in accor-
dance with a learned posterior over the classiﬁer space from training data. Blanchard and Fleuret
[2007] also present a PAC-Bayes bound for the data-independent settings (when the classiﬁer space
is de ﬁned independently of the training data).

Motivated by this result, we derive risk bounds for the randomized sample compressed classiﬁers.
Note that the classiﬁer space in the case of sample compressi on settings, unlike other settings, is
cation of training data.1 The rest of
data-dependent in the sense that it is de ﬁned upon the speciﬁ

1Note that the classi ﬁer space depends on the amount of the tra ining data as we see further and not on
the training data themselves. Hence, a data-independent prior over the classi ﬁer space can still be obtained in
this setting, e.g., in the PAC-Bayes case, owing to the independence of the classi ﬁer space deﬁnition from the
content of the training data.

the paper is organized as follows: Section 2 provides a background on the sample compressed
classiﬁers and establishes the context; Section 3 then stat es the Occam’s Hammer for the data-
independent settings. We then derive bounds for the randomized sample compressed classiﬁer in
Section 4 followed by showing how we can recover bounds for the sample compressed Gibbs case
(classical PAC-Bayes for sample compressed classiﬁers) in Section 5. We conclude in Section 6.

2 Sample Compressed (SC) Classi ﬁers
We consider binary classiﬁcation problems where the input s pace X consists of an arbitrary subset
def= (x, y ) is an input-output pair where
of Rn and the output space Y = {−1, +1}. An example z
x ∈ X and y ∈ Y . Sample Compression learning algorithms are characterized as follows:
Given a training set S = {z1 , . . . , zm} of m examples, the classiﬁer A(S ) returned by algorithm
A is described entirely by two complementary sources of information: a subset zi of S , called the
compression set, and a message string σ which represents the additional information needed to
obtain a classiﬁer from the compression set zi . Given a training set S , the compression set zi is
def= (i1 , i2 , . . . , i|i| ) with ij ∈ {1, . . . , m} ∀j and i1 < i2 < . . . <
de ﬁned by a vector
i of indices i
i|i| and where |i| denotes the number of indices present in i. Hence, zi denotes the ith example of S
whereas zi denotes the subset of examples of S that are pointed to by the vector of indices i de ﬁned
above. We will use i to denote the set of indices not present in i. Hence, we have S = zi ∪ zi for
any vector i ∈ I where I denotes the set of the 2m possible realizations of i.
Finally, a learning algorithm is a sample compression learning algorithm (that is identiﬁed solely
by a compression set zi and a message string σ) iff there exists a Reconstruction Function R :
(X × Y )|i| × K −→ H, associated with A. Here, H is the (data-dependent) classiﬁer space and
K ⊂ I × M s.t. M = ∪i∈IM(i). That is, R outputs a classiﬁer R(σ, zi ) when given an arbitrary
compression set zi ⊆ S and message string σ chosen from the set M(zi ) of all distinct messages
that can be supplied to R with the compression set zi .
We seek a tight risk bound for arbitrary reconstruction functions that holds uniformly for all com-
pression sets and message strings. For this, we adopt the PAC setting where each example z is drawn
according to a ﬁxed, but unknown, probability distribution D on X × Y . The true risk R(f ) of any
classiﬁer f is de ﬁned as the probability that it misclassiﬁes an example
drawn according to D:
R(f ) def= Pr(x,y)∼D (f (x) 6= y ) = E(x,y)∼D I (f (x) 6= y )
where I (a) = 1 if predicate a is true and 0 otherwise. Given a training set S = {z1 , . . . , zm} of m
examples, the empirical risk RS (f ) on S , of any classiﬁer f , is de ﬁned according to:
m
Xi=1
Let Zm denote the collection of m random variables whose instantiation gives a training sample
S = zm = {z1 , . . . , zm}. To obtain the tightest possible risk bound, we will fully exploit the
fact that the distribution of classiﬁcation errors is a bino mial. We now discuss the generic Occam’s
Hammer principle (w.r.t. the classiﬁcation scenario) and t hen go on to show how it can be applied
to the sample compression setting.

I (f (xi ) 6= yi ) def= E(x,y)∼S I (f (x) 6= y )

RS (f ) def=

1
m

3 Occam’s Hammer for data independent setting
In this section, we brie ﬂy detail the Occam’s hammer [Blanch ard and Fleuret, 2007] for data-
independent setting. For the sake of simplicity, we retain the key notations of Blanchard and Fleuret
[2007]. Occam’s hammer work by bounding the probability of bad event de ﬁned as follows. For
every classiﬁer h ∈ H, and a con ﬁdence parameter δ ∈ [0, 1], the bad event B (h, δ) is de ﬁned as
the region where the desired property on the classiﬁer h does not hold, with probability δ . That is,
PrS∼Dm [S ∈ B (h, δ)] ≤ δ . Further, it assumes that this region is nondecreasing in δ . Intuitively,
this means that with decreasing δ the bound on the true error of the classiﬁer h becomes tighter.

With the above assumption satisﬁed, let, P be a non-negative reference measure on the classiﬁer
space H known as the volumic measure. Let Π be a probability distribution on H absolutely contin-
uous w.r.t. P such that π = dΠ
dP . Let Γ be a probability distribution on (0, +∞) (the inverse density
prior). Then Occam’s Hammer [Blanchard and Fleuret, 2007] states that:

Theorem 1 [Blanchard and Fleuret, 2007] Given the above assumption and P, Π, Γ de ﬁned as
above, de ﬁne the level function

∆(h, u) = min(δπ(h)β (u), 1).
where β (x) = R x
0 udΓ(u) for x ∈ (0, +∞). Then for any algorithm S 7→ θS returning a probability
density θS over H with respect to P, and such that (S, h) 7→ θS (h) is jointly measurable in its two
variables, it holds that

S∼Dm ,h∼Q (cid:2)S ∈ B (h, ∆(h, θS (h)−1 ))(cid:3) ≤ δ,
Pr
where Q is the distribution on H such that dQ
dP = θS .
Note above that Q is the (data-dependent) posterior distribution on H after observing the data sample
S while P is the data-independent prior on H. The subscript S in θS denotes this. Moreover, the
distribution Π on the space of classiﬁers may or may not be data-dependent. A s we will see later, in
the case of sample compression learning settings we will consider priors over the space of classiﬁers
without reference to the data (such as PAC-Bayes case). To this end, we can either opt for a prior Π
independent of the data or make it the same as the volume measure P which establishes a distribution
on the classiﬁer space without reference to the data.

4 Bounds for Randomized SC Classi ﬁers
We work in the sample compression settings and as mentioned before, each classiﬁer in this setting
is denoted in terms of a compression set and a message string. A reconstruction function then
uses these two information sources to reconstruct the classiﬁer. This essentially means that we deal
with a data-dependent hypothesis space. This is in contrast with other notions of hypothesis class
complexity measures such as VC dimension. The hypothesis space is de ﬁned, in our case, based on
the size of data sample (and not the actual contents of the sample). Hence, we consider the priors
built on the size of the possible compression sets and associated message strings. More precisely, we
consider prior distribution P with probability density P (zi , σ) to be facotorizable in its compression
set dependent component and message string component (conditioned on a given compression set)
such that:

(1)
P (zi , σ) = PI (i)PM(zi ) (σ)
p(|i|) such that Pm
with PI (i) = 1
d=0 p(d) = 1. The above choice of the form for PI (i) is
|i|)
(m
appropriate since we do not have any a priori information to distinguish one compression set from
other. However, as we will see later, we should choose p(d) such that we give more weight to smaller
compression sets.
Let PK be the set of all distributions P on K satisfying above equation. Then, we are interested
in algorithms that output a posterior Q ∈ PK over the space of classiﬁers with probability den-
sity Q(zi , σ) factorizable as QI (i)QM(zi ) (σ). A sample compressed classiﬁer is then de ﬁned by
choosing a classiﬁer (zi , σ) according to the posterior Q(zi , σ). This is basically the Gibbs classiﬁer
de ﬁned in the PAC-Bayes settings where the idea is to bound th e true risk of this Gibbs classiﬁer
de ﬁned as R(GQ ) = E(zi ,σ)∼QR((zi , σ)). On the other hand, we are interested in bounding the true
risk of the speciﬁc classiﬁer
(zi , σ) output according to Q. As shown in [Laviolette and Marchand,
2007], a rescaled posterior Q of the following form can provide tighter guarantees while maintaining
the Occam’s principle of parsimony.

De ﬁnition 2 Given a distribution Q ∈ PK , we denote by Q the distribution:
QI (i)QM(zi ) (σ)
Q(zi , σ)
Q(zi , σ) def=
1
|i|E(zi ,σ)∼Q
|i|E(zi ,σ)∼Q
|i|

= QI (i)QM(zi ) (σ)

=

1
|i|

∀(zi , σ) ∈ K

Hence, note that the posterior is effectively rescaled for the compression set part. Hence, any
(zi , σ) ∼ Q = i ∼ QI , σ ∼ QM(zi ) . Further, if we denote by dQ the expected
classiﬁer
value of the compression set size over the choice of parameters according to the scaled posterior,
def= Ei∼QI ,σ∼QM(z
|i|, then,
dQ
i )

E(zi ,σ)∼Q

1
|i|

=

1
Ei∼QI ,σ∼QM(z
i )

|i|

=

1
m − dQ

Now, we proceed to derive the bounds for the randomized sample compressed classiﬁers starting
with a PAC-Bayes bound.

4.1 A PAC-Bayes Bound for randomized SC classiﬁer
We exploit the fact that the distribution of the errors is binomial and de ﬁne the following error
quantities (for a given i, and hence zi over z|i| ):

De ﬁnition 3 Let S ∈ Dm with D a distribution on X × Y , and (zi , σ) ∈ K. We denote by
BinS (i, σ), the probability that the classiﬁer R(zi , σ) of (true) risk R(z bi , σ) makes |i|Rzi (zi , σ) or
∼ D |i| . That is,
fewer errors on z′
i

BinS (i, σ) =

|i|Rz
(zi ,σ)
i
Xλ=0
and by BS (i, σ), the probability that this classiﬁer makes exactly |i|Rzi (zi , σ) errors on z′
i
That is,

λ (cid:19)(R(σ, zi ))λ (1 − R(σ, zi ))|i|−λ
(cid:18)|i|

∼ D |i| .

|i|
BS (i, σ) = (cid:18)
|i|Rzi (zi , σ)(cid:19)(R(zi , σ))|i|Rz
i
Now, approximating the binomial by relative entropy Chernoff bound [Langford, 2005], we have,
for a classiﬁer f :

(zi ,σ) (1 − R(zi , σ))|i|−|i|Rz
i

(zi ,σ)

mRS (f )
(cid:18)m
j (cid:19)(R(f ))j (1 − R(f ))m−j ≤ exp(−m · kl(RS (f )kR(f )))
Xj=0
for all RS (f ) ≤ R(f ).
As also shown in [Laviolette and Marchand, 2007], since (cid:0)m
j (cid:1) = (cid:0) m
m−j(cid:1) and kl(RS (f )kR(f )) =
kl(1 − RS (f )k1 − R(f )), the above inequality holds true for each factor inside the sum on the
left hand side. Consequently, in the case of sample compressed classiﬁer, ∀(zi , σ) ∈ K and ∀S ∈
(X × Y )m :

(2)

(3)

BS (i, σ) ≤ exp (cid:2)−|i| · kl(Rzi (σ, zi )kR(σ, zi ))(cid:3)
Bounding this by δ yields:
ln 1
|i| (cid:19) ≥ 1 − δ
PrS∼Dm (cid:18)kl(Rzi (σ, zi )kR(σ, zi )) ≤
δ
Now, consider the quantity in the probability in Equation 3 as the bad event over classiﬁers de ﬁned
by a compression set i and an associated message string σ . Let ψzm (i, σ) be the posterior probability
density of the rescaled data-dependent posterior distribution Q over the classiﬁer space with respect
to the volume measure P. We can now replace δ for this bad event by the delta of the Occam’s
hammer de ﬁned as:
ln(min(δπ(hS )β (ψzm (i, σ)−1 ), 1)−1 ) = ln+ (cid:18) 1
k , 1) (cid:19)
1
min((k + 1)−1ψzm (i, σ)− k+1
δ ·π(h)
k , 1)(cid:19)
= ln+ (cid:18) 1
k+1
δ ·π(h)
≤ ln+ (cid:18) 1
k , 1)(cid:19)
k+1
· (k + 1) max(ψzm (i, σ)
δ ·π(h)
· (k + 1)(cid:19) + ln+ (cid:18)ψzm (i, σ)
k (cid:19)
≤ ln (cid:18) 1
k+1
δ ·π(h)

· max((k + 1)ψzm (i, σ)

·

where ln+ denotes max(0, ln), the positive part of the logarithm.

However, note that we are interested in data-independent priors over the space of classiﬁers 2 , and
hence, we consider our prior Π to be the same as the volume measure P over the classiﬁer space
yielding π as unity. That is, our prior gives a distribution over the classiﬁer space without any
regard to the data. Substituting for ψzm (i, σ) (the fraction of respective densities; Radon-Nikodym
derivative)3, we obtain the following result:

Theorem 4 For any reconstruction function R : Dm × K −→ H and for any prior distribution
P over compression set and message strings, the sample compression algorithms A(S ) returns a
posterior distribution Q, then, for δ ∈ (0, 1] and k > 0, we have:
i ) (cid:20)kl(Rzi (zi , σ)kR(zi , σ))
Pr
S∼Dm ,i∼QI ,σ∼QM(z
m − |i| (cid:20) ln (cid:0)
k + 1
1
(cid:1) + (1 +
δ
where Rzi (zi , σ) is the empirical risk of the classiﬁer reconstructed from (zi , σ) on the training
examples not in the compression set and R(zi , σ) is the corresponding true risk.

) ln+ (cid:18) Q(zi , σ)
P (zi , σ) (cid:19)(cid:21)(cid:21) ≥ 1 − δ

1
k

≤

1
1
factor in the bound instead of
m−|i| unlike the bound
Note that we do not encounter the
m−dQ
of Laviolette and Marchand [2007]. This is because the PAC-Bayes bound of Laviolette and Marc-
hand [2007] computes the expectancy over the kl-divergence of the empirical and true risk of the
classiﬁers chosen according to Q. This, as a result of rescaling of Q in preference of smaller com-
pression sets, is re ﬂected in the bound. On the other hand, th e bound of Theorem 4 is a point-wise
version bounding the true error of the speciﬁc classiﬁer chosen
according to Q and hence concerns
the speciﬁc compression set utilized by this classiﬁer.

4.2 A Binomial Tail Inversion Bound for randomized SC classiﬁer
A tighter condition can be imposed on the true risk of the classiﬁer by considering the binomial tail
m , δ(cid:1) is de ﬁned as the
inversion over the distribution of errors. The binomial tail inversion Bin (cid:0) k
largest risk value that a classiﬁer can have while still havi ng a probability of at least δ of observing
at most k errors out of m examples:
Bin (cid:18) k
, δ(cid:19) def= sup (cid:26)r : Bin (cid:18) k
m
m

, r(cid:19) ≥ δ(cid:27)

where

, r(cid:19) def=

Bin (cid:18) k
m

(cid:18)m
j (cid:19)rj (1 − r)m−j

k
Xj=0
From this de ﬁnition, it follows that Bin (RS (f ), δ) is the smallest upper bound, which holds with
probability at least 1 − δ , on the true risk of any classiﬁer f with an observed empirical risk RS (f )
on a test set of m examples (test set bound):
PZm (cid:26)R(f ) ≤ Bin(cid:16)RZm (f ), δ(cid:17)(cid:27) ≥ 1 − δ ∀f
This bound can be converted to a training set bound in a standard manner by considering a measure
over the classiﬁer space (see for instance [Langford, 2005, Theorem 4.1]). Moreover, in the sample
compression case, we are interested in the empirical risk of the classiﬁer on the examples not in the
compression set (consistent compression set assumption). Now, let δr be a δ -weighed measure on
the classiﬁer space, i.e.,
i and σ . Then, for the compression sets and associated message strings,

(4)

2Hence, the missing S in the subscript of π (h) in the r.h.s. above.
3Alternatively, let P (zi , σ ) and Q(zi , σ ) denote the probability densities of the prior distribution P and
rescaled posterior distributions Q over classi ﬁers such that dQ = Q(zi , σ )dµ and dP = P (zi , σ )dµ w.r.t.
dP = Q(zi ,σ)
some measure µ. This too yields dQ
P (zi ,σ) . Note that the ﬁnal expression is independent of the underly ing
measure µ.

consider the following bad event with empirical risk of the classiﬁer measured as BinS ((zi , σ)) for
i ∼ QI , σ ∼ QM(zi ) :

B (h, δ) = (cid:8)R(zi , σ) > Bin(Rzi (zi , σ), δr )(cid:9)
Now, we replace δr with the level function of Occam’s hammer (with the same assumption of Π =
P, π = 1):

min(δπ(hS )β (ψzm (i, σ)−1 ), 1) ≤ δ · min((k + 1)−1ψzm (i, σ)− k+1
k , 1)
1

≤ δ ·

≤ δ

max((k + 1)ψzm (i, σ)
1

k+1
k , 1)

(k + 1) max(ψzm (i, σ)
δ

k+1
k , 1)

Hence, we have proved the following:

≤

(k + 1)ψzm (i, σ)

k+1
k

Theorem 5 For any reconstruction function R : Dm × K −→ H and for any prior distribution P
over the compression set and message strings, the sample compression algorithms A(S ) returns a
posterior distribution Q, then, for δ ∈ (0, 1] and k > 0, we have:
i ) (cid:20)R(zi , σ) ≤ Bin(cid:18)Rzi (zi , σ),
Pr
S∼Dm ,i∼QI ,σ∼QM(z

δ
(k + 1)(cid:0) Q(zi ,σ)
P (zi ,σ) (cid:1)
We can obtain a looser bound by approximating the binomial tail inversion bound using [Laviolette
et al., 2005, Lemma 1]:

k (cid:19)(cid:21) ≥ 1 − δ
k+1

Corollary 6 Given all our previous de ﬁnitions, the following holds with probability 1 − δ over the
joint draw of S ∼ Dm and i ∼ QI , σ ∼ QM(zi ) :
m − |i| − |i|Rzi (zi , σ) (cid:20) ln (cid:18) m − |i|
δ (cid:19)
R(zi , σ) ≤ 1 − exp (cid:18)
|i|Rzi (zi , σ)(cid:19) + ln (cid:18) k + 1
−1
P (zi , σ) (cid:19)(cid:21)(cid:19)
) ln (cid:18) Q(zi , σ)
1
k

+ (1 +

5 Recovering the PAC-Bayes bound for SC Gibbs Classi ﬁer
Let us now see how a bound can be obtained for the Gibbs setting. We follow the general line of
argument of Blanchard and Fleuret [2007] to recover the PAC-Bayes bound for the Sample Com-
pressed Gibbs classiﬁer. However, note that we do this for th e data-dependent setting here and also
utilize the rescaled posterior over the space of sample compressed classiﬁers.

The PAC-Bayes bound of Theorem 4 basically states that

ES∼Dm [
Pr
i∼QI ,σ∼QM(z
i )

[kl(Rzi (zi , σ)kR(zi , σ)) > ϕ(δ)]] ≤ δ

ϕ(δ) =

m − |i| (cid:20) ln (cid:0)
1

k + 1
δ

(cid:1) + (1 +

1
k

) ln+ (cid:18) Q(zi , σ)
P (zi , σ) (cid:19)(cid:21)

where

Consequently,

ES∼Dm [
Pr
i∼QI ,σ∼QM(z
i )

[kl(Rzi (zi , σ)kR(zi , σ)) > ϕ(δγ )]] ≤ δγ

Now, bounding the argument of expectancy above using the Markov inequality, we get:
[kl(Rzi (zi , σ)kR(zi , σ)) > ϕ(δγ )] > γ (cid:21) ≤ δ
S∼Dm (cid:20)
Pr

Pr
i∼QI ,σ∼QM(z
i )

Now, discretizing the argument over (δi , γi ) = (δ2−i , 2−i ), we obtain
[kl(Rzi (zi , σ)kR(zi , σ)) > ϕ(δi γi )] > γi(cid:21) ≤ δi
S∼Dm (cid:20)
Pr
Pr
i∼QI ,σ∼QM(z
i )
Taking the union bound over δi , i ≥ 1 now yields:
[kl(Rzi (zi , σ)kR(zi , σ)) > ϕ(δ2−2i ] ≤ 2−i(cid:21) > 1 − δ
S∼Dm (cid:20)
Pr
Pr
i∼QI ,σ∼QM(z
i )
Now, let us consider the argument of the above statement for a ﬁxed sample S . Then, for all i ≥ 0,
the following holds with probability 1 − δ :
i ) (cid:20)kl(Rzi (zi , σ)kR(zi , σ)) >
Pr
i∼QI ,σ∼QM(z

k + 1
δ

∀i ≥ 0

m − |i| (cid:20) ln (cid:0)
1
+ (1 +

(cid:1) + 2i ln 2
) ln+ (cid:18) Q(zi , σ)
P (zi , σ) (cid:19)(cid:21)(cid:21) ≤ 2−i
1
k
i ) (cid:20)ΦS (zi , σ) > 2i ln 2(cid:21) ≤ 2−i
Pr
i∼QI ,σ∼QM(z

and hence:

where:
ΦS (zi , σ) = (m − |i|)kl(Rzi (zi , σ)kR(zi , σ)) − ln (cid:0)
We wish to bound, for the Gibbs classiﬁer, Ei∼QI ,σ∼QM(z
i )
[ΦS (zi , σ)] ≤ Z2i ln 2>0
≤ 2 ln 2 Xi≥0

Ei∼QI ,σ∼QM(z
i )

Pr
i∼QI ,σ∼QM(z
i )

k + 1
(cid:1) − (1 +
δ
ΦS (zi , σ):

1
k

P (zi , σ) (cid:19)
) ln+ (cid:18) Q(zi , σ)

[ΦS (zi , σ) ≥ 2i ln 2]d(2i ln 2)

Pri∼QI ,σ∼QM(z
i )

[ΦS (zi , σ) ≥ 2i ln 2] ≤ 3

(5)

Now, we have:

Lemma 7 [Laviolette and Marchand, 2007] For any f : K −→ R+ , and for any Q, Q′ ∈ PK
related by

Q′ (zi , σ)f (zi , σ) =

1
E(zi ,σ)∼Q

Q(zi , σ),

1
f (zi ,σ)

we have:
E(zi ,σ)∼Q′ (cid:18)f (zi , σ)kl(Rzi (zi , σ)kR(zi , σ))(cid:19) ≥
1
1
f (zi ,σ) (cid:1)
E(zi ,σ)∼Q (cid:0)
where RS (GQ ) and R(GQ ) denote the empirical and true risk of the Gibbs classiﬁer wit h posterior
Q respectively.

kl(RS (GQ )kR(GQ ))

Hence, with Q′ = Q and f (zi , σ) = |i|, Lemma 7 yields:

E(zi ,σ)∼Q (|i|kl(Rzi (zi , σ)kR(zi , σ))) ≥

1
1
m−dQ

kl(RS (GQ )kR(GQ ))

(6)

Further,

i ) (cid:20) ln+
Ei∼QI ,σ∼QM(z

P (zi , σ) (cid:21) = Ei∼QI ,σ∼QM(z
PI (i)PM(zi ) (σ) (cid:19)(cid:21)
i ) (cid:20) ln+ (cid:18)
Q(zi , σ)
Q(zi , σ)
= E(zi ,σ)∼P (cid:20)(cid:18)
PI (i)PM(zi ) (σ) (cid:19) · ln+ (cid:18)
PI (i)PM(zi ) (σ) (cid:19)(cid:21)
Q(zi , σ)
Q(zi , σ)
PI (i)PM(zi ) (σ) (cid:19)(cid:21)
PI (i)PM(zi ) (σ) (cid:19) · ln (cid:18)
≤ E(zi ,σ)∼P (cid:20)(cid:18)
Q(zi , σ)
Q(zi , σ)
− max
x ln x
0≤x<1

≤ KL(QkP ) + 0.5

(7)

Equations 6 and 7 along with Equation 5 and substituting k = m − 1 yields the ﬁnal result:

Theorem 8 For any reconstruction functionR : Dm × K −→ H and for any prior distribution P
over compression set and message strings, for δ ∈ (0, 1], we have:
S∼Dm (cid:18)∀Q ∈ PK : kl(RS (GQ )kR(GQ ))
Pr
m − dQ (cid:20)(cid:0)1 +
δ (cid:1) + 3.5(cid:21)(cid:19) ≥ 1 − δ
1
1
m
m − 1 (cid:1)KL(QkP ) +
+ ln (cid:0)
≤
Theorem 8 recovers almost exactly the PAC-Bayes bound for the Sample Compressed Classiﬁers
1
(m−dQ )(m−1) weighted
of Laviolette and Marchand [2007]. The key differences are an additional
δ ) instead of the ln( m+1
KL-divergence term, ln( m
) and the additional trailing terms bounded by
δ
4
. Note that the bound of Theorem 8 is derived in a relatively more straightforward manner
m−dQ
with the Occam’s Hammer criterion.

1
2(m − 1)

6 Conclusion
It has been shown that stochastic classiﬁer selection is pre ferable to deterministic selection by the
PAC-Bayes principle resulting in tighter risk bounds over averaged risk of classiﬁers according to
the learned posterior. Further, this observation resulted in tight bounds in the case of stochastic
sample compressed classiﬁers [Laviolette and Marchand, 20 07] also showing that sparsity consid-
erations are of importance even in this scenario via. the rescaled posterior. However, of immediate
relevance are the guarantees of the speciﬁc classiﬁer outpu
t by such algorithms according to the
learned posterior and hence a point-wise version of this bound is indeed needed. We have derived
bounds for such randomized sample compressed classiﬁers by adapting Occam’s Hammer principle
to the data-dependent sample compression settings. This has resulted in bounds on the speciﬁc clas-
siﬁer output by a sample compression learning algorithm acc ording to the learned data-dependent
posterior and is more relevant in practice. Further, we also showed how classical PAC-Bayes bound
for the sample compressed Gibbs classiﬁer can be recovered i n a more direct manner and show that
this compares favorably to the existing result of Laviolette and Marchand [2007].

Acknowledgments

The author would like to thank John Langford for interesting discussions.

References
Gilles Blanchard and Franc¸ois Fleuret. Occam’s hammer. In Proceedings of the 20th Annual Con-
ference on Learning Theory (COLT-2007), volume 4539 of Lecture Notes on Computer Science,
pages 112 –126, 2007.
Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis
dimension. Machine Learning, 21(3):269 –304, 1995.
John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learn-
ing Research, 3:273 –306, 2005.
Franc¸ois Laviolette and Mario Marchand. PAC-Bayes risk bounds for stochastic averages and major-
ity votes of sample-compressed classiﬁers. Journal of Machine Learning Research, 8:1461 –1487,
2007.
Francois Laviolette, Mario Marchand, and Mohak Shah. Margin-sparsity trade-off for the set cov-
ering machine. In Proceedings of the 16th European Conference on Machine Learning, ECML
2005, volume 3720 of Lecture Notes in Artiﬁcial Intelligence , pages 206 –217. Springer, 2005.
N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical report,
University of California Santa Cruz, Santa Cruz, CA, 1986.
Mario Marchand and John Shawe-Taylor. The Set Covering Machine. Journal of Machine Learning
Reasearch, 3:723 –746, 2002.
David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355 –363, 1999.


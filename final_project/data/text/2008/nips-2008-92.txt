Theory of matching pursuit

Zakria Hussain and John Shawe-Taylor
Department of Computer Science
University College London, UK
{z.hussain,j.shawe-taylor}@cs.ucl.ac.uk

Abstract

We analyse matching pursuit for kernel principal components analysis (KPCA)
by proving that the sparse subspace it produces is a sample compression scheme.
We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al
[7] and highly predictive of the size of the subspace needed to capture most of the
variance in the data. We analyse a second matching pursuit algorithm called ker-
nel matching pursuit (KMP) which does not correspond to a sample compression
scheme. However, we give a novel bound that views the choice of subspace of the
KMP algorithm as a compression scheme and hence provide a VC bound to upper
bound its future loss. Finally we describe how the same bound can be applied to
other matching pursuit related algorithms.

1

Introduction

Matching pursuit refers to a family of algorithms that generate a set of bases for learning in a greedy
fashion. A good example of this approach is the matching pursuit algorithm [4]. Viewed from this
angle sparse kernel principal components analysis (PCA) looks for a small number of kernel ba-
sis vectors in order to maximise the Rayleigh quotient. The algorithm was proposed by [8]1 and
motivated by matching pursuit [4], but to our knowledge sparse PCA has not been analysed theo-
retically. In this paper we show that sparse PCA (KPCA) is a sample compression scheme and can
be bounded using the size of the compression set [3, 2] which is the set of training examples used
in the construction of the KPCA subspace. We also derive a more general framework for this algo-
rithm that uses the principle “maximise Rayleigh quotient and deﬂate”. A related algorithm called
kernel matching pursuit (KMP) [10] is a sparse version of least squares regression but without the
property of being a compression scheme. However, we use the number of basis vectors constructed
by KMP to help upper bound the loss of the KMP algorithm using the VC dimension. This bound
is novel in that it is applied in an empirically chosen low dimensional hypothesis space and applies
independently of the actual dimension of the ambient feature space (including one constructed from
the Gaussian kernel). In both cases we illustrate the use of the bounds on real and/or simulated data.
Finally we also show that the KMP bound can be applied to a sparse kernel canonical correlation
analysis that uses a similar matching pursuit technique. We do not describe the algorithm here due to
space constraints and only concentrate on theoretical results. We begin with preliminary deﬁnitions.

2 Preliminary deﬁnitions

Throughout the paper we consider learning from samples of data. For the regression section the
i=1 of input-output pairs drawn from a joint space X × Y where
data is a sample S = {(xi , yi )}m
x ∈ Rn and y ∈ R. For the principal components analysis the data is a sample S = {xi }m
i=1 of
1The algorithm was proposed as a low rank kernel approximation – however the algorithm turns out to be a
sparse kernel PCA (to be shown).

1

multivariate examples drawn from a space X . For simplicity we always assume that the examples
are already projected into the kernel deﬁned feature space, so that the kernel matrix K has entries
K[i, j ] = (cid:104)xi , xj (cid:105). The notation K[i, :] and K[:, i] will denote the ith row and ith column of the
matrix K, respectively. When using a set of indices i = {i1 , . . . , ik } (say) then K[i, i] denotes the
square matrix deﬁned solely by the index set i. The transpose of a matrix X or vector x is denoted
by X(cid:48) or x(cid:48) respectively. The input data matrix X will contain examples as row vectors.
For analysis purposes we assume that the training examples are generated i.i.d. according to an un-
known but ﬁxed probability distribution that also governs the generation of the test data. Expectation
over the training examples (empirical average) is denoted by ˆE [·], while expectation with respect to
the underlying distribution is denoted E [·]. For the sample compression analysis the compression
function Λ induced by a sample compression learning algorithm A on training set S is the map
Λ : S (cid:55)−→ Λ(S ) such that the compression set Λ(S ) ⊂ S is returned by A. A reconstruction func-
tion Φ is a mapping from a compression set Λ(S ) to a set F of functions Φ : Λ(S ) (cid:55)−→ F . Let A(S )
be the function output by learning algorithm A on training set S . Therefore, a sample compression
scheme is a reconstruction function Φ mapping a compression set Λ(S ) to some set of functions F
such that A(S ) = Φ(Λ(S )). If F is the set of Boolean-valued or Real-valued functions then the
sample compression scheme is said to be a classiﬁcation or regression algorithm, respectively.

2.1 Sparse kernel principal components analysis

Principal components analysis [6] can be expressed as the following maximisation problem:
w(cid:48)X(cid:48)Xw
max
(1)
w(cid:48)w ,
w
where w is the weight vector. In a sparse KPCA algorithm we would like to ﬁnd a sparsely repre-
sented vector w = X[i, :](cid:48) ˜α, that is a linear combination of a small number of training examples
indexed by vector i. Therefore making this substitution into Equation (1) we have the following
sparse dual PCA maximisation problem,
˜α(cid:48)X[i, :]X(cid:48)XX[i, :](cid:48) ˜α
max
,
˜α(cid:48)X[i, :]X[i, :](cid:48) ˜α
˜α
which is equivalent to sparse kernel PCA (SKPCA) with sparse kernel matrix K[:, i](cid:48) = X[i, :]X(cid:48) ,
˜α(cid:48)K[:, i](cid:48)K[:, i] ˜α
max
,
˜α(cid:48)K[i, i] ˜α
˜α
where ˜α is a sparse vector of length k = |i|. Clearly maximising the quantity above will lead to
the maximisation of the generalised eigenvalues corresponding to ˜α – and hence a sparse subset of
the original KPCA problem. We would like to ﬁnd the optimal set of indices i. We proceed in a
greedy manner (matching pursuit) in much the same way as [8]. The procedure involves choosing
basis vectors that maximise the Rayleigh quotient without the set of eigenvectors. Choosing basis
vectors iteratively until some pre-speciﬁed number of k vectors are chosen. An orthogonalisation
of the kernel matrix at each step ensures future potential basis vectors will be orthogonal to those
already chosen. The quotient to maximise is:

e(cid:48)
iK2ei
e(cid:48)
iKei
where ei is the ith unit vector. After this maximisation we need to orthogonalise (deﬂate) the kernel
matrix to create a projection into the space orthogonal to the basis vectors chosen to ensure we ﬁnd
the maximum variance of the data in the projected space. The deﬂation step can be carried out as
follows. Let τ = K[:, i] = XX(cid:48)ei where ei is the ith unit vector. We know that primal PCA
(cid:19)
(cid:18)
deﬂation can be carried out with respect to the features in the following way:
I − uu(cid:48)
X(cid:48) ,
ˆX(cid:48) =
u(cid:48)u
where u is the projection directions deﬁned by the chosen eigenvector and ˆX is the deﬂated matrix.
However, in sparse KPCA, u = X(cid:48)ei because the projection directions are simply the examples in
(cid:19) (cid:18)
(cid:19)
(cid:18)
X. Therefore, for sparse KPCA we have:
= K − K[:, i]K[:, i](cid:48)
X(cid:48) = XX(cid:48) − XX(cid:48)eie(cid:48)
I − uu(cid:48)
I − uu(cid:48)
iXX(cid:48)
ˆX ˆX(cid:48) = X
e(cid:48)
u(cid:48)u
u(cid:48)u
iXX(cid:48)ei
K[i, i]

(2)

max ρi =

,

.

2

Therefore, given a kernel matrix K the deﬂated kernel matrix ˆK can be computed as follows:
ˆK = K − τ τ (cid:48)
K[ik , ik ]
where τ = K[:, ik ] and ik denotes the latest element in the vector i. The algorithm is presented
below in Algorithm 1 and we use the notation K.2 to denote component wise squaring. Also,
division of vectors are assumed to be component wise.

(3)

Algorithm 1: A matching pursuit algorithm for kernel principal components analysis (i.e., sparse
KPCA)
Input: Kernel K, sparsity parameter k > 0.
(cid:110) (K.2 )(cid:48) 1
(cid:111)
1: initialise i = [ ]
2: for j = 1 to k do
Set ij to index of max
3:
diag{K}
set τ = K[:, ij ] to deﬂate kernel matrix like so: K = K − τ τ (cid:48)
4:
K[ij ,ij ]
5: end for
6: Compute ˜K using i and Equation (5)
Output: Output sparse matrix approximation ˜K

This algorithm is presented in Algorithm 1 and is equivalent to the algorithm proposed by [8].
However, their motivation comes from the stance of ﬁnding a low rank matrix approximation of
the kernel matrix. They proceed by looking for an approximation ˜K = K[:, i]T for a set i such
that the Frobenius norm between the trace residuals tr{K − K[:, i]T } = tr{K − ˜K} is minimal.
Their algorithm ﬁnds the set of indices i and the projection matrix T . However, the use of T in
computing the low rank matrix approximation seems to imply the need for additional information
from outside of the chosen basis vectors in order to construct this approximation. However, we
show that a projection into the space deﬁned solely by the chosen indices is enough to reconstruct
the kernel matrix and does not require any extra information.2 The projection is the well known
Nystr ¨om method [11].
An orthogonal projection Pi (φ(xj )) of a feature vector φ(xj ) into a subspace deﬁned only by the
set of indices i can be expressed as: Pi (xj ) = ˜X(cid:48) ( ˜X ˜X(cid:48) )−1 ˜Xφ(xj ), where ˜X = X[i, :] are the i
training examples from data matrix X. It follows that,
Pi (xj )(cid:48)Pi (xj ) = φ(xj )(cid:48) ˜X(cid:48) ( ˜X ˜X(cid:48) )−1 ˜X ˜X(cid:48) ( ˜X ˜X(cid:48) )−1 ˜Xφ(xj )
= K[i, j ]K[i, i]−1K[j, i],
(4)
with K[i, j ] denoting the kernel entries between the index set i and the feature vector φ(xj ). Giving
us the following projection into the space deﬁned by i:
˜K = K[:, i]K[i, i]−1K[:, i](cid:48) .
Claim 1. The sparse kernel principal components analysis algorithm is a compression scheme.

(5)

Proof. We can reconstruct the projection from the set of chosen indices i using Equation (4). Hence,
i forms a compression set.

We now prove that Smola and Sch ¨olkopf ’s low rank matrix approximation algorithm [8] (without
sub-sampling)3 is equivalent to sparse kernel principal components analysis presented in this paper
(Algorithm 1).
Theorem 1. Without sub-sampling, Algorithm 1 is equivalent to Algorithm 2 of [8].

2 In their book, Smola and Sch ¨olkopf redeﬁne their kernel approximation in the same way as we have done
[5], however they do not make the connection that it is a compression scheme (see Claim 1).
3We do not use the “59-trick” in our algorithm – although it’s inclusion would be trivial and would result in
the same algorithm as in [8]

3

Proof. Let K be the kernel matrix and let K[:, i] be the ith column of the kernel matrix. Assume X is
the input matrix containing rows of vectors that have already been mapped into a higher dimensional
feature space using φ such that X = (φ(x1 ), . . . , φ(xm ))(cid:48) . Smola and Sch ¨olkopf [8] state in section
4.2 of their paper that their algorithm 2 ﬁnds a low rank approximation of the kernel matrix such that
Frob = tr{K− ˜K} where ˜X is the low rank approximation
it minimises the Frobenius norm (cid:107)X− ˜X(cid:107)2
of X. Therefore, we need to prove that Algorithm 1 also minimises this norm.
We would like to show that the maximum reduction in the Frobenius norm between the kernel K
and its projection ˜K is in actual fact the choice of basis vectors that maximise the Rayleigh quotient
and deﬂate according to Equation (3). At each stage we deﬂate by,
K = K − τ τ (cid:48)
The trace tr{K} = (cid:80)m
K[ik , ik ] .
i=1 K[i, i] is the sum of the diagonal elements of matrix K. Therefore,
= tr{K} − tr{τ (cid:48)τ }
tr{K} = tr{K} − tr{τ τ (cid:48)}
= tr{K} − K2 [ik , ik ]
K[ik , ik ] .
K[ik , ik ]
K[ik , ik ]
The last term of the ﬁnal equation corresponds exactly to the Rayleigh quotient of Equation (2).
Therefore the maximisation of the Rayleigh quotient does indeed correspond to the maximum re-
duction in the Frobenius norm between the approximated matrix ˜X and X.

2.2 A generalisation error bound for sparse kernel principal components analysis

We use the sample compression framework of [3] to bound the generalisation error of the sparse
KPCA algorithm. Note that kernel PCA bounds [7] do not use sample compression in order to
bound the true error. As pointed out above, we use the simple fact that this algorithm can be viewed
as a compression scheme. No side information is needed in this setting and a simple application
of [3] is all that is required. That said the usual application of compression bounds has been for
classiﬁcation algorithms, while here we are considering a subspace method.
Theorem 2. Let Ak be any learning algorithm having a reconstruction function that maps compres-
sion sets to subspaces. Let m be the size of the training set S , let k be the size of the compression
set, let ˆEm−k [(cid:96)(Ak (S ))] be the residual loss between the m − k points outside of the compression
set and their projections into a subspace, then with probability 1 − δ , the expected loss E [(cid:96)(Ak (S ))]
(cid:115)
(cid:34)
(cid:19)(cid:21)(cid:35)
(cid:18) 2m
(cid:20)
(cid:16) em
(cid:17)
of algorithm Ak given any training set S can be bounded by,
E [(cid:96)(Ak (S ))] ≤ min
ˆEm−t [(cid:96)(At (S ))] +
R
t ln
+ ln
2(m − t)
,
1≤t≤k
t
δ
where (cid:96)(·) ≥ 0 and R = sup (cid:96)(·).
Proof. Consider the case where we have a compression set of size k . Then we have (cid:0)m
(cid:1) different
points not in the compression set once for each choice by setting it equal to δ/(cid:0)m
(cid:1). Solving for 
ways of choosing the compression set. Given δ conﬁdence we apply Hoeffding’s bound to the m − k
k
k
gives us the theorem when we further apply a factor 1/m to δ to ensure one application for each
possible choice of k . The minimisation over t chooses the best value making use of the fact that
using more dimensions can only reduce the expected loss on test points.

We now consider the application of the above bound to sparse KPCA. Let the corresponding loss
function be deﬁned as
(cid:96)(At (S ))(x) = (cid:107)x − Pit (x)(cid:107)2 ,
where x is a test point and Pit (x) its projection into the subspace determined by the set it of indices
returned by At (S ). Thus we can give a more speciﬁc loss bound in the case where we use a Gaussian
kernel in the sparse kernel principal components analysis.
(cid:115)
(cid:19)(cid:21)(cid:35)
(cid:34)
Corollary 1 (Sample compression bound for sparse KPCA). Using a Gaussian kernel and all of the
(cid:18) 2m
(cid:20)
(cid:17)
(cid:16) em
m−t(cid:88)
deﬁnitions from Theorem 2, we get the following bound:
1
(cid:107)xi − Pit (xi )(cid:107)2 +
E [(cid:96)(A(S ))] ≤ min
t ln
m − t
1≤t≤k
t
δ
i=1

1
2(m − t)

+ ln

,

4

Note that R corresponds to the smallest radius of a ball that encloses all of the training points. Hence,
for the Gaussian kernel R equals 1. We now compare the sample compression bound proposed
above for sparse KPCA with the kernel PCA bound introduced by [7]. The left hand side of Figure 1
shows plots for the test error residuals (for the Boston housing data set) together with its upper
bounds computed using the bound of [7] and the sample compression bound of Corollary 1. The
sample compression bound is much tighter than the KPCA bound and also non-trivial (unlike the
KPCA bound).
The sample compression bound is at its lowest point after 43 basis vectors have been added. We
speculate that at this point the “true” dimensions of the data have been found and that all other di-
mensions correspond to “noise”. This corresponds to the point at which the plot of residual becomes
linear, suggesting dimensions with uniform noise. We carry out an extra toy experiment to help
assess whether or not this is true and to show that the sample compression bound can help indicate
when the principal components have captured most of the actual data. The right hand side plot of
Figure 1 depicts the results of a toy experiment where we randomly sampled 1000 examples with
450 dimensions from a Gaussian distribution with zero mean and unit variance. We then ensured
that 50 dimensions contained considerably larger eigenvalues than the remaining 400. From the
right plot of Figure 1 we see that the test residual keeps dropping at a constant rate after 50 basis
vectors have been added. The compression bound picks 46 dimensions with the largest eigenvalues,
however, the KPCA bound of [7] is much more optimistic and is at its lowest point after 30 basis
vectors, suggesting erroneously that SKPCA has captured most of the data in 30 dimensions. There-
fore, as well as being tighter and non-trivial, the compression bound is much better at predicting the
best choice for the number of dimensions to use with sparse KPCA. Note that we carried out this
experiment without randomly permuting the projections into a subspace because SKPCA is rotation
invariant and will always choose the principal components with the largest eigenvalues.

Figure 1: Bound plots for sparse kernel PCA comparing the sample compression bound proposed
in this paper and the already existing PCA bound. The plot on the left hand side is for the Boston
Housing data set and the plot on the right is for a Toy experiment with 1000 training examples (and
450 dimensions) drawn randomly from a Gaussian distribution with zero mean and unit variance.

3 Kernel matching pursuit

Unfortunately, the theory of the last section, where we gave a sample compression bound for SKPCA
cannot be applied to KMP. This is because the algorithm needs information from outside of the
compression set in order to construct its regressors and make predictions. However, we can use
a VC argument together with a sample compression trick in order to derive a bound for KMP in
terms of the level of sparsity achieved, by viewing the sparsity achieved in the feature space as a

5

010203040506070809010011012013014015000.20.40.60.811.21.41.61.8Level of sparsityResidualBound plots for sparse kernel PCA  PCA boundsample compression boundtest residual010203040506070809010011012013014015000.511.522.5  X: 46Y: 0.7984X: 30Y: 1.84Level of sparsityResidualBound plots for sparse kernel PCAPCA boundsample compression boundtest residualcompression scheme. Please note that we do not derive or reproduce the KMP algorithm here and
advise the interested reader to read the manuscript of [10] for the algorithmic details.

3.1 A generalisation error bound for kernel matching pursuit

VC bounds have commonly been used to bound learning algorithms whose hypothesis spaces are
inﬁnite. One problem with these results is that the VC-dimension can sometimes be inﬁnite even in
cases where learning is successful (e.g., the SVM). However, in this section we can avoid this issue
by making use of the fact that the VC-dimension of the set of linear threshold functions is simply the
dimensionality of the function class. In the kernel matching pursuit algorithm this translates directly
into the number of basis vectors chosen and hence a standard VC argument.
The natural loss function for KMP is regression – however in order to use standard VC bounds we
map the regression loss into a classiﬁcation loss in the following way.
Deﬁnition 1. Let S ∼ D be a regression training sample generated iid from a ﬁxed but unknown
probability distribution D . Given the error (cid:96)(f ) = |f (x) − y | for a regression function f between
training example x and regression output y we can deﬁne, for some ﬁxed positive scalar α ∈ R, the
corresponding true classiﬁcation loss (error) as
(cid:96)α (f ) =

{|f (x) − y | > α} .

Pr
(x,y)∼D

Similarly, we can deﬁne the corresponding empirical classiﬁcation loss as
{|f (x) − y | > α} = E(x,y)∼S {I(|f (x) − y | > α)} ,
ˆ(cid:96)α (f ) = (cid:96)S
α (f ) =
Pr
(x,y)∼S
where I is the indicator function and S is suppressed when clear from context.

Now that we have a loss function that is binary we can make a simple sample compression argument,
that counts the number of possible subspaces, together with a traditional VC style bound to upper
bound the expected loss of KMP. To help keep the notation consistent with earlier deﬁnitions we
will denote the indices of the chosen basis vectors by i. The indices of i are chosen from the training
sample S and we denote Si to be those samples indexed by the vector i. Given these deﬁnitions and
the bound of Vapnik and Chervonenkis [9] we can upper bound the true loss of KMP as follows.
are outside of the set i and let t = (cid:80)m
Theorem 3. Fix α ∈ R, α > 0. Let A be the regression algorithm of KMP, m the size of the training
set S and k the size of the chosen basis vectors i. Let S be reordered so that the last m − k points
I(|f (xi ) − yi | > α) be the number of errors for those
i=m−k
points in S (cid:114) Si . Then with probability 1 − δ over the generation of the training set S the expected
(cid:20)
(cid:18) 4e(m − k − t)
(cid:19)
(cid:17)
(cid:16) em
loss E [(cid:96)(·)] of algorithm A can be bounded by,
2
(cid:18) e(m − k)
(cid:19)(cid:21)
(cid:18) 2m2
(cid:19)
E [(cid:96)(A(S ))] ≤
+ k log
(k + 1) log
m − k − t
k + 1
k
t
δ

+t log

+ log

.

Proof. First consider a ﬁxed size k for the compression set and number of errors t. Let S1 =
{xi1 , . . . , xik } be the set of k training points chosen by the KMP regressor, S2 = {xik+1 , . . . , xik+t }
the set of points erred on in training and ¯S = S (cid:114) (S1 ∪ S2 ) the points outside of the compression
set (S1 ) and training error set (S2 ). Suppose that the ﬁrst k points form the compression set and
the next t are the errors of the KMP regressor. Since the remaining m − k − t points ¯S are drawn
(cid:18) 4e(m − k − t)
(cid:19)k+1
(cid:110) ¯S : (cid:96) ¯S
(cid:111) ≤ 2
independently we can apply the VC bound [9] to the (cid:96)α loss to obtain the bound
2−(m−k−t)/2 ,
α (f ) = 0, (cid:96)α (f ) > 
Pr
k + 1
(cid:1) which is ≤ (cid:16) e(md−1)
(cid:17)k+d−1
(cid:0)md−1
hyperplanes [1], which is (cid:80)k+d−1
where we have made use of a bound on the number of dichotomies that can be generated by parallel
, where d is the number
k+d−1
i=0
i
of parallel hyperplanes and equals 2 in our case. We now need to consider all of the ways that the

6

Pr

k basis vectors and t error points might have occurred and apply the union bound over all of these
(cid:110)
(cid:111)
possibilities. This gives the bound
(cid:19)k+1
(cid:18) 4e(m − k − t)
(cid:19)
(cid:19)(cid:18)m − k
(cid:18)m
S : ∃ f ∈ span{S1 } s.t. (cid:96)S2
α (f ) = 1, (cid:96) ¯S
α (f ) = 0, (cid:96)α (f ) > 
≤
2−(m−k−t)/2 .
k + 1
k
t
Finally we need to consider all possible choices of the values of k and t. The number of these
possibilities is clearly upper bounded by m2 . Setting m2 times the rhs of (6) equal to δ and solving
for  gives the result.

(6)

2

This is the ﬁrst upper bound on the generalisation error for KMP that we are aware of and as such
we cannot compare the bound against any others. Figure 2 plots the KMP test error against the loss

Figure 2: Plot of KMP bound against its test error. We used 450 examples for training and the 56
for testing. Bound was scaled down by a factor of 5.

bound given by Theorem 3. The bound value has been scaled by 5 in order to get the correct pictorial
representation of the two plots. Figure 2 shows its minima directly coincides with the lowest test
error (after 17 basis vectors). This motivates a training algorithm for KMP that would use the bound
as the minimisation criteria and stop once the bound fails to become smaller. Hence, yielding a more
automated training procedure.

4 Extensions

,

max
i

ρi =

The same approach that we have used for bounding the performance of kernel matching pursuit can
be used to bound a matching pursuit version of kernel canonical correlation analysis (KCCA) [6].
By choosing the basis vectors greedily to optimise the quotient:
(cid:113)
e(cid:48)
iKxKy ei
e(cid:48)
xeie(cid:48)
iK2
iK2
y ei
and proceeding in the same manner as Algorithm 1 by deﬂating after each pair of basis vectors are
chosen, we create a sparsely deﬁned subspace within which we can run the standard CCA algorithm.
This again means that the overall algorithm fails to be a compression scheme as side information is
required. However, we can use the same approach described for KMP to bound the expected ﬁt of
the projections from the two views. The resulting bound has the following form.
last m − k paired data points are outside of the set i and deﬁne t = (cid:80)m
Theorem 4. Fix α ∈ R, α > 0. Let A be the SKCCA algorithm, m the size of the paired training sets
S X ×Y and k the cardinality of the set i of chosen basis vectors. Let S X ×Y be reordered so that the
I(|fx (xi ) − fy (yi )| >
i=m−k
7

0510152025303540455000.10.20.30.40.50.60.70.80.9Level of sparsityLossKMP error on Boston housing data set  boundKMP test errorα) to be the number of errors for those points in S X ×Y (cid:114) S X ×Y
, where fx is the projection function
of the X view and fy the projection function of the Y view. Then with probability 1 − δ over the
i
generation of the paired training sets S X ×Y the expected loss E [(cid:96)(·)] of algorithm A can be bounded
(cid:18) 4e(m − k − t)
(cid:19)
(cid:20)
(cid:17)
(cid:16) em
by,
(cid:18) e(m − k)
(cid:18) 2m2
(cid:19)
(cid:19)(cid:21)
+ k log
(k + 1) log
k + 1
k
t
δ

2
m − k − t

E [(cid:96)(A(S ))] ≤

+t log

+ log

.

5 Discussion

Matching pursuit is a meta-scheme for creating learning algorithms for a variety of tasks. We have
presented novel techniques that make it possible to analyse this style of algorithm using a combina-
tion of compression scheme ideas and more traditional learning theory. We have shown how sparse
KPCA is in fact a compression scheme and demonstrated bounds that are able to accurately guide di-
mension selection in some cases. We have also used the techniques to bound the performance of the
kernel matching pursuit (KMP) algorithm and to reinforce the generality of the approach indicated
and how the approach can be extended to a matching pursuit version of KCCA.
The results in this paper imply that the performance of any learning algorithm from the matching
pursuit family can be analysed using a combination of sparse and traditional learning bounds. The
bounds give a general theoretical justiﬁcation of the framework and suggest potential applications
of matching pursuit methods to other learning tasks such as novelty detection, ranking and so on.

Acknowledgements

The work was sponsored by the PASCAL network of excellence and the SMART project.

References
[1] M. Anthony. Partitioning points by parallel planes. Discrete Mathematics, 282:17–21, 2004.
[2] S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis
dimension. Machine Learning, 21(3):269–304, 1995.
[3] N. Littlestone and M. K. Warmuth. Relating data compression and learnability. Technical
report, University of California Santa Cruz, Santa Cruz, CA, 1986.
[4] S. Mallat and Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Transactions
on Signal Processing, 41(12):3397–3415, 1993.
[5] B. Sch ¨olkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[6] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, Cambridge, U.K., 2004.
[7] J. Shawe-Taylor, C. K. I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the
Gram matrix and the generalization error of kernel-PCA. IEEE Transactions on Information
Theory, 51(7):2510–2522, 2005.
[8] A. J. Smola and B. Sch ¨olkopf. Sparse greedy matrix approximation for machine learning. In
Proceedings of 17th International Conference on Machine Learning, pages 911–918. Morgan
Kaufmann, San Francisco, CA, 2000.
[9] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and its Applications, 16(2):264–280, 1971.
[10] P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning, 48:165–187, 2002.
[11] C. K. I. Williams and M. Seeger. Using the Nystr ¨om method to speed up kernel machines. In
Advances in Neural Information Processing Systems, volume 13, pages 682–688. MIT Press,
2001.

8


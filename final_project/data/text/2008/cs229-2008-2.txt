A Predictive Model of Gene Expression in E. Coli

Brendan O’Donoghue, Evan Rosenfeld
Advised by Profs. Markus Covert and Daphne Koller
Pro ject for CS 229, Stanford University, Fall 2008-09

December 10, 2008

1

Introduction

The transcription and expression of genes in organisms are
primarily moderated by transcription factors. A wealth of
gene expression data on E. Coli – recorded under various
experimental conditions – have been collected by the sci-
entiﬁc community. Using this data, a model was trained
to learn the relationship between transcription factors and
gene expression. Bonneau et. al. demonstrated that this
type of analysis was possible using a smaller subset of genes
on H. salinarum NRC-1 [1]. In this paper we show that
their method is generalizable to other organisms and can be
extended to a nearly-complete genome. Their work was fur-
ther extended by using novel machine learning techniques.

2 Motivation

Covert et. al. demonstrated that the steady-state concen-
trations of metabolites in a cell can be predicted to a rea-
sonable level of accuracy using a computational metabolic
network with Boolean constraints [2]. Accuracy of the pre-
dictions may be improved by removing the inﬂexibility of
Boolean constraints and replacing them with functional
constraints from our predictive model. Furthermore, the
Boolean constraints were developed ‘by hand’ from gene ex-
pression and literature data. The method presented in this
paper automates discovery of relationships between gene
expression and transcription factors. These relationships
can be used to replace or augment Boolean constraints.

3 Methods

Gene expression data was collected for approximately 4500
genes across approximately 1000 experimental conditions
from the University of Oklahoma E. Coli Gene Expression
Database [3].

3.1 Data Preprocessing
Data preprocessing is an important step which aggregates
the data into a standard format and corrects for data

1

anomalies which could confound our ﬁndings. A PHP
script was written to correct for common data-entry mis-
takes such as frame-shift errors and to prepare the data to
be imported into MATLAB. E. Coli transcription factors
were identiﬁed from a separate database. The transcrip-
tion factors were removed from the dataset and placed in
a separate matrix. Experiments or genes that were miss-
ing more than 10% of their total data were removed from
the dataset so that they would not bias the biclusters. It
was necessary to normalize the data along the experiment
dimension because diﬀerent technicians and diﬀerent labs
introduce signiﬁcant variation into the collected data. Nor-
malization is an attempt to remove some of this variation.

3.2 Biclustering

Initially, hierarchical K-means clustering was run on the
dataset to identify co-expressed genes along a subset of
experimental conditions.
It was later decided that co-
expressed genes are not necessarily co-regulated. As this
paper seeks to explore transcription factor regulation of
genes, a more advanced technique was required to identify
co-regulated genes.
Biclustering is a method for simultaneously clustering
along the rows and columns of a matrix to ﬁnd highly corre-
lated subsets of rows and columns within a dataset. Church
and Cheng have proposed that biclustering is a more bio-
logically relevant form of clustering [4] for co-regulated net-
work discovery. The variation of biclustering employed in
this paper worked as follows:

1. A random subset of genes and experimental conditions
is chosen as a seed bicluster

2. Let B be the set of rows in the bicluster. The
variances of all columns in the dataset and the means
of all columns using only the rows in B are found:
I(cid:88)
Mij
i=1

µj =

1
I

(cid:48)=

µj

Mij

1
I

1
B

j =
σ2

I(cid:88)
(Mij − µj )2
i=1
(cid:88)
i∈B
3. A Z-score is calculated for every element in the
(cid:48)
dataset: Zij = Mij −µ
.
j
σj
4. Z-scores are transformed into probabilities p(xij |B ) by
integrating the two tails above and below ±|Zij | of a
standard normal distribution. The values represent
the probability of each element being in the bicluster
given the current bicluster composition.

5. Elementwise products are taken across rows and
columns to generate the probabilities of the rows and
columns being in the bicluster.
p(xj |B ) = (cid:89)
p(xi |B ) = (cid:89)
i
j
6. These probabilities are compared to a random number
to select rows and columns to be included into the
bicluster based on an annealing schedule.

p(xij |B )

p(xij |B )

7. Steps 2 to 6 are repeated until convergence: when the
rows and columns of the bicluster do not change.

The biclustered genes traces are then averaged to produce
one signal to be regressed on. This reduces the noise of the
signal to be regressed, as co-regulated genes should have the
same expression level under their co-regulation conditions.

3.3 Generating Regressors
In order to reduce the dimensionality of the problem the
transcription factors were clustered using K-means cluster-
ing. Within K-means, the Pearson correlation coeﬃcient
distance metric was used instead of the more common (cid:96)2
norm. This is because the goal is to cluster correlated
genes. Two genes that are perfectly correlated but scaled
may have a larger (cid:96)2 distance than two genes that are not
as well correlated but of similar range.
It was desirable to cluster highly correlated genes be-
cause the regressors generated from this process were used
in an (cid:96)1 regularized regression. If two genes are perfectly
correlated but scaled versions of each other, the (cid:96)1 regu-
larization will pick out only the larger one because it can

2

attach to it a smaller coeﬃcient. Therefore, we would not
identify a relationship between the smaller scaled gene and
the signal. By using the Pearson correlation coeﬃcient dis-
tance metric, we have a higher probability of identifying
correlated but scaled genes as important regressors of the
signal.
The transcription factor signals in the clusters are then
averaged.
Next pairwise minimums of all of the cluster traces were
generated. The minimum of two continuous functions is
the continuous analog of the Boolean AND of two binary
signals. By incorporating the ‘AND’s and the signals them-
selves, the continuous analogs of OR and XOR can also be
generated as shown in the table below [5]:

min(a,b)
a
b

AND OR XOR
-2
-1
1
1
1
0
0
1
1

There is a problem that arises from this table. Because
the (cid:96)1 regularization penalizes each coeﬃcient, XORs and
ORs are more expensive to include than ANDs. An at-
tempt was made to incorporate ANDs, ORs and XORs as
their own variables in order to solve this problem, but this
process was too computationally intensive.

3.4 Regression
In order to test our regression models, holdout cross val-
idation was employed. Regression was performed on 60%
of the bicluster averages, and the remaining 40% was used
to test the regression model. Regressions were performed
in two steps:

1. A standard least-squares regression was performed to
identify a parameter vector βunc
minimize (cid:107)Rβunc − s(cid:107)2 ,

where s is the training subset of the bicluster average
and R is the training subset of the matrix of regressors.

2. A least-squares regression with (cid:96)1 -regularization is
performed:

(cid:107)Rβc − s(cid:107)2
minimize
sub ject to (cid:107)βc(cid:107)1 ≤ t(cid:107)βunc(cid:107)1

These ob jectives are all convex and as such can be easily
solved with a standard convex solver [6, 7]. At the end
of this process, with the appropriate value for t , only the
regressors that best explain the data will be active. It is

possible to perform a third regression on these active regres-
sors without a constraint term to ﬁnd the optimal values
for the active regressors, this is called polishing. However,
polishing was not found to signiﬁcantly improve the results.
Thus Rβc = ˆs becomes our best ﬁt estimate of the signal,

s.

3.5 Measures of Goodness

Three measures of goodness were used to evaluate the ac-
curacy of the regressions on the test set.

(cid:107)s − ˆs(cid:107)2
1. η =
: a normalized squared error ratio.
2
(cid:107)s(cid:107)2
When 0 ≤ η < 1, the regression predicts the signal
2
to some degree.

2. ρ(s,ˆs): the standard cross-correlation between s and
ˆs.

N (cid:107)s − ˆs(cid:107)2
3. M SE = 1
2 : the mean-squared error of the
regression.

4 Results

After preprocessing, the dataset comprised of 3638 genes
across 664 experimental conditions and 157 transcription
factors. K-means clustering resulted in approximately 130
clusters of transcription factors, depending on the partic-
ular bicluster. These were converted into approximately
8, 500 regressors, including a constant regressor, which was
not sub ject to the (cid:96)1 regularization.
An example bicluster with 10 genes across 100 condi-
tions is shown in ﬁgure 1. As you can see, the biclustering
algorithm has selected a highly correlated subset of genes
across a subset of conditions. The genes selected by this
bicluster were yeiM, hyfE, hyfG, pbpC, ygdB, ygeX, hofQ,
sgbH, ulaG, and ulaA. These genes show an enrichment
in E. Coli.
energy metabolism and transport, although
many were not yet classiﬁed. The results of the regression
selected several transcription factors that were suspected
regulators of many of the biclustered genes, as well as new
ones that may play an active role not yet discovered exper-
imentally.
It was noted that many biclusters were similar, even with
diﬀerent random starting conditions. This is due to the fact
that many genes and conditions are quite dominant and are
repeatedly selected by the algorithm.

Figure 1: Example Bicluster with 10 genes across 100 con-
ditions.

Figure 2 shows the results of the regressions on the train-
ing and testing signals of the bicluster in Figure 1 with
less than 0.4% of the regressors active. In the top subﬁg-
ure, samples to the left of the green line are the training
set, samples to the right are the test set. The regression
typically produced excellent results, for this regression in
particular, η = 0.579, ρ = .796, and MSE = 0.0287.
Overﬁtting of the training data is always a concern in
learning applications. Figure 3 shows the bias/variance
trade-oﬀ as a function of the regularization constant, t. As
t increases, the training error decreases to zero as the vari-
ance of the regression predictions overﬁt the training signal.
At the same time, the testing error initially decreases as
the regression learns the underlying relationships between
the regressors and the training signal, but then the testing
error increases as the high variance of the prediction de-

3

020406080100−1.5−1−0.500.511.5Bliclustered Genes and ConditionsGene Expression Level Log RatioExperimental Condition NumberBicluster Heat Map10203040506070809010012345678910Figure 3: Bias/Variance trade-oﬀ curve

creases the ability to generalize. There is an optimal point
where the training data is neither overﬁt nor underﬁt, and
the testing error is minimized. This occurs around 0.25 for
this bicluster.
Figure 4 shows some other bicluster signal and predicted
outputs. By comparing to Figure 2 we observe the domi-
nance of certain genes and conditions that appear in many
independent biclusters.

5 Conclusions

In this paper, we have demonstrated that it is possible to
take freely available data and to generate an accurate pre-
dictive model of gene expression using only transcription
factor data. Many of the genes in E. Coli.
(and other
organisms) have not yet been classiﬁed. The methods pre-
sented in this paper can provide clues for gene function
classiﬁcation both by the bicluster a gene belongs to and
the predicted output of expression data under various con-
ditions.
On top of this, the functional relationships that we have
discovered between gene expression data and transcription
factors can potentially be used to replace the Boolean con-
straints in the E. Coli metabolic network of Covert et. al[2].
We have also shown that Bonneau’s work on H. Salinarum
NRC-1 can be performed on nearly-complete genomes of
other organisms.

6 Further Work

The methods presented in this paper are a good ﬁrst step
to explain expression data. There are several other consid-

4

Figure 2: Regression onto Training (Left) and Testing Sig-
nal [above], Close-up of Testing Signal [below]

020406080100−1.2−1−0.8−0.6−0.4−0.200.20.40.60.8Signal and Prediction tracesExperimental Condition NumberGene Expression Level Log Ratio  SignalPredicted Output6065707580859095100−1−0.8−0.6−0.4−0.200.20.40.6Signal and Prediction traces for Test SetExperimental Condition NumberGene Expression Level Log Ratio  SignalPredicted Output00.20.40.60.8100.020.040.060.080.10.120.140.160.18Trade−off Curvet−value for l1 norm constraintError  Training ErrorTesting Errorerations that would extend this work.
In order to improve the accuracy of the biclustering al-
gorithm, transcription factor binding motif data and pri-
ors based on other network connectivity models could be
incorporated. A penalty factor could also be incorporated
to prevent dominant genes and conditions from repeatedly
entering biclusters.
The AND, XOR, and OR signals could be represented as
single regressors to remove the (cid:96)1 bias that makes XOR and
OR more expensive to incorporate into regression models
than AND.
It is important to consider what factors are regulating
the transcription factors.
It is possible that one tran-
scription factor is regulating another, or both are being
co-regulated by a third, confounding transcription factor.
Due to these uncertainties, the causality of predictions gen-
erated using the model in this paper should be examined
further.
Transcription factors are not the sole regulators of gene
expression, environmental factors play a strong role too.
Any full predictive model would have to incorporate these
data to be accurate and robust.
Finally, it is important to experimentally validate any
predictions generated by this model.

References

[1] R. Bonneau, et. al (2007). A Predictive Model for Tran-
scriptional Control of Physiology in a Free Living Cell.
Cell 131, pp. 1354-1365.

[2] M. Covert et. al, Regulation of Gene Expression in Flux
Balance Models of Metabolism (Journal of Theoretical
Biology 213:73-88, 2001).

[3] Oklahoma University E. Coli Gene Expression
Database. Available: http://genexpdb.ou.edu/.

[4] Cheng Y, Church GM (2000). Biclustering of expression
data. Proceedings of the 8th International Conference
on Intelligent Systems for Molecular Biology: 93–103.

[5] R. Bonneau, et. al (2006). The Inferelator: an algo-
rithm for learning parsimonious regulatory networks
from systems-biology data sets de novo. Genome Bi-
ology 7:R36

[6] S. Boyd, L. Vandenberghe, Convex Optimization (Cam-
bridge Univ. Press, 2004).

[7] M. Grant and S. Boyd. CVX: Matlab software for dis-
ciplined convex programming (web page and software).
http://stanford.edu/~boyd/cvx, May 2008.

Figure 4: Signal and prediction traces for various biclus-
ters, the blue trace is the signal and the red trace is the
prediction. Samples to the right of the green line are the
validation set.

5

020406080100−1−0.500.511.5Experimental Condition NumberGene Expression Level Log RatioSignal and Prediction traces020406080100−1.5−1−0.500.51Experimental Condition NumberGene Expression Level Log RatioSignal and Prediction traces020406080100−1−0.8−0.6−0.4−0.200.20.40.6Experimental Condition NumberGene Expression Level Log RatioSignal and Prediction traces020406080100−1−0.500.511.5Experimental Condition NumberGene Expression Level Log RatioSignal and Prediction traces
Learning a Discriminative Hidden Part Model for
Human Action Recognition

Yang Wang
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada, V5A 1S6
ywang12@cs.sfu.ca

Greg Mori
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada, V5A 1S6
mori@cs.sfu.ca

Abstract

We present a discriminative part-based approach for human a ction recognition
from video sequences using motion features. Our model is based on the recently
proposed hidden conditional random ﬁeld (hCRF) for object r ecognition. Similar
to hCRF for object recognition, we model a human action by a ﬂe xible constel-
lation of parts conditioned on image observations. Different from object recogni-
tion, our model combines both large-scale global features and local patch features
to distinguish various actions. Our experimental results show that our model is
comparable to other state-of-the-art approaches in action recognition. In partic-
ular, our experimental results demonstrate that combining large-scale global fea-
tures and local patch features performs signi ﬁcantly bette r than directly applying
hCRF on local patches alone.

1

Introduction

Recognizing human actions from videos is a task of obvious scienti ﬁc and practical importance.
In this paper, we consider the problem of recognizing human actions from video sequences on a
frame-by-frame basis. We develop a discriminatively trained hidden part model to represent human
actions. Our model is inspired by the hidden conditional random ﬁeld (hCRF) model [16] in object
recognition.

In object recognition, there are three major representatio ns: global template (rigid, e.g. [3], or de-
formable, e.g. [1]), bag-of-words [18], and part-based [7, 6]. All three representations have been
shown to be effective on certain object recognition tasks. In particular, recent work [6] has shown
that part-based models outperform global templates and bag-of-words on challenging object recog-
nition tasks.

A lot of the ideas used in object recognition can also be found in action recognition. For example,
there is work [2] that treats actions as space-time shapes and reduces the problem of action recog-
nition to 3D object recognition. In action recognition, both global template [5] and bag-of-words
models [14, 4, 15] have been shown to be effective on certain tasks. Although conceptually ap-
pealing and promising, the merit of part-based models has no t yet been widely recognized in action
recognition. The goal of this work is to address this gap.

Our work is partly inspired by a recent work in part-based eve nt detection [10].
In that work,
template matching is combined with a pictorial structure model to detect and localize actions in
crowded videos. One limitation of that work is that one has to manually specify the parts. Unlike
Ke et al. [10], the parts in our model are initialized automatically.

(a)

(b)

(c)

(d)

(e)

Figure 1: Construction of the motion descriptor. (a) origin al image; (b) optical ﬂow; (c) x and y com-
ponents of optical ﬂow vectors Fx , Fy ; (d) half-wave recti ﬁcation of x and y components to obtain
y .
y ; (e) ﬁnal blurry motion descriptors F b+
4 separate channels F +
y , F b−
x , F b+
x , F b−
y , F −
x , F +
x , F −

The major contribution of this work is that we combine the ﬂex ibility of part-based approaches with
the global perspectives of large-scale template features in a discriminative model. We show that the
combination of part-based and large-scale template features improves the ﬁnal results.

2 Our Model

The hidden conditional random ﬁeld model [16] was originall y proposed for object recognition
and has also been applied in sequence labeling [19]. Objects are modeled as ﬂexible constella-
tions of parts conditioned on the appearances of local patches found by interest point operators.
The probability of the assignment of parts to local features is modeled by a conditional random
ﬁeld (CRF) [11]. The advantage of the hCRF is that it relaxes t he conditional independence as-
sumption commonly used in the bag-of-words approaches of object recognition.

Similarly, local patches can also be used to distinguish actions. Figure. 4(a) shows some examples
of human motion and the local patches that can be used to distinguish them. A bag-of-words repre-
sentation can be used to model these local patches for action recognition. However, it suffers from
the same restriction of conditional independence assumption that ignores the spatial structures of
the parts. In this work, we use a variant of hCRF to model the constellation of these local patches in
order to alleviate this restriction.

There are also some important differences between objects and actions. For objects, local patches
could carry enough information for recognition. But for act ions, we believe local patches are not
sufﬁciently informative. In our approach, we modify the hCR F model to combine local patches and
large-scale global features. The large-scale global features are represented by a root model that takes
the frame as a whole. Another important difference with [16] is that we use the learned root model
to ﬁnd discriminative local patches, rather than using a gen eric interest-point operator.

2.1 Motion features

Our model is built upon the optical ﬂow features in [5]. This m otion descriptor has been shown to
perform reliably with noisy image sequences, and has been applied in various tasks, such as action
classi ﬁcation, motion synthesis, etc.

To calculate the motion descriptor, we ﬁrst need to track and stabilize the persons in a video se-
quence. Any reasonable tracking or human detection algorithm can be used, since the motion de-
scriptor we use is very robust to jitters introduced by the tr acking. Given a stabilized video sequence
in which the person of interest appears in the center of the ﬁe ld of view, we compute the optical ﬂow
at each frame using the Lucas-Kanade [12] algorithm. The optical ﬂow vector ﬁeld
F is then split
into two scalar ﬁelds Fx and Fy , corresponding to the x and y components of F . Fx and Fy are fur-
y , so that Fx = F +
y , F −
x , F +
x , F −
ther half-wave recti ﬁed into four non-negative channels F +
x − F −
x
y . These four non-negative channels are then blurred with a Gaussian kernel and
and Fy = F +
y − F −
normalized to obtain the ﬁnal four channels F b+
x ,F b−
x ,F b+
y ,F b−
y (see Fig. 1).

2.2 Hidden conditional random ﬁeld(hCRF)

Now we describe how we model a frame I in a video sequence. Let x be the motion feature of
this frame, and y be the corresponding class label of this frame, ranging over a ﬁnite label alphabet
Y . Our task is to learn a mapping from x to y . We assume each image I contains a set of salient
patches {I1 , I2 , ..., Im }. we will describe how to ﬁnd these salient patches in Sec. 3. O ur training set
consists of labeled images (xt , y t ) (as a notation convention, we use superscripts to index training
images and subscripts to index patches) for t = 1, 2, ..., n, where y t ∈ Y and xt = (xt
m ).
1 , xt
2 ..., xt
i ) is the feature vector extracted from the global motion featu re xt at the location of the
xt
i = xt (I t
i . For each image I = {I1 , I2 , ..., Im }, we assume there exists a vector of hidden “part ”
patch I t
variables h = {h1 , h2 , ..., hm }, where each hi takes values from a ﬁnite set H of possible parts.
Intuitively, each hi assigns a part label to the patch Ii , where i = 1, 2, ..., m. For example, for the
action “waving-two-hands ”, these parts may be used to chara
cterize the movement patterns of the
left and right arms. The values of h are not observed in the training set, and will become the hidden
variables of the model.
We assume there are certain constraints between some pairs o f (hj , hk ). For example, in the case of
“waving-two-hands ”, two patches hj and hk at the left hand might have the constraint that they tend
to have the same part label, since both of them are characterized by the movement of the left hand. If
we consider hi (i = 1, 2, ..., m) to be vertices in a graph G = (E , V ), the constraint between hj and
hk is denoted by an edge (j, k) ∈ E . See Fig. 2 for an illustration of our model. Note that the graph
structure can be different for different images. We will des cribe how to ﬁnd the graph structure E in
Sec. 3.

y

class label

hi

xi

x

hj

xj

φ(·)
ϕ(·)
ψ(·)
ω(·)

hk

hidden parts

xk

image

Figure 2: Illustration of the model. Each circle correspond s to a variable, and each square corre-
sponds to a factor in the model.

Given the motion feature x of an image I , its corresponding class label y , and part labels h, a hidden
exp(Ψ(y ,x,h;θ))
, where θ is the
conditional random ﬁeld is deﬁned as
p(y , h|x; θ) =
P ˆy∈Y P ˆh∈Hm exp(Ψ( ˆy ,x, ˆh;θ))
model parameter, and Ψ(y , h, x; θ) ∈ R is a potential function parameterized by θ . It follows that
p(y , h|x; θ) = Ph∈Hm exp(Ψ(y , h, x; θ))
p(y |x; θ) = Xh∈Hm
P ˆy∈Y Ph∈Hm exp(Ψ( ˆy , h, x; θ))
We assume Ψ(y , h, x) is linear in the parameters θ = {α, β , γ , η}:
Ψ(y , h, x; θ) = Xj∈V
α⊤ · φ(xj , hj )+Xj∈V
β⊤ · ϕ(y , hj )+ X(j,k)∈E
γ⊤ · ψ(y , hj , hk )+ η⊤ · ω(y , x) (2)
where φ(·) and ϕ(·) are feature vectors depending on unary hj ’s, ψ(·) is a feature vector depending
on pairs of (hj , hk ), ω(·) is a feature vector that does not depend on the values of hidden variables.
The details of these feature vectors are described in the fol lowing.
Unary potential α⊤ · φ(xj , hj ) : This potential function models the compatibility between xj and
the part label hj , i.e., how likely the patch xj is labeled as part hj . It is parameterized as
α⊤ · φ(xj , hj ) = Xc∈H

α⊤
{hj =c} · [f a (xj ) f s (xj )]
c · 1

(1)

(3)

where we use [f a (xj ) f s (xj )] to denote the concatenation of two vectors f a (xj ) and f s (xj ).
f a (xj ) is a feature vector describing the appearance of the patch xj .
In our case, f a (xj ) is
simply the concatenation of four channels of the motion features at patch xj , i.e., f a (xj ) =
y (xj )]. f s (xj ) is a feature vector describing the spatial location
[F b+
x (xj ) F b−
x (xj ) F b+
y (xj ) F b−
of the patch xj . We discretize the whole image locations into l bins, and f s (xj ) is a length l vector
of all zeros with a single one for the bin occupied by xj . The parameter αc can be interpreted as
the measurement of compatibility between feature vector [f a (xj ) f s (xj )] and the part label hj = c.
The parameter α is simply the concatenation of αc for all c ∈ H.
Unary potential β⊤ · ϕ(y , hj ) : This potential function models the compatibility between class label
y and part label hj , i.e., how likely an image with class label y contains a patch with part label hj .
It is parameterized as

(4)

(5)

(6)

{y=a} · 1
βa,b · 1
{hj =b}

{hj =b} · 1
{y=a} · 1
γa,b,c · 1
{hk=c}

β⊤ · ϕ(y , hj ) = Xa∈Y Xb∈H
where βa,b indicates the compatibility between y = a and hj = b.
Pairwise potential γ⊤ · ψ(y , hj , hk ): This pairwise potential function models the compatibility
between class label y and a pair of part labels (hj , hk ), i.e., how likely an image with class label y
contains a pair of patches with part labels hj and hk , where (j, k) ∈ E corresponds to an edge in
the graph. It is parameterized as
γ⊤ · ψ(y , hj , hk ) = Xa∈Y Xb∈H Xc∈H
where γa,b,c indicates the compatibility of y = a, hj = b and hk = c for the edge (j, k) ∈ E .
Root model η⊤ · ω(y , x): The root model is a potential function that models the compatibility of
class label y and the large-scale global feature of the whole image. It is parameterized as
η⊤ · ω(y , x) = Xa∈Y
where g(x) is a feature vector describing the appearance of the whole image. In our case, g(x)
is the concatenation of all the four channels of the motion features in the image, i.e., g(x) =
y ]. ηa can be interpreted as a root ﬁlter that measures the compatib ility be-
[F b+
x F b−
x F b+
y F b−
tween the appearance of an image g(x) and a class label y = a. And η is simply the concatenation
of ηa for all a ∈ Y .
The parameterization of Ψ(y , h, x) is similar to that used in object recognition [16]. But there are
two important differences. First of all, our deﬁnition of th e unary potential function φ(·) encodes
both appearance and spatial information of the patches. Secondly, we have a potential function ω(·)
describing the large scale appearance of the whole image. The representation in Quattoni et al. [16]
only models local patches extracted from the image. This may be appropriate for object recognition.
But for human action recognition, it is not clear that local patches can be sufﬁciently informative.
We will demonstrate this experimentally in Sec. 4.

η⊤
a · 1
{y=a} · g(x)

3 Learning and Inference

The model parameters θ are learned by maximizing the conditional log-likelihood on the training
images:

θ∗ = arg max
θ

log  Xh
p(y t , h|xt ; θ)! (7)
θ Xt
θ Xt
log p(y t |xt ; θ) = arg max
L(θ) = arg max
The objective function L(θ) in Quattoni et al.[16] also has a regularization term −1
2σ2 ||θ ||2 . In our
experiments, we ﬁnd that the regularization does not seem to have much effect on the ﬁnal results,
so we will use the un-regularized version. Different from co nditional random ﬁeld (CRF) [11], the
objective function L(θ) of hCRF is not concave, due to the hidden variables h. But we can still use

∂Lt (θ)
∂α

∂Lt (θ)
∂β

gradient ascent to ﬁnd θ that is locally optimal. The gradient of the log-likelihood with respect to
the t-th training image (xt , y t ) can be calculated as:
= Xj∈V (cid:2)Ep(hj |yt ,xt ;θ)φ(xt
j , hj ) − Ep(hj ,y |xt ;θ)φ(xt
j , hj )(cid:3)
= Xj∈V (cid:2)Ep(hj |yt ,xt ;θ)ϕ(hj , y t ) − Ep(hj ,y |xt ;θ)ϕ(hj , y)(cid:3)
= X(j,k)∈E (cid:2)Ep(hj ,hk |yt ,xt ;θ)ψ(y t , hj , hk ) − Ep(hj ,hk ,y |xt ;θ)ψ(y , hj , hk )(cid:3)
= ω(y t , xt ) − Ep(y |xt ;θ)ω(y , xt )

∂Lt (θ)
∂ η
Assuming the edges E form a tree, the expectations in Eq. 8 can be calculated in O(|Y ||E ||H|2 )
time using belief propagation.

∂Lt (θ)
∂ γ

(8)

log

(9)

η by

Now we describe several details about how the above ideas are implemented.
Learning root ﬁlter η : Given a set of training images (xt , y t ), we ﬁrstly learn the root ﬁlter
solving the following optimization problem:
exp (cid:0)η⊤ · ω(y t , xt )(cid:1)
η Xt
η Xt
η∗ = arg max
log L(y t |xt ; η) = arg max
Py exp (η⊤ · ω(y , xt ))
In other words, η∗ is learned by only considering the feature vector ω(·). We then use η∗ as the
starting point for η in the gradient ascent (Eq. 8). Other parameters α, β , γ are initialized randomly.
Patch initialization: We use a simple heuristic similar to that used in [6] to initialize ten salient
patches on every training image from the root ﬁlter η∗ trained above. For each training image I with
class label a, we apply the root ﬁlter ηa on I , then select an rectangle region of size 5 × 5 in the
image that has the most positive energy. We zero out the weights in this region and repeat until ten
patches are selected. Figure 4(a) shows examples of the patches found in some images. The tree
G = (V , E ) is formed by running a minimum spanning tree algorithm over t he ten patches.
Inference: During testing, we do not know the class label of a given test image, so we cannot use the
patch initialization described above to initialize the patches, since we do not know which root ﬁlter
to use. Instead, we run root ﬁlters from all the classes on a te st image, then calculate the probabilities
of all possible instantiations of patches under our learned model, and classify the image by picking
the class label that gives the maximum of the these probabilities. In other words, for a testing image
with motion descriptor x, we ﬁrst obtain |Y | instances {x(1) , x(2) , ..., x(|Y |) }, where each x(k) is
obtained by initializing the patches on x using the root ﬁlter ηk . The ﬁnal class label y∗ of x is
obtained as y∗ = arg maxy (cid:2)max{p(y |x(1) ; θ), p(y |x(2) ; θ), ..., p(y |x(|Y |) ; θ)}(cid:3).
4 Experiments

We test our algorithm on two publicly available datasets that have been widely used in action recog-
nition: Weizmann human action dataset [2], and KTH human motion dataset [17]. Performance on
these benchmarks is saturating – state-of-the-art approac hes achieve near-perfect results. We show
our method achieves results comparable to the state-of-the-art, and more importantly that our ex-
tended hCRF model signi ﬁcantly outperforms a direct applic ation of the original hCRF model [16].
Weizmann dataset: The Weizmann human action dataset contains 83 video sequences show-
ing nine different people, each performing nine different actions:
running, walking, jumping-
jack,
jumping-forward-on-two-legs,jumping-in-place-on-two-legs, galloping-sideways, waving-
two-hands, waving-one-hand, bending. We track and stabilize the ﬁgures using the background
subtraction masks that come with this dataset.

We randomly choose videos of ﬁve subjects as training set, an d the videos in the remaining four
subjects as test set. We learn three hCRF models with different sizes of possible part labels, |H| =
6, 10, 20. Our model classi ﬁes every frame in a video sequence (i.e., p er-frame classi ﬁcation), but

bend

1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00

bend

1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00

jack

0.02 0.93 0.01 0.02 0.00 0.00 0.00 0.00 0.01

jack

0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00

jump

0.01 0.03 0.74 0.00 0.06 0.02 0.12 0.02 0.00

jump

0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00

pjump

0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00

pjump

0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00

run

0.00 0.05 0.00 0.00 0.72 0.06 0.17 0.00 0.00

run

0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00

side

0.00 0.01 0.07 0.00 0.02 0.73 0.17 0.00 0.00

side

0.00 0.00 0.00 0.00 0.00 0.75 0.25 0.00 0.00

walk

0.00 0.00 0.01 0.00 0.05 0.06 0.88 0.00 0.00

walk

0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00

wave1

0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.99 0.00

wave1

0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00

wave2

0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

wave2

0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00

bend

jack

jump

run
pjump
wave2
wave1
Frame-by-frame classi ﬁcation

walk

side

bend

jack

jump

run
pjump
Video classiﬁcation

side

walk

wave2
wave1

Figure 3: Confusion matrices of classi ﬁcation results on We izmann dataset. Horizontal rows are
ground truths, and vertical columns are predictions.

method

root model

per-frame
per-video

0.7470
0.8889

|H| = 6
0.5722
0.5556

local hCRF
|H| = 10
0.6656
0.6944

|H| = 20
0.6383
0.6111

|H| = 6
0.8682
0.9167

our approach
|H| = 10
0.9029
0.9722

|H| = 20
0.8557
0.9444

Table 1: Comparison of two baseline systems with our approach on Weizmann dataset.

we can also obtain the class label for the whole video sequence by the majority voting of the labels
of its frames (i.e., per-video classi ﬁcation). We show the c onfusion matrix with |H| = 10 for both
per-frame and per-video classi ﬁcation in Fig. 3.

We compare our system to two baseline methods. The ﬁrst basel
ine (root model) only uses the root
ﬁlter η⊤ · ω(y , x), which is simply a discriminative version of Efros et al. [5] . The second baseline
(local hCRF) is a direct application of the original hCRF model [16]. It is similar to our model, but
without the root ﬁlter η⊤ · ω(y , x), i.e., local hCRF only uses the root ﬁlter to initialize the s alient
patches, but does not use it in the ﬁnal model. The comparativ e results are shown in Table 1. Our
approach signi ﬁcantly outperforms the two baseline method s. We also compare our results(with
|H| = 10) with previous work in Table 2. Note [2] classi ﬁes space-tim e cubes. It is not clear how it
can be compared with other methods that classify frames or videos. Our result is signi ﬁcantly better
than [13], and comparable to [8]. Although we accept the fact that the comparison is not completely
fair, since [13] does not use any tracking or background subt raction.

We visualize the learned parts in Fig. 4(a). Each patch is represented by a color that corresponds to
the most likely part label of that patch. We also visualize the root ﬁlters applied on these images in
Fig. 4(b).

KTH dataset: The KTH human motion dataset contains six types of human actions (walking,
jogging, running, boxing, hand waving and hand clapping) performed several times by 25 subjects
in four different scenarios: outdoors, outdoors with scale variation, outdoors with different clothes
and indoors. We ﬁrst run an automatic preprocessing step to t rack and stabilize the video sequences,
so that all the ﬁgures appear in the center of the ﬁeld of view.

We split the videos roughly equally into training/test sets and randomly sample 10 frames from each
video. The confusion matrices (with |H| = 10) for both per-frame and per-video classi ﬁcation are

Our method
Jhuang et al. [8]
Niebles & Fei-Fei [13]
Blank et al. [2]

per-frame(%)
90.3
N/A
55
N/A

per-video(%)
97.2
98.8
72.8
N/A

per-cube(%)
N/A
N/A
N/A
99.64

Table 2: Comparison of classi ﬁcation accuracy with previou s work on the Weizmann dataset.

(a)

(b)

Figure 4: (a) Visualization of the learned parts. Patches are colored according to their most likely
part labels. Each color corresponds to a part label. Some int eresting observations can be made.
For example, the part label represented by red seems to corre spond to the “moving down” patterns
mostly observed in the “bending” action. The part label repr
esented by green seems to correspond
to the motion patterns distinctive of “hand-waving” action s; (b) Visualization of root ﬁlters applied
on these images. For each image with class label c, we apply the root ﬁlter ηc . The results show the
ﬁlter responses aggregated over four motion descriptor cha nnels. Bright areas correspond to positive
energies, i.e., areas that are discriminative for this clas s.

boxing

0.55

0.04

0.03

0.10

0.17

0.12

boxing

0.86

0.00

0.03

0.02

0.05

0.05

handclapping

0.02

0.74

0.10

0.07

0.04

0.02

handclapping

0.00

0.97

0.00

0.03

0.00

0.00

handwaving

0.02

0.10

0.77

0.01

0.05

0.04

handwaving

0.00

0.02

0.98

0.00

0.00

0.00

jogging

0.02

0.01

0.04

0.55

0.20

0.18

jogging

0.00

0.00

0.00

0.67

0.19

0.14

running

0.01

0.00

0.07

0.09

0.67

0.16

running

0.00

0.00

0.02

0.03

0.84

0.11

walking

0.73

walking

0.02

boxing

0.08
0.05
0.01
jogging
handwaving
handclapping
Frame-by-frame classi ﬁcation

0.10
running

walking

boxing

0.00

0.01
0.04
0.00
jogging
handwaving
handclapping
Video classiﬁcation

0.01
running

0.93

walking

Figure 5: Confusion matrices of classi ﬁcation results on KT H dataset. Horizontal rows are ground
truths, and vertical columns are predictions.

shown in Fig. 5. The comparison with the two baseline algorithms is summarized in Table 3. Again,
our approach outperforms the two baselines systems.

The comparison with other approaches is summarized in Table 4. We should emphasize that we do
not attempt a direct comparison, since different methods listed in Table 4 have all sorts of variations
in their experiments (e.g., different split of training/te st data, whether temporal smoothing is used,
whether per-frame classi ﬁcation can be performed, whether
tracking/background subtraction is used,
whether the whole dataset is used etc.), which make it impossible to directly compare them. We
provide the results only to show that our approach is compara ble to the state-of-the-art.

method

root model

per-frame
per-video

0.5377
0.7339

|H| = 6
0.4749
0.5607

local hCRF
|H| = 10
0.4452
0.5814

|H| = 20
0.4282
0.5504

|H| = 6
0.6633
0.7855

our approach
|H| = 10
0.6698
0.8760

|H| = 20
0.6444
0.7512

Table 3: Comparison of two baseline systems with our approach on KTH dataset.

methods
Our method
Jhuang et al. [8]
Nowozin et al. [15]
Niebles et al. [14]
Doll ´ar et al. [4]
Schuldt et al. [17]
Ke et al. [9]

accuracy(%)
87.60
91.70
87.04
81.50
81.17
71.72
62.96

Table 4: Comparison of per-video classi ﬁcation accuracy wi

th previous approaches on KTH dataset.

5 Conclusion

We have presented a discriminatively learned part model for human action recognition. Unlike
previous work [10], our model does not require manual speci ﬁ cation of the parts. Instead, the parts
are initialized by a learned root ﬁlter. Our model combines b oth large-scale features used in global
templates and local patch features used in bag-of-words models. Our experimental results show that
our model is quite effective in recognizing actions. The results are comparable to the state-of-the-
art approaches. In particular, we show that the combination of large-scale features and local patch
features performs signi ﬁcantly better than using either of
them alone.

References

[1] A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion corre-
spondence. In IEEE CVPR, 2005.
[2] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. In IEEE
ICCV, 2005.
[3] N. Dalal and B. Triggs. Histogram of oriented gradients for human detection. In IEEE CVPR, 2005.
[4] P. Doll ´ar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal
features. In VS-PETS Workshop, 2005.
[5] A. A. Efros, A. C. Berg, G. Mori, and J. Malik. Recognizing action at a distance. In IEEE ICCV, 2003.
[6] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part
model. In IEEE CVPR, 2008.
[7] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. IJCV, 61(1):55–79,
January 2003.
[8] H. Jhuang, T. Serre, L. Wolf, and T. Poggio. A biologically inspired system for action recognition. In
IEEE ICCV, 2007.
[9] Y. Ke, R. Sukthankar, and M. Hebert. Efﬁcient visual event dete ction using volumetric features. In IEEE
ICCV, 2005.
[10] Y. Ke, R. Sukthankar, and M. Hebert. Event detection in crowded videos. In IEEE ICCV, 2007.
[11] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁe lds: Probabilistic models for segmenting
and labeling sequence data. In ICML, 2001.
[12] B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision.
In Proc. DARPA Image Understanding Workshop, 1981.
[13] J. C. Niebles and L. Fei-Fei. A hierarchical model of shape and appearance for human action classiﬁcation.
In IEEE CVPR, 2007.
[14] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learning of human action categories using spatial-
temporal words. In BMVC, 2006.
[15] S. Nowozin, G. Bakir, and K. Tsuda. Discriminative subsequence mining for action classiﬁcation. In
IEEE ICCV, 2007.
[16] A. Quattoni, M. Collins, and T. Darrell. Conditional random ﬁelds for object recognition. In NIPS 17,
2005.
[17] C. Schuldt, L. Laptev, and B. Caputo. Recognizing human actions: a local SVM approach.
ICPR, 2004.
[18] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Discovering objects and their
location in images. In IEEE ICCV, 2005.
[19] S. B. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian, and T. Darrell. Hidden conditional random ﬁelds
for gesture recognition. In IEEE CVPR, 2006.

In IEEE


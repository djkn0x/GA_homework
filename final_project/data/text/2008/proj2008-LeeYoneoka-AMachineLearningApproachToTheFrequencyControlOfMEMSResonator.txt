CS229 Autumn 2008 

A MACHINE LEARNING APPROACH TO 
THE FREQUENCY CONTROL OF MEMS RESONATORS 
Hyung Kyu Lee and Shingo Yoneoka 
Stanford University, California, USA 

 
1. INTRODUCTION 
Silicon   MEMS   resonators  have  been   considered   as  
replacemen ts   for  quar tz  crystal  resonators   in   electronic  
systems.  MEMS   resonators  have  many  advantages  over  
quartz  references  such  as  small  size,  low  cost,  less  power  
consumption,  and  CMOS   compatib ility .  Therefore,  MEMS  
resonators   are  suitab le  for  frequency  references  (clock)  in  
miniatur ized handheld electronic devices. 
Silicon   MEMS   resonators   consist  of   double-ended  
tuning  forks  (DETF)  that  vibrate  at  a  designed  frequency  
when  actuation  signal  is  applied  (Fig.  1) .    The  resonant  
frequency  is  proven  to  be  a  function  of  temperature  of  the  
device,  dimension  of   a 
tun ing  fork,  and  
the  b ias  
voltage(Vbias)  for  actuation.  Here, 
the 
temperature 
is  
environmen tal  var iab le  determined  by  ambient  condition,  
the  geometrical  dimensions  of  a  tuning  fork  are  design  
variab les   fixed  during  fabr ication,  and  the  b ias   voltage  is   a  
control  variable.  Since  the  frequency  of  silicon  MEMS  
resonators   is   greatly  affected  by   temperature  var iation,  
much worse  than that of quar tz crystal resonators , frequency  
stab ilization is necessary  for MEMS  resonators. 
The  resonan t  frequency   of   MEMS   resonators  can   be  
tuned  by   changing  the  bias   voltage;  therefore,  we  can  
stab ilize  the  frequency  by  applying  proper  b ias   voltage  
accord ing  to  the  measured  dev ice  temperature.  To  ach ieve  
this   goal,  we  need  a  calibration 
tab le 
that  shows  
temperature-bias  voltage 
relation 
for  a 
fixed 
target  
frequency.  To  build  th is  tab le, we  (1) measure  frequency  at  
array  of   measuremen t  po ints   (F ig.  2),  (2)   fit  the  data  using  
polynomial equation , (3)  and  obtain   iso-frequency   line from  
the equation. 
Conventional  calibration  method  stated  above  does  not  
care  about  the  efficiency  in  step  (1)  and  (2).  Therefore,  a  
large  amount  of   data  is  gathered   and   as   many   features  are  
used 
for 
f itting 
as   possib le.  However, 
for 
the  
commercialization  of  MEMS  resonators,  cost  effective  
calibration   process  is   needed.  For  that,  it  is  necessary  to  
minimize  the  number   of  measurement  poin ts   to   generate  a  
fitting  curve  w ithout  losing  the  f itting  accuracy .  Here,  a  
mach ine  learning  approach  to  generate  the  calibration  tab le  
would be necessary.  
If  we  figure  ou t  signif icant  features   and   optimized  
measurement  poin ts   using   mach ine  learn ing,  we  can   build  
the  same  calibration  tab le  with  shorter  measuremen t  time.  
Also, 
the  device-to-dev ice  var iation  error  caused  by  
uncertain ties  in  fabrication  process  would  be  so lved  using  
the Gaussian process  regression algorithm . 
For  this  study,  we  measure  the  frequency  of  multip le  
resonators  as  a function  of temperature and  bias  voltage. We  
will evaluate the performance of our study using this  data. 
 

 
Figure 1: Schema tic o f the double-ended tuning  fork (DETF)  
MEMS resonator . 
 

 
Figure 2: Resonant  frequency of MEMS resonator  is  plotted  
as a function of tempera ture and the  b ias vo ltage. 
 
2. FEATURE SELECTION 
First,  we  investigate  that  wh ich  polynomial  is  the  best  
to  f it  the data  for  an   ind ividual  resonator. Hence, we  ignore  
the  variation  of  geometrical  d imensions   between  each  
device  at  this  point,  and  assume  that  the  frequency  is  on ly  
the  function  of   temperature  and  b ias   voltage.  Let’s  define  
the  temperature  as   x1,  and  a  b ias   voltage  as  x2. We  have  m  
training examples  {x(i), f(i):  i = 1,2, …  , m}, where x(i) = [x1
(i) 
(i)]T  and   f(i)  is   measured   frequency.  The  fitting   curve  for  
,  x2
the  frequency  of  the  resonator  can  be  descr ibed  as  a  
combination of polynomials in two var iab les . 
 

f ( i )

=

!
j

!
k

( i ) ) j ( x2
( i ) ) k
a( j , k ) ( x1

 

(1) 

where, 0 " j , 0 " k , i + j " n
  
 
If  we  treat  all  variables  as   high  d imensional  input  
 n, x2
n, x1
 n-1x2, x1x2
 n-1, … , x1, x2, 1] T 
features  such  that  z = [x1
then,  we  can   linearized  the  equation  as  f    =zTθ.  θ  can  be  
calculated w ith  a normal equation. 
 

 

 
(ii) 

 
(iii) 

 

(iv) 

 

 

CS229 Autumn 2008 

   

 

 

f (1)

(1)T

z

( m )T

z

!
#
#
#
"

!
#
#
#
"

!Z =

"

!
f =

"
f ( m )

$
&
&
&  
%

! = ( !Z T !Z )"1 !Z T "
f , 
 

$
&
&
&  
%
Since  we  have  a  large  number   of  features(28  features  
for n =6  ), we conduct  feature  selection  to ex tract  important  
features  using  backward  and  forward  search.  We  use  
hold-out  cross  validation  
(70%  of 
the  data)  and  
leave-one-out  cross  validation   (LOOCV)  to  calculate  the  
generalization  error  for  the  feature  selections.  We  use  two  
criteria  in  cross  validation  routine;  ‘max imum  error method  
(MaxE)’  and   ‘average  error  method  (AvgE)’.  In  each  
iteration step, we obtain  the array of error values correspond  
to  learning  examp les . Here, MaxE uses  the max imum value  
of errors  and AvgE uses   the averaged  value  to  choose more  
significan t feature set. We star t feature selection from  the 6th  
order-2 variab le polynomial. 
Fig.  3   shows  the  result  of  feature  selections  for  three  
different  devices .  We  compared  the  number   of   removed  
features  w ith  the  error  for  the  backward  search  (Fig .  3  (i,  
ii)),  and  the  number  of  features  with  the  error  for  the  
forward  search  (F ig.  3  (iii,  iv)).  AvgE  and  MaxE  are  
examined in each case. 
From  the  calculation  result,  we  can  observe  that  the  
average  error  is  min imized  when   the  number  of   removed  
features  is  13  – 15  when  we apply backward search. For  the  
forward  search,  the  average  error  is  min imized  when  the  
number  of  feature  is  10  –  15.  Larger  ‘number  of  removed  
features’  is   preferred  for  backward  search   and   smaller  
‘number  of  features’   for  forward  search   since  we  wan t  to  
reduce  the measurement po ints   to generate  the  f itting curve.  
Using 
the  measurement  data  of 
those  devices,  we  
summar ize  top 14  features  to be removed  or chosen  by each  
search algorithm( Table 1) . 
 From  the  analysis  of  the  resu lts  from  bo th  search  
methods ,  we  found  that  we  can  obtain  the  best  f itting  error  
when  the  number  of  feature  is  ~13.  We  conducted  feature  
selection  for  the  data  from  three  d ifferen t  dev ices,  and  
obtain  similar  trend  for  the  op timized  number  of  features.  
Therefore,  we  conclude  that  the  numbers  of  signif icant  
features are common for different devices. 
  

(i) 

 

 
Figure  3:  Calcula tion  result 
fea ture  selection  
form 
algorithm .  ( i)  and  (ii)  show  the  average  and  the  maximum  
error  for  backward  search.  (iii)  and  (iv)  show  the  average  
and the maximum  error for  forward  search. 2  different types  
of cross validation  methods are used  to eva luate errors. 
 
 
 
 
 

 

 

CS229 Autumn 2008 
Table  1:   Top  14-feature  list  chosen   by  feature  selection  
algorithm . 
 
Backward 
LOOCV 
Hold out 

AvgE 
2, x1
4x2
3, x1
3x2
6, x1
5x2, x1
3, x1
x1
5 
6, x1, x1
5, x2
4, x1x2
2, x2
x1
6,  x1
5x2,  x1
2x2,  x1
5,  x1
4x2,  x1
3x2
3, 
x1
3x2,  x1x2 
4, x2,  x1
5,  x1
2,  x1x2
3x2
3,  x1
2x2
2,  x1
x2

4x2, x2, x1

3x2

2, 

2, 

3, 

1, 

3, 

3x2

2x2

2,x1

2x2

5x2, 

3, x2

6, x1

4x2, x1, 

5, x2, x2

  
Forward 
LOOCV 
Hold out 

 
Backward  MaxE 
LOOCV 
6, x1
3x2, x1
2, x1
2x2, x1
2x2
4, x1
3, x1
2x2
3,  x1
x1
3,  x1 
2, const,  x1x2
2, x1
5, x1
4x2
3x2
x1x2
Hold out 
5, x1
3x2, x1x2
4x2, x1
6, x1
4, x1
5, x1
4x2
2, x1
x1
4 
3x2
2, x1
2x2, x1
3, x1x2
2, x1x2, x1x2
x1
AvgE 
4x2
5, x1
2, x1x2
2, x1
1, x1
4x2
2, x2
const, x1
6 
5, x1
6, x1, x1
2, x2
3x2
5x2 , x1
3,  x1
 x1
3x2
3x2
2, x1
2, x1x2
5, x1
4x2
1, x1
const, x2, x1
6 
2 , x2
3, x1x2
2x2
3, x1
5, x1x2
2x2 , x1
3, x1
 x1
 
Forward  MaxE 
LOOCV 
2, x1
3, x1
5, x2
const, x1x2
2 
3x2
4, x1
2x2
4, x1
6, x1
x1
Hold out 
5, x1x2, x2, x1
4, x1
3x2, x2
const, x1x2
4x2, x1 
4x2
2, x1
3, x2
5, x1x2
5, x1
x1
  
Random search for feature selection 
From  the  previous  section,  we  decided  how  many  and  
which features are needed for the fit w ith  the smallest f itting  
error.  However,  feature  selection  methods  (forward   search  
and  backward   search)   we  used   above  are  heuristic  so   that  
they  do   not  guarantee  that  the  solu tion   has   the  min imum  
fitting  error.  I t  becomes  more  ev iden t  if  we  see  solu tions  
presented  on   Tab le  1. Although  there  are  some  correlations  
among  solu tions,  no  unique  so lution  exists .  Therefore,  we  
can  conclude  that  there  is  room   for  improvemen t w ith  other  
search  method.  Th is  is  the  motivation  to  imp lement  the  
‘random search’ method for feature selection.  
The  random   search   method   compares   fitting   error  of   a  
large  number  of  possib le  feature  sets  in  a  loop  and  returns  
the  feature  set  with  the  smallest  error.  A t  the  first  step,  
randomized  feature  set  is   generated   by  the  code.  Since  we  
star t from 6th order-2 variable polynomial equation, we have  
total of 28 features  in  the feature pool. Therefore, if we want  
to  select  N  features   through  random   search   algorithm,  we  
have 
differen t  features  sets   to  compare. Here, we w il l 
28 CN
use N=13 because we learned 13  is  the min imum number  of  
features  with  wh ich   we  can  ach ieve  sub-ppm   error.  Hence,  
at least
=37,442,260 iterations  are needed  to obtain the 
28 C13
solution  in  an  ideal  case.  This   is   compu tationally  heavy  
process  so  that  couple  of  weeks  is  required  on  a  personal  
computer .  Therefore,  we  decided  to  run  a  routine  un til  top  
10  smallest  errors  are  smaller  than  backward  /  forward  
search  resu lt,  and  then  analyze  feature  sets   in  a  top  10  list.  
This  modif ication  makes  the  resu lt  of  random   search  
method  not  to  be  the  guaran teed  best  so lution,  but  pseudo  

 

(2) 

best  solu tions   will  be  still  meaningful  if   we  can   improve  
fitting error.  
In  the  implemen tation,  device  #1  and  #2  are  used  w ith  
‘LOOCV’  for  cross  validation  and  MaxE  for  error  cr iteria.  
After  1  million  iterations,  results  are  compared  to  forward  
search  results   (Tab le  2).  If  we  obtain   the  most  frequently  
appeared   features  from  the  top  10  feature  set  list,  we  can  
build  a  feature  set  w ith  small  fitting  error.  The  resu ltant  
feature set is shown in Table 3. 
 
Table  2:  Comparison  o f  fitting  errors  between  forward  
search and random search. 
 
Device  Forward search   Random  search : top 10 
0.5462 to 0.6070 ppm 
0.6712 ppm 
#1 
#2 
0.1473 ppm 
0.0941 to 0.1149 ppm 

 
Comparison w ith ana lytical model 
Finally, we compare the result of feature selection from  
random  search  with  the  analy tical  model  based  on  the  
physics of  the resonan t beam . The frequency of  the resonant  
beam  is  a  function  of  the  temperature  and  the  b ias  voltage  
and it can be descr ibed as fo llow ing in  a simple model: 
 

1

22.4 2
l (T )4 B(T ) #

2hSi (T )$0$3
2
Vb
SiO2
(2t SiO2
(T )$0 + gbeam (T )$SiO2

)3

2! "Si (T )Ac (T )

f (T ,Vb ) =
 
where  ρSi  is  the  mass  density  of  silicon,  AC  is  the  
cross-sectional  area  of  the  beam,  l  is  the  length ,  B  is  the  
bending  stiffness,  hSi  is  the  height,  tSiO2  is  the  th ickness  of  
SiO2 layer , and  gbeam is  the gap for  electrostatic transduction.  
These mater ial and dimensional variables are known  to be a  
function of temperature. ε0 and εSi are permittiv ity constants.  
We  conduct  Taylor  expansion  of  the  Eqn.  2  to  express  
frequency  as   a po lynomial  function  of  temperature  and b ias  
voltage  to make  Eqn.  2   comparab le  to  Eqn.  1 . Af ter  Tay lor  
expansion,  we  compare  which  terms   in   the  resu ltant  
polynomials  are  importan t  by  check ing  variance  of  each  
terms in the g iven temperature and bias vo ltage range.  
Table  3   shows  the  comparison  of  important  features  
from  the  analytical  model  approach   and   the  random   search  
method.  Features  in  red   co lor  are  common  features  from  
those  two  d ifferen t  methods.  From   this   result,  we  observe  
that  7   out  of   13  features   are  shared .  This   is   sligh tly   more  
than  50%, wh ich  imp lies  that  our  feature  selection  methods  
are  reasonable  in  physical  sense.  However,  there  are  still  
mismatch  of  features.  I t  suggests  the  current  analytical  
model  is  no t  sophisticated   enough  to  describe  the  relation  
between  frequency,  temperature,  and  the  bias  voltage  
correctly. This  is  true because a known phenomenon such  as  
‘A-f  effect’  is  no t  included   in  Eqn.  2   due  to  its  comp lex ity.  
Also,  it  is  possible  that  unknown  factors  affect  the  
measurement resu lt, causing Eqn. 2 no t  to predict frequency  
precisely. 
In  conclusion,  comparison  between   analy tical  model  
and  results  from  feature  selection  proved  the  reliability  of  
feature  selection method we used . Also,  it g ives us   the need  
to improve the analytical model for better  prediction. 

2, 

4,  

2x2

2x2

6, x1

3,    x1

2, x1x2
5, x2
2, x1
3,  x1
3x2
const.,  x1
2x2
5, x1
2, x2
4x2
4, x1, x1
3, x1
4 
x1x2
2, x1,  x1
4, x1
2, x2
2, x2
1x2
const., x2
4, x2
5, x1
2x2
4, x1x2
6, x2
3 
x1x2

CS229 Autumn 2008 
 
Table 3:  Comparison o f  the comb ination of 13 best  fea tures  
extracted   by  random  feature  selection  a lgorithm  and  the  
analytica l mode. 
 
Random   feature  
selection 
Analytical 
model 
 
2. OPTIMIZED CALIBRATION POINT 
In 
the  
the  previous   chapter ,  we  could   specify 
combination  of  op timized   features  using  feature  selection.  
As  a  next  step ,  we  minimize  the  number  of  measurement  
points  to  generate  calibration  tab les  with  similar  accuracy.  
Since  the  optimized  number   of  feature  is  13,  we  on ly  need  
13  measuremen t  po ints   to  generate  calibration  tab le.  We  
reduce  un important  measurement  points  using  backward  
search  algorithm.  Fig .  4  show  the  f itting  curve  wh ich   is  
generated   by  13  measurement  poin ts   chosen  by  backward  
search algorithm , and the error between the f itting  curve and  
the  measurement  values.  The  maximum  error  of  the  f itting  
curve  is  0 .520  ppm.  Since we  orig inally  need   323  points   to  
generate  the  calibration   table  w ith  0 .09  ppm  max imum  
error, we  could  ach ieve  25  times   reduction  of  measurement  
points w ith main taining sub ppm error. 
Optimized   measuremen t  po ints   chosen  by   a  certain  
device  should   be  applicable  for  other  dev ices.  Hence,  we  
generate  fitting   curve  for   other   dev ice  using   the  same  
measurement  point.  Comparison  of   max imum   and  mean  
error of each dev ice is shown in Table 3. We can see  that all  
maximum error is  sub-ppm. 
 
 (i) 

(ii) 

 

 

0.0965 

0.139 

0.0701 

0.0710 

3 
0.290 

1 
0.797 

2 
0.302 

Original 
0.520 

 
Figure 4: ( i) Ca libra tion table genera ted  by the optim ized  13  
measurements   poin t  using   backward   search.  ( ii)   The  error  
between  the  genera ted   ca libra tion  table  and  measurement  
result. 
 
Table  3:  Maximum   and  mean   errors   of  calibration  table  
generated by 13 measurement poin ts . Measurement poin t  is  
specified   by  “original”  device  with  backward  search  
algorithm ,  and  those  points   are  used  for   the  ca libra tion  in  
device # 1, 2, and 3. 
 
Device # 
Max. error 
[ppm] 
Mean error 
[ppm] 
 
3. DEVICE-TO-DEVICE VARIATION 
In previous  two  parts, we  figured  out  the way  to obtain  
a  calibration   curve  for   a  device  in  efficient  way .  A lthough  
those  are  great  accomplishment,  we  want  to  further  reduce  
the number of measurement points .  
The  idea  star ts  from  the  fact  that  resonators  w ith  the  
same  design   share  general  frequency-temperature-b ias  
voltage  characteristics  even  if  there  is   a  small  var iation  due  
to  uncertainties  in  manufacturing   process .    In  other  words,  
although we  cannot  apply   the  character ization  result  of  one  
device  to other devices directly ,  it is  possib le to estimate  the  
fitting   curve  of   a 
if   we  combine  fu ll  
test  dev ice 
characterization  data  of  train ing  devices  with  a  few  
measurement data of the test dev ice.  
Since  the  var iation  is  caused  by  latent  (unobservable)  
variab les   such  as  d imensional  variables  of  the  resonant  
beam ,  we  need  add itional  var iab les   on  top  of  temperature  
and  bias  vo ltage  to  learn  general  characteristics.  The  
frequency  of   at  cer tain  temperature  and   bias  voltage  po ints  
are  only  op tion   for  th is   purpose  because  there  are  no   more  
observable var iab les .  
Unlike  Eqn.  1, which   is   a  good  hypothesis   for  a  single  
resonator  characteristic,  we  don’t  know  wh ich  hypothesis  
effective to  treat device- to-device var iation  due to  additional  
input  variables.  Therefore,  we  decide  to   use  the  Gaussian  
process   regression   since  it  free  us   from   the  d ifficu lty   to  
choose r ight hypothesis . 
Let’s  def ine  training  input  as  XIN,  training  output  as  F,  
testing  input  as  X*IN,  and  the  testing  output  as  F*.  As  
additional  inpu t  var iables  to  identify  each   device  and  to  
learn  general  characteristic,  we  include  the  frequency   of  
certain  measuremen t poin ts  to input XIN.  If we define: 
 
m:  # of on calibration data on each  device 
n:  # of devices  for train ing 
k:   # of measurement data on a test dev ice 
 
 
 
 
 

CS229 Autumn 2008 
Input  for  the  Gaussian  process  regression  can  be  
described as: 
 

’ R ( nm + k )(1 ,   X IN =

X1 Y1

"

"

X n Yn
X t
Yt

!
#
#
#
#
"

$
&
&
&
&
%

’ R ( nm + k )( ( k + 2 )

 

$% ’ R ( m ) k )(1 ,   X *IN = X t * Yt *
[

] ’ R ( m ) k )( ( k + 2 )

F =

!
#
#
#
#
#
#
"

!
f1!
f2

$
&
&
&
"
&
!
fn!
&
&
ft
%
!
ft *!"

F* =
 
 
where, 
fi ( x (1) )
!
#
fi ( x ( 2 ) )
#
#
"
#
fi ( x ( m ) )
#
"

!
fi =

!
ft =

!
#
#
#
#
#
"

!
ft * =

ft ( x (1) )
ft ( x ( 2 ) )
"
ft ( x ( k ) )

$
&
&
&
&
&
%
ft ( x ( k +1) )
ft ( x ( k + 2 ) )
"
ft ( x ( m ) )

!
#
#
#
#
#
"

$
&
&
&
&
&
%

$
&
&
&
&
&
%

,  X i =

,  X t =

(1)
xi ,1
( 2 )
xi ,1

"
( m )
xi ,1

!
#
#
#
#
#
"

(1)
xt ,1
( 2 )
xt ,1

"
( k )
xt ,1

!
#
#
#
#
#
"

(1)
xi ,2
(1)
xi ,2

"
( m )
xi ,2

$
&
&
&
&
&
%

,   Yi =

!
#
#
#
#
"

fi ,1 # fi ,k
fi ,1

"

"

fi ,1 # fi ,k

$
&
&
&
&
%

(1)
xt ,2
( 2 )
xt ,2

"
( k )
xt ,2

$
&
&
&
&
&
%

,   Yt =

!
#
#
#
#
"

ft ,1 # ft ,k
ft ,k
ft ,1

"

"

ft ,1 # ft ,k

$
&
&
&
&
%

 

,  X t * =

,   Yt * =

( k +1)
xt ,1
( k + 2 )
xt ,1

"
( m )
xt ,1

( k +1)
xt ,2
( k + 2 )
xt ,2

"
( m )
xt ,2

!
#
#
#
#
#
"

$
&
&
&
&
&
%

!
#
#
#
#
"

ft ,1 # ft ,k
ft ,1
ft ,k

"

"

ft ,1 # ft ,k

$
&
&
&
&
%

 

 
In the imp lemen tation, we prepare measurement data of  
3  devices  with 
identical  design.  Then,  we  use  fu ll  
characterization  data  of  2  dev ices  w ith  data  from  a  few  
measurement  point  of  the  test  dev ice  for   train ing  with  the  
squared exponen tial kernel. After optimizing parameters   for  
the  regression,  we  generate  the  calibration  table  (F ig.  5).  
Less   than   20  ppm   error   is   ach ieved   w ith   5  po ints  
measurement,  and  less   than  40  ppm  error  with  1  point  
measurement.  Th is  is  great  result  consider ing  we  use  on ly  
two dev ices for training. Further  improvemen t is expected in  
the fu ture when more train ing dev ices are prepared .  
 

(i) 

 
 
 
 
 
 

 

(ii) 

 

 
Figure  5:  Calibration  table  genera ted  by  (i)   5  point  
measurements   and  ( ii)   1   point  measurement  based   on  the  
train ing data o f 2 devices. 
 
CONCLUSION 
Using  too ls  in mach ine  learning, we  success   to develop  
effective  calibration  process   for  the  frequency   con trol  of  
MEMS  resonators .  We  ach ieve  25  times   reduction  of  
measurement  poin ts  for  the  calibration  with  maintain ing  
sub-ppm  error  between  the  fitting  curve  and  measurement  
values .  Also,  we  demonstrate  the  use  of  Gaussian  process  
regression  to   solve  dev ice-to-device  var iation  issues.  S ince  
the  curren t  calibration  error  in  this  part  is  not  sufficien t  for  
the commercialization, future work would be focused on  the  
improvement of  this  problem  using  large amount of  train ing  
data. 
 
ACKNOWLEDGEMENT 
We  wou ld  like  to  thank  Prof.  Ng  and  CS229  teach ing  
team for great learning experiences. 
 
REFERENCES 
[1]  CS229 Machine Learn ing, Lecture Note 
[2]  S.  Chatterjee  and  A.  S.  Hadi,  Regression   Ana lysis   by  
Example. Hoboken: John Wiley  & Sons, 2006. 
[3]  C.  E.  Rasmussen   and  C.  K .  I.  Williams ,  “Gaussian  
Process for Mach ine Learn ing,” MIT Press, 2006. 
[4]    C.  T.  C.  Nguyen,  "MEMS  technology  for  tim ing  and  
frequency  control,"  IEEE  transactions  on  ultrasonics,  
ferroelectrics,  and   frequency   con trol,  vol.  54 ,  pp.  
251-70 (2007). 
[5]   G. K. Ho, K. Sundaresan , S .  Pourkamali,  and F . Ayazi,  
"Tempera ture 
reference  
IBAR 
compensated 
oscilla tors,"  in  19th  IEEE  International  Conference  on  
Micro  Electro  Mechan ical  Systems 
(MEMS'06),  
Istanbul, Turkey (2006), pp. 910-913. 
[6]    R.  Melamud,  B. K im ,  S.  Chandorkar  ,  M. A. Hopcroft,  
M.  Agarwal,  C.  M.  Jha,  and  T.  W.  Kenny,  
"Tempera ture-compensated 
high-stability 
silicon  
resonators ,"  Applied  physics  letters ,  vol.  90,  244107  
(2007). 
[7]   http://www.vectron.com/products/tcxo/tcxo_index .htm 
 


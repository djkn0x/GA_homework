Logistic Normal Priors for Unsupervised
Probabilistic Grammar Induction

Shay B. Cohen Kevin Gimpel Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{scohen,kgimpel,nasmith}@cs.cmu.edu

Abstract

We explore a new Bayesian model for probabilistic grammars, a family of
distributions over discrete structures that includes hidden Markov models
and probabilistic context-free grammars. Our model extends the correlated
topic model framework to probabilistic grammars, exploiting the logistic
normal distribution as a prior over the grammar parameters. We derive
a variational EM algorithm for that model, and then experiment with the
task of unsupervised grammar induction for natural language dependency
parsing. We show that our model achieves superior results over previous
models that use diﬀerent priors.

1 Introduction

Unsupervised learning of structured variables in data is a diﬃcult problem that has received
considerable recent attention. In this paper, we consider learning probabilistic grammars,
a class of structure models that includes Markov models, hidden Markov models (HMMs)
and probabilistic context-free grammars (PCFGs). Central to natural language process-
ing (NLP), probabilistic grammars are recursive generative models over discrete graphical
structures, built out of conditional multinomial distributions, that make independence as-
sumptions to permit eﬃcient exact probabilistic inference.
There has been an increased interest in the use of Bayesian methods as applied to probabilis-
tic grammars for NLP, including part-of-speech tagging [10, 20], phrase-structure parsing
[7, 11, 16], and combinations of models [8].
In Bayesian-minded work with probabilistic
grammars, a common thread is the use of a Dirichlet prior for the underlying multinomials,
because as the conjugate prior for the multinomial, it bestows computational feasibility.
The Dirichlet prior can also be used to encourage the desired property of sparsity in the
learned grammar [11].
A related widely known example is the latent Dirichlet al location (LDA) model for topic
modeling in document collections [5], in which each document’s topic distribution is treated
as a hidden variable, as is the topic distribution from which each word is drawn.1 Blei and
Laﬀerty [4] showed empirical improvements over LDA using a logistic normal distribution
that permits diﬀerent topics to correlate with each other, resulting in a correlated topic
model (CTM). Here we aim to learn analogous correlations such as: a word that is likely
to take one kind of argument (e.g., singular nouns) may be likely to take others as well
(e.g., plural or proper nouns). By permitting such correlations via the distribution over the

1A certain variant of LDA can be seen as a Bayesian version of a zero-order HMM, where the
unigram state (topic) distribution is sampled ﬁrst for each sequence (document).

Figure 1: A graphical model for the logistic normal probabilistic grammar. y is the deriva-
tion, x is the observed string.

parameters, we hope to break independence assumptions typically made about the behavior
of diﬀerent part-of-speech tags.
In this paper, we present a model, in the Bayesian setting, which extends CTM for proba-
bilistic grammars. We also derive an inference algorithm for that model, which is ultimately
used to provide a point estimate for the grammar, permitting us to perform fast and exact
inference. This is required if the learned grammar is to be used as a component in an
application.
The rest of the paper is organized as follows. §2 gives a general form for probabilistic
grammars built out of multinomial distributions. §3 describes our model and an eﬃcient
variational inference algorithm. §4 presents a probabilistic context-free dependency gram-
mar often used in unsupervised natural language learning. Experimental results showing
the competitiveness of our method for estimating that grammar are presented in §5.

2 Probabilistic Grammars

A probabilistic grammar deﬁnes a probability distribution over a certain kind of structured
ob ject (a derivation of the underlying symbolic grammar) explained through step-by-step
stochastic process. HMMs, for example, can be understood as a random walk through
a probabilistic ﬁnite-state network, with an output symbol sampled at each state. PCFGs
generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of
“child” symbols (each itself either a nonterminal symbol or a terminal symbol analogous to
the emissions of an HMM). Each step or emission of an HMM and each rewriting operation
of a PCFG is conditionally independent of the others given a single structural element (one
HMM or PCFG state); this Markov property permits eﬃcient inference.
In general, a probabilistic grammar deﬁnes the joint probability of a string x and a gram-
NkX
KX
NkY
KY
matical derivation y:
i=1
i=1
k=1
k=1
where fk,i is a function that “counts” the number of times the kth distribution’s ith event
occurs in the derivation. The parameters θ are a collection of K multinomials hθ1 , ..., θK i,
the kth of which includes Nk events. Note that there may be many derivations y for a given
string x—perhaps even inﬁnitely many in some kinds of grammars. HMMs and vanilla
PCFGs are the best known probabilistic grammars, but there are others. For example, in
§5 we experiment with the “dependency model with valence,” a probabilistic grammar for
dependency parsing ﬁrst proposed in [14].

p(x, y | θ) =

fk,i (x, y) log θk,i

θfk,i (x,y)
k,i

= exp

(1)

3 Logistic Normal Prior on Probabilistic Grammars

A natural choice for a prior over the parameters of a probabilistic grammar is a Dirichlet
prior. The Dirichlet family is conjugate to the multinomial family, which makes the inference
more elegant and less computationally intensive. In addition, a Dirichlet prior can encourage
sparse solutions, a property which is important with probabilistic grammars [11].

μk∑kηk              θk                     y                xKNKHowever, in [4], Blei and Laﬀerty noticed that the Dirichlet distribution is limited in its
expressive power when modeling a corpus of documents, since it is less ﬂexible about cap-
turing relationships between possible topics. To solve this modeling issue, they extended the
on the d-dimensional probability simplex, Sd = {hz1 , ..., zd i ∈ Rd : zi ≥ 0, Pd
LDA model to use a logistic normal distribution [2] yielding correlated topic models. The
logistic normal distribution maps a d-dimensional multivariate Gaussian to a distribution
i=1 zi = 1}, by
exponentiating the normally-distributed variables and normalizing.
Here we take a step analogous to Blei and Laﬀerty, aiming to capture correlations between
the grammar’s parameters. Our hierarchical generative model, which we call a logistic-
normal probabilistic grammar, generates a sentence and derivation tree hx, yi as follows (see
also Fig. 1):
.PNk
1. Generate ηk ∼ N(µk , Σk ) for k = 1, ..., K .
2. Set θk,i = exp(ηk,i )
i0=1 exp(ηk,i0 ) for k = 1, ..., K and i = 1, ..., Nk .
3. Generate x and y from p(x, y | θ) (i.e., sample from the probabilistic grammar).

(2)

(3)

We now turn to derive a variational inference algorithm for the model.2 Variational Bayesian
inference seeks an approximate posterior function q(η , y) which maximizes a lower bound
(the negated variational free energy) on the log-likelihood [12], a bound which is achieved
log p(x, y | µ, Σ) ≥ PK
using Jensen’s inequality:
Eq [log p(η i | µi , Σi )] + Eq [log p(x, y | η)] + H (q)
i=1
(cid:16)QK
(cid:17) × q(y)
QNk
We make a mean-ﬁeld assumption, and assume that the posterior has the following form:
i=1 q(ηk,i | ˜µk,i , ˜σ2
q(η , y) =
k,i )
k=1
where q(ηk,i | ˜µk,i , ˜σ2
k,i ).
k,i ) is a Gaussian N( ˜µk,i , ˜σ2
Unfolding the expectation with respect to q(y) in the second term in Eq. 2, while recalling
hPK
i
PNk
P
that θ is a deterministic function of η , we have that:
|
{z
}
Eq [log p(x, y | η)] = Eq(η)
y q(y)fk,i (x, y)
log θk,i
hPK
(cid:16)
(cid:17)i
k=1
i=1
PNk
ηk,i − log PNk
˜fk,i
= Eq(η)
˜fk,i
i0=1 exp ηk,i0
k=1
i=1
where ˜fk,i is the expected number of occurrences of the ith event in distribution k , under
q(y).3 The logarithm term in Eq. 4 is problematic, so we follow [4] in approximating it with
(cid:16)PNk
(cid:17) ≤ log ˜ζk − 1 +
PNk
a ﬁrst-order Taylor expansion, introducing K more variational parameters ˜ζ1 , ..., ˜ζK :
1
i0=1 exp ηk,i0
i0=1 exp ηk,i0
˜ζk
(cid:17)i
(cid:16)
hPK
PNk
PNk
We now have
(cid:18)
(cid:17)(cid:19)
(cid:16)
Eq [log p(x, y | η)] ≥ Eq(η)
ηk,i − log ˜ζk + 1 − 1
PNk
= PK
PNk
˜fk,i
i0=1 exp ηk,i0
(6)
˜ζk
i=1
k=1
˜µk,i − log ˜ζk + 1 − 1
{z
}
|
˜µk,i + ˜σ2
˜fk,i
i0=1 exp
k,i
˜ζk
i=1
k=1
2
PNk
= PK
˜ψk,i
˜ψk,i
(7)
k=1
i=1
2We note that variational inference algorithms have been successfully applied to grammar learn-
ing tasks, for example, in [16] and [15].
3With probabilistic grammars, this quantity can be computed using a summing dynamic pro-
gramming algorithm like the forward-backward or inside-outside algorithm.

˜fk,i

log

(4)

(5)

Note the shorthand ˜ψk,i to denote an expression involving ˜µ, ˜σ , and ˜ζ .
(cid:16)PK
log p(x, y | µ, Σ) ≥ (cid:16)PK
(cid:17)
(cid:17)
PNk
The ﬁnal form of our bound is:4
Eq [log p(ηk | µk , Σk )]
k=1
i=1
k=1
Since, we are interested in EM-style algorithm, we will alternate between ﬁnding the max-
imizing q(η) and the maximizing q(y). Maximization with respect to q(η) is not hard,
because q(η) is parametrized (see Appendix A). The following lemma shows that fortu-
nately, ﬁnding the maximizing q(y), which we did not parametrize originally, is not hard
either:
Lemma 1. Let r(y | x, e ˜ψ ) denote the conditional distribution over y given x deﬁned as:
QNk
QK
1
r(y | x, e ˜ψ ) =
i=1 exp ˜ψk,i fk,i (x, y)
Z ( ˜ψ)
k=1

+ H (q)

˜ψk,i

(8)

+

˜fk,i

(9)

where Z ( ˜ψ) is a normalization constant. Then q(y) = r(y | x, e ˜ψ ) maximizes the bound in
Eq. 8.
Proof. First note that H (q) = H (q(η | ˜µ, ˜σ)) + H (q(y)). This means that the terms we are
interested in maximizing from Eq. 8 are the following, after writing down ˜fk,i explicitly:
(cid:16)PK
(cid:17)
PNk
P
i=1 fk,i (x, y) ˜ψk,i
y q(y)
k=1

+ H (q(y))

(10)

L = argmax
q(y)

However, note that:
L = argmin
q(y)
with the fact that PK
PNk
where DKL denotes the KL divergence. To see that, combine the deﬁnition of KL divergence
i=1 fk,i (x, y) ˜ψk,i − log Z ( ˜ψ) = log r(y | x, e ˜ψ ) where log Z ( ˜ψ)
k=1
does not depend on q(y). Eq. 11 is minimized when q = r .

DKL (q(y)kr(y | x, e ˜ψ ))

(11)

Interestingly, from the above lemma, the minimizing q(y) has the same form as the prob-
abilistic grammar in discussion, only without having sum-to-one constraints on θ (leading
to the required normalization constant). As in classic EM with probabilistic grammars, we
never need to represent q(y) explicitly; we need only ˜f , which can be calculated as expected
feature values under r(y | x, e ˜ψ ) using dynamic programming.
As noted, we are interested in a point estimate of θ . To achieve this, we will use the above
variational method within an EM algorithm that estimates µ and Σ in empirical Bayes
fashion, then estimates θ as µ, the mean of the learned prior. In the E-step, we maximize
the bound with respect to the variational parameters ( ˜µ, ˜σ , ˜ζ , and ˜f ) using coordinate
ascent. We optimize each of these separately in turn, cycling through, using appropriate
optimization algorithms for each (conjugate gradient for ˜µ, Newton’s method for ˜σ , a closed
form for ˜ζ , and dynamic programming to solve for ˜f ). In the M-step, we apply maximum
likelihood estimation with respect to µ and Σ given suﬃcient statistics gathered from the
variational parameters in the E-step. The full algorithm is given in Appendix A.

4 Probabilistic Dependency Grammar Model

Dependency grammar [19] refers to linguistic theories that posit graphical representations
of sentences in which words are vertices and the syntax is a tree. Such grammars can
be context-free or context-sensitive in power, and they can be made probabilistic [9]. De-
pendency syntax is widely used in information extraction, machine translation, question

4A tighter bound was proposed in [1], but we follow [4] for simplicity.

x = hNNP VBD JJ NNPi; y =

Figure 2: An example of a dependency tree (derivation y). NNP denotes a proper noun,
VBD a past-tense verb, and JJ an adjective, following the Penn Treebank conventions.

answering, and other natural language processing applications. Here, we are interested in
unsupervised dependency parsing using the “dependency model with valence” [14]. The
model is a probabilistic head automaton grammar [3] with a “split” form that renders in-
ference cubic in the length of the sentence [6].
Let x = hx1 , x2 , ..., xn i be a sentence (here, as in prior work, represented as a sequence
of part-of-speech tags). x0 is a special “wall” symbol, $, on the left of every sentence. A
tree y is deﬁned by a pair of functions yleft and yright (both {0, 1, 2, ..., n} → 2{1,2,...,n} )
that map each word to its sets of left and right dependents, respectively. Here, the graph
is constrained to be a projective tree rooted at x0 = $: each word except $ has a single
parent, and there are no cycles or crossing dependencies. yleft (0) is taken to be empty, and
yright (0) contains the sentence’s single head. Let y(i) denote the subtree rooted at position
i. The probability P (y(i) | xi , θ) of generating this subtree, given its head word xi , is deﬁned
P (y(i) | xi , θ) = Q
recursively:
× Q
D∈{left ,right } θs (stop | xi , D , [yD (i) = ∅])
(12)
j∈yD (i) θs (¬stop | xi , D , ﬁrsty (j )) × θc (xj | xi , D ) × P (y(j ) | xj , θ)
where ﬁrsty (j ) is a predicate deﬁned to be true iﬀ xj is the closest child (on either side)
to its parent xi . The probability of the entire tree is given by p(x, y | θ) = P (y(0) | $, θ).
The parameters θ are the multinomial distributions θs (· | ·, ·, ·) and θc (· | ·, ·). To follow
the general setting of Eq. 1, we index these distributions as θ1 , ..., θK . Figure 2 shows a
dependency tree and its probability under this model.

5 Experiments

Data Following the setting in [13], we experimented using part-of-speech sequences from
the Wal l Street Journal Penn Treebank [17], stripped of words and punctuation. We follow
standard parsing conventions and train on sections 2–21,5 tune on section 22, and report
ﬁnal results on section 23.

Evaluation After learning a point estimate θ , we predict y for unseen test data (by parsing
with the probabilistic grammar) and report the fraction of words whose predicted parent
matches the gold standard corpus, known as attachment accuracy. Two parsing methods
were considered: the most probable “Viterbi” parse (argmaxy p(y | x, θ)) and the minimum
Ep(y0 |x,θ) [‘(y; x, y0 )]) with dependency attachment error
Bayes risk (MBR) parse (argminy
as the loss function.

Settings Our experiment compares four methods for estimating the probabilistic gram-
mar’s parameters:
EM Maximum likelihood estimate of θ using the EM algorithm to optimize p(x | θ) [14].
EM-MAP Maximum a posteriori estimate of θ using the EM algorithm and a ﬁxed sym-
metric Dirichlet prior with α > 1 to optimize p(x, θ | α). Tune α to maximize the
likelihood of an unannotated development dataset, using grid search over [1.1, 30].

5Training in the unsupervised setting for this data set can be expensive, and requires running a
cubic-time dynamic programming algorithm iteratively, so we follow common practice in restricting
the training set (but not development or test sets) to sentences of length ten or fewer words. Short
sentences are also less structurally ambiguous and may therefore be easier to learn from.

NNPPatrick  spokelittleFrenchNNPVBDJJVB-Dirichlet Use variational Bayes inference to estimate the posterior distribution p(θ |
x, α), which is a Dirichlet. Tune the symmetric Dirichlet prior’s parameter α to
maximize the likelihood of an unannotated development dataset, using grid search
over [0.0001, 30]. Use the mean of the posterior Dirichlet as a point estimate for θ .
VB-EM-Dirichlet Use variational Bayes EM to optimize p(x | α) with respect to α. Use
the mean of the learned Dirichlet as a point estimate for θ (similar to [5]).
VB-EM-Log-Normal Use variational Bayes EM to optimize p(x | µ, Σ) with respect to
µ and Σ. Use the (exponentiated) mean of this Gaussian as a point estimate for θ .

Initialization is known to be important for EM as well as for the other algorithms we
experiment with, since it involves non-convex optimization. We used the successful initializer
from [14], which estimates θ using soft counts on the training data where, in an n-length
sentence, (a) each word is counted as the sentence’s head 1
n times, and (b) each word xi
attaches to xj proportional to |i − j |, normalized to a single attachment per word. This
initializer is used with EM, EM-MAP, VB-Dirichlet, and VB-EM-Dirichlet. In the case of
VB-EM-Log-Normal, it is used as an initializer both for µ and inside the E-step. In all
experiments reported here, we run the iterative estimation algorithm until the likelihood of
a held-out, unannotated dataset stops increasing.
For learning with the logistic normal prior, we consider two initializations of the covariance
matrices Σk . The ﬁrst is the Nk × Nk identity matrix. We then tried to bias the solution
by injecting prior knowledge about the part-of-speech tags. Injecting a bias to parameter
estimation of the DMV model has proved to be useful [18]. To do that, we mapped the tag
set (34 tags) to twelve disjoint tag families.6 The covariance matrices for all dependency
distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to
the same family, and 0 otherwise. These results are given in Table 1 with the annotation
“families.”

Results Table 1 shows experimental results. We report attachment accuracy on three
subsets of the corpus: sentences of length ≤ 10 (typically reported in prior work and most
similar to the training dataset), length ≤ 20, and the full corpus. The Bayesian methods all
outperform the common baseline (in which we attach each word to the word on its right),
but the logistic normal prior performs considerably better than the other two methods as
well.
The learned covariance matrices were very sparse when using the identity matrix to ini-
tialize. The diagonal values showed considerable variation, suggesting the importance of
variance alone. When using the “tag families” initialization for the covariance, there were
151 elements across the covariance matrices which were not identically 0 (out of more than
1,000), pointing to a learned relationship between parameters. In this case, most covariance
matrices for θc dependencies were diagonal, while many of the covariance matrices for the
stopping probabilities (θs ) had signiﬁcant correlations.

6 Conclusion

We have considered a Bayesian model for probabilistic grammars, which is based on the
logistic normal prior. Experimentally, several diﬀerent approaches for grammar induction
were compared based on diﬀerent priors. We found that a logistic normal prior outperforms
earlier approaches, presumably because it can capitalize on similarity between part-of-speech
tags, as diﬀerent tags tend to appear as arguments in similar syntactic contexts. We achieved
state-of-the-art unsupervised dependency parsing results.

6These are simply coarser tags: adjective, adverb, conjunction, foreign, interjection, noun, num-
ber, particle, preposition, pronoun, proper, verb. The coarse tags were chosen manually to ﬁt seven
treebanks in diﬀerent languages.

Attach-Right
EM
EM-MAP, α = 1.1
VB-Dirichlet, α = 0.25
VB-EM-Dirichlet
VB-EM-Log-Normal, Σ(0)
k = I
VB-EM-Log-Normal, families

attachment accuracy (%)
Viterbi decoding
MBR decoding
|x| ≤ 20
|x| ≤ 10
|x| ≤ 20
|x| ≤ 10
33.4
38.4
33.4
38.4
45.8
39.1
46.1
39.9
40.6
46.2
39.5
45.9
41.1
47.1
40.0
46.9
40.6
46.1
39.4
45.9
56.6
43.3
59.1
45.9
45.9
59.4
45.1
59.3

all
31.7
34.2
34.9
35.7
34.9
37.4
39.0

all
31.7
35.9
36.7
37.6
36.9
39.9
40.5

Table 1: Attachment accuracy of diﬀerent learning methods on unseen test data from the
Penn Treebank of varying levels of diﬃculty imposed through a length ﬁlter. Attach-Right
attaches each word to the word on its right and the last word to $. EM and EM-MAP with
a Dirichlet prior (α > 1) are reproductions of earlier results [14, 18].

Acknowledgments

The authors would like to thank the anonymous reviewers, John Laﬀerty, and Matthew
Harrison for their useful feedback and comments. This work was made possible by an IBM
faculty award, NSF grants IIS-0713265 and IIS-0836431 to the third author and computa-
tional resources provided by Yahoo.

References

[1] A. Ahmed and E. Xing. On tight approximate inference of the logistic normal topic
admixture model. In Proc. of AISTATS, 2007.
[2] J. Aitchison and S. M. Shen. Logistic-normal distributions: some properties and uses.
Biometrika, 67:261–272, 1980.
[3] H. Alshawi and A. L. Buchsbaum. Head automata and bilingual tiling: Translation
with minimal representations. In Proc. of ACL, 1996.
[4] D. Blei and J. D. Laﬀerty. Correlated topic models. In Proc. of NIPS, 2006.
[5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[6] J. Eisner. Bilexical grammars and a cubic-time probabilistic parser. In Proc. of IWPT,
1997.
[7] J. Eisner. Transformational priors over grammars. In Proc. of EMNLP, 2002.
[8] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annotation pipelines. In Proc. of EMNLP,
2006.
[9] H. Gaifman. Dependency systems and phrase-structure systems.
Control, 8, 1965.
[10] S. Goldwater and T. L. Griﬃths. A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proc. of ACL, 2007.
[11] M. Johnson, T. L. Griﬃths, and S. Goldwater. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of NAACL, 2007.
[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K. Saul. An introduction to
variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.
[13] D. Klein and C. D. Manning. A generative constituent-context model for improved
grammar induction. In Proc. of ACL, 2002.
[14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models
of dependency and constituency. In Proc. of ACL, 2004.

Information and

[15] K. Kurihara and T. Sato. Variational Bayesian grammar induction for natural language.
In Proc. of ICGI, 2006.
[16] P. Liang, S. Petrov, M. Jordan, and D. Klein. The inﬁnite PCFG using hierarchical
Dirichlet processes. In Proc. of EMNLP, 2007.
[17] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated
corpus of English: The Penn treebank. Computational Linguistics, 19:313–330, 1993.
[18] N. A. Smith and J. Eisner. Annealing structural bias in multilingual weighted grammar
induction. In Proc. of COLING-ACL, 2006.
[19] L. Tesni`ere. ´El´ement de Syntaxe Structurale. Klincksieck, 1959.
[20] K. Toutanova and M. Johnson. A Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proc. of NIPS, 2007.

A VB-EM for Logistic-Normal Probabilistic Grammars

The algorithm for variational inference with probabilistic grammars using logistic normal
prior follows.7 Since the updates for ˜ζ l,(t)
are fast, we perform them after each optimization
k
routine in the E-step (suppressed for clarity). There are variational parameters for each
training example, indexed by ‘. We denote by B the variational bound in Eq. 8. Our
stopping criterion relies on the likelihood of a held-out set (§5) using a point estimate of the
model.
Input: initial parameters µ(0) , Σ(0) , training data x, and development data x0
Output: learned parameters µ, Σ
t ← 1 ;
repeat
”
“ ˜fk,i0 / ˜ζk
”
= − “
exp ` ˜µk,i0 + ˜σ2
k,i0 /2´;
k,i + PNk
E-step (for each training example ‘ = 1, ..., M ): repeat
optimize for ˜µ‘,(t)
, k = 1, ..., K : use conjugate gradient descent with
k
− ˜µ‘
− ˜f ‘
)−1 )(µ(t−1)
(Σ(t−1)
∂L
k )
i0=1
k,ii /2 − “PNk
”
k
k
∂ ˜µ‘
i
k,i
optimize ˜σ ‘,(t)
, k = 1, ..., K : use Newton’s method for each coordinate (with ˜σ ‘
k,i > 0) with
“
”
k
k ← PNk
= −Σ(t−1)
k,i /2)/2 ˜ζk + 1/2 ˜σ2
˜fk,i0
exp( ˜µk,i + ˜σ2
∂L
k,i ;
i0=1
∂ ˜σ2
“
”
PNk
k,i
, ∀k : ˜ζ ‘,(t)
update ˜ζ ‘,(t)
k,i + ( ˜σ ‘,(t)
˜µ‘,(t)
k,i )2 /2
;
i=1 exp
k
k + 1 − 1
, ∀k : ˜ψ ‘,(t)
k,i ← ˜µ‘,(t)
k,i − log ˜ζ ‘,(t)
‘,(t)
update ˜ψ
k,i + ( ˜σ ‘,(t)
˜µ‘,(t)
k,i )2 /2
i0=1 exp
‘,(t)
k
˜ζ
k
compute expected counts ˜f ‘,(t)
, k = 1, ..., K : use an inside-outside algorithm to re-estimate
k
in weighted grammar q(y) with weights e ˜ψ‘
expected counts ˜f ‘,(t)
k,i
until B does not change ;
PM
M-step: Estimate µ(t) and Σ(t) using the following maximum likelihood closed form solution:
i
h
“PM
PM
PM
k,i ← 1
µ(t)
‘=1 ˜µ‘,(t)
k,i
M
k,i − µ(t)
← 1
k,j − µ(t)
‘=1 ˜µ‘,(t)
‘=1 ˜µ‘,(t)
k,iµ(t)
k,i δi,j + M µ(t)
k,i ˜µ‘,(t)
‘=1 ˜µ‘,(t)
k,j + ( ˜σ ‘,(t) )2
k,i
k,j
k,j
M
where δi,j = 1 if i = j and 0 otherwise.
until likelihood of held-out data, p(x0 | E [µ(t) ]), decreases ;
t ← t + 1;
return µ(t) , Σ(t)

Σ(t)
k

i,j

;

;

”

7An implementation of the algorithm is available at http://www.ark.cs.cmu.edu/DAGEEM.


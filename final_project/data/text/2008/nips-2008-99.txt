Multi-label Multiple Kernel Learning

Shuiwang Ji
Arizona State University
Tempe, AZ 85287
shuiwang.ji@asu.edu

Rong Jin
Michigan State University
East Lansing, MI 48824
rongjin@cse.msu.edu

Liang Sun
Arizona State University
Tempe, AZ 85287
sun.liang@asu.edu

Jieping Ye
Arizona State University
Tempe, AZ 85287
jieping.ye@asu.edu

Abstract

We present a multi-label multiple kernel learning (MKL) formulation in which
the data are embedded into a low-dimensional space directed by the instance-
label correlations encoded into a hypergraph. We formulate the problem in the
kernel-induced feature space and propose to learn the kernel matrix as a linear
combination of a given collection of kernel matrices in the MKL framework. The
proposed learning formulation leads to a non-smooth min-max problem, which
can be cast into a semi-in ﬁnite linear program (SILP). We fur ther propose an ap-
proximate formulation with a guaranteed error bound which involves an uncon-
strained convex optimization problem. In addition, we show that the objective
function of the approximate formulation is differentiable with Lipschitz continu-
ous gradient, and hence existing methods can be employed to compute the optimal
solution efﬁciently. We apply the proposed formulation to t he automated annota-
tion of Drosophila gene expression pattern images, and promising results have
been reported in comparison with representative algorithms.

1 Introduction

Spectral graph-theoretic methods have been used widely in unsupervised and semi-supervised learn-
ing recently. In this paradigm, a weighted graph is constructed for the data set, where the nodes
represent the data points and the edge weights characterize the relationships between vertices. The
structural and spectral properties of graph can then be exploited to perform the learning task. One
fundamental limitation of using traditional graphs for this task is that they can only represent pair-
wise relationships between data points, and hence higher-order information cannot be captured [1].
Hypergraphs [1, 2] generalize traditional graphs by allowing edges, called hyperedges, to connect
more than two vertices, thereby being able to capture the relationships among multiple vertices.

In this paper, we propose to use a hypergraph to capture the correlation information for multi-label
learning [3].
In particular, we propose to construct a hypergraph for multi-label data in which
all data points annotated with a common label are included in a hyperedge, thereby capturing the
similarity among data points with a common label. By exploiting the spectral properties of the
constructed hypergraph, we propose to embed the multi-label data into a lower-dimensional space
in which data points with a common label tend to be close to each other. We formulate the multi-label
learning problem in the kernel-induced feature space, and show that the well-known kernel canonical
correlation analysis (KCCA) [4] is a special case of the proposed framework. As the kernel plays an
essential role in the formulation, we propose to learn the kernel matrix as a linear combination of a
given collection of kernel matrices in the multiple kernel learning (MKL) framework. The resulting

formulation involves a non-smooth min-max problem, and we show that it can be cast into a semi-
in ﬁnite linear program (SILP). To further improve the efﬁci
ency and reduce the non-smoothness
effect of the SILP formulation, we propose an approximate formulation by introducing a smoothing
term into the original problem. The resulting formulation is unconstrained and convex. In addition,
the objective function of the approximate formulation is shown to be differentiable with Lipschitz
continuous gradient. We can thus employ the Nesterov’s method [5, 6], which solves smooth convex
problems with the optimal convergence rate, to compute the solution efﬁciently.

We apply the proposed formulation to the automated annotation of Drosophila gene expression
pattern images, which document the spatial and temporal dynamics of gene expression during
Drosophila embryogenesis [7]. Comparative analysis of such images can potentially reveal new
genetic interactions and yield insights into the complex regulatory networks governing embryonic
development. To facilitate pattern comparison and searching, groups of images are annotated with a
variable number of labels by human curators in the Berkeley Drosophila Genome Project (BDGP)
high-throughput study [7]. However, the number of available images produced by high-throughput
in situ hybridization is now rapidly increasing. It is therefore tempting to design computational
methods to automate this task [8]. Since the labels are associated with groups of a variable number
of images, we propose to extract invariant features from each image and construct kernels between
groups of images by employing the vocabulary-guided pyramid match algorithm [9]. By applying
various local descriptors, we obtain multiple kernel matrices and the proposed multi-label MKL
formulation is applied to obtain an optimal kernel matrix for the low-dimensional embedding. Ex-
perimental results demonstrate the effectiveness of the kernel matrices obtained by the proposed
formulation. Moreover, the approximate formulation is shown to yield similar results to the original
formulation, while it is much more efﬁcient.

2 Multi-label Learning with Hypergraph

An essential issue in learning from multi-label data is how to exploit the correlation information
among labels. We propose to capture such information through a hypergraph as described below.

2.1 Hypergraph Spectral Learning

Hypergraphs generalize traditional graphs by allowing hyperedges to connect more than two ver-
tices, thus capturing the joint relationships among multiple vertices. We propose to construct a
hypergraph for multi-label data in which each data point is represented as a vertex. To document the
joint similarity among data points annotated with a common label, we propose to construct a hyper-
edge for each label and include all data points annotated with a common label into one hyperedge.
Following the spectral graph embedding theory [10], we propose to compute the low-dimensional
embedding through a linear transformation W by solving the following optimization problem:
tr (cid:0)W T φ(X )Lφ(X )T W (cid:1)
min
W
W T (cid:0)φ(X )φ(X )T + λI (cid:1) W = I ,
subject to
where φ(X ) = [φ(x1 ), · · · , φ(xn )] is the data matrix consisting of n data points in the feature
space, φ is the feature mapping, L is the normalized Laplacian matrix derived from the hypergraph,
and λ > 0 is the regularization parameter. In this formulation, the instance-label correlations are
encoded into L through the hypergraph, and data points sharing a common label tend to be close to
each other in the embedded space.

(1)

It follows from the representer theorem [11] that W = φ(X )B for some matrix B ∈ Rn×k where
k is the number of labels. By noting that L = I − C for some matrix C , the problem in Eq. (1) can
be reformulated as
tr (cid:0)B T (K CK )B (cid:1)
max
B
B T (K 2 + λK )B = I ,
subject to
where K = φ(X )T φ(X ) is the kernel matrix. Kernel canonical correlation analysis (KCCA) [4] is
a widely-used method for dimensionality reduction. It can be shown [4] that KCCA is obtained by
substituting C = Y T (Y Y T )−1Y in Eq. (2) where Y ∈ Rk×n is the label indicator matrix. Thus,
KCCA is a special case of the proposed formulation.

(2)

2.2 A Semi-inﬁnite Linear Program Formulation

It follows from the theory of kernel methods [11] that the kernel K in Eq. (2) uniquely determines the
feature mapping φ. Thus, kernel selection (learning) is one of the central issues in kernel methods.
Following the MKL framework [12], we propose to learn an optimal kernel matrix by integrating
multiple candidate kernel matrices, that is,
θj Kj (cid:12)(cid:12)θT e = 1, θ ≥ 0 
K ∈ K = 
p
Xj=1


where {Kj }p
j=1 are the p candidate kernel matrices, {θj }p
j=1 are the weights for the linear combi-
nation, and e is the vector of all ones of length p. We have assumed in Eq. (3) that all the candidate
kernel matrices are normalized to have a unit trace value. It has been shown [8] that the optimal
weights maximizing the objective function in Eq. (2) can be obtained by solving a semi-in ﬁnite lin-
ear program (SILP) [13] in which a linear objective is optimized subject to an in ﬁnite number of
linear constraints, as summarized in the following theorem:
Theorem 2.1. Given a set of p kernel matrices {Kj }p
j=1 , the optimal kernel matrix in K that maxi-
mizes the objective function in Eq. (2) can be obtained by solving the following SILP problem:

K =

(3)

,

max
θ ,γ

γ

subject to

θ ≥ 0, θT e = 1,

p
Xj=1

θj Sj (Z ) ≥ γ , for all Z ∈ Rn×k ,

(4)

(5)

(6)

where Sj (Z ), for j = 1, · · · , p, is de ﬁned as
k
Xi=1 (cid:18) 1
i hi(cid:19) ,
z T
i Kj zi − z T
4
Z = [z1 , · · · , zk ], H is obtained from C such that H H T = C , and H = [h1 , · · · , hk ].

z T
i zi +

Sj (Z ) =

1
4λ

Note that the matrix C is symmetric and positive semide ﬁnite. Moreover, for the L considered in
this paper, we have rank(C ) = k . Hence, H ∈ Rn×k is always well-de ﬁned. The SILP formulation
in Theorem 2.1 can be solved by the column generation technique as in [14].

3 The Approximate Formulation

The multi-label kernel learning formulation proposed in Theorem 2.1 involves optimizing a linear
objective subject to an in ﬁnite number of constraints. The c olumn generation technique used to solve
this problem adds constraints to the problem successively until all the constraints are satisﬁed. Since
the convergence rate of this algorithm is slow, the problem solved at each iteration may involve a
large number of constraints, and hence is computationally expensive. In this section, we propose an
approximate formulation by introducing a smoothing term into the original problem. This results in
an unconstrained and smooth convex problem. We propose to employ existing methods to solve the
smooth convex optimization problem efﬁciently in the next s ection.

By rewriting the formulation in Theorem 2.1 as

p
Xj=1
and exchanging the minimization and maximization, the SILP formulation can be expressed as

max
θ :θT e=1,θ≥0

θj Sj (Z )

min
Z

where f (Z ) is de ﬁned as

min
Z

f (Z )

f (Z ) =

max
θ :θT e=1,θ≥0

θj Sj (Z ).

p
Xj=1

(7)

(8)

,

(9)

fµ (Z ) =

θj Sj (Z ) − µ

The maximization problem in Eq. (8) with respect to θ leads to a non-smooth objective function for
f (Z ). To reduce this effect, we introduce a smoothing term and modify the objective to fµ (Z ) as
θj log θj 
θ :θT e=1,θ≥0 
p
p
Xj=1
Xj=1
max


where µ is a positive constant controlling the approximation. The following lemma shows that the
problem in Eq. (9) can be solved analytically:
Lemma 3.1. The optimization problem in Eq. (9) can be solved analytically, and the optimal value
can be expressed as
fµ (Z ) = µ log 
Sj (Z )(cid:19)
p
exp (cid:18) 1
Xj=1
 .

µ
Proof. De ﬁne the Lagrangian function for the optimization problem in Eq. (9) as
αj θj + 
θj − 1
p
p
p
p
Xj=1
Xj=1
Xj=1
Xj=1
 β ,

where {αj }p
j=1 and β are Lagrangian dual variables. Taking the derivative of the Lagrangian func-
µ (Sj (Z ) + αj + β − µ)(cid:17) .
tion with respect to θj and setting it to zero, we obtain that θj = exp (cid:16) 1
It follows from the complementarity condition that αj θj = 0 for j = 1, · · · , p. Since θj 6= 0, we
have αj = 0 for j = 1, · · · , p. By removing {αj }p
j=1 and substituting θj into the objective function
in Eq. (9), we obtain that fµ (Z ) = µ − β . Since µ − β = Sj (Z ) − µ log θj , we have
θj = exp ((Sj (Z ) − fµ (Z ))/µ) .
Following 1 = Pp
j=1 θj = Pp
j=1 exp ((Sj (Z ) − fµ (Z ))/µ) , we obtain Eq. (10).
The above discussion shows that we can approximate the original non-smooth constrained min-max
problem in Eq. (7) by the following smooth unconstrained minimization problem:

θj Sj (Z ) − µ

θj log θj +

(12)

(10)

L =

(11)

min
Z
where fµ (Z ) is de ﬁned in Eq. (10). We show in the following two lemmas that
the approximate
formulation in Eq. (13) is convex and has a guaranteed approximation bound controlled by µ.
Lemma 3.2. The problem in Eq. (13) is a convex optimization problem.

fµ (Z ),

(13)

min
j=1 ,{vj }p
Z,{uj }p
j=1

Proof. The optimization problem in Eq. (13) can be expressed equivalently as
i hi!
µ log 
p
exp  uj + vj −
Xj=1
z T


k
k
1
Xi=1
Xi=1
z T
µuj ≥
i Kj zi , j = 1, · · · , p.
4
Since the log-exponential-sum function is a convex function and the two constraints are second-order
cone constraints, the problem in Eq. (13) is a convex optimization problem.

k
Xi=1
1
4λ

z T
i zi , µvj ≥

subject to

(14)

Lemma 3.3. Let f (Z ) and fµ (Z ) be de ﬁned as above. Then we have fµ (Z ) ≥ f (Z ) and |fµ (Z ) −
f (Z )| ≤ µ log p.
Proof. The term − Pp
j=1 θj log θj de ﬁnes the entropy of {θj }p
j=1 when it is considered as a proba-
bility distribution, since θ ≥ 0 and θT e = 1. Hence, this term is non-negative and fµ (Z ) ≥ f (Z ). It
is known from the property of entropy that − Pp
j=1 θj log θj is maximized with a uniform {θj }p
j=1 ,
p for j = 1, · · · , p. Thus, we have − Pp
i.e., θj = 1
j=1 θj log θj ≤ log p and |fµ (Z ) − f (Z )| =
−µ Pp
j=1 θj log θj ≤ µ log p. This completes the proof of the lemma.

4 Solving the Approximate Formulation Using the Nesterov’s Method

The Nesterov’s method (known as “the optimal method ” in [5])
is an algorithm for solving smooth
convex problems with the optimal rate of convergence. In this method, the objective function needs
to be differentiable with Lipschitz continuous gradient. In order to apply this method to solve the
proposed approximate formulation, we ﬁrst compute the Lips chitz constant for the gradient of func-
tion fµ (Z ), as summarized in the following lemma:
Lemma 4.1. Let fµ (Z ) be de ﬁned as in Eq. (10). Then the Lipschitz constant L of the gradient of
fµ (Z ) can be bounded from above as

where Lµ is de ﬁned as

L ≤ Lµ ,

(15)

(16)

(17)

(18)

, (19)

1
λ

2

,

Lµ =

+

max
1≤j≤p

λmax (Kj ) +

tr(Z T Z ) max
1≤i,j≤p

Kj (cid:21) Cj (cid:19)!

λmax ((Ki − Kj )(Ki − Kj )T ),

1
1
1
8µλ2
2
2λ
and λmax (·) denotes the maximum eigenvalue. Moreover, the distance from the origin to the optimal
set of Z can be bounded as tr(Z T Z ) ≤ R2
µ where R2
µ is de ﬁned as
Xi=1  ||[Cj ]i ||2 + s4µ log p + tr (cid:18)C T
k
j (cid:20)I +
H and [Cj ]i denotes the ith column of Cj .

R2
µ =
λ Kj (cid:1)−1
Cj = 2 (cid:0)I + 1
Proof. To compute the Lipschitz constant for the gradient of fµ (Z ), we ﬁrst compute the ﬁrst and
second order derivatives as follows:
p
gj (cid:18) vec(Z )
Xj=1
2
p
1
gj
Xj=1
Dk (Kj )
I +
2
2λ
p
(cid:19)T
(cid:19) (cid:18) vec(KiZ )
gigj (cid:18) vec(KiZ )
Xi,j=1
λ
λ
where vec(·) converts a matrix into a vector, Dk (Kj ) ∈ R(n×k)×(n×k) is a block diagonal matrix
with the k th diagonal block as Kj , and gj = exp(Sj (Z )/µ)/ Pp
i=1 exp(Si (Z )/µ). Then we have
1
1
1
tr(Z T (Ki − Kj )(Ki − Kj )T Z ) ≤ Lµ .
λmax (Kj ) +
+
max
8µλ2 max
L ≤
2
2λ
1≤j≤p
1≤i,j≤p
where Lµ is de ﬁned in Eq. (16).

(cid:19) − vec(H ),

vec(Kj Z )
2λ

vec(Kj Z )
λ

▽2 fµ (Z ) =

−

vec(Kj Z )
λ

+

1
8µ

▽fµ (Z ) =

+

−

1
4

1
λ

Sj (Z ) =

We next derive the upper bound for tr(Z T Z ). To this end, we ﬁrst rewrite Sj (Z ) as
j (cid:20)I +
tr (cid:18)(Z − Cj )T (cid:20)I +
Kj (cid:21) Cj (cid:19) .
tr (cid:18)C T
Kj (cid:21) (Z − Cj )(cid:19) −
Since min fµ (Z ) ≤ fµ (0) = µ log p, and fµ (Z ) ≥ Sj (Z ), we have Sj (Z ) ≤ µ log p for j =
4 tr (cid:0)(Z − Cj )T (Z − Cj )(cid:1) ≤ µ log p + 1
j (cid:2)I + 1
1, · · · , p. It follows that 1
4 tr (cid:0)C T
λ Kj (cid:3) Cj (cid:1) . By using
this inequality, it can be veriﬁed that tr (Z T Z ) ≤ R2
µ where R2
µ is de ﬁned in Eq. (17).
The Nesterov’s method for solving the proposed approximate formulation is presented in Table 1.
After the optimal Z is obtained from the Nesterov’s method, the optimal {θj }p
j=1 can be computed
from Eq. (12).
It follows from the convergence proof in [5] that after N iterations, as long as
fµ (X i ) ≤ fµ (X 0) for i = 1, · · · , N , we have

1
λ

1
4

fµ (Z N +1 ) − fµ (Z ∗ ) ≤

4LµR2
µ
(N + 1)2 ,

(20)

Table 1: The Nesterov’s method for solving the proposed multi-label MKL formulation.
• Initialize X 0 = Z 1 = Q0 = 0 ∈ Rn×k , t0 = 1, L0 = 1
2 + 1
2λ max1≤j≤p λmax (Kj ), and
µ = 1
N where N is the prede ﬁned number of iterations
• for i = 1, · · · , N do
• Set X i = Z i − 1
(Z i + Qi−1 )
ti−1
• Compute fµ (X i ) and ▽fµ (X i )
• Set L = Li−1
2L tr((▽fµ (X i ))T ▽fµ (X i )) do
• while fµ (X i − ▽fµ (X i )/L) > fµ (X i ) − 1
• L = L × 2
• end while
• Set Li = L
▽fµ (X i ), Qi = Qi−1 + ti−1
• Set Z i+1 = X i − 1
Li
Li
2 (cid:16)1 + q1 + 4t2
i−1(cid:17)
• Set ti = 1
• end for

▽fµ (X i )

where Z ∗ = arg minZ fµ (Z ). Furthermore, since fµ (Z N +1 ) ≥ f (Z N +1 ) and fµ (Z ∗ ) ≤ f (Z ∗) +
µ log p, we have

f (Z N +1) − f (Z ∗ ) ≤ µ log p +

4LµR2
µ
(N + 1)2 .
By setting µ = O(1/N ), we have that Lµ ∝ O(1/µ) ∝ O(N ). Hence, the convergence rate of the
Nesterov’s method is on the order of O(1/N ). This is signiﬁcantly better than the convergence rates
of O(1/N 1/3 ) and O(1/N 1/2 ) for the SILP and the gradient descent method, respectively.

(21)

5 Experiments

In this section, we evaluate the proposed formulation on the automated annotation of gene expression
pattern images. The performance of the approximate formulation is also validated.
Experimental Setup The experiments use a collection of gene expression pattern images retrieved
from the FlyExpress database (http://www.flyexpress.net). We apply nine local descrip-
tors (SIFT, shape context, PCA-SIFT, spin image, steerable ﬁlters, differential invariants, complex
ﬁlters, moment invariants, and cross correlation) on regul ar grids of 16 and 32 pixels in radius and
spacing on each image. These local descriptors are commonly used in computer vision problems
[15]. We also apply Gabor ﬁlters with different wavelet scal es and ﬁlter orientations on each image
to obtain global features of 384 and 2592 dimensions. Moreover, we sample the pixel values of each
image to obtain features of 10240, 2560, and 640 dimensions. After generating the features, we
apply the vocabulary-guided pyramid match algorithm [9] to construct kernels between the image
sets. A total of 23 kernel matrices (2 grid size × 9 local descriptors + 2 Gabor + 3 pixel) are con-
structed. Then the proposed MKL formulation is employed to obtain the optimal integrated kernel
matrix based on which the low-dimensional embedding is computed. We use the expansion-based
approach (star and clique) to construct the hypergraph Laplacian, since it has been shown [1] that
the Laplacians constructed in this way are similar to those obtained directly from a hypergraph. The
performance of kernel matrices (either single or integrated) is evaluated by applying the support
vector machine (SVM) for each term using the one-against-rest scheme. The F1 score is used as
the performance measure, and both macro-averaged and micro-averaged F1 scores across labels are
reported. In each case, the entire data set is randomly partitioned into training and test sets with a
ratio of 1:1. This process is repeated ten times, and the averaged performance is reported.
Performance Evaluation It can be observed from Tables 2 and 3 that in terms of both macro and
micro F1 scores, the kernels integrated by either star or clique expansions achieve the highest per-
formance on almost all of the data sets. In particular, the integrated kernels outperform the best
individual kernel signiﬁcantly on all data sets. This shows
that the proposed formulation is effective

Table 2: Performance of integrated kernels and the best individual kernel (denoted as BIK) in terms
of macro F1 score. The number of terms used are 20, 30, and 40, and the number of image sets
used are 1000, 1500, and 2000. “SILP”, “APP”, “SVM1 ”, and “Un
iform ” denote the performance
of kernels combined with the SILP formulation, the approximate formulation, the 1-norm SVM for-
mulation proposed in [12] applied for each label separately, and the case where all kernels are given
the same weight, respectively. The subscripts “star ” and “c
lique ” denote the way that Laplacian is
constructed, and “KCCA” denotes the case where C = Y T (Y Y T )−1 Y .
30
20
# of labels
# of sets
1500
1500
0.4437
0.4903
SILPstar
SILPclique
0.4747
0.5125
0.4240
0.4635
SILPKCCA
APPstar
0.4930
0.4494
0.4741
0.5125
APPclique
0.4313
0.4805
APPKCCA
0.4352
0.4640
SVM1
Uniform
0.4703
0.4410
0.4312
0.4515
BIK

40
1500
0.4019
0.4346
0.3872
0.4100
0.4338
0.3914
0.4048
0.4111
0.3954

1000
0.4396
0.4536
0.3987
0.4404
0.4510
0.4029
0.3780
0.3727
0.4241

1000
0.3852
0.4065
0.3497
0.3896
0.4060
0.3571
0.3523
0.3513
0.3782

1000
0.3768
0.4145
0.3538
0.3900
0.4180
0.3642
0.3741
0.3719
0.3914

2000
0.4162
0.4563
0.4063
0.4267
0.4563
0.4146
0.4200
0.4191
0.3996

2000
0.4575
0.4926
0.4477
0.4703
0.4917
0.4586
0.4356
0.4480
0.4344

2000
0.3927
0.4283
0.3759
0.3983
0.4281
0.3841
0.3955
0.3986
0.3827

Table 3: Performance in terms of micro F1 score. See the caption of Table 2 for explanations.
40
30
20
# of labels
1500
1500
1500
# of sets
0.4470
0.4837
0.5199
SILPstar
0.4796
0.5127
0.5422
SILPclique
SILPKCCA
0.4994
0.4737
0.4420
0.4541
0.4875
0.5211
APPstar
0.4793
0.5124
0.5421
APPclique
0.4488
0.4828
0.5174
APPKCCA
SVM1
0.5024
0.4844
0.4234
0.4358
0.4939
0.5096
Uniform
BIK
0.4735
0.4484
0.3905

1000
0.4861
0.5039
0.4581
0.4852
0.5013
0.4612
0.4361
0.4390
0.4614

2000
0.4473
0.4894
0.4532
0.4582
0.4894
0.4605
0.4632
0.4683
0.4178

1000
0.4472
0.4682
0.4209
0.4484
0.4673
0.4299
0.4239
0.4242
0.4189

1000
0.4277
0.4610
0.4095
0.4355
0.4633
0.4194
0.3947
0.3999
0.3869

2000
0.4847
0.5247
0.4887
0.4973
0.5239
0.5018
0.4844
0.4975
0.4562

2000
0.4305
0.4660
0.4271
0.4346
0.4658
0.4350
0.4188
0.4226
0.3781

in combining multiple kernels and exploiting the complementary information contained in different
kernels constructed from various features. Moreover, the proposed formulation based on a hyper-
graph outperforms the classical KCCA consistently.
SILP versus the Approximate Formulation In terms of classiﬁcation performance, we can observe
from Tables 2 and 3 that the SILP and the approximate formulations are similar. More precisely,
the approximate formulations perform slightly better than SILP in almost all cases. This may be
due to the smoothness nature of the formulations and the simplicity of the computational procedure
employed in the Nesterov’s method so that it is less prone to numerical problems. Figure 1 compares
the computation time and the kernel weights of SILPstar and APPstar . It can be observed that in
general the approximate formulation is signiﬁcantly faste r than SILP, especially when the number
of labels and the number of image sets are large, while they both yields very similar kernel weights.

6 Conclusions and Future Work

We present a multi-label learning formulation that incorporates instance-label correlations by a hy-
pergraph. We formulate the problem in the kernel-induced feature space and propose to learn the
kernel matrix in the MKL framework. The resulting formulation leads to a non-smooth min-max
problem, and it can be cast as an SILP. We propose an approximate formulation by introducing a
smoothing term and show that the resulting formulation is an unconstrained convex problem that can
be solved by the Nesterov’s method. We demonstrate the effectiveness and efﬁciency of the method
on the task of automated annotation of gene expression pattern images.

)
s
d
n
o
c
e
s
 
n
i
(
 
e
m
i
t
 
n
o
i
t
a
t
u
p
m
o
C

400

350

300

250

200

150

100

50

0

 

SILP
star

APP
star

20(1000) 20(1500) 20(2000) 30(1000) 30(1500) 30(2000) 40(1000) 40(1500) 40(2000)
The number of labels and the number of image sets

 

s
l
e
n
r
e
k
 
r
o
f
 
t
h
g
i
e
W

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

 

SILPstar
APPstar

 

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Kernel number

(b) Comparison of kernel weights
(a) Comparison of computation time
Figure 1: Comparison of computation time and kernel weights for SILPstar and APPstar . The left
panel plots the computation time of two formulations on one partition of the data set as the number
of labels and image sets increase gradually, and the right panel plots the weights assigned to each of
the 23 kernels by SILPstar and APPstar on a data set of 40 labels and 1000 image sets.

The experiments in this paper focus on the annotation of gene expression pattern images. The
proposed formulation can also be applied to the task of multiple object recognition in computer
vision. We plan to pursue other applications in the future. Experimental results indicate that the
best individual kernel may not lead to a large weight by the proposed MKL formulation. We plan to
perform a detailed analysis of the weights in the future.

Acknowledgements
This work is supported in part by research grants from National Institutes of Health (HG002516 and
1R01-GM079688-01) and National Science Foundation (IIS-0612069 and IIS-0643494).

References
[1] S. Agarwal, K. Branson, and S. Belongie. Higher order learning with graphs. In ICML, pages 17–24,
2006.
[2] D. Zhou, J. Huang, and B. Sch ¨olkopf. Learning with hypergraphs: Clustering, classi ﬁcation, and embed-
ding. In NIPS, pages 1601–1608. 2007.
[3] Z. H. Zhou and M. L. Zhang. Multi-instance multi-label learning with application to scene classi ﬁcation.
In NIPS, pages 1609–1616. 2007.
[4] D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-taylor. Canonical correlation analysis: An overview with
application to learning methods. Neural Computation, 16(12):2639–2664, 2004.
[5] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2003.
[6] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127–
152, 2005.
[7] P. Tomancak and et al. Systematic determination of patterns of gene expression during Drosophila em-
bryogenesis. Genome Biology, 3(12), 2002.
[8] S. Ji, L. Sun, R. Jin, S. Kumar, and J. Ye. Automated annotation of Drosophila gene expression patterns
using a controlled vocabulary. Bioinformatics, 24(17):1881–1888, 2008.
[9] K. Grauman and T. Darrell. Approximate correspondences in high dimensions. In NIPS, pages 505–512.
2006.
[10] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[11] S. Sch ¨olkopf and A. Smola. Learning with Kernels: Support Vector Machines,Regularization, Optimiza-
tion and Beyond. MIT Press, 2002.
[12] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semideﬁnite programming.
Journal of Machine Learning Research, 5:27–72, 2004.
[13] R. Hettich and K. O. Kortanek. Semi-inﬁnite programmin g: Theory, methods, and applications. SIAM
Review, 35(3):380–429, 1993.
[14] S. Sonnenburg, G. R ¨atsch, C. Sch¨afer, and B. Sch ¨olkopf. Large scale multiple kernel learning. Journal of
Machine Learning Research, 7:1531–1565, July 2006.
[15] K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 27(10):1615–1630, 2005.


Algorithms for In ﬁnitely Many-Armed Bandits

Yizao Wang∗
Department of Statistics - University of Michigan
437 West Hall, 1085 South University, Ann Arbor, MI, 48109-1107, USA
yizwang@umich.edu

Jean-Yves Audibert
Université Paris Est, Ecole des Ponts, ParisTech, Certis
& Willow - ENS / INRIA, Paris, France
audibert@certis.enpc.fr

Rémi Munos
INRIA Lille - Nord Europe, SequeL project,
40 avenue Halley, 59650 Villeneuve d’Ascq, France
remi.munos@inria.fr

Abstract

We consider multi-armed bandit problems where the number of arms is larger
than the possible number of experiments. We make a stochastic assumption on
the mean-reward of a new selected arm which characterizes its probability of be-
ing a near-optimal arm. Our assumption is weaker than in previous works. We
describe algorithms based on upper-conﬁdence-bounds appl
ied to a restricted set
of randomly selected arms and provide upper-bounds on the resulting expected
regret. We also derive a lower-bound which matches (up to a logarithmic factor)
the upper-bound in some cases.

1

Introduction

Multi-armed bandit problems describe typical situations where learning and optimization should be
balanced in order to achieve good cumulative performances. Usual multi-armed bandit problems
(see e.g. [8]) consider a ﬁnite number of possible actions (o r arms) from which the learner may
choose at each iteration. The number of arms is typically much smaller than the number of ex-
periments allowed, so exploration of all possible options is usually performed and combined with
exploitation of the apparently best ones.

In this paper, we investigate the case when the number of arms is inﬁnite (or larger than the available
number of experiments), which makes the exploration of all the arms an impossible task to achieve:
if no additional assumption is made, it may be arbitrarily hard to ﬁnd a near-optimal arm. Here we
consider a stochastic assumption on the mean-reward of any new selected arm. When a new arm
k is pulled, its mean-reward µk is assumed to be an independent sample from a ﬁxed distributi on.
Moreover, given the mean-reward µk for any arm k , the distribution of the reward is only required
to be uniformly bounded and non-negative without any further assumption. Our assumptions essen-
tially characterize the probability of pulling near-optimal arms. That is, given µ∗ ∈ [0, 1] as the best
possible mean-reward and β ≥ 0 a parameter of the mean-reward distribution, the probability that a
new arm is -optimal is of order β for small , i.e. P(µk ≥ µ∗ − ) = Θ(β ) for  → 0. Note that we
write f () = Θ(g()) for  → 0 when ∃c1 , c2 , 0 > 0 such that ∀ ≤ 0 , c1 g() ≤ f () ≤ c2 g().
∗The major part of this work was completed during the research internship at Certis and INRIA SequeL.

1

Like in multi-armed bandits, this setting exhibits a trade off between exploitation (selection of the
arms that are believed to perform well) and exploration. The exploration takes two forms here:
discovery (pulling a new arm that has never been tried before) and sampling (pulling an arm already
discovered in order to gain information about its actual mean-reward).

Numerous applications can be found e.g.
It includes labor markets (a worker has many
in [5].
opportunities for jobs), mining for valuable resources (such as gold or oil) when there are many
areas available for exploration (the miner can move to another location or continue in the same
location, depending on results), and path planning under uncertainty in which the path planner has
to decide among a route that has proved to be efﬁcient in the pa st (exploitation), or a known route
that has not been explored many times (sampling), or a brand new route that has never been tried
before (discovery).
Let us write kt the arm selected by our algorithm at time t. We deﬁne the regret up to time n as
Rn = nµ∗ − Pn
t=1 µkt . From the tower rule, ERn is the expectation of the difference between
the rewards we would have obtained by drawing an optimal arm (an arm having a mean-reward
equal to µ∗ ) and the rewards we did obtain during the time steps 1, . . . , n. Our goal is to design an
arm-pulling strategy such as to minimize this regret.
Overview of our results: We write vn = ˜O(un ) when for some n0 , C > 0, vn ≤ C un (log(un ))2 ,
for all n ≥ n0 . We assume that the rewards of the arms lie in [0, 1]. Our regret bounds depend on
whether µ∗ = 1 or µ∗ < 1. For µ∗ = 1, our algorithms are such that ERn = ˜O(nβ/(1+β ) ). For
µ∗ < 1, we have ERn = ˜O(nβ/(1+β ) ) if β > 1, and (only) ERn = ˜O(n1/2 ) if β ≤ 1. Moreover
we derive the lower-bound: for any β > 0, µ∗ ≤ 1, any algorithm satis ﬁes ERn ≥ C nβ/(1+β ) for
some C > 0. Finally we propose an algorithm having the anytime property, which is based on an
arm-increasing rule.

Our algorithms essentially consist in pulling K different arms randomly chosen, where K is of
order nβ/2 if µ∗ < 1 and β < 1, and nβ/(1+β ) otherwise, and using a variant of the UCB (Upper
Conﬁdence Bound) algorithm ([3],[2]) on this set of K arms, which takes into account the empirical
variance of the rewards. This last point is crucial to get the proposed rate for µ∗ = 1 and β < 1, i.e.
in cases where there are many arms with small variance.
Previous works on many-armed bandits: In [5], a speci ﬁc setting of an inﬁnitely many-armed
bandit is considered, namely that the rewards are Bernoulli random variables with parameter p,
where p follows a uniform law over a given interval [0, µ∗ ]. All mean-rewards are therefore in
[0, µ∗ ]. They proposed three algorithms. (1) The 1-failure strategy where an arm is played as long
as 1s are received. When a 0 is received, a new arm is played and this strategy is repeated forever.
(2) The m-run strategy uses the 1-failure strategy until either m continuous 1s are received (from the
same arm) or m different arms have been played. In the ﬁrst case, we continu e to play forever the
current arm. In the second case, the arm that gave the most wins is chosen to play for the remaining
rounds. Finally, (3) the m-learning strategy uses the 1-failure strategy during the ﬁrst m rounds, and
for the remaining rounds it chooses the arm that gave the most 1s during the ﬁrst m rounds.
For µ∗ = 1, the authors of [5] have shown that 1-failure strategy, √n-run strategy, and log(n)√n-
learning strategy have a regret ERn ≤ 2√n. They also provided a lower bound on the regret of any
strategy: ERn ≥ √2n. For µ∗ < 1, the corresponding optimal strategies are √nµ∗ -run strategy
and √nµ∗ log(nµ∗ )-learning strategy. All these algorithms require the knowledge of the horizon n
of the game. In many applications, it is important to design algorithms having the anytime property,
that is, the upper bounds on the expected regret ERn have the similar order for all n. Under the
same Bernoulli assumption on the reward distributions, such algorithms has been obtained in [9].

In comparison to their setting (uniform distribution corresponds to β = 1), our upper- and lower-
bounds are also of order √n up to a logarithmic factor, and we do not assume that we know exactly
the distribution of the mean-reward. However it is worth noting that the proposed algorithms in
[5, 9] heavily depend on the Bernoulli assumption of the rewards and are not easily transposable to
general distributions. Note also that the Bernoulli assumption does not work for the real problems
mentioned above, where the outcomes may take several possible values.

Thus an important aspect of our work, compared to previous many-armed bandits, is that our setting
allows general reward distributions for the arms, under a simple assumption on the mean-reward.

2

2 Main results

In our framework, each arm of a bandit is characterized by the distribution of the rewards (obtained
by drawing that arm) and the essential parameter of the distribution of rewards is its expectation.
Another parameter of interest is the standard deviation. With low variance, poor arms will be easier
to spot while good arms will have higher probability of not being disregarded at the beginning due
to unlucky trials. To draw an arm is equivalent to draw a distribution ν of mean-rewards. Let
µ = R wν (dw) and σ2 = R (w − µ)2 ν (dw) denote the expectation and variance of ν . The quantities
µ and σ are random variables. Our assumptions are the following:
(A) Rewards are uniformly bounded: without loss of generality, we assume all rewards are in [0, 1].
(B) the expected reward of a randomly drawn arm satis ﬁes: there e xist µ∗ ∈ (0, 1] and β > 0 s.t.
(1)
P{µ > µ∗ − } = Θ(β ), for  → 0
(C) there is a function V : [0, 1] → R such that P{σ2 ≤ V (µ∗ − µ)} = 1.
The key assumption here is (B). It gives us (the order of) the number of arms that needs to be drawn
before ﬁnding an arm that is -close to the optimum1 (i.e., an arm for which µ ≥ µ∗ −). Assumption
(B) implies that there exists positive constants c1 and c2 such that for any  ∈ [0, µ∗ ], we have2
c1 β ≤ P{µ > µ∗ − } ≤ P{µ ≥ µ∗ − } ≤ c2 β .
For example, the uniform distribution on (0, µ∗ ) satis ﬁes the Condition (1) with β = 1.
Assumption (C) always holds for V (u) = µ∗ (1 − µ∗ + u) (since Var W ≤ EW (1 − EW ) when
W ∈ [0, 1]). However it is convenient when the near-optimal arms have low variance (for instance,
this happens when µ∗ = 1).
Let Xk,1 , Xk,2 , . . . denote the rewards obtained when pulling arm k . These are i.i.d.
random
s Ps
variables with common expected value denoted µk . Let X k,s , 1
j=1 Xk,j and Vk,s ,
s Ps
1
j=1 (Xk,j − X k,s )2 be the empirical mean and variance associated with the ﬁrst s draws of
arm k . Let Tk (t) denote the number of times arm k is chosen by the policy during the ﬁrst
t plays.
We will use as a subroutine of our algorithms the following version of UCB (Upper Conﬁdence
Bound) algorithm as introduced in [2]. Let (Et )t≥0 be a nondecreasing sequence of nonnegative
real numbers. It will be referred to as the exploration sequence since the larger it is, the more UCB
explores. For any arm k and nonnegative integers s, t, introduce
Bk,s,t , X k,s + r 2Vk,sEt
3Et
s
s
with the convention 1/0 = +∞. Deﬁne the UCB-V (for Variance estimate) policy:
UCB-V policy for a set K of arms:
At time t, play an arm in K maximizing Bk,Tk (t−1),t .

(3)

(2)

+

From [2, Theorem 1], the main property of Bk,s,t is that with probability at least 1 − 5(log t)e−Et /2 ,
for any s ∈ [0, t] we have µk ≤ Bk,s,t . So provided that Et is large, Bk,Tk (t−1),t is an observable
quantity at time t which upper bounds µk with high probability. We consider nondecreasing se-
quence (Et ) in order that these bounds hold with probability increasing with time. This ensures that
the low probability event, that the algorithm might concentrate the draws on suboptimal arms, has a
decreasing probability with time.

2.1 UCB revisited for the inﬁnitely many-armed bandit

When the number of arms of the bandit is greater than the total number of plays, it makes no sense
to apply UCB-V algorithm (or other variants of UCB [3]) since its ﬁrst step is to draw each arm once
(to have Bk,Tk (t−1),t ﬁnite). A more meaningful and natural approach is to decide a t the beginning

1Precise computations lead to a number which is of order 
−β up to possibly a logarithmic factor.
2 Indeed, (1) implies that for some 0 < c
∗ such that for any  ≤ 0 ,
2 , there exists 0 < 0 < µ
0
0
1 < c
−β
β
. Then one may take c1 = c
0 and c2 = max(
2 ).
β ≤ P{µ > µ
β
0
∗ − } ≤ c
∗ − } ≤ P{µ ≥ µ
0
0
1 
0 , c
2 

0
c
1 

3

that only K arms will be investigated in the entire experiment. The K should be sufﬁciently small
with respect to n (the total number of plays), as in this way we have fewer plays on bad arms and
most of the plays will be on the best of K arms. The number K should not be too small either, since
we want that the best of the K arms has an expected reward close to the best possible arm.
It is shown in [2, Theorem 4] that in the multi-armed bandit, taking a too small exploration se-
quence (e.g. such as Et ≤ 1
2 log t) might lead to polynomial regret (instead of logarithmic for e.g.
Et = 2 log t) in a simple 2-armed bandit problem. However, we will show that this is not the case
in the inﬁnitely many-armed bandit, where one may (and shoul d) take much smaller exploration
sequences (typically of order log log t). The reason for this phenomenon is that in this setting, there
are typically many near-optimal arms so that the subroutine UCB-V may miss some good arms (by
unlucky trials) without being hurt: there are many other near-optimal arms to discover! This illus-
trates a trade off between the two aspects of exploration: sample the current, not well-known, arms
or discover new arms.
We will start our analysis by considering the following UCB-V(∞) algorithm:
UCB-V(∞) algorithm: Given parameters K and the exploration sequence (Et )
• Randomly choose K arms,
• Run the UCB-V policy on the set of the K selected arms.
Theorem 1 If the exploration sequence satis ﬁes 2 log(10 log t) ≤ Et ≤ log t, then for n ≥ 2 and
K ≥ 2 the expected regret of the UCB-V(∞) algorithm satis ﬁes:
ERn ≤ C n(log K )nK −1/β + K (log n)Eh(cid:0) V (∆)
∆ + 1(cid:1) ∧ (n∆)io,
where ∆ = µ∗ − µ with µ the random variable corresponding to the expected reward of a sampled
arm from the pool, and where C is a positive constant depending only on c1 and β (see (2)).
Proof: The UCB-V(∞) algorithm has two steps: randomly choose K arms and run a UCB sub-
routine on the selected arms. The ﬁrst part of the proof studi es what happens during the UCB
subroutine, that is, conditionally to the arms that have been randomly chosen during the ﬁrst step
of the algorithm. In particular we consider in the following that µ1 , . . . , µK are ﬁxed. From the
equality (obtained using Wald’s theorem):
ERn = PK
(5)
E{Tk (n)}∆k
k=1
with ∆k = µ∗ − µk , it sufﬁces to bound ETk (n). The proof is inspired from the ones of Theorems
2 and 3 in [2]. The novelty of the following lemma is to include the product of probabilities in the
last term of the right-hand-side. This enables us to incorporate the idea that if there are a lot of
near-optimal arms, it is very unlikely that suboptimal arms are often drawn.

(4)

Lemma 1 For any real number τ and any positive integer u, we have
ETk (n) ≤ u + Pn
t=u+1 Pt
P(cid:0)Bk,s,t > τ (cid:1) + Pn
P(∃s0 ∈ [0, t], Bk0 ,s0 ,t ≤ τ (cid:1)(6)
t=u+1 Qk0 6=k
s=u
where the expectations and probabilities are conditioned on the set of selected arms.
Proof: We have Tk (n) − u ≤ Pn
t=u+1 Zk (u, t) where Zk (u, t) = 1It=k;Tk (t)>u . We have
Zk (u, t) ≤ 1
∀k0 6=k Bk,Tk (t−1),t≥Bk0 ,T
∃s∈[u,t] Bk,s,t>τ + 1
≤ 1
∀k0 6=k ∃s0∈[0,t] Bk0 ,s0 ,t≤τ
where the last inequality holds since if the two terms in the last sum are equal to zero, then it implies
that there exists k 0 6= k such that for any s0 ∈ [0, t] and any s ∈ [u, t], Bk0 ,s0 ,t > τ ≥ Bk,s,t . Taking
the expectation of both sides, using a union bound and the independence between rewards obtained
from different arms, we obtain Lemma 1. (cid:3)

k0 (t−1),t ;Tk (t−1)≥u

Now we use Inequality (6) with τ = µ∗+µk
2 = µ∗ − ∆k
2 = µk + ∆k
2 , and u the smallest integer
larger than 32(cid:16) σ2
∆k (cid:17) log n. These choices are made to ensure that the probabilities in the r.h.s.
+ 1
k
∆2
k
4

+ 3

. Thus:

of (6) are small. Precisely, for any s ≥ u and t ≤ n, we have
s ≤ r [2σ2
r 2[σ2
k + ∆k /2] log n
k + ∆k /4]Et
+ 3 Et
log n
s
u
u
4 (cid:20)r σ2
≤ r [2σ2
k+∆k (cid:21) ≤ ∆k
k+∆k ] + 3∆2
k+∆k /2]∆2
k+∆k /4
∆k
k+∆k ] = ∆k
+ 3
4 ,
k
k
σ2
σ2
32[σ2
32[σ2
8
k+∆k
where the last inequality holds since it is equivalent to (x − 1)2 ≥ 0 for x = r σ2
k+∆k /4
σ2
k+∆k
s + 3 Et
P(Bk,s,t > τ ) ≤ P(cid:0)X k,s + q 2Vk,s Et
> µk + ∆k /2(cid:1)
s
≤ P(cid:0)X k,s + q 2[σ2
k+∆k /4]Et
+ 3 Et
s > µk + ∆k /2(cid:1) + P(cid:0)Vk,s ≥ σ2
k + ∆k /4(cid:1)
s
j=1 (Xk,j −µk )2
≤ P(cid:0)X k,s − µk > ∆k /4(cid:1) + P(cid:16) P s
k ≥ ∆k /4(cid:17)
− σ2
s
≤ 2e−s∆2
k /(32σ2
k+8∆k /3) ,
where in the last step we used Bernstein’s inequality twice. Summing up we obtain
t
∞
k /(32σ2
e−u∆2
k+8∆k /3)
e−s∆2
k /(32σ2
Xs=u
Xs=u
k+8∆k /3) = 2
P(Bk,s,t > τ ) ≤ 2
1 − e−∆2
k /(32σ2
k+8∆k /3)
k+8∆k /3) ≤ (cid:16) 80σ2
≤ (cid:16) 80σ2
∆k (cid:17) e−u∆2
∆k (cid:17) n−1 ,
k /(32σ2
+ 7
+ 7
(8)
k
k
∆2
∆2
k
k
where we have used that 1 − e−x ≥ 4x/5 for 0 ≤ x ≤ 3/8. Now let us bound the product of
probabilities in (6). Since τ = µ∗ − ∆k /2, we have
Yk0 6=k
Yk0 :µk0 >µ∗−∆k /2
P(∃s ∈ [0, t], Bk0 ,s,t < µ0
P(∃s ∈ [0, t], Bk0 ,s,t ≤ τ (cid:1) ≤
k (cid:1) .
Now from [2, Theorem 1], with probability at least 1 − 5(log t)e−Et /2 , for any s ∈ [0, t] we have
µk ≤ Bk,s,t . For Et ≥ 2 log(10 log t), this gives P(∃s ∈ [0, t], Bk0 ,s,t < µ0
k (cid:1) ≤ 1/2. Putting all
the bounds of the different terms of (6) leads to
ETk (n) ≤ 1 + 32 (cid:18) σ2
∆k (cid:19) log n + (cid:18) 80σ2
∆k (cid:19) + n2−N∆k ,
7
1
k
k
+
+
∆2
∆2
k
k
with N∆k the cardinal of (cid:8)k 0 ∈ {1, . . . , K } : µk0 > a − ∆k /2(cid:9). Since ∆k ≤ µ∗ ≤ 1 and
Tk (n) ≤ n, the previous inequality can be simpli ﬁed into
ETk (n) ≤ nh50(cid:16) σ2
∆k (cid:17) log ni ∧ no + n2−N∆k ,
+ 1
(9)
k
∆2
k
Here, for the sake of simplicity, we are not interested in having tight constants. From here on, we
will take the expectations with respect to all sources of randomness, that is including the one coming
from the ﬁrst step of UCB-V( ∞). The quantities ∆1 , . . . , ∆K are i.i.d. random variables satisfying
0 ≤ ∆k ≤ µ∗ and P(∆k ≤ ) = Θ(β ). The quantities σ1 , . . . , σk are i.i.d. random variables
satisfying almost surely σ2
k ≤ V (∆k ). From (5) and (9), we have
+ 1(cid:17) log ni ∧ (n∆1 ) + n∆1 2−N∆1 (cid:27)
ERn = K E(cid:8)T1 (n)∆1(cid:9) ≤ K E(cid:26)h50(cid:16) V (∆1 )
∆1
Let p denote the probability that the expected reward µ of a randomly drawn arm satis ﬁes µ >
µ∗ − δ/2 for a given δ . Conditioning on ∆1 = δ , the quantity N∆1 follows a binomial distribution
with parameters K − 1 and p, hence E(2−N∆1 |∆1 = δ) = (1 − p + p/2)K−1 . By using (2), we get:
E(cid:8)∆1 2−N∆1 (cid:9) = E(cid:8)∆1 (1 − P(µ > µ∗ − ∆1 /2)/2)K−1(cid:9) ≤ Eχ(∆1 ),
with χ(u) = u(1 − c3uβ )K−1 and c3 = c1 /2β . We have χ0 (u) = (1 − c3uβ )K−2 (cid:2)1 − c3 (1 + (K −
1+(K−1)β )K−1
1
(1−
1
[c3 (1+(K−1)β )]1/β and χ(u0 ) =
1)β )uβ (cid:3) so that χ(u) ≤ χ(u0 ) with u0 =
[c3 (1+(K−1)β )]1/β ≤
C 0K −1/β for C 0 a positive constant depending only c1 and β . For any u1 ∈ [u0 , µ∗ ], we have
Eχ(∆1 ) ≤ χ(u0 )P(∆1 ≤ u1 ) + χ(u1 )P(∆1 > u1 ) ≤ χ(u0 )P(∆1 ≤ u1 ) + χ(u1 ) .
K (cid:1)1/β for C 00 a positive constant depending on c1 and β sufﬁciently large
Let us take u1 = C 00 (cid:0) log K
to ensure u1 ≥ u0 and χ(u1 ) ≤ K −1−1/β . We obtain Eχ(∆1 ) ≤ CK −1/β log K
K for an appropriate
constant C depending on c1 and β . Putting this into (10), we obtain the result of Theorem 1. (cid:3)

(7)

(10)

5

The r.h.s. of Inequality (4) contains two terms. The ﬁrst ter m is the bias: when we randomly draw
K arms, the expected reward of the best drawn arm is ˜O(K −1/β )-optimal. So the best algorithm,
˜O(nK −1/β ). The second term is the estimation. It
once the K arms are ﬁxed, will yield a regret
indicates the difference between the UCB subroutine’s performance and the best drawn arm.

2.2 Strategy for ﬁxed play number

Consider that we know in advance the total number of plays n and the value of β . In this case,
one can use the UCB-V(∞) algorithm with parameter K of order of the minimizer of the r.h.s. of
Inequality (4). This leads to the following UCB-F (for Fixed horizon) algorithm.

UCB-F ( ﬁxed horizon) : given total number of plays n, and parameters µ∗ and β of (1)
• Choose K arms with K of order ( n
β
if β < 1, µ∗ < 1
2
β
otherwise, i.e. if µ∗ = 1 or β ≥ 1
n
β+1
• Run the UCB-V algorithm with the K chosen arms and an exploration sequence satisfying
(11)
2 log(10 log t) ≤ Et ≤ log t
Theorem 2 For any n ≥ 2, the expected regret of the UCB-F algorithm satis ﬁes
C (log n)√n
if β < 1 and µ∗ < 1
ERn ≤ 
C (log n)2√n
if β = 1 and µ∗ < 1
β
otherwise, i.e. if µ∗ = 1 or β > 1
C (log n)n

1+β
with C a constant depending only on c1 , c2 and β (see (2)).
Proof: The result comes from Theorem 1 by bounding the expectation E = E(cid:2)(cid:0) V (∆)
∆ + 1(cid:1) ∧ (n∆)(cid:3).
First, as mentioned before, Assumption (C) is satis ﬁed for V (∆) = µ∗ (1 − µ∗ + ∆). So for µ∗ = 1
and this choice of function V , we have E ≤ 2. For µ∗ < 1, since ∆ ≤ µ∗ , we have E ≤ EΨ(∆)
with Ψ(t) = 2µ∗
t ∧ (nt). The function Ψ is continuous and differentiable by parts. Using Fubini’s
theorem and Inequality (2), we have
EΨ(∆) = Ψ(µ∗ ) − E R µ∗
∆ Ψ0 (t)dt = Ψ(µ∗ ) − R µ∗
0 Ψ0 (t)P(∆ ≤ t)dt
t2 c2 tβ dt ≤ 
2 + 2(1+β)/2 c2
1−β
if β < 1
n
2
1−β
≤ 2 + R 1√2/n
2
if β = 1
2 + c2 log(n/2)
2 + 2c2
if β > 1

β−1
Putting these bounds in Theorem 1, we get

2 o if β < 1 and µ∗ < 1
C n(log K )nK −1/β + (log n)K n
1−β
C n(log K )nK −1/β + (log n)2K o
if β = 1 and µ∗ < 1

C n(log K )nK −1/β + (log n)K o
otherwise: µ∗ = 1 or β > 1
with C a constant only depending on c1 , c2 and β . The number K of selected arms in UCB-F is
taken of the order of the minimizer of these bounds up to a logarithmic factor. (cid:3)
Theorem 2 makes no difference between a logarithmic exploration sequence and an iterated loga-
rithmic exploration sequence. However in practice, it is clearly better to take an iterated logarithmic
exploration sequence, for which the algorithm spends much less time on exploring all suboptimal
arms. For sake of simplicity, we have ﬁxed the constants in (1 1).
It is easy to check that for
Et = ζ logt and ζ ≥ 1, Inequality (12) still holds but with a constant C depending linearly in ζ .
Theorem 2 shows that when µ∗ = 1 or β ≥ 1, the bandit subroutine takes no time in spotting near-
optimal arms (the use of UCB-V algorithm using variance estimate is crucial for this), whereas for
β < 1 and µ∗ < 1, which means a lot of near-optimal arms with possibly high variances, the bandit
subroutine has difﬁculties in achieving low regret.

ERn ≤

(12)

.

The next theorem shows that our regret upper bounds are optimal up to logarithmic terms except for
the case β < 1 and µ∗ < 1. We do not know whether the rate O(nβ/2 log n) for β < 1 and µ∗ < 1
is improvable. This remains an open problem.

6

Theorem 3 For any β > 0 and µ∗ ≤ 1, any algorithm suffers a regret larger than cn
small enough constant c depending on c2 and β .

β
1+β for some

Sketch of proof. If we want to have a regret smaller than cnβ/(1+β ) we need that most draws are
done on an arm having an individual regret smaller than  = cn−1/(1+β ) . To ﬁnd such an arm, we
need to try a number of arms larger than C 0 −β = C 0 c−β nβ/(1+β ) arms for some C 0 > 0 depending
on c2 and β . Since these arms are drawn at least once and since most of these arms give a constant
regret, it leads to a regret larger than C 00 c−β nβ/(1+β ) with C 00 depending on c2 and β . For c small
enough, this contradicts that the regret is smaller than cnβ/(1+β ) . So it is not possible to improve on
the nβ/(1+β ) rate. (cid:3)

2.3 Strategy for unknown play number

To apply the UCB-F algorithm we need to know the total number of plays n and we choose the
corresponding K arms before starting. When n is unknown ahead of time, we propose here an
anytime algorithm with a simple and reasonable way of choosing K by adding a new arm from time
to time into the set of sampled arms. Let Kn denote the number of arms played up to time n. We
set K0 = 0. We deﬁne the UCB-AIR (for Arm-Increasing Rule):

β
2

β
β+1

UCB-AIR (Arm-Increasing Rule): given parameters µ∗ and β of (1),
• At time n, try a new arm if
Kn−1 < ( n
if β < 1 and µ∗ < 1
otherwise: µ∗ = 1 or β ≥ 1
n
• Otherwise apply UCB-V on Kn−1 drawn arms with an exploration sequence satisfying
2 log(10 log t) ≤ Et ≤ log t
This arm-increasing rule makes our algorithm applicable for the anytime problem. This is a more
reasonable approach in practice than restarting-based algorithms like the ones using the doubling
trick (see e.g. [4, Section 5.3]). Our second main result is to show that the UCB-AIR algorithm has
the same properties as the UCB-F algorithm (proof omitted from this extended abstract).
Theorem 4 For any horizon time n ≥ 2, the expected regret of the UCB-AIR algorithm satis ﬁes
ERn ≤ (cid:26) C (log n)2√n
if β < 1 and µ∗ < 1
β
C (log n)2n
otherwise, i.e. if µ∗ = 1 or β ≥ 1
1+β
with C a constant depending only on c1 , c2 and β (see (2)).

(13)

3 Comparison with continuum-armed bandits and conclusion

In continuum-armed bandits (see e.g. [1, 6, 4]), an inﬁnity o f arms is also considered. The arms
lie in some Euclidean (or metric) space and their mean-reward is a deterministic and smooth (e.g.
Lipschitz) function of the arms. This setting is different from ours since our assumption is stochastic
and does not consider regularities of the mean-reward w.r.t. the arms. However, if we choose an
arm-pulling strategy which consists in selecting randomly the arms, then our setting encompasses
continuum-armed bandits. For example, consider the domain [0, 1]d and a mean-reward function µ
assumed to be locally equivalent to a Hölder function (of ord er α ∈ [0, +∞)) around any maximum
x∗ (the number of maxima is assumed to be ﬁnite), i.e.
µ(x∗ ) − µ(x) = Θ(kx∗ − xkα ) when x → x∗ .
(14)
Pulling randomly an arm X according to the Lebesgue measure on [0, 1]d , we have: P(µ(X ) >
µ∗ − ) = Θ(P(kX − x∗ kα < )) = Θ(d/α ), for  → 0. Thus our assumption (1) holds with
β = d/α, and our results say that if µ∗ = 1, we have ERn = ˜O(nβ/(1+β ) ) = ˜O(nd/(α+d) ).
|µ(x)−µ(y)| ≤ c kx − ykα for 0 < α ≤ 1),
For d = 1, under the assumption that µ is α-Hölder (i.e.
[6] provides upper- and lower-bounds on the regret Rn = Θ(n(α+1)/(2α+1) ). Our results gives

7

ERn = ˜O(n1/(α+1) ) which is better for all values of α. The reason for this apparent contradiction
is that the lower bound in [6] is obtained by the construction of a very irregular function, which
actually does not satisfy our local assumption (14).

Now, under assumptions (14) for any α > 0 (around a ﬁnite set of maxima), [4] provides the rate
ERn = ˜O(√n). Our result gives the same rate when µ∗ < 1 but in the case µ∗ = 1 we obtain the
improved rate ERn = ˜O(n1/(α+1) ) which is better whenever α > 1 (because we are able to exploit
the low variance of the good arms). Note that like our algorithm, the algorithms in [4] as well as in
[6], do not make an explicit use (in the procedure) of the smoothness of the function. They just use
a ‘uniform’ discretization of the domain.

On the other hand, the zooming algorithm of [7] adapts to the smoothness of µ (more arms are
sampled at areas where µ is high). For any dimension d, they obtain ERn = ˜O(n(d0+1)/(d0+2) ),
where d0 ≤ d is their ’zooming dimension’. Under assumptions (14) we deduce d0 = α−1
α d using
the Euclidean distance as metric, thus their regret is ERn = ˜O(n(d(α−1)+α)/(d(α−1)+2α) ). For
locally quadratic functions (i.e. α = 2), their rate is ˜O(n(d+2)/(d+4) ), whereas ours is ˜O(nd/(2+d) ).
Again, we have a smaller regret although we do not use the smoothness of µ in our algorithm. Here
the reason is that the zooming algorithm does not make full use of the fact that the function is locally
quadratic (it considers a Lipschitz property only). However, in the case α < 1, our rates are worse
than algorithms speci ﬁcally designed for continuum armed b andits.

Hence, the comparison between the many-armed and continuum-armed bandits settings is not easy
because of the difference in nature of the basis assumptions. Our setting is an alternative to the
continuum-armed bandit setting which does not require the existence of an underlying metric space
in which the mean-reward function would be smooth. Our assumption (1) naturally deals with
possibly very complicated functions where maxima may be located in any part of the space. For the
continuum-armed bandit problems when there are relatively many near-optimal arms, our algorithm
will be also competitive compared to the speci ﬁcally design ed continuum-armed bandit algorithms.
This result matches the intuition that in such cases, a random selection strategy will perform well.

To conclude, our contributions are: (i) Compared to previous results on many-armed bandits, our
setting allows general mean-reward distributions for the arms, under a simple assumption on the
probability of pulling near-optimal arms. (ii) We show that, for inﬁnitely many-armed bandits, we
need much less exploration of each arm than for ﬁnite-armed b andits (the log term may be replaced
by log log). (iii) Our variant of UCB algorithm, making use of the variance estimate, enables to
obtain higher rates in cases when the variance of the near-optimal arms is small. (iv) We propose the
UCB-AIR algorithm, which is anytime, taking advantage of an arm-increasing rule. (v) We provide
a lower-bound matching the upper-bound (up to a logarithmic factor) in the case β ≥ 1 or µ∗ = 1.
References
[1] R. Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33:1926–1951,
1995.
[2] J.-Y. Audibert, R. Munos, and C. Szepesvári. Tuning bandit algorithms in stochastic environments. In
M. Hutter, R. A. Servedio, and E. Takimoto, editors, ALT, volume 4754 of Lecture Notes in Computer
Science, pages 150–165. Springer, 2007.
[3] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine
Learning, 47(2/3):235–256, 2002.
[4] P. Auer, R. Ortner, and C. Szepesvári. Improved rates for the stochastic continuum-armed bandit problem.
20th COLT, San Diego, CA, USA, 2007.
[5] D. A. Berry, R. W. Chen, A. Zame, D. C. Heath, and L. A. Shepp. Bandit problems with inﬁnitely many
arms. The Annals of Statistics, 25(5):2103–2116, 1997.
[6] R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In NIPS-2004, 2004.
[7] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandit problems in metric spaces. In Proceedings of
the 40th ACM Symposium on Theory of Computing, 2008.
[8] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathe-
matics, 6:4–22, 1985.
[9] O. Teytaud, S. Gelly, and M. Sebag. Anytime many-armed bandit. Conférence francophone sur
l’Apprentissage automatique (CAp) Grenoble, France, 2007.

8


 

 

B r e a k i n g  A u d i o   CA PTC HA s    

Jenn i f er   Tam 
Compu ter  Sc ience Depar tmen t 
Carneg ie  Mel lon  Un iversi ty  
5000  F o rbes  Ave ,   P i t t sbu rgh  15217  
jd tam@cs .cmu .edu  
 
Sea n  Hy de 
E lectr ical  and   Compu ter  Eng ineer ing  
Carneg ie  Mel lon  Un iversi ty  
5000  F o rbes  Ave ,   P i t t sbu rgh  15217  
sean .a .hyde@gma i l .com  

 

J ir i   S im sa  
Compu ter  Sc ience Depar tmen t 
Carneg ie  Mel lon  Un iversi ty  
5000  F o rbes  Ave ,   P i t t sbu rgh  15217  
j s im sa@cs .cmu .edu  
 
L u i s  Vo n  A h n  
Compu ter  Sc ience Depar tmen t 
Carneg ie  Mel lon  Un iversi ty  
5000  F o rbes  Ave ,   P i t t sbu rgh  15217  
b ig lou@cs .cmu .edu  

Abstract 

CA P T CHAs   are  compu ter-generated   test s   tha t  humans   can   pass   bu t  curren t  
compu ter   sys tems   canno t .   CA P T CHA s   p rov ide  a  method   for  au toma t ical ly  
d i s t ingu i sh ing   a  human  from   a  compu ter  program ,  and   therefore  can   pro tect  
Web   serv ices   from   abuse  by   so-cal led   “b o t s .”  Mo s t  CA P T CHA s   consi s t   of 
d i s to r ted   images ,   usual ly   tex t ,   for  wh ich   a  user  m us t  p rov ide  some 
descr ip t ion .   Unfor tunately,   v i sual  CA P T CHAs   l im i t  access   to   the  m i l l ions  
of  v isual ly   impaired   peop le  using   the  Web .   Aud io   CA P T CHAs   were 
created   to   so lve  th i s   access ib i l i ty   i ssue ;  however,  the  secur ity   of  aud io  
CA P T CHAs   was   never  formal ly   tes ted .   S ome  v i sual  CA P T CHAs   have 
been   broken   using   mach ine  learn ing   techn iques ,  and   we  propose  using  
s im i lar   ideas   to   tes t  the  secur i ty   of  aud io   CA P T CHAs .   Aud io   CA P T CHAs  
are  generally   compo sed   of  a  set  of  words   to   be  iden t if ied,   layered   on   top   of 
no i se .  We  analyzed   the  secur i ty   of  curren t  aud io   CA P T CHAs   from   popu lar  
Web   s i te s   by   us ing   AdaBoos t ,   SVM ,   and   k-NN ,  and   ach ieved   correct 
so lu t ion s   for  test   samp les   w i th   accuracy   up   to   71% .   Such   accuracy   is  
enough   to   consider   these  CA P T CHAs   b roken .  Tra in ing   several  d ifferen t  
mach ine  learn ing   algor i thm s   on   d ifferen t  types   of  aud io   CA P T CHAs  
al lowed   us   to   analyze  the  s treng ths   and   weaknesses   of  the  algo r i thm s   so  
tha t  we  cou ld   suggest  a  des ign  for  a  mo re  robust  aud io   CAP T CHA .  

 
1  
I n t r o d u c t i o n  
CA P T CHAs   [1]   are  au tomated   tes t s   des igned   to   tel l   compu ters   and   humans   apar t  by  
presen t ing   users   w i th   a  prob lem   tha t  humans   can  so lve  bu t  curren t  compu ter   programs  
canno t .   Because  CA P T CHAs   can   d i s t ingu i sh   between  humans   and   compu ters   w i th   h igh  
probab i l i t y,   they   are  used   for  many   d ifferen t  secur ity   app l icat ion s :  they   preven t  bo t s   from  
vo t ing   con t inuous ly   in   on l ine  po l l s ,   au toma t ical ly   reg i s ter ing   for  m i l l ions   of  spam   emai l  
accoun ts ,  au toma t ical ly   pu rchasing   t ickets   to   buy   ou t  an   even t ,   etc .  Once  a  CA P T CHA  i s  
broken  ( i .e . ,   compu ter   programs   can   successfu lly   pass   the  tes t) ,  bo ts   can   impersonate 
humans   and   gain   access   to   serv ices   that  they   shou ld   no t .  Therefore,  i t   i s   impo r tan t  for 
CA P T CHAs   to  be   secure. 
To   pass   the  typ ical  v i sual  CA P T CHA ,   a  user  mus t   correct ly   type  the  characters   d i sp layed   in  
an  image  of  d is to r ted   tex t .   Many   v i sual  CA P T CHAs   have  been   broken  w i th   mach ine 

 

learn ing   techn iques   [2]-[3] ,  though   some  remain   secure  agains t  such   at tacks .  Because 
v i sual ly   impaired   users  who   surf  the  Web   us ing   screen-read ing   programs   canno t  see  th i s   type 
of  CA P T CHA ,   aud io   CA P T CHAs   were  created .  Ty p ical   aud io   CA P T CHAs   consi s t   of  one 
or   several  speakers   say ing   let ters   or   d ig i t s   at  randomly   spaced   in tervals .   A  u ser   mus t  
correctly   iden t ify   the  d ig i t s   o r   characters   spoken   in   the  aud io   f ile  to   pass   the  CA P T CHA .   To  
make  th is  
tes t  d ifficu l t   for   curren t  compu ter   sys tem s ,   specifical ly   au tomat ic  speech  
recogn i t ion  (A SR)  p rograms ,  background  no i se   i s   in jected   in to   the  aud io  f iles .    
S ince  no   official  evaluat ion   of  ex is t ing   aud io   CA P T CHAs   has   been   repor ted ,  we  tested   the 
secur i ty   of  aud io   CA P T CHAs   used   by   many   popu lar   Web   s i tes   by   runn ing   mach ine  learn ing  
exper imen ts   des igned   to   b reak   them .   In   the  nex t  sect ion ,  we  prov ide  an  overv iew  of  the 
l i teratu re  related   to   our   pro ject .   Sect ion   3   descr ibes   our  methods   for  creating   train ing   data ,  
and   sect ion   4   descr ibes   how   we  create  class if iers   that   can  recogn ize  let ters ,   d ig i t s ,   and   no i se .  
In   sect ion   5 ,   we  d iscuss   how   we  evaluated   our  methods   on   w idely   used   aud io   CA P T CHAs  
and   we  g ive  our   resu l t s .   In   par ticu lar,   we  show   that  the  aud io   CA P T CHAs   used   by   si tes  
such   as  Goog le  and  D igg   are  suscep t ib le  to   mach ine  learning   at tacks .  Sect ion   6   men t ion s   the 
proposed  des ign  of  a  new  m o re  secure aud io   CA P T CHA   based  on  ou r  find ings .  
 
2  
L i t e r a t u r e   r e v i e w  
To   b reak   the  aud io   CA P T CHAs ,   we  der ive  features   from   the  CA P T CHA  aud io   and   use 
several  mach ine  learn ing   techn iques   to   perform   AS R  on   segmen ts   of  the  CA P T CHA .   There 
are  many   popu lar   techn iques   for  ex tract ing   features   from  speech .  The  th ree  techn iques   we  use 
are  mel- f requency  cepst ra l  coef f icien t s  (MF CC) ,   pe rceptua l   l inear   p red ict ion   ( PLP) ,   and  
re la t ive  spectra l   t rans fo rm-PLP   (RA S TA - PLP) .   M F CC   i s   one  of  the  mos t   popu lar   speech  
feature  represen tat ion s   used .   S im i lar   to   a  fast  Fou r ier   transform   (FF T ) ,   MF CC   transforms   an  
aud io   file  in to   frequency  bands ,  bu t  (un l ike  FF T )   MF CC   uses   mel-frequency  bands ,  wh ich  
are  better  for  approx ima t ing   the  range  of  frequencies   humans   hear.  PL P  was   designed   to  
ex tract  speaker- independen t  features   from   speech  [4] .  Th erefore,  by   us ing   PL P  and   a  var ian t 
such   as   RA S TA- PL P,   we  were  ab le  to   train   ou r   class ifiers   to   recogn ize  le t ters   and   d ig i t s  
independen t ly   of  who   spoke  them .   S ince  many   d ifferen t  peop le  recorded   the  d ig i t s   used   in  
one  of  the  types   of  aud io   CA P T CHAs   we  tes ted ,  PL P   and   RA S TA- PL P   were  needed   to  
ex tract   the  features   that  were m os t  u sefu l  for  so lv ing   them .  
In   [4]-[5] ,  the  au tho rs   conducted   exper imen ts   on   recogn izing   i so la ted   d ig i t s   in   the  presence 
of  no ise  using   bo th   PL P  and   RA S TA- PL P.   However,  th e  no ise  used   consi s ted   of  telephone 
or   m icrophone  stat ic  caused   by   record ing   in   d ifferen t  locat ions .   The  aud io   CA P T CHAs   we 
use  con tain   th i s   type  of  no i se ,  as   wel l  as   added   vocal  no i se  and /o r   mus ic ,   wh ich   is   supposed  
to  make   the  au tomated  recogn i t ion  p rocess  m uch  harder.  
T he  au tho rs   of  [3]   emphasize  how   many   v isual  CA P T C HAs   can   be  broken   by   successfu lly  
sp l i t t ing   the  task   in to   two   smal ler   tasks :  segmen tat ion   and   recogn i t ion .  We  fo llow   a  sim i lar  
approach   in   that  we  first  au tomat ical ly   sp l i t   the  aud io   in to   segmen t s ,   and   then   we  class ify 
these   segmen t s  as  no i se  o r  wo rds . 
In   ear ly   March   2008 ,  concurren t  to   ou r   work ,  the  b log   of  Win tercore  Labs   [6]   claimed   to  
have  successfu lly   b roken   the  Goog le  aud io   CA P T CHA .   After  read ing   their   Web   ar t icle  and 
v iew ing   the  v ideo   of  how   they   so lve  the  CA P T CHAs ,   we  are  unconv inced   tha t  the  p rocess  
i s   en t irely   au toma t ic ,  and   i t  i s   unclear what  their   exact  pass   rate  i s .   Because  we  are  unab le  to  
find   any   formal  techn ical  analys i s   of  th i s   p rogram ,  we  can   nei ther   be  sure  of  i ts   accuracy   nor 
the  ex ten t  of   i t s  au tomat ion .  
 
3  
C r e a t i o n   o f   t r a i n i n g   d a t a  
S ince  au tomated   programs   can   at temp t  to   pass   a  CA P T CHA  repeated l y,   a  CA P T CHA  i s  
essen t ial ly   b roken   when   a  program   can   pass   i t  m o re  than   a  non- tr iv ial   fraction   of  the  t ime ;  
e.g . ,  a  5%  pass  rate   i s  enough .  
Our   approach   to   b reak ing   the  aud io   CA P T CHA s   began   by   f irst  sp l i t t ing   the  aud io   f iles   in to  
segmen ts  of  no i se  o r  wo rds :  for  ou r  exper imen t s ,   the  wo rds  were  spoken   le t ters  o r  d ig i t s .  We  
used   manual  transcr ip t ions   of  the  aud io   CA P T CHAs   to   get  informat ion   regard ing   the 
locat ion   of  each   spoken  word  w i th in   the  aud io   f ile.  W e  were  ab le  to   label  ou r  segmen ts  
accurately  by  us ing   th i s   informat ion .  

 

We   gathered   1 ,000   aud io   CA P T CHAs   from   each   of  the  fo l low ing   Web   s i te s :   goog le .com ,  
d igg .com ,   and   an  o lder  version   of  the  aud io   CA P T C HA  in   recap tcha.net .  Each  of  the 
CA P T CHAs   was   anno tated   w i th   the  informat ion   regard ing   let ter /d ig i t   locat ions   p rov ided   by 
the  manual  transcr ip t ions .   F o r   each   type  of  CAP T CHA ,   we  random ly   selected   900   samp les  
for   train ing  and  u sed   the  remain ing  100  for   tes t ing .    
Using  
the  d ig i t / le t ter  
locat ion  
informat ion   p rov ided  
in  
the  manual  CA P T CHA  
transcr ip t ion s ,   each   train ing   CA P T CHA  i s   d iv ided   in to   segmen ts   of  no ise ,  the  le t ters   a-z,  o r  
the  d ig i t s   0 -9 ,  and   labeled  as   such .  We  igno re  the  anno tat ion   informat ion   of  the 
CA P T CHAs   we  use  for  tes t ing ,   and   therefore  we  canno t  iden t ify  the  size  of  those  segmen ts .  
Ins tead ,  each   tes t  CA P T CHA  i s   d iv ided   in to   a  number  of f ixed-size  segmen ts .   The  segmen ts  
w i th   the  h ighes t   energy   peaks   are  then   class if ied   using   mach ine  learn ing   techn iques   (F igu re 
1) .  S ince  the  size  of  a  feature  vecto r   ex tracted   from   a  segmen t   generally   depends   on   the  s ize 
of  the  segmen t ,   u s ing   f ixed-size  segmen t s   al lows   each   segmen t   to   be  descr ibed   w i th   a  feature 
vector   of  the  same  leng th .     We   chose  the  w indow   size  by   l i s ten ing   to   a  few  train ing  
segmen ts   and   ad jus ted   accord ing ly   to   ensure  that  the  segmen t  con tained   the  en t ire 
d ig i t / le t ter .     There  i s   undoub ted ly   a  mo re  op t imal  way   of  select ing   the  w indow   s ize,  
however,  we  were  s t i l l  ab le   to  b reak   the   th ree  CA P T CHA s  we   te s ted  w i th  ou r  method .  
 

 

 
F igu re 1 :  A   te s t  aud io   CA P T CHA  w i th   the  f ixed-size  segmen t s  con tain ing   the  h ighest  
energy  peaks  h igh l igh ted . 

 
T he  informat ion   p rov ided   in   the  manual  transcr ip t ions   o f  the  aud io   CA P T CHAs   con ta ins   a 
l i s t   of  the  t ime  in tervals   w i th in   wh ich   words   are  spoken .  However,  these  in tervals   are  of 
var iab le  size  and  the  word   m igh t  be  spoken   anywhere  w i th in   th i s   in terval .  To   p rov ide  fixed-
s ize  segmen ts   for  train ing ,   we  developed   the  fo llow ing   heur is t ic .   F irs t ,   d iv ide  each   file  in to  
var iab le-size  segmen ts   u s ing   the  t ime  in tervals   p rov ided   and   label  each   segmen t  accord ing l y.  
T hen ,   w i th in   each  segmen t ,   detect  the  h ighes t  energy   peak   and   return   i t s   f ixed-size 
neighborhood  labeled  w i th   the  curren t  segmen t ’ s   label .  T h i s   heur is t ic  ach ieved   near ly   perfect 
label ing   accuracy   for  the  train ing   set .   Rare  m i s takes   occurred  when   the  h ighest  energy   peak  
of  a  d ig i t  o r   le t ter   segmen t  co rresponded   to  no i se  rather   than   to  a  d ig i t  o r   le t ter.  
To   summar ize  th i s   subsect ion ,   an   aud io   file  is   transformed  in to   a  set  of  fixed-size  segmen ts  
labeled   as   no i se ,   a  d ig i t   be tween   0   and   9 ,  o r   a  let ter   between   a  and   z.   These  segmen t s   are 
then  u sed  for   train ing .   C las s if iers  are  trained  for  one   type   of  CA P T CHA  a t  a   t ime .  
 
4  
C l a s s i f i e r   c o n s t r u c t i o n  
From   the  train ing   data  we  ex tracted   five  sets   of  features  us ing   twelve  MF CC s   and   twelfth-

 

order  spectral  (SPE C)   and   cepstral  (CEP S)   coefficien t s   from   PL P  and   RA S TA- PL P.   The  
Ma t lab   function s   for  ex tracting   these  features   were  prov ided   on l ine  at  [7]   and   as   par t  of  the 
Vo icebox  package.  We  use  AdaBoos t ,   SVM ,   and   k-NN  algo r i thm s   to   imp lemen t  au tomated  
d ig i t   and   let ter   recogn i t ion .  We  detai l  ou r   imp lemen ta t ion   of  each   algor i thm   in   the 
fo llow ing   subsect ions .  
 
4 . 1  
A d a B o o s t  
Using   deci s ion   s tump s   as   weak   class ifiers   for  AdaBoo s t ,   anywhere  from   11   to   37   ensemb le 
class if iers   are  bu i l t .   The  number   of class if iers   bu i l t   depends   on   wh ich   type  of  CAP T CHA  we 
are  so lv ing .   Each   class if ier  trains   on   al l  the  segmen t s   associated   w i th   tha t  type  of 
CA P T CHA ,   and   for  the  purpose  of  bu i ld ing   a  sing le  class if ier,  segmen t s   are  labeled   by 
ei ther   -1   (negat ive  examp le)   or  +1   (posi t ive  examp le) .  Us ing   cross-valida t ion ,   we  choose  to  
use  50   i terat ions   for  our   AdaBoos t   algo r i thm .   A  segmen t  can  then   be  classif ied   as   a 
par ticu lar  let ter,   d ig i t ,   o r   no ise  accord ing   to   the  ensemb le  class if ier  that  ou tpu t s   the  number 
closes t   to  1 .  
 
4 . 2  
S u p p o r t   v e c t o r   m a c h i n e  
To   conduct  d ig i t   recogn i t ion   w i th   SVM ,   we  used   the  C+ +   im p lemen tat ion s   of  lib SVM  [8]  
version   2 .85   w i th   C- SMV  and   RB F   kernel .   F irs t ,   al l   feature  values   are  scaled   to   the  range  of 
-1   to   1   as   sugges ted   by   [8] .  The  scale  parameters   are  s to red   so   that  tes t   samp les   can   be 
scaled   accord ing l y.   Then ,   a  sing le  mu l t ic las s   class if ier  i s   created   for  each   set  of  features  
us ing   al l   the  segmen ts   for  a  par t icu lar   type  of  CA P T CHA .  We  u se  cross-val idat ion   and   gr id  
search   to  d i scover   the  op t ima l   s lack  penal ty  (C= 32 )  and  kernel  parameter  (γ=0 .0 11 ) . 
 
k - n e a r e s t   n e i g h b o r   ( k - N N )  
4 . 3  
We   u se  k-NN   as   our  final  method   for  class ify ing   d ig i t s .   Fo r   each   type  of  CAP T CHA ,   five 
d ifferen t  class if iers   are  created   by   us ing   al l   of  the  train ing   data  and   the  f ive  sets   of  features 
associated  w i th   that  par t icu lar  type  of  CAP T CHA .  Again   we  use  cross-val idat ion   to   d iscover 
the  op t ima l  parameter,   in   th i s  case  k=1 .  We  u se  Eucl id ian  d i s tance  as  ou r  d i s tance  metr ic . 
 
5  
A s s e s s m e n t   o f   c u r r e n t   a u d i o   C A P T C H A s  
Our  method   for  so lv ing   CA P T CHAs  
i terat ively   ex tracts   an   aud io   segmen t  from   a 
CA P T CHA ,   inpu t s   the  segmen t  to   one  of  our   d ig i t   o r  let ter   recogn izers ,  and   ou tpu t s   the 
label  for  that  segmen t .  We  con t inue  th i s   p rocess   un t i l  th e  max imum   so lu t ion   s ize  i s   reached  
or   there  are  no   un labeled   segmen ts   left .  S ome  of  the  CA P T CHAs   we  evaluated   have 
so lu t ion s   that   vary   in   leng th .   Ou r   method   ensu res   that  we  get  so lu t ion s   of  vary ing   leng th  
tha t  are  never  longer   than   the  max imum   so lu t ion   leng th .   A  segmen t  to   be  class if ied   i s  
iden t if ied  by   tak ing   the  neighborhood   of  the  h ighest  energy   peak  of  an   as   yet  un labeled   par t 
of  the   CA P T CHA .  
Once  a  pred ict ion   of  the  so lu t ion   to   the  CA P T CHA  i s   compu ted ,   i t   i s   compared   to   the  true 
so lu t ion .   G iven   that  at  leas t  one  of  the  aud io   CA P T CHAs   al lows   users   to   make  a  m i s take  in  
one  of  the  d ig i t s   (e .g . ,   reCA P T CHA ) ,  we  compu te  the  pass   rate for  each   of  the  d ifferen t  types  
of  CA P T CHAs  w i th  a l l  of   the  fo llow ing  cond i t ions :  
•   T he  p red ict ion  ma tches   the   true   so lu t ion  exact l y.  
•  
Inser ting  one  d ig i t   in to   the  p red ict ion  wou ld  make   i t  ma t ch   the  so lu t ion  exact l y.  
•   Rep lacing  one  d ig i t   in   the  p red ict ion  wou ld  make   i t  ma tch   the   so lu t ion  exact l y.  
•   Remov ing  one  d ig i t  from   the  p red ict ion  wou ld  make   i t  m atch   the   so lu t ion  exact l y.  
However,  s ince  we  are  on ly   su re  that   these  cond i t ions   app ly   to   reCA P T CHA  aud io  
CA P T CHAs ,   we  al so   calcu late  the  percen tage  of  exact  so lu t ion   matches   in   ou r   resu l t s   for 
each   type  of  aud io   CA P T CHA .   T hese  resu l t s  are  descr ibed   in   the  fo llow ing   subsect ions .    
 
5 . 1  
G o o g l e  
Goog le  aud io   CA P T CHA s   consi s t  of  one  speaker  saying   random   d ig i t s   0-9 ,  the  phrase 
“ o nce  again ,”  fo llowed   by   the  exact  same  recorded   sequence  of  d ig i t s   o r ig inal ly   p resen ted .  

 

T he  background   no ise  cons is t s   of  human   vo ices   speak in g   backwards   at  vary ing   vo lumes .   A 
so lu t ion   can   range  in   leng th   from   f ive  to   eigh t  wo rds .  We   set  ou r   class if ier   to   find   the  12 
loudes t  segmen t s   and   class ify   these  segmen ts   as   d ig i t s   o r   no ise .     Because  the  ph rase  “o nce 
again”  marks   the  halfway   po in t   of  the  CA P T CHA ,   we  p reprocessed   the  aud io   to   on ly   serve 
th i s   half  of  the  CA P T CHA  to   ou r   class if iers .   I t  i s   im por tan t  to   no te ,   however,   that   the 
class if iers   were  always   ab le  to   iden t ify   the  segmen t  con ta in ing   “ o nce  again ,”  and   these 
segmen ts   were  iden t if ied   before  al l  o ther   segmen t s .   Therefore,  if  necessary,   we  cou ld   have 
had  ou r   sys tem  cu t   the  f ile   in  half  after  f irst   label ing   th i s   segmen t .  
F o r   AdaBoos t ,   we  create  12   class if iers:  one  class if ier   for  each   d ig i t ,   one  for  no ise ,   and   one 
for  the  phrase  “o nce  again .”    Our   resu l t s   ( Tab le  1 )   show   tha t  at  bes t  we  ach ieved   a  90%  pass  
rate  us ing   the  “ o ne  m i s take”  pass ing   cond i t ions   and   a  66%  exact  so lu t ion   match   rate.  Us ing  
SVM   and   the  “ o ne  m i s take”  pass ing   cond i t ion s ,   a t  bes t   we  ach ieve  a  92%  pass   rate  and   a 
67%  exact  so lu t ion   ma tch .  F o r   k-NN ,   the  “ o ne  m i s take”  pass   rate  i s   62%  and   the  exact 
so lu t ion  ma tch  rate   i s  26% .    
 
Tab le  1 :  Goog le  aud io   CA P T CHA  resu l t s :  Max imum  67%  accuracy  was  ach ieved  by   SVM .  
 

AdaBoo st  
exact  
One 
m i s take 
ma tch  
61% 
88% 

90% 

90% 

66% 

66% 

C las s i f iers  Used  
SVM  

one  
m i s take 
92% 

90% 

92% 

exact  
ma tch  
67% 

67% 

67% 

k-NN  

one  
m i s take 
30% 

60% 

62% 

exact  
ma tch  
1% 

26% 

23% 

88% 

48% 

90% 

61% 

29% 

1% 

90% 

63% 

92% 

67% 

33% 

2% 

 

M F CC  
PLP-
S PEC  
PLP-
CEP S  
RA S TA -
PLP-
S PEC  
RA S TA -
PLP-
CEP S  

 
d
e
s
U
 
s
e
r
u
t
a
e
F

 

5 . 2  
D i g g  
D igg   CA P T CHAs   also   consi s t  of  one  speaker,  in   th i s   case  say ing   a  random   comb inat ion   of 
le t ters   and   d ig i t s .   The  background   no ise  consi s t s   of  stat ic  or   what  sound s   l ike  tr ick l ing  
water   and   i s   no t   con t inuou s   th roughou t  the  en t ire  f ile.   We   no t iced   in   ou r   train ing   data  that  
the  fo llow ing   characters   were  never  presen t  in   a  so lu t ion :   0 ,  1 ,   2 ,   5 ,  7 ,  9 ,  i ,   o ,   z .  S ince  the 
D igg   aud io   CA P T CHA  i s   al so   the  verbal  transcr ip t ion   o f  the  v isual  CA P T CHA ,  we  bel ieve 
tha t  these  characters   are  excluded   to   avo id   confusion   between  d ig i t s   and   letters   that  are 
s im i lar   in   appearance.  The  so lu t ion   leng th   var ies   between   th ree  and   s ix   words .  Us ing  
AdaBoos t ,   we  create  28   class if iers:  one  class if ier  for  each   d ig i t   o r   let ter   that  appears   in   our  
train ing   data  and   one  class if ier   for  no ise .   Perhaps   because  we  had   fewer   segmen ts   to   train  
w i th  and   there  was  a  far  h igher  p ropor t ion  of  no i se   segmen t s ,  AdaBoo s t  failed   to  p roduce any  
correct  so lu t ion s .   We  bel ieve  that  the  overwhelm ing   number   of  negative  train ing   examp les  
versus   the  sma l l  number   of  pos i t ive  train ing   samp les   used   to   create  each   decis ion   s tump  
severely  affected  AdaBoos t ’ s  ab i l i ty   to  c lass ify  aud io   segmen t s  co rrectl y.    
A  h i s tog ram   of  the  train ing   samp les   i s   p rov ided   in   F i gure  2   to   i l lu s trate  the  amoun t  of 
train ing   data  availab le  for   each   character.   W hen   us ing   S VM ,   the  best  feature  set  passed   w i th  
96%  using   “ o ne  m i s take”  pass ing   cond i t ion s   and   passed   w i th   71%  when   match ing   the 
so lu t ion   exact ly.   F o r   k-NN ,  the  best  feature  set  produced  a  90%  “o ne  m i s take”  pass   rate  and  
a 49%  exact   so lu t ion  ma tch .   Fu l l  resu l t s  can  be  found   in   Tab le  2 .  

 

 

Tab le  2 :  D igg  aud io   CA P T CHA  resu l t s :  Max imum  71% accuracy  was  ach ieved  by   SVM .  
 

 

M F CC  
PLP-
S PEC  
PLP-
CEP S  
RA S TA -
PLP-
S PEC  
RA S TA -
PLP-
CEP S  

AdaBoo st  
exact  
one  
m i s take 
ma tch  
-  
-  

-  

-  

-  

-  

-  

-  

-  

-  

C las s i f iers  Used  
SVM  

one  
m i s take 
96% 

94% 

96% 

exact  
ma tch  
71% 

65% 

71% 

k-NN  

one  
m i s take 
89% 

90% 

64% 

exact  
ma tch  
49% 

47% 

17% 

17% 

3% 

67% 

17% 

96% 

71% 

82% 

34% 

 

e 3 4 6 8 a b c d e f g h j k l m n p q r s t u v w x y
s
i
o
n

 
d
e
s
U
 
s
e
r
u
t
a
e
F

s
t
n
e
m
g
e
S
 
f
o
 
#

1000

900

800

700

600

500

400

300

200

100

0

Segment Label
 
F igu re 2 :  D igg   CA P T CHA   train ing  data  d i s tr ibu t ion .  
 

 

5 . 3  
r e C A P T C H A  
T he  o lder   version   of  reCA P T CHA ’ s   aud io   CA P T CHAs   we  tested   consi s t  of  several  speakers 
who   speak   random   d ig i t s .   The  background   no ise  cons i s t s   of  human   vo ices   speak ing  
backwards   at  vary ing   vo lumes .   The  so lu t ion   i s   a lways   eigh t  d ig i t s   long .     F o r   AdaBoos t ,   we 
create  11   class if iers:  one  class if ier  for  each   d ig i t   and   one  class if ier  for  no ise .  Because  we 

 

know   that  the  reCA P T CHA  pass ing   cond i t ion s   are  the  “ o ne  m i s take”  pass ing   cond i t ion s ,  
SVM  p roduces  ou r  bes t  pass  rate  of  58%.  Ou r  bes t  exact  ma tch  rate   i s  45%  ( Tab le  3 ) . 
 

Tab le  3 :  reCA P T CHA  aud io   CA P T CHA  resu l t s :  Max im um  45%  accuracy  was  ach ieved  by  
SVM .  
 

 

M F CC  
PLP-
S PEC  
PLP-
CEP S  
RA S TA -
PLP-
S PEC  
RA S TA -
PLP-
CEP S  

 
d
e
s
U
 
s
e
r
u
t
a
e
F

AdaBoo st  
exact  
one  
ma tch  
m i s take 
18% 
6% 

27% 

23% 

9% 

9% 

10% 

10% 

3% 

3% 

C las s i f iers  Used  
SVM  

one  
m i s take 
56% 

58% 

56% 

exact  
ma tch  
43% 

39% 

45% 

k-NN  

one  
m i s take 
22% 

43% 

29% 

exact  
ma tch  
11% 

25% 

14% 

36% 

18% 

24% 

4% 

46% 

30% 

32% 

12% 

 
6  
P r o p e r t i e s   o f   w e a k   v e r s u s   s t r o n g   C A P T C H A s  
From   ou r   resu l t s ,   we  no te  that  the  easiest  CA P T CHAs   to   b reak   were  from   D igg .    Goog le 
had   the  nex t  s trongest  CA P T CHAs   fo l lowed   by   the  s tronges t  from   reCA P T CHA .     A l though  
the  D igg   CA P T CHAs   have  the  largest  vocabu lar y,   g iv in g   us   les s   train ing   data  per   label ,   the 
same  woman   recorded   them   al l .     Mo re  impo r tan t l y,   the  same  type  of  no ise  i s   used  
th roughou t  the  en t ire  CA P T CHA .    The  no ise  sounds   l ike  runn ing   water  and  s tat ic  wh ich 
sounds   very   d ifferen t  from   the  human   vo ice  and   does   no t   p roduce  the  same  energy   sp ikes  
needed   to   locate  segmen ts ,   therefore  mak ing   segmen tat io n   qu i te  easy.   The  CA P T CHAs   from  
Goog le  and   reCA P T CHA  used   o ther   human   vo ices   for  background   no ise ,   mak ing  
segmen tat ion   much   mo re  d ifficu l t .     A l though   Goog le  used  a  smal ler   vocabu lary  than   D igg  
and   also   on ly   used   one  speaker,  Goog le ’ s   background  no i se  made  the  CA P T CHA  mo re 
d ifficu l t  to   so lve .    Af ter  l i s ten ing   to   a  few  of  Goog le’ s   CA P T CHAs ,  we  no t iced   that  
al though   the  background   no ise  consi s ted   of  human  vo ices ,  the  same  background   no i se  was  
repeated .    reCA P T CHA  had   s im i lar   no ise  to   Goog le ,   bu t   they   had   a  larger  select ion   of  no ise 
thu s   mak ing   i t   harder   to   learn .     reCA P T CHA  al so   has   th e  longest  so lu t ion   leng th   mak ing   i t  
mo re  d ifficu l t  to   get  perfectly   correct .    F ina l l y,   reCA PT CHA  u sed   many   d ifferen t  speakers 
caus ing   i t  to   be  the  s trongest  CA P T CHA  of  the  th ree  we  tested .     In   conclus ion ,   an   aud io  
CA P T CHA  that  consi s t s   of  a  fin i te  vocabu lary  and   background   no i se  shou ld   have  mu l t ip le 
speakers  and  no i se   s im i lar   to   the   speakers . 
 
7  
R e c o m m e n d a t i o n s   f o r   c r e a t i n g   s t r o n g e r   a u d i o  
C A P T C H A s  
Due  to   our   success   in   so lv ing   aud io   CA P T CHAs ,  we  have  decided   to   s tar t  develop ing   new 
aud io   CA P T CHAs   that  ou r   method s ,   and   mach ine  learn ing   method s   in   general ,  w i l l   be  les s  
l ikely   to   so lve .  From   ou r   exper imen t s ,   we  no te  that  CA P T CHAs   con tain ing   longer 
so lu t ion s   and   mu l t ip le  speakers   tend   to   be  mo re  d ifficu l t   to   so lve .   A lso ,   because  our 
me thods   depend   on   the  amoun t  of  train ing   data  we  have,  hav ing   a  large  vocabu lary   wou ld  
make  i t  mo re  d ifficu l t  to   co l lect  enough   train ing   data .    A lready   s ince  ob tain ing   these  resu l t s ,  
reCA P T CHA .net   has   updated   the ir   aud io   CA P T CHA  to   con tain   mo re  d is to r t ion s   and   a 

 

larger   vocabu lary :  the  d ig i t s   0   th rough   99 .  In   design ing   a  new   aud io   CA P T CHA  we  are  also  
concerned  w i th   the  human   pass   rate .  The  curren t  human   pass   rate for  the  reCA P T CHA  aud io  
CA P T CHAs   i s   on ly   70% .   To   develop   an   aud io   CA P T CHA  w i th   an   imp roved   human   pass  
rate ,  we  p lan   to   take  advan tage  of  the  human   m ind ’ s   ab i l i ty   to   understand   d i s to r ted   aud io  
th rough   con tex t  clues .   By   l i s ten ing   to   a  phrase  ins tead   of  to   random   i so lated   words ,   humans  
are  bet ter   ab le  to   decipher   d is to r ted   u t terances   because  they   are  fami l iar   w i th   the  phrase  or 
can   use  con tex tual  clues   to   decipher  the  d i s to r ted   aud io .   Us ing   th i s   idea ,  the  aud io   for  our 
new   aud io   CA P T CHA  w i l l   be  taken  from   o ld - t ime  rad io   programs   in   wh ich   the  poor  qual i ty  
of  the  aud io   makes   transcr ip t ion   by   A S R   sys tem s   d ifficul t .   U sers   w i l l   be  p resen ted   w i th   an  
aud io   cl ip   cons is t ing   of  a  4-6   word   phrase.  Half  of  the  CAP T CHA  cons i s t s   of  words ,  wh ich  
val idate  a  user  to   be  human ,  wh i le  the  o ther   half  of  the words   need   to   be  transcr ibed .  Th i s   i s  
the   same   idea beh ind   the  v i sual  reCA P T CHA   that   i s  cu rren t ly  d ig i t iz ing   tex t  on  wh ich  OCR  
fails .   We  expect  that  th i s   new   aud io   CA P T CHA  w i l l   be mo re  secure  than   the  curren t  version  
and  eas ier  for  humans   to  pass .  In i t ia l  exper imen t s  u s ing   t h i s   idea   show   th i s   to  be   true  [9] . 
 
8  
C o n c l u s i o n  
We   have  succeeded   in   “ b reak ing”  th ree  d ifferen t  types   of  w idely   used   aud io   CA P T CHAs ,  
even   though   these  were  developed   w i th   the  purpose  of  defeating   at tacks   by   mach ine  learn ing  
techn iques .  We  bel ieve  our  resu l t s   can   be  imp roved   by   select ing   op t ima l  segmen t  s izes ,   bu t  
tha t  i s   unnecessary   g iven   our   already   h igh   success   rate .  F o r   our   exper imen t s ,   segmen t  s izes  
were  no t  chosen   in   a  special  way ;  occasional ly   y ie ld in g   resu l t s   in   wh ich   a  segmen t  on ly  
con tained   half  of  a  word ,  causing   ou r   pred ict ion   to   con tain   that  par t icu lar   word   tw ice .  We  
also   bel ieve  that  the  AdaBoo s t  resu l t s   can   be  imp roved ,  par t icu lar ly   for  the  D igg   aud io  
CA P T CHAs ,   by   ensur ing   tha t  the  number   of  negat ive  train ing   samp les   i s   c loser   to   the 
number   of  posi t ive  train ing   samp les .   We   have  shown   that   ou r   approach   is   successfu l  and   can 
be u sed  w i th  many  d ifferen t  aud io   CA P T CHAs   that  con tain   sma l l  f in i te  vocabu lar ies .  

A c k n o w l e d g m e n t s  
T h i s   wo rk   was   par t ial ly   suppo r ted  by   generous   g if ts   from   the  Heinz  Endowmen t ,  by   an  
equ ipmen t   g ran t  from   In tel  Co rporat ion ,   and   by   the  Army   Research   Office  th rough   gran t  
number   DAAD19-02-1-0389   to   CyLab   at  Carneg ie  Mel lon   Un iversi t y.   Lu i s   von   Ahn   was  
par tial ly   suppo r ted   by   a  M icrosof t  Research  New   Facu l ty   Fe l lowsh ip   and   a  MacAr thur  
Fe l lowsh ip .   Jenn ifer   Tam  was  par t ial ly   suppo r ted  by  a  G oog le  An i ta   Bo rg   Scho larsh ip .  

R e f e r e n c e s  
[1 ] L . v o n  A h n , M . B lum, an d  J. L an gfo rd .  “ Te l l i n g   H uman s   an d  C omp u t e r s  A p a r t A u t oma t i ca l l y, ”  
C om m u n i c a t i o n s  o f   t h e  AC M , v o l . 4 7 , n o . 2 , p p . 5 7 - 6 0 , Feb . 2 0 0 4 . 
[2 ]  G .  M o r i   an d   J.  Ma l i k .  “ R eco g n i z i n g   O b j ec t s   i n   A d v e r sa r i a l   C l u t t e r :   B r eak i n g   a  Vi s u a l  
CA PTCH A , ” In  C o m p u t e r   V i s i o n  a n d  P a t t e r n  R eco g n i t i o n  C V P R ' 0 3 , Ju n e 2 0 0 3 . 
[3 ] K .  C h e l l a p i l l a,   an d   P.   S imard ,  “ U s i n g   M ach i n e   L ea rn i n g   t o   B reak   V i s u a l   H uman   In t e r ac t i o n  
P r o o fs  (H I P s ), ”  A d v a n ce s  i n   N eu r a l   In f o r m a t i o n   P ro ce s s i n g   S y s t em s   1 7 ,  Neu r a l   In f o rm a t i o n  
P ro ce s s i n g  S y s t em s  (N I P S ' 2 0 0 4 ), MIT P r e s s . 
[4 ] H . H e rman s k y,   “ P e r cep t u a l   L i n ea r   P r e d i c t i v e  ( P L P )   A n a l y s i s   of Sp eech ,”  J. A co u s t .  S o c. A m .,  
v o l . 8 7 , n o . 4 , p p . 1 7 3 8 -1 7 5 2 ,  A p r. 1 9 9 0 . 
[5 ]  H .  H e rman s k y,  N .  M o rg an ,  A .  Bay y a,  an d   P.   K o h n .  “ R A S TA - P L P   S p eech   A n a l y s i s  
Tech n i q u e, ”  In   P ro c.  IE E E   In t ’ l   C o n f .  A co u s t i c s ,  S p eech   &   S i g n a l   P ro ce s s i n g ,  v o l .  1 ,  p p .  1 2 1 -
1 2 4 , San  F ran c i s c o , 1 9 9 2 . 
[6 ] R . San t amar t a.  “ B r eak i n g  Gma i l ’ s  A u d i o  C ap t c h a, ” h t t p : / / b l o g . w i n t e r c o r e.com / ? p = 1 1 , 2 0 0 8 . 
[7 ]  D .  E l l i s .   “ P L P   a n d   RA S TA  ( an d   MFCC ,  an d   in v e r s i o n )  i n   M a t l a b   u s i n g   me lfcc.m  an d  
i n vme lfcc.m,” h t t p : / / w w w . ee.co l umb i a. ed u / ~ d p w e / r e s o u r ce s /ma t l a b / r a s t ama t / , 2 0 0 6 . 
[8 ] C . C h an g  an d  C . L i n . L IBSVM :  a  l i b r a ry  fo r  s u p p o r t  v ec t o r mach i n e s , 2 0 0 1 . S oftw a r e  av a i l a b l e  
a t  h t t p : / / w w w. c s i e.n t u .ed u . t w / ~ c j l i n / l i b s vm  
[9 ]  A .  Sch l a i k j e r.  “ A   D u a l - U s e  S p eech   CA PTCH A :   A i d i n g   Vi s u a l l y   Impa i r ed   We b   U s e r s   w h i l e  
P r o v i d i n g   T ran s c r i p t i o n s   o f  A u d i o   S t r eams,”  Tech n i c a l   R ep o r t   C M U -LT I-0 7 -0 1 4 ,  C arn e g i e  
Me l l o n  U n i v e r s i t y.  N o v emb er 2 0 0 7 . 

 


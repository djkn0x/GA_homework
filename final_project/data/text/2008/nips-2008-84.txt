Online Prediction on Large Diameter Graphs

Mark Herbster, Guy Lever, Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, England, UK
{m.herbster, g.lever, m.pontil}@cs.ucl.ac.uk

Abstract

We continue our study of online prediction of the labelling of a graph. We show a
fundamental limitation of Laplacian-based algorithms: if the graph has a large di-
ameter then the number of mistakes made by such algorithms may be proportional
to the square root of the number of vertices, even when tackling simple problems.
We overcome this drawback by means of an efﬁcient algorithm which achieves
a logarithmic mistake bound. It is based on the notion of a spine, a path graph
which provides a linear embedding of the original graph. In practice, graphs may
exhibit cluster structure; thus in the last part, we present a modiﬁed algorithm
which achieves the “best of both worlds”: it performs well locally in the presence
of cluster structure, and globally on large diameter graphs.

1
Introduction
We study the problem of predicting the labelling of a graph in the online learning framework. Con-
sider the following game for predicting the labelling of a graph: Nature presents a graph; nature
queries a vertex vi1 ; the learner predicts ˆy1 ∈ {−1, 1}, the label of the vertex; nature presents a
label y1 ; nature queries a vertex vi2 ; the learner predicts ˆy2 ; and so forth. The learner’s goal is to
minimise the total number of mistakes M = |{t : ˆyt (cid:54)= yt}|. If nature is adversarial, the learner
will always mispredict, but if nature is regular or simple, there is hope that a learner may make only
a few mispredictions. Thus, a central goal of online learning is to design algorithms whose total
mispredictions can be bounded relative to the complexity of nature’s labelling. In [9, 8, 7], the cut
size (the number of edges between disagreeing labels) was used as a measure of the complexity of a
graph’s labelling, and mistake bounds relative to this and the graph diameter were derived.
The strength of the methods in [8, 7] is in the case when the graph exhibits “cluster structure”. The
apparent deﬁciency of these methods is that they have poor bounds when the graph diameter is large
relative to the number of vertices. We observe that this weakness is not due to insufﬁciently tight
bounds, but is a problem in their performance. In particular, we discuss an example of a n-vertex
labelled graph with a single edge between disagreeing label sets. On this graph, sequential prediction
√
using the common method based upon minimising the Laplacian semi-norm of a labelling, subject to
constraints, incurs θ(
n) mistakes (see Theorem 3). The expectation is that the number of mistakes
incurred by an optimal online algorithm is bounded by O(ln n).
We solve this problem by observing that there exists an approximate structure-preserving embedding
of any graph into a path graph. In particular the cut-size of any labelling is increased by no more than
a factor of two. We call this embedding a spine of the graph. The spine is the foundation on which we
build two algorithms. Firstly we predict directly on the spine with the 1-nearest-neighbor algorithm.
We demonstrate that this equivalent to the Bayes-optimal classiﬁer for a particular Markov random
ﬁeld. A logarithmic mistake bound for learning on a path graph follows by the Halving algorithm
analysis. Secondly, we use the spine of the graph as a foundation to add a binary support tree to the
original graph. This enables us to prove a bound which is the “best of both worlds” – if the predicted
set of vertices has cluster-structure we will obtain a bound appropriate for that case, but if instead,
the predicted set exhibits a large diameter we will obtain a polylogarithmic bound.

Previous work. The seminal approach to semi-supervised learning over graphs in [3] is to predict
with a labelling which is consistent with a minimum label-separating cut. More recently, the graph
Laplacian has emerged as a key object in semi-supervised learning, for example the semi-norm
induced by the Laplacian is commonly either directly minimised subject to constraints, or used as
a regulariser [14, 2]. In [8, 7] the online graph labelling problem was studied. An aim of those
papers was to provide a natural interpretation of the bound on the cumulative mistakes of the kernel
perceptron when the kernel is the pseudoinverse of the graph Laplacian – bounds in this case being
relative to the cut and (resistance) diameter of the graph. In this paper we necessarily build directly
on the very recent results in [7] as those results depend on the resistance diameter of the predicted
vertex set as opposed to the whole graph [8]. The online graph labelling problem is also studied in
[13], and here the graph structure is not given initially. A slightly weaker logarithmic bound for the
online graph labelling problem has also been independently derived via a connection to an online
routing problem in the very recent [5].

2 Preliminaries
We study the process of predicting a labelling deﬁned on the vertices of a graph. Following the
classical online learning framework, a sequence of labelled vertices {(vi1 , y1 ), (vi2 , y2 ), . . . }, the
trial sequence, is presented to a learning algorithm such that, on sight of each vertex vit , the learner
makes a prediction ˆyt for the label value, after which the correct label is revealed. This feedback
information is then used by the learning algorithm to improve its performance on further examples.
We analyse the performance of a learning algorithm in the mistake bound framework [12] – the aim
is to minimise the maximum possible cumulative number of mistakes made on the training sequence.
A graph G = (V , E ) is a collection of vertices V = {v1 , . . . , vn} joined by connecting (possibly
weighted) edges. Denote i ∼ j whenever vi and vj are connected so that E = {(i, j ) : i ∼ j } is the
set of unordered pairs of connected vertex indices. Associated with each edge (i, j ) ∈ E is a weight
Aij , so that A is the n × n symmetric adjacency matrix. We say that G is unweighted if Aij = 1
n × n matrix G = D − A, where D is the diagonal degree matrix such that Dii = (cid:80)
for every (i, j ) ∈ E and is 0 otherwise. In this paper, we consider only connected graphs – that is,
graphs such that there exists a path between any two vertices. The Laplacian G of a graph G is the
j Aij . The
quadratic form associated with the Laplacian relates to the cut size of graph labellings.
(cid:88)
Deﬁnition 1. Given a labelling u ∈ IRn of G = (V , E ) we deﬁne the cut size of u by
1
1
Aij (ui − uj )2 .
ΦG (u) =
4 uT Gu =
(1)
4
(i,j )∈E
In particular, if u ∈ {−1, 1}n we say that a cut occurs on edge (i, j ) if ui (cid:54)= uj and ΦG (u) measures
the number of cuts.

We evaluate the performance of prediction algorithms in terms of the cut size and the resistance
diameter of the graph. There is an established natural connection between graphs and resistive
networks where each edge (i, j ) ∈ E is viewed as a resistor with resistance 1/Aij [4]. Thus the
effective resistance rG (vi , vj ) between vertex vi and vj is the potential difference needed to induce a
unit current ﬂow between vi and vj . The effective resistance may be computed by the formula [11]
rG (vi , vj ) = (ei − ej )T G+ (ei − ej ),
(2)
where “+ ” denotes the pseudoinverse and e1 , . . . , en are the canonical basis vectors of IRn . The
resistance diameter of a graph RG := maxvi ,vj ∈V rG (vi , vj ) is the maximum effective resistance
between any pair of vertices on the graph.

3 Limitations of online minimum semi-norm interpolation
As we will show, it is possible to develop online algorithms for predicting the labelling of a graph
which have a mistake bound that is a logarithmic function of the number of vertices. Conversely, we
ﬁrst highlight a deﬁciency in a standard Laplacian based method for predicting a graph labelling.
Given a partially labelled graph G = (V , E ) with |V | = n – that is, such that for some (cid:96) ≤ n,
y(cid:96) ∈ {−1, 1}(cid:96) is a labelling deﬁned on the (cid:96) vertices V(cid:96) = {vi1 , vi2 , . . . , vi(cid:96) } – the minimum
semi-norm interpolant is deﬁned by
¯y = argmin{uT Gu : u ∈ IRn , uik = yk , k = 1, . . . , (cid:96)}.

We then predict using ˆyi = sgn( ¯yi ), for i = 1, . . . , n.
The common justiﬁcation behind the above learning paradigm [14, 2] is that minimizing the cut (1)
encourages neighbouring vertices to be similarly labelled. However, we now demonstrate that in the
√
online setting such a regime will perform poorly on certain graph constructions – there exists a trial
sequence on which the method will make at least θ(
n) mistakes.
Deﬁnition 2. An octopus graph of size d is deﬁned to be d path graphs (the tentacles) of length d
(that is, with d + 1 vertices) all adjoined at a common end vertex, to which a further single head
vertex is attached, so that n = |V | = d2 + 2. This corresponds to the graph O1,d,d discussed in [8].
which online minimum semi-norm interpolation makes θ((cid:112)|V |) mistakes.
Theorem 3. Let G = (V , E ) be an octopus graph of size d and y = (y1 , . . . , y|V | ) the labelling
such that yi = 1 if vi is the head vertex and yi = −1 otherwise. There exists a trial sequence for
Proof. Let the ﬁrst query vertex be the head vertex, and let the end vertex of a tentacle be queried at
¯y (that is, for every unlabeled vertex vj , (cid:80)n
each subsequent trial. We show that this strategy forces at least d mistakes. The solution to the min-
imum semi-norm interpolation with boundary values problem is precisely the harmonic solution [4]
i=1 Aij ( ¯yi − ¯yj ) = 0). If the graph is connected ¯y is
unique and the graph labelling problem is identical to that of identifying the potential at each vertex
of a resistive network deﬁned on the graph where each edge corresponds to a resistor of 1 unit; the
harmonic principle corresponds to Kirchoff ’s current law in this case. Using this analogy, suppose
that the end points of k < d tentacles are labelled and that the end vertex vq of an unlabelled tentacle
is queried. Suppose a current of kλ ﬂows from the head to the body of the graph. By Kirchoff ’s
law, a current of λ ﬂows along each labelled tentacle (in order to obey the harmonic principle at
every vertex it is clear that no current ﬂows along the unlabelled tentacles). By Ohm’s law λ = 2
d+k .
Minimum semi-norm interpolation therefore results in the solution
¯yq = 1 − 2k
≥ 0 iﬀ k ≤ d.
d + k
Hence the minimum semi-norm solution predicts incorrectly whenever k < d and the algorithm
makes at least d mistakes.
The above demonstrates a limitation in the method of online Laplacian minimum semi-norm inter-
polation for predicting a graph labelling – the mistake bound can be proportional to the square root
of the number of data points. We solve these problems in the following section.

4 A linear graph embedding
We demonstrate a method of embedding data represented as a connected graph G into a path graph,
we call it a spine of G , which partially preserves the structure of G . Let Pn be the set of path graphs
with n vertices. We would like to ﬁnd a path graph with the same vertex set as G , which solves
ΦP (u)
max
minP ∈ Pn
ΦG (u) .
u∈{−1,1}n
If a Hamiltonian path H of G (a path on G which visits each vertex precisely once) exists, then
ΦG (u) ≤ 1. The problem of ﬁnding a Hamiltonian path is NP-complete
the approximation ratio is ΦH (u)
however, and such a path is not guaranteed to exist. As we shall see, a spine S of G may be found
ΦG (u) ≤ 2.
efﬁciently and satisﬁes ΦS (u)
We now detail the construction of a spine of a graph G = (V , E ), with |V | = n. Starting from
any node, G is traversed in the manner of a depth-ﬁrst search (that is, each vertex is fully explored
before backtracking to the last unexplored vertex), and an ordered list VL = {vl1 , vl2 , . . . , vl2m+1 }
of the vertices (m ≤ |E |) in the order that they are visited is formed, allowing repetitions when
a vertex is visited more than once. Note that each edge in EG is traversed no more than twice
(cid:80)
(cid:80)
when forming VL . Deﬁne an edge multiset EL = {(l1 , l2 ), (l2 , l3 ), . . . , (l2m , l2m+1 )} – the set
of pairs of consecutive vertices in VL . Let u be an arbitrary labelling of G and denote, as usual,
(i,j )∈EL (ui − uj )2 . Since the multiset EL
(i,j )∈EG (ui − uj )2 and ΦL (u) = 1
ΦG (u) = 1
4
4
contains every element of EG no more than twice, ΦL (u) ≤ 2ΦG (u).
We then take any subsequence V (cid:48)
L of VL containing every vertex in V exactly once. A spine
S = (V , ES ) is a graph formed by connecting each vertex in V to its immediate neighbours in

L with an edge. Since a cut occurs between connected vertices vi and vj in S
the subsequence V (cid:48)
only if a cut occurs on some edge in EL located between the corresponding vertices in the list VL
we have

ΦS (u) ≤ ΦL (u) ≤ 2ΦG (u).
(3)
Thus we have reduced the problem of learning the cut on a generic graph to that of learning the
cut on a path graph. In the following we see that 1-nearest neighbour (1-NN) algorithm is a Bayes
√
optimal algorithm for this problem. Note that the 1-NN algorithm does not perform well on general
graphs; on the octopus graph discussed above, for example, it can make at least θ(
n) mistakes,
and even θ(n) mistakes on a related graph construction [8].

5 Predicting with a spine

We consider implementing the 1-NN algorithm on a path graph and demonstrate that it achieves a
mistake bound which is logarithmic in the length of the line. Let G = (V , E ) be a path graph, where
V = {v1 , v2 , . . . , vn} is the set of vertices and E = {(1, 2), (2, 3), . . . , (n − 1, n)}. The nearest
neighbour algorithm, in the standard online learning framework described above, attempts to predict
a graph labelling by producing, for each query vertex vit , the prediction ˆyt which is consistent with
the label of the closest labelled vertex (and predicts randomly in the case of a tie).
Theorem 4. Given the task of predicting the labelling of any unweighted, n-vertex path graph P in
(cid:18) n − 1
(cid:19)
the online framework, the number of mistakes, M , incurred by the 1-NN algorithm satisﬁes
ΦP (u)
M ≤ ΦP (u) log2
+
+ 1,
ln 2
ΦP (u)
where u ∈ {−1, 1}n is any labelling consistent with the trial sequence.

(4)

MH ≤ log2

Proof. We shall prove the result by noting that the Halving algorithm [1] (under certain conditions
on the probabilities assigned to each hypothesis) implements the nearest neighbour algorithm on a
path graph. Given any input space X and ﬁnite binary concept class C ⊂ {−1, 1}|X | , the Halving
algorithm learns any target concept c∗ ∈ C as follows. Each hypothesis c ∈ C is given an associated
probability p(c). A sequence of labelled examples {(x1 , y1 ), . . . , (xt−1 , yt−1 )} ⊂ X × {−1, 1}, is
revealed in accordance with the usual online framework. Let Ft be the set of feasible hypotheses at
P
trial t; Ft = {c : c(xs ) = ys ∀s < t}. Given an unlabelled example xt ∈ X at trial t the predicted
P
p(c)
c∈Ft ,c(xt )= ˆyt
label ˆyt is that which agrees with the majority vote – that is, such that
2 (and
> 1
p(c)
c∈Ft
(cid:18) 1
(cid:19)
2 ). It is well known [1] that the Halving algorithm makes at
it predicts randomly if this is equal to 1
most MH mistakes with
p(c∗ )
We now deﬁne a probability distribution over the space of all labellings u ∈ {−1, 1}n of P such that
the Halving algorithm with these probabilities implements the nearest neighbour algorithm. Let a cut
occur on any given edge with probability α, independently of all other cuts; Prob(ui+1 (cid:54)= ui ) = α
∀i < n. The position of all cuts ﬁxes the labelling up to ﬂipping every label, and each of these
two resulting possible arrangements are equally likely. This recipe associates with each possible
labelling u ∈ {−1, 1}n a probability p(u) which is a function of the labelling’s cut size
1
2 αΦP (u) (1 − α)n−1−ΦP (u) .
This induces a full joint probability distribution on the space of vertex labels. In fact (6) is a Gibbs
measure and as such deﬁnes a Markov random ﬁeld over the space of vertex labels [10]. The mass
function p therefore satisﬁes the Markov property
p(ui = γ | uj = γj ∀j (cid:54)= i) = p(ui = γ | uj = γj ∀j ∈ Ni ),
(7)
where here Ni is the set of vertices neighbouring vi – those connected to vi by an edge. We will
give an equivalent Markov property which allows a more general conditioning to reduce to that over
boundary vertices.

p(u) =

.

(5)

(6)

Deﬁnition 5. Given a path graph P = (V , E ), a set of vertices V (cid:48) ⊂ V and a vertex vi ∈ V , we
deﬁne the boundary vertices v(cid:96) , vr (either of which may be vacuous) to be the two vertices in V (cid:48) that
are closest to vi in each direction along the path; its nearest neighbours in each direction.

(8)

(9)

(1 − (1 − 2α)|k−j | ).

(10)

(1 + (1 − 2α)|k−j | ).

The distribution induced by (6) satisﬁes the following Markov property; given a partial labelling of
P deﬁned on a subset V (cid:48) ⊂ V , the label of any vertex vi is independent of all labels on V (cid:48) except
those on the vertices v(cid:96) , vr (either of which could be vacuous)
p(ui = γ | uj = γj , ∀j : vj ∈ V (cid:48) ) = p(ui = γ | u(cid:96) = γ(cid:96) , ur = γr ).
Given the construction of the probability distribution formed by independent cuts on graph edges,
we can evaluate conditional probabilities. For example, p(uj = γ | uk = γ ) is the probability of an
are (cid:0)|k−j |
(cid:1) possible arrangements of s cuts we have
even number of cuts between vertex vj and vertex vk . Since cuts occur with probability α and there
(cid:19)
(cid:18)|k − j |
p(uj = γ | uk = γ ) = (cid:88)
s
αs (1 − α)|k−j |−s =
s
(cid:19)
(cid:18)|k − j |
p(uj (cid:54)= γ | uk = γ ) = (cid:88)
s even
Likewise we have that
1
αs (1 − α)|k−j |−s =
2
s
s odd
2 for γ ∈ {−1, 1}.
Note also that for any single vertex we have p(ui = γ ) = 1
Lemma 6. Given the task of predicting the labelling of an n-vertex path graph online, the Halving
algorithm, with a probability distribution over the labellings deﬁned as in (6) and such that 0 <
2 , implements the nearest neighbour algorithm.
α < 1
Proof. Suppose that t − 1 trials have been performed so that we have a partial labelling of a subset
V (cid:48) ⊂ V , {(vi1 , y1 ), (vi2 , y2 ), . . . , (vit−1 , yt−1 )}. Suppose the label of vertex vit is queried so that
the Halving algorithm makes the following prediction ˆyt for vertex vit : ˆyt = y if p(uit = y | uij =
2 , ˆyt = −y if p(uit = y | uij = yj ∀ 1 ≤ j < t) < 1
yj ∀ 1 ≤ j < t) > 1
2 (and predicts randomly
if this probability is equal to 1
2 ). We ﬁrst consider the case where the conditional labelling includes
vertices on both sides of vit . We have, by (8), that
p(uit = y | uij = yj ∀ 1 ≤ j < t) = p(uit = y | u(cid:96) = yτ ((cid:96)) , ur = yτ (r) )
p(u(cid:96) = yτ ((cid:96)) | ur = yτ (r) , uit = y)p(ur = yτ (r) , uit = y)
=
p(u(cid:96) = yτ ((cid:96)) , ur = yτ (r) )
p(u(cid:96) = yτ ((cid:96)) | uit = y)p(ur = yτ (r) | uit = y)
p(u(cid:96) = yτ ((cid:96)) | ur = yτ (r) )
where v(cid:96) and vr are the boundary vertices and τ ((cid:96)) and τ (r) are trials at which vertices v(cid:96) and vr
are queried, respectively. We can evaluate the right hand side of this expression using (9, 10). To
2 , we have from (9, 10, 11)
show equivalence with the nearest neighbour method whenever α < 1
(1 + (1 − 2α)|(cid:96)−it | )(1 − (1 − 2α)|r−it | )
p(uit = y | u(cid:96) = y , ur (cid:54)= y) =
2(1 − (1 − 2α)|(cid:96)−r| )
2 if |(cid:96) − it | < |r − it | and less than 1
2 if |(cid:96) − it | > |r − it |. Hence, this
which is greater than 1
produces predictions exactly in accordance with the nearest neighbour scheme. We also have more
simply that for all it , (cid:96) and r and α < 1
2
1
p(uit = y | u(cid:96) = y , ur = y) >
2 , and p(uit = y | u(cid:96) = y) >
This proves the lemma for all cases.
(cid:19)
(cid:18) 1
(cid:19)
(cid:18)
A direct application of the Halving algorithm mistake bound (5) now gives
2
M ≤ log2
αΦP (u) (1 − α)n−1−ΦP (u)
p(u)

= log2

=

(11)

1
2

1
2 .

where u is any labelling consistent with the trial sequence. We choose α = min( ΦP (u)
2 ) (note
n−1 , 1
(cid:18)
(cid:18) n − 1
(cid:19)
(cid:19)
that the bound is vacuous when ΦP (u)
2 since M is necessarily upper bounded by n) giving
n−1 > 1
(cid:18) n − 1
(cid:19)
ΦP (u)
+ (n − 1 − ΦP (u)) log2
1 +
n − 1 − ΦP (u)
ΦP (u)
ΦP (u)
ΦP (u)
ln 2

M ≤ ΦP (u) log2
≤ ΦP (u) log2

+ 1

+

+ 1.

+ 1,

This proves the theorem.
The nearest neighbour algorithm can predict the labelling of any graph G = (V , E ), by ﬁrst trans-
ferring the data representation to that of a spine S of G , as presented in Section 4. We now apply the
above argument to this method and immediately deduce our ﬁrst main result.
Theorem 7. Given the task of predicting the labelling of any unweighted, connected, n-vertex graph
G = (V , E ) in the online framework, the number of mistakes, M , incurred by the nearest neighbour
(cid:20)
(cid:18) n − 1
(cid:19)(cid:21)
algorithm operating on a spine S of G satisﬁes
2ΦG (u)
M ≤ 2ΦG (u) max
0, log2
+
ln 2
2ΦG (u)
where u ∈ {−1, 1}n is any labelling consistent with the trial sequence.

(12)
(cid:16) n−1
(cid:17)
Proof. Theorem 4 gives bound (4) for predicting on any path, hence M ≤ ΦS (u) log2
+
ΦS (u)
ln 2 + 1. Since this is an increasing function of ΦS (u) for ΦS (u) ≤ n − 1 and is vacuous at
ΦS (u)
ΦS (u) ≥ n − 1 (M is necessarily upper bounded by n) we upper bound substituting ΦS (u) ≤
2ΦG (u) (equation (3)).
We observe that predicting with the spine is a minimax improvement over Laplacian minimal semi-
to θ((cid:112)ΦG (u)n) mistakes by creating a colony of ΦG (u) octopi then identifying each previously
√
norm interpolation. Recall Theorem 3, there we showed that there exists a trial sequence such that
Laplacian minimal semi-norm interpolation incurs θ(
n) mistakes. In fact this trivially generalizes
separate head vertex as a single central vertex. The upper bound (12) is smaller than the prior lower
bound.
The computational complexity for this algorithm is O(|E | + |V | ln |V |) time. We compute the spine
in O(|E |) time by simply listing vertices in the order in which they are ﬁrst visited during a depth-
ﬁrst search traversal of G . Using online 1-NN requires O(|V | ln |V |) time to predict an arbitrary
vertex sequence using a self-balancing binary search tree (e.g., a red-black tree) as the insertion of
each vertex into the tree and determination of the nearest left and right neighbour is O(ln |V |).

6 Prediction with a binary support tree
The Pounce online label prediction algorithm [7] is designed to exploit cluster structure of a graph
G = (V , E ) and achieves the following mistake bound
M ≤ N (X, ρ, rG ) + 4ΦG (u)ρ + 1,
(13)
for any ρ > 0. Here, u ∈ IRn is any labelling consistent with the trial sequence, X =
{vi1 , vi2 , . . . } ⊆ V is the set of inputs and N (X, ρ, rG ) is a covering number – the minimum
number of balls of resistance diameter ρ (see Section 2) required to cover X . The mistake bound
(13) can be preferable to (12) whenever the inputs are sufﬁciently clustered and so has a cover of
small diameter sets. For example, consider two (m + 1)-cliques, one labeled “+1”, one “−1” with
cm arbitrary interconnecting edges (c ≥ 1) here the bound (12) is vacuous while (13) is M ≤ 8c + 3
m , N (X, ρ, rG ) = 2, and ΦG (u) = cm). An input space V may have both local clus-
(with ρ = 2
ter structure yet have a large diameter. Imagine a “universe” such that points are distributed into
many dense clusters such that some sets of clusters are tightly packed but overall the distribution is
quite diffuse. A given “problem” X ⊆ V may then be centered on a few clusters or alternatively
encompass the entire space. Thus, for practical purposes, we would like a prediction algorithm

which achieves the “best of both worlds”, that is a mistake bound which is no greater, in order of
magnitude, than the maximum of (12) and (13). The rest of this paper is directed toward this goal.
We now introduce the notion of binary support tree, formalise the Pounce method in the support tree
setting and then prove the desired result.
Deﬁnition 8. Given a graph G = (V , E ), with |V | = n, and spine S , we deﬁne a binary support tree
of G to be any binary tree T = (VT , ET ) of least possible depth, D , whose leaves are the vertices
of S , in order. Note that D < log2 (n) + 1.
We show that there is a weighting of the support tree which ensures that the resistance diameter of
the support tree is small, but also such that any labelling of the leaf vertices can be extended to the
support tree such that its cut size remains small. This enables effective learning via the support tree.
A related construction has been used to build preconditioners for solving linear systems [6].
Lemma 9. Given any spine graph S = (V , E ) with |V | = n, and labelling u ∈ {−1, 1}n , with
support tree T = (VT , ET ), there exists a weighting A of T , and a labelling ¯u ∈ [−1, 1]|VT |
of T such that ¯u and u are identical on V , ΦT ( ¯u) < ΦS (u) and RT ≤ (log2 n + 1)(log2 n +
4)(log2 (log2 n + 2))2 .
Proof. Let vr be the root vertex of T . Suppose each edge (i, j ) ∈ ET has a weight Aij , which
is a function of the edge’s depth d = max{dT (vi , vr ), dT (vj , vr )}, Aij = W (d) where dT (v , v (cid:48) )
is the number of edges in the shortest path from v to v (cid:48) . Consider the unique labelling ¯u such
that, for 1 ≤ i ≤ n we have ¯ui = ui and such that for every other vertex vp ∈ VT , with child
vertices vc1 , vc2 , we have ¯up = ¯uc1 + ¯uc2
, or ¯up = ¯uc in the case where vp has only one child, vc .
Suppose the edges (p, c1 ), (p, c2 ) ∈ ET are at some depth d in T , and let V (cid:48) ⊂ V correspond to
2
the leaf vertices of T descended from vp . Deﬁne ΦS (uV (cid:48) ) to be the cut of u restricted to vertices
in V (cid:48) . If ¯uc1 = ¯uc2 then ( ¯up − ¯uc1 )2 + ( ¯up − ¯uc2 )2 = 0 ≤ 2ΦS (uV (cid:48) ), and if ¯uc1 (cid:54)= ¯uc2 then
W (d) (cid:0)( ¯up − ¯uc1 )2 + ( ¯up − ¯uc2 )2 (cid:1) ≤ 2W (d)ΦS (uV (cid:48) )
( ¯up − ¯uc1 )2 + ( ¯up − ¯uc2 )2 ≤ 2 ≤ 2ΦS (uV (cid:48) ). Hence
(14)
(a similar inequality is trivial in the case that vp has only one child). Since the sets of leaf descendants
of all vertices at depth d form a partition of V , summing (14) ﬁrst over all parent nodes at a given
D(cid:88)
depth and then over all integers d ∈ [1, D] gives
4ΦT ( ¯u) ≤ 2
d=1

W (d)ΦS (u).

(15)

(16)

We then choose
1
2 + ln2 2 (cid:82) ∞
and note that (cid:80)∞
W (d) =
(d + 1)(log2 (d + 1))2
Further, RT = 2 (cid:80)D
(d+1)(log2 (d+1))2 ≤ 1
x ln2 x dx = 1
2 + ln 2 < 2.
1
1
d=1
2
d=1 (d + 1)(log2 (d + 1))2 ≤ D(D + 3)(log2 (D + 1))2 and so D ≤ log2 n + 1
gives the resistance bound.
Deﬁnition 10. Given the task of predicting the labelling of an unweighted graph G = (V , E ) the
augmented Pounce algorithm proceeds as follows: An augmented graph ¯G = ( ¯V , ¯E ) is formed
by attaching a binary support tree of G , with weights deﬁned as in (16), to G ; formally let T =
(VT , ET ) be such a binary support tree of G , then ¯G = (VT , E ∪ ET ). The Pounce algorithm is
then used to predict the (partial) labelling deﬁned on ¯G .
Theorem 11. Given the task of predicting the labelling of any unweighted, connected, n-vertex
graph G = (V , E ) in the online framework, the number of mistakes, M , incurred by the augmented
Pounce algorithm satisﬁes
M ≤ min
{N (X, ρ, rG ) + 12ΦG (u)ρ} + 1,
(17)
ρ>0
where N (X, ρ, rG ) is the covering number of the input set X = {vi1 , vi2 , . . . } ⊆ V relative to
the resistance distance rG of G and u ∈ IRn is any labelling consistent with the trial sequence.
Furthermore,
M ≤ 12ΦG (u)(log2 n + 1)(log2 n + 4)(log2 (log2 n + 2))2 + 2.

(18)

Proof. Let u be some labelling consistent with the trial sequence. By (3) we have that ΦS (u) ≤
2ΦG (u) for any spine S of G . Moreover, by the arguments in Lemma 9 there exists some labelling
¯u of the weighted support tree T of G , consistent with u on V , such that ΦT ( ¯u) < ΦS (u). We then
have

Φ ¯G ( ¯u) = ΦT ( ¯u) + ΦG (u) < 3ΦG (u).
(19)
By Rayleigh’s monotonicity law the addition of the support tree does not increase the resistance
between any vertices on G , hence
N (X, ρ, r ¯G ) ≤ N (X, ρ, rG ).
Combining inequalities (19) and (20) with the pounce bound (13) for predicting ¯u on ¯G , yields
M ≤ N (X, ρ, r ¯G ) + 4Φ ¯G ( ¯u)ρ + 1 ≤ N (X, ρ, rG ) + 12ΦG (u)ρ + 1.
which proves (17). We prove (18) by covering ¯G with single ball so that M ≤ 4Φ ¯G ( ¯u)R ¯G + 2 ≤
12ΦG (u)RT + 2 and the result follows from the bound on RT in Lemma 9.

(20)

7 Conclusion
We have explored a deﬁciency with existing online techniques for predicting the labelling of a graph.
As a solution, we have presented an approximate cut-preserving embedding of any graph G =
(V , E ) into a simple path graph, which we call a spine, such that an implementation of the 1-
nearest-neighbours algorithm is an efﬁcient realisation of a Bayes optimal classiﬁer. This therefore
achieves a mistake bound which is logarithmic in the size of the vertex set for any graph, and the
complexity of our algorithm is of O(|E | + |V | ln |V |). We further applied the insights gained to
a second algorithm – an augmentation of the Pounce algorithm, which achieves a polylogarithmic
performance guarantee, but can further take advantage of clustered data, in which case its bound is
relative to any cover of the graph.

References
[1] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady,
13:1224–1228, 1972.
[2] M. Belkin and P. Niyogi. Semi-supervised learning on riemannian manifolds. Machine Learning, 56:209–
239, 2004.
[3] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19–26. Morgan Kaufmann, San Francisco, CA, 2001.
[4] P. Doyle and J. Snell. Random walks and electric networks. Mathematical Association of America, 1984.
[5] J. Fakcharoenphol and B. Kijsirikul. Low congestion online routing and an improved mistake bound for
online prediction of graph labeling. CoRR, abs/0809.2075, 2008.
[6] K. Gremban, G. Miller, and M. Zagha. Performance evaluation of a new parallel preconditioner. Parallel
Processing Symposium, International, 0:65, 1995.
[7] M. Herbster. Exploiting cluster-structure to predict the labeling of a graph. In The 19th International
Conference on Algorithmic Learning Theory, pages 54–69, 2008.
In B. Sch ¨olkopf, J. Platt, and
[8] M. Herbster and M. Pontil. Prediction on a graph with a perceptron.
T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 577–584. MIT Press,
Cambridge, MA, 2007.
[9] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML ’05: Proceedings of the
22nd international conference on Machine learning, pages 305–312, New York, NY, USA, 2005. ACM.
[10] R. Kinderman and J. L. Snell. Markov Random Fields and Their Applications. Amer. Math. Soc., Provi-
dence, RI, 1980.
[11] D. Klein and M. Randi ´c. Resistance distance. Journal of Mathematical Chemistry, 12(1):81–95, 1993.
[12] N. Littlestone. Learning when irrelevant attributes abound: A new linear-threshold algorithm. Machine
Learning, 2:285–318, 1988.
[13] K. Pelckmans and J. A. Suykens. An online algorithm for learning a labeling of a graph. In In Proceedings
of the 6th International Workshop on Mining and Learning with Graphs, 2008.
[14] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic
functions. In 20-th International Conference on Machine Learning (ICML-2003), pages 912–919, 2003.


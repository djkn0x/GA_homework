CS229 Final Project, Dec 2008 

1

Protein Secondary Structure Prediction  
based on Neural Network Models  
and Support Vector Machines 

 

Jaewon Yang 
Departments of Electrical Engineering, Stanford University 
jaewony@stanford.edu 
these  problems  [1].  Research  in  computational  structure 
prediction  concerns  itself  mainly  with  predicting  secondary 
structure  from  known  experimentally  determined  primary 
structure. This is due to the relative ease of determining primary 
structure and the complexity involved in tertiary structure.  
The  secondary-structure prediction  approaches  in  today  can 
be categorized into three groups: neighbor-based, model-based, 
and  metapredictor-based  [2].  The  neighbor-based  approaches   
predict  the  secondary  structure  by  identifying  a  set  of  similar 
sequence  fragments  with  known  secondary  structure;  the 
model-based  approaches  employ  sophisticated  machine 
learning  techniques  to  learn  a  predictive  model  trained  on 
sequences  of  known  structure,  whereas  the  metapredictor 
-based approaches predict based on a combination of the results 
of various neighbor and/or model-based techniques.  
Historically,  the  most  successful  model-based  approaches, 
such  as  PSIPRED  [4]  were  based  on  neural  network  (NN) 
learning  techniques  [5].  However,  in  recent  years,  secondary 
structure  prediction  algorithms  based  on  support  vector 
machines  have  been  developed  and  have  been  showing  good 
performance  [7].  In  this  paper,  these  two  successful  methods 
will be compared.  
 

Abstract  The  prediction  of  protein  secondary  structure  is  an 
important  step  in  the  prediction  of  protein  tertiary  structure. 
Protein  tertiary  structure  prediction  is  of  great  interest  to 
biologists because proteins are able  to perform  their  functions by 
coiling their amino acid sequences into specific three-dimensional 
shapes  (tertiary  structure).  Therefore,  this  subject  is  of  high 
importance  in  medicine  (e.g.  drug  design)  and  biotechnology. 
Instead  of  costly  and  time-consuming  experimental  approaches, 
effective  methods  have  been  developed  continuously.  The 
secondary-structure  prediction  approaches  in  use  today  can  be 
categorized  into  three  groups:  neighbor-based, model-based,  and 
metapredictor-based  approaches.  The  model-based  approaches 
employ sophisticated machine  learning  techniques such as neural 
networks, hidden markov models, and support vector machines to 
learn a predictive model trained on sequences of known structure.  
With  the  help  of  growing  databases  and  the  evolutionary 
information  available 
from  multiple-sequence  alignments, 
resources  for  secondary  structure  prediction  became  abundant. 
However,  this  paper  focused  on  single-  sequence  prediction  in 
order 
to  compare  algorithmic  efficiency  and 
to 
save 
computational-time.  In  this  paper,  the  neural  network  and  the 
support vector machine based algorithms will be compared.  
Keywords:  protein  structure  prediction/  secondary  structure/  neural 
network/back-propagation/ support vector machines 

I.  INTRODUCTION 

 

Protein  structure  prediction  is  one  of  the  most  important 
goals pursued by bioinformatics and theoretical chemistry. This 
subject  is  of  great  interest  to  biologists  because  proteins  are 
able  to  perform  their  functions  by  coiling  their  amino  acid 
sequences  (primary  structure)  into  specific  three-dimensional 
shapes  (tertiary  structure)  –  this  process  is  called  protein 
folding.  In  other  words,  the  linear  ordering  of  amino  acids 
forms  secondary  structure,  arranging  secondary  structures 
yields tertiary structure. Therefore, protein structure prediction 
is  of  high  importance  in  medicine  (e.g.  drug  design)  and 
biotechnology (e.g. the design of novel enzymes). 
A  number  of  factors  exists  that  make  protein  structure 
prediction a very difficult task. Two main problems are that the 
number  of  possible  protein  structures  is  extremely  large,  and 
that the physical basis of protein structural stability is not fully 
understood.  In  this  sense,  the  techniques  such  as  spectroscopy 
and far-ultraviolet (far-UV, 170-250 nm) circular dichroism for 
structure  prediction  are 
time-consuming  and  expensive. 
However, due to the increase in computer power and especially 
new  algorithms,  much  progress  is  being  made  to  overcome 

 

II.  METHOD 

A.  Database 
 
a.  Definition  
 
following  structural  classes:    H  (α-helix),  G  ( 3(cid:2869)(cid:2868) -helix),  I 
No  unique  method  of  assigning  residues  to  a  particular 
secondary-structure  exists,  although  the most widely  accepted 
protocol  is  based  on  the  DSSP  algorithm.  DSSP  uses  the 
(π-helix), E (β-strand), B (isolated-β bridge), T (turn), S (bend), 
and – (other). In this paper, the reduction scheme that converts 
this  eight-state  assignment  to  three  states  by  assigning  H    the 
helix state (H),  E to the strand state (E), and the rest (I,T,S and 
-)  to  a  coil  state  (C).  This  is  the  simplest  format  used  in 
structure databases. 
 

b.  Training and testing sets[3] 
 
Cross-validation appears to remove the problem of a limited 
data  set  for  training  and  test.  However,  artificially  high 

 

1

CS229 Final Project, Dec 2008 

2

accuracies  can  be  obtained  if  the  set  of  proteins  used  in 
cross-validation  show  sequence  similarity  to  each  other. 
Accordingly,  cross-validation  sets  must  be  pruned  stringently 
to remove internal sequence similarities, but if it is not possible, 
then a completely independent test set must be used. Therefore, 
in this paper, the hold-out cross validation technique, where test 
proteins  are  removed  from  the  training  set,  was  used.  The 
training  data  set  is  the  CB396  and  the  RS126  set  is  used  for 
testing.  The  11  pairs  which  showed  up  homologies  in  the 
CB396  set  were  removed  from  the  RS126  set,  and  protein 
chains of <30  residues were  also  removed.  In  conclusion, 382 
proteins (61455 residues) from CB396 set for training and 115 
protein chains (21755 residues) from RS126 set for testing were 
used. 
 

Secondary structure fractions 
Total number of residues for training = 61455 
Total number of residues for testing = 21755 
Test set(%) 
Train set(%) 
 
H(α-helix) 
28.46 
31.87 
E(β-sheet) 
21.56 
21.15 
–(others) 
46.56 
50.37 
 
Table 1. Proteins in training and testing set 

c.  Performance measures 
 
There  are  many  ways  to  access  the  performance  of  the 
method  for  predicting  secondary  structures.  The  most 
commonly used measure  is  a  simple  success  rate, Q3, which is 
Q(cid:2871) (cid:3404)   ∑
  (cid:3400)  100
the percentage of correctly predicted residues on three types of 
(cid:2913)(cid:2925)(cid:2928)(cid:2928)(cid:2915)(cid:2913)(cid:2930)(cid:2922)(cid:2935) (cid:2926)(cid:2928)(cid:2915)(cid:2914)(cid:2919)(cid:2913)(cid:2930)(cid:2915)(cid:2914)(cid:3167)
secondary structures: 
(cid:4666)(cid:2919)(cid:2880)H,E,C(cid:4667)
(cid:2925)(cid:2912)(cid:2929)(cid:2915)(cid:2928)(cid:2932)(cid:2915)(cid:2914)(cid:3167)
 
(%) 
B.  Neural Network 

 

 

 

The  neural  network  is  trained  using  the  supervised  learning 
method.  Here  the  training  process  is  finding  the  appreciate 
value  for  the weight of  each  layer  in  the network  to maximize 
accuracy  of  prediction.  In  supervised  learning,  a  training  data 
set is encoded into feature vectors combined with correct class 
labels,  such  as  helix,  sheet,  or  coil.  The  PSIPRED  method  by 
Jones (1999) is a successful approach for predicting secondary 
structure  [4].  In  PSIPRED,  a  two-stage  neural  network  was 
used based on  the position-specific scoring matrices generated 
by  PSI-BLAST.  This  paper  is  based  on  the  algorithm  of 
PSIPRED,  but  instead  of  applying  PSSM  (Position-specific 
Scoring  Matrices)  into  input,  single  sequence  prediction 
method is used in order to focus on the algorithm and to avoid 
expensive computational time.  
 

a.  Neural network and properties 

A feedforward network is composed of two or more layers of 
processing  units.  The  first  is  the  input  layer,  the  last  is  the 
output layer, and all the other layers between are termed hidden 
layers.  The  state  of  each  unit  has  a  real  value  in  the  range 
between 0 and 1. The all input units (I) that form an input vector 

 

 

 

 

are  determined  by  an  input  window  of  amino  acid  residues 
through an input coding scheme. Starting from the hidden layer 
O(cid:2919) (cid:3404) ∑ v(cid:2919)(cid:2920) ·
                      
(h)  and  moving  toward  the  output  layer  (O),  the  state  of  each 
(cid:2869)
hidden unit i in the network is determined by: 
(cid:2920)
(cid:2869)(cid:2878)(cid:2915)(cid:2934)(cid:2926) (cid:4666)(cid:2879)(cid:2918)(cid:3168) (cid:4667)
h(cid:2919) (cid:3404)   ∑ w(cid:2919)(cid:2920) · I(cid:2920)
 
(cid:2920)
 
                      
 
  
 
The  back-propagation  learning  algorithm  can  be  used  in 
networks  with  hidden  layers  to  find  a  set  of  weights  that 
performs  correct  mapping  between  sequences  and  structures. 
Starting with  an  initial  set  of  randomly  assigned  numbers,  the 
weights  are  altered  by  gradient  descent  to  minimize  the  error 
between the desired and the actual output vectors. 
 

(1) 

(2) 

 

 

 

Figure 1. The 1st neural network design 

 

b.  Network design 

The neural network was trained with different sequence and 
structural  information  using  a  sliding  window  scheme  which 
was  also  used  in  the  SVMmulticlass  and  SVMhmm.  In  the  sliding 
window method, a window becomes one training pattern for the 
predicting structure of  the residue at  the center of  the window. 
In  many  input-encoding  methods,  orthogonal  encoding  was 
used.  Each  residue  has  a  unique  binary  vector,  such  as 
(1,0,0,…),(0,1,0,…),(0,0,1,…),  …  ,(0,…,0,1)  in  orthogonal 
encoding. Each binary vector  is 21-dimensional  that means 21 
amino  acids.  In  this  method,  the  weights  of  all  residues  in  a 
window are assigned to 1.  
For  a  given  input  and  set  of  weights,  the  output  of  the 
network  will  be  a  set  of  numbers  between  0  and  1.  The 
secondary  structure  chosen  was  the  output  unit  that  had  the 
highest  activity  level;  this  was  equivalent  to  choosing  the 
output unit  that had  the  least mean square error with  the  target 
output. 
The  performance  of  the  network  on  the  training  and  testing 
sets  depends  on  many  variables,  including,  the  number  of 
training  examples,  the number of hidden units,  and  the  size of 
window.  The  ability  of  a  network  to  extract  higher  order 
features  from  the  training  set  depends  on  the  layer  of  hidden 
units and the types of encoding scheme.  
 

C.  SVMhmm and SVMmulticlass 

 

2

CS229 Final Project, Dec 2008 

3

Among  the  many  machine  learning  approaches,  support 
vector machine (SVM) methods are the most recent to be used 
for  structure  prediction.  The  paper  [7]  designed  classifiers  for 
the  three  cluster  problems  based  on  the  binary  classifiers 
generated  by  SVMs.  However,  in  this  paper,  the  generalized 
multi-class SVMs were used [10]. Unlike the case of multiclass 
classification  where  output  space  with 
interchangeable, 
arbitrarily  numbered  labels,  structured  output  spaces  are 
considered  in  generalized  multiclass  SVMS.  I  used  SVMlight 
which  is  widely  used  software  implementations  of  SVM. 
SVM:  min(cid:2933),ξ    (cid:2869)(cid:2870) (cid:1313)w(cid:1313)(cid:2870) (cid:3397) C(cid:2924) ∑ ξ(cid:2919)
  , s. t. (cid:1482)i,   ξ(cid:2919) (cid:3410) 0     (3) 
SVMhmm  and  SVMmulticlass  are  applications  using  SVMlight  for 
(cid:2924)(cid:2919)(cid:2880)(cid:2869)
s. t. (cid:1482)i, (cid:1482)y (cid:1488) У: wTδΨ(cid:2919)(cid:4666)y(cid:4667) (cid:3410)   ∆(cid:4666)y(cid:2919) , y(cid:4667) (cid:3398)   ξ(cid:2919)          (4) 
generalized multiclass classification.   
 
where δΨ(cid:2919) (cid:4666)y(cid:4667)   (cid:1568) Ψ(cid:4666)x(cid:2919) , y(cid:2919) (cid:4667) (cid:3398)  Ψ(cid:4666)x(cid:2919) , y(cid:4667)             (5) 
 
described  in  [8].  For  a  training  set  (cid:4666)x(cid:2869) , y(cid:2869) (cid:4667) (cid:1710) (cid:4666)x(cid:2924) , y(cid:2924) (cid:4667)  with 
labels y(cid:2919)  in i = 1, 2, 3 in the secondary structure prediction. For 
 
SVMmulticlass  is  an  implementation  of  the  multi-class  SVM 
(really)  supported.  The  loss  function ∆(cid:4666)y(cid:2919) , y(cid:4667) is  the  number  of 
linear kernels, this is very fast and runtime scales linearly with 
the  number  of  training  examples.  Non-linear  kernels  are  not 
  min  (cid:2869)(cid:2870) (cid:1313)w(cid:1313)(cid:2870) (cid:3397) C(cid:2924) ∑ ξ(cid:2919)
misclassified tags. 
s. t. for all y (cid:1527)  x(cid:2869) · w(cid:2935)(cid:2919) (cid:3410) x(cid:2869) · w(cid:2935) (cid:3397)  100∆(cid:4666)y(cid:2869) , y(cid:4667) (cid:3398) ξ(cid:2869)   (7) 
 
(cid:2919)
(cid:1709) 
SVMmulticlass:  
 x(cid:2869) · w(cid:2935)(cid:2919) (cid:3410) x(cid:2869) · w(cid:2935) (cid:3397)  100∆(cid:4666)y(cid:2869) , y(cid:4667) (cid:3398) ξ(cid:2869)    (8) 
                                                     (6) 
x (cid:3404) (cid:4666)x(cid:2869) , x(cid:2870) , (cid:1710) , x(cid:2870)(cid:2869) (cid:4667) of feature vectors, the model predicts a tag 
 
sequence  y (cid:3404) (cid:4666) y(cid:2869) ,  y(cid:2870) , (cid:1710) , y(cid:2870)(cid:2869) (cid:4667)  according  to  the  following 
SVMhmm  is  an  implementation  of  structural  SVMs  for 
sequence  tagging  [9].  Given  an  observed  input  sequence 
(cid:4666)x(cid:2869) , y(cid:2869) (cid:4667), (cid:4666)x (cid:2870) , y (cid:2870) (cid:4667), (cid:1710) , (cid:4666)x(cid:2924) , y(cid:2924) (cid:4667) , 
is  x (cid:2920) (cid:3404)
(cid:4666)x(cid:2869)(cid:2920) , (cid:1710) , x(cid:2870)(cid:2869)(cid:2920) (cid:4667) with their correct tag sequence,  y (cid:2920) (cid:3404) (cid:4666)y(cid:2869)(cid:2920) , (cid:1710) , y(cid:2870)(cid:2869)(cid:2920) (cid:4667). 
the  given  training  examples   
optimization  problem.  For 
the  feature  vector 
For the secondary structure prediction, only one entry has 1 and 
min (cid:2869)(cid:2870) (cid:1313)w(cid:1313)(cid:2870) (cid:3397) C(cid:2924) ∑ ξ(cid:2919)
others are zero by orthogonal encoding.  
  s. t. for all y: 
 
(cid:2919)
SVMhmm: 
   ∑
, y(cid:2919)(cid:2869) (cid:4667) · w(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929)    
(cid:4672)x (cid:2919)(cid:2869) · w(cid:2935)(cid:3167)(cid:3117) (cid:4673) (cid:3397)   φ(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:4666)y(cid:2919)(cid:2879)(cid:2869)(cid:2869)
                                                           (9) 
(cid:2919)(cid:2880)(cid:2869),(cid:2870),…,(cid:2870)(cid:2869)
(cid:3410)   ∑
, y(cid:2919) (cid:4667) · w(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:3397)   ∆(cid:4666)y(cid:2869) , y(cid:4667)     (10) 
(cid:4672)x (cid:2919)(cid:2869) · w(cid:2935)(cid:3167) (cid:4673) (cid:3397)   φ(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:4666)y(cid:2919)(cid:2879)(cid:2869)
(cid:1709) 
(cid:2919)(cid:2880)(cid:2869),(cid:2870),…,(cid:2870)(cid:2869)
 
      ∑
(cid:4672)x(cid:2919)(cid:2924) · w(cid:2935)(cid:3167)(cid:3117) (cid:4673) (cid:3397)   φ(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:4666)y(cid:2919)(cid:2879)(cid:2869)(cid:2924)
, y(cid:2919)(cid:2924) (cid:4667) · w(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929)   
(cid:2919)(cid:2880)(cid:2869),(cid:2870),…,(cid:2870)(cid:2869)
(cid:3410)   ∑
, y(cid:2919) (cid:4667) · w(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:3397)   ∆(cid:4666)y(cid:2869) , y(cid:4667)   (11) 
(cid:4672)x (cid:2919)(cid:2924) · w(cid:2935)(cid:3167) (cid:4673) (cid:3397)   φ(cid:2930)(cid:2928)(cid:2911)(cid:2924)(cid:2929) (cid:4666)y(cid:2919)(cid:2879)(cid:2869)
(cid:2919)(cid:2880)(cid:2869),(cid:2870),…,(cid:2870)(cid:2869)
 
 
a.  Parameter optimization  

 

 

 

3

In generalized multi-class SVMs, we need  to select a kernel 
function  and  the  regularization  parameter  C.  The  primal 
formulation  of  the  generalized  soft-margin  SVMs  maximize 
margin  and minimize  training  error  simultaneously by  solving 
the previous optimization problem (3)-(5). 
 
b.  Dynamic programming for cutting-plane 
 
The key challenge  in solving  the quadratic problems  for  the 
constraints, n3L  because  each  amino  acid  can  be  one  of  three 
generalized  multi-class  SVM  learning  is  the  large  number  of 
margin  constraints.  If  the  length  of  each  amino  acid  is  L  and 
there  are  n  training  data,  we  have  exponential  number  of 
classes.  
   However, only a much smaller subset of constraints needs to 
be  explicitly  examined  by  using  cutting-plane.  The 
cutting-plane using dynamic  programming  (Viterbi algorithm) 
aims  at  finding  a  small  set  of  active  constraints  that  ensures  a 
sufficiently  accurate  solution.  This  method  can  reduce  the 
number  of  constraints  to  a  polynomial-sized  subset  of 
constraints 
fulfills  all 
the  corresponding  solution 
that 
constraints with a precision of ε. In other words, the remaining 
exponentially many constraints are guaranteed to be violated by 
no more  than ε, without  the need for explicitly adding  them  to 
the optimization problem.  
 

III.  RESULTS 

 

 

A.  Neural Network 

a.  Training and testing with increasing the number of 
iteration. 

Figure 2. Learning curve for real proteins  
with 75 hidden units and 15 window size. 

 

 

Further  training  improved  the  performance  of  the  networks 
with  hidden  units  on  the  training  set,  but  performance  on  the 
testing set did not improve but tended to decrease. This result is 
an indication that memorization of the detail of the training set 
is interfering with the ability of the network to generalize. The 

CS229 Final Project, Dec 2008 

4

peak  performance  for  a  network  with  15  window  size  and  75 
hidden units was Q3 = 67.42% after 30 iterations.  
 

b.  Dependence on the number of hidden units 

Hidden Units 
0 
25 
50 
75 
100 
125 

Q3(%) 
61.39 
63.68 
65.64 
67.42 
66.35 
66.91 

 
Table 2. Dependence of testing success on hidden units 

Table  2  shows  that  the  peak  performance  on  the  testing  set 
depends  on  the  number  of  hidden  units. Also,  it  is  shown  that 
having  more  hidden  units  is  not  always  good  because  it  can 
cause  high-variance  problem  as  mentioned  in  the  previous 
section. 
 

c.  Dependence on the size of window 

Window Size 
1 
3 
5 
7 
9 
11 
13 
15 
17 
19 
21 

Q3(%) 
50.37 
54.04 
59.51 
63.17 
65.40
65.09 
66.61 
67.04 
66.67 
66.64 
66.72 

 
Table 3. Dependence of testing success on window size 

Table 3 shows the dependence of testing accuracy rate on the 
size of the input window with 75 hidden units. The result shown 
in Table 3 indicates that when the size of the window was small 
the  performance  on  the  testing  set  was  reduced,  probably 
because information outside the window is not available for the 
prediction.  When  the  size  of  the  window  was  increased,  the 
performance  reached  a  maximum  at  around  15  window  size. 
For  larger  window  sizes,  the  performance  deteriorated, 
probably because of  the effects of extra weights  that could not 
contain  any  information  about  the  secondary  structure  of  the 
center.  Thus,  irrelevant  weights  can  interfere  with  the 
performance of the network.  
 

d.  2nd network  

 
Q3(%)  

1st network 
67.42 

2nd network 
67.45 

 
Table 4. Testing accuracy of the 1st and 2nd network 

 

 

 

 

 

 

input network comprising just 63 input units.  For this network, 
a smaller hidden layer of 75 units were used instead of 60 units 
of  the  PSIPRED.  Q3  performance  was  similar  but  showed  a 
little improvement.  
 

B.  SVMhmm and SVMmulticlass 

Non-linear  kernels  are  not  supported  in  the  SVMhmm  and 
SVMmulticlass. Without  the  optimization  of  kernel  parameters,  I 
focused  on  selecting  the  optimal  parameter  C  which  plays  a 
critical  role. Common  practice  is  to  choose  the  parameter  that 
maximizes  the  accuracy  by  using  hold-out  cross  validation 
method.  
 

C 
2 
4 
6 
8 
10 
12 
14 
16 
18 
20 

Q3(%) (W=1) Q3(%) (W=17) 
50.37 
61.83 
61.83 
47.24 
61.83 
47.24 
47.24 
63.28 
47.24 
63.13 
63.13 
47.24 
59.63 
47.24 
59.63 
47.24 
42.95 
59.63 
59.78 
42.95 

# SV(W=17)
4 
4 
4 
4 
3 
3 
4 
4 
4 
4 

 
Table 5. Dependence of the number of support vector and 
testing success on C(Regularized Parameter) (SVMmulticlass) 

The  optimal  window  size  of  the  encoding  scheme  for 
SVMmulticlass and SVMhmm was obtained by testing the accuracy 
for  the  various  window  sizes,  and  it  was  shown  that  17  for 
SVMmulticlass and 19 for SVMhmm are optimal window size. The 
interpretation  of  the  result  is  similar  with  that  of  the  neural 
networks. Also this shows that the local sequence environment 
of a residue substantially determines  its secondary structure. It 
was  considered  the  fact  that  residues  far  apart  in  sequence but 
close in three dimensions can have the tertiary interactions. 
 

C 
2 
4 
6 
8 
10 
12 
14 
16 
18 
20 
22 
24 

Q3(%) (W=1) 
59.26 
59.88 
60.31 
59.87 
60.21 
59.87 
59.76 
60.18 
60.07 
59.84 
60.08 
60.12 

Q3(%) (W=19) 
64.81 
64.91 
64.94 
64.97 
65.12 
65.07 
65.09 
64.95 
65.03 
65.22 
65.07 
64.93 

# SV (W=19)
20 
35 
44 
38 
46 
49 
53 
57 
46 
59 
62 
72 

 
Table 6. Dependence of the number of support vector and  
testing success on C(Regularized Parameter) (SVMhmm) 

 

 

 

In  PSIPRED,  a  second  network  is  used  to  filter  successive 
outputs  from  the  main  network.  As  only  three  possible  inputs 
are necessary for each amino acid position, the network has an 

Interestingly,  the performance of SVMhmm  is better  than  that 
of SVMmulticlass. Also, we can see that SVMhmm has much more 
support vectors than SVMmulticlass does. Based on the algorithm 
of  HMM(Hidden-Markov  Model),  SVMhmm  considers  the 

 

4

5

 

 

Figure 4. Bar graphs showing the distribution of Q3 scores 

CS229 Final Project, Dec 2008 

transition  probabilities  between  hidden-state  which  means 
secondary structure in this problem, but this information is not 
involved in SVMmulticlass. Intuitively, the secondary structure of 
an  amino  acid  are  correlated  with  that  of  the  neighborhood 
sequences,  not  far  from  it.  This  concept  is  very  similar  to  the 
reason why we take window to input feature vector.  
 

 
Figure 3. Hidden-Markov Model(HMM)  
for protein secondary structure prediction  

 

 
C.  Compare 
 
Training with  SVMs  has  crucial  advantages  including much 
faster  convergence  than  neural  networks  (NNs).  The  SVMs 
tend not to overfit and the ability to find the global optimum by 
solving  the  dual  problem  of  quadratic  convex  function 
minimization. However,  the  neural  networks  approach  suffers 
from  the  local  minima,  determination  of  appropriate  structure 
of neural networks and too many parameters.  
 

 

 

 

5

 

 

 

As  we  can  see,  the  performance  of  the  neural  network 
outperforms the SVMs. In the neural network, I used non-linear 
network by using sigmoid function which attributed to increase 
the  accuracy  because  this  problem  is  not  linear.  However,  the 
SVMs  are  still  linear  because  kernels  cannot  be  applied. As  a 
result  of  it,  the  performance  of  SVMs  was  worse  than  that  of 
NNs.  
 

Methods 
Non-linear NNs with 75 hidden units 
SVMhmm 
SVMmulticlass 
Linear NNs with no hidden units 

Q3(%)
67.42
65.22
63.28
62.50

Table 7. The best performance of linear NNs and SVMs 

In a different point of view, the overall accuracy of SVMs is 
less  than  the  nonlinear  neural  network,  but  the  variance  of 
accuracy is smaller than that of the neural network. This means 
that the average accuracy of SVMs is more reliable.   
However, the prediction levels of all three methods I used in 
this  paper  are  not  sufficient  to  compare  with  the  result  using 
multiple sequence alignments. I will discuss about  the ways  to 
improve the accuracy of both approaches in the next section.  
 

CS229 Final Project, Dec 2008 

6

IV.  DISCUSSION AND FUTURE WORK 

 
The highest accuracy is 67.42% of the neural network, but it 
is  not  high  enough.  I  think  that  there  are  two  weak  points  for 
this  low  accuracy:  multiple  sequence  alignments  and  kernel 
methods.  For  example,  one  recent  study  adopted  frequency 
profiles with  evolutionary  information  as  an  encoding  scheme 
for  SVM  [6],  and  another  approach  is  to  use  incorporated 
PSI-BLAST  PSSM  profiles  as  an  input  vector  [4][7].  Both  of 
them  showed  the  improvement  of  accuracy. And  based  on  the 
result of these studies, the success of SVM methods depends on 
the proper choice of encoding profiles and kernel function.  
 

A.  Encoding Profile 

 

In frequency matrix encoding, the frequency of occurrence of 
21  amino  acid  residues  at  each  position  in  the  multiple 
sequence  alignment  is  calculated  for  each  residue.  And  in 
PSSM  coding  the  individual  profiles  were  used  to  reflect 
detailed conservation of amino acids in a family of homologous 
proteins.  The  previous  study  reported  that  by  using  reliable 
local  alignment,  the  prediction  accuracy  could  be  improved. 
Therefore, if multiple sequence alignment methods are applied, 
the accuracy could be higher. 
 

B.  Kernel Methods 

 

The choice of kernel function is critical to the success of SVM. 
By  applying  the  kernel  method,  a  nonlinear  classifier  can  be 
built.  Most  studies  adopted  the  radial  basis  function  (RBF) 
kernel [7]. Therefore, applying kernels methods could increase 
the performance. However, we have  still  a  remaining problem 
to handle huge training data sets, which prevents from applying 
kernels to SVMs. 
 

 
 

V.  ACKNOWLEDGEMENT 
Specially  thanks  to  Chung(Tom) Do,  Ph.D  candidate  in  the 
Department of the Computer Science, in Stanford. Without his 
advice and guideline, I was not able  to make a progress on  the 
final project. I really appreciate his efforts to help me out.  
 

 

REFERENCES 
[1]  Hae-Jin  Hu,  Robert  W.  Harrison,  Phang  C,  Tai,  and  Yi  Pan,  “Current 
Methods  For  Protein  Secondary-Structure  Prediction  Based  on  Support 
Vector  Machines”,  Ch.1,  Knowledge  Discovery  in  Bioinformatics: 
Techniques, Methods, and Applications, 2007. 
[2]  Hae-Jin  Hu,  Robert  W.  Harrison,  Phang  C,  Tai,  and  Yi  Pan,  “Protein 
Structure  Prediction  using  String Kernels”, Ch.8, Knowledge Discovery 
in Bioinformatics: Techniques, Methods, and Applications, 2007. 
James A. Cuff and Geoffrey J. Barton, “Evaluation and  Improvement of 
Multiple Sequence Methods for Protein Secondary Structure Prediction”,  
Proteins, 1998. 
[4]  David  T.  Jones,  “Protein  Secondary  Structure  Prediction  Based  on 
Position-specific Scoring Matrices”, JMB, 1999. 
[5]  Ning  Qian  and  Terrence  J.  Sejnowski,  “Predicting  the  Secondary 
Structure  of  Globular  Proteins  Using  Neural  Network  Models”,  JMB, 
1988. 

[3] 

 

6

[6]  S.  Hua  and  Z.  Sun,  “A  novel  method  of  protein  secondary  structure 
prediction  with  high  segment  overlap  measure:  support  vector  machine 
approach”, JMB, 2001. 
[7]  H. Kim and H. Park, “Protein secondary structure prediction. based on an 
improved support vector machines approach”, Protein Eng. 2003. 
[8]  K.  Crammer  and  Y.  Singer.  On  the  Algorithmic  Implementation  of 
Multi-class SVMs, JMLR, 2001. 
[9]  Y.  Altun,  I.  Tsochantaridis,  T.  Hofmann,  “Hidden  Markov  Support 
Vector  Machines”,  International  Conference  on  Machine  Learning 
(ICML), 2003. 
[10]  I.  Tsochantaridis,  T.  Hofmann,  T.  Joachims,  and  Y.  Altun,  “Support 
Vector  Learning  for  Interdependent  and  Structured  Output  Spaces”, 
ICML, 2004. 
[11]  I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, “Large Margin 
Methods for Structured and Independent Output Variables”, JMLR, 2005 
[12]  L.  Rabiner,  “A  Tutorial  on  Hidden  Markov  Models  and  Selected 
Applications in Speech Recognition”, IEEE (1989)  

Appendix 
A.  Proteins in training and testing set  
Amino acid fractions 
Total number of residues = 61455 
Train set 
Amino Acid 
Test set 
A(Alanine) 
0.2473 
0.0875 
R(Arginine) 
0.1315 
0.0465 
N(Asparagine) 
0.0471 
0.1330 
D(Aspartic acid) 
0.1686 
0.0597 
C(Cysteine) 
0.0418 
0.0148 
E(Glutamic acid) 
0.1715 
0.0607 
Q(Glutamine) 
0.0371 
0.1048 
G(Glycine) 
0.2207 
0.0781 
H(Histidine) 
0.0617 
0.0218 
I(Isoleucine) 
0.1564 
0.0553 
L(Leucine) 
0.2432 
0.0861 
K(Lysine) 
0.0579 
0.1637 
M(Methionine) 
0.0565 
0.0200 
F(Phenylalanine) 
0.1102 
0.0390 
P(Proline) 
0.1303 
0.0461 
S(Serine) 
0.0606 
0.1712 
T(Threonine) 
0.1671 
0.0591 
W(Tryptophan) 
0.0423 
0.0149 
Y(Tyrosine) 
0.1052 
0.0372 
V(Valine) 
0.0687 
0.1941 
Others 
0.0027 
0.0009 

B.  The result of Neural Network for each test data 
Qcoil
Qalpha 
#residues
Protein 
Qbeta 
0  0.3829 
107 
1acx.all 
0.85
0.3571 
125 
1azu.all 
0.5  0.7594
0.7058  0.5060  0.8611
172 
1bbpa.all 
1bds.all 
42 
0  0.1818  0.9032
0.6250  0.3733  0.7128
184 
1bmv1.all
0.6842  0.3257  0.7252
373 
1bmv2.all
0 
35 
1cbh.all 
0.25  0.9130
1cc5.all 
82 
0.6929 
0  0.8372
0  0.2592  0.7812
59 
1cdta.all 
0.5  0.9090
0.2105 
45 
1crn.all 
0.6363  0.5789 
62 
1csei.all 
0.75
1eca.all 
135 
0.5360 
0  0.7105

Q3(%)
0.6448
0.648
0.6744
0.7142
0.5706
0.5817
0.6857
0.7682
0.5423
0.5777
0.6774
0.5851

103 
73 
84 
256 
177 
212 
142 
207 
327 
174 
74 
229 
103 
248 
116 
36 
460 
318 
272 
235 
39 
53 
144 
50 
316 
392 
102 
329 
332 
128 
753 
306
404
428
153
48
315
497
413
123
373
338
35
211
170

0.6463 
0  0.8095
0  0.3428  0.6578
0.6666  0.5789  0.6363
0.7191  0.5217  0.7933
0  0.5243  0.7368
0.8166  0.2131  0.7472
0.8977 
0.25  0.8478
0.8437  0.3333  0.5737
0.4  0.3416  0.7582
0.6629  0.0869  0.3870
0.9767 
0  0.9032
0.8695 
0.6  0.6896
0.3529  0.4642  0.8620
0.7113  0.6578  0.7345
0.4285 
0.25  0.8135
0 
0  0.8333
0.6590  0.4414  0.7327
0.7686  0.6949 
0.76
0.6666  0.4337  0.7683
0.6666  0.4637  0.7814
0  0.7428
0 
0.375 
0 
0.8
0.5940 
0  0.5348
0  0.4166  0.8947
0.8  0.2333  0.6946
0.7341  0.4285  0.7771
0.4761 
0  0.7333
0.4615  0.3154  0.7532
0.5967  0.5483  0.6045
0.5789 
0.25  0.7560
0.6026  0.5075  0.7909
0.5840 
0.48  0.7972
0.6444  0.2195  0.8142
0.6428 
0  0.7764
0.5806  0.2888  0.7532
0 
1  0.7857
0.6694  0.4038  0.7586
0.5839  0.4084  0.7785
0.7070  0.4133  0.6906
0.9090  0.2439 
0.8
0.4523  0.4823  0.6911
0.7961  0.1517  0.6910
0  0.5294 
0.4693  0.5277  0.8174
0.1875 
0.375  0.9420

7

0.6796
0.5068
0.6309
0.7187
0.6384
0.6132
0.8450
0.6328
0.5779
0.4885
0.9459
0.7379
0.6699
0.7137
0.6120
0.6944
0.6413
0.7515
0.6617
0.6808
0.6666
0.7358
0.5763
0.78
0.7025
0.7270
0.6274
0.5319
0.5963
0.6718
0.6852
0.6666
0.6782
0.6869
0.5816
0.8125
0.6666
0.6720
0.6464
0.6341
0.5898
0.5443
0.7714
0.6872
0.8176

2wrpr.all 
3ait.all 
3b5c.all 
3blm.all 
3cd4.all 
3cla.all 
3cln.all 
3gapa.all 
3hmga.all 
3hmgb.all
3icb.all 
3pgm.all 
3rnt.all 
3tima.all 
4bp2.all 
4cpai.all 
4gr1.all 
4pfk.all 
4rhv1.all 
4rhv3.all 
4rhv4.all 
4rxn.all 
4sdha.all 
4sgbi.all 
4ts1a.all 
4xiaa.all 
5cytr.all 
5er2e.all 
5ldh.all 
5lyz.all 
6acn.all 
6cpa.all 
6cpp.all 
6cts.all 
6dfr.all 
6hir.all 
6tmne.all 
7cata.all 
7icd.all 
7rsa.all 
8adh.all 
9apia.all 
9apib.all 
9pap.all 
9wgaa.all 

CS229 Final Project, Dec 2008 

176 
42 
217 
53 
106 
295 
95 
333 
152 
182 
84 
70 
163 
480 
86 
30 
55 
119 
35 
279 
254 
173 
292 
274 
47 
151 
75 
247 
384 
105 
395 
225 
197 
255 
126 
292 
137 
80 
308 
172 
467 
86 
113 
152 
180 
46 
57 
62 
113 
390 
114 
140 
150 
183 
57 
153 
263 
69 

1etu.all 
1fc2c.all 
1fdlh.all 
1fdx.all 
1fkf.all 
1fnd.all 
1fxia.all 
1gd1o.all 
1gdj.all 
1gp1a.all 
1hip.all 
1il8a.all 
1l58.all 
1lap.all 
1lmb3.all 
1mrt.all 
1ovoa.all 
1paz.all 
1ppt.all 
1pyp.all 
1r092.all 
1rbp.all 
1rhd.all 
1s01.all 
1sh1.all 
1tnfa.all 
1ubq.all 
1wsya.all 
1wsyb.all 
256ba.all 
2aat.all 
2ak3a.all 
2alp.all 
2cab.all 
2ccya.all 
2cyp.all 
2fox.all 
2fxb.all 
2gbp.all 
2gcr.all 
2glsa.all 
2gn5.all 
2hmza.all 
2i1b.all 
2ltna.all 
2ltnb.all 
2mev4.all 
2or1l.all 
2paba.all 
2phh.all 
2rspa.all 
2sns.all 
2sodb.all 
2stv.all 
2tgpi.all 
2tmvp.all 
2tsca.all 
2utga.all 

 

0.6153
0.6190
0
0
1
0.8235
0.1250
0.7397
0.6111
0.5714
0.9
0.9375
0.7196
0.6741
0.8813
0
0.4
0.8823
1
0.6944
0.2222
1
0.7037
0.4756
0
0
0.5
0.8373
0.7482
0.7160
0.7851
0.6534
0.875
1
0.7444
0.5671
0.78
0.4615
0.6693
0
0.7152
0
0.6285
0
0
0.25
0
0.775
0.625
0.7794
0.5
0.9615
0
0.7222
0.625
0.6406
0.6216
0.6666

0.4444  0.7741
0  0.6666
0.3425  0.8256
0  0.7954
0.5121  0.8793
0.3950  0.7123
0.6666  0.8166
0.5263  0.6787
0  0.5681
0.5517  0.7927
0.2222  0.7230
0.6666 
0.75
0.5  0.7619
0.4313 
0.78
0  0.4814
0  0.7666
0.0833  0.8787
0.5454  0.6206
0  0.8823
0.4642  0.6930
0.4606  0.6538
0.2317  0.7051
0.4062  0.6648
0.5106 
0.8
0  0.9090
0.2461  0.6976
0.625  0.7692
0.2727  0.8021
0.4461  0.7558
0 
0.875
0.4  0.6622
0.375  0.8152
0.3269  0.7176
0.1898  0.7924
0  0.9444
0.125  0.8239
0.7586  0.7931
0.5  0.6981
0.5789  0.7795
0.4492  0.6699
0.3375  0.7983
0.5  0.6829
0  0.6279
0.3478  0.7469
0.4805  0.6893
0.2333  0.5833
0  0.6226
0  0.9545
0.3389  0.8260
0.2636  0.7638
0.4042  0.8524
0.6071  0.6279
0.5555  0.7916
0.4268  0.5903
0.4285  0.9142
0.2857  0.5243
0.2615  0.8145
0  0.8333

0.6363
0.6428
0.5852
0.6603
0.7452
0.6508
0.7157
0.6486
0.5986
0.7032
0.6904
0.7714
0.7116
0.6666
0.7558
0.7666
0.6181
0.6302
0.9428
0.6702
0.5708
0.5028
0.6472
0.6532
0.4255
0.5033
0.68
0.7489
0.7005
0.7523
0.6810
0.68
0.5177
0.6196
0.8015
0.6678
0.7810
0.625
0.6980
0.5813
0.6937
0.6744
0.6283
0.5657
0.6
0.3260
0.5789
0.8387
0.5575
0.6282
0.6491
0.6857
0.7066
0.5300
0.7543
0.5620
0.6235
0.7101

 

7


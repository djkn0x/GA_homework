Improving on Expectation Propagation

Manfred Opper
Computer Science, TU Berlin
opperm@cs.tu-berlin.de

Ulrich Paquet
Computer Laboratory, University of Cambridge
ulrich@cantab.net

Ole Winther
Informatics and Mathematical Modelling, Technical University of Denmark
owi@imm.dtu.dk

Abstract

A series of corrections is developed for the ﬁxed points of Ex pectation Propaga-
tion (EP), which is one of the most popular methods for approximate probabilistic
inference. These corrections can lead to improvements of the inference approxi-
mation or serve as a sanity check, indicating when EP yields unrealiable results.

1 Introduction

The expectation propagation (EP) message passing algorithm is often considered as the method of
choice for approximate Bayesian inference when both good accuracy and computational efﬁciency
are required [5]. One recent example is a comparison of EP with extensive MCMC simulations for
Gaussian process (GP) classiﬁers [4], which has shown that n ot only the predictive distribution, but
also the typically much harder marginal likelihood (the partition function) of the data, are approxi-
mated remarkably well for a variety of data sets. However, while such empirical studies hold great
value, they can not guarantee the same performance on other data sets or when completely different
types of Bayesian models are considered.

In this paper methods are developed to assess the quality of the EP approximation. We compute
explicit expressions for the remainder terms of the approximation. This leads to various corrections
for partition functions and posterior distributions. Under the hypothesis that the EP approximation
works well, we identify quantities which can be assumed to be small and can be used in a series
expansion of the corrections with increasing complexity. The computation of low order corrections
in this expansion is often feasible, typically require only moderate computational efforts, and can
lead to an improvement to the EP approximation or to the indication that the approximation cannot
be trusted.

2 Expectation Propagation in a Nutshell

Since it is the goal of this paper to compute corrections to the EP approximation, we will not dis-
cuss details of EP algorithms but rather characterise the ﬁxed points which are reached wh en such
algorithms converge.

EP is applied to probabilistic models with an unobserved latent variable x having an intractable
distribution p(x). In applications p(x) is usually the Bayesian posterior distribution conditioned on
a set of observations. Since the dependency on the latter variables is not important for the subsequent
theory, we will skip them in our notation.

1

p(x) =

fn (x) ,

It is assumed that p(x) factorizes into a product of terms fn such that
1
Z Yn
where the normalising partition function Z = R dx Qn fn (x) is also intractable. We then assume
an approximation to p(x) in the form
q(x) = Yn
(2)
gn (x)
where the terms gn(x) belong to a tractable, e.g. exponential family of distributions. To compute
the optimal parameters of the gn term approximation a set of auxiliary tilted distributions is de ﬁned
via
gn(x) (cid:19) .
Zn (cid:18) q(x)fn (x)
1
Here a single approximating term gn is replaced by an original term fn . Assuming that this re-
placement leaves qn still tractable, the parameters in gn are determined by the condition that q(x)
and all qn (x) should be made as similar as possible. This is usually achieved by requiring that these
distributions share a set of generalised moments (which usually coincide with the sufﬁcient statistics
of the exponential family). Note, that we will not assume that this expectation consistency [8] for
the moments is derived by minimising a Kullback –Leibler div ergence, as was done in the original
derivations of EP [5]. Such an assumption would limit the applicability of the approximate inference
and exclude e.g. the approximation of models with binary, Ising variables by a Gaussian model as
in one of the applications in the last section.

qn (x) =

(1)

(3)

The corresponding approximation to the normalising partition function in (1) was given in [8] and
[7] and reads in our present notation1

ZEP = Yn

Zn .

(4)

3 Corrections to EP

An expression for the remainder terms which are neglected by the EP approximation can be obtained
by solving for fn in (3), and taking the product to get
fn (x) = Yn (cid:18) Zn qn (x)gn (x)
(cid:19) = ZEP q(x) Yn (cid:18) qn (x)
q(x) (cid:19) .
Yn
q(x)
Hence Z = R dx Qn fn (x) = ZEP R, with
q(x) (cid:19) and p(x) =
q(x) (cid:19) .
q(x) Yn (cid:18) qn (x)
R = Z dx q(x) Yn (cid:18) qn (x)
This shows that corrections to EP are small when all distributions qn are indeed close to q , justifying
the optimality criterion of EP. For related expansions, see [2, 3, 9].

1
R

(6)

(5)

Exact probabilistic inference with the corrections described here again leads to intractable computa-
tions. However, we can derive exact perturbation expansions involving a series of corrections with
increasing computational complexity. Assuming that EP already yields a good approximation, the
computation of a small number of these terms maybe sufﬁcient
to obtain the most dominant correc-
tions. On the other hand, when the leading corrections come out large or do not sufﬁciently decrease
with order, this may indicate that the EP approximation is inaccurate. Two such perturbation expan-
sions are be presented in this section.

1The deﬁnition of partition functions Zn is slightly different from previous works.

2

3.1 Expansion I: Clusters

(7)

(8)

The most basic expansion is based on the variables εn (x) = qn (x)
q(x) − 1 which we can assume to be
typically small, when the EP approximation is good. Expanding the products in (6) we obtain the
correction to the partition function
R = Z dx q(x) Yn
(1 + εn (x))
= 1 + Xn1<n2 (cid:10)εn1 (x)εn2 (x)(cid:11)q + Xn1<n2<n3 (cid:10)εn1 (x)εn2 (x)εn3 (x)(cid:11)q + . . . ,
which is a ﬁnite series in terms of growing clusters of “inter
acting ” variables εn (x). Here the
brackets h. . .iq denote expectations with respect to the distribution q . Note, that the ﬁrst order term
Pn hεn (x)iq = 0 vanishes by the normalization of qn and q . As we will see later, the computation
of corrections is feasible when qn is just a ﬁnite mixture of K simpler densities from the exponential
family to which q belongs. Then the number of mixture components in the j -th term of the expansion
of R is just of the order O(K j ) and an evaluation of low order terms should be tractable.
In a similar way, we get
q(x) (cid:0)1 + Pn εn (x) + Pn1<n2 εn1 (x)εn2 (x) + . . .(cid:1)
1 + Pn1<n2 hεn1 (x)εn2 (x)iq + . . .
In order to keep the resulting density normalized to one, we should keep as many terms in the
numerator as in the denominator. As an example, the ﬁrst orde r correction to q(x) is
p(x) ≈ Xn
qn (x) − (N − 1)q(x) .
3.2 Expansion II: Cumulants

p(x) =

(10)

(9)

,

One of most important applications of EP is to the case of statistical models with Gaussian process
priors. Here x is a latent variable with Gaussian prior distribution and covariance E[xx⊤ ] = K
where K is the kernel matrix. In this case we have N + 1 terms f0 , f1 , . . . , fN in (1) where f0(x) =
g0 (x) = exp[− 1
x⊤K−1x]. For n ≥ 1 each fn(x) = tn (xn ) is the likelihood term for the nth
2
observation which depends only on a single component xn of the vector x.
The corresponding approximating terms are chosen to be Gaussian of the form gn (x) ∝
2 λn x2 . The 2N parameters γn and λn are determined in such a way that q(x) and the dis-
eγnx− 1
n i. In this case, the
tributions qn (x) have the same ﬁrst and second marginal moments hxn i and hx2
computation of corrections (7) would require the computation of multivariate integrals of increasing
dimensionality. Hence, a different type of expansion seems more appropriate. The main idea is to
expand with respect to the higher order cumulants of the distributions qn .
To derive this expansion, we simplify (6) using the fact that q(x) = q(x\n |xn )q(xn ) and qn (x) =
q(x\n |xn )qn (xn ), where we have (with a slight abuse of notation) introduced q(xn ) and qn (xn ),
the marginals of q(x) and qn (x). Thus p(x) = 1
R q(x)F (x) and R = R dx q(x)F (x), where
q(xn ) (cid:19) .
F (x) = Yn (cid:18) qn (xn )
Since q(xn ) and the qn (xn ) have the same ﬁrst two cumulants, corrections can be express ed by the
higher cumulants of the qn (xn ) (note, that the higher cumulants of q(xn ) vanish). The cumulants
cln of qn (xn ) are de ﬁned by their characteristic functions χn (k) via
qn (xn ) = Z dk
ln χn (k) = Xl
2π
Expressing the Gaussian marginals q(xn ) by their ﬁrst and second cumulants, the means mn and
the variances Snn and introducing the function
rn (k) = Xl≥3
3

e−ikxn χn (k)

(i)l cln
l!

(i)l cln
l!

(11)

(12)

(13)

k l

and

k l .

which contains the contributions of all higher order cumulants, we get
F (x) = Yn   R dkn exp (cid:2)−ikn (xn − mn ) − 1
!
2 Snnk2
n + rn (kn )(cid:3)
R dkn exp (cid:2)−ikn(xn − mn ) − 1
n (cid:3)
2 Snnk2
# exp "Xn
exp "− Xn
= Z dη sYn
Snnη2
rn (cid:18)ηn − i
Snn
n
2π
2
where in the last equality we have introduced a shift of variables ηn = kn + i (xn−mn )
Snn
An expansion can be performed with respect to the cumulants in the terms gn which had been ne-
glected in the EP approximation. The basic computations are most easily explained for the correction
R to the partition function.

(xn − mn )
Snn

(cid:19)#
.

(15)

(14)

3.2.1 Correction to the partition function
Since q(x) is a multivariate Gaussian of the form q(x) = N (x; m, S), the correction R to the
partition Z involves a double Gaussian average over the vector x and the set of ηn . This can be
simpliﬁed by combining them into a single complex zero mean Gaussian random vector de ﬁned as
zn = ηn − i xn−mn
such that
Snn
R = *exp "Xn
rn (zn )#+z
The most remarkable property of the Gaussian z is its covariance which is easily found to be
Sij
and hz 2
when i 6= j,
i iz = 0 .
hzi zj iz = −
Sii Sjj
The last equation has important consequences for the surviving terms in an expansion of R!
Assuming that the gn are small we perform a power series expansion of ln R
ln R = ln *exp h Xn
rn (zn ) i+z
rn(cid:17)2Ez
1
1
2 D(cid:16) Xn
2 (cid:16) Xn
= Xn
−
hrn iz +
SnnSmm (cid:19)l
l! (cid:18) Snm
1
cln clm
2 Xm 6=n
= Xm 6=n Xl≥3
± . . .
hrm rn iz ± . . .
=
Here we have repeatedly used the fact that each factor zn in expectations hz l
mi have to be paired
n z s
(by Wick’s theorem) with a factor zm where m 6= n (diagonal terms vanish by (17)). This gives
nonzero contributions only, when l = s and there are l! ways for pairing.2
This expansion gives a hint why EP may work typically well for multivariate models when covari-
ances Sij are small compared to the variances Sii . While we may expect that ln ZEP = O(N )
where N is the number of variables xn , the vanishing of the “self interactions” indicates that co rrec-
tions may not scale with N .

hrn iz(cid:17)2

± . . . (18)

(17)

(16)

(19)

3.2.2 Correction to marginal moments

The predictive density of a novel observation can be treated by extending the Gaussian prior to
include a new latent variable x∗ with E[x∗x] = k∗ and E[x2
∗ ] = k∗ , and appears as an average of a
likelihood term over the posterior marginal of x∗ .
A correction for the predictive density can also be derived in terms of the cumulant expansion by
averaging the conditional distribution p(x∗ |x) = N (x∗ ; k⊤
K−1k∗ .
∗ = k∗ − k⊤
K−1x, σ2
∗ ) with σ2
∗
∗
Using the expression (15) we obtain (where we set R = 1 in (6) to lowest order)
x∗ ) *1 + Xn
Snn (cid:19) + . . .+η,x∼N (x;µ,Σ)
xn − mn
rn (cid:18)ηn − i
p(x∗ ) = Z dx p(x∗ |x) p(x) = N (x∗ ; µx∗ , s2
(20)
2The terms in the expansion might be organised in Feynman graphs, where “self interaction” loops are
absent.

4

Z
g
o
l

−195

−200

−205

−210

−215

−220

−225

−230

−235

1

2
5
4
3
Numb er of comp on ent s K

6

Figure 1:
ln Z approximations obtained from
q(x)’s factorization in (2), for sec. 4.1’s mixture
model, as obtained by: variational Bayes (see [1]
for details) as red squares; α = 1
2 in Minka’s α-
divergence message passing scheme, described in
[6], as magenta triangles; EP as blue circles; EP
with the 2nd order correction in (8) as green di-
amonds. For 20 runs each, the colour intensities
correspond to the frequency of reaching different
estimates. A Monte Carlo estimate of the true
ln Z , as found by parallel tempering with thermo-
dynamic integration, is shown as a line with two-
standard deviation error bars.

x∗ = k∗ − k⊤
∗ (K + Λ−1 )−1k∗ and Λ = diag(λ) denotes
where µx∗ = k⊤
K−1m and variance s2
∗
the parameters in the Gaussian terms gn . The average in (20) is over a Gaussian x with Σ−1 =
∗ )−1 + Λ−1 and µ = (x∗ − µx∗ )σ−2
ΣK−1k∗ + m. By simplifying the inner
(K − k−1
k∗k⊤
∗
∗
expectation over the complex Gaussian variables η we obtain
x∗ ) 
+ · · · 
√Snn (cid:19)l *hl (cid:18) xn − mn
√Snn (cid:19)+x∼N (x;µ,Σ)
l! (cid:18) 1
cln
1 + Xn Xl≥3
p(x∗ ) = N (x∗ ; µx∗ , s2
(21)
where hl is the lth Hermite polynomial. The Hermite polynomials are averaged over a Gaussian
density where the only occurrence of x∗ is through (x∗ − µx∗ ) in µ, so that the expansion ultimately
appears as a polynomial in x∗ . A correction to the predictive density follows from averaging t∗ (x∗ )
over (21).

4 Applications

4.1 Mixture of Gaussians

This section illustrates an example where a large ﬁrst nontr ivial correction term in (8) re ﬂects an
inaccurate EP approximation. We explain this for a K -component Gaussian mixture model.
Consider N observed data points ζn with likelihood terms fn (x) = Pκ πκN (ζn ; µκ , Γ−1
κ ), with
n ≥ 1 and with the mixing weights πκ forming a probability vector. The latent variables are then
κ=1 . For our prior on x we use a Dirichlet distribution and product of Normal-
x = {πκ , µκ , Γκ}K
Wisharts densities so that f0 (x) = D(π ) Qκ N W (µκ , Γκ ). When we multiply the fn terms we
see that intractability for the mixture model arises because the number of terms in the marginal
likelihood is K N , rather than because integration is intractable. The computation of lower-order
terms in (8) should therefore be immediately feasible. The approximation q(x) and each gn(x) are
chosen to be of the same exponential family form as f0 (x), where we don’t require gn (x) to be
normalizable.

For brevity we omit the details of the EP algorithm for this mixture model, and assume here that an
EP ﬁxed point has been found, possibly using some damping. Fi g. 1 shows various approximations
to the log marginal likelihood ln Z for ζn coming from the acidity data set. It is evident that the
“true peak ” doesn’t match the peak obtained by approximate i nference, and we will wrongly predict
which K maximizes the log marginal likelihood. Without having to resort to Monte Carlo methods,
the second order correction for K = 3 both corrects our prediction and already con ﬁrms that the
original approximation might be inadequate.

4.2 Gaussian Process Classiﬁcation

The GP classiﬁcation model arises when we observe N data points ζn with class labels yn ∈
{−1, 1}, and model y through a latent function x with the GP prior mentioned in sec. 3.2. The

5


Hierarchical Fisher Kernels for Longitudinal Data

Zhengdong Lu
Todd K. Leen
Dept. of Computer Science & Engineering
Oregon Health & Science University
Beaverton, OR 97006
luz@cs.utexas.edu,tleen@csee.ogi.edu

Jeffrey Kaye
Layton Aging & Alzheimer’s Disease Center
Oregon Health & Science University
Portland, OR 97201
kaye@ohsu.edu
Abstract

We develop new techniques for time series classiﬁcation based on hierarchical Bayesian
generative models (called mixed-effect models) and the Fisher kernel derived from them.
A key advantage of the new formulation is that one can compute the Fisher informa-
tion matrix despite varying sequence lengths and varying sampling intervals. This avoids
the commonly-used ad hoc replacement of the Fisher information matrix with the iden-
tity which destroys the geometric invariance of the kernel. Our construction retains the
geometric invariance, resulting in a kernel that is properly invariant under change of co-
ordinates in the model parameter space. Experiments on detecting cognitive decline show
that classiﬁers based on the proposed kernel out-perform those based on generative models
and other feature extraction routines, and on Fisher kernels that use the identity in place
of the Fisher information.

1 Introduction
Time series classiﬁcation arises in diverse application. This paper develops new techniques based on hi-
erarchical Bayesian generative models and the Fisher kernel derived from them. A key advantage of the
new formulation is that, despite varying sequence lengths and sampling times, one can compute the Fisher
information matrix. This avoids its common ad hoc replacement with the identity matrix. The latter strategy,
common in the biological sequence literature [4], destroys the geometrical invariance of the kernel. Our con-
struction retains the proper geometric structure, resulting in a kernel that is properly invariant under change
of coordinates in the model parameter space.
This work was motivated by the need to classify clinical longitudinal data on human motor and psychometric
test performance. Clinical studies show that at the population level progressive slowing of walking and the
rate at which a subject can tap their ﬁngers are predictive of cognitive decline years before its manifestation
[1]. Similarly, performance on psychometric tests such as delayed recall of a story or word lists( tests
not used in diagnosis), are predictive of cognitive decline [8]. An early predictor of cognitive decline for
individual patients based on such longitudinal data would improve medical care and planning for assistance.

1

Our new Fisher kernels use mixed-effects models [6] as the generative process. These are hierarchical
models that describe the population (consisting of many individuals) as a whole, and variations between
individuals in the population. The population model parameters (called ﬁxed effects), the covariance of
the between-individual variability (the random effects), and the additive noise variance are ﬁt by maximum
likelihood. The overall population model together with the covariance of the random effects comprise a set
of parameters for the prior on an individual subject model, so the ﬁtting scheme is a hierarchical empirical
Bayesian procedure.

Data Description The data for this study was drawn from the Oregon Brain Aging Study (OBAS) [2], a
longitudinal study spanning up to ﬁfteen years with roughly yearly assessment of subjects. For our work,
we grouped the subjects into two classes: those who remain cognitively healthy through the course of the
study (denoted normal), and those who progress to mild cognitive impairment (MCI) or further to dementia
(denoted impaired). Since we are interested in prediction, we retain only data taken prior to diagnosis of
impairment. We use 97 subjects from the normal group and 46 from the group that becomes impaired.
Motor task data included the time (denoted as seconds) and the number of steps (denoted as steps) to walk
9 meters, and the number of times the subject can tap their foreﬁnger, both dominant (tappingD) and non-
dominant hands (tappingN) in 10 seconds. Psychometric test data include delayed-recall, which measures
the number of words from a list of 10 that the subject can recall one minute after hearing the list, and logical
memory II in which the subject is graded on recall of a story told 15-20 minutes earlier.

2 Mixed-effect Models
2.1 Mixed-effect Regression Models
In this paper, we conﬁne attention to parametric regression. Suppose there are k individuals (indexed by i =
n}, n = 1, . . . , N i as a function
1, . . . , k) contributing data to the sample, and we have observations {ti
n , y i
n = f (ti
n ; γ i ) + i
of time t for individual i. The data are modeled as y i
n , where γ i are the regression
n is zero-mean white Gaussian noise with (unknown) variance σ2 . The superscript on the
parameters and i
model parameters γ i indicates that the regression parameters are different for each individual contributing to
the population. Since the model parameters vary between individuals, it is natural to consider them generated
by the sum of a ﬁxed and a random piece: γ i = α + β i , where β i (called the random effect), is assumed
distributed N (0, D) with unknown covariance D. The expected parameter vector α, called the ﬁxed effect,
determines the model for the population as a whole, and the random effect β i accounts for the differences
between individuals. This intuition is most precise for the case in which the model is linear in parameters
f (t; γ ) = γ T Φ(t) = αT Φ(t) + β T Φ(t)
(1)
where Φ(t) = [φ1 (t), φ2 (t), ..., φd (t)]T denotes a vector of basis functions1 . We use M = {α, D, σ} to
denote the mixed-effect model parameters. The feature values, observation times, and observation noise are
i ≡ [i
1 , · · · , i
ti ≡ [ti
1 , · · · , ti
yi ≡ [y i
1 , · · · , y i
N i ]T .
N i ]T ,
N i ]T ,
2.2 Maximum Likelihood Fitting
Model ﬁtting uses the entire collection of data {ti , yi}, i = 1, . . . , k to determine the parameters M =
(cid:90)
{α, D, σ} by maximum likelihood. The likelihood of the data {ti , yi} given M is
p(yi ; ti , M) =
p(yi | β i ; ti , σ)p(β i | M)dβ i
= (2π)−N i /2 |Σi |−1/2 exp((yi − αT Φ(ti ))T (Σi )−1 (yi − αT Φi (ti )))

(3)

(2)

where
1More generally, the ﬁxed and random effects can be associated with different basis functions.

2

Seconds: Normal

Seconds: Impaired

logical memory II: Normal

logical memory II: Impaired

Σi =

αT Φ(t). The two green lines stand for αT Φ(t) ± (cid:112)
Figure 1: The ﬁt mixed-effect models for two tests. In each panel, the red line stands for the ﬁxed effect
ΦT (t)DΦ(t), i.e., the population model ± the s.t.d. of
the deviation from the uncertainty of the β . The black dash line is the s.t.d of the deviation when we consider
the observation noise.
N i(cid:88)
2 ), · · · , Φ(ti
n )]T .
1 ), Φ(ti
and Φ(ti ) = [Φ(ti
n )DΦ(ti
n )T + σ2 I,
Φ(ti
(cid:81)k
n=1
The data likelihood for Y = {y1 , y2 , · · · , yk } with T = {t1 , t2 , · · · , tk } is then p(Y ; T, M) =
i=1 p(yi | ti ; M). The maximum likelihood values of {α, D, σ} are found using the Expectation-
Maximization algorithm [6] with {β 1 , β 2 , · · · , β k } considered as the latent variable:
Q(M, Mg ) = E{β i } (log p(Y , {β i }; T, M)|Y ; T, Mg )
(4)
E-step:
M-step: M = arg maxM Q(M, Mg ),
(5)
where Mg stands for the model parameters estimated in previous step, and the expectation in the E-step is
with respect to the posterior distribution of on {β i} when Y is known and the model parameter is Mg . For
the linear mixed-effect model in Equation (1), the M-step can be given in a closed form. The details of the
updating equations are given by Laird et al. [6].
We use the linear mixed-effect model with polynomial basis functions
Φ(t) = [1, t]T . We trained separate mixed-effect models for each of the
six measurements. For the four motor behavior measurements, we use
the logarithm of data to reduce the skew of the residuals. Figure 1 shows
the ﬁt models for seconds and logical memory II, as the representatives of
the six measurements. The plots show the ﬁxed effect regression αT Φ(t)
(red curve), and the standard deviations arising from the random effects
(green curves) and measurement noise (dashed black curve, see caption).
The data are the blue spaghetti plots. The plots conﬁrm that subjects that
become impaired deteriorate faster than those who remain healthy.
With multiple classes (or component subpopulations), it is natural to use
M1 ), with Mm = {αm , Dm , σm}, m = 0, 1. Here, we use (cid:102)M =
a mixture of mixed-effect models. We have two components: one ﬁt on
the normal group (denoted M0 ) and one ﬁt on impaired group (denoted
{π0 , M0 , π1 , M1} to denote the parameters of this mixture, with π0 and
π1 being the mixing proportions (prior) estimated from the training data.
The overall generative process for any individual (ti , yi ) is summarized
in Figure 2. Here z i ∈ {0, 1} is the latent variable indicating which model component is used to generate
yi .

Figure 2: The graphical model of
the mixture of mixed-effect models.

3

70809010011.522.533.54Agelog(seconds)70809010011.522.533.54Agelog(seconds)7080901000246810Age# of words7080901000246810Age# of words3 Hierarchical Fisher Kernel
3.1 Fisher Kernel Background
The Fisher kernel [4] provides a way to extract discriminative features from the generative model. For any
θ-parameterized model p(x; θ), the Fisher kernel between xi and xj is deﬁned as
K (xi , xj ) = (∇θ log p(xi ; θ))T I−1∇θ log p(xj ; θ),
(cid:90)
where I is the Fisher information matrix with the (n, m) entry
∂ log p(x; θ)
∂ log p(x; θ)
∂ θn
∂ θm
x
The kernel entry K (xi , xj ) can be viewed as the inner product of the natural gradient I−1∇θ log p(x; θ) at
xi and xj with metric I, and is invariant to re-parametrization of θ . Jaakkola and Haussler [4] prove that a
linear classiﬁer based on the Fisher kernel performs at least as well as the generative model.

p(x; θ)dx.

In,m =

(6)

(7)

3.2 Retaining the Fisher Information Matrix
In the bioinformatics literature [3] and for longitudinal data such as ours, p(xi ; θ) is different for each
individual owing to different sequence lengths, and (for longitudinal data) different sampling times ti . The
integral in Equation (7) must therefore include the distribution sequence lengths and observation times.
Where only sequence lengths differ, an empirical average can be used. However where observation times
are non-uniform and vary considerably between individuals (as is the case here), there is insufﬁcient data to
form an estimate by empirical averaging.
The usual response to the difﬁculty is to replace the Fisher information with the identity matrix [4]. This
spoils the geometric structure, in particular the invariance of the the kernel K (xi , xj ) under change of coor-
dinates in the model parameter space (model re-parameterization). This is a signiﬁcant ﬂaw: the coordinate
system used to describe the model is immaterial and should not inﬂuence the value of K (xi , xj ). For proba-
bilistic kernel regression, the choice of metric is immaterial in the limit of large training sets [4]. However for
our application, which uses a support vector machine (SVM), we found the difference cannot be neglected.
In our case, replacing Fisher information matrix with the identity matrix is grossly unsuitable. For the
mixed-effect model with polynomial basis functions the Fisher score components associated with higher
order terms (such as slope and curvature) are far larger than the entries associated with lower order term
(such as intercept). Without the proper normalization provided by the Fisher information matrix, the kernel
will be dominated by higher order entries2 . A principled extension of the Fisher kernel provided by our
hierarchical model allows proper calculation of the Fisher information matrix.

3.3 Hierarchical Fisher Kernel
Our design of kernel is based on the generative hierarchy of mixture of mixed-effect models, in Figure 2. We
with p(z i , γ i ; (cid:101)Θ) = πz i p(γz i ; αz i , Dz i ).
notice that the individual-speciﬁc information ti enter into this generative process at the last step, but the “la-
tent” variables γ i and z i are drawn from the Gaussian mixture model (GMM) ˜Θ = {π0 , α0 , D0 , π1 , α1 , D1 },
We can thus build a standard Fisher kernel for the latent variables, and use it to induce a kernel on the
observed data. Denoting the latent variables by v i , the Fisher kernel between v i and v j is
K (v i , v j ) = (∇Θ log p(v i ; θ))T (Iv )−1∇θ log p(v j ; Θ),
2Our experiments on the OBAS data show that replacing the Fisher information with the identity compromises
classiﬁer performance.

4

; ∂ log p
∂D1

]T ,

; ∂ log p
∂π1

; ∂ log p
∂α1

where the Fisher score ∇ ˜Θ log p(v i ; ˜Θ) is a column vector
∇ ˜Θ log p(v i ; ˜Θ) = [ ∂ log p
; ∂ log p
; ∂ log p
∂D0
(cid:90)
∂π0
∂α0
and Iv is the well-deﬁned Fisher information matrix for v :
∂ log p(v ; ˜Θ)
∂ log p(v ; ˜Θ)
p(v | ˜Θ)dv .
n,m =
(8)
Iv
∂ ˜Θn
∂ ˜Θm
(cid:90) (cid:90)
v
K (v i , v j )p(v i | yi ; ti , (cid:102)M)p(v j | yj ; tj , (cid:102)M)dv idv j
K (yi , yj ) = Evi ,vj [K (v i , v j )| yi , yj ; ti , tj , (cid:102)M] =
The kernel for yi and yj is the expectation of K (v i , v j ) given the observation yi and yj .
With different choices of latent variable v , we have three kernel design strategies in the following subsec-
tions. This extension to the Fisher kernel, named hierarchical Fisher kernel (HFK), enables us to deal with
time series with irregular sampling and different sequence lengths. To our knowledge it has not been reported
elsewhere in the literature.

Design A: v i = γ i
This kernel design marginalizes out the higher level variable {z i } and constructs Fisher kernel between the
{γ i}. This generative process is illustrated in Figure 3 (left panel), which is the same graphical model in
Figure 2 with latent variable z i marginalized out3 . The Fisher kernel for γ is
K (γ i , γ j ) = (∇ ˜Θ log p(γ i | ˜Θ))T (Iγ )−1∇ ˜Θ log p(γ i | ˜Θ).
(9)
K (yi , yj ) = Eγ i ,γ j (K (γ i , γ j )| yi , yj ; ti , tj , (cid:102)M)
The kernel between yi and yj as the expectation of K (γ i , γ j ):
(cid:90)
(cid:90)
∇ ˜Θ log p(γ j | ˜Θ)p(γ j | yj ; tj (cid:102)M)dγ j . (11)
∇ ˜Θ log p(γ i | ˜Θ)p(γ i | yi ; ti , (cid:102)M)dγ i )T (Iγ )−1
(10)
(cid:82) ∇ ˜Θ log p(γ j | ˜Θ)p(γ j | yj ; tj (cid:102)M)dγ j
= (
The computational drawback is that the integral required to evaluate
and Ir do not have an analytical solution. In our experiments, we estimated the integral with Monte-Carlo
sampling.

Design B: v i = (z i , γ i )
This design strategy takes both γ i and z i as joint latent variable and build a Fisher kernel for them. The
generative process, as summarized in Figure 3 (middle panel), gives the probability for latent variables
p(z i , γ i ; ˜Θ) = πz i p(γi ; αz i , Dz i ).
The Fisher kernel for the joint variable (γ i , z i ) is
K ((z i , γ i ), (z j , γ j )) = (∇ ˜Θ log p(z i , γ i ; ˜Θ))T (Iz ,γ )−1∇ ˜Θ log p(z i , γ i ; ˜Θ),
(12)
where Iz ,γ is the Fisher information matrix associated with distribution p(z , γ ; ˜Θ). It can be shown that
1
πz i
3 Strictly speaking, we cannot sum out z i at this step since the group membership is used later in generating the
observation noise. However this is a reasonable approximation since the noise variance from M0 and M1 are similar.

δ(z i , z j )(1 + Kz i (γ i , γ j ))

K ((z i , γ i ), (z j , γ j )) =

5

(13)

(14)

where Km (γ i , γ j ) is the Fisher kernel for γ i associated with component m (= 0, 1)
Km (γ i , γ j ) = (∇Θm log p(γ i ; αm , Dm ))T I−1
m ∇Θm log p(γ i ; αm , Dm ),
K (yi , yj ) = Ez i ,γ i ,zj ,γ j (K ((z i , γ i ), (z j , γ j ))| yi , yj ; ti , tj , (cid:102)M)
The kernel for yi and yj is deﬁned similarly as in Design A:
Design C: (cid:102)M = Mm , m = 0, 1
where the integral can be evaluated analytically.
This design uses one mixed-effect component in-
stead of the mixture as the generative model, as il-
lustrated in Figure 3 (right panel). Although any
single Mm is not a satisfying generative model
for the whole population, the resulting kernel is
still useful for classiﬁcation as follows. For either
model, m = 0, 1, the Fisher score for the ith indi-
vidual ∇Θm log p(γ i ; Θm ) describes how the prob-
ability p(γ i ; Θm ) responds to the change of param-
eters Θm . This is a discriminative feature vector
since the likelihood of γi for individuals from dif-
Design C
Design B
Design A
ferent group are likely to have different response to
the change of parameters Θm . The kernel between
Figure 3: The graphical model of the mixture of
γ i and γ j is Km (γ i , γ j ) deﬁned in Equation (13).
mixed-effect models for Design A, B, and C.
And then the kernel for yi and yj :
K (yi , yj ) = Eγ i ,γ j (K (γ i , γ j )| yi , yj ; ti , tj , Mm )
(15)
Our experiments show that the kernel based on the impaired group is signiﬁcantly better than others; we
therefore use this kernel as the representative of Design C. It is easy to see that the designed kernel is a
special case of Design A or Design B when π0 = 1 and π1 = 0.
3.4 Related Models
Marginalized Kernel Our HFK is related to the marginalized kernel (MK) proposed by Tsuda et. al. [10].
(cid:88)
(cid:88)
(cid:101)K (xi , xj ) =
P (hi |xi )P (hj |xj ) (cid:101)K (xi , xj )
MK uses a distribution with discrete latent variable h (indicating the generating component) and observable
x, which form a complete data pair x = (h, x). The kernel for observable xi and xj is deﬁned as
where (cid:101)K (xi , xj ) is the joint kernel for complete data. Tsuda et. al. [10] uses the form:
(cid:101)K (xi , xj ) = δ(hi , hj )Khi (xi , xj ),
hj
hi
(17)
tion (17) says that (cid:101)K (xi , xj ) takes the value of kernel deﬁned for the mth component model if xi and xj
are generated from the same component hi = hj = m; otherwise, (cid:101)K (xi , xj ) = 0. HFK can be viewed as a
where Khi (xi , xj ) is the pre-deﬁned kernel for observables associated the hi generative component. Equa-
(cid:101)K (xi , xj ) = Ehi ,hj ( (cid:101)K (xi , xj )|xi , xj )
special case of the generalized marginalized kernel that allows continuous latent variables h. This is clear if
and view (cid:101)K (xi , xj ) as a generalization of kernel between hi and hj . Nevertheless HFK is different from the
we re-write Equation (16) as
original work in [10], in that MK requires existing kernels for observable, such as Kh (xi , xj ) in Equation
(17). In our problem setting, this kernel does not exist due to the different lengths of time series.

(16)

6

(cid:80)
K (yi , yj ) = Evi ,vj (K (v i , v j )| yi , yj ; ti , tj , (cid:102)M) =
m P (z i = m|yi ; ti , (cid:102)M)P (z j = m|yj ; tj , (cid:102)M),
Probability Product Kernel We can get a family of kernels by employing various kernel designs
of K (v i , v j ). The simplest example is to let K (v i , v j ) = δ(z i , z j ), which immediately leads to
which is obviously related to the posterior probabilities of samples, and is essentially a special case of the
probability product kernels [5] proposed by Jebara et. al.
4 Experiments
Performance Evaluation We use the empirical ROC curve (detection rate vs. false alarm rate) to eval-
uate classiﬁers. We compare different classiﬁers using the area under the curve (AUC), and calculate the
statistical signiﬁcance following the method given by Pepe [9]. We tested the classiﬁers on the ﬁve features:
steps, seconds, tappingD, tappingN, and logical memory II. The results of delayed-recall are omitted, they are
very close to those for logical memory II. The mixed-effect models for each feature were trained separately
with order-1 polynomials (linear) as the basis functions. For each feature, the kernels are used in support
vector machines (SVM) for classiﬁcation, and the ROC is obtained by thresholding the classiﬁer output with
varying values. The classiﬁers are evaluated by leave-one-out cross-validation, the left-out sample consisting
of an individual subject’s complete time series (which is also held out of the ﬁtting of the generative model).
Classiﬁers For comparison, we also examined the following two classiﬁers. First, we consider the likeli-
hood ratio test based on mixed-effect models {M0 , M1 }. For any given observation (t, y), the likelihood
that it is generated by mixed-effect model Mm is given by p(y; t, Mm ), which is deﬁned similarly as in
Equation (3). The classiﬁcation decision for a likelihood ratio classiﬁer is made by thresholding the ratio
p(y;t,M0 )
p(y;t,M1 ) . Second, we consider a feature extraction routine independent of any generative model. We sum-
marize each individual i with the least-square ﬁt coefﬁcients for a d-degree polynomial regression model,
denoted as pi . To get a reliable ﬁtting we only consider the case d = 1 since many individuals only have four
or ﬁve observations. We use the coefﬁcients (normalized to their s.t.d.), denoted as ˆpi , as the feature vector,
and build a RBF kernel Gij = exp(− || ˆpi− ˆpj ||2
), where s is the kernel width estimated with leave-one-out
2
2s2
cross validation in our experiment. The obtained kernel matrix G will be referred to as LSQ kernel.
Results We ﬁrst compare three HFK designs, using the ROC curves plotted in Figure 4 (upper row). On
all four motor tests, Design A and Design B are very much comparable except on tappingD, on which
Design A is marginally better than Design B with p = 0.136. Also on the motor tests, Design C is slightly
but consistently better than other two designs. On logical memory II (story recall), the three designs have
comparable performance. We thus use Design C as the representative of HFK, and compare it with the
likelihood ratio classiﬁer and SVM based on LSQ kernel, as shown in Figure 4 (lower row). On four motor
test, the classiﬁer based on HFK obviously out-performs the other two classiﬁers, and on logical memory II,
the three classiﬁers have very much comparable performance.
5 Discussion
Fisher kernels derived from mixed-effect generative models retain the Fisher information matrix, and hence
the proper invariance of the kernel under change of coordinates in the model parameter space. In additional
experiments, classiﬁers constructed with the proper kernel out-perform those constructed with the identity
matrix in place of the Fisher information on our data. For example, on seconds, the HKF (Deign C) achieves
AUC = 0.7333, while the Fisher kernel computed with the identity matrix as metric on p(yi ; ti , M) achieves
a AUC = 0.6873, with the p-value (Z-test) 0.0435.
Our classiﬁers built with Fisher kernels derived from mixed-effect models outperform those based solely
on the generative model (using likelihood ratio tests) for the motor task data, and are comparable on the
psychometric tests. The hierarchical kernels also produce better classiﬁers than a standard SVM using the
coefﬁcients of a least squares ﬁt to the individual’s data. This shows that the generative model provides real
advantage for classiﬁcation. The mixed-effect models capture both the population behavior (through α),
and the statistical variability of the individual subject models (through the covariance of β ). Knowledge of

7

steps
p1 =0.486, p2 =0.326

seconds
p1 =0.387, p2 =0.158

tappingD
p1 =0.136, p2 =0.210

tappingN
p1 =0.482, p2 =0.286

logical memory II
p1 =0.491, p2 =0.452

p1 =0.041, p2 =0.038

p1 =0.056, p2 =0.083

p1 =0.042, p2 =0.085

p1 =0.38, p2 =0.049

p1 =0.485, p2 =0.523

Figure 4: Comparison of classiﬁers. Upper row: Three HFK designs. The number in the parenthesis is the
p-value (Z-test) for the null-hypothesis “the AUC of Classiﬁer 1 is the same as the AUC of Classiﬁer 2”.
Upper row: Three HKF designs. p1 : Design A vs. Design B, p2 : Design C vs. Design A; Lower row:
HFK & other classiﬁers. p1 : Design C vs. Likelihood ratio, p2 : Design C vs. LSQ kernel.

the statistics of the subject variability is extremely important for classiﬁcation: although not discussed here,
classiﬁers based only on the population model (α) perform far worse than those presented here [7].
Acknowledgements
This work was supported by Intel Corp. under the OHSU BAIC award. Milar Moore and to Robin Guariglia
of the Layton Aging & Alzheimer’s Disease Center gave invaluable help with data from the Oregon Brain
Aging Study. We thank Misha Pavel, Tamara Hayes, and Nichole Carlson for helpful discussion.
References
[1] R. Camicioli, D. Howieson, B. Oken, G. Sexton, and J. Kaye. Motor slowing precedes cognitive impairment in the
oldest old. Neurology, 50:1496–1498, 1998.
[2] M. Green, J. Kaye, and M. Ball. The Oregon brain aging study: Neuropathology accompanying healthy aging in
the oldest old. Neurology, 54(1):105–113, 2000.
[3] T. Jaakkola, M. Diekhaus, and D. Haussler. Using the ﬁsher kernel method to detect remote protein homologies.
7th Intell. Sys. Mol. Biol., pages 149–158, 1999.
[4] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. Technical report, Dept. of
Computer Science, Univ. of California, 1998.
[5] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. Journal of Machine Learning Research, 5:819–
844, 2004.
[6] N. Laird and J. Ware. Random-effects models for longitudinal data. Biometrics, 38(4):963–974, 1982.
[7] Z. Lu. Constrained Clustering and Cognitive Decline Detection. PhD thesis, OHSU, 2008.
[8] S. Marquis, M. Moore, D. Howieson, G. Sexton, H. Payami, J. Kaye, and R. Camicioli. Independent predictors of
cognitive decline in healthy elderly persons. Arch. Neurol., 59:601–606, 2002.
[9] M. Pepe. The Statistical Evaluation of Medical Tests for Classiﬁcation and Prediction. Oxford University Press,
Oxford, 2003.
[10] K. Tsuda, T. Kin, and K. Asai. Marginalized kernels for biological sequences. Bioinformatics, 1(1):1–8, 2002.

8

00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignADesignBDesignC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignADesignBDesignC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignADesignBDesignC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignADesignBDesignC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignADesignBDesignC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignCLSQKLKHD00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignCLSQKLKHD00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignCLSQKLKHD00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignCLSQKLKHD00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Alarm RateDetection Rate  DesignCLSQKLKHD
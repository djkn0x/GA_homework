Exploring Large Feature Spaces
with Hierarchical Multiple Kernel Learning

Francis Bach
INRIA - Willow Project, ´Ecole Normale Sup ´erieure
45, rue d’Ulm, 75230 Paris, France
francis.bach@mines.org

Abstract

For supervised and unsupervised learning, positive de ﬁnit e kernels allow to use
large and potentially in ﬁnite dimensional feature spaces w ith a computational cost
that only depends on the number of observations. This is usually done through
the penalization of predictor functions by Euclidean or Hilbertian norms. In this
paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or
the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of
individual basis kernels which can be embedded in a directed acyclic graph; we
show that it is then possible to perform kernel selection through a hierarchical
multiple kernel learning framework, in polynomial time in the number of selected
kernels. This framework is naturally applied to non linear variable selection; our
extensive simulations on synthetic datasets and datasets from the UCI repository
show that efﬁciently exploring the large feature space thro ugh sparsity-inducing
norms leads to state-of-the-art predictive performance.

1 Introduction

In the last two decades, kernel methods have been a proliﬁc th eoretical and algorithmic machine
learning framework. By using appropriate regularization by Hilbertian norms, representer theorems
enable to consider large and potentially in ﬁnite-dimensio nal feature spaces while working within an
implicit feature space no larger than the number of observations. This has led to numerous works on
kernel design adapted to speciﬁc data types and generic kern el-based algorithms for many learning
tasks (see, e.g., [1, 2]).
Regularization by sparsity-inducing norms, such as the ℓ1 -norm has also attracted a lot of interest in
recent years. While early work has focused on efﬁcient algor ithms to solve the convex optimization
problems, recent research has looked at the model selection properties and predictive performance of
such methods, in the linear case [3] or within the multiple kernel learning framework (see, e.g., [4]).
In this paper, we aim to bridge the gap between these two lines of research by trying to use ℓ1 -norms
inside the feature space. Indeed, feature spaces are large and we expect the estimated predictor
function to require only a small number of features, which is exactly the situation where ℓ1 -norms
have proven advantageous. This leads to two natural questions that we try to answer in this paper: (1)
Is it feasible to perform optimization in this very large feature space with cost which is polynomial
in the size of the input space? (2) Does it lead to better predictive performance and feature selection?

More precisely, we consider a positive de ﬁnite kernel that c an be expressed as a large sum of positive
de ﬁnite basis or local kernels. This exactly corresponds to the situation where a large feature space is
the concatenation of smaller feature spaces, and we aim to do selection among these many kernels,
which may be done through multiple kernel learning. One major difﬁculty however is that the
number of these smaller kernels is usually exponential in the dimension of the input space and
applying multiple kernel learning directly in this decomposition would be intractable.

In order to peform selection efﬁciently, we make the extra as sumption that these small kernels can
be embedded in a directed acyclic graph (DAG). Following [5], we consider in Section 2 a spe-
ciﬁc combination of
ℓ2 -norms that is adapted to the DAG, and will restrict the authorized sparsity
patterns; in our speciﬁc kernel framework, we are able to use the DAG to design an optimization
algorithm which has polynomial complexity in the number of selected kernels (Section 3). In simu-
lations (Section 5), we focus on directed grids, where our framework allows to perform non-linear
variable selection. We provide extensive experimental validation of our novel regularization frame-
work; in particular, we compare it to the regular ℓ2 -regularization and shows that it is always com-
petitive and often leads to better performance, both on synthetic examples, and standard regression
and classiﬁcation datasets from the UCI repository.

Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple ker-
nel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization
framework by giving necessary and sufﬁcient conditions for model consistency. In particular, we
show that our framework is adapted to estimating consistently only the hull of the relevant variables.
Hence, by restricting the statistical power of our method, we gain computational efﬁciency.

2 Hierarchical multiple kernel learning (HKL)
We consider the problem of predicting a random variable Y ∈ Y ⊂ R from a random variable X ∈
X , where X and Y may be quite general spaces. We assume that we are given n i.i.d. observations
(xi , yi ) ∈ X × Y , i = 1, . . . , n. We de ﬁne the
empirical risk of a function f from X to R as
n Pn
1
i=1 ℓ(yi , f (xi )), where ℓ : Y × R 7→ R+ is a loss function. We only assume that ℓ is convex
with respect to the second parameter (but not necessarily differentiable). Typical examples of loss
functions are the square loss for regression, i.e., ℓ(y , ˆy) = 1
2 (y − ˆy )2 for y ∈ R, and the logistic loss
ℓ(y , ˆy) = log(1 + e−y ˆy ) or the hinge loss ℓ(y , ˆy) = max{0, 1 − y ˆy} for binary classiﬁcation, where
y ∈ {−1, 1}, leading respectively to logistic regression and support vector machines [1, 2].
2.1 Graph-structured positive de ﬁnite kernels
We assume that we are given a positive de ﬁnite kernel k : X × X → R, and that this kernel can
be expressed as the sum, over an index set V , of basis kernels kv , v ∈ V , i.e, for all x, x′ ∈ X ,
k(x, x′ ) = Pv∈V kv (x, x′ ). For each v ∈ V , we denote by Fv and Φv the feature space and feature
map of kv , i.e., for all x, x′ ∈ X , kv (x, x′ ) = hΦv (x), Φv (x′ )i. Throughout the paper, we denote
by kuk the Hilbertian norm of u and by hu, vi the associated dot product, where the precise space is
omitted and can always be inferred from the context.
Our sum assumption corresponds to a situation where the feature map Φ(x) and feature space F
for k is the concatenation of the feature maps Φv (x) for each kernel kv , i.e, F = Qv∈V Fv and
Φ(x) = (Φv (x))v∈V . Thus, looking for a certain β ∈ F and a predictor function f (x) = hβ , Φ(x)i
is equivalent to looking jointly for βv ∈ Fv , for all v ∈ V , and f (x) = Pv∈V hβv , Φv (x)i.
As mentioned earlier, we make the assumption that the set V can be embedded into a directed acyclic
graph. Directed acyclic graphs (DAGs) allow to naturally de ﬁne th e notions of parents, children,
descendants and ancestors. Given a node w ∈ V , we denote by A(w) ⊂ V the set of its ancestors,
and by D(w) ⊂ V , the set of its descendants. We use the convention that any w is a descendant
and an ancestor of itself, i.e., w ∈ A(w) and w ∈ D(w). Moreover, for W ⊂ V , we let denote
sources(W ) the set of sources of the graph G restricted to W (i.e., nodes in W with no parents
belonging to W ). Given a subset of nodes W ⊂ V , we can de ﬁne the hull of W as the union of all
ancestors of w ∈ W , i.e., hull(W ) = Sw∈W A(w). Given a set W , we de ﬁne the set of extreme
points of W as the smallest subset T ⊂ W such that hull(T ) = hull(W ) (note that it is always well
de ﬁned, as TT ⊂V , hull(T )=hull(W ) T ). See Figure 1 for examples of these notions.
The goal of this paper is to perform kernel selection among the kernels kv , v ∈ V . We essentially
use the graph to limit the search to speciﬁc subsets of V . Namely, instead of considering all possible
subsets of active (relevant) vertices, we are only interested in estimating correctly the hull of these
relevant vertices; in Section 2.2, we design a speciﬁc spars ity-inducing norms adapted to hulls.

In this paper, we primarily focus on kernels that can be expressed as “products of sums”, and on the
associated p-dimensional directed grids, while noting that our framework is applicable to many other
kernels. Namely, we assume that the input space X factorizes into p components X = X1 × · · · × Xp
and that we are given p sequences of length q + 1 of kernels kij (xi , x′
i ), i ∈ {1, . . . , p}, j ∈

Figure 1: Example of graph and associated notions. (Left) Example of a 2D-grid. (Middle) Example
of sparsity pattern (× in light blue) and the complement of its hull (+ in light red). (Right) Dark
blue points (×) are extreme points of the set of all active points (blue ×); dark red points (+) are the
sources of the set of all red points (+).

i=1 (cid:16)Pq
i )(cid:17). We
{0, . . . , q}, such that k(x, x′ ) = Pq
i ) = Qp
j1 ,...,jp=0 Qp
i=1 kiji (xi , x′
ji=0 kiji (xi , x′
thus have a sum of (q+1)p kernels, that can be computed efﬁciently as a product of p sums. A natural
DAG on V = Qp
i=1{0, . . . , q} is de ﬁned by connecting each (j1 , . . . , jp ) to (j1 + 1, j2 , . . . , jp ),
. . . , (j1 , . . . , jp−1 , jp + 1). As shown in Section 2.2, this DAG will correspond to the constraint
of selecting a given product of kernels only after all the subproducts are selected. Those DAGs
are especially suited to nonlinear variable selection, in particular with the polynomial and Gaussian
kernels. In this context, products of kernels correspond to interactions between certain variables, and
our DAG implies that we select an interaction only after all sub-interactions were already selected.
i ) = (cid:0)q
Polynomial kernels We consider Xi = R, kij (xi , x′
i )j ; the full kernel is then equal
j (cid:1)(xix′
i )j = Qp
i=1 Pq
to k(x, x′ ) = Qp
j=0 (cid:0)q
i )q . Note that this is not exactly the usual
i=1 (1 + xix′
j (cid:1)(xix′
polynomial kernel (whose feature space is the space of multivariate polynomials of total degree less
than q ), since our kernel considers polynomials of maximal degree q .
Gaussian kernels We also consider Xi = R, and the Gaussian-RBF kernel e−b(x−x′ )2 . The
following decomposition is the eigendecomposition of the non centered covariance operator for a
normal distribution with variance 1/4a (see, e.g., [6]):
Hk (√2cx′ )],
Hk (√2cx)][e− b
(b/A)k
e−b(x−x′ )2
A (a+c)x2
A (a+c)(x′ )2
= P∞
[e− b
k=0
2k k!
where c2 = a2 + 2ab, A = a + b + c, and Hk is the k-th Hermite polynomial. By appropriately
truncating the sum, i.e, by considering that the ﬁrst
q basis kernels are obtained from the ﬁrst
q
single Hermite polynomials, and the (q + 1)-th kernel is summing over all other kernels, we ob-
tain a decomposition of a uni-dimensional Gaussian kernel into q + 1 components (q of them are
one-dimensional, the last one is in ﬁnite-dimensional, but can be computed by differencing). The
decomposition ends up being close to a polynomial kernel of in ﬁnite degree, modulated by an ex-
ponential [2]. One may also use an adaptive decomposition using kernel PCA (see, e.g., [2, 1]),
which is equivalent to using the eigenvectors of the empirical covariance operator associated with
the data (and not the population one associated with the Gaussian distribution with same variance).
In simulations, we tried both with no signiﬁcant difference s.
ANOVA kernels When q = 1, the directed grid is isomorphic to the power set (i.e., the set
of subsets) with the inclusion DAG. In this setting, we can decompose the ANOVA kernel [2] as
J k2
i )2
i )2
Qp
= PJ ⊂{1,...,p} e−bkxJ −x′
) = PJ ⊂{1,...,p} Qi∈J e−b(xi−x′
i=1 (1 + e−b(xi−x′
2 , and our
framework will select the relevant subsets for the Gaussian kernels.
Kernels or features?
In this paper, we emphasize the kernel view, i.e., we are given a kernel (and
thus a feature space) and we explore it using ℓ1 -norms. Alternatively, we could use the feature view,
i.e., we have a large structured set of features that we try to select from; however, the techniques
developed in this paper assume that (a) each feature might be in ﬁnite-dimensional and (b) that we
can sum all the local kernels efﬁciently (see in particular S ection 3.2). Following the kernel view
thus seems slightly more natural.

2.2 Graph-based structured regularization
Given β ∈ Qv∈V Fv , the natural Hilbertian norm kβk is de ﬁned through kβk2 = Pv∈V kβv k2 .
Penalizing with this norm is efﬁcient because summing all ke rnels kv is assumed feasible in polyno-
mial time and we can bring to bear the usual kernel machinery; however, it does not lead to sparse
solutions, where many βv will be exactly equal to zero.

As said earlier, we are only interested in the hull of the selected elements βv ∈ Fv , v ∈ V ; the hull
of a set I is characterized by the set of v , such that D(v) ⊂ I c , i.e., such that all descendants of v
are in the complement I c : hull(I ) = {v ∈ V , D(v) ⊂ I c }c . Thus, if we try to estimate hull(I ), we
need to determine which v ∈ V are such that D(v) ⊂ I c . In our context, we are hence looking at
selecting vertices v ∈ V for which βD(v) = (βw )w∈D(v) = 0.
We thus consider the following structured block ℓ1 -norm de ﬁned as Pv∈V dv kβD(v)k =
Pv∈V dv (Pw∈D(v) kβw k2 )1/2 , where (dv )v∈V are positive weights. Penalizing by such a norm
will indeed impose that some of the vectors βD(v) ∈ Qw∈D(v) Fw are exactly zero. We thus con-
sider the following minimization problem1:
2 (cid:0)Pv∈V dv kβD(v)k(cid:1)2
n Pn
i=1 ℓ(yi , Pv∈V hβv , Φv (xi )i) + λ
1
(1)
.
minβ∈Qv∈V Fv
Our Hilbertian norm is a Hilbert space instantiation of the hierarchical norms recently introduced
by [5] and also considered by [7] in the MKL setting. If all Hilbert spaces are ﬁnite dimensional, our
particular choice of norms corresponds to an “ ℓ1 -norm of ℓ2 -norms”. While with uni-dimensional
groups/kernels, the “ ℓ1 -norm of ℓ∞ -norms” allows an efﬁcient path algorithm for the square los
s
and when the DAG is a tree [5], this is not possible anymore with groups of size larger than one, or
when the DAG is a not a tree. In Section 3, we propose a novel algorithm to solve the associated
optimization problem in time polynomial in the number of selected groups/kernels, for all group
sizes, DAGs and losses. Moreover, in Section 4, we show under which conditions a solution to the
problem in Eq. (1) consistently estimates the hull of the sparsity pattern.

Finally, note that in certain settings ( ﬁnite dimensional H ilbert spaces and distributions with abso-
lutely continuous densities), these norms have the effect of selecting a given kernel only after all of
its ancestors [5]. This is another explanation why hulls end up being selected, since to include a
given vertex in the models, the entire set of ancestors must also be selected.

3 Optimization problem
In this section, we give optimality conditions for the problems in Eq. (1), as well as optimization
algorithms with polynomial time complexity in the number of selected kernels. In simulations we
consider total numbers of kernels larger than 1030 , and thus such efﬁcient algorithms are essential
to the success of hierarchical multiple kernel learning (HKL).

3.1 Reformulation in terms of multiple kernel learning

Following [8, 9], we can simply derive an equivalent formulation of Eq. (1). Using Cauchy-Schwarz
inequality, we have that for all η ∈ RV such that η > 0 and Pv∈V d2
v ηv 6 1,
kβD(v) k2
(Pv∈V dv kβD(v)k)2 6 Pv∈V
= Pw∈V (Pv∈A(w) η−1
v )kβw k2 ,
ηv
with equality if and only if ηv = d−1
v kβD(v)k(Pv∈V dv kβD(v)k)−1 . We associate to the vector
v . We use the natural convention
η ∈ RV , the vector ζ ∈ RV such that ∀w ∈ V , ζ −1
w = Pv∈A(w) η−1
that if ηv is equal to zero, then ζw is equal to zero for all descendants w of v . We let denote H the
set of allowed η and Z the set of all associated ζ . The set H and Z are in bijection, and we can
interchangeably use η ∈ H or the corresponding ζ (η) ∈ Z . Note that Z is in general not convex 2
(unless the DAG is a tree, see [10]), and if ζ ∈ Z , then ζw 6 ζv for all w ∈ D(v), i.e., weights of
descendant kernels are smaller, which is consistent with the known fact that kernels should always
be selected after all their ancestors.

The problem in Eq. (1) is thus equivalent to
n Pn
i=1 ℓ(yi , Pv∈V hβv , Φv (xi )i) + λ
1
2 Pw∈V ζw (η)−1 kβw k2 .
min
min
η∈H
β∈Qv∈V Fv
Using the change of variable ˜βv = βv ζ −1/2
and ˜Φ(x) = (ζ 1/2
v Φv (x))v∈V , this implies that given
v
the optimal η (and associated ζ ), β corresponds to the solution of the regular supervised learning
problem with kernel matrix K = Pw∈V ζwKw , where Kw is n × n the kernel matrix associated
1We consider the square of the norm, which does not change the regularization properties, but allow simple
links with multiple kernel learning.
2Although Z is not convex, we can still maximize positive linear combinations over Z , which is the only
needed operation (see [10] for details).

(2)

with kernel kw . Moreover, the solution is then βw = ζw Pn
i=1 αiΦw (xi ), where α ∈ Rn are the
dual parameters associated with the single kernel learning problem.
Thus, the solution is entirely determined by α ∈ Rn and η ∈ RV (and its corresponding ζ ∈ RV ).
More precisely, we have (see proof in [10]):
Proposition 1 The pair (α, η) is optimal for Eq. (1), with ∀w, βw = ζw Pn
i=1 αiΦw (xi ), if and
only if (a) given η , α is optimal for the single kernel learning problem with kernel matrix K =
Pw∈V ζw (η)Kw , and (b) given α, η ∈ H maximizes Pw∈V (Pv∈A(w) η−1
v )−1α⊤Kwα.
Moreover, the total duality gap can be upperbounded as the sum of the two separate duality gaps for
the two optimization problems, which will be useful in Section 3.2 (see [10] for more details). Note
that in the case of “
ﬂat” regular multiple kernel learning, w
here the DAG has no edges, we obtain
back usual optimality conditions [8, 9].

Following a common practice for convex sparsity problems [11], we will try to solve a small problem
where we assume we know the set of v such that kβD(v)k is equal to zero (Section 3.3). We then
“simply ” need to check that variables in that set may indeed b e left out of the solution. In the next
section, we show that this can be done in polynomial time although the number of kernels to consider
leaving out is exponential (Section 3.2).

3.2 Conditions for global optimality of reduced problem

We let denote J the complement of the set of norms which are set to zero. We thus consider the
optimal solution β of the reduced problem (on J ), namely,
2 (cid:0)Pv∈V dv kβD(v)∩J k(cid:1)2
n Pn
1
i=1 ℓ(yi , Pv∈J hβv , Φv (xi )i) + λ
(3)
minβJ ∈Qv∈J Fv
,
with optimal primal variables βJ , dual variables α and optimal pair (ηJ , ζJ ). We now consider
necessary conditions and sufﬁcient conditions for this sol ution (augmented with zeros for non active
variables, i.e., variables in J c ) to be optimal with respect to the full problem in Eq. (1). We denote
by δ = Pv∈J dv kβD(v)∩J k the optimal value of the norm for the reduced problem.
Proposition 2 (NJ )
If the reduced solution is optimal for the full problem in Eq. (1) and all kernels
t 6 δ2 .
in the extreme points of J are active, then we have maxt∈sources(J c ) α⊤Ktα/d2
Proposition 3 (SJ,ε )
If maxt∈sources(J c ) Pw∈D(t) α⊤Kwα/(Pv∈A(w)∩D(t) dv )2 6 δ2 + ε/λ,
then the total duality gap is less than ε.

The proof is fairly technical and can be found in [10]; this result constitutes the main technical
contribution of the paper:
it essentially allows to solve a very large optimization problem over
exponentially many dimensions in polynomial time.
The necessary condition (NJ ) does not cause any computational problems. However, the sufﬁcient
condition (SJ,ε ) requires to sum over all descendants of the active kernels, which is impossible in
practice (as shown in Section 5, we consider V of cardinal often greater than 1030 ). Here, we need
to bring to bear the speciﬁc structure of the kernel k . In the context of directed grids we consider
in this paper, if dv can also be decomposed as a product, then Pv∈A(w)∩D(t) dv is also factorized,
and we can compute the sum over all v ∈ D(t) in linear time in p. Moreover we can cache the sums
Pw∈D(t) Kw /(Pv∈A(w)∩D(t) dv )2 in order to save running time.
3.3 Dual optimization for reduced or small problems
When kernels kv , v ∈ V have low-dimensional feature spaces, we may use a primal rep-
resentation and solve the problem in Eq. (1) using generic optimization toolboxes adapted to
conic constraints (see, e.g., [12]). However, in order to reuse existing optimized supervised
learning code and use high-dimensional kernels,
it is preferable to use a dual optimization.
Namely, we use the same technique as [8]: we consider for ζ ∈ Z , the function B (ζ ) =
n Pn
1
i=1 ℓ(yi , Pv∈V hβv , Φv (xi )i)+ λ
w kβw k2 , which is the optimal value
2 Pw∈V ζ −1
minβ∈Qv∈V Fv
of the single kernel learning problem with kernel matrix Pw∈V ζwKw . Solving Eq. (2) is equivalent
to minimizing B (ζ (η)) with respect to η ∈ H .
If a ridge (i.e., positive diagonal) is added to the kernel matrices, the function B is differentiable [8].
Moreover, the function η 7→ ζ (η) is differentiable on (R∗
+ )V . Thus, the function η 7→ B [ζ ((1 −

ε)η + ε
|V | d−2 )] , where d−2 is the vector with elements d−2
v , is differentiable if ε > 0. We can then
use the same projected gradient descent strategy as [8] to minimize it. The overall complexity of
the algorithm is then proportional to O(|V |n2 )—to form the kernel matrices—plus the complexity
of solving a single kernel learning problem —typically betw een O(n2 ) and O(n3 ). Note that this
algorithm is only used for small reduced subproblems for which V has small cardinality.

3.4 Kernel search algorithm

We are now ready to present the detailed algorithm which extends the feature search algorithm
of [11]. Note that the kernel matrices are never all needed explicitly, i.e., we only need them (a)
explicitly to solve the small problems (but we need only a few of those) and (b) implicitly to compute
the sufﬁcient condition (SJ,ε ), which requires to sum over all kernels, as shown in Section 3.2.
• Input: kernel matrices Kv ∈ Rn×n , v ∈ V , maximal gap ε, maximal # of kernels Q
• Algorithm
1. Initialization: set J = sources(V ),
compute (α, η) solutions of Eq. (3), obtained using Section 3.3
2. while (NJ ) and (SJ,ε ) are not satisﬁed and #(V ) 6 Q
If (NJ ) is not satisﬁed, add violating variables in sources(J c ) to J
else, add violating variables in sources(J c ) of (SJ,ε ) to J
– Recompute (α, η) optimal solutions of Eq. (3)
• Output: J , α, η
The previous algorithm will stop either when the duality gap is less than ε or when the maximal
number of kernels Q has been reached. In practice, when the weights dv increase with the depth of
v in the DAG (which we use in simulations), the small duality gap generally occurs before we reach
a problem larger than Q. Note that some of the iterations only increase the size of the active sets to
check the sufﬁcient condition for optimality; forgetting t hose does not change the solution, only the
fact that we may actually know that we have an ε-optimal solution.
In order to obtain a polynomial complexity, the maximal out-degree of the DAG (i.e., the maximal
number of children of any given node) should be polynomial as well. Indeed, for the directed p-
grid (with maximum out-degree equal to p), the total running time complexity is a function of the
number of observations n, and the number R of selected kernels; with proper caching, we obtain the
following complexity, assuming O(n3 ) for the single kernel learning problem, which is conservative:
O(n3R + n2Rp2 + n2R2p), which decomposes into solving O(R) single kernel learning problems,
caching O(Rp) kernels, and computing O(R2 p) quadratic forms for the sufﬁcient conditions.

4 Consistency conditions

As said earlier, the sparsity pattern of the solution of Eq. (1) will be equal to its hull, and thus we
can only hope to obtain consistency of the hull of the pattern, which we consider in this section. For
simplicity, we consider the case of ﬁnite dimensional Hilbe rt spaces (i.e., Fv = Rfv ) and the square
loss. We also hold ﬁxed the vertex set of V , i.e., we assume that the total number of features is ﬁxed,
and we let n tend to in ﬁnity and λ = λn decrease with n.
Following [4], we make the following assumptions on the underlying joint distribution of (X, Y ):
(a) the joint covariance matrix Σ of (Φ(xv ))v∈V (de ﬁned with appropriate blocks of size fv × fw )
is invertible, (b) E (Y |X ) = Pw∈W hβw , Φw (x)i with W ⊂ V and var(Y |X ) = σ2 > 0 almost
surely. With these simple assumptions, we obtain (see proof in [10]):
kΣwW Σ−1
W W Diag(dv kβD(v) k−1 )v∈W βW k2
Proposition 4 (Suf ﬁcient condition)
If max
t∈sources(W c )Pw∈D(t)
(Pv∈A(w)∩D(t) dv )2
< 1, then β and the hull of W are consistently estimated when λnn1/2 → ∞ and λn → 0.
Proposition 5 (Necessary condition) If the β and the hull of W are consistently estimated for
some sequence λn , then maxt∈sources(W c ) kΣwW Σ−1
W W Diag(dv /kβD(v) k)v∈W βW k2/d2
t 6 1.
Note that the last two propositions are not consequences of the similar results for ﬂat MKL [4],
because the groups that we consider are overlapping. Moreover, the last propositions show that we
indeed can estimate the correct hull of the sparsity pattern if the sufﬁcient condition is satisﬁed. In
particular, if we can make the groups such that the between-group correlation is as small as possible,

–
1

0.5

r
o
r
r
e
 
t
e
s
 
t
s
e
t

HKL
greedy
L2

 

1

0.5

r
o
r
r
e
 
t
e
s
 
t
s
e
t

HKL
greedy
L2

 

0
 
2

3

4
5
4
5
log
(p)
log
(p)
2
2
Figure 2: Comparison on synthetic examples: mean squared error over 40 replications (with halved
standard deviations). Left: non rotated data, right: rotated data. See text for details.

6

6

7

7

0
 
2

3

dataset
HKL
lasso-α MKL
greedy
L2
k #(V )
p
n
44.2±1.3 43.9±1.4 47.9±0.7 44.5±1.1 43.3±1.0
abalone
4177 10 pol4 ≈107
4177 10 rbf ≈1010 43.0±0.9 45.0±1.7 49.0±1.7 43.7±1.0 43.0±1.1
abalone
8192 32 pol4 ≈1022 40.1±0.7 39.2±0.8 41.3±0.7 38.7±0.7 38.9±0.7
bank-32fh
8192 32 rbf ≈1031 39.0±0.7 39.7±0.7 66.1±6.9 38.4±0.7 38.4±0.7
bank-32fh
5.0±0.2
bank-32fm 8192 32 pol4 ≈1022
5.1±0.1
7.0±0.2
6.0±0.1
6.1±0.3
4.6±0.2
bank-32fm 8192 32 rbf ≈1031
5.7±0.2
5.8±0.4
36.3±4.1 5.9±0.2
8192 32 pol4 ≈1022 44.3±1.2 46.3±1.4 45.8±0.8 46.0±1.2 43.6±1.1
bank-32nh
8192 32 rbf ≈1031 44.3±1.2 49.4±1.6 93.0±2.8 46.1±1.1 43.5±1.0
bank-32nh
bank-32nm 8192 32 pol4 ≈1022 17.2±0.6 18.2±0.8 19.5±0.4 21.0±0.7 16.8±0.6
bank-32nm 8192 32 rbf ≈1031 16.9±0.6 21.0±0.6 62.3±2.5 20.9±0.7 16.4±0.6
17.1±3.6 24.7±10.8 29.3±2.3 22.2±2.2 18.1±3.8
boston
506 13 pol4 ≈109
506 13 rbf ≈1012 16.4±4.0 32.4±8.2 29.4±1.6 20.7±2.1 17.1±4.7
boston
pumadyn-32fh 8192 32 pol4 ≈1022 57.3±0.7 56.4±0.8 57.5±0.4 56.4±0.7 56.4±0.8
pumadyn-32fh 8192 32 rbf ≈1031 57.7±0.6 72.2±22.5 89.3±2.0 56.5±0.8 55.7±0.7
3.1±0.0
pumadyn-32fm 8192 32 pol4 ≈1022
6.9±0.1
6.4±1.6
7.5±0.2
7.0±0.1
3.4±0.0
pumadyn-32fm 8192 32 rbf ≈1031
5.0±0.1 46.2±51.6 44.7±5.7 7.1±0.1
pumadyn-32nh 8192 32 pol4 ≈1022 84.2±1.3 73.3±25.4 84.8±0.5 83.6±1.3 36.7±0.4
pumadyn-32nh 8192 32 rbf ≈1031 56.5±1.1 81.3±25.0 98.1±0.7 83.7±1.3 35.5±0.5
pumadyn-32nm 8192 32 pol4 ≈1022 60.1±1.9 69.9±32.8 78.5±1.1 77.5±0.9 5.5±0.1
pumadyn-32nm 8192 32 rbf ≈1031 15.7±0.4 67.3±42.4 95.9±1.9 77.6±0.9 7.2±0.1
Table 1: Mean squared errors (multiplied by 100) on UCI regression datasets, normalized so that the
total variance to explain is 100. See text for details.

we can ensure correct hull selection. Finally, it is worth noting that if the ratios dw / maxv∈A(w) dv
tend to in ﬁnity slowly with n, then we always consistently estimate the depth of the hull, i.e., the
optimal interaction complexity. We are currently investigating extensions to the non parametric
case [4], in terms of pattern selection and universal consistency.

5 Simulations
Synthetic examples We generated regression data as follows: n = 1024 samples of p ∈ [22 , 27 ]
variables were generated from a random covariance matrix, and the label y ∈ R was sampled as a
random sparse fourth order polynomial of the input variables (with constant number of monomials).
We then compare the performance of our hierarchical multiple kernel learning method (HKL) with
the polynomial kernel decomposition presented in Section 2 to other methods that use the same
kernel and/or decomposition: (a) the greedy strategy of selecting basis kernels one after the other, a
procedure similar to [13], and (b) the regular polynomial kernel regularization with the full kernel
(i.e., the sum of all basis kernels). In Figure 2, we compare the two approaches on 40 replications in
the following two situations: original data (left) and rotated data (right), i.e., after the input variables
were transformed by a random rotation (in this situation, the generating polynomial is not sparse
anymore). We can see that in situations where the underlying predictor function is sparse (left),
HKL outperforms the two other methods when the total number of variables p increases, while in
the other situation where the best predictor is not sparse (right), it performs only slightly better: i.e.,
in non sparse problems, ℓ1 -norms do not really help, but do help a lot when sparsity is expected.
UCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-
RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same

HKL
greedy
L2
dataset
k #(V )
p
n
0.1±0.1
0.1±0.2
0.4±0.4
mushrooms 1024 117 pol4 ≈1082
mushrooms 1024 117 rbf ≈10112 0.1±0.2
0.1±0.2
0.1±0.2
2.0±0.3
5.9±1.3
ringnorm 1024 20 pol4 ≈1014
3.8±1.1
1.2±0.4
1.6±0.4
2.4±0.5
ringnorm 1024 20 rbf ≈1019
8.1±0.7
9.7±1.8
spambase 1024 57 pol4 ≈1040
8.3±1.0
9.4±1.3 10.6±1.7 8.4±1.0
spambase 1024 57 rbf ≈1054
2.9±0.5
twonorm 1024 20 pol4 ≈1014
4.7±0.5
3.2±0.6
2.8±0.6
twonorm 1024 20 rbf ≈1019
5.1±0.7
3.2±0.6
15.9±1.0 16.0±1.6 15.6±0.8
magic04 1024 10 pol4 ≈107
magic04 1024 10 rbf ≈1010 15.7±0.9 17.7±1.3 15.6±0.9
Table 2: Error rates (multiplied by 100) on UCI binary classiﬁcation datasets. See text for details.

kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regulariza-
tion by the ℓ1 -norm directly on the vector α, a strategy which is sometimes used in the context of
sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-α), multiple
kernel learning with the p kernels obtained by summing all kernels associated with a single variable
(MKL). For all methods, the kernels were held ﬁxed, while in T able 1, we report the performance
for the best regularization parameters obtained by 10 random half splits.

We can see from Table 1, that HKL outperforms other methods, in particular for the datasets bank-
32nm, bank-32nh, pumadyn-32nm, pumadyn-32nh, which are datasets dedicated to non linear re-
gression. Note also, that we efﬁciently explore DAGs with ve ry large numbers of vertices #(V ).
For binary classiﬁcation datasets, we compare HKL (with the logistic loss) to two other methods (L2,
greedy) in Table 2. For some datasets (e.g., spambase), HKL works better, but for some others, in
particular when the generating problem is known to be non sparse (ringnorm, twonorm), it performs
slightly worse than other approaches.

6 Conclusion
We have shown how to perform hierarchical multiple kernel learning (HKL) in polynomial time in
the number of selected kernels. This framework may be applied to many positive de ﬁnite kernels
and we have focused on polynomial and Gaussian kernels used for nonlinear variable selection.
In particular, this paper shows that trying to use ℓ1 -type penalties may be advantageous inside the
feature space. We are currently investigating applications to string and graph kernels [2].

References
[1] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002.
[2] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Camb. U. P., 2004.
[3] P. Zhao and B. Yu. On model selection consistency of Lasso. JMLR, 7:2541–2563, 2006.
[4] F. Bach. Consistency of the group Lasso and multiple kernel learning. JMLR, 9:1179–1225, 2008.
[5] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute
penalties. Ann. Stat., To appear, 2008.
[6] C. K. I. Williams and M. Seeger. The effect of the input density distribution on kernel-based classi ﬁers.
In Proc. ICML, 2000.
[7] M. Szafranski, Y. Grandvalet, and A. Rakotomamonjy. Composite kernel learning. In Proc. ICML, 2008.
[8] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMLR, 9:2491–2521, 2008.
[9] M. Pontil and C.A. Micchelli. Learning the kernel function via regularization. JMLR, 6:1099–1125, 2005.
[10] F. Bach. Exploring large feature spaces with hierarchical MKL. Technical Report 00319660, HAL, 2008.
[11] H. Lee, A. Battle, R. Raina, and A. Ng. Efﬁcient sparse co ding algorithms. In NIPS, 2007.
[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.
[13] K. Bennett, M. Momma, and J. Embrechts. Mark: A boosting algorithm for heterogeneous kernel models.
In Proc. SIGKDD, 2002.
[14] V. Roth. The generalized Lasso. IEEE Trans. on Neural Networks, 15(1), 2004.


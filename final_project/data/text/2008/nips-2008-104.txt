Extracting State Transition Dynamics from Multiple
Spike Trains with Correlated Poisson HMM

Kentaro Katahira1,2 , Jun Nishikawa2 , Kazuo Okanoya2 and Masato Okada1,2
1Graduate School of Frontier Sciences The University of Tokyo
Kashiwa, Chiba 277-8561, Japan
2RIKEN Brain Science Institute
Wako, Saitama 351-0198, Japan
katahira@mns.k.u-tokyo.ac.jp

Abstract

Neural activity is non-stationary and varies across time. Hidden Markov Models
(HMMs) have been used to track the state transition among quasi-stationary dis-
crete neural states. Within this context, independent Poisson models have been
used for the output distribution of HMMs; hence, the model is incapable of track-
ing the change in correlation without modulating the ﬁring rate. To achieve this,
we applied a multivariate Poisson distribution with correlation terms for the out-
put distribution of HMMs. We formulated a Variational Bayes (VB) inference
for the model. The VB could automatically determine the appropriate number of
hidden states and correlation types while avoiding the overlearning problem. We
developed an efﬁcient algorithm for computing posteriors using the recursive rela-
tionship of a multivariate Poisson distribution. We demonstrated the performance
of our method on synthetic data and a real spike train recorded from a songbird.

1 Introduction

Neural activities are highly non-stationary and vary from time to time according to stimuli and
internal state changes. Hidden Markov Models (HMMs) have been used for segmenting spike trains
into quasi-stationary states, in which the spike train is regarded as stationary, hence the statistics
(e.g., cross-correlation and inter-spike interval) can be calculated [1, 2, 3]. We can also calculate
these statistics by using time-binned count data (e.g., the Peri-Stimulus Time Histogram or PSTH).
However, we need a large trial set to obtain good estimates for all bins, which can be problematic
in neurophysiological experiments. HMMs enlarge the effective amount of data for estimating the
statistics. Moreover, the PSTH approach cannot be applied to cases where we cannot align spike
data to stimuli or the behaviors of animals. HMMs are suitable for such situations.

Previous studies using HMMs have assumed that all neural activities were independent of one an-
other given the hidden states; hence, the models could not discriminate states whose ﬁring rates
were almost the same but whose correlations among neurons were different. However, there has
been reports that shows the correlation between neurons changes within a fraction of a second with-
out modulating the ﬁring rate (e.g., [4]). We developed a method that enabled us to segment spike
trains based on differences in neuronal correlation as well as the ﬁring rate. Treating neuronal cor-
relations (including higher-order, and not only pairwise correlations) among multiple spike trains
has been one of the central challenges in computational neuroscience. There have been approaches
to calculating correlations by binarizing spike trains with small bin sizes [5, 6]. These approaches
are limited to treating correlations of short bin length that includes at most one spike. Here, we
introduce a multivariate Poisson distribution with a higher-order correlation structure (simply ab-
breviated as a correlated Poisson distribution) as the output distribution for HMMs. The correlated
Poisson distribution can incorporate correlation at arbitrary time intervals.

1

To construct optimal model from limited neurophysiological data, it is crucial to select a model
that has appropriate complexity, and avoid over-ﬁtting.
In our model, model complexity corre-
sponds to the number of hidden states and types of correlations (we have a choice as to whether
to include pairwise correlation, third-order correlation, or higher order correlation). The maximum
likelihood approach adopted in previous studies [1, 7, 8] cannot be used for this purpose since the
likelihood criterion simply increases as the number of model parameters increases. A number of
model-selection criteria used with the maximum likelihood approach, i.e., Akaike’s information cri-
teria (AIC), minimum description length (MDL), and Bayesian information criteria (BIC) are based
on the asymptotic assumption that only holds when a large number of data is obtained. Furthermore,
asymptotic normality, which is assumed in these criteria, does not hold in non-identiﬁable models
including HMMs [9].

In this study, we applied the variational Bayes (VB) method [10, 11] to HMMs whose output dis-
tribution is a correlated Poisson distribution. VB is one of the approximations of the Bayes method
and can avoid over-ﬁtting even when the sample size is small. An optimal model structure can
be determined based on tractable variational free energy, which is the upper bound of the negative
marginal log-likelihood. Since the variational free energy does not need the asymptotic assumption,
VB works well even when the sample size is small in practice [12]. The computation of posteri-
ors for a correlated Poisson distribution imposes serious computational burdens. We developed an
efﬁcient algorithm to calculate these by using the recurrence relationship of a multivariate Poisson
distribution [13]. To the best of our knowledge, this is the ﬁrst report that has introduced VB method
for a correlated Poisson distribution. Although Markov chain Monte Carlo (MCMC) methods has
been applied to inferring posteriors for a correlated Poisson distribution [14], MCMC schemes are
computationally demanding.

We demonstrate the performance of the method on multiple spike data both on a synthesized spike
train and real spike data recorded from the forebrain nucleus for the vocal control (HVC) of an
anesthetized songbird.

2 Method

2.1 HMM with multivariate Poisson distribution

Suppose that we obtain spike trains of C neurons by using simultaneous recordings. As pre-
processing, we ﬁrst discretize the spike trains with a non-overlapping window whose length is
∆ to obtain spike-count data. The number of spikes of neurons c in the tth window of the nth
. The spike-count data are summarized as X n,t = {xn,t
}C
c=1 and X =
trial is denoted by xn,t
{X n,t }N ,T
c
c
n=1,t=1 . Let us assume spike-count data-set X is produced by a K -valued discrete hidden
state, Y = {yn,t}N ,T
∑
∑
n=1,t=1 , and the sequences of hidden states are generated by a ﬁrst-order Markov
i=1,j=1 : aij = p(yn,t = j |yn,t−1 = i), ∀n,t and
process whose state transition matrix is a = {aij }K,K
the initial state probability is π = {πi } : πi = p(yn,1 = i), ∀n , where
K
K
i=1 πi = 1,
j=1 aij = 1,
aij ≥ 0, ∀i,j . Hidden states yt are represented by a binary variable yn,t
such that if the hidden state
k
at the tth window of the nth trial is k , then yn,t
k = 1; otherwise 0. At state k , the spike count is
|λk ), whose speciﬁc form is given in the following.
assumed to be generated according to p(xn,t
c
Next, we introduce the correlated Poisson distribution. For brevity, we have omitted the superscript,
n, t, for the moment. As an example, let us ﬁrst consider cases of the trivariate Poisson model
(C = 3) with second- and third-order correlations. We will introduce an auxiliary hidden variable,
sl , l ∈ Φ ≡ {1, 2, 3, 12, 13, 23, 123} which satisﬁes
x1 =s1 + s12 + s13 + s123 ,
x2 =s2 + s12 + s23 + s123 ,
x3 =s3 + s13 + s23 + s123 .
Each sl obeys P (sl |λl ), where P (x|λ) denotes a univariate Poisson distribution: P (x|λ) = λx
−λ .
x! e
Due to the reproducing properties of the Poisson distribution, each xi also marginally follows a
Poisson distribution with parameter λi + λij + λik + λijk , i, j, k ∈ {1, 2, 3}, i ̸= j ̸= k . The mean
vector of this distribution is (λ1 + λ12 + λ13 + λ123 , λ2 + λ12 + λ23 + λ123 , λ3 + λ13 + λ23 + λ123 )T

2

.

B1 =

)

(
(T denotes the transposition) and its variance-covariance matrix is given by
λ13 + λ123
λ12 + λ123
λ1 + λ12 + λ13 + λ123
λ23 + λ123
λ2 + λ12 + λ23 + λ123
λ12 + λ123
λ13 + λ123
λ23 + λ123
λ3 + λ13 + λ23 + λ123
The general deﬁnition of the multivariate Poisson distribution is given using the vector, S =
(s1 , s2 , ..., sL )T , and C × L matrix B = [B1 , B2 , ..., BJ ], C ≤ L with 0 and 1 elements, where
Bj , j = 1, ..., J is a sub-matrix of dimensions C ×C Cj , where C Cj is the number of combinations
of choosing j from C elements. Vector x = (x1 , x2 , ..., xC )T de ﬁned as x = BS follows a mul-
tivariate Poisson distribution. In the above trivariate example, S = (s1 , s2 , s3 , s12 , s13 , s23 , s123 )T
0@ 1
0@ 1
0@ 1
1A , B2 =
1A , B3 =
1A .
and B = [B1 , B2 , B3 ], where
0
0
1
0
0
1
1
1
0
0
1
0
0
1
0
1
1
1
We can also consider only the second-order correlation model by setting B = [B1 , B2 ] and S =
∑
∏
(s1 , s2 , s3 , s12 , s13 , s23 )T , or only the third-order correlation model by setting B = [B1 , B3 ] and
S = (s1 , s2 , s3 , s123 )T . The probability mass function of x is given by
P (sl |λk,l ),
p(x|λk ) =
l∈Φ
S∈G(x)
where G(x) denotes the set of S such that x = BS . The calculation of this probability can be
computationally expensive, since summations over possible S might be exhaustive, especially when
there is a large number of spikes per window. However, the computational burden can be alleviated
by using recurrence relations for a multivariate Poisson distribution [13]. For further details on
computation, see the Appendix. We call the HMM with this output distribution the Correlated
C∏
Poisson HMM (CP-HMM). When we assume that the spike counts for all neurons are independent,
(i.e., B = B1 , S = (s1 , s2 , s3 )T ), the output distribution is reduced to
p(x|λk ) =
P (xc |λk,c ).
c=1
[
N∑
K∑
T∑
K∑
K∑
We call the HMM with this distribution the independent Poisson HMM (IP-HMM). IP-HMM is a
special case of CP-HMM. The complete log-likelihood for CP-HMM is
}]
{
log p(X, Y , S |θ) =
yn,t−1
∏
T∑
K∑
log πk +
log akk′
k
k′=1
t=2
n=1
k=1
k=1
|λk,l )
P (sn,t
yn,t
+
1Sn,t [G(X n,t )]
log
(4)
,
l
k
l∈Φ
t=1
k=1
where θ = (π , a, λ) and 1A [x] is an indicator function, which equals 1 if A ∈ x and 0 otherwise.

yn,1
k

yn,t
k′

(3)

(1)

(2)

2.2 Variational Bayes

|{u(π)
k

Here, we derive VB for CP-HMMs. We use conjugate prior distributions for all parameters of CP-
HMMs, which enabled the posterior distribution to have the same form as the prior distribution. The
K∏
prior distribution for initial probability distribution π and state transition matrix a is the Dirichlet
distribution:
D({aik }K
}K
φ(π) = D({πk }K
|{u(A)
∏
k=1 ), φ(a) =
k=1
k=1
k
k=1 ) = Γ(PK
i=1
QK
where D(·) is deﬁned as D({ak }K
|{uk }K
k=1 auk−1
k=1 uk )
k
. The conjugate prior for
k=1
k
k=1,l=1 , of each auxiliary hidden variable, {sl }l∈Φ ,
the parameter of the Poisson mean, λ = {λk,l }K
k=1 Γ(uk )
∏
K∏
is
l∈Φ
k=1

G (λk,l |κ0 , ξ0 ),

}K
k=1 ).

φ(λ) =

(5)

(6)

3

Q(Z ) =

where G (·) denotes the Gamma distribution de ﬁned as G (λ|κ, ξ ) = ξκ
Γ(κ) λκ−1 e
−λξ . In the exper-
j = 0.1, ∀j , and
iments we discuss in the following, we set the hyperparameters as u(π)
j = u(A)
κ0 = 0.1, ξ0 = 0.1.
The Bayesian method calculates p(θ, Z |X, M ), which is a posterior of unknown parameters and
hidden variable set Z = (Y , S ) given the data and model structure, M (in our case, this indicates
the number of hidden states, and correlation structure). However, the calculation of the posterior
involves a difﬁcult integral. The VB approach approximates the true posterior, p(θ, Z |X, M ), by
⟨
⟩
factored test distribution r(θ)Q(Z ). To make the test distribution closer to the true posterior, we
need to minimize Kullback-Leibler (KL) divergence from r(θ)Q(Z ) to p(θ, Z |X, M ):
KL(r(θ)Q(Z )||p(θ, Z |X, M )) ≡
r(θ)Q(Z )
p(Z, θ |X, M )
log
r(θ)Q(Z )
= log p(X |M ) − ⟨log p(X, Z, θ |M )⟩r(θ)Q(Z ) − Hr (θ) − HQ (Z ),
(7)
where ⟨·⟩p(x) denotes the expectation over p(x) and Hp (x) = ⟨− log p(x)⟩p(x) is the entropy of the
distribution, p(x). Since the log marginal likelihood log p(X |M ) is independent of r(θ) and Q(Z ),
minimizing KL divergence is equivalent to minimizing variational free energy
F ≡ −⟨log p(X, Z, θ |M )⟩r(θ)Q(Z ) − Hr (θ) − HQ (Z ).
(8)
VB alternatively minimizes F with respect to Q(Z ) and r(θ). This minimization with respect to
Q(Z ) is called the VB-E step, and the VB-M step for r(θ).
VB-E step By using the Lagrange multiplier method, the VB-E step is derived as
exp⟨log p(X, Z |θ)⟩r(θ) ,
1
CQ
∑
where CQ is a normalization constant. More speciﬁcally, the following quantities are calculated:
k = 1|X n,1:t ) ˜p(X n,t+1:T |yn,t
˜p(yn,t
⟨yn,t
⟩Q(Z ) =
k = 1)
i = 1|X n,1:t ) ˜p(X n,t+1:T |yn,t
∑
∑
k
K
i=1 ˜p(yn,t
i = 1)
= 1|X n,1:t−1 )˜akk′ ˜p(X n,t |λ
k ) ˜p(X n,t+1:T |yn,t
˜p(yn,t−1
′
k′ ⟩Q(Z ) =
k′ = 1)
yn,t
k
= 1|X n,1:t−1 )˜aij ˜p(X n,t |λj ) ˜p(X n,t+1:T |yn,t
j=1 ˜p(yn,t−1
K
K
j = 1)
i
i=1
∏
∑
These quantities are obtained by the forward-backward algorithm [11]. The subnormarized quantity
˜aij is deﬁned as ˜aij = exp(⟨log aij ⟩r(a) ) and ˜p(X n,t |λk ) is
˜P (sn,t
˜p(X n,t |λk ) =
k,l
l∈Φ
∈G(X n,t )
Sn,t
{
k
where ˜P (sl |λk,l ) is a sub-normalized distribution:
˜P (sl |λk,l ) = exp
sl log ˜λk,l − log(sl !) − ¯λk,l
}
{⟨log λk,l ⟩r(λk )
, ¯λk,l = ⟨λk,l ⟩r(λk ) .
˜λk,l = exp
∑
∏
These quantities can be calculated by using the recurrence relations of the multivariate Poisson
distribution (See the Appendix). The calculation of the posterior for S is given as:
∑
∏
˜P (sn,t
|λk,l )
∈G(X n,t ) sn,t
l∈Φ
⟩Q(Z )
⟩Q(Z ) = ⟨yn,t
⟨sn,t
Sn,t
k,l
k,l
˜P (sn,t
|λk,l )
k
k
k,l
l∈Φ
∈G(X n,t )
Sn,t
k,l
k
This is also calculated by using the recurrence relations of the multivariate Poisson distribution.
VB-M step By again using the Lagrange multiplier method, the VB-M step is derived as
φ(θ) exp⟨log p(X, Z |θ)⟩Q(Z ) ,
1
Cr

|λk,l ),
}

⟨yn,t−1
k

.

(11)

(9)

(10)

where

,

r(θ) =

4

Figure 1: Typical examples of estimation results for correlated Poisson-HMM with third-order corre-
lation applied to simulated spike train. A: From top, 1) spike train of three neurons, 2) the probability
⟩Q(Z ) , 3) spike count data xt
of state k staying at window t denoted by ⟨y t
i , and 4) posterior mean
k
k,l . B: Posterior mean for Poisson mean λk,l for all states. C: Variational free
for hidden variables st
energy calculated for all models.

G (λk,l |wκ
k,l , wξ
k ),

where Cr is a normalization constant. More speciﬁcally, r(θ) = r(π)r(a)r(λ), and
KY
r(π) = D({πk }K
ik }K
k=1 |{wa
D({aik }K
k }K
k=1 |{wπ
k=1 ), r(a) =
k=1 ),
Y
KY
i=1
l∈Φ
k=1
TX
NX
NX
TX
⟨yn,t
k
NX
TX
NX
t=1
n=1
t=1
n=1
⟨yn,1
⟨yn,t−1
j
i
n=1
t=2
n=1
The VB computes the VB-E and VB-M steps alternatively until the variational free energy converges
to a local minimum. In the experiment we discuss in the following, we started the algorithm from
10 different initializations to avoid a poor local minimum solution.

⟩Q(Z ) , wa
ij = u(a)
j +

⟨sn,t
k,l

⟩Q(Z ) , wξ
k = ξ0 +

⟩Q(Z ) .

yn,t
j

⟩Q(Z ) ,

where

r(λ) =

wκ
k,l = κ0 +

j = u(π)
wπ
j +

3 Demonstration on synthetic spike train

By using the synthetic spike train of three neurons, let us ﬁrst demonstrate how to apply our method
to a spike train. In the case of three neurons, we have four choices for the correlation types that have
(1) no correlation term, (2) only a second-order correlation term, (3) only a third-order correlation
term, and (4) both of these. After this, we will call them IP-HMM, 2CP-HMM, 3CP-HMM, and full-
CP-HMM. We generated spike trains by using a multivariate Poisson distribution with only a third-
order correlation whose Poisson mean depends on periods as: (a) λ1 = λ2 = λ3 = 0.5, λ123 = 0.0
for t ∈ [1, 10], (b) λ1 = λ2 = λ3 = 1.5, λ123 = 0.0 for t ∈ [11, 50], (c) λ1 = λ2 = λ3 = 0.5,
λ123 = 1.0 for t ∈ [51, 90], and (d) λ1 = λ2 = λ3 = 0.5, and λ123 = 0.0 for t ∈ [91, 100].
The periods (b) and (c) have the same mean ﬁring rate (the mean spike count in one window is
λi + λ123 = 1.5, i ∈ {1, 2, 3}), but they only differ in the third-order correlation. Therefore,
classical Poisson-HMMs that employ an independent Poisson assumption [1, 2, 7] are not able to
segment them into distinct states. Figure 1A shows that our method was able to do so. We generated

5

010505050505050204060801000501200.511.500.51123454000410042004300440045004600Variational free energyNumber of hidden states A(a)B(b)(c)(d)CIndependent2nd-order3rd-order (true model)Full-ordert (window index)State 3State 1State 2State 3State 3State 2State 1Table 1: Results of model selection for spike
trains from HVC
Stimulus K Correlation Structure
Independent
4
BOS
REV
4
3rd-order
Independent
3
Silent

Table 2: Results of model selection with time
stationary assumption (K = 1)
Correlation Structure
Stimulus
2nd order
BOS
REV
2nd order
Full order
Silent

Figure 2: Typical examples of estimates of VB for spike train from HVC with (A) bird’s own song,
(B) its reversed song, and (C) no stimuli presented. Selected model based on variational free energy
was used for each condition (see Table 1). Each row corresponds to different trials. Background
color indicates most probable state at each time window. Right panels indicate posterior mean ¯λk,l
for all states.

spike trains for 10 trials, but only one trial is shown. The periods (b) and (c) are segmented into
states 1 and 2, whose Poisson means are different (Fig. 1B). The bottom four lines in Fig. 1A plot the
}l∈Φ (Here, we omitted the index of trial n). These plots separately visualize
posterior mean for {st
c , c ∈ {1, 2, 3}.
k,l
the contribution of the independent factor and correlation factor on spike counts xt
The spike counts in period (b) can be viewed as independent ﬁring. Even if the spikes are in the
same window, this can be regarded as just a coincidence predicted by the assumption of independent
ﬁring. In contrast, the spike counts in period (c) can be regarded as having been contributed by
2,i , i ∈ {1, 2, 3}. Here, we used a 3CP-HMM
2,123 , as well as independent factors st
common factor st
having three hidden states. Because periods (a) and (d) have identical statistics, it is clear that the
model with three states (K = 3) is sufﬁcient for modeling this spike train. Then, can we select this
model from the data? Figure 1C shows the variational free energy, F . The 3CP-HMM with three
hidden states yields the lowest F , implying that it is optimal. The 3CP-HMMs with fewer hidden
states, IP-HMMs, or 2CP-HMMs cannot represent the statistical structure of the data, and hence
yield higher F . The 3CP-HMMs with more hidden states (K > 3) or full-CP-HMMs (K ≥ 3) can
include an optimal model, but by being penalized by a Bayesian Occam’s razor, yield higher F .
Thus, we can select the optimal model based on F , at least in this example.

4 Application to spike trains from HVC in songbird

We applied our method to data collected from the nucleus HVC of songbird. HVC is an important
nucleus that integrates auditory information and motor information of song sequences [15]. We
obtained spike trains of three single units by using a silicon probe from one anesthetized Bengalese
ﬁnch. The bird’s own song (BOS) and reversed song (REV) were presented 50 times for each

6

01234502040State 101020State 20510State 3012State 4A: BOSB: REV02040State 101020State 201020State 3012State 4C: Silent01234501234502040State 101020State 200.10.2State 3Time (sec.)Method
Independent & stationary assumption (K = 1)
Stationary assumption (K = 1, correlation type is selected)
Independent assumption (IP-HMM) (K is selected)
CP-HMM (all selected)
full-CP-HMM (K is selected)

Table 3: Log-likelihood on test data (REV).
Log-likelihood (mean ± s.d.)
-255.691 (± 2.074)
-247.640 (± 1.659)
-230.353 (± 0.958)
-229.143 (± 1.242)
-230.272 (± 1.244)

stimulus during recording. Spontaneous activities (Silent) were recorded so that we could obtain the
same amount of data as the stimulus-presented data. More details on the recordings are described
elsewhere [16].

We modeled spike trains for all stimuli using IP-HMMs and CP-HMMs by varying the number of
states K and various correlation structure. We then selected the model that yielded the lowest free
energy. We used window length ∆ = 100 (ms). The selected models are summarized in Table 1.
Figure 2 shows a typical example of spike trains and the segmentation results for the selected models.
The CP-HMMs were only selected for spike trains when REV was presented. If we assume that the
spike statistics did not change over the trials (in our case, this corresponds to the model with only
one hidden state, K = 1), CP-HMMs were selected under all experimental conditions. These results
reﬂect the fact that neurons in anesthetized animals simultaneously transit between high-ﬁring and
low-ﬁring states [17], which can be captured by a Poisson distribution with correlation terms. Time-
stationary assumptions have often been employed to obtain a sufﬁcient sample size for estimating
correlation (e.g., [6]). Our results suggest that we should be careful when interpreting such results;
even when the spike trains seem to have a correlation, if we take state transition into account, spike
trains may be better captured by using an independent Poisson model.

We measured predictive performance on test data to verify how well our model capture the statistical
properties of the spike train. Here, we used spike trains for REV where 3CP-HMM was selected.
We ﬁrst divided spike trains into 20 training and 20 test trials. In the training phase, we constructed
models using the model selection based on the variational free energy with four restrictions: (1)
an independent & stationary assumption (K = 1), (2) a stationary assumption (K = 1, correla-
tion type was selected), (3) IP-HMM (K was selected), (4) CP-HMM (no restrictions), and (5) the
full-CP-HMM (K was selected). In the prediction phase, we calculated the log-likelihood on test
data under the posterior mean ⟨θ⟩r(θ) of selected models. The results are summarized in Table 3.
We took averaged over different choices of training set and prediction sets. We can see that the
log-likelihood on the test data improved by taking both the state transition and correlation structure
into consideration. These results imply that CP-HMMs can characterize the spike train better than
classical Poisson-HMMs. The full-CP-HMM include 2nd-order CP-HMM, but shows lower pre-
dictive performance than the model in which correlation type were selected. This is likely due to
over-ﬁtting to the training data. The VB approach selected the model with tappropriate complexity
avoiding over-ﬁtting.

5 Discussion

We constructed HMMs whose output is a correlated multivariate Poisson distribution for extracting
state-transition dynamics from multiple spike trains. We applied the VB method for inferring the
posterior over the parameter and hidden variables of the models. We have seen that VB can be
used to select an appropriate model (the number of hidden states and correlation structure), which
gives a better prediction. Our method incorporated the correlated Poisson distribution for treating
pairwise and higher-order correlations. There have been approaches that have calculated correlations
by binarizing spike data with log-linear [5] or maximum-entropy models [6]. These approaches are
limited to treating correlations in short bin lengths, which include at most one spike. In contrast,
our approach can incorporate correlations in an arbitrary time window from exact synchronization
to ﬁring-rate correlations on a modest time scale. The major disadvantages of our model are that it
is incapable of negative correlations. It can be incorporated by employing a mixture of multivariate-
Poisson distributions for the output distribution of HMMs. VB can easily be extended to such
models.

7

Appendix: Calculation of correlated Poisson distribution in VB-E step

The sub-normalized distribution (Eq.9) can be calculated by using the recurrence relation of mul-
tivariate Poisson distribution [13]. Let us consider the tri-variate (C = 3) with the second-order
correlation case, where B = [B1 , B2 ]. Here, the recursive scheme for the calculating Eq.9 is:
• If all elements of X = (X1 , X2 , X3 ) are non-zero, then
x1 ˜P (X1 = x1 , X2 = x2 , X3 = x3 |λ) =˜λ1 ˜P (X1 = x1 − 1, X2 = x2 , X3 = x3 |λ)
+ ˜λ12 ˜P (X1 = x1 − 1, X2 = x2 − 1, X3 = x3 |λ)
+ ˜λ13 ˜P (X1 = x1 − 1, X2 = x2 , X3 = x3 − 1|λ).
 3∏
−
∑
• If at most one element of X is non-zero, then
˜P (X1 = x1 , X2 = x2 , X3 = x3 |λ) = exp
i<j
i=1
• If only one of the xi ’s (say, xk ) is zero, then
˜P (X1 = x1 , X2 = x2 , X3 = x3 |λ) = exp{−˜λik − ˜λjk } ˜P (Xi = xi , Xj = xj |λi , λj , λij ).
∑
This recursive scheme can be generalized to more than three dimensions. We use the alternative
k
de ﬁnition of the multivariate Poisson random vector, x such that x =
l=1 ϕl sl , where the vectors,
∗ = (˜λ1 ˜P (X = x − ϕ1 |λ), ..., ˜λk ˜P (X =
ϕl , denote a lth column of matrix B . Let us de ﬁne vector λ
x − ϕk |λ))T . Then, the recurrence relations are rewritten as
x ˜P (X = x|λ) = Bλ
∗
.
By using the quantities obtained in this calculation, ⟨sn,t
⟩Q(Z ) is calculated as
k,l
˜λk,l ˜P (X n,t − ϕl |λk )
⟩Q(Z )
⟩Q(S ) = ⟨yn,t
⟨sn,t
˜P (X n,t |λk )
k,l
k

˜P (Xi = xi |λi ), i, j ∈ 1, 2, 3.

˜λij

(12)

(13)

.

References
[1] M. Abeles, H. Bergman, I. Gat, I. Meilijson, E. Seidemann, N. Thishby, and E. Vaadia, Proc
Nat Acad Sci USA, 92:8616-8620, 1995.
[2] I. Gat, N. Tishby, and M. Abeles, Network: Computation in Neural Systems, 8:297-22, 1997.
[3] L. M. Jones, A. Fontanini, B. F. Sadacca, P. Miller, and D. B. Katz, Proc Nat Acad Sci USA
104:18772-18777, 2007.
[4] E. Vaadia, I. Haalman, M. Abeles. H. Bergman, Y. Prut, H. Slovin, and A. Aertsen, Nature,
373:515-518, 1995.
[5] H. Nakahara, and S. Amari, Neural Computation 14:2269-2316, 2002.
[6] E. Schneidman, M. J. Berry, R. Segev and W. Bialek, Nature 440:1007-1012, 2006.
[7] G. Radons, J.D. Becker, B. D ¨ulfer, and J Kr ¨uger, Biological Cybernetics, 71:359-73, 1994.
[8] M. Danoczy and R. Hahnloser, Advances in NIPS, 18, 2005.
[9] K. Yamazaki and S. Watanabe, Neurocomputing 69:62-84, 2005.
[10] H. Attias, in Proc. of 15th Conference on Uncertainty in Artiﬁcial Intelligence , 21-30, 1999.
[11] M. J. Beal, Variational Algorithms for Approximate Bayesian Inference, Ph.D thesis, Univer-
sity College London, 2003.
[12] S. Watanabe, Y. Minami, A. Nakamura, and N. Ueda, Advances in NIPS, 15, 2002.
[13] K. Kano and K. Kawamura, Communications in Statistics, 20:165-178, 1991.
[14] L. Meligkotsidou, Statistics and Computing, 17:93-107, 2007
[15] A.C. Yu and D. Margoliash, Science, 273:1871-1875, 1996.
[16] J. Nishikawa and K. Okanoya, in preparation.
[17] G. Uchida, M. Fukuda, and M. Tanifuji, Physical Review E, 73:031910, 2006

8


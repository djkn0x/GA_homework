Video Montage

Abhishek Gupta, Shakti Sinha

December 12, 2008

1

Introduction

Unedited video often has signiﬁcant content that is of
low interest to the viewer. One way to present such
videos is by creating a video montage preserving in-
teresting content. Manually doing this is diﬃcult for
a non-expert without specialized video editing soft-
ware. We propose and demonstrate a technique
for creating a video montage automatically from
unedited video with minimal need for user interven-
tion.
Interestingness in a video is inherently sub jective,
therefore to incorporate the user’s notion of interest-
ingness we ask for user input. This constrains us
to using a small amount of data for training. The
scenario that we have in mind is: the user browses
through a few segments of the original video, mark-
ing them as interesting or uninteresting. We use this
labeled data to train a classiﬁer about what is in-
teresting to the user, and then classify the rest of
the video. The video classiﬁed into interesting and
uninteresting segments is then processed to join the
interesting segments and to shorten the uninteresting
segments.

2 Building the feature set

After the user input, we partition the video into shots
(a small portion of the original video) based on scene
changes. The label that the user selected for a seg-
ment is considered for the whole shot. All the labelled
shots are treated as part of the training set and each
unlabelled shot as a test video. For every shot in
the test set, we compute the percentage of its frames
classiﬁed as interesting with the aim of classifying the
whole shot into one category.
We Compute Pyramidal Lucas-Kanade Optical
ﬂow for every pair of adjacent frames. We get k (Op-
tical Flow parameter) 2-D positions and 2-D veloci-
ties as a combined vectror(x,y,Vx,Vy) of the k fastest
moving points with respect to the next frame. We use

these vectors and compute the following features:
1. Optimal Number of Clusters (using a modiﬁed
K-means algorithm).
(cid:113)
k(cid:88)
2. Mean speed of the cluster
(1/k)(
i=1
(cid:113)
3. Maximum speed of the cluster
(cid:113)
4. Minimum speed of the cluster

i + V y2
i )
V x2

max
i=1..k

i + V y2
i )
V x2

min
i=1..k

i + V y2
i )
V x2

Figure 1: Illustration of the division of frames into 25
equal sized rectangles. The points indicate the fastest
moving points in this frame relative to its next frame.
The arrow originating from these points indicate the
velocity vectors of these points.

As shown in Figure 1, we divide the whole video
into 5 × 5 (25) equal rectangular frames and compute
the following features:

1. Mean of the YCbCr values of the points that lie
inside each of these squares. (25 each for Y, Cb
and Cr)

1

2. Number of points that lie inside each of these
squares when we run optical ﬂow algorithm for
every pair of successive frames. (25 features)

3. Sum of speed of all points that lie inside each
such square. (25 features)

4. Histogram of L2-norm of (Vx, Vy) of the ”k”
vectors as computed from the optical ﬂow. (giv-
ing us 21 features)

3 Forward Backward Smooth-
ing

We also want to capture the time co-relation between
the frames in the shots. Consider that our classiﬁer
predicts a shot to contain many interesting frames
together and one or two non-interesting frames in
between. We know that there is some time corre-
lation between frames and therefore such a scenario
is unlikely to occur. Hence we try to smooth the pre-
dictions for every shot. Furthermore, we divide the
frames in the shot into the following categories:

1. BOUNDARY FRAMES: frames at the start of
the shot and the ones at the end of the shot

do

Algorithm 3.1: FBSmoothing(y)
do (cid:8)w[i] = 0
for i ← 1 to N U M F RAM ES

conf idence = w[i] ∗ y [i] ∗ λ[f rametype ]
for iter ← 1 to N U M I T ER
for i ← 1 to N U M F RAM ES
smooth = y [i + 1] ∗ w[i − 1] ∗ µ[f rametype ]
do
conf idence = w[i] ∗ y [i] ∗ λ[f rametype ]
w[i] = arg maxw[i]∈+1,−1 (conf idence + smooth)
for i ← N U M F RAM ES to 1
smooth = y [i − 1] ∗ w[i + 1] ∗ µ[f rametype ]
w[i] = arg maxw[i]∈+1,−1 (conf idence + smooth)
return (w)
where y[i] - the labels computed by running the
shot as the test set and using the parameters learned
by the training set using logistic regression

do

1. For STATIC FRAMES since there is not much
motion in the frame relative to its previous or
next frame, our classiﬁer may not be able to pre-
dict its label correctly. But we are sure of the fact
that if the frame preceding it and the frame suc-
ceeding it are labeled as interesting then proba-
bly this frame is also equally interesting.

2. STATIC FRAMES: frames that don’t change
much w.r.t the neighboring frames

2. For NORMAL FRAMES we give higher weight
to the prediction made by the classiﬁer.

3. NORMAL FRAMES: the rest of the frames in
the shot

For the diﬀerent type of frames we have varying
degree of conﬁdence in the prediction made by the
learning algorithms and coupled with aim of time
correlation between the frames we have devised the
following equations:

3. For BOUNDARY FRAMES, typically there
might be some errors in prediction near the start
of the shot because it takes time to build some
continuity within the shots. Similarly, typically
near the end of the shot there might be a sharp
break in continuity (as the shot is about to ﬁn-
ish). So we don’t trust the prediction made by
the classiﬁer and neither do we trust the conti-
nuity of the adjacent frame.

FRAME TYPE
BOUNDARY FRAMES
STATIC FRAMES
NORMAL FRAMES

λ
0.1
0.1
0.85

µ
0.03
0.9
0.15

Finally, if 75% of the frames in a shot are classiﬁed
as interesting then we mark the whole shot as inter-
esting else we mark it as non-interesting. We apply
Forward-Backward Smoothing on the output of the
classiﬁer for every shot (y vector). Our smoothing
algoithm is as follows:

4 Seams for transitions

We claim that seams (deﬁned below) can be used ef-
fectively for creating smooth and visually appealing
transitions in the videos. We get information about
the interesting and non-interesting video segments us-
ing our classiﬁer as described in the previous sections.
The techniques outlined in this section are then used
to remove the non-interesting portions of the video
and to join the interesting portions together.

2

A seam is a monotonic and connected manifold of
pixels cutting across the time axis of a video repre-
sented as a video cube. Seams are associated with
regions of low energy. Depending on the energy func-
tion, seams can represent regions of low activity and
low importance.

in space and time. We use the energy equation
E (x, y , t) = |I (x + 1, y , t) − I (x, y , t) |
+|I (x, y + 1, t) − I (x, y , t) |
+|I (x, y , t + 1) − I (x, y , t) |

5 Computing the seam

We represent the video cube as a graph where each
voxel is a node. The nodes are connected to their
adjacent nodes using the energy value of the voxel.
With the graph construction shown in Figure 2, a
minimum cut on the graph gives us the optimal seam
in the video [1]. For ﬁnding the minimum graph cut,
we use an implementation of [2]. We resize the videos
to a lower resolution for eﬃciency.

Figure 2: Graph construction for ﬁnding seam. This
ﬁgure shows a longitudinal section of the video cube
along the Y×T plane. Similar construction is present
in the X×T plane. The unlabelled edges have inﬁnite
weights. These edges maintain the seam monotonic-
ity and connectivity [1]. The nodes in the ﬁrst frame
are connected to the source and the nodes in the last
frame are connected to the sink.

The seam computed on a graph using the above en-
ergy equation is the minimum energy manifold in
the video cube. Removing this manifold reduces
the length of the video by one frame. We repeat
this process till we have removed the desired number
of frames. Shot boundaries have very high energies
which prevent seams from cutting across them. We
use median ﬁltering at places where we ﬁnd energy
spikes to allow the seam to cut across shots.

7 Joining videos

Videos should be joined along a seam such that the
transition is smooth and preferably occurs in low im-
portance regions. With this aim, we deﬁne the energy
function as a weighted sum of the individual voxel en-
ergies and the diﬀerence in intensities of voxels in the
two videos. We consider the combined video to be
an overlap of the source videos, such that the output
contains one of the videos before the seam and the
other video after the seam. Consider the video cube
conﬁgurations shown in Figure 3. We start with a
conﬁguration where a small number of frames from
the beginning of the ﬁrst video overlap with frames
from the end of the second video (conﬁguration A),
and compute the seam for the overlapping region.
We then shift the videos one frame at a time and
ﬁnd seams for each position till we reach conﬁgura-
tion C. The seam having the minimum cost among all
the seams found is the optimal transition manifold at
which we join the videos.

6 Shortening videos

We reduce the length of the video by removing low
importance regions from the video cube. To represent
the importance of a point, we use the diﬀerence in
intensity of the point with the points surrounding it

Figure 3: Conﬁgurations for ﬁnding seams. The two
cubes represent the two videos being joined.

If in some conﬁguration, t1 and t2 are the overlap-
ping frames, the energy function we use while con-
structing the graph for computing the seam is given

3

by
E (x, y , t) = α (E1 + E2 ) + (I (x, y , t2 + 1) − I (x, y , t1 ))
where E1 and E2 are the individual voxel energies
in both the videos as deﬁned in the previous section,
I (x, y , tn ) is the intensity of voxel in video n at point
(x, y) in frame t. α is a parameter that controls the
relative importance of the individual voxel energies
and the diﬀerence in energies for ﬁnding the seam. A
small value of α makes the seam join similar regions
of videos, but it might cut through important areas.
A higher value of α causes the seam to go through low
activity areas of the video, but the transition might
be less smooth. We used values of α between 0.1 and
0.01 for our results.

8 Experimental setup and re-
sults

First the user is asked to mark some of the segments
as interesting and some other ones as non-interesting.
We then split the video into shots based on scene
changes. For all the frames that were marked as in-
teresting, we treat the shots containing as interesting.
We do the same for non-interesting parts. With this
we have a collection of shots labeled as interesting and
another collection of shots labeled as non-interesting.
We use these shots as training sets. The rest of the
shots are treated as the test set. We then compute
the above described features for every shot. We use
logistic regression to learn the parameters. For each
shot in the testing set, we use the learned parame-
ters, run logistic regression and get a vector which
contains a label (interesting or non-interesting) for
every frame in the test video. We have heuristically
decided that if greater than or equal to 75% of the
frames within a shot are predicted as interesting then
we consider the shot to be interesting else the shot is
marked as non-interesting.
We present here the results from one of the three
experiements that we carried out. The experiemnt
was performed on 2718 frames (1348 Interesting and
1370 Non-interesting frames ) as training set i.e. ap-
proximately 90 seconds of video. The test set com-
prised of 18924 frames ( around 4629 interesting
frames and about 14295 non-interesting frames) i.e.
approximately 630 seconds of video. This experiment
was done using tennis videos. Similar experiment was
also performed and similar results were obtained for
basketball and snowboarding videos of approximately
the same duration.

Figure 6: PR curves for logistic regression and Linear
SVM

Classiﬁer
Logistic Regression
Linear SVM

Accuracy
70.68
78.96

Class
Interesting
Non-Interesting

Precision Recall F-Measure
0.711
0.622
0.831
0.612
0.824
0.702

From the results we see that SVM performs slightly
better than logistic regression for our data set.
In
general, we want our classiﬁer to identify interesting
segments as much as possible so that user’s notion
of interestingness is preserved. We can see from the
results that the precision for the interesting class is
quite high i.e. around 83.1%. On the downside the
precision for the non-interesting class is low, equalling
only 61.2%. This means we are performing poorly
at identifying the non-interesting segments but are
performing reasonably well in identifying interesting
segments. This also means that we are able to shorten
only 61.2% of the actual non-interesting segments.
We tried the seam techniques on a number of
videos. We observed that joining and shorten-
ing videos preserved areas with high variation con-
tent and motion.
Since it is easier to evaluate
splicing of
images compared to videos, we also
tried the techniques on images, and found that
images were joined with minimal artifacts.
To
ensure that joining videos worked, we took two
overlapping segments of the same video, and the

4

Figure 4: Shortening videos: Frames in the original video (ﬁrst two images) and the shortened video (third
image) show that static frame regions have been replaced by ob jects from other frames. (Right): One of the
manifolds that were removed demonstrates how it cuts through the static regions of the video.

Figure 5: Joining videos: Some frames from the transition of two videos. We can observe that the videos
have been joined at a position of similar content, and the snowboarders from the two videos have been
matched. Figure also shows the seam along which the videos are joined.

output video was a perfect join as in the orig-
inal video.
The results are available online at
http://www.stanford.edu/˜shakti/VideoMontage/.

gorithms for Energy Minimization in Computer
Vision. InIEEE Transactions on Pattern Analy-
sis and Machine Intelligence, September 2004.

9 Acknowledgements

We thank Ashutosh Saxena and the Stanford AI Lab
for guidance and resources for this pro ject.

10 References

1. Rubinstein, M., Shamir, A., and Avidan, S.
2008. Improved seam carving for video retarget-
ing. In ACM Trans. Graph. 27, 3(Aug. 2008),
1-9.

2. Boykov, Y., Kolmogorov, V., 2004. An Exper-
imental Comparison of Min-Cut/Max-Flow Al-

5


Learning and Predicting Flow Duration and Rate

Kok-Kiong Yap, Kwong-Aik Goh and Myunghwan Kim
a ﬁnal report for CS229, Fall 2008-09

December 12, 2008

1

Introduction

OpenFlow is an exciting Stanford technology, which enabled ﬂow-based networking with centralized control.
The architecture is intended to open up opportunities for networking. Here, we explore an interesting
possibility which is to provide network monitoring and dynamic traﬃc engineering in network. However,
these applications require the central controller to be aware of the rate and duration of each ﬂow. While this
information can be readily polled, it is often diﬃcult to do so for each and every ﬂow, given the number of
ﬂows in a network.
In an OpenFlow network, each ﬂow can be identiﬁed with the source, destination, entry switch, transport
type and etcetera. This often communicates a fair amount of information on the nature of the ﬂow. For
example, TCP traﬃc on port 22 is likely to be secure shell connections. Exploiting this, this pro ject attempts
to investigate the feasibility of learning and predicting ﬂow rates based on the above mentioned metrics in a
network. This allows for adaptation of the prediction in accordance of the diﬀerent networks. For example, an
enterprise network is likely to have diﬀerent traﬃc combined to Google’s data center or Stanford’s network.
This pro ject investigates the accuracy of various learning algorithms in prediction of ﬂow rate and duration
in a network. To compare diﬀerent algorithms, exact data are logged from a network and used in quick
assessments of the algorithms’ suitability to the problem. Network data are recorded.

2 Problem Setup

Supervised learning algorithms is applied to problem above. This is because the outputs, i.e., ﬂow rate and
duration of the training sets, are known values, which are in the space of positive real numbers.

2.1 Data acquisition
To evaluate accuracy and eﬃciency of the various learning algorithms, we log over 69,000 ﬂows in an Open-
Flow network, for a single user. The inputs associated with each ﬂow are

1.

Ingress port and switch data-path identity

2. VLAN identity and type

3. Ethernet source and destination address

4.

IP source and destination address

5. Transport source and destination address

With these, we seek to predict the ﬂow rate and duration of each ﬂow. In this pro ject, we use hold-out
cross-validation to assess the accuracy of each learning algorithm. Here, we use 49,585 ﬂows as the training
set while using the rest of the 20,837 ﬂows to estimate the generalization error.

1

Output Types
Durations(sec)
Rates(bps)

... Class 1
1 ∼ 10
...
1 ∼ 10
...

Class 2
10 ∼ 100
10 ∼ 100

...
Class 6
Class 5
Class 4
Class 3
100 ∼ 1K 1K ∼ 10K 10K ∼ 100K 100K ∼ 1M ...
100 ∼ 1K 1K ∼ 10K 10K ∼ 100K 100K ∼ 1M ...

Table 1: Output Classes

2.2 Ouput classiﬁcation
We discretize the continuous output into a few classes rather than exact values. This allows us to use more
advanced algorithms such as GDA and Naive Bayes. We intend to discretize ﬂow rates and durations into
the following classes respectively. Note that we choose classes to discretize along a logarithm scale due to
the large range of values and the heavy-tailed distribution of ﬂow rate and duration.

3 Learning Algorithm

We implement and test the following learning algorithms to obtain our goals. In this section, we describe what
is the mathematical problem of each algorithm and how it is applied. We predict each output independently,
thus only for the deﬁnition, we state the problem using one output feature, y ∈ Rm where m = 49, 585 is the
number of training samples. Then, we obtain the cross-validation error by examining mC V = 20, 837 hold-
out samples.

3.1 Linear regression(LR)

1
2

minθ

m(cid:88)
j=1
where x(j ) ∈ R(n+1) is each training sample whose components indicate input features and an intercept
term(n = 10). Every input feature is deﬁnitely a discrete class label, neither continuous nor meaningful in
a vector space. However, these features can be managed as coded values, therefore we can regard models
diﬀerently according to coding scheme. We do not gurantee that this coding scheme is the best one, but at
least we know this provides a lower bound of the prediction. Then, we classify outputs based on the values,
θT x(i) for i = 1, 2, ..., mC V .

(y (j ) − θT x(j ) )2

3.2 Generalized Linear Model(GLM)
For most of network ﬂows, we can imagine that their durations are short and rates are low. It could be a great
guess that given network features, durations and ﬂow rates are exponentially distributed, y |x; θ ∼ Exp(λ).
Under this assumption, we can try to ﬁt the generalized linear model, since Exp(λ) belongs to exponential
family where η = −λ, b(y) = 1, T (y) = y , a(η) = −log(−η). We solve the following problem,
m(cid:88)
j=1

(y (j ) − hθ (x(j ) ))2

minθ

1
2

where hθ (x) = − 1
θT x and input features are deﬁned as same as section 3.1. Similar concept about the input
features can be applied here. Then, we classify outputs based on the values, hθ (x(i) ) for i = 1, 2, ..., mC V .

2

3.3 Gaussian Discriminant Analysis(GDA)
Another thought is that there exists a centroid for each class and farther points from the centroid is less likely
to belong to the given class. One of possible assumption is that network ﬂows in the same class are normally
distributed. After estimating of Σ (common covariance), µk (a centroid for each class), φk (probability for
each class) according to maximum likelyhood, we select a class that maximizes p (y = k |x). This class is
equivalent to

exp(− 1
(x − µk )T Σ−1 (x − µk )) φk
arg max
2
k
where input features, x, are exactly same as section 3.1, therefore the concept is still same.

3.4 Simple Naive Bayes(SNB)
We can make a rough assumption that every input feature is conditionally independent given output, y .
Thus, by estimating each probability p (xi |y) for i = 1, 2, ..., n, we need to solve
n(cid:89)
p (xi |y)
i=1
Unlike above cases, Naive Bayes works indepently of the coding scheme, since this algorithm does not use
input features’ actual value. Rather, it counts every event and estimate the probability of each feature given
outputs.

arg max
k

p(y)

3.5 Complex Naive Bayes(CNB)
Acutually, we cannot guarantee conditional independence described in section 3.4.
It is easy to show a
counterexample; for instance, when xi and xj are respectively a source IP address and a source transport
port, p(xj |y , xi ) is diﬀerent since diﬀerent server provides diﬀerent services. Or, it is possible that xi does
not oﬀer any service because xi is a client. Therefore, at least, coupling IP address and transport port for
n(cid:89)
both source and destination seems necessary. The class number to choose is
p (xi |y)
p(y) p(x1 , x2 |y) p(x3 , x4 |y)
i=5
where x1 , x2 , x3 , and x4 respectively indicate source IP address, source transport port, destination IP
address, and destnation transport port.

arg max
k

3.6 Support Vector Machine(SVM)
The basic idea of this algorithm is similar to section 3.3, however its detail process is totally diﬀerent.
We do one-versus-all SVM which partitions the problem to 5 independent problems and picks up the class
corresponding to the largest function value f (x) = wT φ(x)+b. Since this algorithm is rooted on the geometric
distance, it is more important to deﬁne the distance between two ﬂows than in the other algorithms. However,
the diﬀerence of class labels mentioned in section 3.1 has no meaning. For example, we cannot say that
a1 =10.10.10.255 is closer to a2 =10.10.11.0 than a3 =10.10.11.10, even though dist(a1 , a2 ) < dist(a2 , a3 ).
A kernel method gives a great possibility to transform the input features, then we solve the following problem
(cid:80)m
(cid:80)m
for each binary classiﬁcation,
i=1 αi − 1
(cid:80)m
i,j=1 y (i) y (j )αiαj K (x(i) , x(j ) )
maxα
0 ≤ αi ≤ C
2
s.t.
(cid:80)n
i=1 αi y (i) = 0
where K (x(i) , x(j ) ) = exp(− 1
p=1 1{x(i)
p (cid:54)= x(j )
p }). The computational complexity is too large for the entire
2
training set, we apply the randomized algorithm in [LR06].

3

4 Result

Using Matlab, we make a prediction and evaluate the error rate for each algorithm. The table shows their
error rates; on the other hand, Figures display the error distributions. The error rate of the best ﬁt is below
1% for duration and approximately 25% for ﬂow rate in terms of CV error.

Figure 1: LR(left), GLM(right) / Duration(upper), Flow rate(lower)

Figure 2: SNB(left), CNB(right) / Duration(upper), Flow rate(lower)

Algorithm Training error(duration) CV error(duration) Training error(ﬂow rate) CV error(ﬂow rate)
LR
0.0087
0.0077
0.7694
0.7553
0.4084
0.4410
0.0077
0.0087
GLM
0.3576
0.2938
0.0077
0.0086
SNB
0.3043
0.2688
0.0077
0.0085
CNB
0.2136
0.3456
0.0313
0.0268
GDA
SVM
0.0090
0.0079
0.4018
0.4266

4

Figure 3: GDA(left), SVM(right) / Duration(upper), Flow rate(lower)

5 Conclusion and Future Works

As shown in the previous section, the accuracy of duration-prediction is very good, while that of rate-
prediction is not. Unfortunately, in our data set, duration does not vary much compared to ﬂow-rate. When
the duration becomes to change a lot, we cuold not be sure that its error rate still remains low.
To evaluate how our models work well, we need to know the lower bound of error rate, Bayes rate.
However, to estimate the Bayes rate, statistically many samples with the same network features are required.
Due to the issue, with leaving it as the future work, we see the results in the practical view.
For the ﬂow rate, over 25% error is not good enough for the prediction to be useful. Assuming that it
might be possible to improve the accuracy, we consider reasons why they do not work well. One possible
reason is that there exist nonlinear factors in the features. Because we usually apply linear algorithms, it
may not reﬂect any high nonlinear factors. Another reason which we can come up with is that the number
of features is not enough. For instance, if the ﬂow rate of TCP stream depend on TCP state rather than IP
address or transport port, then our feature selection is wrong.

References

[LR06] Yumao Lu and Vwani Roychowdhury. Parallel randomized support vector machine. In PAKDD,
2006.

5


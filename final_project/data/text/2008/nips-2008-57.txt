Nonparametric Bayesian Learning of Switching
Linear Dynamical Systems

Emily B. Fox
Electrical Engineering & Computer Science, Massachusetts Institute of Technology
ebfox@mit.edu

Erik B. Sudderth† , Michael I. Jordan†‡
†Electrical Engineering & Computer Science and ‡Statistics, University of California, Berkeley
{sudderth, jordan}@eecs.berkeley.edu

Alan S. Willsky
Electrical Engineering & Computer Science, Massachusetts Institute of Technology
willsky@mit.edu

Abstract
Many nonlinear dynamical phenomena can be effectively modeled by a system
that switches among a set of conditionally linear dynamical modes. We con-
sider two such models: the switching linear dynamical system (SLDS) and the
switching vector autoregressive (VAR) process. Our nonparametric Bayesian ap-
proach utilizes a hierarchical Dirichlet process prior to learn an unknown number
of persistent, smooth dynamical modes. We develop a sampling algorithm that
combines a truncated approximation to the Dirichlet process with efﬁcient joint
sampling of the mode and state sequences. The utility and ﬂex ibility of our model
are demonstrated on synthetic data, sequences of dancing honey bees, and the
IBOVESPA stock index.

1 Introduction

Linear dynamical systems (LDSs) are useful in describing dynamical phenomena as diverse as hu-
man motion [9], ﬁnancial time-series [4], maneuvering targ ets [6, 10], and the dance of honey bees
[8]. However, such phenomena often exhibit structural changes over time and the LDS models
which describe them must also change. For example, a coasting ballistic missile makes an evasive
maneuver; a country experiences a recession, a central bank intervention, or some national or global
event; a honey bee changes from a waggle to a turn right dance. Some of these changes will ap-
pear frequently, while others are only rarely observed. In addition, there is always the possibility
of a new, previously unseen dynamical behavior. These considerations motivate us to develop a
nonparametric Bayesian approach for learning switching LDS (SLDS) models. We also consider
a special case of the SLDS—the switching vector autoregress
ive (VAR) process—in which direct
observations of the underlying dynamical process are assumed available. Although a special case of
the general linear systems framework, autoregressive models have simplifying properties that often
make them a practical choice in applications.

One can view switching dynamical processes as an extension of hidden Markov models (HMMs)
in which each HMM state, or mode, is associated with a dynamical process. Existing methods for
learning SLDSs and switching VAR processes rely on either ﬁx ing the number of HMM modes,
such as in [8], or considering a change-point detection formulation where each inferred change is
to a new, previously unseen dynamical mode, such as in [14]. In this paper we show how one can
remain agnostic about the number of dynamical modes while still allowing for returns to previously
exhibited dynamical behaviors.

Hierarchical Dirichlet processes (HDP) can be used as a prior on the parameters of HMMs with
In this paper we make use of a variant of the HDP-
unknown mode space cardinality [2, 12].
HMM —the
sticky HDP-HMM of [5] —that provides improved control over the number of mod
es
inferred by the HDP-HMM; such control is crucial for the problems we examine. Although the
HDP-HMM and its sticky extension are very ﬂexible time serie s models, they do make a strong
Markovian assumption that observations are conditionally independent given the HMM mode. This
assumption is often insufﬁcient for capturing the temporal dependencies of the observations in real
data. Our nonparametric Bayesian approach for learning switching dynamical processes extends the
sticky HDP-HMM formulation to learn an unknown number of persistent, smooth dynamical modes
and thereby capture a wider range of temporal dependencies.
2 Background: Switching Linear Dynamic Systems
A state space (SS) model provides a general framework for analyzing many dynamical phenomena.
The model consists of an underlying state, xt ∈ Rn , with linear dynamics observed via y t ∈ Rd . A
linear time-invariant SS model, in which the dynamics do not depend on time, is given by

y t =

xt =

(2)

(3)

xt−1 +

et

y t = [I

0

. . .

0] xt .

Aiy t−i + et

et ∼ N (0, Σ).

(1)
y t = C xt + wt ,
xt = Axt−1 + et
where et and wt are independent Gaussian noise processes with covariances Σ and R, respectively.
An order r VAR process, denoted by VAR(r), with observations y t ∈ Rd , can be de ﬁned as
r
Xi=1
Here, the observations depend linearly on the previous r observation vectors. Every VAR(r) process
can be described in SS form by, for example, the following transformation:
. . . Ar
A1 A2
I




. . .
0
I
0
0




...
...
...
...
. . .
I
0
0
. . .
0
Note that there are many such equivalent minimal SS representations that result in the same input-
output relationship, where minimality implies that there does not exist a realization with lower state
dimension. On the other hand, not every SS model may be expressed as a VAR(r) process for ﬁnite
r [1]. We can thus conclude that considering a class of SS models with state dimension r · d and
arbitrary dynamic matrix A subsumes the class of VAR(r) processes.
The dynamical phenomena we examine in this paper exhibit behaviors better modeled as switches
between a set of linear dynamical models. Due to uncertainty in the mode of the process, the overall
model is nonlinear. We de ﬁne a switching linear dynamical system (SLDS) by
xt = A(zt )xt−1 + et (zt )
(4)
y t = C xt + wt .
The ﬁrst-order Markov process zt indexes the mode-speciﬁc LDS at time
t, which is driven by
Gaussian noise et (zt ) ∼ N (0, Σ(zt ) ). We similarly de ﬁne a switching VAR(r) process by
r
Xi=1
Note that the underlying state dynamics of the SLDS are equivalent to a switching VAR(1) process.
3 Background: Dirichlet Processes and the Sticky HDP-HMM
A Dirichlet process (DP), denoted by DP(γ , H ), is a distribution on discrete measures
∞
Xk=1
on a parameter space Θ. The weights are generated via a stick-breaking construction [11]:

A(zt )
i y t−i + et (zt )

et (zt ) ∼ N (0, Σ(zt ) ).

G0 =

βk δθk

(5)

(6)

y t =

θk ∼ H

βk = β ′
k

k−1
Yℓ=1

(1 − β ′
ℓ )

β ′
k ∼ Beta(1, γ ).

(7)

(a)
(b)
(c)
(d)
Figure 1: For all graphs, β ∼ GEM(γ ) and θk ∼ H (λ). (a) DP mixture model in which zi ∼ β and
yi ∼ f (y | θzi ). (b) HDP mixture model with πj ∼ DP(α, β ), zj i ∼ πj , and yj i ∼ f (y | θzji ). (c)-(d)
Sticky HDP-HMM prior on switching VAR(2) and SLDS processes with the mode evolving as zt+1 ∼ πzt for
πk ∼ DP(α + κ, (αβ + κδk )/(α + κ)). The dynamical processes are as in Eq. (13).
We denote this distribution by β ∼ GEM(γ ). The DP is commonly used as a prior on the parameters
of a mixture model, resulting in a DP mixture model (see Fig.1(a)). To generate observations, we
choose ¯θi ∼ G0 and yi ∼ F ( ¯θi ). This sampling process is often described via a discrete variable
zi ∼ β indicating which component generates yi ∼ F (θzi ).
The hierarchical Dirichlet process (HDP) [12] extends the DP to cases in which groups of data are
produced by related, yet distinct, generative processes. Taking a hierarchical Bayesian approach, the
HDP draws G0 from a Dirichlet process prior DP(γ , H ), and then draws group speciﬁc distributions
Gj ∼ DP(α, G0 ). Here, the base measure G0 acts as an “average ” distribution ( E [Gj | G0 ] = G0 )
encoding the frequency of each shared, global parameter:
∞
Xt=1
∞
Xk=1
Because G0 is discrete, multiple ˜θj t ∼ G0 may take identical values θk . Eq. (9) aggregates these
probabilities, allowing an observation yj i to be directly associated with the unique global parameters
via an indicator random variable zj i ∼ πj . See Fig. 1(b).
An alternative, non –constructive characterization of sam ples G0 ∼ DP(γ , H ) from a Dirichlet
process states that for every ﬁnite partition {A1 , . . . , AK } of Θ,
(10)
(G0 (A1 ), . . . , G0 (AK )) ∼ Dir(γH (A1 ), . . . , γH (AK )).
Using this expression, it can be shown that the following ﬁni te, hierarchical mixture model converges
in distribution to the HDP as L → ∞ [7, 12]:
πj ∼ Dir(αβ1 , . . . , αβL ).
β ∼ Dir(γ /L, . . . , γ /L)
This weak limit approximation is used by the sampler of Sec. 4.2.

πj ∼ DP(α, β ) .

˜πj ∼ GEM(α)

=

πjk δθk

(8)

(9)

Gj =

˜πj t δ ˜θjt

(11)

The HDP can be used to develop an HMM with a potentially in ﬁnit e mode space [2, 12]. For
this HDP-HMM, each HDP group-speciﬁc distribution, πj , is a mode-speciﬁc transition distribution
and, due to the in ﬁnite mode space, there are in ﬁnitely many g
roups. Let zt denote the mode of the
Markov chain at time t. For discrete Markov processes zt ∼ πzt−1 , so that zt−1 indexes the group
to which yt is assigned. The current HMM mode zt then indexes the parameter θzt used to generate
observation yt . See Fig. 1(c), ignoring the direct correlation in the observations.
By sampling πj ∼ DP(α, β ), the HDP prior encourages modes to have similar transition distri-
butions (E [πjk | β ] = βk ). However, it does not differentiate self –transitions fro m moves be-
tween modes. When modeling dynamical processes with mode persistence, the ﬂexible nature of
the HDP-HMM prior allows for mode sequences with unrealistically fast dynamics to have large
posterior probability. Recently, it has been shown [5] that one may mitigate this problem by instead
considering a sticky HDP-HMM where πj is distributed as follows:
πj ∼ DP (cid:18)α + κ,
α + κ (cid:19) .
αβ + κδj

(12)

Here, (αβ + κδj ) indicates that an amount κ > 0 is added to the j th component of αβ . The measure
of πj over a ﬁnite partition (Z1 , . . . , ZK ) of the positive integers Z+ , as described by Eq. (10), adds
an amount κ only to the arbitrarily small partition containing j , corresponding to a self-transition.
When κ = 0 the original HDP-HMM is recovered. We place a vague prior on κ and learn the
self-transition bias from the data.
4 The HDP-SLDS and HDP-AR-HMM Models
For greater modeling ﬂexibility, we take a nonparametric ap proach in de ﬁning the mode space of
our switching dynamical processes. Speciﬁcally, we develo p extensions of the sticky HDP-HMM
for both the SLDS and switching VAR models. For the SLDS, we consider conditionally-dependent
emissions of which only noisy observations are available (see Fig. 1(d)). For this model, which we
refer to as the HDP-SLDS, we place a prior on the parameters of the SLDS and infer their posterior
from the data. We do, however, ﬁx the measurement matrix, C , for reasons of identiﬁability. Let
˜C ∈ Rd×n , n ≥ d, be the measurement matrix associated with a dynamical system de ﬁned by
˜A,
and assume ˜C has full row rank. Then, without loss of generality, we may consider C = [I 0] since
there exists an invertible transformation T such that the pair C = ˜C T = [I 0] and A = T −1 ˜AT
de ﬁnes an equivalent input-output system. The dimensional ity of I is determined by that of the data.
Our choice of the number of columns of zeros is, in essence, a choice of model order.

The previous work of Fox et al. [6] considered a related, yet simpler formulation for modeling a
maneuvering target as a ﬁxed LDS driven by a switching exogen ous input. Since the number of
maneuver modes was assumed unknown, the exogenous input was taken to be the emissions of a
HDP-HMM. This work can be viewed as an extension of the work by Caron et. al. [3] in which
the exogenous input was an independent noise process generated from a DP mixture model. The
HDP-SLDS is a major departure from these works since the dynamic parameters themselves change
with the mode and are learned from the data, providing a much more expressive model.

The switching VAR(r) process can similarly be posed as an HDP-HMM in which the observations
are modeled as conditionally VAR(r). This model is referred to as the HDP-AR-HMM and is de-
picted in Fig. 1(c). The generative processes for these two models are summarized as follows:
HDP-SLDS
HDP-AR-HMM
Mode dynamics
zt ∼ πzt−1
zt ∼ πzt−1
i=1 A(zt )
Observation dynamics y t = Pr
i y t−i + et (zt ) xt = A(zt )xt−1 + et (zt )
y t = C xt + wt
Here, πj is as de ﬁned in Sec. 3 and the additive noise processes as in Se c. 2.
4.1 Posterior Inference of Dynamic Parameters
In this section we focus on developing a prior to regularize the learning of different dynamical modes
conditioned on a ﬁxed mode assignment z1:T . For the SLDS, we analyze the posterior distribution of
the dynamic parameters given a ﬁxed, known state sequence x1:T . Methods for learning the number
of modes and resampling the sequences x1:T and z1:T are discussed in Sec. 4.2.
Conditioned on the mode sequence, one may partition the observations into K different linear re-
gression problems, where K = |{z1 , . . . , zT }|. That is, for each mode k , we may form a matrix
Y(k) with Nk columns consisting of the observations y t with zt = k . Then,
Y(k) = A(k) ¯Y(k) + E(k) ,

(14)

(13)

where A(k) = [A(k)
. . . A(k)
], ¯Y(k) is a matrix of lagged observations, and E(k) the associated
r
1
noise vectors. Let D(k) = {Y(k) , ¯Y(k) }. The posterior distribution over the VAR(r) parameters
associated with the k th mode decomposes as follows:
p(A(k) , Σ(k) | D(k) ) = p(A(k) | Σ(k) , D(k) )p(Σ(k) | D(k) ).

(15)

We place a conjugate matrix-normal inverse-Wishart prior on the parameters {A(k) , Σ(k)} [13],
providing a reasonable combination of ﬂexibility and analy tical convenience. A matrix A ∈ Rd×m
has a matrix-normal distribution MN (A; M , V , K ) if
|K | d
2
|2πV | m
2

2 tr“(A−M )T V −1 (A−M )K ” ,
e− 1

p(A) =

(16)

where M is the mean matrix and V and K −1 are the covariances along the rows and columns,
respectively. A vectorization of the matrix A results in
p(vec(A)) = N (vec(M ), K −1 ⊗ V ),
where ⊗ denotes the Kronecker product. The resulting posterior is derived as
, Σ−(k) , S(k)
S−(k)
p(A(k) | Σ(k) , D(k) ) = MN (A(k) ; S(k)
¯y ¯y ),
¯y ¯y
y ¯y
with B−(k) denoting (B (k) )−1 for a given matrix B , and
y ¯y = Y(k) ¯Y(k)T
¯y ¯y = ¯Y(k) ¯Y(k)T
+ K S(k)
S(k)

yy = Y(k)Y(k)T
+ M K S(k)

+ M KM T .

(17)

(18)

We place an inverse-Wishart prior IW(S0 , n0 ) on Σ(k) . Then,
(k)
p(Σ(k) | D(k) ) = IW(S
y | ¯y + S0 , Nk + n0 ),

(19)

(20)

S(k)T
S−(k)
yy − S(k)
y | ¯y = S(k)
where S(k)
. When A is simply a vector, the matrix-normal inverse-
y ¯y
¯y ¯y
y ¯y
Wishart prior reduces to the normal inverse-Wishart prior with scale parameter K .
For the HDP-SLDS, we additionally place an IW(R0 , r0 ) prior on the measurement noise covariance
R, which is shared between modes. The posterior distribution is given by
p(R | y1:T , x1:T ) = IW(SR + R0 , T + r0 ),
with SR =PT
t=1 (y t − C xt )(y t − C xt )T . Further details are provided in supplemental Appendix I.
4.2 Gibbs Sampler
For the switching VAR(r) process, our sampler iterates between sampling the mode sequence, z1:T ,
and both the dynamic and sticky HDP-HMM parameters. The sampler for the SLDS is identical to
that of a switching VAR(1) process with the additional step of sampling the state sequence, x1:T ,
and conditioning on the state sequence when resampling dynamic parameters. The resulting Gibbs
sampler is described below and further elaborated upon in supplemental Appendix II.
Sampling Dynamic Parameters Conditioned on a sample of the mode sequence, z1:T , and the ob-
servations, y1:T , or state sequence, x1:T , we can sample the dynamic parameters θ = {A(k) , Σ(k)}
from the posterior density described in Sec. 4.1. For the HDP-SLDS, we additionally sample R.
Sampling z1:T As shown in [5], the mixing rate of the Gibbs sampler for the HDP-HMM can
be dramatically improved by using a truncated approximation to the HDP, such as the weak limit
approximation, and jointly sampling the mode sequence using a variant of the forward-backward
algorithm. Speciﬁcally, we compute backward messages mt+1,t (zt ) ∝ p(y t+1:T |zt , y t−r+1:t , π , θ)
and then recursively sample each zt conditioned on zt−1 from
p(zt | zt−1 , y1:T , π , θ ) ∝ p(zt | πzt−1 )p(y t | y t−r :t−1 , A(zt ) , Σ(zt ) )mt+1,t (zt ),
i=1 A(zt )
where p(y t | y t−r :t−1 , A(zt ) , Σ(zt ) ) = N (Pr
i y t−i , Σ(zt ) ). Joint sampling of the mode se-
quence is especially important when the observations are directly correlated via a dynamical process
since this correlation further slows the mixing rate of the direct assignment sampler of [12]. Note
that the approximation of Eq. (11) retains the HDP’s nonparametric nature by encouraging the use
of fewer than L components while allowing the generation of new components, upper bounded by
L, as new data are observed.
Sampling x1:T (HDP-SLDS only) Conditioned on the mode sequence z1:T and the set of dy-
namic parameters θ , our dynamical process simpliﬁes to a time-varying linear d ynamical sys-
tem. We can then block sample x1:T by ﬁrst running a backward ﬁlter to compute mt+1,t (xt ) ∝
p(y t+1:T |xt , zt+1:T , θ) and then recursively sampling each xt conditioned on xt−1 from
p(xt | xt−1 , y1:T , z1:T , θ ) ∝ p(xt | xt−1 , A(zt ) , Σ(zt ) )p(y t | xt , R)mt+1,t (xt ).
(22)
The messages are given in information form by mt,t−1(xt−1 ) ∝ N −1 (xt−1 ; θt,t−1 , Λt,t−1 ), where
the information parameters are recursively de ﬁned as
θt,t−1 = A(zt )T
Σ−(zt ) (Σ−(zt ) + C T R−1C + Λt+1,t)−1 (C T R−1y t + θt+1,t )
Λt,t−1 = A(zt )T
Σ−(zt )A(zt ) − A(zt )T
Σ−(zt ) (Σ−(zt ) + C T R−1C + Λt+1,t )−1Σ−(zt )A(zt ) .
See supplemental Appendix II for a more numerically stable version of this recursion.

(23)

(21)

14

12

10

8

6

4

2

0

−2

0

16

14

12

10

8

6

4

2

0
0

200

150

100

50

0

500

1000
Time

1500

2000

200

400
600
Time

800

1000

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

200

400

600
Iteration

800

1000

1000

2000
3000
Iteration

4000

5000

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

200

400

600
Iteration

800

1000

1000

2000
3000
Iteration

4000

5000

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

0.5

0.4

0.3

0.2

0.1

0

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

200

400

600
Iteration

800

1000

1000

2000
3000
Iteration

4000

5000

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

e
c
n
a
t
s
i
D
 
g
n
i
m
m
a
H
 
d
e
z
i
l
a
m
r
o
N

200

400

600
Iteration

800

1000

1000

2000
3000
Iteration

4000

5000

0

0

800

0

200

1000

4000

5000

1000

1000

2000
3000
2000
3000
2000
3000
2000
3000
400
600
Iteration
Iteration
Iteration
Iteration
Time
(a)
(e)
(d)
(c)
(b)
Figure 2: (a) Observation sequence (blue, green, red) and associated mode sequence (magenta) for a 5-mode
switching VAR(1) process (top), 3-mode switching AR(2) process (middle), and 3-mode SLDS (bottom). The
associated 10th, 50th, and 90th Hamming distance quantiles over 100 trials are shown for the (b) HDP-VAR(1)-
HMM, (c) HDP-VAR(2)-HMM, (d) HDP-SLDS with C = I (top and bottom) and C = [1 0] (middle), and
(e) sticky HDP-HMM using ﬁrst difference observations.

4000

5000

4000

5000

0

1000

0

1000

4000

5000

5 Results
Synthetic Data In Fig. 2, we compare the performance of the HDP-VAR(1)-HMM, HDP-VAR(2)-
HMM, HDP-SLDS, and a baseline sticky HDP-HMM on three sets of test data (see Fig. 2(a)). The
Hamming distance error is calculated by ﬁrst choosing the op timal mapping of indices maximiz-
ing overlap between the true and estimated mode sequences. For the ﬁrst scenario, the data were
generated from a 5-mode switching VAR(1) process. The three switching linear dynamical models
provide comparable performance since both the HDP-VAR(2)-HMM and HDP-SLDS with C = I
contain the class of HDP-VAR(1)-HMMs. Note that the HDP-SLDS sampler is slower to mix since
the hidden, three-dimensional continuous state is also sampled. In the second scenario, the data were
generated from a 3-mode switching AR(2) process. The HDP-AR(2)-HMM has signiﬁcantly better
performance than the HDP-AR(1)-HMM while the performance of the HDP-SLDS with C = [1 0]
is comparable after burn-in. As shown in Sec. 2, this HDP-SLDS model encompasses the class of
HDP-AR(2)-HMMs. The data in the third scenario were generated from a 3-mode SLDS model
with C = I . Here, we clearly see that neither the HDP-VAR(1)-HMM nor HDP-VAR(2)-HMM is
equivalent to the HDP-SLDS. Together, these results demonstrate both the differences between our
models as well as the models’ ability to learn switching processes with varying numbers of modes.
Finally, note that all of the switching models yielded signiﬁcant improvements relative to the base-
line sticky HDP-HMM, even when the latter was given ﬁrst diff erences of the observations. This
input representation, which is equivalent to an HDP-VAR(1)-HMM with random walk dynamics
(A(k) = I for all k), is more effective than using raw observations for HDP-HMM learning, but still
much less effective than richer models which switch among learned LDS.

IBOVESPA Stock Index We test the HDP-SLDS model on the IBOVESPA stock index (Sao
Paulo Stock Exchange) over the period of 01/03/1997 to 01/16/2001. There are ten key world
events shown in Fig. 3 and cited in [4] as affecting the emerging Brazilian market during this time
period. In [4], a 2-mode Markov switching stochastic volatility (MSSV) model is used to identify
periods of higher volatility in the daily returns. The MSSV assumes that the log-volatilities follow an
AR(1) process with a Markov switching mean. This underlying process is observed via conditionally
independent and normally distributed daily returns. The HDP-SLDS is able to infer very similar
change points to those presented in [4]. Interestingly, the HDP-SLDS consistently identiﬁes three
regimes of volatility versus the assumed 2-mode model. In Fig. 3, the overall performance of the

t
n
i
o
P
 
e
g
n
a
h
C
 
f
o
 
y
t
i
l
i
b
a
b
o
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.8

0.6

0.4

 

1

0.8

0.6

0.4

e
t
a
R
 
n
o
i
t
c
e
t
e
D

t
n
i
o
P
 
e
g
n
a
h
C
 
f
o
 
y
t
i
l
i
b
a
b
o
r
P

0.2

6/1/98

1/13/00

1/3/97 7/2/97

0
1/3/97 7/2/97

HDP−SLDS
HDP−SLDS, non−sticky
HDP−AR(1)−HMM
HDP−AR(2)−HMM
0.4
0.8
0.6
1/15/99
1/15/99
1/15/99
Date
Date
Date
False Alarm Rate
(d)
(c)
(b)
(a)
Figure 3: (a) IBOVESPA stock index daily returns from 01/03/1997 to 01/16/2001. (b) Plot of the estimated
probability of a change point on each day using 3000 Gibbs samples for the HDP-SLDS. The 10 key events are
indicated with red lines. (c) Similar plot for the non-sticky HDP-SLDS with no bias towards self-transitions.
(d) ROC curves for the HDP-SLDS, non-sticky HDP-SLDS, HDP-AR(1)-HMM, and HDP-AR(2)-HMM.

0
1/3/97 7/2/97

1/13/00

1/13/00

0
 
0

0.2

0.2

6/1/98

6/1/98

1

HDP-SLDS is compared to that of the HDP-AR(1)-HMM, HDP-AR(2)-HMM, and HDP-SLDS
with no bias for self-transitions (i.e., κ = 0.) The ROC curves shown in Fig. 3(d) are calculated
by windowing the time axis and taking the maximum probability of a change point in each window.
These probabilities are then used as the con ﬁdence of a chang e point in that window. We clearly
see the advantage of using a SLDS model combined with the sticky HDP-HMM prior on the mode
sequence. Without the sticky extension, the HDP-SLDS over-segments the data and rapidly switches
between redundant states which leads to a dramatically larger number of inferred change points.

Dancing Honey Bees We test the HDP-VAR(1)-HMM on a set of six dancing honey bee se-
quences, aiming to segment the sequences into the three dances displayed in Fig. 4. (Note that we
did not see performance gains by considering the HDP-SLDS, so we omit showing results for that
architecture.) The data consist of measurements y t = [cos(θt )
sin(θt ) xt yt ]T , where (xt , yt )
denotes the 2D coordinates of the bee’s body and θt its head angle. We compare our results to
those of Xuan and Murphy [14], who used a change-point detection technique for inference on this
dataset. As shown in Fig. 4(d)-(e), our model achieves a superior segmentation compared to the
change-point formulation in almost all cases, while also identifying modes which reoccur over time.

Oh et al. [8] also presented an analysis of the honey bee data, using an SLDS with a ﬁxed number of
modes. Unfortunately, that analysis is not directly comparable to ours, because [8] used their SLDS
in a supervised formulation in which the ground truth labels for all but one of the sequences are
employed in the inference of the labels for the remaining held-out sequence, and in which the kernels
used in the MCMC procedure depend on the ground truth labels. (The authors also considered a
“parameterized segmental SLDS (PS-SLDS),” which makes use
of domain knowledge speciﬁc to
honey bee dancing and requires additional supervision during the learning process.) Nonetheless,
in Table 1 we report the performance of these methods as well as the median performance (over
100 trials) of the unsupervised HDP-VAR(1)-HMM to provide a sense of the level of performance
achievable without detailed, manual supervision. As seen in Table 1, the HDP-VAR(1)-HMM yields
very good performance on sequences 4 to 6 in terms of the learned segmentation and number of
modes (see Fig. 4(a)-(c)); the performance approaches that of the supervised method. For sequences
1 to 3 —which are much less regular than sequences 4 to 6 —the pe
rformance of the unsupervised
procedure is substantially worse. This motivated us to also consider a partially supervised variant
of the HDP-VAR(1)-HMM in which we ﬁx the ground truth mode sequences for ﬁve ou
t of six of
the sequences, and jointly infer both a combined set of dynamic parameters and the left-out mode
sequence. As we see in Table 1, this considerably improved performance for these three sequences.

Not depicted in the plots in Fig. 4 is the extreme variation in head angle during the waggle dances
of sequences 1 to 3. This dramatically affects our performance since we do not use domain-speciﬁc
information. Indeed, our learned segmentations consistently identify turn-right and turn-left modes,
but often create a new, sequence-speciﬁc waggle dance mode. Many of our errors can be attributed to
creating multiple waggle dance modes within a sequence. Overall, however, we are able to achieve
reasonably good segmentations without having to manually input domain-speciﬁc knowledge.

6 Discussion

In this paper, we have addressed the problem of learning switching linear dynamical models with
an unknown number of modes for describing complex dynamical phenomena. We presented a non-

(1)

(2)

4

3

2

e
d
o
m
 
d
e
t
a
m
i
t
s
E

3

2

e
d
o
m
 
d
e
t
a
m
i
t
s
E

(3)
4

3

2

e
d
o
m
 
d
e
t
a
m
i
t
s
E

1

0.8

0.6

0.4

e
t
a
R
 
n
o
i
t
c
e
t
e
D

(4)
1

(5)

 

(6)
 

1

0.8

0.6

0.4

e
t
a
R
 
n
o
i
t
c
e
t
e
D

1

1

600

0.2

0

200

0

200

600

800

HDP−VAR−HMM, unsupervised
HDP−VAR−HMM, unsupervised
HDP−VAR−HMM, supervised
HDP−VAR−HMM, supervised
Change−point formulation
Change−point formulation
Viterbi sequence
Viterbi sequence
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
400
400
False Alarm Rate
False Alarm Rate
Time
Time
Time
(a)
(e)
(d)
(c)
(b)
Figure 4: (top) Trajectories of the dancing honey bees for sequences 1 to 6, colored by waggle (red), turn
right (blue), and turn left (green) dances. (a)-(c) Estimated mode sequences representing the median error for
sequences 4, 5, and 6 at the 200th Gibbs iteration, with errors indicated in red. (d)-(e) ROC curves for the
unsupervised HDP-VAR-HMM, partially supervised HDP-VAR-HMM, and change-point formulation of [14]
using the Viterbi sequence for segmenting datasets 1-3 and 4-6, respectively.

0
 
0

0
 
0

0.2

0

200

400

600

1

1

Sequence
1
2
3
4
5
6
HDP-VAR(1)-HMM unsupervised
46.5
44.1
45.6
83.2
93.2
88.7
HDP-VAR(1)-HMM partially supervised
65.9
88.5
79.2
86.9
92.3
89.1
SLDS DD-MCMC
74.0
86.1
81.3
93.4
90.2
90.4
PS-SLDS DD-MCMC
75.9
92.4
83.1
93.4
90.4
91.0
Table 1: Median label accuracy of the HDP-VAR(1)-HMM using unsupervised and partially supervised Gibbs
sampling, compared to accuracy of the supervised PS-SLDS and SLDS procedures, where the latter algorithms
were based on a supervised MCMC procedure (DD-MCMC) [8].

parametric Bayesian approach and demonstrated both the utility and versatility of the developed
HDP-SLDS and HDP-AR-HMM on real applications. Using the same parameter settings, in one
case we are able to learn changes in the volatility of the IBOVESPA stock exchange while in an-
other case we learn segmentations of data into waggle, turn-right, and turn-left honey bee dances.
An interesting direction for future research is learning models of varying order for each mode.

References
[1] M. Aoki and A. Havenner. State space modeling of multiple time series. Econ. Rev., 10(1):1–59, 1991.
[2] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The inﬁnit e hidden Markov model. In NIPS, 2002.
[3] F. Caron, M. Davy, A. Doucet, E. Duﬂos, and P. Vanheeghe. B ayesian inference for dynamic models with
Dirichlet process mixtures. In Int. Conf. Inf. Fusion, July 2006.
[4] C. Carvalho and H. Lopes. Simulation-based sequential analysis of Markov switching stochastic volatility
models. Comp. Stat. & Data Anal., 2006.
[5] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. An HDP-HMM for systems with state
persistence. In ICML, 2008.
[6] E. B. Fox, E. B. Sudderth, and A. S. Willsky. Hierarchical Dirichlet processes for tracking maneuvering
targets. In Int. Conf. Inf. Fusion, July 2007.
[7] H. Ishwaran and M. Zarepour. Exact and approximate sum–r epresentations for the Dirichlet process. Can.
J. Stat., 30:269–283, 2002.
[8] S. Oh, J. Rehg, T. Balch, and F. Dellaert. Learning and inferring motion patterns using parametric seg-
mental switching linear dynamic systems. IJCV, 77(1–3):103–124, 2008.
[9] J. M. Pavlovi ´c, V. Rehg and J. MacCormick. Learning swit ching linear models of human motion. In
NIPS, 2000.
[10] X. Rong Li and V. Jilkov. Survey of maneuvering target tracking. Part V: Multiple-model methods. IEEE
Trans. Aerosp. Electron. Syst., 41(4):1255–1321, 2005.
[11] J. Sethuraman. A constructive deﬁnition of Dirichlet p riors. Stat. Sinica, 4:639–650, 1994.
[12] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. J. Amer. Stat.
Assoc., 101(476):1566–1581, 2006.
[13] M. West and J. Harrison. Bayesian Forecasting and Dynamic Models. Springer, 1997.
[14] X. Xuan and K. Murphy. Modeling changing dependency structure in multivariate time series. In ICML,
2007.


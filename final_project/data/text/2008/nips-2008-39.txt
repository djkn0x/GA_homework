Empirical performance maximization
for linear rank statistics

St ´ephan Cl ´emenc¸ on
Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141
stephan.clemencon@telecom-paristech.fr

Nicolas Vayatis
ENS Cachan & UniverSud - CMLA UMR CNRS 8536
vayatis@cmla.ens-cachan.fr

Abstract

The ROC curve is known to be the golden standard for measuring performance of
a test/scoring statistic regarding its capacity of discrimination between two popu-
lations in a wide variety of applications, ranging from anomaly detection in signal
processing to information retrieval, through medical diagnosis. Most practical
performance measures used in scoring applications such as the AUC, the local
AUC, the p-norm push, the DCG and others, can be seen as summaries of the
ROC curve. This paper highlights the fact that many of these empirical criteria
can be expressed as (conditional) linear rank statistics. We investigate the proper-
ties of empirical maximizers of such performance criteria and provide preliminary
results for the concentration properties of a novel class of random variables that
we will call a linear rank process.

1

Introduction

In the context of ranking, several performance measures may be considered. Even in the simplest
framework of bipartite ranking, where a binary label is available, there is not one and only natural cri-
terion, but many possible options. The ROC curve provides a complete description of performance
but its functional nature renders direct optimization strategies rather complex. Empirical risk mini-
mization strategies are thus based on summaries of the ROC curve, which take the form of empirical
risk functionals where the averages involved are no longer taken over i.i.d. sequences. The most
popular choice is the so-called AUC criterion (see [AGH+05] or [CLV08] for instance), but when
top-ranked instances are more important, various choices can be considered: the Discounted Cumu-
lative Gain or DCG [CZ06], the p-norm push (see [Rud06]), or the local AUC (refer to [CV07]).
The present paper starts from the simple observation that all these summary criteria have a common
feature: conditioned upon the labels, they all belong to the class of linear rank statistics. Such statis-
tics have been extensively studied in the mathematical statistics literature because of their optimality
properties in hypothesis testing, see [HS67]. Now, in the statistical learning view, with the impor-
tance of excess risk bounds, the theory of rank tests needs to be revisited and new problems come
up. The arguments required to deal with risk functionals based on linear rank statistics have been
sketched in [CV07] in a special case. The empirical AUC, known as the Wilcoxon-Mann-Whitney
statistic, is also a U -statistic and this particular dependence structure was extensively exploited in
[CLV08]. In the present paper, we describe the generic structure of linear rank statistics as an or-
thogonal decomposition after projection onto the space of sums of i.i.d. random variables (Section
2). This projection method is the key to all statistical results related to maximizers of such criteria:
consistency, (fast) rates of convergence or model selection. We relate linear rank statistics to perfor-
mance measures relevant for the ranking problem by showing that the target of ranking algorithms

1

correspond to optimal ordering rules in that sense (Section 3). Eventually, we provide some pre-
liminary results in Section 4 for empirical maximizers of performance criteria based on linear rank
statistics with smooth score-generating functions.

2 Criteria based on linear rank statistics

Along the paper, we shall consider the standard binary classiﬁcation model. Take a random pair
(X, Y ) ∈ X ×{−1, +1}, where X is an observation vector in a high dimensional space X ⊂ Rd and
Y is a binary label, and denote by P the distribution of (X, Y ). The dependence structure between X
and Y can be described by conditional distributions. We can consider two descriptions: either P =
(µ, η) where µ is the marginal distribution of X and η is the posterior distribution deﬁned by η(x) =
P{Y = 1 | X = x} for all x ∈ Rd , or else P = (p, G, H ) with p = P{Y = 1} being the proportion
of positive instances, G = L(X | Y = +1) the conditional distribution of positive instances and
H = L(X | Y = −1) the conditional distribution of negative instances. A sample of size n of i.i.d.
realizations of this statistical model can be represented as a set of pairs {(Xi , Yi )}1≤i≤n , where
(Xi , Yi ) is a copy of (X, Y ), but also as a set {X +
m , } where L(X +
k , X −
1 , . . . , X −
i ) = G,
1 , . . . , X +
L(X −
i ) = H , and k + m = n. In this setup, the integers k and m are random, drawn as binomials
of size n and respective parameters p and 1 − p.

2.1 Motivation

Most of the statistical learning theory has been developed for empirical risk minimizers (ERM) of
sums of i.i.d. random variables. Mathematical results were elaborated with the use of empirical
processes techniques and particularly concentration inequalities for such processes (see [BBL05]
for an overview). This was made possible by the standard assumption that, in a batch setup, for
the usual prediction problems (classiﬁcation, regression or density estimation), the sample data
{(Xi , Yi )}i=1,...,n are i.i.d. random variables. Another reason is that the error probability in these
problems involves only ”ﬁrst-order” events, depending only on (X1 , Y1 ). In classiﬁcation, for in-
stance, most theoretical developments were focused on the error probability P{Y1 (cid:54)= g(X1 )} of a
classiﬁer g : X → {−1, +1}, which is hardly considered in practice because the two populations are
rarely symmetric in terms of proportions or costs. For prediction tasks such as ranking or scoring,
more involved statistics need to be considered, such as the Area Under the ROC curve (AUC), the
local AUC, the Discounted Cumulative Gain (DCG), the p-norm push, etc. For instance, the AUC,
a very popular performance measure in various scoring applications, such as medical diagnosis or
credit-risk screening, can be seen as a probability of an ”event of order two”, i.e. depending on
(X1 , Y1 ), (X2 , Y2 ). In information retrieval, the DCG is the reference measure and it seems to have
a rather complicated statistical structure. The ﬁrst theoretical studies either attempt to get back to
sums of i.i.d. random variables by artiﬁcially reducing the information available (see [AGH+05],
[Rud06]) or adopt a plug-in strategy ([CZ06]). Our approach is to i) avoid plug-in in order to under-
stand the intimate nature of the learning problem, ii) keep all the information available and provide
the analysis of the full statistic. We shall see that this approach requires the development of new
tools for handling the concentration properties of rank processes, namely collections of rank statis-
tics indexed by classes of functions, which have never been studied before.

2.2 Empirical performance of scoring rules

The learning task on which we focus here is known as the bipartite ranking problem. The goal of
ranking is to order the instances Xi by means of a real-valued scoring function s : X → R , given the
binary labels Yi . We denote by S the set of all scoring functions. It is natural to assume that a good
Rank(s(Xi )) = (cid:80)n
scoring rule s would assign higher ranks to the positive instances (those for which Yi = +1) than to
the negative ones. The rank of the observation Xi induced by the scoring function s is expressed as
I{s(Xj )≤s(Xi )} and ranges from 1 to n. In the present paper, we consider a
j=1
particular class of simple (conditional) linear rank statistics inspired from the Wilcoxon statistic.

2

Deﬁnition. 1 Let φ : [0, 1] → [0, 1] be a nondecreasing function. We deﬁne the ”empirical W-
(cid:18) Rank(s(Xi ))
(cid:19)
n(cid:88)
ranking performance measure” as the empirical risk functional
(cid:99)Wn (s) =
, ∀s ∈ S .
I{Yi=+1}φ
n + 1
The function φ is called the ”score-generating function” of the ”rank process” {(cid:99)Wn (s)}s∈S .
i=1
We refer to the book by Serﬂing [Ser80] for properties and asymptotic theory of rank statistics.
We point out that our deﬁnition does not match exactly with the standard deﬁnition of linear rank
the variables Yi . We will call statistics (cid:99)Wn (s) conditional linear rank statistics.
statistics. Indeed, in our case, coefﬁcients of the ranks in the sum are random because they involve
It is a very natural idea to consider ranking criteria based on ranks. Observe indeed that the perfor-
mance of a given scoring function s is invariant by increasing transforms of the latter, when evaluated
through the empirical W -ranking performance measure. For speciﬁc choices of the score-generating
function φ, we recover the main examples mentioned in the introduction and many relevant criteria
can be accurately approximated by statistics of this form:
• φ(u) = u - this choice leads to the celebrated Wilcoxon-Mann-Whitney statistic which is
related to the empirical version of the AUC (see [CLV08]).
• φ(u) = u · I{u≥u0 } , for some u0 ∈ (0, 1) - such a score-generating function corresponds
to the local AUC criterion, introduced recently in [CV07]. Such a criterion is of interest
when one wants to focus on the highest ranks.
• φ(u) = up - this is another choice which puts emphasis on high ranks but in a smoother
way than the previous one. This is related to the p-norm push approach taken in [Rud06].
However, we point out that the criterion studied in the latter work relies on a different
deﬁnition of the rank of an observation. Namely, the rank of positive instances among
negative instances (and not in the pooled sample) is used. This choice permits to use
independence which makes the technical part much simpler, at the price of increasing the
variance of the criterion.
• φ(u) = φn (u) = c ((n + 1) u) · I{u≥k/(n+1)} - this corresponds to the DCG criterion in the
bipartite setup, one of the ”gold standard quality measure” in information retrieval, when
grades are binary (namely I{Yi=+1} ). The c(i)’s denote the discount factors, c(i) measur-
ing the importance of rank i. The integer k denotes the number of top-ranked instances to
take into account. Notice that, with our indexation, top positions correspond to the largest
ranks and the sequence {ci } should be chosen to be increasing.

2.3 Uniform approximation of linear rank statistics

This subsection describes the main result of the present analysis, which shall serve as the essential
tool for deriving statistical properties of maximizers of empirical W -ranking performance mea-
sures. For a given scoring function s, we denote by Gs , respectively Hs , the conditional cumulative
distribution function of s(X ) given Y = +1, respectively Y = −1. With these notations, the uncon-
ditional cdf of s(X ) is then Fs = pGs + (1 − p)Hs . For averages of non-i.i.d. random variables, the
underlying statistical structure can be revealed by orthogonal projections onto the space of sums of
i.i.d. random variables in many situations. This projection argument was the key for the study of em-
pirical AUC maximization, which involved U -processes, see [CLV08]. In the case of U -statistics,
this orthogonal decomposition is known as the Hoeffding decomposition and the remainder may be
expressed as a degenerate U -statistic, see [Hoe48]. For rank statistics, a similar though less accurate
decomposition can be considered. We refer to [Haj68] for a systematic use of the projection method
for investigating the asymptotic properties of general statistics.
integrable statistic. The r.v. (cid:98)T = (cid:80)n
Lemma. 2 ([Haj68]) Let Z1 , . . . , Zn be independent r.v.’s and T = T (Z1 , . . . , Zn ) be a square
E[T | Zi ] − (n − 1)E[Z ] is called the H ´ajek projection of
E[ (cid:98)T ] = E[T ] and E[( (cid:98)T − T )2 ] = E[(T − E[T ])2 ] − E[( (cid:98)T − E[ (cid:98)T ])2 ].
i=1
T . It satisﬁes

3

From the perspective of ERM in statistical learning theory, through the projection method, well-
known concentration results for standard empirical processes may carry over to more complex col-
lections of r.v. such as rank processes, as shown by the next approximation result.
on [0, 1]. We set Φs (x) = φ(Fs (s(x))) + p (cid:82) +∞
Proposition. 3 Consider a score-generating function φ which is twice continuously differentiable
s(x) φ(cid:48) (Fs (u))dGs (u) for all x ∈ X . Let S0 ⊂ S be a
VC major class of functions. Then, we have: ∀s ∈ S0 ,
(cid:99)Wn (s) = (cid:98)Vn (s) + (cid:98)Rn (s),
where (cid:98)Vn (s) = (cid:80)n
I{Yi=+1}Φs (Xi ) and (cid:98)Rn (s) = OP (1) as n → ∞ uniformly over s ∈ S .
i=1
The notation OP (1) means bounded in probability and the integrals are represented in the sense of
the Lebesgue-Stieltjes integral. Details of the proof can be found in the Appendix.

Remark 1 (ON THE COM PL EX I TY A S SUM P T ION . ) On the terminology of major sets and major
classes, we refer to [Dud99]. In the Proposition 3’s proof, we need to control the complexity of
subsets of the form {x ∈ X : s(x) ≤ t}. The stipulated complexity assumption garantees that this
collection of sets indexed by (s, t) ∈ S0 × R forms a VC class.
Remark 2 (ON TH E SMOOTHNE S S A S SUM P T ION . ) We point out that it is also possible to deal with
discontinuous score-generating functions as seen in [CV07]. In this case, the lack of smoothness of φ
other approach would consist of approximating (cid:99)Wn (s) by the empirical W-ranking criterion where
has to be compensated by smoothness assumptions on the underlying conditional distributions. An-
the score-generating function ψ would be a smooth approximation of φ. Owing to space limitations,
here we only handle the smooth case.
it as a function of the sampling cdf. Denoting by (cid:98)Fs (x) = n−1 (cid:80)n
An essential hint to the study of the asymptotic behavior of a linear rank statistic consists in rewriting
I{s(Xi )≤x} the empirical
(cid:18) n
i )(cid:1)(cid:19)
i=1
k(cid:88)
(cid:0)s(X +
counterpart of Fs (x), we have:(cid:99)Wn (s) =
n + 1 (cid:98)Fs
i=1
which may easily shown to converge to E[φ(Fs (s(X )) | Y = +1] as n → ∞, see [CS58].
Deﬁnition. 4 For a given score-generating function φ, we will call the functional
Wφ (s) = E[φ(Fs (s(X )) | Y = +1] ,
a ”W-ranking performance measure”.

φ

.

The following result is a consequence of Proposition 3 and its proof can be found in the Appendix.
Proposition. 5 Let S0 ⊂ S be a VC major class of functions with VC dimension V and φ be a
score-generating function of class C 1 . Then, as n → ∞, we have with probability one:
|(cid:99)Wn (s) − kWφ (s)| → 0.
1
n

sup
s∈S0

3 Optimality
We introduce the class S ∗ of scoring functions obtained as strictly increasing transformations of the
regression function η :
S ∗ = { s∗ = T ◦ η | T : [0, 1] → R strictly increasing }.
The class S ∗ contains the optimal scoring rules for the bipartite ranking problem. The next para-
graphs motivate the use of W -ranking performance measures as optimization criteria for this prob-
lem.

4

3.1 ROC curves

A classical tool for measuring the performance of a scoring rule s is the so-called ROC curve
ROC(s, .) : α ∈ [0, 1] (cid:55)→ 1 − Gs ◦ H −1
s (1 − α),
s (x) = inf {t ∈ R | Hs (t) ≥ x}.
where H −1
In the case where s = η , we will denote
ROC(η , α) = ROC∗ (α), for any α ∈ [0, 1]. The set of points (α, β ) ∈ [0, 1]2 which can be
achieved as (α, ROC(s, α)) for some scoring function s is called the ROC space.
It is a well-known fact that the regression function provides an optimal scoring function for the ROC
curve. This fact relies on a simple application of Neyman-Pearson’s lemma. We refer to [CLV08]
for the details. Using the fact that, for a given scoring function, the ROC curve is invariant by
increasing transformations of the scoring function, we get the following result:
Lemma. 6 For any scoring function s and any α ∈ [0, 1], we have:
∀s∗ ∈ S ∗ , ROC(s, α) ≤ ROC(s∗ , α) .= ROC∗ (α) .

The next result states that the set of optimal scoring functions coincides with the set of maximizers
of the Wφ -ranking performance, provided that the score-generating function φ is strictly increasing.

Proposition. 7 Assume that the score-generating function φ is strictly increasing. Then, we have:
∀s ∈ S , Wφ (s) ≤ Wφ (η) .
.= Wφ (η) = Wφ (s∗ ) for any s∗ ∈ S ∗ .

Moreover W ∗
φ

Remark 3 (ON PLUG - IN RANK ING RU LE S) Theoretically, a possible approach to ranking is the
plug-in method ([DGL96]), which consists of using an estimate ˆη of the regression function as a
scoring function. As shown by the subsequent bound, when φ is differentiable with a bounded
derivative, when ˆη is close to η in the L1 -sense, it leads to a nearly optimal ordering in terms of
φ − Wφ ((cid:98)η) ≤ (1 − p)||φ(cid:48) ||∞E[|(cid:98)η(X ) − η(X )|].
W-ranking criterion:
W ∗
However, one faces difﬁculties with the plug-in approach when dealing with high-dimensional data,
see [GKKW02]), which provides the motivation for exploring algorithms based on W-ranking per-
formance maximization.

3.2 Connection to hypothesis testing

From the angle embraced in this paper, the ranking problem is tightly related to hypothesis testing.
Denote by X + and X − two r.v. distributed as G and H respectively. As a ﬁrst go, we can reformu-
late the ranking problem as the one of ﬁnding a scoring function s such that s(X − ) is stochastically
smaller than s(X + ), which means, for example, that: ∀t ∈ R, P{s(X − ) ≥ t} ≤ P{s(X + ) ≥ t}.
It is easy to see that the latter statement means that the ROC curve of s dominates the ﬁrst diagonal
of the ROC space. We point out the fact that the ﬁrst diagonal corresponds to nondiscriminating
scoring functions s0 such that Hs0 = Gs0 . However, searching for a scoring function s fulﬁlling
this property is generally not sufﬁcient in practice. Heuristically, one would like to pick an s in order
to be as far as possible from the case where ”Gs = Hs ”. This requires to specify a certain measure
of dissimilarity between distributions. In this respect, various criteria may be considered such as the
L1 -Mallows metric (see the next remark). Indeed, assuming temporarily that s is ﬁxed and consid-
ering the problem of testing similarity vs. dissimilarity between two distributions Hs and Gs based
k ) and s(X −
1 ), . . . , s(X −
on two independent samples s(X +
1 ), . . . , s(X +
m ), it is well-known that
nonparametric tests based on linear rank statistics have optimality properties. We refer to Chapter
9 in [Ser80] for an overview of rank procedures for testing homogeneity, which may yield relevant
criteria in the ranking context.
criterion: AUC(s) = (cid:82) 1
Remark 4 (CONNECT ION BE TW EEN AUC AND TH E L1 -MA LLOW S M ETR IC ) Consider the AUC
α=0 ROC(s, α)dα. It is well-known that this criterion may be interpreted as

5

the ”rate of concording pairs”: AUC(s) = P{s(X ) < s(X (cid:48) ) | Y = −1, Y (cid:48) = +1} where (X, Y )
(cid:90) ∞
and (X (cid:48) , Y (cid:48) ) denote independent copies. Furthermore, it may be easily shown that
1
{Hs (t) − Gs (t)}dF (t),
2
−∞
where the cdf F may be taken as any linear convex combination of Hs and Gs . Provided that Hs is
stochastically smaller than Gs and that F (dt) is the uniform distribution over (0, 1) (this is always
possible, even if it means replacing s by F ◦ s, which leaves the ordering untouched), the second
term may be identiﬁed as the L1 -Mallows distance between Hs and Gs , a well-known probability
metric widely considered in the statistical literature (also known as the L1 -Wasserstein metric).

AUC(s) =

+

4 A generalization error bound

We now provide a bound on the generalization ability of scoring rules based on empirical maximiza-
tion of W-ranking performance criteria.
Theorem. 8 Set the empirical W -ranking performance maximizer ˆsn = arg maxs∈S (cid:99)Wn (s). Un-
der the same assumptions as in Proposition 3 and assuming in addition that the class of functions
(cid:114) log(1/δ)
(cid:114)
Φs induced by S0 is also a VC major class of functions, we have, for any δ > 0, and with probability
1 − δ :
n

φ − Wφ (ˆsn ) ≤ c1
W ∗
for some positive constants c1 , c2 .

+ c2

V
n

,

The proof is a straightforward consequence from Proposition 3 and it can be found in the Appendix.

5 Conclusion

In this paper, we considered a general class of performance measures for ranking/scoring which can
be described as conditional linear rank statistics. Our overall setup encompasses in particular known
criteria used in medical diagnosis and information retrieval. We have described the statistical nature
√
of such statistics, proved that they ar compatible with optimal scoring functions in the bipartite setup,
and provided a preliminary generalization bound with a
n-rate of convergence. By doing so, we
provided the very results on a class of linear rank processes. Further work is needed to identify a
variance control assumption in order to derive fast rates of convergence and to obtain consistency
under weaker complexity assumptions. Moreover, it is not clear how to formulate convex surrogates
for such functionals yet.

Appendix - Proofs

Proof of Proposition 5
(cid:32)
(cid:33)
By virtue of the ﬁnite increment theorem, we have:
| (cid:98)Fs (t) − Fs (t)|
|(cid:99)Wn (s) − kWφ (s)| ≤ k ||φ(cid:48) ||∞
and the desired result immediately follows from the application of the VC inequality, see Remark 1.

+ sup
(s,t)∈S0×R

1
n + 1

sup
s∈S0

Proof of Proposition 3
Since φ is of class C 2 , a Taylor expansion at the second order immediately yields:
k(cid:88)
i ))) + (cid:98)Bn (s) + (cid:98)Rn (s),
(cid:99)Wn (s) =
φ(Fs (s(X +
i=1

6

with

(I ) =

E

,

φ(cid:48) (Fs (s(Xi ))),

− Fs (s(X +
i ))

(cid:19)
(cid:18) Rank(s(X +
k(cid:88)
(cid:98)Bn (s) =
i ))
φ(cid:48) (Fs (s(X +
(cid:19)2 ||φ(cid:48)(cid:48) ||∞ .
(cid:18) Rank(s(X +
i )))
| (cid:98)Rn (s)| ≤ k(cid:88)
n + 1
i=1
i ))
− Fs (s(X +
i ))
n + 1
Following in the footsteps of [Haj68], we ﬁrst compute the projection of (cid:98)Bn (s) onto the space Σ of
r.v.’s of the form (cid:80)
i=1
(cid:21)
(cid:19)
(cid:20)(cid:18) Rank(s(X +
i (Xi , Yi )] < ∞ for all i ∈ {1, . . . , n}:
i≤n fi (Xi , Yi ) such that E[f 2
k(cid:88)
n(cid:88)
PΣ ( (cid:98)Bn (s)) =
i ))
− Fs (s(X +
φ(cid:48) (Fs (s(Xi ))) | Xj , Yj
E
i ))
n + 1
(cid:19)
(cid:18) 1
i=1
j=1
n(cid:88)
This projection may be splitted into two terms:
E[Rank(s(Xi )) | s(Xi )] − Fs (s(Xi ))
I{Yi=+1}
(cid:21)
(cid:19)
(cid:20)(cid:18) Rank(s(X +
(cid:88)
n(cid:88)
n + 1
i=1
i ))
φ(cid:48) (Fs (s(Xi ))) | s(Xj ), Yj
− Fs (s(X +
I{Yi=+1}
E
(I I ) =
i ))
.
n + 1
j (cid:54)=i
i=1
have E[Rank(s(Xi )) | s(Xi )] = n (cid:98)Fs (s(Xi )) and, by assumption, sup(s,t)∈S×R | (cid:98)Fs (t) − Fs (t)| =
The ﬁrst term is easily handled and may be seen as negligible (it is of order OP (n−1/2 )), since we
OP (n−1/2 ) (see Remark 1). Up to an additive term of order OP (1) uniformly over s ∈ S , the second
n(cid:88)
(cid:88)
E (cid:2)I{s(Xj )≤s(Xi )}φ(cid:48) (Fs (s(Xi ))) | s(Xj ), Yj
(cid:3)
term may be rewritten as
1
I{Yi=+1}
(I I ) =
(cid:90) ∞
(cid:90) ∞
n(cid:88)
n(cid:88)
n + 1
j (cid:54)=i
i=1
φ(cid:48) (Fs (u))dGs (u) − 1
φ(cid:48) (Fs (u))dGs (u).
k
I{Yi=+1}
I{Yi=+1} (cid:82) ∞
As (cid:80)n
=
n + 1
n + 1
s(Xj )
s(Xi )
j=1
i=1
s(Xi ) φ(cid:48) (Fs (u))dGs (u)/(n + 1) ≤ supu∈[0,1] φ(cid:48) (t) and k/(n + 1) ∼ p, we get
i=1
that, uniformly over s ∈ S0 :
k(cid:88)
i ))) + PΣ ( (cid:98)Bn (s))) = (cid:98)Vn (s) + OP (1) as n → ∞.
φ(Fs (s(X +
The term (cid:98)Rn (s) is negligible, since, up to the multiplicative constant ||φ(cid:48)(cid:48) ||∞ , it is bounded by
i=1

2 .
2Fs (s(Xi )) + (cid:88)
n(cid:88)
1
{I{s(Xk )≤s(Xi )} − Fs (s(Xi ))}
E
(n + 1)2
k (cid:54)=i
i=1
 = (cid:88)

2
(cid:88)
As Fs is bounded by 1, it sufﬁces to observe that for all i:
E (cid:2)(I{s(Xk )≤s(Xi )} − Fs (s(Xi ))})2 | s(Xi )(cid:3) .
| s(Xi )
{I{s(Xk )≤s(Xi )} − Fs (s(Xi ))}
k (cid:54)=i
k (cid:54)=i
Bounding the variance of the binomial r.v. E[I{s(Xk ) ≤ s(Xi )} − Fs (s(Xi ))})2 | s(Xi )] by 1/4,
one ﬁnally gets that ˆRn (s) is of order OP (1) uniformly over s ∈ S0 .
Eventually, one needs to evaluate the accuracy of the approximation yield by the projection (cid:98)Bn (s) −
{PΣ ( (cid:98)Bn (s)) − (n − 1)E[ (cid:98)Bn (s)]}. Write, for all s ∈ S0 ,
(cid:18) 1
(cid:19)
n(cid:88)
(cid:98)Bn (s) = n (cid:98)Un (s) +
− Fs (s(Xi ))
I{Yi=+1}
n + 1
i=1

φ(cid:48) (Fs (s(Xi ))),

7

where Un (s) = (cid:80)
i(cid:54)=j qs ((Xi , Yi ), (Xj , Yj ))/(n(n + 1)) is a U -statistic with kernel:
qs ((x, y), (x(cid:48) , y (cid:48) )) = I{y=+1} · I{s(x(cid:48) )≤s(x)} · φ(cid:48) (Fs (s(x))).
n−1 (cid:16) (cid:98)Bn (s) − {PΣ ( (cid:98)Bn (s)) − (n − 1)E[ (cid:98)Bn (s)]}(cid:17)
= (cid:98)Un (s) − {PΣ ( (cid:98)Un (s)) − (n − 1)E[ (cid:98)Un (s)]}
Hence, we have
(cid:98)Un (s). Now, given that sups∈S0 ||qs ||∞ < ∞, it follows from Theorem 11 in [CLV08] for instance,
which actually corresponds to the degenerate part of the Hoeffding decomposition of the U -statistic
| (cid:98)Un (s) − {PΣ ( (cid:98)Un (s)) − (n − 1)E[ (cid:98)Un (s)]}| = OP (n−1 ) as n → ∞,
combined with the basic symmetrization device of the kernel qs , that
sup
s∈S0
which concludes the proof.

pWφ (s) =

Proof of Proposition 7
(cid:90) 1
Using the decomposition Fs = pGs + (1 − p)Hs , we are led to the following expression:
φ(u) du − (1 − p)E[φ(Fs (s(X ))) | Y = −1].
0
(cid:90) 1
Then, using a change of variable:
E[φ(Fs (s(X ))) | Y = −1] =
0
It is now easy to conclude since φ is increasing (by assumption) and because of the optimality of
elements of S ∗ in the sense of Lemma 6.

φ(p(1 − ROC(s, α)) + (1 − p)(1 − α)) dα .

Proof of Theorem 8

|(cid:99)Wn (s)/k − Wφ (s)| ≤ 2
| (cid:98)Vn (s) − kWφ (s)| + OP (n−1 ),
Observe that, by virtue of Proposition 3,
φ − Wφ (ˆsn ) ≤ 2 sup
W ∗
sup
s∈S0
s∈S0
k
and the desired bound derives from the VC inequality applied to the sup term, noticing that it follows
from our assumptions that {(x, y) (cid:55)→ I{y=+1}Φs (x)}s∈S0 is a VC class of functions.

[CLV08]

[BBL05]

References
[AGH+05] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds
for the area under the ROC curve. Journal of Machine Learning Research, 6:393–425,
2005.
S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classiﬁcation: A Survey of
Some Recent Advances. ESAIM: Probability and Statistics, 9:323–375, 2005.
S. Cl ´emenc¸ on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of
U-statistics. The Annals of Statistics, 36(2):844–874, 2008.
J. Chernoff and Savage. Asymptotic normality and efﬁciency of certain non parametric
test statistics. Ann. Math. Stat., 29:972–994, 1958.
S. Cl ´emenc¸ on and N. Vayatis. Ranking the best instances. Journal of Machine Learn-
ing Research, 8:2671–2699, 2007.
D. Cossock and T. Zhang. Subset ranking using regression. In H.U. Simon and G. Lu-
gosi, editors, Proceedings of COLT 2006, volume 4005 of Lecture Notes in Computer
Science, pages 605–619, 2006.
L. Devroye, L. Gy ¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition.
Springer, 1996.

[DGL96]

[CS58]

[CV07]

[CZ06]

8

[Haj68]

[Hoe48]

R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.
[Dud99]
[GKKW02] L. Gy ¨orﬁ, M. K ¨ohler, A. Krzyzak, and H. Walk. A Distribution-Free Theory of Non-
parametric Regression. Springer, 2002.
J. Hajek. Asymptotic normality of simple linear rank statistics under alternatives. Ann.
Math. Stat., 39:325–346, 1968.
W. Hoeffding. A class of statistics with asymptotically normal distribution. Ann. Math.
Stat., 19:293–325, 1948.
J. H ´ajek and Z. Sid ´ak. Theory of Rank Tests. Academic Press, 1967.
C. Rudin. Ranking with a P-Norm Push.
In H.U. Simon and G. Lugosi, editors,
Proceedings of COLT 2006, volume 4005 of Lecture Notes in Computer Science, pages
589–604, 2006.
R.J. Serﬂing. Approximation theorems of mathematical statistics. John Wiley & Sons,
1980.

[HS67]
[Rud06]

[Ser80]

9


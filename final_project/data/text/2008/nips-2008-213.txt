Hierarchical Semi-Markov Conditional Random
Fields for Recursive Sequential Data

Tran The Truyen † , Dinh Q. Phung † , Hung H. Bui ‡ ∗, and Svetha Venkatesh †
†Department of Computing, Curtin University of Technology
GPO Box U1987 Perth, WA 6845, Australia
thetruyen.tran@postgrad.curtin.edu.au
{D.Phung,S.Venkatesh}@curtin.edu.au

‡Arti ﬁcial Intelligence Center, SRI International
333 Ravenswood Ave, Menlo Park, CA 94025, USA
bui@ai.sri.com

Abstract

Inspired by the hierarchical hidden Markov models (HHMM), we present the hier-
archical semi-Markov conditional random ﬁeld (HSCRF), a generalisation of em-
bedded undirected Markov chains to model complex hierarchical, nested Markov
processes. It is parameterised in a discriminative framework and has polynomial
time algorithms for learning and inference. Importantly, we develop efﬁcient al-
gorithms for learning and constrained inference in a partially-supervised setting,
which is important issue in practice where labels can only be obtained sparsely.
We demonstrate the HSCRF in two applications: (i) recognising human activities
of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase
chunking. We show that the HSCRF is capable of learning rich hierarchical mod-
els with reasonable accuracy in both fully and partially observed data cases.

1

Introduction

Modelling hierarchical aspects in complex stochastic processes is an important research issue in
many application domains ranging from computer vision, text information extraction, computa-
tional linguistics to bioinformatics. For example, in a syntactic parsing task known as noun-phrase
chunking, noun-phrases (NPs) and part-of-speech tags (POS) are two layers of semantics associated
with words in the sentence. Previous approach ﬁrst tags the P OS and then feeds these tags as input
to the chunker. The POS tagger takes no information of the NPs. This may not be optimal, as a
noun-phrase is often very informative to infer the POS tags belonging to the phrase. Thus, it is more
desirable to jointly model and infer both the NPs and the POS tags at the same time.

Many graphical models have been proposed to address this challenge, typically extending the ﬂat
hidden Markov models (e.g., hierarchical HMM (HHMM) [2], DBN [6]). These models are, how-
ever, generative in that they are forced to consider the modelling of the joint distribution Pr(x, z ) for
both the observation z and the label x. An attractive alternative is to model the distribution Pr(x|z )
directly, avoiding the modelling of z . This line of research has recently attracted much interest, and
one of the signi ﬁcant results was the introduction of the conditional random ﬁeld (CRF) [4]. Work
in CRFs was originally limited to ﬂat structures for efﬁcien
t inference, and subsequently extended to

∗Hung Bui is supported by the Defense Advanced Research Projects Agency (DARPA) under Contract
No. FA8750-07-D-0185/0004. Any opinions, ﬁndings and conclusion s or recommendations expressed in this
material are those of the authors and do not necessarily reﬂect the view s of DARPA, or the Air Force Research
Laboratory (AFRL).

hierarchical structures, such as the dynamic CRFs (DCRF) [10], and hierarchical CRFs [5]. These
models assume predeﬁned structures, therefore, they are no t ﬂexible to adapt to many real-world
datasets. For example, in the noun-phrase chunking problem, no prior hierarchical structures are
known. Rather, if such a structure exists, it can only be discovered after the model has been suc-
cessfully built and learned.

In addition, most discriminative structured models are trained in a completely supervised fashion
using fully labelled data, and limited research has been devoted to dealing with the partially labelled
data (e.g. [3, 12]). In several domains, it is possible to obtain some labels with minimal effort.
Such information can be used either for training or for decoding. We term the process of learning
with partial labels partial-supervision, and the process of inference with partial labels constrained
inference. Both processes require the construction of appropriate constrained inference algorithms.

We are motivated by the HHMM [2], a directed, generative model parameterised as a standard
Bayesian network. To address the above issues, we propose the Hierarchical Semi-Markov Condi-
tional Random Field (HSCRF), which is a recursive, undirected graphical model that generalises the
undirected Markov chains and allows hierarchical decomposition. The HSCRF is parameterised as
a standard log-linear model, and thus can naturally incorporate discriminative modelling. For exam-
ple, the noun-phrase chunking problem can be modeled as a two level HSCRF, where the top level
represents the NP process, the bottom level the POS process. The two processes are conditioned on
the sequence of words in the sentence. Each NP generally spans one or more words, each of which
has a POS tag. Rich contextual information such as starting and ending of the phrase, the phrase
length, and the distribution of words falling inside the phrase can be effectively encoded. At the
same time, like the HHMM, exact inference in the HSCRFs can be performed in polynomial time in
a manner similar to the Asymmetric Inside-Outside algorithm (AIO) [1].

We demonstrate the effectiveness of HSCRFs in two applications: (i) segmenting and labelling
activities of daily living (ADLs) in an indoor environment and (ii) jointly modelling noun-phrases
and part-of-speeches in shallow parsing. Our experimental results in the ﬁrst application show that
the HSCRFs are capable of learning rich, hierarchical activities with good accuracy and exhibit
better performance when compared to DCRFs and ﬂat-CRFs. Res ults for the partially observable
case also demonstrate that signi ﬁcant reduction of trainin g labels still results in models that perform
reasonably well. We also show that observing a small amount of labels can signi ﬁcantly increase
the accuracy during decoding. In noun-phrase chunking, the HSCRFs can achieve higher accuracy
than standard CRF-based techniques and the recent DCRFs. Our contributions from this paper are
thus:
i) the introduction of the novel and Hierarchical Semi-Markov Conditional Random Field
to model nested Markovian processes in a discriminative framework, ii) the development of an
efﬁcient generalised Asymmetric Inside-Outside (AIO) alg orithm for partially supervised learning
and constrained inference, and iii) the applications of the proposed HSCRFs in human activities
recognition, and in shallow parsing of natural language.

Due to space constraint, in this paper we present only main ideas and empirical evaluations. Com-
plete details and extensions can be found in the technical report [11]. The next section introduces
necessary notations and provides a model description for the HSCRF, followed by the discussion
on learning and inference for fully and partially data cases in section 3 and 4 respectively. Applica-
tions for recognition of activities and natural language parsing are presented in section 5. Finally,
discussions on the implications of the HSCRF and conclusions are given in section 6.

2 Model De ﬁnition and Parameterisation

2.1 The Hierarchical Semi-Markov Conditional Random Fields

Consider a hierarchically nested Markov process with D levels where, by convention, the top level
is the dummy root level that generates all subsequent Markov chains. Then, as in the generative
process of the hierarchical HMMs [2], the parent state embeds a child Markov chain whose states
may in turn contain grand-child Markov chains. The relation among these nested Markov chains is
deﬁned via the model topology, which is a state hierarchy of depth D . It speci ﬁes a set of states S d
at each level d, i.e., S d = {1...|S d |}, where |S d | is the number of states at level d and 1 ≤ d ≤ D .
For each state sd ∈ S d where d 6= D , the model also deﬁnes a set of children associated with
it at the next level ch(sd ) ⊂ S d+1 , and thus conversely, each child sd+1 is associated with a set

of parental states at the upper level pa(sd+1 ) ⊂ S d . Unlike the original HHMMs proposed in [2]
where tree structure is explicitly enforced on the state hierarchy, the HSCRFs allow arbitrary sharing
of children among parental states as addressed in [1]. This way of topology generalization implies
less number of sub-states required when D is large, and thus lead to fewer parameters and possibly
less training data and time complexity [1].

To provide an intuition, the temporal evolution can be informally described as follows. Start with
the root node at the top level, as soon as a new state is created at level d 6= D , it initialises a child
state at level d + 1. The initialisation continues downward until reaching the bottom level1 . This
child process at level d + 1 continues its execution recursively until it terminates, and when it does,
the control of execution returns to its parent at the upper level d. At this point, the parent makes a
decision either to transits to a new state at the same level or returns the control to the grand-parent
at the upper level d − 1.
The key intuition for this hierarchical nesting process is that the lifespan of a child process is a sub-
segment in the lifespan of its parent. To be more precise, consider the case which a parent process
i:j at level d starts a new state2 at time i and persists until time j . At time i, the parent initialises
sd
a child state sd+1
which continues until it ends at time k < j , at which the child state transits to
i
a new child state sd+1
k+1 . The child process exits at time j , at which the control from the child level
is returned to the parent sd
i:j . Upon receiving the control, the parent state sd
i:j may transit to a new
parent state sd
j+1:l , or ends at j and returns the control to the grand-parent at level d − 1.

d = 1

d = 2

x2
2

e2
2

d = D

1

2

T − 1

T

xi−1

xi

xj−1

xj

ei−1 = 1

ei = 0

ej−1 = 0

ej = 1

xd
i

xd
i

xd
i

ed
i−1

= 1

ed
i = 1

xd−1
i+1

e

d−1
i

= 0
xd
i+1

ed
i = 1

xd+1
xd+1
i
i
Figure 1: Graphical presentation for HSCRFs (leftmost). Graph structures for state-persistence
(middle-top), initialisation and ending (middle-bottom), and state-transition (rightmost).
The HSCRF, which is a multi-level temporal graphical model of length T with D levels, can be
described formally as follows (Fig. 1). It starts from the root level indexed as 1, runs for T time
slices and at each time slice a hierarchy of D states are generated. At each level d and time index
i ∈ S d = {1, 2, ..., |S d |}. Associated with each xd
i, there is a node representing a state variable xd
i
i terminates or
i which can be either 1 or 0 to signify whether the state xd
is an ending indicator ed
continues its execution to the next time slice. The nesting nature of the HSCRFs is formally realised
by imposing the speci ﬁc constraints on the value assignment of ending indicators:

T = 1, and all
1:T −1 = 0, e1
• The root state persists during the course of evolution, i.e., e1
states end at the last time-slice, i.e., e1:D
T = 1.
i = 1 implies
• When a state ﬁnishes, all of its descendants must also ﬁnish, i
.e., ed
ed+1:D
= 1; and when a state persists, all of its ancestors must also persist, i.e., ed
i = 0
i
implies e1:d−1
= 0.
i

i = 1, ed−1
i = 0, and states
• When a state transits, its parent must remain unchanged, i.e., ed
at the bottom level terminates at every single slice, i.e., eD
i = 1 for all i ∈ [1, T ].

Thus, speci ﬁc value assignments of ending indicators provi de contexts that realise the evolution of
the model states in both hierarchical (vertical) and temporal (horizontal) directions. Each context at

1 In HHMMs, the bottom level is also called production level, in which the states emit observational symbols.
In HSCRFs, this generative process is not assumed.
2Our notation sd
i:j is to denote the set of variables from time i to j at level d, i.e., sd
j }.
i:j = {sd
i , sd
i+1 , . . . , sd

a level and associated state variables form a contextual clique, and here we identify four contextual
clique types (cf. Fig. 1):

• State-persistence : This corresponds to the life time of a state at a given level Speci ﬁcally,
i−1:j = (1, 0, .., 0, 1)), then σpersist,d
given a context be c = (ed
i:j , c), is a contextual
= (xd
i:j
i:j .
clique that speci ﬁes the life span [i, j ] of any state s = xd
• State-transition : This corresponds to a state at level d ∈ [2, D] at time i transiting to
i = 1) then σ transit,d
c = (ed−1
a new state. Speci ﬁcally, given a context
= 0, ed
=
i
i
(xd−1
i+1 at time i
i to xd
i:i+1 , c) is a contextual clique that speci ﬁes the transition of xd
i+1 , xd
under the same parent xd−1
i+1 .
• State-initialisation : This corresponds to a state at level d ∈ [1, D − 1] initialising a new
i−1 = 1), then
child state at level d + 1 at time i. Speci ﬁcally, given a context
c = (ed
σ init,d
i , xd+1
, c) is a contextual clique that speci ﬁes the initialisation at t
ime i from
= (xd
i
i
i to the ﬁrst child xd+1
.
the parent xd
i
• State-exiting : This corresponds to a state at level d ∈ [1, D−1] to end at time i Speci ﬁcally,
i = 1), then σexit,d
i , xd+1
given a context c = (ed
, c) is a contextual clique that
= (xd
i
i
i at time i with the last child xd+1
speci ﬁes the ending of xd
.
i

In the HSCRF, we are interested in the conditional setting, in which the entire state and ending
variables (x1:D
1:T ) are conditioned on an observational sequence z . For example, in computa-
1:T , e1:D
tional linguistics, the observation is often the sequence of words, and the state variables might be
the part-of-speech tags and the phrases.

To capture the correlation between variables and such conditioning, we deﬁne a non-negative po-
tential function φ(σ, z ) over each contextual clique σ . Figure 2 shows the notations for potentials
that correspond to the four contextual clique types we have identi ﬁed above. Details of potential
speci ﬁcation are described in the Section 2.2.

State persistence potential
State transition potential
State initialization potential
State ending potential

Rd,s,z
i:j = φ(σpersist,d
, z ) where s = xd
i:j .
i:j
Ad,s,z
u,v ,i = φ(σ transit,d
, z ) where s = xd−1
i+1 .
i+1 and u = xd
i , v = xd
i
πd,s,z
u,i = φ(σ init,d
i , u = xd+1
, z ) where s = xd
.
i
i
u,i = φ(σexit,d
E d,s,z
i , u = xd+1
.
, z ) where s = xd
i
i
Figure 2: Shorthands for contextual clique potentials.
1:T ) denote the set of all variables and let τ d = {ik }m
k=1 denote the set of all time
Let V = (x1:D
1:T , e1:D
indices where ed
ik = 1. A conﬁguration ζ of the model is a complete assignment of all the states and
ending indicators (x1:D
1:T ) which satis ﬁes the set of hierarchical constraints describ ed earlier in
1:i , e1:D
ion is the product of all contextual clique
this section. The joint potential deﬁned for each conﬁgurat
potentials over all ending time indexes i ∈ [1, T ] and all semantic levels d ∈ [1, D]:

u,ik (cid:21)
(cid:20) Y(ik ,ik+1 )∈τ d
ik+1:ik+1 (cid:21)(cid:20) Yik ∈τ d ,ik /∈τ d−1
u,ik+1(cid:21)(cid:20) Yik ∈τ d
u,v ,ik (cid:21)(cid:20) Yik ∈τ d
Rd,s,z
Ad,s,z
E d,s,z
πd,s,z
Φ(ζ , z ) = Yd


The conditional distribution is given as:
1
(1)
Φ(ζ , z )
Pr(ζ |z ) =
Z (z )
where Z (z ) = Pζ Φ(ζ , z ) is the partition function for normalisation.
2.2 Log-linear Parameterisation

σ (σ, z ) associated with each type of contextual
In our HSCRF setting, there is a feature vector f d
clique σ , in that φ(σd , z ) = exp (cid:8)θd
σ (σ, z )(cid:9). where a•b denotes the inner product of two vectors
σ • f d
a and b. Thus, the features are active only in the context in which the corresponding contextual
cliques appear. For the state-persistence contextual clique, the features incorporate state-duration,
start time i and end time j of the state. Other feature types incorporate the time index in which the
features are triggered. In what follows, we omit z for clarity, and implicitly use it as part of the
partition function Z and the potential Φ(.).

3 Unconstrained Inference and Fully Supervised Learning

Typical inference tasks in the HSCRF include computing the partition function, MAP assignment
and feature expectations. The key insight is the context-speci ﬁc independence, which is due to
hierarchical constraints described in Section 2.1. Let us call the set of variable assignments Πd,s
i:j =
i:j−1 = 0) the symmetric Markov blanket. Given Πd,s
i:j , the set of
(xd
j = 1, ed
i−1 = 1, ed:D
i:j = s, ed:D
variables inside the blanket is independent of those outside it. A similar relation holds with respect
to the asymmetric Markov blanket, which includes the set of variable assignments Γd,s
i:j (u) = (xd
i:j =
s, xd+1
i−1 = 1, ed+1:D
i:j−1 = 0). Figure 3 depicts an asymmetric Markov blanket
j = u, ed:D
= 1, ed
j
(the covering arrowed line) containing a smaller asymmetric blanket (the left arrowed line) and a
symmetric blanket (the double-arrowed line). Denote by ∆d,s
i:j the sum of products of all clique

level

d

level   +1
d

πd+1,s
u,i

(2)

αd,s
i:j (u)E d,s
u,j ; αd,s
i:j (u) =

i:k−1 (v) ˆ∆d+1,u
v,u,k−1 + ˆ∆d+1,u
αd,s
k:j Ad+1,s
i:j

Figure 3: Decomposition with respect to symmetric/asymmetric Markov blankets.
potentials falling inside the symmetric Markov blanket Πd,s
i:j . The sum is taken over all possible
value assignments of the set of variables inside Πd,s
i:j . In the same manner, let αd,s
i:j (u) be the sum
of products of all clique potentials falling inside the asymmetric Markov blanket Γd,s
i:j (u). Let ˆ∆d,s
i:j
be a shorthand for ∆d,s
i:j Rd,s
i:j . Using the context-speci ﬁc independence described above a nd the
decomposition depicted Figure 3, the following recursions arise:
j
Xk=i+1 Xv∈Sd+1
i:j = Xu∈Sd+1
∆d,s
As the symmetric Markov blanket Π1,s
1:T and the set x1
1:T = s covers every state variable, the
partition function can be computed as Z = Ps∈S 1 ˆ∆1,s
1:T .
MAP assignment is essentially the max-product problem, which can be solved by turning all sum-
mations in (2) into corresponding maximisations.
Parameter estimation in HSCRFs, as in other log-linear models, requires the computation of fea-
ture expectations as a part of the log-likelihood gradient (e.g. see [4]). The gradient is then fed into
any black-box standard numerical optimisation algorithms. As the feature expectations are rather
involved, we intend to omit the details. Rather, we include here as an example the expectation of the
state-persistence features
1
σpersist (i, j )δ(Πd,s
E[f d,s
Z Xi∈[1,T ] Xj∈[i,T ]
Xi∈[1,T ] Xj∈[i,T ]
i:j ∈ ζ )] =
where f d,s
σpersist (i, j ) is the state-persistence feature vector for the state s = xd
i:j starting at i and
ending at j ; Λd,s
i:j is the sum of products of all clique potentials falling outside the symmetric Markov
i:j ∈ ζ ) is the indicator function that the Markov blanket Πd,s
i:j ; and δ(Πd,s
blanket Πd,s
i:j is part of the
random conﬁguration ζ .

i:j Rd,s
i:j Λd,s
∆d,s
i:j

f d,s
σpersist (i, j )

4 Constrained Inference and Partially Supervised Learning

It may happen that the training data is not completely labelled, possibly due to lack of labelling
resources [12].
In this case, the learning algorithm should be robust enough to handle missing

labels. On the other hand, during inference, we may partially obtain high quality labels from external
sources [3]. This requires the inference algorithm to be responsive to the available labels which may
help to improve the performance.

In general, when we make observations, we observe some states and some ending indicators. Let
˜V = { ˜x, ˜e} be the set of observed state and end variables respectively. The procedures to compute
the auxiliary variables such as ∆d,s
i:j and αd,s
i:j (u) must be modi ﬁed to address constraints arisen from
i:j assumes Πd,s
these observations. For example, computing ∆d,s
i:j , which implies the constraint to the
state s at level d starting at i and persisting till terminating at j . Then, if any observations (e.g., there
k 6= s for k ∈ [i, j ]) are made causing this constraint invalid, ∆d,s
is an ˜xd
i:j will be zero. Therefore,
in general, the computation of each auxiliary variable is multiplied by an identity function that
enforces the consistency between the observations and the required constraints associated with the
computation of that variable.

As an example, we consider the computation of ∆d,s
i:j . The sum ∆d,s
i:j is only consistent if all of the
following conditions are satis ﬁed: (a) if there are observe d states at level d within the interval [i, j ]
i−1 = 1, (c) if the ending
they must be s, (b) if there is any observed ending indicator ˜ed
i−1 , then ˜ed
j is
indicator ˜ed
k is observed for some k ∈ [i, j − 1], then ˜ed
k = 0, and (d) if the ending indicator ˜ed
observed, then ˜ed
j = 1. These conditions are captured by using the following identity function
I[∆d,s
i:j ] = δ( ˜xd
k∈[i,j ] = s)δ(˜ed
i−1 = 1)δ(˜ed
k∈[i:j−1] = 0)δ(˜ed
j = 1)
When observations are made, the ﬁrst equation in (2) is thus re placed by
u,j (cid:19)
i:j ](cid:18) Xu∈S d+1
i:j (u)E d,s
αd,s
i:j = I[∆d,s
∆d,s

(3)

(4)

5 Applications

We describe two applications of the proposed hierarchical semi-Markov CRFs in this section: activ-
ity recognition in Secion 5.1 and shallow parsing in Section 5.2.

5.1 Recognising Indoor Activities

In this experiment, we evaluate the HSCRFs with a relatively small dataset from the domain of in-
door video surveillance. The task is to recognise trajectories and activities, which a person performs
in a kitchen, from his noisy locations extracted from video. The data, originally described in [7], has
45 training and 45 test sequences, each of which corresponds to one of 3 the persistent activities:
(1) preparing short-meal, (2) having snack and (3) preparing normal-meal. The persistent activities
share some of the 12 sub-trajectories. Each sub-trajectory is a sub-sequence of discrete locations.
Thus naturally, the data has a state hierarchy of depth 3: the dummy root for each location sequence,
the persistent activities, and the sub-trajectories. The input observations to the model are simply
sequences of discrete locations.

At each level d and time t we count an error if the predicted state is not the same as the ground-truth.
First, we examine the fully observed case where the HSCRF is compared against the DCRF [10] at
both data levels, and against the Sequential CRF (SCRF) [4] at the bottom level. Table 1 (the left
half) shows that (a) both the multilevel models signi ﬁcantl y outperform the ﬂat model and (b) the
HSCRF outperforms the DCRF.

Alg.
Alg.
d = 3
d = 2
d = 3
d = 2
90.4
80.2
PO-HSCRF
93.9
100
HSCRF
83.5
-
PO-SCRF
89.7
96.5
DCRF
-
-
-
82.6
-
SCRF
Table 1: Accuracy (%) for fully observed data (left), and partially observed (PO) data (right).
Next, we consider partially-supervised learning in which about 50% of start/end times of a state and
state labels are observed at the second level. All ending indicators are known at the bottom level.
The results are reported in Table 1 (the right half). As can be seen, although only 50% of the state

labels and state start/end times are observed, the model learned is still performing well with accuracy
of 80.2% and 90.4% at levels 2 and 3, respectively.

We next consider the issue of partially observing labels during decoding and test the effect using
degraded learned models. Such degraded models (emulating noisy training data or lack of training
time) are extracted from the 10th iteration of the fully observed data case. The labels are provided
at random times. Figure 4a shows the decoding accuracy as a function of available state labels. It is
interesting to observe that a moderate amount of observed labels (e.g. 20− 40%) causes the accuracy
rate to go up considerably.

5.2 POS Tagging and Noun-Phrase Chunking

In this experiment, we apply the HSCRF to the task of noun-phrase chunking. The data is from the
CoNLL-2000 shared task 3 , in which 8926 English sentences from the Wall Street Journal corpus
are used for training and 2012 sentences are for testing. Each word in a pre-processed sentence
is labelled by two labels: the part-of-speech (POS) and the noun-phrase (NP). There are 48 POS
labels and 3 NP labels (B-NP for beginning of a noun-phrase, I-NP for inside a noun-phrase or O
for others). Each noun-phrase generally has more than one words. To reduce the computational
burden, we reduce the POS tag-set to 5 groups: noun, verb, adjective, adverb and others. Since in
our HSCRFs we do not have to explicitly indicate which node is the beginning of a segment, the NP
label set can be reduced further into NP for noun-phrase, and O for anything else.

110

100

90

80

70

60

)
%
(
 
e
r
o
c
s
−
1
F
 
e
g
a
r
e
v
a

 

92

90

88

86

84

82

e
r
o
c
s
−
1
F

activities
sub−trajectories

50
 
0

80

90

 

SCRF
HSCRF+POS
HSCRF
DCRF+POS
DCRF
Semi−CRF

10

20
70
60
50
40
30
portion of available labels

103
number of training sentences
(a)
(b)
Figure 4: (a) Decoding accuracy of indoor activities as a function of available information on
label/start/end time .
(b) Performance of various models on Conll2000 noun phrase chunking.
HSCRF+POS and DCRF+POS mean HSCRF and DCRF with POS given at test time, respectively.

80

 

We build an HSCRF topology of 3 levels, where the root is just a dummy node, the second level
has 2 NP states, and the bottom level has 5 POS states. For comparison, we implement a DCRF,
a SCRF, and a semi-Markov CRF (Semi-CRF) [8]. The DCRF has grid structure of depth 2, one
for modelling the NP process and another for the POS process. Since the state spaces are relatively
small, we are able to run exact inference in the DCRF by collapsing both the NP and POS state
spaces to a combined state space of size 3 × 5 = 15. The SCRF and Semi-CRF model only the NP
process, taking the POS tags and words as input.

We extract raw features from the text in the way similar to that in [10]. The features for SCRF and the
Semi-CRF also include the POS tags. Words with less than 3 occurrences are not used. This reduces
the vocabulary and the feature size signi ﬁcantly. We also ma ke use of bi-grams with similar selection
criteria. Furthermore, we use the contextual window of 5 instead of 7 as in [10]. This setting gives
rise to about 32K raw features. The model feature is factorised as f (xc , z ) = I(xc )gc (z ), where
I(xc ) is a binary function on the assignment of the clique variables xc , and gc (z ) are the raw features.
Although both the HSCRF and the Semi-CRF are capable of modelling arbitrary segment durations,
we use a simple exponential distribution (i.e. weighted features activated at each time step are added

3 http://www.cnts.ua.ac.be/conll2000/chunking/

up) since it can be processed sequentially and thus is very efﬁcient. For learning, we use a simple
online stochastic gradient ascent method. At test time, since the SCRF and the Semi-CRF are able
to use the POS tags as input, it is not fair for the DCRF and HSCRF to predict those labels during
inference. Instead, we also give the POS tags to the DCRF and HSCRF and perform constrained
inference to predict only the NP labels. This boosts the performance of the two multi-level models
signi ﬁcantly.

Let us look at the difference between the ﬂat setting of SCRF a nd Semi-CRF and the the multi-
level setting of DCRF and HSCRF. Let x = (xnp , xpos ). Essentially, we are about to model the
distribution Pr(x|z ) = Pr(xnp |xpos , z ) Pr(xpos |z ) in the multi-level models while we ignore the
Pr(xpos |z ) in the ﬂat models. During test time of the multi-level models , we predict only the xnp
by ﬁnding the maximiser of Pr(xnp |xpos , z ). The Pr(xpos |z ) seems to be a waste because we do
not make use of it at test time. However, Pr(xpos |z ) does give extra information about the joint
distribution Pr(x|z ), that is, modelling the POS process may help to get smoother estimate of the
NP distribution.

The performance of these models is depicted in Figure 4b and we are interested in only the prediction
of the noun-phrases since this data has POS tags. Without POS tags given at test time, both the
HSCRF and the DCRF perform worse than the SCRF. This is not surprising because the POS tags
are always given in the case of SCRF. However, with POS tags, the HSCRF consistently works
better than all other models.

6 Discussion and Conclusions

The HSCRFs presented here are not a standard graphical model since the clique structures are not
predeﬁned. The potentials are deﬁned on-the- ﬂy depending o
n the assignments of the ending indica-
tors. Although the model topology is identical to that of shared structure HHMMs [1], the unrolled
temporal representation is an undirected graph, and the model distribution is formulated in a dis-
criminative way. Furthermore, the state persistence potentials capture duration information that is
not available in the DBN representation of the HHMMs in [6]. Thus, the segmental nature of the
HSCRF thus incorporates the recent semi-Markov CRF [8] as a special case [11].

Our HSCRF is related to the conditional probabilistic context-free grammar (C-PCFG) [9] in the
same way that the HHMM is to the PCFG. However, the context-free grammar does not limit the
depth of semantic hierarchy, thus making unnecessarily difﬁcult to map many hierarchical problems
into its form. Secondly, it lacks a graphical model representation, and thus does not enjoy the rich
set of approximate inference techniques available in graphical models.

References

[1] H. H. Bui, D. Q. Phung, and S. Venkatesh. Hierarchical hidden Markov models with general state
hierarchy. In AAAI, pages 324–329, San Jose, CA, Jul 2004.
[2] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden Markov model: Analysis and applications.
Machine Learning, 32(1):41–62, 1998.
[3] T. Kristjannson, A. Culotta, P. Viola, and A. McCallum.
Interactive information extraction with con-
strained conditional random ﬁelds. In AAAI, pages 412–418, San Jose, CA, 2004.
[4] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁeld s: Probabilistic models for segmenting
and labeling sequence data. In ICML, pages 282–289, 2001.
[5] L. Liao, D. Fox, and H. Kautz. Hierarchical conditional random ﬁe lds for GPS-based activity recognition.
In Proceedings of the International Symposium of Robotis Research (ISRR). Springer Verlag, 2005.
[6] K. Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning. PhD thesis, Com-
puter Science Division, University of California, Berkeley, Jul 2002.
[7] N. Nguyen, D. Phung, S. Venkatesh, and H. H. Bui. Learning and detecting activities from movement
trajectories using the hierarchical hidden Markov models. In CVPR, volume 2, pages 955–960, Jun 2005.
[8] S. Sarawagi and W. W. Cohen. Semi-Markov conditional random ﬁ elds for information extraction. In
NIPS. 2004.
[9] C. Sutton. Conditional probabilistic context-free grammars. Master’s thesis, Uni. of Massachusetts, 2004.
[10] C. Sutton, A. McCallum, and K. Rohanimanesh. Dynamic conditional random ﬁelds: Factorized proba-
bilistic models for labeling and segmenting sequence data. JMLR, 8:693–723, Mar 2007.
[11] T. T. Truyen, D. Q. Phung, H. H. Bui, and S. Venkatesh. Hierarchical semi-Markov conditional
random ﬁelds for recursive sequential data.
Technical report, Cur tin University of Technology,
http://www.computing.edu.au/ ˜trantt2/pubs/hcrf.pdf, 2008.
[12] J. Verbeek and B. Triggs. Scene segmentation with CRFs learned from partially labeled images.
Advances in Neural Information Processing Systems 20, pages 1553–1560. MIT Press, 2008.

In


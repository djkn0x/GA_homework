Nonparametric Regression and Classiﬁcation with
Joint Sparsity Constraints

Han Liu John Lafferty Larry Wasserman

Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We propose new families of models and algorithms for high-dimensional nonpara-
metric learning with joint sparsity constraints. Our approach is based on a regular-
ization method that enforces common sparsity patterns across different function
components in a nonparametric additive model. The algorithms employ a coor-
dinate descent approach that is based on a functional soft-thresholding operator.
The framework yields several new models, including multi-task sparse additive
models, multi-response sparse additive models, and sparse additive multi-category
logistic regression. The methods are illustrated with experiments on synthetic data
and gene microarray data.

1 Introduction

Many learning problems can be naturally formulated in terms of multi-category classiﬁcation or
multi-task regression. In a multi-category classiﬁcation problem, it is required to discriminate be-
tween the different categories using a set of high-dimensional feature vectors—for instance, clas-
sifying the type of tumor in a cancer patient from gene expression data. In a multi-task regression
problem, it is of interest to form several regression estimators for related data sets that share common
types of covariates—for instance, predicting test scores across different school districts. In other ar-
eas, such as multi-channel signal processing, it is of interest to simultaneously decompose multiple
signals in terms of a large common overcomplete dictionary, which is a multi-response regression
problem. In each case, while the details of the estimators vary from instance to instance, across
categories, or tasks, they may share a common sparsity pattern of relevant variables selected from a
high-dimensional space. How to ﬁnd this common sparsity pattern is an interesting learning task.
∑
In the parametric setting, progress has been recently made on such problems using regularization
based on the sum of supremum norms (Turlach et al., 2005; Tropp et al., 2006; Zhang, 2006). For
j=1 β (k)
j x(k)
ij + ϵ(k)
example, consider the K -task linear regression problem y (k)
i = β (k)
p
0 +
i where
 K∑

 1
 + λ
y (k)
2
the superscript k indexes the tasks, and the subscript i = 1, . . . , nk indexes the instances within a
nk∑
− p∑
p∑
task. Using quadratic loss, Zhang (2006) suggests the following estimator
bβ = arg min
|
2nk
i
(cid:12)
i=1
j=1
j=1
k=1
where maxk |β (k)
| = ∥βj ∥∞ is the sup-norm of the vector βj ≡ (β (1)
, . . . , β (K )
)T of coefﬁcients
j
j
j
for the j th feature across different tasks. The sum of sup-norms regularization has the effect of
“grouping” the elements in βj such that they can be shrunk towards zero simultaneously. The
problems of multi-response (or multivariate) regression and multi-category classiﬁcation can be
viewed as a special case of the multi-task regression problem where tasks share the same design
matrix. Turlach et al. (2005) and Fornasier and Rauhut (2008) propose the same sum of sup-norms

− β (k)
0

β (k)
j x(k)
ij

|β (k)
j

max
k

(1)

1

regularization as in (1) for such problems in the linear model setting. In related work, Zhang et al.
(2008) propose the sup-norm support vector machine, demonstrating its effectiveness on gene data.
In this paper we develop new methods for nonparametric estimation for such multi-task and multi-
category regression and classiﬁcation problems. Rather than ﬁtting a linear model, we instead esti-
mate smooth functions of the data, and formulate a regularization framework that encourages joint
functional sparsity, where the component functions can be different across tasks while sharing a
common sparsity pattern. Building on a recently proposed method called sparse additive models,
or “SpAM” (Ravikumar et al., 2007), we propose a convex regularization functional that can be
viewed as a nonparametric analog of the sum of sup-norms regularization for linear models. Based
on this regularization functional, we develop new models for nonparametric multi-task regression
and classiﬁcation, including multi-task sparse additive models (MT-SpAM), multi-response sparse
additive models (MR-SpAM), and sparse multi-category additive logistic regression (SMALR).
The contributions of this work include (1) an efﬁcient iterative algorithm based on a functional
soft-thresholding operator derived from subdifferential calculus, leading to the multi-task and multi-
response SpAM procedures, (2) a penalized local scoring algorithm that corresponds to ﬁtting a
sequence of multi-response SpAM estimates for sparse multi-category additive logistic regression,
and (3) the successful application of this methodology to multi-category tumor classiﬁcation and
biomarker discovery from gene microarray data.

2 Nonparametric Models for Joint Functional Sparsity
∫
∑
We begin by introducing some notation. If X has distribution PX , and f is a function of x, its
L2 (PX ) norm is denoted by ∥f ∥2 =
X f 2 (x)dPX = E(f 2 ). If v = (v1 , . . . , vn )T is a vector, de-
j and ∥v∥∞ = maxj |vj |. For a p-dimensional random vector (X1 , . . . , Xp ),
ﬁne ∥v∥2
n
n = 1
j=1 v2
let Hj denote the Hilbert subspace L2 (PXj ) of PXj -measurable functions fj (xj ) of the single scalar
n
∑
variable Xj with zero mean, i.e. E[fj (Xj )] = 0. The inner product on this space is deﬁned as
⟨fj , gj ⟩ = E [fj (Xj )gj (Xj )]. In this paper, we mainly study multivariate functions f (x1 , . . . , xp )
j fj (xj ), with fj ∈ Hj for j = 1, . . . , p.
that have an additive form, i.e., f (x1 , . . . , xp ) = α +
With H ≡ {1} ⊕ H1 ⊕ H2 ⊕ . . . ⊕ Hp denoting the direct sum Hilbert space, we have that f ∈ H.

2.1 Multi-task/Multi-response Sparse Additive Models
), i = 1, . . . , nk , k = 1, . . . , K },
In a K -task regression problem, we have observations {(x(k)
, y (k)
i
i
where x(k)
i1 , . . . , x(k)
i = (x(k)
ip )T is a p-dimensional covariate vector, the superscript k indexes tasks
and i indexes the i.i.d. samples for each task. In the following, for notational simplicity, we assume
that n1 = . . . = nK = n. We also assume different tasks are comparable and each Y (k) and
(
)
X (k)
has been standardized, i.e., has mean zero and variance one. This is not really a restriction
∑
j
of the model since a straightforward weighting scheme can be adopted to extend our approach to
Y (k) | X (k) = x(k)
= f (k) (x(k) ) ≡
handle noncomparable tasks. We assume the true model is E
(x(k)
j=1 f (k)
p
) for k = 1, . . . , K , where, for simplicity, we take all intercepts α(k) to be zero. Let
f (k) (x, y) = (y − f (k) (x))2 denote the quadratic loss. To encourage common sparsity patterns
Q
j
j
p∑
across different function components, we deﬁne the regularization functional (cid:8)K (f ) by
j=1

max
k=1;:::;K

(cid:8)K (f ) =

∥f (k)
j

∥.

(2)

The regularization functional (cid:8)K (f ) naturally combines the idea of the sum of sup-norms penalty
for parametric joint sparsity and the regularization idea of SpAM for nonparametric functional spar-
sity; if K = 1, then (cid:8)1 (f ) is just the regularization term introduced for (single-task) sparse additive
models by Ravikumar et al. (2007). If each f (k)
is a linear function, then (cid:8)K (f ) reduces to the
j
sum of sup-norms regularization term as in (1). We shall employ (cid:8)K (f ) to induce joint functional
sparsity in nonparametric multi-task inference.

2

{
}
n∑
K∑
Using this regularization functional, the multi-task sparse additive model (MT-SpAM) is formulated
bf (1) , . . . , bf (K ) = arg min
as a penalized M-estimator, by framing the following optimization problem
Q
1
2n
f (1) ;:::;f (K )
i=1
k=1
∈ H(k)
where f (k)
for j = 1, . . . , p and k = 1, . . . , K , and λ > 0 is a regularization parameter.
j
j
The multi-response sparse additive model (MR-SpAM) has exactly the same formulation as in (3)
except that a common design matrix is used across the K different tasks.

) + λ(cid:8)K (f )

f (k) (x(k)
i

, y (k)
i

(3)

2.2 Sparse Multi-Category Additive Logistic Regression

In a K -category classiﬁcation problem, we are given n examples (x1 , y1 ), . . . , (xn , yn ) where xi =
)T is a (K − 1)-
, . . . , y (K−1)
(xi1 , . . . , xip )T is a p-dimensional predictor vector and yi = (y (1)
i
i
dimensional response vector in which at most one element can be one, with all the others being
zero. Here, we adopt the common “1-of-K ” labeling convention where y (k)
i = 1 if xi has category
)
(
k and y (k)
i = 0 otherwise; if all elements of yi are zero, then xi is assigned the K -th category.
) , k = 1, . . . , K − 1
(
∑
The multi-category additive logistic regression model is
∑
f (k) (x)
exp
P(Y (k) = 1 | X = x) =
(4)
K−1
f (k′ ) (x)
1 +
k′=1 exp
(xj ) has an additive form. We deﬁne f = (f (1) , . . . , f (K−1) ) to
j=1 f (k)
p
where f (k) (x) = α(k) +
j
f (x) = P(Y (k) = 1 | X = x) to be the conditional probability of
be a discriminant function and p(k)
category k given X = x. The logistic regression classiﬁer hf (·) induced by f , which is a mapping
from the sample space to the category labels, is simply given by hf (x) = arg maxk=1;:::;K p(k)
f (x).
If a variable Xj is irrelevant, then all of the component functions f (k)
are identically zero, for each
k = 1, 2, . . . , K − 1. This motivates the use of the regularization functional (cid:8)K−1 (f ) to zero out
j
, . . . , f (K−1)
)
(
entire vectors fj = (f (1)
).
K−1∑
K−1∑
j
j
Denoting
y (k) f (k) (x) − log
) (x)
1 +
ℓf (x, y) =
k′=1
}
{
k=1
n∑
the sparse multi-category additive logistic regression estimator
as the multinomial
log-loss,
bf (1) , . . . , bf (K−1) = arg min
(SMALR) is thus formulated as the solution to the optimization problem
− 1
ℓf (xi , yi ) + λ(cid:8)K−1 (f )
n
f (1) ;:::;f (K−1)
i=1
for j = 1, . . . , p and k = 1, . . . , K − 1.
∈ H(k)
j

where f (k)
j

exp f (k

(5)

′

3 Simultaneous Sparse Backﬁtting

We use a blockwise coordinate descent algorithm to minimize the functional deﬁned in (3). We ﬁrst
formulate the population version of the problem by replacing sample averages by their expectations.
We then derive stationary conditions for the optimum and obtain a population version algorithm for
computing the solution by a series of soft-thresholded univariate conditional expectations. Finally,
j = Y (k) − ∑
a ﬁnite sample version of the algorithm can be derived by plugging in nonparametric smoothers for
these conditional expectations.
For the j th block of component functions f (1)
, . . . , f (K )
, let R(k)
(X (k)
l ̸=j f (k)
) de-
[
]
}
{
j
j
l
l
(
)2
K∑
note the partial residuals. Assuming all but the functions in the j th block to be ﬁxed, the optimization
bf (1)
, . . . , bf (K )
problem is reduced to
∥
R(k)
j = arg min
.
j
j
(K )
(1)
f
;:::;f
k=1
j
j

+ λ max
k=1;:::;K

− f (k)
j

∥f (k)
j

(X (k)
j

(6)

1
2

E

)

3

(
)
The following result characterizes the solution to (6).
∥, and order the indices according to
j = ∥P (k)
| X (k)
j = E
Theorem 1. Let P (k)
and s(k)
R(k)
P (ki )
j
j
j
≥ . . . ≥ s(kK )
≥ s(k2 )
s(k1 )
]
[
. Then thesolution to (6) isgivenby
j
j
j
∗∑
j
m
f (ki )
j =
1
(∑
m∗
i′=1
− λ
i′=1 s(ki′ )
m
j

P (ki )
j
s(ki )
j
+
and [·]+ denotes thepositivepart.

∗ = argmaxm

∗
for i > m
for i ≤ m
∗

.

− λ

)
s(ki′ )
j

(7)

wherem

1
m

Therefore, the optimization problem in (6) is solved by a soft-thresholding operator, given in equa-
tion (7), which we shall denote as
) = Soft(∞)
, . . . , R(K )
[R(1)
, . . . , f (K )
(f (1)
].
(8)
j
j
j
j
(cid:21)
While the proof of this result is lengthy, we sketch the key steps below, which are a functional ex-
tension of the subdifferential calculus approach of Fornasier and Rauhut (2008) in the linear setting.
First, we formulate an optimality condition in terms of the G ˆateaux derivative as follows.
− P (k)
Lemma 2. Thefunctions f (k)
j aresolutionsto (6) ifandonlyif f (k)
j + λuk vk = 0 (almost
(cid:12)(cid:12)(cid:16)∥f
j
surely),for k = 1, . . . , K,where uk arescalarsand vk aremeasurablefunctionsof X (k)
j ,with
∥(cid:17)T and vk ∈ ∂ ∥f (k)
(u1 , . . . , uK )T ∈ ∂ ∥ · ∥∞
∥, k = 1, . . . , K.
∥;:::;∥f
(1)
(K )
j
j
j
Here the former one denotes the subdifferential of the convex functional ∥ · ∥∞ evaluated at
∥)T , it lies in a K -dimensional Euclidean space. And the latter denotes the sub-
(∥f (1)
∥, . . . , ∥f (K )
j
j
differential of ∥f (k)
∥, which is a set of functions. Next, the following proposition from Rockafellar
j
and Wets (1998) is used to characterize the subdifferential of sup-norms.
{
(cid:12)(cid:12)
Lemma 3. Thesubdifferentialof ∥ · ∥∞ on RK is
∂ ∥ · ∥∞
if x = 0
B 1 (1)
conv{sign(xk )ek : |xk | = ∥x∥∞} otherwise.
where B 1 (1) denotes the ℓ1 ballof radiusone, conv(A) denotes theconvexhullofset A,and ek is
the k-thcanonicalunitvectorin RK .

x =

Using Lemma 2 and Lemma 3, the proof of Theorem 1 proceeds by considering three cases for the
∥)T : (1) ∥f (k)
sup-norm subdifferential evaluated at (∥f (1)
∥ = 0 for k = 1, . . . , K ; (2)
∥, . . . , ∥f (K )
j
j
j
there exists a unique k , such that ∥f (k)
∥ = maxk′=1;:::;K ∥f (k
∥ ̸= 0; (3) there exists at least two
′
)
j
j
k ̸= k
′ , such that ∥f (k)
∥ = ∥f (k
∥ = maxm=1;:::;K ∥f (m)
∥ ̸= 0. The derivations for cases (1) and
′
)
j
j
j
(∑
)
(2) are relatively straightforward, but for case (3) we prove the following.
Lemma 4. The sup-norm is attained precisely at m > 1 entries if only if m is the largest number
− λ
≥ 1
m−1
i′=1 s(ki′ )
such that s(km )
.
m−1
j
j

The proof of Theorem 1 then follows from the above lemmas and some calculus. Based on this
result, the data version of the soft-thresholding operator is obtained by replacing the conditional
| X (k)
) by S (k)
, where S (k)
j = E(R(k)
expectation P (k)
j R(k)
is a nonparametric smoother for
j
j
j
j
variable X (k)
, e.g., a local linear or spline smoother; see Figure 1. The resulting simultaneous
j
sparse backﬁtting algorithm for multi-task and multi-response sparse additive models (MT-SpAM
and MR-SpAM) is shown in Figure 2. The algorithm for the multi-response case (MR-SpAM) has
S (1)
j = . . . = S (K )
since there is only a common design matrix.
j

4

, . . . , S (K )
j

]: DATA VER S ION

; S (1)
SO FT- THR E SHOLD ING O P ERATOR SO F T(∞)
[R(1)
, . . . , R(K )
j
j
j
(cid:21)
i by smoothing: bP (k)
j = E hR(k)
Input: Smoothing matrices S (k)
, residuals R(k)
for k = 1; : : : ; K , regularization parameter (cid:21).
j
j
| X (k)
j = S (k)
j R(k)
(1) Estimate P (k)
;
j = ∥ bPj ∥n and order the indices according to bs(k1 )
j
j
j
(2) Estimate norm: bs(k)
≥ : : : ≥ bs(kK )
≥ bs(k2 )
(cid:16)Pm
− (cid:21)(cid:17) and calculate
j
j
j
∗
i′=1 s(ki′ )
8>><>>:
= arg maxm
(3) Find m
1
bP (ki )
j
m
#
" m
∗
bP (ki )
∗X
bf (ki )
i′=1 bs(ki′ )
j
jbs(ki )
j =
1
m∗
j
j
← bf (k)
(4) Center bf (k)
− mean( bf (k)
+
Output: Functions bf (k)
) for k = 1; : : : ; K .
j
j
j
for k = 1; : : : ; K .
j
Figure 1: Data version of the soft-thresholding operator.

for i > m
for i ≤ m
∗

− (cid:21)

;

;

MU LT I - TA SK AND MULT I -RE S PON S E S PAM
Initialize: Set bf (k)
Input: Data (x(k)
; y (k)
); i = 1; : : : ; n; k = 1; : : : ; K and regularization parameter (cid:21).
i
i
j = 0 and compute smoothers S (k)
for j = 1; : : : ; p and k = 1; : : : ; K ;
j
Iterate until convergence:
j = y (k) − Pk′ ̸=j bf (k)
For each j = 1; : : : ; p:
(2) Threshold: bf (1)
; : : : ; bf (K )
(1) Compute residuals: R(k)
for k = 1; : : : ; K ;
k′
← Soft(∞)
; S (1)
; : : : ; S (K )
Output: Functions bf (k) for k = 1; : : : ; K .
[R(1)
; : : : ; R(K )
j
j
j
j
j
j
(cid:21)
Figure 2: The simultaneous sparse backﬁtting algorithm for MT-SpAM or MR-SpAM. For the multi-
response case, the same smoothing matrices are used for each k .

].

3.1 Penalized Local Scoring Algorithm for SMALR

We now derive a penalized local scoring algorithm for sparse multi-category additive logistic re-
gression (SMALR), which can be viewed as a variant of Newton’s method in function space. At
each iteration, a quadratic approximation to the loss is used as a surrogate functional with the regu-
larization term added to induce joint functional sparsity. However, a technical difﬁculty is that the
approximate quadratic problem in each iteration is weighted by a non-diagonal matrix in function
space, thus a trivial extension of the algorithm in (Ravikumar et al., 2007) for sparse binary non-
parametric logistic regression does not apply. To tackle this problem, we use an auxiliary function
to lower bound the log-loss, as in (Krishnapuram et al., 2005).
second-order Lagrange form Taylor expansion to L(f ) at bf is then
The population version of the log-loss is L(f ) = E[ℓf (X, Y )] with f = (f (1) , . . . , f (K−1) ). A
[
]
[
]
∇L( bf )T (f − bf )
L(f ) = L( bf ) + E
(f − bf )T H ( ef )(f − bf )
for some function ef , where the gradient is ∇L( bf ) = Y − p bf (X ) with p bf (X ) = (p bf (Y (1) =
1
E
+
(9)
(
)
2
1 | X ), . . . , p bf (Y (K−1) = 1 | X ))T , and the Hessian is H ( ef ) = −diag
Deﬁning B = −(1/4)IK−1 , it is straightforward to show that B ≼ H ( ef ), i.e., H ( ef ) − B is
p ef (X )
+ p ef (X )p ef (X )T .
[
]
[
]
L(f ) ≥ L( bf ) + E
∇L( bf )T (f − bf )
(f − bf )T B (f − bf )
positive-deﬁnite. Therefore, we have that
1
E
+
(10)
.
2
5

SMALR : S PAR SE MULT I -CATEGORY ADD I T IV E LOG I ST IC R EGR E S S ION
j = 0 and b(cid:11)(k) = log (cid:16)Pn
. (cid:16)n − Pn
(cid:17)(cid:17), k = 1; : : : ; K − 1
i=1 PK−1
Initialize: bf (k)
Input: Data (xi ; yi ); i = 1; : : : ; n and regularization parameter (cid:21).
′
k′=1 y (k
i=1 y (k)
)
i
i
Iterate until convergence:
(xi ) ≡ P(Y (k) = 1 | X = xi ) as in (4) for k = 1; : : : ; K − 1;
(xi )(cid:17)+ b(cid:11)(k) +Pp
i = 4 (cid:16)y (k)
(1) Compute p(k)bf
j=1 bf (k)
− p(k)bf
(2) Calculate the transformed responses Z (k)
(3) Call subroutines ( bf (1) ; : : : ; bf (K−1) ) ← MR-SpAM (cid:16)(xi ; Z (k)
2(cid:21)(cid:17);
for k = 1; : : : ; K − 1 and i = 1; : : : ; n;
i
j
√
nX
)n
i=1 ;
i
(4) Adjust the intercepts: (cid:11)(k) ← 1
Z (k)
;
i
Output: Functions bf (k) and intercepts b(cid:11)(k) for k = 1; : : : ; K − 1.
n
i=1
Figure 3: The penalized local scoring algorithm for SMALR.
)
(∥Z − Af ∥2
The following lemma results from straightforward calculation.
−1 (Y − p bf ) + A bf .
Lemma 5. The solution f thatmaximizes the righthand side of (10) is equivalent to the solution
∑
where A = (−B )1=2 and Z = A
E
thatminimizes 1
n
2
]
[(
)2
j=1 f (k)
p
Recalling that f (k) = α(k) +
K−1∑
, equation (9) and Lemma 5 then justify the use of the
′(k) − ∑
j
auxiliary functional
(
)
1
′(cid:8)K−1 (f )
∑
E
bf (k)
p
+ λ
j=1 f (k) (Xj )
+ bα(k) +
Z
2
k=1
Y (k) − P bf (Y (k) = 1 | X )
′ =
′(k) = 4
p
(Xj ) and λ
2λ. This is
where Z
j
j=1
precisely in the form of a multi-response SpAM optimization problem in equation (3). The resulting
algorithm, in the ﬁnite sample case, is shown in Figure 3.

(11)

(xij )

√

4 Experiments

4.1 Synthetic Data

In this section, we ﬁrst use simulated data to investigate the performance of the MT-SpAM simulta-
neous sparse backﬁtting algorithm. We then apply SMALR to a tumor classiﬁcation and biomarker
identiﬁcation problem. In all experiments, the data are rescaled to lie in the p-dimensional cube
∑
∑
[0, 1]p . We use local linear smoothing with a Gaussian kernel. To choose the regularization param-
(
)
∑
∑
eter λ, we simply use J -fold cross-validation or the GCV score from (Ravikumar et al., 2007) ex-
∥ bf (k)
))/(n2K 2−(nK )df (λ))2
Q bf (k) (x(k)
, y (k)
K
n
tended to the multi-task setting: GCV(λ) =
i
i
k=1
i=1
j = trace(S (k)
∥n ̸= 0
j=1 ν (k)
, and ν (k)
K
p
where df (λ) =
) is the effective degrees
j I
j
j
k=1
of freedom for the univariate local linear smoother on the j th variable.
∑
We generated n = 100 observations from a 10-dimensional three-task additive model with four
∼ N (0, 1); the com-
relevant variables: y (k)
j=1 f (k)
(x(k)
ij ) + ϵ(k)
, k = 1, 2, 3, where ϵ(k)
4
i =
j
i
i
ponent functions f (k)
are plotted in Figure 4. The 10-dimensional covariates are generated as
j
X (k)
j = (W (k)
j + tU (k) )/(1 + t), j = 1, . . . , 10 where W (k)
, . . . , W (k)
10 and U (k) are i.i.d. sampled
from Uniform(−2.5, 2.5). Thus, the correlation between Xj and Xj ′ is t2 /(1 + t2 ) for j ̸= j
1
′ .
The results of applying MT-SpAM with the bandwidths h = (0.08, . . . , 0.08) and regularization
parameter λ = 0.25 are summarized in Figure 4. The upper 12 ﬁgures show the 12 relevant com-
ponent functions for the three tasks; the estimated function components are plotted as solid black

6

Variable selection accuracy

Estimation accuracy: MSE (sd)

Model
MR−SpAM
MARS

t = 0

t = 1

t = 2

t = 3

t = 0

t = 1

t = 2

t = 3

89
0

80
0

47
0

37
0

7:43 (0:71)
8:66 (0:78)

5:82 (0:60)
7:52 (0:61)

3:83 (0:37)
5:36 (0:40)

3:07 (0:30)
4:64 (0:35)

Figure 4: (Top) Estimated vs. true functions from MT-SpAM; (Middle) Regularization paths using MT-SpAM.
(Bottom) Quantitative comparison between MR-SpAM and MARS
lines and the true function components are plotted using dashed red lines. For all the other variables
(from dimension 5 to dimension 10), both the true and estimated components are zero. The middle
three ﬁgures show regularization paths as the parameter λ varies; each curve is a plot of the max-
imum empirical L1 norm of the component functions for each variable, with the red vertical line
representing the selected model using the GCV score. As the correlation increases (t increases), the
separation between the relevant dimensions and the irrelevant dimensions becomes smaller. Using
the same setup but with one common design matrix, we also compare the quantitative performance
of MR-SpAM with MARS (Friedman, 1991), which is a popular method for multi-response additive
regression. Using 100 simulations, the table illustrates the number of times the models are correctly
identiﬁed and the mean squared error with the standard deviation in the parentheses. (The MARS
simulations are carried out in R, using the default options of the mars function in the mda library.)

4.2 Gene Microarray Data

Here we apply the sparse multi-category additive logistic regression model to a microarray dataset
for small round blue cell tumors (SRBCT) (Khan et al., 2001). The data consist of expression
proﬁles of 2,308 genes (Khan et al., 2001) with tumors classiﬁed into 4 categories: neuroblastoma
(NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL), and the Ewing family of tumors
(EWS). The dataset includes a training set of size 63 and a test set of size 20. These data have
been analyzed by different groups. The main purpose is to identify important biomarkers, which
are a small set of genes that can accurately predict the type of tumor of a patient. To achieve 100%
accuracy on the test data, Khan et al. (2001) use an artiﬁcial neural network approach to identify 96
genes. Tibshirani et al. (2002) identify a set of only 43 genes, using a method called nearest shrunken
centroids. Zhang et al. (2008) identify 53 genes using the sup-norm support vector machine.
In our experiment, SMALR achieves 100% prediction accuracy on the test data with only 20 genes,
which is a much smaller set of predictors than identiﬁed in the previous approaches. We follow
the same procedure as in (Zhang et al., 2008), and use a very simple screening step based on the
marginal correlation to ﬁrst reduce the number of genes to 500. The SMALR model is then trained
using a plugin bandwidth h0 = 0.08, and the regularization parameter λ is tuned using 4-fold cross
validation. The results are tabulated in Figure 5. In the left ﬁgure, we show a “heat map” of the
selected variables on the training set. The rows represent the selected genes, with their cDNA chip
image id. The patients are grouped into four categories according to the corresponding tumors,

7

0.00.20.40.60.81.0−2−1012k=1x10.00.20.40.60.81.0−2−1012k=2x10.00.20.40.60.81.0−101234k=3x10.00.20.40.60.81.0−2−1012k=1x20.00.20.40.60.81.0−2−1012k=2x20.00.20.40.60.81.0−101234k=3x20.00.20.40.60.81.0−2−1012k=1x30.00.20.40.60.81.0−2−1012k=2x30.00.20.40.60.81.0−101234k=3x30.00.20.40.60.81.0−2−1012k=1x40.00.20.40.60.81.0−2−1012k=2x40.00.20.40.60.81.0−101234k=3x40.00.20.40.60.81.00.00.10.20.30.4t=0Path IndexEmpirical sup−L1 norm101230.00.20.40.60.81.00.00.10.20.30.4t=2Path IndexEmpirical sup−L1 norm106430.00.20.40.60.81.00.00.10.20.30.4t=4Path IndexEmpirical sup−L1 norm78132Figure 5: SMALR results on gene data: heat map (left), marginal ﬁts (center), and CV score (right).

as illustrated in the vertical groupings. The genes are ordered by hierarchical clustering of their
expression proﬁles. The heatmap clearly shows four block structures for the four tumor categories.
This suggests visually that the 20 genes selected are highly informative of the tumor type. In the
middle of Figure 5, we plot the ﬁtted discriminant functions of different genes, with their image ids
listed on the plot. The values k = 1, 2, 3 under each subﬁgure indicate the discriminant function
the plot represents. We see that the ﬁtted functions are nonlinear. The right subﬁgure illustrates the
total number of misclassiﬁed samples using 4-fold cross validation, the λ values 0.3, 0.4 are both
zero, for the purpose of a sparser biomarker set, we choose λ = 0.4. Interestingly, only 10 of the
20 identiﬁed genes from our method are among the 43 genes selected using the shrunken centroids
approach of Tibshirani et al. (2002). 16 of them are are among the 96 genes selected by neural
network approach of Khan et al. (2001). This non-overlap may suggest some further investigation.

5 Discussion and Acknowledgements

We have presented new approaches to ﬁtting sparse nonparametric multi-task regression models and
sparse multi-category classiﬁcation models. Due to space constraints, we have not discussed results
on the statistical properties of these methods, such as oracle inequalities and risk consistency; these
theoretical results will be reported elsewhere. This research was supported in part by NSF grant
CCF-0625879.

References
FORNA S I ER , M . and RAUHU T, H . (2008). Recovery algorithms for vector valued data with joint sparsity
constraints. SIAM Journal of Numerical Analysis 46 577–613.
FR I EDMAN , J . H . (1991). Multivariate adaptive regression splines. The Annals of Statistics 19 1–67.
KHAN , J . , W E I , J . S . , R INGN ER , M . , SAA , L . H . , LADANY I , M . , W E ST ERMANN , F., B ERTHO LD , F.,
SCHWAB , M . , ANTON E SCU , C . R . , P ETER SON , C . and M ELTZER , P. S . (2001). Classiﬁcation and diag-
nostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks. Nature Medicine
7 673 –679.
KR I SHNA PURAM , B . , CAR IN , L . , F IGU E IR EDO , M . and HART EM INK , A . (2005). Sparse multinomial logistic
regression: Fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis and Machine
Intelligence 27 957– 968.
RAV IKUMAR , P., L IU , H ., LA FFERTY, J . and WA S SERMAN , L . (2007). SpAM: Sparse additive models. In
Advances in Neural Information Processing Systems 20. MIT Press.
ROCKA FEL LAR , R . T. and W ET S , R . J . -B . (1998). Variational Analysis. Springer-Verlag Inc.
T IB SH IRAN I , R . , HA ST I E , T., NARA S IMHAN , B . , and CHU , G . (2002). Diagnosis of multiple cancer types
by shrunken centroids of gene expression. Proc Natl Acad Sci U.S.A. 99 6567–6572.
TRO P P, J . , G I LB ERT, A . C . and S TRAU S S , M . J . (2006). Algorithms for simultaneous sparse approximation.
Part II: Convex relaxation. Signal Processing 86 572–588.
TUR LACH , B . , V ENABL E S , W. N . and WR IGH T, S . J . (2005). Simultaneous variable selection. Technometrics
27 349–363.
ZHANG , H . H . , L IU , Y., WU , Y. and ZHU , J . (2008). Variable selection for the multicategory SVM via
adaptive sup-norm regularization. Electronic Journal of Statistics 2 149–1167.
ZHANG , J . (2006). A probabilistic framework for multitask learning. Tech. Rep. CMU-LTI-06-006, Ph.D. the-
sis, Carnegie Mellon University.

8

770394377461143586248611038318813474884162032518281210530823137704878422424461879625820727429644881452680649236282701751RMS.T11RMS.T10RMS.T3RMS.T5RMS.T8RMS.T7RMS.T6RMS.T2RMS.T4RMS.T1RMS.C11RMS.C10RMS.C8RMS.C7RMS.C6RMS.C5RMS.C2RMS.C9RMS.C3RMS.C4NB.C8NB.C9NB.C11NB.C10NB.C5NB.C4NB.C7NB.C12NB.C6NB.C3NB.C2NB.C1BL.C4BL.C3BL.C2BL.C1BL.C8BL.C7BL.C6BL.C5EWS.C10EWS.C11EWS.C1EWS.C7EWS.C9EWS.C6EWS.C4EWS.C2EWS.C3EWS.C8EWS.T19EWS.T15EWS.T14EWS.T13EWS.T12EWS.T11EWS.T9EWS.T7EWS.T6EWS.T4EWS.T3EWS.T2EWS.T10.00.20.40.60.81.0−3−2−10123ID.207274k=10.00.20.40.60.81.0−3−2−10123ID.1435862k=10.00.20.40.60.81.0−3−2−10123ID.207274k=30.00.20.40.60.81.0−3−2−10123ID.770394k=10.00.20.40.60.81.0−3−2−10123ID.377048k=20.00.20.40.60.81.0−3−2−10123ID.377048k=30.10.30.50.7lambdaCV Score0.00.51.01.52.02.53.0
Unifying the Sensory and Motor Components
of Sensorimotor Adaptation

Adrian Haith
School of Informatics
University of Edinburgh, UK
adrian.haith@ed.ac.uk

Carl Jackson
School of Psychology
University of Birmingham, UK
c.p.jackson.1@bham.ac.uk

Chris Miall
School of Psychology
University of Birmingham, UK
r.c.miall@bham.ac.uk

Sethu Vijayakumar
School of Informatics
University of Edinburgh, UK
sethu.vijayakumar@ed.ac.uk

Abstract

Adaptation of visually guided reaching movements in novel visuomotor en-
vironments (e.g. wearing prism goggles) comprises not only motor adapta-
tion but also substantial sensory adaptation, corresponding to shifts in the
perceived spatial location of visual and proprioceptive cues. Previous com-
putational models of the sensory component of visuomotor adaptation have
assumed that it is driven purely by the discrepancy introduced between vi-
sual and proprioceptive estimates of hand position and is independent of
any motor component of adaptation. We instead propose a uniﬁed model in
which sensory and motor adaptation are jointly driven by optimal Bayesian
estimation of the sensory and motor contributions to perceived errors. Our
model is able to account for patterns of performance errors during visuo-
motor adaptation as well as the subsequent perceptual aftereﬀects. This
uniﬁed model also makes the surprising prediction that force ﬁeld adap-
tation will elicit similar perceptual shifts, even though there is never any
discrepancy between visual and proprioceptive observations. We conﬁrm
this prediction with an experiment.

1 Introduction

When exposed to a novel visuomotor environment, for instance while wearing prism goggles,
sub jects initially exhibit large directional errors during reaching movements but are able to
rapidly adapt their movement patterns and approach baseline performance levels within
around 30-50 reach trials. Such visuomotor adaptation is multifaceted, comprising both
sensory and motor components [5]. The sensory components of adaptation can be measured
through alignment tests in which sub jects are asked to localize either a visual target or their
unseen ﬁngertip, with their other (also unseen) ﬁngertip (without being able to make contact
between hands). These tests reveal substantial shifts in the perceived spatial location of both
visual and proprioceptive cues, following adaptation to shifted visual feedback [7].

While a shift in visual spatial perception will be partially reﬂected in reaches towards visual
targets, sensory adaptation alone cannot fully account for the completenes of visuomo-
tor adaptation, since the shifts in visual perception are always substantially less than the
experimentally-imposed shift. There must therefore be some additional motor component
of adaptation, i.e. some change in the relationship between the planned movement and the

v
r
t

disturbances}
y
r
t
p
r
t

ut

yt

motor command

hand position

vt

pt

proprioceptive
observation

visual observation

Figure 1: Graphical model of a single reach
in a motor adaptation experiment. Motor
command ut , and visual and proprioceptive
observations of hand position, vt and pt , are
available to the sub ject. Three distinct dis-
turbances aﬀect observations: A motor dis-
turbance ry
t may aﬀect the hand position yt
given the motor command ut . Visual and
t and rp
proprioceptive disturbances, rv
t , may
aﬀect the respective observations given hand
position.

issued motor command. This argument is reinforced by the ﬁnding that patterns of reach
aftereﬀects following visuomotor adaptation depend strongly on the motor task performed
during adaptation [5].

From a modelling point of view, the sensory and motor components of adaptation have
previously only been addressed in isolation of one another. Previously proposed models of
sensory adaptation have assumed that it is driven purely by discrepancies between hand
position estimates from diﬀerent sensory modalities. Ghahramani et al.
[2] proposed a
computational model based on a maximum likelihood principle, details of which we give in
Section 3. On its own, this sensory adaptation model cannot provide a complete description
of visuomotor adaptation since it does not fully account for improvements in performance
from trial to trial. It can, however, be plausibly combined with a conventional error-driven
motor adaptation model in which the performance error is calculated using the maximum
likelihood estimate of hand position. The resulting composite model could plausibly account
for both performance improvements and perceptual shifts during visuomotor adaptation.
According to this view, sensory and motor adaptation are very much independent processes,
one driven by sensory discrepancy and the other driven by (estimated) task performance
error.

In Section 4, we argue for a more uniﬁed view of sensory and motor adaptation in which
all three components of adaptation are jointly guided by optimal Bayesian inference of the
corresponding potential sources of error experienced on each trial, given noisy visual and
proprioceptive observations of performance and noisy motor execution. This uniﬁed sensory
and motor adaptation model is also able to account for both performance improvements and
perceptual shifts during visuomotor adaptation. However, our uniﬁed model also makes the
surprising prediction that a motor disturbance, e.g. an external force applied to hand via
a manipulandum, will also elicit sensory adaptation. The MLE-based model predicts no
such sensory adaptation, since there is never any discrepancy between sensory modalities.
We test this prediction directly with an experiment (Section 5) and ﬁnd that force ﬁeld
adaptation does indeed lead to sensory as well as motor adaptation.

2 Modelling framework

Before describing the details of the models, we ﬁrst outline a basic mathematical frame-
work for describing reaching movements in the context of a motor adaptation experiment,
representing the assumptions common to both the MLE-based and the Bayesian adapta-
tion models. Figure 1 illustrates a graphical model of a single reaching movement during
an adaptation experiment, from the sub ject’s point of view. The multiple components of
visuomotor adaptation described above correspond to three distinct potential sources of
observed outcome error (across both observation) modalities in a single reaching trial.

On trial t, the sub ject generates a (known) motor command ut . This motor command ut
leads to a ﬁnal hand position yt , which also depends on some (unknown) motor disturbance

rv

rp

yt

vt

pt

Figure 2: MLE-based sensor adaptation model.
Visual and proprioceptive disturbances rv , rp are
treated as parameters of the model. Estimates ˆrv
t
and ˆrp
t of these parameters are maintained via an
online EM-like procedure.

ry
t (e.g. an external force applied to the hand) and motor noise ǫu
t . We assume the ﬁnal
hand position yt is given by

yt = ut + ry
t + ǫu
(1)
t ,
where ǫu
t ∼ N (0, σ2
u ). Although this is a highly simpliﬁed description of the forward dynam-
ics of the reaching movement, it can be regarded as a ﬁrst-order approximation to the true
dynamics. Similar assumptions have proved very successful elsewhere in models of force
ﬁeld adaptation, e.g. [1]

The experimenter ultimately measures the hand position yt , however this is not directly
observed by the sub ject. Instead, noisy and potentially shifted observations are available
through visual and proprioceptive modalities,

(2)
(3)

vt = yt + rv
t + ǫv
t ,
pt = yt + rp
t + ǫp
t ,
t and ǫp
where the observation noises ǫv
t are zero-mean and Gaussian with variances σ2
v and
σ2
p , respectively.
We denote the full set of potential disturbances on trial t by
t , rp
t , ry
rt = (rv
t )T .
(4)
t , ˆry
t , ˆrp
t )T of the total
We assume that the sub ject maintains an internal estimate ˆrt = (ˆrv
disturbance rt and selects his motor commands on each trial accordingly. For reaches to a
t , the appropriate motor command is given by
visual target located at v∗
t − ˆry
t − ˆrv
ut = v∗
t .
Adaptation can be viewed as a process of iteratively updating the disturbance estimate, ˆrt ,
following each trial given the new (noisy) observations vt and pt and the motor command
ut . Exactly how the sub ject uses the information available to infer the current disturbances
is the sub ject of subsequent sections of this paper.

(5)

3 Existing sensory adaptation models

The prevailing view of sensory adaptation centres around the principle of maximum likeli-
hood estimation and was ﬁrst proposed by Ghahramani et al. [2] in the context of combining
discrepant visual and auditory cues in a target location task. It has nevertheless been wide-
ley accepted as a model of how the nervous system deals with visual and proprioceptive
cues. Van Beers et al.
[7], for instance, based an analysis of the relative uncertainty of
visual and proprioceptive estimates of hand location on this principle.

We suppose that, given the sub ject’s current estimate of the visual and proprioceptive
t and ˆrp
disturbance, ˆrv
t , the visual and proprioceptive estimates of hand position are given
by

ˆy v
t = vt − ˆrv
t ,
t = pt − ˆrp
ˆyp
t
respectively. These distinct estimates of hand position are combined via maximum likelihood
estimation [7] into a single fused estimate of hand position.The maximum likelihood estimate
(MLE) of the true hand position yt is given by

(6)
(7)

ˆyM LE
t

=

σ2
p
v + σ2
σ2
p

ˆy v
t +

σ2
v
v + σ2
σ2
p

ˆyp
t .

(8)

rv
t

rv
t+1

rp
t

ry
t

ut

yt

rp
t+1

ry
t+1

ut+1

yt+1

vt

pt

vt+1

pt+1

Figure 3: Bayesian com-
bined sensory and motor
adaptation model.
The
sub ject assumes that dis-
turbances vary randomly,
but smoothly, from trial to
trial.

The MLE-based sensory adaptation model states that sub jects adapt their future visual and
proprioceptive estimates of hand location towards the MLE in such a way that the MLE
itself remains unchanged. The corresponding updates are given by
t + ηwp [ ˆyp
t − ˆy v
t+1 = ˆrv
ˆrv
t ] ,
ˆrp
t+1 = ˆrp
t − ˆyp
t + ηwv [ ˆy v
t ] ,
where η is some ﬁxed adaptation rate. This adaptation principle can be interpreted as an
online expectation-maximization (EM) procedure in the graphical model shown in Figure
2. In this model, rv and rp are treated as parameters of the model. The E-step of the EM
procedure corresponds to ﬁnding the MLE of yt and the M-step corresponds to gradient
ascent on the likelihood of ˆrv and ˆrp .

(10)

(9)

3.1 Extending the MLE model to account for motor component of adaptation

As it stands, the MLE-based model described above only accounts for sensory adaptation
and does not provide a complete description of sensorimotor adaptation. Visual adaptation
will aﬀect the estimated location of a visual target, and therefore also the planned movement,
but the eﬀect on performance will not be enough to account for complete (or nearly complete)
adaptation. The performance gain from this component of adaptation will be equal to the
discrepancy between the initial visual setimate of hand posion and the MLE - which will be
substantially less than the experimentally imposed shift.

This sensory adaptation model can, however, be plausibly combined with a conventional
error-driven state space model [6, 1] of motor adaptation to yield an additional motor
component of adaptation ˆry
t . The hand position MLE ˆyt can be used in place of the usual
uni-modal observation assumed in these models when calculating the endpoint error. The
resulting update for the estimated motor disturbance ˆry
t on trial t is given by

t+1 = ˆry
ˆry
t − ˆyM LE
t + γ ( ˆy∗
t
t = (v∗ − ˆrv
where ˆy∗
t ) is the estimated desired hand location, and γ is some ﬁxed adaptation
rate.

(11)

),

This combined model reﬂects the view that sensory and motor adaptation are distinct
processes. The sensory adaptation component is driven purely by discrepancy between the
senses, while the motor adaptation component only has access to a single, fused estimate of
hand position and is driven purely by estimated performance error.

4 Uniﬁed Bayesian sensory and motor adapatation model

We propose an alternative approach to solving the sensorimotor adaptation problem. Rather
than treat the visual shifts rv and rp as parameters, we consider all the disturbances (in-
cluding ry
t ) as dynamic random variables. We assume that the sub ject’s beliefs about how

30

20

10

o
/
r
o
r
r
E
 
l
a
n
o
i
t
c
e
r
i
D

0

 
0

 

Data
Bayesian Model
MLE Model

5

10

15
20
Trial Number

25

30

Figure 4: Model comparison with visuomo-
tor adaptation data. The Bayesian model
(solid blue line) and MLE-based model
(dashed red line) were ﬁtted to performance
data (ﬁlled circles) from a visuomotor adap-
tation experiment [4]. Both models made
qualitatively similar predictions about how
adaptation was distributed across compo-
nents.

these disturbances evolve over time are characterised by a trial-to-trial disturbance dynamics
model given by

(12)
rt+1 = Art + η t ,
where A is some diagonal matrix and η t is a random drift term with zero mean and diagonal
covariance matrix Q, i.e.

η t ∼ N (0, Q).
(13)
A and Q are both diagonal to reﬂect the fact that each disturbance evolves independently.
We denote the diagonal elements of A by a = (av , ap , au ) and the diagonal of Q by q =
(qv , qp , qu ). The vector a describes the timescales over which each disturbance persists,
while q describes the amount of random variation from trial to trial, or volatility of each
disturbance. These parameters reﬂect the statistics of the usual ﬂuctuations in sensory
calibration errors and motor plant dynamics, which the sensorimotor system must adapt to
on an ongoing basis. (Similar assumptions have previously been made elsewhere [3, 4]).

Combining these assumptions with the statistical model of each individual trial described
in Section 2 (and Figure 1), gives rise to a dynamical model of the disturbances and their
impact on reaching movements, across all trials. This model, representing the sub jects
beliefs about how his sensorimotor performance is liable to vary over time, is illustrated in
Figure 4. We propose that the patterns of adaptation and the sensory aftereﬀects exhibited
by sub jects correspond to optimal inference of the disturbances rt within this model, given
the observations on each trial.

The linear dynamics and Gaussian noise of the observer’s model mean that exact inference is
straightforward and equivalent to a Kalman ﬁlter. The latent state tracked by the Kalman
t , rp
t , ry
t )T , with state dynamics given by (12). The
ﬁlter is the vector of disturbances rt = (rv
observations vt and pt are related to the disturbances via
1 (cid:19) (rt + ǫt ) ,
ut (cid:19) + (cid:18) 1 0
pt (cid:19) = (cid:18) ut
(cid:18) vt
1
0 1
t , ǫp
t )T . We can write this in a more conventional form as
t , ǫu
where ǫt = (ǫv
zt = H rt + H ǫt ,
(15)
where zt = (vt − ut , pt − ut )T and H is the matrix of 1’s and 0’s in equation (14). The
observation noise covariance is given by
R = E (cid:2)(H ǫt )(H ǫt )T (cid:3) = (cid:18) σ2
u (cid:19) .
σ2
v + σ2
u
u
p + σ2
σ2
σ2
u
The standard Kalman ﬁlter update equations can be used to predict how a sub ject will
update estimates of the disturbances following each trial and therefore how he will select
his actions on the next trial, leading to a full prediction of performance from the ﬁrst trial
onwards.

(16)

(14)

5 Model comparison and experiments

We have described two alternative models of visuomotor adaptation which we have claimed
can account for both the motor and sensory components of adaptation. We ﬁtted both

(a)

y

x

Error

(b)

Target

Adapted
trajectory

Catch trial
trajectory

Start

Figure 5: (a) Experimental Setup, (b) Sample tra jectories and performance error measure

models to performance data from a visuomotor adaptation experiment [4] to validate this
claim. In this study in which this data was taken from, sub jects performed visually guided
reaching movements to a number of targets. Visual feedback of hand position (given via a
cursor on a screen) was rotated by 30o relative to the starting position of each movement.
The mean directional error (averaged over targets and over sub jects) over trials is plotted in
Figure 4. The Matlab function lsqnonlin was used to ﬁnd the parameters for each model
which minimized the sum of the error between the data and the predictions of each model.
p , σ2
v , σ2
There were 5 free parameters for the MLE-based model (σ2
u , η , γ ). For the Bayesian
model we assumed that all disturbances had the same timescale, i.e. all elements of a were
u , qv , qp , qu , a). The results of the ﬁts are shown
the same, leaving 7 free parameters (σ2
v , σ2
p , σ2
in Figure 4. The spread of adaptation across components of the model was qualitatively
similar between the two models, although no data on perceptual aftereﬀects was available
from this study for quantitative comparison. The Bayesian model clearly displays a closer ﬁt
to the data and the Akaike information criterion (AIC) conﬁrmed that this was not simply
due to extra parameters (AI C = 126.7 for the Bayesian model vs AI C = 159.6 for the
MLE-based model).

Although the Bayesian model appears to describe the data better, this analysis is by no
means conclusive. Furthermore, the similar scope of predictions between the two models
means that gathering additional data from alignment tests may not provide any further
leverage to distinguish between the two models. There is, however, a more striking diﬀerence
in predictions between the two models. While the MLE-based model predicts there will be
sensory adaptation only when there is a discrepancy between the senses, the Bayesian model
predicts that there will also be sensory adaptation in response to a motor disturbance such
as an external force applied to the hand). Just as a purely visual disturbance can lead
to a multifaceted adaptive response, so can a purely motor disturbance, with both motor
and sensory components predicted, even though there is never any discrepancy between the
senses. This prediction enables us to distinguish decisively between the two models.

5.1 Experimental Methods

We experimentally tested the hypothesis that force ﬁeld adaptation would lead to sensory
adaptation. We tested 11 sub jects who performed a series of trials consisting of reaching
movements interleaved with perceptual alignment tests.

Sub jects grasped the handle of a robotic manipulandum with their right hand. The hand
was not visible directly, but a cursor displayed via a mirror/ﬂat screen monitor setup (Fig-
ure 5.1(a)) was exactly co-planar and aligned with the handle of the manipulandum. In
the movement phase, sub jects made an out-and-back reaching movement towards a visual
target with their right hand. In the visual localization phase, a visual target was displayed
pseudorandomly in one of 5 positions and the sub jects moved their left ﬁngertip to the
perceived location of the target. In the proprioceptive localization phase, the right hand
was passively moved to a random target location, with no visual cue of its position, and
sub jects moved their left ﬁngertip to the perceived location of the right hand. Left ﬁngertip

Mean Localization Error − x

Mean Localization Error − y

m
c
 
/
 
r
o
r
r
E
 
n
a
e
M

4

3.5

3

2.5

2

1.5

1

0.5

0

−0.5

−1

 

 

Pre−Adaptation
Post−Adaptation

Vision

Proprioception

Modality

m
c
 
/
 
r
o
r
r
E
 
n
a
e
M

11

10.5

10

9.5

9

8.5

8

7.5

7

6.5

6

 

 

Pre−Adaptation
Post−Adaptation

Vision

Proprioception

Modality

Figure 6: (a) Average lateral (in direction of the perturbation) localization error across
sub jects before vs after adaptation, for vision and proprioception. Error bars indicate
standard errors. (b) Same plots for y -direction

positions were recorded using a Polhemus motion tracker. Neither hand was directly visible
at any time during the experiment.

Sub jects were given 25 baseline trials with zero external force, after which a force ﬁeld was
gradually introduced. A leftward lateral force Fx was applied to the right hand during the
reaching phase. The magnitude of the force was proportional to the forward velocity ˙y of
the hand, i.e.

Fx = −a ˙y.

(17)

The force was applied only on the outward part of the movement (i.e. only when ˙y > 0).
After steadily incrementing a during 50 adaptation trials, the force ﬁeld was then kept
constant at a = 0.3 N/(cms−1) for a further 25 post-adaptation test trials. All sub jects
received a catch trial at the very end in which the force ﬁeld was turned oﬀ.

The particular force ﬁeld used was chosen so that the cursor tra jectories (and motor com-
mands required to counter the perturbation) would be as close as possible to those used
to generate the linear tra jectories required when exposed to a visuomotor shift (such as
that described in [7]). Figure 5.1(b) shows two tra jectories from a typical sub ject, one from
the post-adaptation test phase and one from the catch trial after adaptation. The initial
outward part of the catch trial tra jectory, the initial movement is very straight, implying
that similar motor commands were used to those required by a visuomotor shift.

5.2 Results

We compared the average performance in the visual and proprioceptive alignment tests
before and after adaptation in the velocity-dependent force ﬁeld. The results are summarized
in Figure 6(a). Most sub jects exhibited small but signiﬁcant shifts in performance in both
the visual and proprioceptive alignment tests. Two sub jects exhibited shifts which were
more than two standard deviations away from the average shift and were excluded from the
analysis. We found signiﬁcant lateral shifts in both visual and proprioceptive localization
error in the direction of the perturbation (both p < .05, one-tailed paired t-test). Figure
6(b) shows the same data for the direction perpendicular to the perturbation. Although the
initial localization bias was high, there was no signiﬁcant shift in this direction following
adaptation.

We quantiﬁed each sub ject’s performance on each trial as the perpendicular distance of the
furthest point in the tra jectory from the straight line between the starting point and the
target (Fig. 5.1(b)). We ﬁtted the Bayesian and MLE-based models to the data following the
same procedure as before, only this time penalizing the disagreement between the model
and the data for the alignment tests, in addition to the reaching performance. Figure 7
illustrates the averaged data along with the model ﬁts. Both models were able to account
reasonably well for the trends in reaching performance across trials (7(a)). Figures 7(b) and
7(c) show the model ﬁts for the perceptual localization task. The Bayesian model is able to
account for both the extent of the shift and the timecourse of this shift during adaptation.

m
c
 
/
 
r
o
r
r
E
 
e
c
n
a
m
r
o
f
r
e
P

3

2

1

0

−1

−2

−3

 
0

(a)   Reaching Performance

 

Data
Bayesian Model
MLE Model

m
c
 
/
 
r
o
r
r
E
 
t
n
e
m
n
g
i
l
A

4

2

0

−2

(b)   Visual Alignment

(c)   Proprioceptive Alignment

m
c
 
/
 
r
o
r
r
E
 
t
n
e
m
n
g
i
l
A

4

2

0

−2

20

40
60
Trial Number

80

100

0

20

40
60
Trial Number

80

100

0

20

40
60
Trial Number

80

100

Figure 7: Trial-by-trial data and model ﬁts. (a) Reaching error, (b) Visual alignment test
error, (c) Proprioceptive alignment test error. The Bayesian (solid blue lines) and MLE-
based (dashed red lines) were ﬁtted to averaged data across sub jects (circles).

Since there was never any sensory discrepancy, the MLE-based model predicted no change
in the localization task.

6 Conclusions and discussion

Our experimental results demonstrate that adaptation of reaching movements in a force
ﬁeld results in shifts in visual and proprioceptive spatial perception. This novel ﬁnding
strongly supports the Bayesian model, which predicted such adaptation, and refutes the
MLE-based model, which did not. The Bayesian model was able to account for the trends
in both reaching performance and alignment test errors on a trial-to-trial basis.

Several recent models have similarly described motor adaptation as a process of Bayesian
inference of the potential causes of observed error. K¨ording et al. [3] proposed a model of
saccade adaptation and Krakauer et al. [4] modelled visuomotor adaptation based on this
principle. Our work extends the framework of these models to include multiple observation
modalities instead of just one, and multiple classes of disturbances which aﬀect the diﬀerent
observation modalities in diﬀerent, experimentally measurable ways.

Overall, our results suggest that the nervous system solves the problems of sensory and
motor adaptation in a principled and uniﬁed manner, supporting the view that sensorimotor
adaptation proceeds according to optimal estimation of encountered disturbances.

References

[1] Opher Donchin, Joseph T Francis, and Reza Shadmehr. Quantifying generalization from
trial-by-trial behavior of adaptive systems that learn with basis functions: theory and
experiments in human motor control. J Neurosci, 23(27):9032–9045, Oct 2003.
[2] Z. Ghahramani, D.M. Wolpert, and M.I. Jordan. Computational models for sensorimotor
integration. In P.G. Morasso and V. Sanguineti, editors, Self-Organization, Computa-
tional Maps and Motor Control, pages 117–147. North-Holland, Amsterdam, 1997.
[3] Konrad P. K¨ording, Joshua B. Tenenbaum, and Reza Shadmehr. The dynamics of
memory as a consequence of optimal adaptation to a changing body. Nat Neurosci,
10(6):779–786, June 2007.
[4] John W Krakauer, Pietro Mazzoni, Ali Ghazizadeh, Roshni Ravindran, and Reza Shad-
mehr. Generalization of motor learning depends on the history of prior action. PLoS
Biol, 4(10):e316, Sep 2006.
[5] M.C. Simani, L.M. McGuire, and P.N. Sabes. Visual-shift adaptation is composed of
separable sensory and task-dependent eﬀects. J Neurophysiol, 98:2827–2841, Nov 2007.
[6] K A Thoroughman and R Shadmehr. Learning of action through adaptive combination
of motor primitives. Nature, 407(6805):742–747, Oct 2000.
[7] Robert J van Beers, Daniel M Wolpert, and Patrick Haggard. When feeling is more
important than seeing in sensorimotor adaptation. Curr Biol, 12(10):834–837, May
2002.


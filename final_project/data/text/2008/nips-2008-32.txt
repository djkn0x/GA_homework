Linear Classi ﬁcation and Selective Sampling
Under Low Noise Conditions

Giovanni Cavallanti
DSI, Universit `a degli Studi di Milano, Italy
cavallanti@dsi.unimi.it

Nicol `o Cesa-Bianchi
DSI, Universit `a degli Studi di Milano, Italy
cesa-bianchi@dsi.unimi.it

Claudio Gentile
DICOM, Universit `a dell’Insubria, Italy
claudio.gentile@uninsubria.it

Abstract
We provide a new analysis of an efﬁcient margin-based algori
thm for selective
sampling in classi ﬁcation problems. Using the so-called Ts ybakov low noise con-
dition to parametrize the instance distribution, we show bounds on the conver-
gence rate to the Bayes risk of both the fully supervised and the selective sampling
versions of the basic algorithm. Our analysis reveals that, excluding logarithmic
factors, the average risk of the selective sampler converges to the Bayes risk at
rate N −(1+α)(2+α)/2(3+α) where N denotes the number of queried labels, and
α > 0 is the exponent in the low noise condition. For all α > √3 − 1 ≈ 0.73 this
convergence rate is asymptotically faster than the rate N −(1+α)/(2+α) achieved
by the fully supervised version of the same classi ﬁer, which queries all labels, and
for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data
reveal that simple variants of the proposed selective sampler perform much better
than popular and similarly efﬁcient competitors.

1
Introduction
In the standard online learning protocol for binary classi ﬁ cation the learner receives a sequence of
instances generated by an unknown source. Each time a new instance is received the learner predicts
its binary label, and is then given the true label of the current instance before the next instance is
observed. This protocol is natural in many applications, for instance weather forecasting or stock
market prediction, because Nature (or the market) is spontaneously disclosing the true label after
each learner’s guess. On the other hand, in many other applications obtaining labels may be an
expensive process.
In order to address this problem, a variant of online learning that has been
proposed is selective sampling. In this modi ﬁed protocol th e true label of the current instance is
never revealed unless the learner decides to issue an explicit query. The learner’s performance is then
measured with respect to both the number of mistakes (made on the entire sequence of instances)
and the number of queries. A natural sampling strategy is one that tries to identify labels which are
likely to be useful to the algorithm, and then queries those ones only. This strategy somehow needs
to combine a measure of utility of examples with a measure of conﬁdence. In the case of learning
with linear functions, a statistic that has often been used to quantify both utility and conﬁdence is
the margin. In [10] this approach was employed to deﬁne a sele ctive sampling rule that queries a
new label whenever the margin of the current instance, with respect to the current linear hypothesis,
is smaller (in magnitude) than an adaptively adjusted threshold. Margins were computed using
a linear learning algorithm based on an incremental version of Regularized linear Least-Squares
(RLS) for classi ﬁcation. Although this selective sampling algorithm is efﬁcient, and has simple
variants working quite well in practice, the rate of convergence to the Bayes risk was never assessed
in terms of natural distributional parameters, thus preventing a full understanding of the properties
of this algorithm.

We improve on those results in several ways making three main contributions: (i) By coupling the
Tsybakov low noise condition, used to parametrize the instance distribution, with the linear model
of [10], deﬁning the conditional distribution of labels, we prove that the fully supervised RLS (all
labels are queried) converges to the Bayes risk at rate eO(cid:0)n−(1+α)/(2+α) (cid:1) where α ≥ 0 is the noise
exponent in the low noise condition. (ii) Under the same low noise condition, we prove that the
RLS-based selective sampling rule of [10] converges to the Bayes risk at rate eO(cid:0)n−(1+α)/(3+α) (cid:1),
with labels being queried at rate eO(cid:0)n−α/(2+α) (cid:1). Moreover, we show that similar results can be
established for a mistake-driven (i.e., space and time efﬁc ient) variant. (iii) We perform experiments
on a real-world medium-size dataset showing that variants of our mistake-driven sampler compare
favorably with other selective samplers proposed in the literature, like the ones in [11, 16, 20].
Related work. Selective sampling, originally introduced by Cohn, Atlas and Ladner in [13, 14],
differs from the active learning framework as in the latter the learner has more freedom in selecting
which instances to query. For example, in Angluin’s adversarial learning with queries (see [1] for a
survey), the goal is to identify an unknown boolean function f from a given class, and the learner
can query the labels (i.e., values of f ) of arbitrary boolean instances. Castro and Nowak [9] study a
framework in which the learner also queries arbitrary domain points. However, in their case labels
are stochastically related to instances (which are real vectors). They prove risk bounds in terms
of nonparametric characterizations of both the regularity of the Bayes decision boundary and the
behavior of the noise rate in its proximity. In fact, a large statistical literature on adaptive sampling
and sequential hypothesis testing exists (see for instance the detailed description in [9]) which is
concerned with problems that share similarities with active learning. The idea of querying small
margin instances when learning linear classi ﬁers has been e xplored several times in different active
learning contexts. Campbell, Cristianini and Smola [8], and also Tong and Koller [23], study a pool-
based model of active learning, where the algorithm is allowed to interactively choose which labels
to obtain from an i.i.d. pool of unlabeled instances. A landmark result in the selective sampling
protocol is the query-by-committee algorithm of Freund, Seung, Shamir and Tishby [17]. In the
realizable (noise-free) case, and under strong distributional assumptions, this algorithm is shown to
require exponentially fewer labels than instances when learning linear classi ﬁers (see also [18] for
a more practical implementation). An exponential advantage in the realizable case is also obtained
with a simple variant of the Perceptron algorithm by Dasgupta, Kalai and Monteleoni [16], under
the sole assumption that instances are drawn from the uniform distribution over the unit ball in Rd .
In the general statistical learning case, under no assumptions on the joint distribution of label and
instances, selective sampling bears no such exponential advantage. For instance, K ¨a ¨ari ¨ainen shows
that, in order to approach the risk of the best linear classi ﬁ er f ∗ within error ε, at least Ω((η/ε)2 )
labels are needed, where η is the risk of f ∗ . A much more general nonparametric lower bound for
active learning is obtained by Castro and Nowak [9]. General selective sampling strategies for the
nonrealizable case have been proposed in [3, 4, 15]. However, none of these learning algorithms
seems to be computationally efﬁcient when learning linear c lassi ﬁers in the general agnostic case.

2 Learning protocol and data model
We consider the following online selective sampling protocol. At each step t = 1, 2, . . . the sam-
pling algorithm (or selective sampler) receives an instance xt ∈ Rd and outputs a binary prediction
for the associated label yt ∈ {−1, +1}. After each prediction, the algorithm has the option of “sam -
pling” (issuing a query) in order to receive the label yt . We call the pair (xt , yt ) an example. After
seeing the label yt , the algorithm can choose whether or not to update its internal state using the new
information encoded by (xt , yt ).
We assume instances xt are realizations of i.i.d. random variables X t drawn from an unknown
distribution on the surface of the unit Euclidean sphere in Rd , so that kX tk = 1 for all t ≥ 1.
Following [10], we assume that labels yt are generated according to the following simple linear
noise model: there exists a ﬁxed and unknown vector u ∈ Rd , with Euclidean norm kuk = 1,
such that E(cid:2)Yt (cid:12)(cid:12) X t = xt (cid:3) = u⊤xt for all t ≥ 1. Hence X t = xt has label 1 with probability
(1 + u⊤xt )/2 ∈ [0, 1]. Note that SGN(f ∗ ), for f ∗ (x) = u⊤x, is the Bayes optimal classi ﬁer
for this noise model. In the following, all probabilities P and expectations E are understood with
respect to the joint distribution of the i.i.d. data process {(X 1 , Y1 ), (X 2 , Y2 ), . . . }. We use Pt
to denote conditioning on (X 1 , Y1 ), . . . , (X t , Yt ). Let f : Rd → R be an arbitrary measurable
function. The instantaneous regret R(f ) is the excess risk of SGN(f ) w.r.t.
the Bayes risk, i.e.,
R(f ) = P(Y1 f (X 1 ) < 0) − P(Y1 f ∗ (X 1 ) < 0). Let f1 , f2 , . . . be a sequence of real functions

where each ft is measurable w.r.t. the σ -algebra generated by (X 1 , Y1 ), . . . , (X t−1 , Yt−1 ), X t .
When (X 1 , Y1 ), . . . , (X t−1 , Yt−1 ) is understood from the context, we write ft as a function of
X t only. Let Rt−1 (ft ) be the instantaneous conditional regret Rt−1 (ft ) = Pt−1 (Yt ft (X t ) <
0) − Pt−1 (Yt f ∗ (X t ) < 0). Our goal is to bound the expected cumulative regret E(cid:2)R0 (f1 ) +
R1 (f2 ) + · · · + Rn−1 (fn )(cid:3), as a function of n, and other relevant quantities. Observe that, although
the learner’s predictions can only depend on the queried examples, the regret is computed over all
time steps, including the ones when the selective sampler did not issue a query. In order to model
the distribution of the instances around the hyperplane u⊤x = 0, we use Mammen-Tsybakov low
noise condition [24]:
There exist c > 0 and α ≥ 0 such that P(cid:0)|f ∗ (X 1 )| < ε(cid:1) ≤ c εα
for all ε > 0.
(1)
In order to study
When the noise exponent α is 0 the low noise condition becomes vacuous.
the case α → ∞, one can use the following equivalent formulation of (1) —see
, e.g., [5],
P(cid:0)f ∗ (X 1 )f (X 1 ) < 0(cid:1) ≤ c R(f )α/(1+α) for all measurable f : Rd → R. With this formula-
tion, one can show that α → ∞ implies the hardmargin condition |f ∗ (X 1 )| ≥ 1/(2c) w.p. 1.
3 Algorithms and theoretical analysis
t X t ), where wt ∈ Rd is
We consider linear classi ﬁers predicting the value of Yt through SGN(w⊤
a dynamically updated weight vector which might be intended as the current estimate for u. Our
wt is an RLS estimator deﬁned over the set of previously queried examples. More precisely, let Nt
Nt−1 (cid:3) be the
t time steps, St−1 = (cid:2) x′
be the number of queried examples during the ﬁrst
1 , . . . , x′
Nt−1 (cid:3)⊤ be the vector of the
matrix of the queried instances up to time t − 1, and y t−1 = (cid:2)y ′
1 , . . . , y ′
corresponding labels. Then the RLS estimator is deﬁned by
t (cid:1)−1
wt = (cid:0)I + St−1 S⊤
(2)
t−1 + xtx⊤
St−1 y t−1 ,
where I is the d × d identity matrix. Note that wt depends on the current instance xt . The RLS
estimator in this particular form has been ﬁrst considered b y Vovk [25] and by Azoury and War-
muth [2]. Compared to standard RLS, here xt acts by futher reducing the variance of wt . We
use b∆t to denote the margin w⊤
t X t whenever wt is understood from the context. Thus b∆t is
the current approximation to ∆t . Note that b∆t is measurable w.r.t. the σ -algebra generated by
(X 1 , Y1 ), . . . , (X t−1 , Yt−1 ), X t . We also use ∆t to denote the Bayes margin f ∗ (X t ) = u⊤X t .
The RLS estimator (2) can be stored in space Θ(d2 ), which we need for the inverse of I +
t . Moreover, using a standard formula for small-rank adjustments of inverse
St−1 S⊤
t−1 + xtx⊤
matrices, we can compute updates and predictions in time Θ(d2 ). The algorithm in (2) can also
be expressed in dual variable form. This is needed, for instance, when we want to use the feature
expansion facility provided by kernel functions. In this case, at time t the RLS estimator (2) can be
t−1 ) space. The update time is also quadratic in Nt−1 .
represented in O(N 2
Our ﬁrst result establishes a regret bound for the fully supe rvised algorithm, i.e., the algorithm that
predicts using RLS as in (2), queries the label of every instance, and stores all examples. This result
is the baseline against which we measure the performance of our selective sampling algorithm. The
regret bound is expressed i.t.o. the whole spectrum of the process covariance matrix E[X 1X ⊤
1 ].
Theorem 1 Assume the low noise condition (1) holds with exponent α ≥ 0 and constant c > 0.
Then the expected cumulative regret after n steps of the fully supervised algorithm based on (2) is
bounded by E (cid:20)(cid:16)4c(1 + ln |I + SnS⊤
2+α (cid:21) n
n |)(cid:17) 1+α
1
2+α . This, in turn, is bounded from above by
2+α (cid:17) . Here | · | denotes the determi-
2+α = O (cid:16)(cid:0)d ln n(cid:1) 1+α
i=1 ln(1 + nλi )(cid:17)(cid:17) 1+α
(cid:16)4c(cid:16)1 + Pd
1
1
2+α
n
2+α n
nant of a matrix, Sn = (cid:2)X 1 , X 2 , . . . , X n (cid:3), and λi is the i-th eigenvalue of E[X 1X ⊤
1 ].
When α = 0 (corresponding to a vacuous noise condition) the bound of Theorem 1 reduces to
O(cid:0)√d n ln n(cid:1). When α → ∞ (corresponding to a hard margin condition) the bound gives the
logarithmic behavior O(cid:0)d ln n(cid:1). Notice that Pd
i=1 ln(1 + nλi ) is substantially smaller than d ln n
whenever the spectrum of E[X 1X ⊤
1 ] is rapidly decreasing. In fact, the second bound is clearly
meaningful even when d = ∞, while the third one only applies to the ﬁnite dimensional ca se.

Parameters: λ > 0, ρt > 0 for each t ≥ 1.
Initialization: weight vector w = (0, . . . , 0)⊤ ; storage counter N = 0.
At each time t = 1, 2, . . . do the following:
1. Observe instance xt ∈ Rd : ||xt || = 1;
2. Predict the label yt ∈ {−1, 1} with SGN(w⊤
t xt ), where wt is as in (2).
3. If N ≤ ρt then query label yt and store (xt , yt );
4. Else if b∆2
t ≤ 128 ln t
λ N then schedule the query of yt+1 ;
5. If (xt , yt ) is scheduled to be stored, then increment N and update wt using (xt+1 , yt+1 ).
Figure 1: The selective sampling algorithm.

Fast rates of convergence have typically been proven for batch-style algorithms, such as empirical
risk minimizers and SVM (see, e.g., [24, 22]), rather than for online algorithms. A reference closer
to our paper is Ying and Zhou [26], where the authors prove bounds for online linear classi ﬁcation
using the low noise condition (1), though under different distributional assumptions.
Our second result establishes a new regret bound, under low noise conditions, for the selective
sampler introduced in [10]. This variant, described in Figure 1, queries all labels (and stores all
examples) during an initial stage of length at least (16d)/λ2 , where λ denotes the smallest nonzero
eigenvalue of the process covariance matrix E[X 1X ⊤
1 ]. When this transient regime is over, the
sampler issues a query at time t based on both the query counter Nt−1 and the margin b∆t . Specif-
ically, if evidence is collected that the number Nt−1 of stored examples is smaller than our current
t , that is if b∆2
t ≤ (128 ln t)/(λNt−1 ), then we query (and store) the label of the next
estimate of 1/∆2
instance xt+1 . Note that the margin threshold explicitly depends, through λ, on additional informa-
tion about the data-generating process. This additional information is needed because, unlike the
fully supervised classi ﬁer of Theorem 1, the selective samp ler queries labels at random steps. This
prevents us from bounding the sum of conditional variances of the involved RLS estimator through
n (cid:12)(cid:12), as we can do when proving Theorem 1 (see below). Instead, we have to individu-
ln(cid:12)(cid:12)I + Sn S⊤
ally bound each conditional variance term via the smallest empirical eigenvalue of the correlation
matrix. The transient regime in Figure 1 is exactly needed to ensure that this smallest empirical
eigenvalue gets close enough to λ. Compared to the analysis contained in [10], we are able to better
capture the two main aspects of the selective sampling protocol: First, we control the probability
of making a mistake when we do not query labels; second, the algorithm is able to adaptively op-
timize the sampling rate by exploiting the additional information provided by the examples having
small margin. The appropriate sampling rate clearly depends on the (unknown) amount of noise α
which the algorithm implicitly learns on the ﬂy. In this resp ect, our algorithm is more properly an
adaptive sampler, rather than a selective sampler. Finally, we stress that it is fairly straightforward
to add to the algorithm in Figure 1 a mistake-driven rule for storing examples. Such a rule provides
that, when a small margin is detected, a query be issued (and the next example be stored) only if
SGN( b∆t ) 6= yt (i.e., only if the current prediction is mistaken). This turns out to be highly advanta-
geous from a computational standpoint, because of the sparsity of the computed solution. It is easy
to adapt our analysis to obtain even for this algorithm the same regret bound as the one established
in Theorem 2. However, in this case we can only give guarantees on the expected number of stored
examples (which can indeed be much smaller than the actual number of queried labels).

Theorem 2 Assume the low noise condition (1) holds with unknown exponent α ≥ 0 and assume
the selective sampler of Figure 1 is run with ρt = 16
λ2 max{d, ln t}. Then, after n steps, the expected
cumulative regret is bounded by O (cid:18) d + ln n
3+α (cid:19) whereas the expected number
λ (cid:17) 1+α
λ2 + (cid:16) ln n
2
3+α
n
2+α (cid:19) .
of queried labels (including the stored ones) is bounded by O (cid:18) d + ln n
λ (cid:17) α
λ2 + (cid:16) ln n
2
2+α
n
The proof, sketched below, hinges on showing that b∆t is an almost unbiased estimate of the true
margin ∆t , and relies on known concentration properties of i.i.d. processes. In particular, we show
that our selective sampler is able to adaptively estimate the number of queries needed to ensure a
1/t increase of the regret when a query is not issued at time t.

As expected, when we compare our semi-supervised selective sampler (Theorem 2) to the fully
supervised “yardstick” (Theorem 1), we see that the per-ste p regret of the former vanishes at a sig-
3+α vs. n− 1+α
ni ﬁcantly slower rate than the latter, i.e., n− 1+α
2+α . Note, however, that the per-step regret
of the semi-supervised algorithm vanishes faster than its fully-supervised counterpart when both re-
grets are expressed in terms of the number N of issued queries. To see this consider ﬁrst the case
α → ∞ (the hard margin case, essentially analyzed in [10]). Then both algorithms have a per-step
regret of order (ln n)/n. However, since the semi-supervised algorithm makes only N = O(ln n)
queries, we have that, as a function of N , the per-step regret of the semi-supervised algorithm is of
order N/eN where the fully supervised has only (ln N )/N . We have thus recovered the exponen-
tial advantage observed in previous works [16, 17]. When α = 0 (vacuous noise conditions), the
per-step regret rates in terms of N become (excluding logarithmic factors) of order N −1/3 in the
semi-supervised case and of order N −1/2 in the fully supervised case. Hence, there is a critical value
of α where the semi-supervised bound becomes better. In order to ﬁnd this critical value we write the
rates of the per-step regret for 0 ≤ α < ∞ obtaining N − (1+α)(2+α)
(semi-supervised algorithm) and
2(3+α)
N − 1+α
2+α (fully supervised algorithm). By comparing the two exponents we ﬁnd that, asymptotically,
the semi-supervised rate is better than the fully supervised one for all values of α > √3 − 1. This
indicates that selective sampling is advantageous when the noise level (as modeled by the Mammen-
Tsybakov condition) is not too high. Finally, observe that the way it is stated now, the bound of
Theorem 2 only applies to the ﬁnite-dimensional ( d < ∞) case. It turns out this is a ﬁxable artifact
of our analysis, rather than an intrinsic limitation of the selective sampling scheme in Figure 1. See
Remark 3 below.
Proof of Theorem 1. The proof proceeds by relating the classi ﬁcation regret to t he square loss re-
gret via a comparison theorem. The square loss regret is then controlled by applying a known point-
wise bound. For all measurable f : Rd → R, let Rφ (f ) = E[(cid:0)1 − Y1 f (X 1 )(cid:1)2
− (cid:0)1 − Y1 f ∗ (X 1 )2 (cid:1)]
be the square loss regret, and Rt−1,φ its conditional version. We apply the comparison theo-
rem from [5] with the ψ -transform function ψ(z ) = z 2 associated with the square loss. Under
the low noise condition (1) this yields R(f ) ≤ (cid:0)4c Rφ (f )(cid:1) 1+α
2+α for all measurable f . We thus
2+α i≤ Ehn(cid:16) 4c
2+α i ,
t=1 Rt−1 (ft )(cid:3)≤ EhPn
t=1(cid:16)4c Rφ,t−1 (ft )(cid:17) 1+α
t=1 Rφ,t−1 (ft )(cid:17) 1+α
have E(cid:2)Pn
n Pn
the last term following from Jensen’s inequality. Further, we observe that in our probabilistic model
f ∗ (x) = u⊤x is Bayes optimal for the square loss. In fact, for any unit norm x ∈ Rd , we have
f ∗ (x) = arginf z∈R(cid:16)(1 − z )2 1+u
(cid:17)= u⊤x . Hence Pn
⊤
⊤
2 + (1 + z )2 1−u
t=1 Rφ,t−1 (ft ) =
x
x
2
t X t )2 − (Yt − u⊤X t )2 (cid:1) which, in turn, can be bounded pointwise (see, e.g., [12,
t=1 (cid:0)(Yt − w⊤
Pn
n (cid:12)(cid:12). Putting together gives the ﬁrst bound. Next, we take the
Theorem 11.8]) by 1 + ln(cid:12)(cid:12)I + Sn S⊤
1+α
bound just obtained and apply Jensen’s inequality twice, ﬁr st to the concave function (·)
2+α of a real
argument, and then to the concave function ln | · | of a (positive deﬁnite) matrix argument. Observing
n = E[Pn
t=1 X tX ⊤
t ] = n EX 1X ⊤
that ESnS⊤
1 yields the second bound. The third bound derives
from the second one just by using λi ≤ 1.
(cid:3)
Proof sketch of Theorem 2.
We aim at bounding from above the cumulative regret
t=1 (cid:16)P(Yt b∆t < 0) − P(Yt ∆t < 0)(cid:17) which, according to our probabilistic model, can be shown
Pn
to be at most c n ε1+α + Pn
P(∆t b∆t ≤ 0, |∆t | ≥ ε) . The last sum is upper bounded by
t=1
P (cid:18) b∆2
, Nt−1 > ρt , |∆t | ≥ ε(cid:19)
nXt=1
nXt=1
128 ln t
+
t ≤
P (Nt−1 ≤ ρt )
λNt−1
{z
}
{z
}
|
|
(I)
(II)
, Nt−1 > ρt(cid:19)
P (cid:18)∆t b∆t ≤ 0, b∆2
nXt=1
128 ln t
t >
+
λNt−1
|
{z
}
(III)
where: (I) are the initial time steps; (II) are the time steps on which we trigger the query of the next
label (because b∆2
t is smaller than the threshold at time t); (III) are the steps that do not trigger any
queries at all.

.

Note that (III) bounds the regret over non-sampled examples. In what follows, we sketch the way
we bound each of the three terms separately. A bound on (I) is easily obtained as (I) ≤ ρn =
O( d+ln n
) just because ρn ≥ ρt for all t ≤ n. To bound (II) and (III) we need to exploit the fact that
λ2
the subsequence of stored instances and labels is a sequence of i.i.d. random variables distributed
as (X 1 , Y1 ), see [10]. This allows us to carry out a (somewhat involved) bias-variance analysis
showing that for any ﬁxed number Nt−1 = s of stored examples, b∆t is an almost unbiased estimator
of ∆t , whose bias and variance tend to vanish as 1/s when s is sufﬁciently large. In particular, if
|∆t | ≥ ε then b∆t ≈ ∆t as long as Nt−1 is of the order of ln n
λ ε2 . The variance of b∆t is controlled
by known results (the one we used is [21, Theorem 4.2]) on the concentration of eigenvalues of
s Pi X iX ⊤
an empirical correlation matrix 1
to the eigenvalues of the process covariance matrix
i
E[X 1X ⊤
1 ]. For such a result to apply, we have to impose that Nt−1 ≥ ρt . By suitably combining
λ2 + ln n
these concentration results we can bound term (II) by O( d+ln n
λε2 ) and term (III) by O(ln n).
Putting together and choosing ε of the order of (cid:0) ln n
λ n (cid:1) 1+α
3+α gives the desired regret bound. The bound
on the number of queried labels is obtained in a similar way.
(cid:3)
Remark 3 The linear dependence on d in Theorem 2 derives from a direct application of the con-
centration results in [21].
In fact, it is possible to take into account in a fairly precise manner
the way the process spectrum decreases (e.g., [6, 7]), thereby extending the above analysis to the
inﬁnite-dimensional case. In this paper, however, we decid ed to stick to the simpler analysis leading
to Theorem 2, since the resulting bounds would be harder to read, and would somehow obscure
understanding of regret and sampling rate behavior as a function of n.

4 Experimental analysis
In evaluating the empirical performance of our selective sampling algorithm, we consider two addi-
tional variants obtained by slightly modifying Step 4 in Figure 1. The ﬁrst variant (which we just
call S S, Selective Sampler) queries the current label instead of the next one. The rationale here is that
we want to leverage the more informative content of small margin instances. The second variant is
a mistake-driven version (referred to as S SMD, Selective Sampling Mistake Driven) that queries the
current label (and stores the corresponding example) only if the label gets mispredicted. For clarity,
the algorithm in Figure 1 will then be called S SN L (Selective Sampling Next Label) since it queries
the next label whenever a small margin is observed. For all three algorithms we dropped the intial
transient regime (Step 3 in Figure 1).
We run our experiments on the ﬁrst, in chronological order,
40,000 newswire stories from the
Reuters Corpus Volume 1 dataset (RCV1). Every example in this dataset is encoded as a vector
of real attributes computed through a standard T F - ID F bag-of-words processing of the original news
stories, and is tagged with zero or more labels from a set of 102 classes. The online categorization
of excerpts from a newswire feed is a realistic learning problem for selective sampling algorithms
since a newswire feed consists of a large amount of uncategorized data with a high labeling cost. The
classi ﬁcation performance is measured using a macroaverag ed F -measure 2RP /(R + P ), where P
is the precision (fraction of correctly classi ﬁed document s among all documents that were classi ﬁed
positive for the given topic) and R is the recall (fraction of correctly classi ﬁed documents am ong all
documents that are labelled with the given topic). All algorithms presented here are evaluated using
dual variable implementations and linear kernels.
The results are summarized in Figures 2 and 3. The former only refers to (an average over) the 50
most frequent categories, while the latter includes them all. In Figure 2 (left) we show how S SMD
compares to S SN L , and to its most immediate counterpart, S S. In Figure 2 (right) we compare S SMD
to other algorithms that are known to have good empirical performance, including the second-order
version of the label efﬁcient classi ﬁer (
SO L E), as described in [11], and the DKM P ERC variant of
the DKM algorithm (see, e.g., [16, 20]). DKM P ERC differs from DKM since it adopts a standard
perceptron update rule. The perceptron algorithm (P ERC) and its second-order counterpart (SO P)
are reported here as a reference, since they are designed to query all labels. In particular, SO P is
a mistake-driven variant of the algorithm analyzed in Theorem 1. It is reasonable to assume that
in a selective sampling setup we are interested in the performance achieved when the fraction of
queried labels stays below some threshold, say 10%. In this range of sampling rate, S SMD has the
steepest increase in the achieved F -measure, and surpasses any other algorithm. Unsurprisingly, as
the number of queried labels gets larger, S SMD, SO L E and SO P exhibit similar behaviors. Moreover,
the less than ideal plot of S SN L seems to conﬁrm the intuition that querying small margin ins tances

 0.75
 0.7
 0.65
 0.6
 0.55
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
 0.01

 0.02

 0.03

 0.04
 0.07
 0.06
 0.05
Fraction of queried labels

 0.08

 0.09

 0.1

SSMD
SSNL
SS

e
r
u
s
a
e
m
-
F

 0.75
 0.7
 0.65
 0.6
 0.55
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
 0.01

SSMD
DKMperc
SOLE
SOP
PERC

 0.02

 0.03

 0.04
 0.07
 0.06
 0.05
Fraction of queried labels

 0.08

 0.09

 0.1

Figure 2: Average F -measure obtained by different algorithms after 40,000 examples, as a function
of the number of queried labels. The average only refers to the 50 most frequent categories. Points
are obtained by repeatedly running each algorithm with different values of parameters (in Figure
1, the relevant parameter is λ). Trend lines are computed as approximate cubic splines connecting
consecutive points.

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

Number of stored examples (normalized)
Norm of the SVM weight vector (normalized)

 20

 40

 60

 80

 100

Topics

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

F-measure
Fraction of positive examples
Fraction of queried labels

 20

 40

 60

 80

 100

Topics

Figure 3: Left: Correlation between the fraction of stored examples and the difﬁculty of each binary
task, as measured by the separation margin. Right: F -measure achieved on the different binary
classi ﬁcation tasks compared to the number of positive exam ples in each topic, and to the fraction of
queried labels (including the stored ones). In both plots, topics are sorted by decreasing frequency
of positive examples. The two plots are produced by S SMD with a speci ﬁc value of the λ parameter.
Varying λ does not signi ﬁcantly alter the reported trend.

provides a signi ﬁcant advantage. Under our test conditions DKM P ERC proved ineffective, probably
because most tasks in the RCV1 dataset are not linearly separable. A similar behavior was observed
in [20]. It is fair to remark that DKM P ERC is a perceptron-like linear-threshold classi ﬁer while the
other algorithms considered here are based on the more computationally intensive ridge regression-
like procedure.
In our selective sampling framework it is important to investigate how harder problems inﬂuence
the sampling rate of an algorithm and, for each binary problem, to assess the impact of the number
of positive examples on F-measure performance. Coarsely speaking, we would expect that the hard
topics are the infrequent ones. Here we focus on S SMD since it is reasonably the best candidate,
among our selective samplers, as applied to real-world problems. In Figure 3 (left) we report the
fraction of examples stored by S SMD on each of the 102 binary learning tasks (i.e., on each individual
topic, including the infrequent ones), and the corresponding levels of F -measure and queried labels
(right). Note that in both plots topics are sorted by frequency with the most frequent categories
appearing on the left. We represent the difﬁculty of a learni ng task by the norm of the weight vector
obtained by running the C - SVM algorithm on that task1 . Figure 3 (left) clearly shows that S SMD
rises the storage rate on difﬁcult problems. In particular,
even if two different tasks have largely
different numbers of positive examples, the storage rate achieved by S SMD on those tasks may be

1The actual values were computed using SVM - L IGH T [19] with default parameters. Since the examples in
the Reuters Corpus Volume 1 are cosine normalized, the choice of default parameters amounts to indirectly
setting the parameter C to approximately 1.0.

similar when the norm of the weight vectors computed by C - SVM is nearly the same. On the other
hand, the right plot shows (to our surprise) that the achieved F-measure is fairly independent of the
number of positive examples, but this independence is obtained at the cost of querying more and
more labels. In other words, S SMD seems to realize the difﬁculty of learning infrequent topic s and,
in order to achieve a good F-measure performance, it compensates by querying many more labels.

References

JASA,

In 20th COLT, pages 35–50.

[1] D. Angluin. Queries revisited. In 12th ALT, pages 12–31. Springer, 2001.
[2] K.S. Azoury and M.K. Warmuth. Relative loss bounds for on-line density estimation with the exponential
family of distributions. Machine Learning, 43(3):211–246, 2001.
[3] M.F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In 23rd ICML, pages 65–72.
ACM Press, 2006.
[4] M.F. Balcan, A. Broder, and T. Zhang. Margin-based active learning.
Springer, 2007.
[5] P.L. Bartlett, M.I. Jordan, and J.D. McAuliffe. Convexity, classiﬁ cation, and risk bounds.
101(473):138–156, 2006.
[6] G. Blanchard, O. Bousquet, and L. Zwald. Statistical properties of kernel principal component analysis.
Machine Learning, 66:259–294, 2007.
[7] M.L. Braun. Accurate error bounds for the eigenvalues of the kernel matrix. JMLR, 7:2303–2328, 2006.
[8] C. Campbell, N. Cristianini, and A. Smola. Query learning with large margin classiﬁers. In 17th ICML,
pages 111–118. Morgan Kaufmann, 2000.
[9] R. Castro and R.D. Nowak. Minimax bounds for active learning. IEEE Trans. IT, 2008. To appear.
[10] N. Cesa-Bianchi, A. Conconi, and C. Gentile. Learning probabilistic linear-threshold classiﬁers via se-
lective sampling. In 16th COLT, pages 373–387. Springer, 2003.
[11] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Worst-case analysis of selective sampling for linear classi-
ﬁcation.
JMLR, 7:1205–1230, 2006.
[12] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.
[13] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning,
15(2):201–221, 1994.
[14] R. Cohn, L. Atlas, and R. Ladner. Training connectionist networks with queries and selective sampling.
In NIPS 2. MIT Press, 1990.
[15] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In NIPS 20, pages
353–360. MIT Press, 2008.
[16] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of Perceptron-based active learning. In 18th COLT,
pages 249–263. Springer, 2005.
[17] Y. Freund, S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algo-
rithm. Machine Learning, 28(2/3):133–168, 1997.
[18] R. Gilad-Bachrach, A. Navot, and N. Tishby. Query by committee made real. NIPS, 18, 2005.
[19] T. Joachims. Making large-scale SVM learning practical. In B. Sch ¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods: Support Vector Learning. MIT Press, 1999.
[20] C. Monteleoni and M. K ¨a ¨ari ¨ainen. Practical online active learning for classiﬁcation. In 24th IEEE CVPR,
pages 249–263. IEEE Computer Society Press, 2007.
[21] J. Shawe-Taylor, C.K.I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the Gram
matrix and the generalization error of kernel-PCA. IEEE Trans. IT, 51(7):2510–2522, 2005.
[22] I. Steinwart and C. Scovel Fast Rates for Support Vector Machines using Gaussian Kernels Annals of
Statistics, 35: 575-607, 2007.
[23] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In
17th ICML, pages 999–1006. Morgan Kaufmann, 2000.
[24] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learnin g. The Annals of Statistics, 32(1):135–
166, 2004.
[25] V. Vovk. Competitive on-line statistics. International Statistical Review, 69:213–248, 2001.
[26] Y. Ying and D.X. Zhou. Online regularized classiﬁcation algorithms.
IEEE Transactions on Information
Theory, 52:4775–4788, 2006.


A Convex Upper Bound on the Log-Partition Function
for Binary Graphical Models

Laurent El Ghaoui
Department of Electrical Engineering and Computer Science
University of California Berkeley
Berkeley, CA 9470
elghaoui@eecs.berkeley.edu

Assane Gueye
Department of Electrical Engineering and Computer Science
University of California Berkeley
Berkeley, CA 9470
agueye@eecs.berkeley.edu

Abstract

We consider the problem of bounding from above the log-partition function corresponding to
second-order Ising models for binary distributions. We introduce a new bound, the cardinality
bound, which can be computed via convex optimization. The corresponding error on the log-
partition function is bounded above by twice the distance, in model parameter space, to a class of
“standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld
term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition
function, while constraining the distance to the class of standard Ising models, leads not only to a
good approximation to the log-partition function, but also to a model that is parsimonious, and eas-
ily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright
and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough,
the latter is outperformed by the new bound.

1 Introduction

1.1 Problem statement

This paper is motivated by the problem ﬁtting of binary distributions to experimental data. In the second-order Ising
model, PUT REF HERE the ﬁtted distribution p is assumed to have the parametric form
p(x; Q, q) = exp(xT Qx + qT x − Z (Q, q)), x ∈ {0, 1}n ,
where Q = QT ∈ Rn and q ∈ Rn contain the parameters of the model, and Z (Q, q), the normalization constant, is
 .
 (cid:88)
called the log-partition function of the model. Noting that xT Qx + qT x = xT (Q + D(q))x for every x ∈ {0, 1}n ,
we will without loss of generality assume that q = 0, and denote by Z (Q) the corresponding log-partition function
x∈{0,1}n
In the Ising model, the maximum-likelihood approach to ﬁtting data leads to the problem
Q∈Q Z (Q) − TrQS,
min

Z (Q) := log

exp[xT Qx]

(1)

(2)

where Q is a subset of the set S n of symmetric matrices, and S ∈ S n
(cid:88)
+ is the empirical second-moment matrix. When
Q = S n , the dual to (2) is the maximum entropy problem
H (p) : p ∈ P , S =
max
p
x∈{0,1}n
(cid:88)
where P is the set of distributions with support in {0, 1}n , and H is the entropy
H (p) = −
x∈{0,1}n

p(x) log p(x).

p(x)xxT ,

(3)

(4)

The constraints of problem (3) deﬁne a polytope in R2n called the marginal polytope.
For general Q’s, computing the log-partition function is NP-hard. Hence, except for special choices of Q, the
maximum-likelihood problem (2) is also NP-hard.
It is thus desirable to ﬁnd computationally tractable approxi-
mations to the log-partition function, such that the resulting maximum-likelihood problem is also tractable. In this
regard, convex, upper bounds on the log-partition function are of particular interest, and our focus here: convexity
usually brings about computational tractability, while using upper bounds yields a parameter Q that is suboptimal for
the exact problem.
Using an upper bound in lieu of Z (Q) in (2), leads to a problem we will generically refer to as the pseudo maximum-
likelihood problem. This corresponds to a relaxation to the maximum-entropy problem, which is (3) when Q = S n .
Such relaxations may involve two ingredients: an upper bound on the entropy, and an outer approximation to the
marginal polytope.

1.2 Prior work

Due to the vast applicability of Ising models, the problem of approximating their log-partition function, and the
related maximum-likelihood problem, has received considerable attention in the literature for decades, ﬁrst in statistical
physics, and more recently in machine learning.
The so-called log-determinant bound has been recently introduced, for a large class of Markov random ﬁelds, by
Wainwright and Jordan [2]. (Their paper provides an excellent overview of the prior work, in the general context of
graphical models.) The log-determinant bound is based on an upper bound on the differential entropy of continuous
random variable, that is attained for a Gaussian distribution. The log-determinant bound enjoys good tractability
properties, both for the computation of the log-partition function, and in the context of the maximum-likelihood
problem (2). A recent paper by Ravikumar and Lafferty [1] discusses using bounds on the log-partition function to
estimate marginal probabilities for a large class of graphical models, which adds extra motivation for the present study.

1.3 Main results and outline

The main purpose of this note is to introduce a new upper bound on the log-partition function that is computationally
tractable. The new bound is convex in Q, and leads to a restriction to the maximum-likelihood problem that is also
tractable. Our development crucially involves a speciﬁc class of Ising models, which we’ll refer to as standard Ising
models, in which the model parameter Q has the form Q = µI + λ11T , where λ, µ are arbitrary scalars. Such models
are indeed standard in statistical physics: the ﬁrst term µI describes interaction with the external magnetic ﬁeld, and
the second (λ11T ) is a simple mean ﬁeld approximation to ferro-magnetic coupling.
For standard Ising models, it can be shown that the log-partition functions has a computationally tractable, closed-form
expression. Due to space limitation, such proof is omitted in this paper. Our bound is constructed so as to be exact in
the case of standard Ising models. In fact, the error between our bound and the true value of the log-partition function
is bounded above by twice the l1 -norm distance from the model parameters (Q) to the class of standard Ising models.
The outline of the note reﬂects our main results: in section 2, we introduce our bound, and show that the approximation
error is bounded above by the distance to the class of standard Ising models. We discuss in section 3 the use of our
bound in the context of the maximum-likelihood problem (2) and its dual (3). In particular, we discuss how imposing
a bound on the distance to the class of standard Ising models may be desirable, not only to obtain an accurate approx-
imation to the log-partition function, but also to ﬁnd a parsimonious model, having good interpretability properties.
We then compare the new bound with the log-determinant bound of Wainwright and Jordan in section 4. We show

that our new bound outperforms the log-determinant bound when the norm (cid:107)Q(cid:107)1 is small enough (less than 0.08n),
and provide numerical experiments supporting the claim that our comparison analysis is quite conservative: our bound
appears to be better over a wide range of values of (cid:107)Q(cid:107)1 .
Notation. Throughout the note, n is a ﬁxed integer. For k ∈ {0, . . . , n}, deﬁne ∆k := {x ∈ {0, 1}n : Card(x) =
k}. Let ck = |∆k | denote the cardinal of ∆k , and πk := 2−n ck the probability of ∆k under the uniform distribution.
For a distribution p, the notation Ep refers to the corresponding expectation operator, and Probp (S ) to the probability
of the event S under p. The set P is the set of distributions with support on {0, 1}n .
For X ∈ Rn×n , the notation (cid:107)X (cid:107)1 denotes the sum of the absolute values of the elements of X , and (cid:107)X (cid:107)∞ the
largest of these values. The set S n is the set of symmetric matrices, S n
+ the set of symmetric positive semideﬁnite
matrices. We use the notation X (cid:186) 0 for the statement X ∈ S n
+ . If x ∈ Rn , D(x) is the diagonal matrix with x
on its diagonal. If X ∈ Rn×n , d(X ) is the n-vector formed with the diagonal elements of X . Finally, X is the set
{(X, x) ∈ S n × Rn : d(X ) = x} and X+ = {(X, x) ∈ S n × Rn : X (cid:186) xxT , d(X ) = x}.

2 The Cardinality Bound

2.1 The maximum bound

To ease our derivation, we begin with a simple bound based on replacing each term in the log-partition function by its
maximum over {0, 1}n . This leads to an upper bound on the log-partition function:
Z (Q) ≤ n log 2 + φmax (Q),

where

xT Qx.

ψmax (Q) = min
t,ν

φmax (Q) := max
x∈{0,1}n
Computing the above quantity is in general NP-hard. Starting with the expression
TrQX : rank(X ) = 1,
φmax (Q) = max
(X,x)∈X+
and relaxing the rank constraint leads to the upper bound φmax (Q) ≤ ψmax (Q), where ψmax (Q) is deﬁned via a
semideﬁnite program:
ψmax (Q) = max
(5)
TrQX,
(X,x)∈X+
(cid:181)
(cid:182)
where X+ = {(X, x) ∈ S n × Rn : X (cid:186) xxT , d(X ) = x}. For later reference, we note the dual form:
D(ν ) − Q 1
(cid:186) 0
2 ν
t :
1
2 ν T
t
1
4 ν T (D(ν ) − Q)−1 ν : D(ν ) (cid:194) Q.
The corresponding bound on the log-partition function, referred to as the maximum bound, is
Z (Q) ≤ Zmax (Q) := n log 2 + ψmax (Q).
The complexity of this bound (using interior-point methods) is roughly O(n3 ).
Let us make a few observations before proceeding. First, the maximum-bound is a convex function of Q, which is
important in the context of the maximum-likelihood problem (2). Second, we have Zmax (Q) ≤ n log 2 + (cid:107)Q(cid:107)1 ,
which follows from (5), together with the fact that any matrix X that is feasible for that problem satisﬁes (cid:107)X (cid:107)∞ ≤ 1.
Finally, we observe that the function Zmax is Lipschitz continuous, with constant 1 with respect to the l1 -norm. It can
be shown that the same property holds for the log-partition function Z itself. Due to space limitation such proof is
omitted in this paper. Indeed, for every symmetric matrices Q, R we have the sub-gradient inequality
Zmax (R) ≥ Zmax (Q) + TrX opt (R − Q),
where X opt is any optimal variable for the dual problem (5). Since any feasible X satisﬁes (cid:107)X (cid:107)∞ ≤ 1, we can bound
the term TrX opt (Q − R) from below by −(cid:107)Q − R(cid:107)1 , and after exchanging the roles of Q, R, obtain the desired result.

= min
ν

(6)

(7)

2.2 The cardinality bound
For every k ∈ {0, . . . , n}, consider the subset of variables with cardinality k , ∆k := {x ∈ {0, 1}n : Card(x) = k}.
(cid:195)
(cid:33)
n(cid:88)
(cid:88)
This deﬁnes a partition of {0, 1}n , thus
exp[xT Qx]
x∈∆k
(cid:33)
(cid:195)
k=0
n(cid:88)
We can reﬁne the maximum bound by replacing the terms in the log-partition by their maximum over ∆k , leading to
ck exp[φk (Q)]
k=0

Z (Q) ≤ log

Z (Q) = log

,

.

where, for k ∈ {0, . . . , n}, ck = |∆k |, and

(8)

xT Qx.

We deﬁne the cardinality bound, as

φk (Q) := max
x∈∆k
Computing φk (Q) for arbitrary k ∈ {0, . . . , n} is NP-hard. Based on the identity
TrQX : xT x = k , 1T X 1 = k2 , rankX = 1,
φk (Q) = max
(X,x)∈X+
and using rank relaxation as before, we obtain the bound φk (Q) ≤ ψk (Q), where
TrQX : xT x = k , 1T X 1 = k2 .
ψk (Q) = max
(X,x)∈X+
(cid:33)
(cid:195)
n(cid:88)
ck exp[ψk (Q)]
k=0
The complexity of computing ψk (Q) (using interior-point methods) is roughly O(n3 ). The upper bound Zcard (Q) is
computed via n semideﬁnite programs of the form (9). Hence, its complexity is roughly O(n4 ).
(cid:182)
(cid:181)
Problem (9) admits the dual form

D(ν ) + µI + λ11T − Q 1
(cid:186) 0.
2 ν
:= min
ψk (Q)
t + kµ + λk2 :
(10)
1
2 ν T
t
t,µ,ν,λ
The fact that ψk (Q) ≤ ψmax (Q) for every k is obtained upon setting λ = µ = 0 in the semi-deﬁnite programming
problem (10). In fact, we have

Zcard (Q) := log

(9)

.

(11)

kµ + k2λ + ψmax (Q − µI − λ11T ).

ψk (Q) = min
µ,λ
The above expression can be directly obtained from the following, valid for every µ, λ:
φk (Q) = kµ + k2λ + φk (Q − µI − λ11T )
≤ kµ + k2λ + φmax (Q − µI − λ11T )
≤ kµ + k2λ + ψmax (Q − µI − λ11T ).
It can be shown (proof which we omit due to space limitation) that, in the case of standard Ising models, that is if Q
has the form µI + λ11T for some scalars µ, λ, then the bound ψk (Q) is exact. Since the values of xT Qx when x
ranges ∆k are constant, the cardinality bound is also exact.
By construction, Zcard (Q) is guaranteed to be better (lower) than Zmax (Q), since the latter is obtained upon replacing
ψk (Q) by its upper bound ψ(Q) for every k . The cardinality bound thus satisﬁes
Z (Q) ≤ Zcard (Q) ≤ Zmax (Q) ≤ n log 2 + (cid:107)Q(cid:107)1 .

(12)

Using the same technique as used in the context of the maximum bound, we can show that the function ψk is Lipschitz-
continuous, with constant 1 with respect to the l1 -norm. Using the Lipschitz continuity of positively weighted log-sum-
(cid:33)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)log
(cid:195)
(cid:33)
(cid:195)
exp functions (with constant 1 with respect to the l∞ norm), we deduce that Zcard (Q) is also Lipschitz-continuous:
n(cid:88)
n(cid:88)
for every symmetric matrices Q, R,
|Zcard (Q) − Zcard (R)| ≤
ck exp[ψk (R)]
ck exp[ψk (Q)]
k=0
k=0
≤ max
|ψk (Q) − ψk (R)|
0≤k≤n
≤ (cid:107)Q − R(cid:107)1 ,

− log

as claimed.

2.3 Quality analysis

TrQX : xT x = k , 1T X 1 = k2 .

We now seek to establish conditions on the model parameter Q, which guarantee that the approximation error
Zcard (Q) − Z (Q) is small. The analysis relies on the fact that, for standard Ising models, the error is zero.
We begin by establishing an upper bound on the difference between maximal and minimal values of xT Qx when
x ∈ ∆k . We have the bound
xT Qx ≥ ηk (Q) := min
min
x∈∆k
(X,x)∈X+
In the same fashion as for the quantity ψk (Q), we can express ηk (Q) as
kµ + k2λ + ψmin (Q − µI − λ11T ),
ηk (Q) = max
µ,λ
where ψmin (Q) := min
TrQX . Based on this expression , we have, for every k :
(X,x)∈X+
k(µ − µ(cid:48) ) + k2 (λ − λ(cid:48) ) +
0 ≤ ψk (Q) − ηk (Q) = min
λ,µ, λ(cid:48) ,µ(cid:48)
ψmax (Q − µI − λ11T ) − ψmin (Q − µ(cid:48) I − λ(cid:48)11T )
ψmax (Q − µI − λ11T ) − ψmin (Q − µI − λ11T )
≤ min
λ,µ
(cid:107)Q − µI − λ11T (cid:107)1 ,
≤ 2 minλ,µ
where we have used the fact that , for every symmetric matrix R, we have
0 ≤ ψmax (R) − ψmin (R) =
max
(X,x),(Y ,y)∈X+
≤
max
(cid:107)X (cid:107)∞≤1, (cid:107)Y (cid:107)∞≤1
= 2(cid:107)R(cid:107)1 .

TrR(X − Y )
TrR(X − Y )

(cid:33)
(cid:195)
(cid:33)
(cid:195)
Using again the Lipschitz continuity properties of the weighted log-sum-exp function, we obtain that for every Q, the
n(cid:88)
n(cid:88)
absolute error between Z (Q) and Zcard (Q) is bounded as follows:
0 ≤ Zcard (Q) − Z (Q) ≤ log
ck exp[ηk (Q)]
ck exp[ψk (Q)]
k=0
k=0
≤ max
(ψk (Q) − ηk (Q))
0≤k≤n
≤ 2Dst (Q), Dst (Q) := min
λ,µ
Thus, a measure of quality is Dst (Q), the distance, in l1 -norm, between the model and the class of standard Ising
models. Note that this measure is easily computed, in O(n2 log n) time, by ﬁrst setting λ to be the median of the
values Qij , 1 ≤ i < j ≤ n, and then setting µ to be the median of the values Qii − λ, i = 1, . . . , n.
We summarize our ﬁndings so far with the following theorem:

(cid:107)Q − µI − λ11T (cid:107)1 ,

− log

(13)

(cid:195)
n(cid:88)
Theorem 1 (Cardinality bound) The cardinality bound is
k=0
where φk (Q), k = 0, . . . , n, is deﬁned via the semideﬁnite program (9), which can be solved in O(n3 ). The approxi-
mation error is bounded above by twice the distance (in l1 -norm) to the class of standard Ising models:
0 ≤ Zcard (Q) − Z (Q) ≤ 2 min
(cid:107)Q − µI − λ11T (cid:107)1 .
λ,µ

(cid:33)
ck exp[ψk (Q)]

Zcard (Q) := log

.

3 The Pseudo Maximum-Likelihood Problem

3.1 Tractable formulation

(cid:195)
(cid:33)
Using the bound Zcard (Q) in lieu of Z (Q) in the maximum-likelihood problem (2) leads to a convex restriction of
n(cid:88)
that problem, referred to as the pseudo-maximum likelihood problem. This problem can be cast as
(cid:181)
(cid:182)
− TrQS
ck exp[tk + kµk + k2λk ]
min
t,µ,ν,Q
k=0
D(νk ) + µk I + λk 11T − Q 1
s.t. Q ∈ Q,
2 νk
1
2 ν T
tk
k
The complexity of this bound is XXX. For numerical reasons, and without loss of generality, it is advisable to scale
the ck ’s and replace them by πk := 2−n ck ∈ [0, 1].

(cid:186) 0, k = 0, . . . , n.

log

3.2 Dual and interpretation
When Q = S n , the dual to the above problem is

max
(Yk ,yk ,qk )n
k=0

−D(q ||π)

(cid:181)
: S =

(cid:186) 0, d(Yk ) = yk ,

(cid:182)
Yk , q ≥ 0, qT 1 = 1,

n(cid:88)
k=0
yk
Yk
yT
qk
k
1T yk = kqk , 1T Yk 1 = k2 qk , k = 0 . . . , n.
where π is the distribution on {0, . . . , n}, with πk = Probu∆k = 2−n ck , and D(q ||π) is the relative entropy
n(cid:88)
(Kullback-Leibler divergence) between the distributions q , π :
k=0
k Yk , xk := q−1
To interpret this dual, we assume without loss of generality q > 0, and use the variables Xk := q−1
k yk .
n(cid:88)
We obtain the equivalent (non-convex) formulation
qkXk , q ≥ 0, qT 1 = 1,
: S =
k=0
(Xk , xk ) ∈ X+ , 1T xk = k , 1T Xk 1 = k2 , k = 0 . . . , n.

max
(Xk ,xk ,qk )n
k=0

qk log qk
πk

.

D(q ||π) :=

−D(q ||π)

(14)

The above problem can be obtained as a relaxation to the dual of the exact maximum-likelihood problem (2), which
is the maximum entropy problem (3). The relaxation involves two steps: one is to form an outer approximation to the
marginal polytope, the other is to ﬁnd an upper bound on the entropy function (4).

(15)

if x ∈ ∆k ,
otherwise.

n(cid:88)
First observe that we can express any distribution on {0, 1}n as
k=0

where

qk pk (x),
(cid:189)

p(x) =
(cid:88)
q−1
k p(x)
qk = Probp∆k =
p(x), pk (x) =
0
x∈∆k
Note that the functions pk are valid distributions on {0, 1}n as well as ∆k .
n(cid:88)
To obtain an outer approximation to the marginal polytope, we then write the moment-matching equality constraint in
problem (3) as
qkXk ,
(cid:88)
k=0
where Xk ’s are the second-order moment matrices with respect to pk :
Xk = Epk xxT = q−1
k
x∈∆k
To relax the constraints in the maximum-entropy problem (3), we simply use the valid constraints Xk (cid:186) xk xT
(cid:88)
k ,
d(Xk ) = xk , 1T xk = k , 1T Xk 1 = k2 , where xk is the mean under pk :
xk = Epk x = q−1
k
x∈∆k
This process yields exactly the constraints of the relaxed problem (14).
To ﬁnalize our relaxation, we now form an upper bound on the entropy function (4). To this end, we use the fact that,
(cid:88)
n(cid:88)
(cid:88)
since each pk has support in ∆k , its entropy is bounded above by log |∆k |, as follows:
−H (p) =
n(cid:88)
(cid:88)
x∈∆k
x∈{0,1}n
k=0
n(cid:88)
=
x∈∆k
k=0
qk (log qk − H (pk ))
≥ n(cid:88)
=
k=0
≥ n(cid:88)
k=0
k=0
which is, up to a constant, the objective of problem (14).

qk (log qk − log |∆k |)

qk log qk
πk

− n log 2,

p(x) log p(x) =

p(x) log p(x)

p(x)xxT .

p(x)x.

S = EpxxT =

qk pk (x) log(qk pk (x))

(|∆k | = 2nπk )

3.3 Ensuring quality via bounds on Q
We consider the (exact) maximum-likelihood problem (2), with Q = {Q = QT : (cid:107)Q(cid:107)1 ≤ }:
Z (Q) − TrQS : (cid:107)Q(cid:107)1 ≤ ,
min
Q=QT

and its convex relaxation:

Zcard (Q) − TrQS : (cid:107)Q(cid:107)1 ≤ .

min
Q=QT

(16)

(17)

The feasible sets of problems (16) and (17) are the same, and on it the difference in the objective functions is uniformly
bounded by 2. Thus, any -suboptimal solution of the relaxation (17) is guaranteed to by 3-suboptimal for the exact
problem, (16).
In practice, the l1 -norm constraint in (17) encourages sparsity of Q, hence the interpretability of the model. It also has
good properties in terms of the generalization error. As seen above, the constraint also implies a better approximation
to the exact problem (16). All these beneﬁts come at the expense of goodness-of-ﬁt, as the constraint reduces the
expressive power of the model. This is an illustration of the intimate connections between computational and statistical
properties of the model.
A more accurate bound on the approximation error can be obtained by imposing the following constraint on Q and
two new variables λ, µ:

(cid:107)Q − µI − λ11T (cid:107)1 ≤ .

We can draw similar conclusions as before. Here, the resulting model will not be sparse, in the sense of having many
elements in Q equal to zero. However, it will still be quite interpretable, as the bound above will encourage the number
of off-diagonal elements in Q that differ from their median, to be small.
A yet more accurate control on the approximation error can be induced by the constraints ψk (Q) ≤  + ηk (Q) for
every k , each of which can be expressed as an LMI constraint. The corresponding constrained relaxation to the
(cid:195)
(cid:33)
n(cid:88)
maximum-likelihood problem has the form
(cid:182)
(cid:181)
− TrQS
log
k + k2λ+
k + kµ+
ck exp[t+
k ]
k=0
(cid:181)
(cid:182)
k 11T − Q 1
diag(ν +
k ) + µ+
k I + λ+
2 ν +
k
t+
2 ν +
1
k
k
k I − λ−
k ) − µ−
Q − diag(ν −
2 ν −
k 11T
1
t−
2 ν −
k
1
k
k
k − t−
k ≤ , k = 0, . . . , n.
t+
Using this model instead of ones we saw previously, we sacriﬁce less on the front of the approximation to the true
likelihood, at the expense of increased computational effort.

(cid:186) 0, k = 0, . . . , n,

(cid:186) 0, k = 0, . . . , n,

min
t,µ± ,ν± ,Q

s.t.

4 Links with the Log-Determinant Bound

4.1 The log-determinant bounds

The bound in Wainwright and Jordan [2] is based on an upper bound on the (differential) entropy of a continuous
random variable, which is attained for a Gaussian distribution. It has the form Z (Q) ≤ Zld (Q), with

1
1
Zld (Q) := αn + max
12 I )
(X,x)∈X+
2
where α := (1/2) log(2πe) ≈ 1.42. Wainwright and Jordan suggest to further relax this bound to one which is easier
to compute:

log det(X − xxT +

TrQX +

(18)

Zld (Q) ≤ Zrld (Q) := αn + max
(X,x)∈X

TrQX +

1
2

log det(X − xxT +

1
12 I ).

(19)

Like Z and the bounds examined previously, the bound Zld and Zrld are Lipschitz-continuous, with constant 1 with
respect to the l1 norm. The proof starts with the representations above, and exploits the fact that (cid:107)Q(cid:107)1 is an upper
bound on TrQX when (X, x) ∈ X+ .

s.t.

(cid:186) 0.

log det

log 2+
(cid:182)
Tr(D(ν ) − Q − F ) − 1
2

The dual of the log-determinant bound has the form (see appendix (??))
(cid:181)
log π − 1
Zld (Q) = n
2
2
(cid:181)
t +
min
t,ν,F,g ,h

D(ν ) − Q − F − 1
2 ν − g
1
2 ν T − gT
− 1
t − h
12
F g
g
h
(cid:181)
The relaxed counterpart Zrld (Q) is obtained upon setting F, g , h to zero in the dual above:
D(ν ) − Q − 1
Tr(D(ν ) − Q) − 1
1
log π − 1
Zrld (Q) = n
2 ν
log 2 + min
− 1
2
2
12
2
2 ν T
t
t,ν
Using Schur complements to eliminate the variable t, we further obtain
1
Zrld (Q) = n
log π +
+
2
2
1
4 ν T (D(ν ) − Q)−1ν +
min
ν
4.2 Comparison with the maximum bound

1
12

Tr(D(ν ) − Q) − 1
2

log det(D(ν ) − Q).

t +

log det

(cid:182)

(cid:182)

.

(20)

(21)

We ﬁrst note the similarity in structure between the dual problem (5) deﬁning Zmax (Q) and that of the relaxed log-
determinant bound.
Despite these connections, the log-determinant bound is neither better nor worse than the cardinality or maximum
bounds. Actually, for some special choices of Q (e.g. when Q is diagonal), the cardinality bound is exact, while the
log-determinant one is not. Conversely, one can choose Q so that Zcard (Q) > Zld (Q), so no bound dominates the
other. The same can be said for Zmax (Q) (see section 4.4 for numerical examples).
However, when we impose an extra condition on Q, namely a bound on its l1 norm, more can be said. The analysis is
based on the case Q = 0, and exploits the Lipschitz continuity of the bounds with respect to the l1 -norm.
First notice (although not shown in this paper because of space limitation) that, for Q = 0, the relaxed log-determinant
bound writes

1
2πe
Zrld (0) = n
log
+
2
3
2
1
log πe
= Zmax (0) + n
2 .
6
2
Now invoke the Lipschitz continuity properties of the bounds Zrld (Q) and Zmax (Q), and obtain that
Zrld (Q) − Zmax (Q) = (Zrld (Q) − Zrld (0)) + (Zrld (0) − Zmax (0)) + (Zmax (0) − Zmax (Q))
≥ −2(cid:107)Q(cid:107)1 + (Zrld (0) − Zmax (0))
1
= −2(cid:107)Q(cid:107)1 + + n
log πe
+
2 .
2
6
This proves that if (cid:107)Q(cid:107)1 ≤ n
6 + 1
4 log πe
4 , then the relaxed log-determinant bound Zrld (Q) is worse (larger) than the
maximum bound Zmax (Q). We can strengthen the above condition to (cid:107)Q(cid:107)1 ≤ 0.08n.

+

4.3 Summary of comparison results

To summarize our ﬁndings:

Theorem 2 (Comparison) We have for every Q:
Z (Q) ≤ Zcard (Q) ≤ Zmax (Q) ≤ n log 2 + (cid:107)Q(cid:107)1 .
In addition, we have Zmax (Q) ≤ Zrld (Q) whenever (cid:107)Q(cid:107)1 ≤ 0.08n.

4.4 A numerical experiment

We now illustrate our ﬁndings on the comparison between the log-determinant bounds and the cardinality and maxi-
mum bounds. We set the size of our model to be n = 20, and for a range of values of a parameter ρ, generate N = 10
random instances of Q with (cid:107)Q(cid:107)1 = ρ. Figure ?? shows the average values of the bounds, as well as the associated
error bars. Clearly, the new bound outperforms the log-determinant bounds for a wide range of values of ρ. Our
predicted threshold value of (cid:107)Q(cid:107)1 for which the new bound becomes worse, namely ρ = 0.08n ≈ 1.6 is seen to be
very conservative, with respect to the observed threshold of ρ ≈ 30. On the other hand, we observe that for large
values of (cid:107)Q(cid:107)1 , the log-determinant bounds do behave better. Across the range of ρ, we note that the log-determinant
bound is indistinguishable from its relaxed counterpart.

5 Conclusion and Remarks

We have introduced a new upper bound (the cardinality bound) for the log-partition function corresponding to second-
order Ising models for binary distribution. We have shown that such a bound can be computed via convex optimization,
and, when compared to the log-determinant bound introduced by Wainwright and Jordan (2006), the cardinality bound
performs better when the l1 -norm of the model parameter vector is small enough.
Although not shown in the paper, the cardinality bound becomes exact in the case of standard Ising model, while the
maximum bound (for example) is not exact for such model.
As was shown in section 2, the cardinality bound was computed by deﬁning a partition of {0, 1}. This idea can be
generalized to form a class of bounds which we call partition bounds. It turns out that partitions bound are closely
linked to the more general class bounds that are based on worst-case probability analysis.
We acknowledge the importance of applying our bound to real-word data. We hope to include such results in subse-
quent versions of this paper.

References
[1] P. Ravikumar and J. Lafferty. Variational Chernoff bounds for graphical models. In Proc. Advances in Neural
Information Processing Systems (NIPS), December 2007.
[2] Martin J. Wainwright and Michael I. Jordan. Log-determinant relaxation for approximate inference in discrete
Markov random ﬁelds. IEEE Trans. Signal Processing, 2006.


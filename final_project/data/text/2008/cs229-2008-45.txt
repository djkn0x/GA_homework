Incentives and Machine Learning

John Hegeman

December 12, 2008

In 2007, US paid search ad spend was $8.9 billion - most of which used a
pay per click (PPC) billing model.
In PPC, advertisers only pay the search
engine when their ad receives a click. Hence, the ability to accurately estimate
an ad(cid:146)s click-through rate (CTR) is extremely valuable. The more likely that
an ad is clicked, the more likely that the search engine is paid.
An important but often ignored feature of this environment, is the adver-
sarial nature of the estimation problem. The cost per click (CPC) that an
advertiser pays is often inversely proportional to the estimated CTR of the
ad. Hence, an advertiser always prefers that the search engine assign a higher
CTR estimate rather than a lower estimate. This is true regardless of the ad(cid:146)s
true CTR. The con(cid:135)icting ob jectives of the advertiser and the search engine
wouldn(cid:146)t necessarily be a problem except that the advertiser often has control
over the inputs to the learning algorithm and has some ability to distort them.
One might be tempted to think that this shouldn(cid:146)t matter at all.
If the
machine learning algorithm is continually trained on new data as it comes in,
then it should automatically adjust to this behavior. The problem with this
argument is that the resulting "naive" equilibrium results in a distribution of
x that is less informative about y than the undistorted distribution is. The
following example illustrates this point.

Example 1 Ads are of two types. With probability pl an ad is of type (cid:18) = l
and has CTR yl . With probability ph an ad is of type (cid:18) = h and has CTR
equal to yh (we assume yh > yl ). An ad(cid:146)s type is not observable.
Instead, the
learning algorithm must estimate y based on the landing page type, x. Ads of
type l normal ly have landing page x = h while ads of type h have landing page
x = h. However, advertisers are able to send tra¢ c through a landing page
of the alternate type for a cost c < yh (cid:0) yl . Advertisers maximize the utility
function U ( ^y) = E [ ^y ] (cid:0) c (cid:1) 1fx6=(cid:18)g and the search engine has an objective function
E [(y (cid:0) ^y)2 ].
Ignoring incentives, the optimal estimation strategy is to choose
^y to approximate E [y jx]. However, in our setting, the equilibrium resulting
from this estimation strategy has al l advertisers using landing page x = h and
consequently the search engine is no better o⁄ than if it hadn(cid:146)t used the landing
page data at al l.
In contrast, the optimal learning algorithm for this example would choose
^y to approximate (cid:11)E [y jx] + (1 (cid:0) (cid:11))E [y ] with (cid:11) chosen so that the bene(cid:133)t to

1

advertisers of type l from distorting their landing page is slightly less than the
cost of distorting the landing page: (cid:11)(E [y jh] (cid:0) E [y jl]) = c (cid:0) (cid:15).
In what follows, we generalize this example to allow for a distribution of dis-
tortion costs and show that the optimal learning algorithm in this environment
still seeks to approximate the weighted average (cid:11)E [y jx] + (1 (cid:0) (cid:11))E [y ] rather
than the standard E [y jx]. Unfortunately, the correct choice of (cid:11) will depend
on the distribution of the manipulation cost which is both unknown and impos-
sible to estimate statically. However, this result can still provide some guidance
in practical applications since the principal, if given enough time, could make
small adjustments to (cid:11) and then observe whether the resulting equilibrium is
better or worse than the previous equilibrium.

1 The Model

A principal (the search engine in the above example) must estimate an unob-
served characteristic y as a function of a characteristic x reported by an agent.
The distribution of y is a function of the agent(cid:146)s type, (cid:18) 2 f0; 1g which is unob-
served by the principal. We assume y 2 [yl ; yh ] and yl < E [y j(cid:18) = 0] < E [y j(cid:18) =
1] < yh . After the principal observes x, she makes an estimate of y which we
denote ^y(x). We will refer to ^y(x) as the estimation function. The principal(cid:146)s
payo⁄ is a function of the expected accuracy of the estimate:

Up = (cid:0)E [(y (cid:0) ^y(x))2 ].
The agent(cid:146)s payo⁄ is equal to the principal(cid:146)s estimate, ^y , minus the distortion
cost if the agent reported misreported x:

Ua (x; c; (cid:18)) = ^y(x) (cid:0) c (cid:1) 1fx6=(cid:18)g .
The cost of distortion, c, is distributed according to the distribution function
F (x). We assume F (x) is continuous with density function f (x) such that
F (yl ) = 0 and f (c) > 0 for c > 0.
We will evaluate estimation functions in terms of their equilibrium perfor-
mance. That is, we assume agents choose x(cid:3) (c; (cid:18) ; ^y) = arg max
Ua (x; c; (cid:18) ; ^y).
x
The principal(cid:146)s equilibrium payo⁄, U (cid:3)p is her expected utility when agents choose
x = x(cid:3) : U (cid:3)p ( ^y) = (cid:0)E [(y (cid:0) ^y(x(cid:3) (c; (cid:18) ; ^y)))2 ]. We will use ^yM to denote an esti-
mation function that maximizes U (cid:3)p .

2 Results

Our initial concern in Section 3.1 will be to analyze the optimal estimation func-
tion ^y(x) without regard for how ^y(x) might be approximated by a principal
without knowledge of the distribution of y given (cid:18) . Section 3.2 will then turn

2

to methods for approximating the optimal estimation function using a converg-
ing series of estimation functions. We refer to such a series as an estimation
strategy.

2.1 Estimation Functions
While we cannot solve for the optimal estimation function explicitly, the fol-
lowing proposition provides some insight into the form of ^yM which will prove
useful later.
Proposition 2 There exists (cid:11) 2 [0; 1] such that ^yM (x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) =
x] + (1 (cid:0) (cid:11))E [y ].
Proof. First, observe that adding a constant to ^y has no e⁄ ect on an agent(cid:146)s
optimal report: x(cid:3) (c; (cid:18) ; ^y) = x(cid:3) (c; (cid:18) ; ^y + a). This al lows us to prove the fol lowing
lemma:

Lemma 3 E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))] = E [y ].

It fol lows that

Proof. Let ^y 0 (x) = ^yM (x) + E [y ] (cid:0) E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))].
U (cid:3)p ( ^y 0 ) = (cid:0)E [(y (cid:0) ^y 0 (x(cid:3) (c; (cid:18) ; ^y)))2 ]
= (cid:0)E [(y (cid:0) ^yM (x(cid:3) (c; (cid:18) ; ^y)) (cid:0) E [y ] + E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))])2 ]
= (cid:0)E [((y (cid:0) ^yM (x(cid:3) (c; (cid:18) ; ^y)))2 ] + (E [y ] (cid:0) E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))])2
= U (cid:3)p ( ^yM ) + (E [y ] (cid:0) E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))])2
(cid:21) U (cid:3)p ( ^y 0 ) + (E [y ] (cid:0) E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))])2
Thus, we must have that (E [y ] (cid:0) E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))])2 = 0 which implies
E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))] = E [y ].

Since only at agents of at most one type wil l pay the distortion cost, we wil l
always have E [ ^yM (x(cid:3) (c; (cid:18) ; ^yM ))] 6= E [y ]. This ensures that we can select a value
of (cid:11) 2 ((cid:0)1; 1) such that ^yM (0) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = 0] + (1 (cid:0) (cid:11))E [y ]. For a
(cid:133)xed value of ^yM (0), there is a unique value of ^yM (1) that ensures Lemma 1 is
satis(cid:133)ed. Moreover, observe that E [(cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = x]+(1(cid:0)(cid:11))E [y ]] = E [y ].
Thus, if ^yM (0) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = 0] + (1 (cid:0) (cid:11))E [y ] then Lemma 1 requires
that ^yM (1) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = 1] + (1 (cid:0) (cid:11))E [y ].
It fol lows that there must
exist (cid:11) such that ^yM (x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = x] + (1 (cid:0) (cid:11))E [y ]. To complete
the proof of the proposition, it remains to show that (cid:11) 2 (0; 1).
We (cid:133)rst rule out the case of (cid:11) < 0:
U (cid:3)p ( ^yM ) = E [(y (cid:0) (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM )] (cid:0) (1 (cid:0) (cid:11))E [y ])2 ]
= E [(y (cid:0) E (y) (cid:0) (cid:11)(E [y jx(cid:3) (c; (cid:18) ; ^yM )] (cid:0) E [y ])2 ]
= E [(y (cid:0) E (y))2 ] + (cid:11)(1 (cid:0) (cid:11))E [(E [y jx(cid:3) (c; (cid:18) ; ^yM )] (cid:0) E [y ])2 ]
(cid:21) E [(y (cid:0) E (y))2 ]
Thus, if (cid:11) were less than 0 then the optimal estimation strategy would per-
form worse than simply using ^y(x) = E [y ] which is a contradiction.
We now rule out the possibility of (cid:11) > 1. First, note that if (cid:11) > 1 then
^yM (0) < E [y jx(cid:3) (c; (cid:18) ; ^yM ) = 0]. Consider the estimation strategy ^y 0 formed

3

by setting ^y 0 (0) = E [y jx(cid:3) (c; (cid:18) ; ^yM ) = 0] and ^y 0 (1) = ^yM (1). Note that the
incentive for type (cid:18) = 0 to report x = 1 has been strictly decreased and thus fewer
agents wil l misreport. We can now group the agents into three categories: (1)
those of type (cid:18) = 0 who report x = 0 in response to either estimation strategy,
(2) those of type (cid:18) = 0 who report x = 1 in response to ^yM but x = 0 in
response to ^y 0 , and (3) those who report x = 1 in response to either estimation
strategy. Observe that for agents in groups (1) and (2) ^y 0 is closer than ^yM
to the expected value of y given which of the above three groups the agent is
in. Since estimation accuracy is una⁄ ected for agents in the third category this
implies that U (cid:3)p ( ^y 0 ) > U (cid:3)p ( ^yM ) which is a contradiction. It fol lows that (cid:11) 2 [0; 1]
for the optimal estimation strategy.

2.2 Estimation Strategies
Proposition 2 tells us that the optimal estimation function always results in
an equilibrium in which the predicted value of y is a simple weighted average
of the unconditional expectation of y and the expectation of y conditional on
the agent(cid:146)s reported x. Let ~E [y jx = x0 ] be the average value of y across all
observations prior to i with x = x0 (or yH if x0 has not been observed) and let
~E [y ] be the average value of y across all observations prior to i. This suggests
that the optimal estimation function can be approximated by the estimation
strategy fg (i) (x)g1i=1 where g (i) (x) = (cid:11) ~E [y (i) jx(i) ] + (1 (cid:0) (cid:11)) ~E [y (i) ]. Due to
space constraints we will not formally discuss convergence issues for estimation
strategies and will simply assume that an estimation strategy of this form will
converge to some estimation function ^g . Note that by the de(cid:133)nition of g (i) ,
^g must satisfy ^g(x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^g) = x] + (1 (cid:0) (cid:11))E [y ]. The following
proposition ensures that if (cid:11) is chosen correctly then this estimation strategy
will converge to the optimal equilibrium.
Proposition 4 Let ^y and ^y 0 satisfy ^y(x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^y) = x] + (1 (cid:0) (cid:11))E [y ]
and ^y 0 (x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = x] + (1 (cid:0) (cid:11))E [y ].
It fol lows that ^y(x) = ^y 0 (x).
Proof. Subtracting the expression for ^y(0) from the expression for ^y(1) yields
(1)
^y(1) (cid:0) ^y(0) = (cid:11)(E [y jx(cid:3) (c; (cid:18) ; ^y) = 1] (cid:0) E [y jx(cid:3) (c; (cid:18) ; ^y) = 0])
and similarly
^y 0 (1) (cid:0) ^y 0 (0) = (cid:11)(E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = 1] (cid:0) E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = 0]).
(2)
Observe that ^y(1) (cid:0) ^y(0) > ^y 0 (1) (cid:0) ^y 0 (0) implies that more agents of type
(cid:18) = 0 report x = 1 in response to ^y than in response to ^y 0 which implies that
E [y jx(cid:3) (c; (cid:18) ; ^y) = 1] < E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = 0]. However, E [y jx(cid:3) (c; (cid:18) ; ^y) = 0] =
E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = 0] and thus subtracting equation (2) from (1) yields
( ^y(1) (cid:0) ^y(0)) (cid:0) ( ^y 0 (1) (cid:0) ^y 0 (0)) = E [y jx(cid:3) (c; (cid:18) ; ^y) = 1] (cid:0) E [y jx(cid:3) (c; (cid:18) ; ^y 0 ) = 0]
Let (cid:11)(cid:3) be the value of (cid:11) such that ^yM (x) = (cid:11)E [y jx(cid:3) (c; (cid:18) ; ^yM ) = x] + (1 (cid:0)
(cid:11))E [y ] (Proposition 2 guarantees that (cid:11)(cid:3) exists).
Corollary 5 The estimation strategy g (i)
a(cid:3) (x) = (cid:11)(cid:3) eE [y (i) jx(i) ] + (1 (cid:0) (cid:11)(cid:3) ) eE [y (i) ]
converges to the optimal estimation function.
4

Proof. ga(cid:3) converges to some ^ga(cid:3) such that ^ga(cid:3) (x) = (cid:11)(cid:3)E [y jx(cid:3) (c; (cid:18) ; ^ga(cid:3) ) =
x] + (1 (cid:0) (cid:11)(cid:3) )E [y ]. By proposition 4, this su¢ cient to ensure that the equilibrium
outcome of the estimation strategy ga(cid:3) is identical to that of ^yM .
The following proposition improves on the range of possible values of (cid:11) by
ruling out (cid:11) = 0 and (cid:11) = 1:
Proposition 6 The naive estimation strategy g1 = ~E [y jx] always outperforms
the trivial estimation strategy g0 = ~E [y ]. However, it is never optimal. In
particular, there exists (cid:11)(cid:3) < 1 such that g(cid:11) = (cid:11) ~E [y jx] + (1 (cid:0) (cid:11)) ~E [y ] outperforms
g1 for every (cid:11) 2 ((cid:11)(cid:3) ; 1).
Proof. For brevity, we provide only a sketch of the proof. The (cid:133)rst claim (that
g1 outperforms g0 ) fol lows immediately from the observation that not al l agents
choose to manipulate in the equilibrium resulting from g1 .
For the second claim, consider the equilibrium resulting from the naive esti-
mation strategy ((cid:11) = 1). Switching to the estimation strategy (cid:11) = 1 (cid:0) (cid:15) has a
cost of order (cid:15)2 as ^y moves away from E [y jx]. However, it also causes a mass
of order (cid:15) of type l advertisers from x = xh to x = xl which has a bene(cid:133)t of
order (cid:15). Thus, for su¢ ciently smal l (cid:15) the impact wil l be positive.

2.3 Agent Surplus
The previous analysis has been focused on the principal(cid:146)s equilibrium payo⁄.
However, in a richer model, the principal might also care to some extent about
the payo⁄ to the agent.
In our example setting of a search engine and ad-
vertisers, this could arise in a model with entry costs since a higher payo⁄ to
advertisers will result in more advertisers entering the market which will in turn
could yield a higher payo⁄ to the principal. We brie(cid:135)y touch on the agent(cid:146)s
surplus with Proposition 7:
Proposition 7 For estimation functions of the form ^y(x) = (cid:11) eE [y jx] + (1 (cid:0)
(cid:11)) eE [y ], equilibrium advertiser surplus is strictly decreasing in (cid:11) for (cid:11) > 0.
Proof. E [Ua (x; c; (cid:18))] = E [ ^y(x) (cid:0) c (cid:1) 1fx6=(cid:18)g ] = E [ ^y(x)] (cid:0) c Pr[x 6= (cid:18) ]
First, note that E [(cid:11) eE [y jx] + (1 (cid:0) (cid:11)) eE [y ]] = E [y ] is independent of (cid:11). Since
the equilibrium value of ^y(1)(cid:0) ^y(0) is increasing in (cid:11), Pr[x 6= (cid:18) ] is also increasing
in (cid:11) and thus E [Ua (x; c; (cid:18))] is decreasing in (cid:11).
3 Conclusion

In many practical machine learning applications, agents with ob jectives contrary
to the ob jectives of the principal may have the ability to manipulate some or
all of the input data.
Ignoring incentive issues in such environments will result
in a sub-optimal estimation accuracy. Our contribution is to demonstrate
that the optimal learning algorithm in such an environment under-utilizes the
manipulable input by using an ob jective function that is a weighted average of
the conditional and unconditional expectations of the output variable.

5


Clusters and Coarse Partitions in LP Relaxations

David Sontag
CSAIL, MIT
dsontag@csail.mit.edu

Amir Globerson
School of Computer Science and Engineering
The Hebrew University
gamir@cs.huji.ac.il

Tommi Jaakkola
CSAIL, MIT
tommi@csail.mit.edu

Abstract

We propose a new class of consistency constraints for Linear Programming (LP)
relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical mod-
els. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of
a cluster of variables, with computational cost increasing exponentially with the
size of the clusters. By partitioning the state space of a cluster and enforcing con-
sistency only across partitions, we obtain a class of constraints which, although
less tight, are computationally feasible for large clusters. We show how to solve
the cluster selection and partitioning problem monotonically in the dual LP, us-
ing the current beliefs to guide these choices. We obtain a dual message passing
algorithm and apply it to protein design problems where the variables have large
state spaces and the usual cluster-based relaxations are very costly. The result-
ing method solves many of these problems exactly, and signiﬁcantly faster than a
method that does not use partitioning.

1

Introduction

A common inference task in graphical models is ﬁnding the most likely setting of the values of the
variables (the MAP assignment). Indeed, many important practical problems can be formulated as
MAP problems (e.g., protein-design problems [9]). The complexity of the MAP problem depends
on the structure of the dependencies between the variables (i.e. the graph structure) and is known to
be NP-hard in general. Speciﬁcally, for problems such as protein-design, the underlying interaction
graphs are dense, rendering standard exact inference algorithms useless.
A great deal of effort has been spent recently on developing approximate algorithms for the MAP
problem. One promising approach is based on linear programming relaxations, solved via message
passing algorithms akin to belief propagation [2, 3]. In this case, the MAP problem is ﬁrst cast as an
integer linear program, and then is relaxed to a linear program by removing the integer constraints
and adding new constraints on the continuous variables. Whenever the relaxed solution is integral, it
is guaranteed to be the optimal solution. However, this happens only if the relaxation is sufﬁciently
“tight” (with respect to a particular objective function).
Relaxations can be made increasingly tight by introducing LP variables that correspond to clusters
of variables in the original model. In fact, in recent work [6] we have shown that by adding a set
of clusters over three variables, complex problems such as protein-design and stereo-vision may be
solved exactly. The problem with adding clusters over variables is that computational cost scales
exponentially with the cluster size. Consider, for example, a problem where each variable has 100
states (cf. protein-design). Using clusters of s variables means adding 100s LP variables, which is
computationally demanding even for clusters of size three.
Our goal in the current paper is to design methods that introduce constraints over clusters at a reduced
computational cost. We achieve this by representing clusters at a coarser level of granularity. The
key observation is that it may not be necessary to represent all the possible joint states of a cluster of
variables. Instead, we partition the cluster’s assignments at a coarser level, and enforce consistency

e

max
x

(1)

(2)

only across such partitions. This removes the number of states per variable from consideration, and
instead focuses on resolving currently ambiguous settings of the variables. Following the approach
of [2], we formulate a dual LP for the partition-based LP relaxations and derive a message passing
algorithm for optimizing the dual LP based on block coordinate descent. Unlike standard message
passing algorithms, the algorithm we derive involves passing messages between coarse and ﬁne
representations of the same set of variables.
MAP and its LP relaxation. We consider discrete pairwise Markov random ﬁelds on a graph
G = (V , E ), deﬁned as the following exponential family distribution1
(cid:80)
1
p(x; θ) =
ij∈E θij (xi ,xj )
Z
Here θ is a parameter vector specifying how pairs of variables in E interact. The MAP problem we
xM that maximizes the function f (x; θ) = (cid:80)
consider here is to ﬁnd the most likely assignment of the variables under p(x; θ) (we assume that the
evidence has already been incorporated into the model). This is equivalent to ﬁnding the assignment
ij∈E θij (xi , xj ).
The resulting discrete optimization problem may also be cast as a linear program. Deﬁne µ to
be a vector of marginal probabilities associated with the interacting pairs of variables (edges)
{µij (xi , xj )}ij∈E as well as {µi (xi )}i∈V for the nodes.
The set of µ’s that could arise from
some joint distribution on G is known as the marginal polytope M(G) [7]. The MAP problem is
then equivalent to the following linear program:
where µ · θ = (cid:80)
(cid:80)
θij (xi , xj )µij (xi , xj ). The extreme points of the marginal polytope
ij∈E
xi ,xj
are integral and correspond one-to-one with assignments x. Thus, there always exists a maximiz-
ing µ that is integral and corresponds to xM . Although the number of variables in this LP is only
O(|E | + |V |), the difﬁculty comes from an exponential number of linear inequalities typically re-
quired to describe the marginal polytope M(G).
LP relaxations replace the difﬁcult global constraint that the marginals in µ must arise from some
are consistent with the node marginals, {µ | (cid:80)
common joint distribution by ensuring only that the marginals are locally consistent with one an-
other. The most common such relaxation, pairwise consistency, enforces that the edge marginals
µij (xi , xj ) = µi (xi )}. The integral extreme
xj
points of this local marginal polytope also correspond to assignments. If a solution is obtained at
one such extreme point, it is provably the MAP assignment. However, the local marginal polytope
also contains fractional extreme points, and, as a relaxation, will in general not be tight.
We are therefore interested in tightening the relaxation. There are many known ways to do so, in-
cluding cycle inequalities [5] and semi-deﬁnite constraints [8]. However, perhaps the most straight-
forward approach corresponds to lifting the relaxation by adding marginals over clusters of nodes to
the model (cf. generalized belief propagation [10]) and constraining them to be consistent with the
edge marginals. However, each cluster comes with a computational cost that grows as ks , where s
is the number of variables in the cluster and k is the number of states for each variable. We seek to
offset this exponential cost by introducing coarsened clusters, as we show next.

f (x; θ) = max
µ∈M(G)

µ · θ ,

2 Coarsened clusters and consistency constraints

We begin with an illustrative example. Suppose we have a graphical model that is a triangle with
each variable taking k states. We can recover the exact marginal polytope in this case by forcing the
pairwise marginals µij (xi , xj ) to be consistent with some distribution µ123 (x1 , x2 , x3 ). However,
when k is large, introducing the corresponding k3 variables to our LP may be too costly and perhaps
unnecessary, if a weaker consistency constraint would already lead to an integral extreme point. To
this end, we will use a coarse-grained version of µ123 where the joint states are partitioned into
larger collections, and consistency is enforced over the partitions.

1We do not use potentials on single nodes θi (xi ) since these can be folded into θij (xi , xj ). Our algorithm
can also be derived with explicit θi (xi ), and we omit the details for brevity.

Figure 1: A graphical illustration of the consistency constraint between the original (ﬁne granularity)
edge (xi , xk ) and the coarsened triplet (zi , zj , zk ). The two should agree on the marginal of zi , zk .
For example, the shaded area in all three ﬁgures represents the same probability mass.
has ﬁve states, Zi might be deﬁned as (cid:8){1, 2}, {3, 5}, {4}(cid:9). Given such a partitioning scheme,
The simplest partitioning scheme builds on coarse-grained versions of each variable Xi . Let Zi
denote a disjoint collection of sets covering the possible values of Xi . For example, if variable Xi
graphically in Fig. 1. In the case when Zi individuates each state, i.e., (cid:8){1}, {2}, {3}, {4}(cid:9), we
we can introduce a distribution over coarsened variables µ123 (z1 , z2 , z3 ) and constrain it to agree
with µik (xi , xk ) in the sense that they both yield the same marginals for zi , zk . This is illustrated
recover the usual cluster consistency constraint.
We use the above idea to construct tighter outer bounds on the marginal polytope and incorporate
them into the MAP-LP relaxation. We assume that we are given a set of clusters C . For each cluster
c ∈ C and variable i ∈ c we also have a partition Z c
i as in the above example2 (the choice of clusters
µij (xi , xj ) = (cid:88)
(cid:88)
and partitions will be discussed later). We introduce marginals over the coarsened clusters µ(zc )
and constrain them to agree with the edge variables µij (xi , xj ) for all edges ij ∈ c:
µc (zc ).
i ,xj ∈zc
xi∈zc
zc \{zc
j }
i ,zc
j
The key idea is that the coarsened cluster represents higher-order marginals albeit at a lower res-
olution, whereas the edge variables represent lower-order marginals but at a ﬁner resolution. The
constraint in Eq. 3 implies that these two representations should agree.
We can now state the LP that we set out to solve. Our LP optimizes over the following marginal
variables: µij (xi , xj ), µi (xi ) for the edges and nodes of the original graph, and µc (zc ) for the
(cid:80)
 µ ≥ 0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

coarse-grained clusters. We would like to constrain these variables to belong to the following outer
(cid:80)
(cid:80)
bound on the marginal polytope:
µij (xi , xj ) = µi (xi )
(cid:80)
xj
µij (xi , xj ) =
i ,xj ∈zc
xi∈zc
zc \{zc
j }
i ,zc
j
µij (xi , xj ) = 1
xi ,xj
µc (zc ) = 1 is implied by the above constraints. The corresponding MAP-LP relax-
µ · θ

MC (G) =
Note that (cid:80)
zc
ation is then:

µc (zc )

max
µ∈MC (G)

(3)

(4)

(5)

This LP could in principle be solved using generic LP optimization tools. However, a more efﬁcient
and scalable approach is to solve it via message passing in the dual LP, which we show how to do
in the next section. In addition, for this method to be successful, it is critical that we choose good
coarsenings, meaning that it should have few partitions per variable, yet still sufﬁciently tightens the
relaxation. Our approach for choosing the coarsenings is to iteratively solve the LP using an initial
relaxation (beginning with the pairwise consistency constraints), then to introduce additional cluster
constraints, letting the current solution guide how to coarsen the variables. As we showed in earlier
work [6], solving with the dual LP gives us a simple method for “warm starting” the new LP (the
tighter relaxation) using the previous solution, and also results in an algorithm for which every step
monotonically decreases an upper bound on the MAP assignment. We will give further details of
the coarsening scheme in Section 4.
2We use a superscript of c to highlight the fact that different clusters may use different partitionings for Zi .
Also, there can be multiple clusters on the same set of variables, each using a different partitioning.

xizjzkzizkxkzi3 Dual linear program and a message passing algorithm

max
xi

min
β

s.t.

(6)

(7)

λc→ij (z c
j ) =
i , z c

In this section we give the dual of the partition-based LP from Eq. 5, and use it to obtain a message
passing algorithm to efﬁciently optimize this relaxation. Our approach extends earlier work by
Globerson and Jaakkola [2] who gave the generalized max-product linear programming (MPLP)
algorithm to solve the usual (non-coarsened) cluster LP relaxation in the dual.
The dual formulation in [2] was derived by adding auxiliary variables to the primal. We fol-
lowed a similar approach to obtain the LP dual of Eq. 5. The dual variables are as follows:
βij→i (xi , xj ), βij→j (xi , xj ), βij→ij (xi , xj ) for every edge ij ∈ E , and βc→ij (zc ) for every coars-
ened cluster c and edge ij ∈ c. As in [2], we deﬁne the following functions of β :
βij→i (xi , xj ),
λij→ij (xi , xj ) = βij→ij (xi , xj )
λij→i (xi ) = max
xj
j } βc→ij (zc )
max
zc \{zc
i ,zc
As we show below, the variables λ correspond to the messages sent in the message passing algorithm
that we use for optimizing the dual. Thus λij→i (xi ) should be read as the message sent from edge
j ) is the message from the coarsened cluster to one of its intersection
ij to node i, and λc→ij (z c
i , z c
(cid:105)
(cid:104)
edges. Finally, λij→ij (xi , xj ) is the message sent from an edge to itself. The dual of Eq. 5 is the
(cid:88)
(cid:88)
λik→i (xi ) + (cid:88)
λij→ij (xi , xj ) + (cid:88)
following constrained minimization problem:
λc→ij (z c
i [xi ], z c
j [xj ])
max
xi ,xj
c:ij∈c
ij∈E
k∈N (i)
(cid:88)
i
∀ij ∈ E , xi , xj
βij→i (xi , xj ) + βij→j (xi , xj ) + βij→ij (xi , xj ) = θij (xi , xj )
∀c, zc
βc→ij (zc ) = 0
ij∈c
i such that xi ∈ z c
i ∈ Z c
i [xi ] refers to the mapping from xi ∈ Xi to the coarse state z c
The notation z c
i .
By convex duality, the dual objective evaluated at a dual feasible point upper bounds the primal LP
optimum, which in turn upper bounds the value of the MAP assignment. It is illustrative to compare
this dual LP with [2] where the cluster dual variables were βc→ij (xc ). Our dual corresponds to
c ) whenever zc [xc ] = zc [x(cid:48)
introducing the additional constraint that βc→ij (xc ) = βc→ij (x(cid:48)
c ].
The advantage of the above dual is that it can be optimized via a simple message passing algorithm
that corresponds to block coordinate descent. The key idea is that it is possible to ﬁx the values of
the β variables corresponding to all clusters except one, and to ﬁnd a closed form solution for the
non-ﬁxed β s. It then turns out that one does not need to work with β variables directly, but can
keep only the λ message variables. Fig. 2 provides the form of the updates for all three message
types. S (c) is the set of edges in cluster c (e.g. ij, j k , ik). Importantly, all messages outgoing from
a cluster or edge must be sent simultaneously.
Here we derive the cluster to edge updates, which differ from [2]. Assume that all values of β are
j ) for all ij ∈ c in some cluster c. The term in the dual objective that
ﬁxed except for βc→ij (z c
i , z c
(cid:104)
(cid:105)
λij→ij (xi , xj ) + (cid:88)
j ) can be written equivalently as
depends on βc→ij (z c
i , z c
i [xi ], z c(cid:48)
λc(cid:48)→ij (z c(cid:48)
(cid:105)
(cid:104)
i [xi ], z c
j [xj ]) + λc→ij (z c
j [xj ])
max
xi ,xj
c(cid:48) :c(cid:48) (cid:54)=c,ij∈c(cid:48)
Due to the constraint (cid:80)
= max
j [xj ])
bij (z c
j ) + λc→ij (z c
i [xi ], z c
i , z c
.
zc
i ,zc
j
ij∈c βc→ij (zc ) = 0, all of the βc→ij need to be updated simultaneously. It
can be easily shown (using an equalization argument as in [2]) that the βc→ij (zc ) that satisfy the
(cid:88)
constraint and minimize the objective are given by
1
βc→ij (zc ) = −bij (z c
j ) +
i , z c
|S (c)|
st∈c
The message update given in Fig. 2 follows from the deﬁnition of λc→ij . Note that none of the
cluster messages involve the original cluster variables xc , but rather only zc . Thus, we have achieved
the goal of both representing higher-order clusters and doing so at a reduced computational cost.

bst (z c
t ).
s , z c

(8)

(11)

(12)

max
xj

(cid:104) (cid:88)
• Edge to Node: For every edge ij ∈ E and node i (or j ) in the edge:
1
λij→i (xi )←− 2
−j
j [xj ])+λij→ij (xi , xj )+λ−i
i (xi )+
λc→ij (z c
i [xi ], z c
j (xj )+θij (xi , xj )
3 λ
i (xi ) = (cid:80)
3
c:ij∈c
−j
k∈N (i)\j λik→i (xi ).
where λ
(cid:88)
• Edge to Edge: For every edge ij ∈ E :
λij→ij (xi , xj )← − 2
3
c:ij∈c
• Cluster to Edge: First deﬁne

(cid:104)
1
λ−i
j (xj ) + λ
i [xi ], z c
λc→ij (z c
j [xj ]) +
3
λij→ij (xi , xj ) + (cid:88)
c(cid:48) (cid:54)=c:ij∈c(cid:48)

(cid:105)
−j
i (xi ) + θij (xi , xj )

i [xi ], z c(cid:48)
λc(cid:48)→ij (z c(cid:48)
j [xj ])

j ) = max
bij (z c
i , z c
xi ∈ z c
xj ∈ z c
i
j

(cid:105)

(9)

The update is then:

j )← − bij (z c
λc→ij (z c
j ) +
i , z c
i , z c

1
|S (c)| max
zc \{zc
j }
i ,zc

(cid:88)
st∈c

bst (z c
t )
s , z c

(10)

Figure 2: The message passing updates for solving the dual LP given in Eq. 8.

The algorithm in Fig. 2 solves the dual for a given choice of coarsened clusters. As mentioned
in Sec. 2, we would like to add such clusters gradually, as in [6]. Our overall algorithm is thus
solution x by locally maximizing the single node beliefs bi (xi ) = (cid:80)
similar in structure to [6] and proceeds as follows (we denote the message passing algorithm from
Fig. 2 by MPLP): 1. Run MPLP until convergence using the pairwise relaxation, 2. Find an integral
k∈N (i) λki→i (xi ), 3. If the
dual objective given in Eq. 8 is sufﬁciently close to the primal objective f (x; θ), terminate, 4. Add
a new coarsened cluster c using the strategy given in Sec. 4, 5. Initialize messages going out of
the new cluster c to zero, and keep all the previous message values (this will not change the bound
value), 6. Run MPLP for N iterations, then return to 2.

4 Choosing coarse partitions

Until now we have not discussed how to choose the clusters to add and their partitionings. Our
strategy for doing so closely follows that of our earlier work [6]. Given a set C of candidate clusters
to add (e.g., the set of all triplets in the graph as in [6]), we would like to add a cluster that would
result in the maximum decrease of the dual bound on the MAP. In principle such a cluster could be
found by optimizing the dual for each candidate cluster, then choosing the best one. However, this is
computationally costly, so in [6] we instead use the bound decrease resulting from just once sending
messages from the candidate cluster to its intersection edges.
d(c) = (cid:88)
(cid:88)
If we were to add the full (un-coarsened) cluster, this bound decrease would be:
bij (xi , xj ) − max
max
where bij (xi , xj ) = λij→ij (xi , xj ) + (cid:80)
xi ,xj
xc
ij∈c
ij∈c
c:ij∈c λc→ij (z c
i [xi ], z c
j [xj ]).
Our strategy now is as follows: we add the cluster c that maximizes d(c), and then choose a parti-
i for all i ∈ c that is guaranteed to achieve a decrease that is close to d(c). This can clearly
tioning Z c
i = Xi (which achieves d(c)). However, in many cases
be achieved by using the trivial partition Z c
it is also possible to achieve it while using much coarser partitionings.

bij (xi , xj ),

(13)

i is too large to optimize over. Instead, we consider just |Xi |
The set of all possible partitionings Z c
candidate partitions that are generated based on the beliefs bi (xi ). Intuitively, the states with lower
belief values bi (xi ) are less likely to inﬂuence the MAP, and can thus be bundled together. We
will therefore consider partitions where the k states with lowest belief values are put into the same
“catch-all” coarse state sc
i , and all other states of xi get their own coarse state. Formally, a partition
i is the set of all xi with bi (xi ) < κi . The question next
i is characterized by a value κi such that sc
Z c
is how big we can make the catch-all state without sacriﬁcing the bound decrease.
We employ a greedy scheme whereby each i ∈ c (in arbitrary order) is partitioned separately, while
i = Xi for all i ∈ c. We would like to
the other partitions are kept ﬁxed. The process starts with Z c
i such that it is sufﬁciently separated from the state that achieves d(c). Formally, given a
choose sc
(cid:88)
(cid:88)
margin parameter γ we choose κi to be as large as possible such that the following constraint still
holds3 :
t ) ≤ max
bst (xs , xt ) − γ ,
bst (z c
s , z c
xc
st∈c
st∈c

max
zc \{z c
i },
i = sc
z c
i
where the ﬁrst maximization is over the coarse variables Zc\i , and Z c
i is ﬁxed to the catch-all state sc
i
i is a function of κi ). We can ﬁnd the optimal κi in time O(|Xi ||c| )
(note that the partitioning for Z c
by starting with κi = −∞ and increasing it until the constraint is violated. Since each subsequent
i differs by one additional state xi , we can re-use the maximizations over zc\i for the
value of sc
previous value of sc
i in evaluating the constraint for the current sc
i .
It can be shown by induction that this results in a coarsening that has a guaranteed bound decrease
of at least d(c) + min(0, γ ). Setting γ < 0 would give a partitioning with fewer coarse states at the
cost of a smaller guaranteed bound decrease. On the other hand, setting γ > 0 results in a margin
between the value of the dual objective (after sending the coarsened cluster message) and its value
if we were to ﬁx xi in the max terms of Eq. 11 to a value in sc
i . This makes it less likely that a state
in sc
i will become important again in subsequent message passing iterations. For the experiments in
this paper we use γ = 3d(c), scaling γ with the value of the guaranteed bound decrease for the full
cluster. Note that this greedy algorithm does not necessarily ﬁnd the partitioning with the fewest
number of coarse states that achieves the bound decrease.

5 Experiments

We report results on the protein design problem, originally described in [9]. The protein design
problem is the inverse of the protein folding problem. Given a desired backbone structure for the
protein, the goal is to construct the sequence of amino-acids that results in a low energy, and thus
stable, conﬁguration. We can use an approximate energy function to guide us towards ﬁnding a
set of amino-acids and rotamer conﬁgurations with minimal energy. In [9] the design problem was
posed as ﬁnding a MAP conﬁguration in a pairwise MRF. The models used there (which are also
available online) have a number of states per variable that is between 2 and 158, and contain up to
180 variables per model. The models are also quite dense so that exact calculation is not feasible.
Recently we showed [6] that all but one of the problems described in [9] can be solved exactly by
using a LP relaxation with clusters on three variables. However, since each individual state has
roughly 100 possible values, processing triplets required 106 operations, making the optimization
costly. In what follows we describe two sets of experiments that show that, by coarsening, we can
both signiﬁcantly reduce the computation time and achieve similar performance as if we had used
un-coarsened triplets [6]. The experiments differ in the strategy for adding triplets, and illustrate
two performance regimes. In both experimental setups we ﬁrst run the standard edge-based message
passing algorithm for 1000 iterations.
In the ﬁrst experiment, we add all triplets that correspond to variables whose single node beliefs are
tied (within 10−5 ) at the maximum after running the edge-based algorithm. Since tied beliefs cor-
respond to fractional LP solutions, it is natural to consider these in tighter relaxations. The triplets
correspond to partitioned variables, as explained in Sec. 2. The partitioning is guided by the ties in
the single node beliefs. Speciﬁcally, for each variable Xi we ﬁnd states whose single node beliefs
are tied at the maximum. Denote the number of states maximizing the belief by r . Then, we partition

3The constraint may be infeasible for γ > 0, in which case we simply choose Z c
i = Xi .

Figure 3: Comparison with algorithm from [6] for the protein “1aac”, after the ﬁrst 1000 iterations.
Left: Dual objective as a function of time. Right: The cost per one iteration over the entire graph.

the states into r subsets, each containing a different maximizing state. The other (non-maximizing)
states are split randomly among the r subsets. The triplets are then constructed over the coarsened
variables Z c
i and the message passing algorithm of Sec. 3 is applied to the resulting structure. After
convergence of the algorithm, we recalculate the single node beliefs. These may result in a different
partition scheme, and hence new variables Z c
i . We add new triplets corresponding to the new vari-
ables and re-run. We repeat until the dual-LP bound is sufﬁciently close to the value of the integral
assignment obtained from the messages (note that these values would not coincide if the relaxation
were not tight; in these experiments they do, so the ﬁnal relaxation is tight).
We applied the above scheme to the ten smallest proteins in the dataset used in [6] (for the larger
proteins we used a different strategy described next). We were able to solve all ten exactly, as in
[6]. The mean running time was six minutes. The gain in computational efﬁciency as a result of
using coarsened-triplets was considerable: The average state space size for coarsened triplets was
on average 3000 times smaller than that of the original triplet state space, resulting in a factor 3000
speed gain over a scheme that uses the complete (un-coarsened) triplets.4 This big factor comes
about because a very small number of states are tied per variable, thus increasing the efﬁciency of
our method where the number of partitions is equal to the number of tied states. While running on
full triplets was completely impractical, the coarsened message passing algorithm is very practical
and achieves the exact MAP assignments.
Our second set of experiments follows the setup of [6] (see Sec. 3), alternating between adding 5
triplets to the relaxation and running MPLP for 20 more iterations. The only difference is that, after
deciding to add a cluster, we use the algorithm from Sec. 4 to partition the variables. We tried various
settings of γ , including γ = 0 and .01, and found that γ = 3d(c) gave the best overall runtimes.
We applied this second scheme to the 15 largest proteins in the dataset.5 Of these, we found the exact
MAP in 47% of the cases (according to the criterion used in [6]), and in the rest of the cases were
within 10−2 of the known optimal value. For the cases that were solved exactly, the mean running
time was 1.5 hours, and on average the proteins were solved 8.1 times faster than with [6].6 To
compare the running times on all 15 proteins, we checked how long it took for the difference between
the dual and primal objectives to be less than .01f (xM ; θ), where xM is the MAP assignment. This
revealed that our method is faster by an average factor of 4.3. The reason why these factors are
less than the 3000 in the previous setup is that, for the larger proteins, the number of tied states is
typically much higher than that for the small ones.
Results for one of the proteins that we solved exactly are shown in Fig. 3. The cost per iteration
increases very little after adding each triplet, showing that our algorithm signiﬁcantly coarsened the
clusters. The total number of iterations and number of triplets added were roughly the same. Two
triplet clusters were added twice using different coarsenings, but otherwise each triplet only needed
to be added once, demonstrating that our algorithm chose the right coarsenings.

4These timing comparisons do not apply to [6] since that algorithm did not use all the triplets.
5We do not run on the protein 1fpo, which was not solved in [6].
6We made sure that differences were not due to different processing powers or CPU loads.

012345160180200220240260HoursObjective  This paperSontag et al. UAI ’08Primal (best decoding)Dual1000120014001600180005101520253035Iteration NumberTime (Seconds)  This paperSontag et al. UAI ’086 Discussion

We presented an algorithm that enforces higher-order consistency constraints on LP relaxations,
but at a reduced computational cost. Our technique further explores the trade-offs of representing
complex constraints on the marginal polytope while keeping the optimization tractable. In applying
the method, we chose to cluster variables’ states based a bound minimization criterion after solving
using a looser constraint on the polytope.
A class of approaches related to ours are the “coarse-to-ﬁne” applications of belief propagation [1,
4]. In those, one solves low-resolution versions of an MRF, and uses the resulting beliefs to initialize
ﬁner resolution versions. Although they share the element of coarsening with our approach, the goal
of coarse-to-ﬁne approaches is very different from our objective. Speciﬁcally, the low-resolution
MRFs only serve to speed-up convergence of the full resolution MRF via better initialization. Thus,
one typically should not expect it to perform better than the ﬁnest granularity MRF. In contrast,
our approach is designed to strictly improve the performance of the original MRF by introducing
additional (coarse) clusters. One of the key technical differences is that in our formulation the
setting of coarse and ﬁne variables are reﬁned iteratively whereas in [1], once a coarse MRF has
been solved, it is not revisited.
There are a number of interesting directions to explore. Using the same ideas as in this paper, one
can introduce coarsened pairwise consistency constraints in addition the full pairwise consistency
constraints. Although this would not tighten the relaxation, by passing messages more frequently
in the coarsened space, and only occasionally revisiting the full edges, this could give signiﬁcant
computational beneﬁts when the nodes have large numbers of states. This would be much more
similar to the coarse-to-ﬁne approach described above.
With the coarsening strategy used here, the number of variables still grows exponentially with the
cluster size, albeit at a lower rate. One way to avoid the exponential growth is to partition the
states of a cluster into a ﬁxed number of states (e.g., two), and then constrain such partitions to be
consistent with each other. Such a process may be repeated recursively, generating a hierarchy of
coarsened variables. The key advantage in this approach is that it represents progressively larger
clusters, but with no exponential growth. An interesting open question is to understand how these
hierarchies should be constructed.
Our techniques may also be helpful for ﬁnding the MAP assignment in MRFs with structured poten-
tials, such as context-speciﬁc Bayesian networks. Finally, these constraints can also be used when
calculating marginals.

References
[1] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient belief propagation for early vision. Int. J. Comput.
Vision, 70(1):41–54, 2006.
[2] A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP
LP-relaxations. In Advances in Neural Information Processing Systems 21. MIT Press, 2008.
IEEE Trans.
[3] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization.
Pattern Anal. Mach. Intell., 28(10):1568–1583, 2006.
[4] C. Raphael. Coarse-to-ﬁne dynamic programming. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 23(12):1379–1390, 2001.
[5] D. Sontag and T. Jaakkola. New outer bounds on the marginal polytope. In Advances in Neural Informa-
tion Processing Systems 21. MIT Press, 2008.
[6] D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and T. Jaakkola. Tightening LP relaxations for MAP using
message-passing. In UAI, 2008.
[7] M. Wainwright and M. I. Jordan. Graphical models, exponential families and variational inference. Tech-
nical report, UC Berkeley, Dept. of Statistics, 2003.
[8] M. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete
Markov random ﬁelds. IEEE Transactions on Signal Processing, 54(6):2099–2109, June 2006.
[9] C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation – an
empirical study. JMLR, 7:1887–1907, 2006.
[10] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized
belief propagation algorithms. IEEE Trans. on Information Theory, 51(7):2282– 2312, 2005.


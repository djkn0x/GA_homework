Translated Learning: Transfer Learning across
Different Feature Spaces

†Wenyuan Dai, †Yuqiang Chen, †Gui-Rong Xue, ‡Qiang Yang and †Yong Yu
†Shanghai Jiao Tong University
Shanghai 200240, China
{dwyak,yuqiangchen,grxue,yyu}@apex.sjtu.edu.cn
‡Hong Kong University of Science and Technology
Kowloon, Hong Kong
qyang@cse.ust.hk

Abstract
This paper investigates a new machine learning strategy called translated learn-
ing. Unlike many previous learning tasks, we focus on how to use labeled data
from one feature space to enhance the classi ﬁcation of other
entirely different
learning spaces. For example, we might wish to use labeled text data to help learn
a model for classifying image data, when the labeled images are difﬁcult to ob-
tain. An important aspect of translated learning is to build a “bridge” to link one
feature space (known as the “source space”) to another space
(known as the “tar-
get space”) through a translator in order to migrate the know ledge from source to
target. The translated learning solution uses a language model to link the class
labels to the features in the source spaces, which in turn is translated to the fea-
tures in the target spaces. Finally, this chain of linkages is completed by tracing
back to the instances in the target spaces. We show that this path of linkage can
be modeled using a Markov chain and risk minimization. Through experiments
on the text-aided image classi ﬁcation and cross-language c lassi ﬁcation tasks, we
demonstrate that our translated learning framework can greatly outperform many
state-of-the-art baseline methods.

1

Introduction

Traditional machine learning relies on the availability of a large amount of labeled data to train a
model in the same feature space. However, labeled data are often scarce and expensive to obtain. In
order to save much labeling work, various machine learning strategies have been proposed, including
semi-supervised learning [13], transfer learning [3, 11, 10], self-taught learning [9], etc. One com-
monality among these strategies is they all require the training data and test data to be in the same
feature space. For example, if the training data are documents, then the classi ﬁers cannot accept test
data from a video space. However, in practice, we often face the problem where the labeled data are
scarce in its own feature space, whereas there are sufﬁcient
labeled data in other feature spaces. For
example, there may be few labeled images available, but there are often plenty of labeled text docu-
ments on the Web (e.g., through the Open Directory Project, or ODP, http://www.dmoz.org/).
Another example is cross-language classi ﬁcation where lab eled documents in English are much
more than ones in some other languages such as Bangla, which has only 21 Web pages in the ODP.
Therefore, it would be great if we could learn the knowledge across different feature spaces and to
save a substantial labeling effort.

To address the transferring of knowledge across different feature spaces, researchers have proposed
multi-view learning [2, 8, 7] in which each instance has multiple views in different feature spaces.
Different from multi-view learning, in this paper, we focus on the situation where the training data
are in a source feature space, and the test data are in a different target feature space, and that there
is no correspondence between instances in these spaces. The source and target feature spaces can be

(a) Supervised Learning

(b) Semi-supervised Learning

(c) Transfer Learning

(d) Self-taught Learning

Elephants
are
large
and gray ...

big
mammals
on earth...

thick-
skinned,
...

massive
hoofed
mammal ...

(e) Multi-view Learning
Test Data
(f) Translated Learning
Figure 1: An intuitive illustration to different kinds of learning strategies using classi ﬁcation of
image elephants and rhinos as the example. The images in orange frames are labeled data, while the
ones without frames are unlabeled data.

very different, as in the case of text and images. To solve this novel learning problem, we develop
a novel framework named as translated learning, where training data and test data can be in totally
different feature spaces. A translator is needed to be exploited to link the different feature spaces.
Clearly, the translated learning framework is more general and difﬁcult than traditional learning
problems. Figure 1 presents an intuitive illustration of six different learning strategies, including
supervised learning, semi-supervised learning [13], transfer learning [10], self-taught learning [9],
multi-view learning [2], and ﬁnally, translated learning.

An intuitive idea for translated learning is to somehow translate all the training data into a target
feature space, where learning can be done within a single feature space. This idea has already been
demonstrated successful in several applications in cross-lingual text classi ﬁcation [1]. However, for
the more general translated learning problem, this idea is hard to be realized, since machine trans-
lation between different feature spaces is very difﬁcult to accomplish in many non-natural language
cases, such as translating documents to images. Furthermore, while a text corpus can be exploited
for cross-langauge translation, for translated learning, the learning of the “feature-space translator ”
from available resources is a key issue.

Our solution is to make the best use of available data that have both features of the source and target
domains in order to construct a translator. While these data may not be sufﬁcient in building a good
classi ﬁer for the target domain, as we will demonstrate in ou r experimental study in the paper, by
leveraging the available labeled data in the source domain, we can indeed build effective translators.
An example is to translate between the text and image feature spaces using the social tagging data
from Web sites such as Flickr (http://www.flickr.com/).

The main contribution of our work is to combine the feature translation and the nearest neighbor
learning into a uni ﬁed model by making use of a language model
[5]. Intuitively, our model can be
represented using a Markov chain c → y → x, where y represents the features of the data instances
x. In translated learning, the training data xs are represented by the features ys in the source feature
space, while the test data xt are represented by the features yt in the target feature space. We model
the learning in the source space through a Markov chain c → ys → xs , which can be connected to
another Markov chain c → yt → xt in the target space. An important contribution of our work then
is to show how to connect these two paths, so that the new chain c → ys → yt → xt , can be used
to translate the knowledge from the source space to the target one, where the mapping ys → yt is
acting as a feature-level translator. In our ﬁnal solution, which we call TLRisk, we exploit the risk
minimization framework in [5] to model translated learning. Our framework can accept different
distance functions to measure the relevance between two models.

2 Translated Learning Framework

2.1 Problem Formulation
We ﬁrst deﬁne the translated learning problem formally. Let Xs be the source instance space. In this
space, each instance xs ∈ Xs is represented by a feature vector (y (1)
s , . . . , y (ns )
), where y (i)
s ∈ Ys
s

and Ys is the source feature space. Let Xt be the target instance space, in which each instance
xt ∈ Xt is represented by a feature vector (y (1)
), where y (i)
, . . . , y (nt )
t ∈ Yt and Yt is the target
t
t
feature space. We have a labeled training data set Ls = {(x(i)
s , c(i)
i=1 in the source space, where
s )}n
x(i)
s ∈ Xs and c(i)
s ∈ C = {1, . . . , |C |} is the true class-label of x(i)
s . We also have another labeled
training data set Lt = {(x(i)
, c(i)
t ∈ Xt and c(i)
i=1 in the target space, where x(i)
t ∈ C . Usually, m
t )}m
t
is assumed to be small, so that Lt is not enough to train a reliable prediction model. The unlabeled
test data set U is a set of k examples {x(i)
i=1 , where x(i)
u ∈ Xt . Note that x(i)
is in a different
u }k
s
feature space from x(i)
and x(i)
u . For example, x(i)
s may be a text document, while x(i)
and x(i)
u may
t
t
be visual images.
To link the two feature spaces, a feature translator p(yt |ys ) ∝ φ(yt , ys ) is constructed. However,
for ease of explanation, we ﬁrst assume that the translator φ is given, and will discuss the derivation
of φ later in this section, based on co-occurrence data. We focus on our main objective in learning,
which is to estimate a hypothesis ht : Xt 7→ C to classify the instances x(i)
u ∈ U as accurately as
possible, by making use of the labeled training data L = Ls ∪ Lt and the translator φ.
2.2 Risk Minimization Framework

First, we formulate our objective in terms of how to minimize an expected risk function with respect
to the labeled training data L = Ls ∪ Lt and the translator φ by extending the risk minimization
framework in [5].
In this work, we use the risk function R(c, xt ) to measure the the risk for classifying xt to the
category c. Therefore, to predict the label for an instance xt , we need only to ﬁnd the class-label c
which minimizes the risk function R(c, xt ), so that
ht (xt ) = arg min
c∈C
The risk function R(c, xt ) can be formulate as the expected loss when c and xt are relevant; formally,
R(c, xt ) ≡ L(r = 1|c, xt ) = ZΘC
ZΘXt
L(θC , θXt , r = 1)p(θC |c) p(θXt |xt ) dθXt dθC .
Here, r = 1 represents the event of “relevant ”, which means (in Equatio n (2)) “ c and xt are relevant ”,
or “the label of xt is c”. θC and θXt are the models with respect to classes C and target space instances
Xt respectively. ΘC and ΘXt are two corresponding model spaces involving all the possible models.
Note that, in Equation (2), θC only depends on c and θXt only depends to xt . Thus, we use p(θC |c) to
replace p(θC |c, xt ), and use p(θXt |xt ) to replace p(θXt |c, xt ). L(θC , θXt , r = 1) is the loss function
with respect to the event of θC and θXt being relevant. We next address the estimation of the risk
function in Equation (2).

R(c, xt ).

(1)

(2)

2.3 Estimation
The risk function in Equation (2) is difﬁcult to estimate, si nce the sizes of ΘC and ΘXt can be
exponential in general. Therefore, we have to use approximation for estimating the risk function
for efﬁciency. First of all, the loss function L(θC , θXt , r = 1) can be formulated using distance
functions between the two models θC and θXt , so that L(θC , θXt , r = 1) = α∆(θC , θXt ), where
∆(θC , θXt ) is the distance (or dissimilarity) function, e.g. the Kullback-Leibler divergence. Replac-
ing L(θC , θXt , r = 1) with ∆(θC , θXt ), the risk function is reformulated as
R(c, xt ) ∝ ZΘC
ZΘXt
∆(θC , θXt )p(θC |c) p(θXt |xt ) dθXt dθC .
Since the sizes of ΘC and ΘXt are exponential in general, we cannot calculate Equation (3) straight-
forwardly. In this paper, we approximate the risk function by its value at the posterior mode:
R(c, xt ) ≈ ∆( ˆθc , ˆθxt )p( ˆθc |c)p( ˆθxt |xt ) ∝ ∆( ˆθc , ˆθxt )p( ˆθc |c),
where ˆθc = arg maxθC p(θC |c), and ˆθxt = arg maxθXt p(θXt |xt ).
In Equation (4), p( ˆθc |c) is the prior probability of ˆθc with respect to the target class c. This prior can
be used to balance the inﬂuence of different classes in the cl ass-imbalance case. When we assume
there is no prior difference among all the classes, the risk function can be rewritten into

(3)

(4)

Algorithm 1 Risk Minimization Algorithm for Translated Learning: (TLRisk)
Input: Labeled training data L in the source space, unlabeled test data U in the target space, a
translator φ to link the two feature spaces Ys and Yt and a dissimilarity function ∆(·, ·).
Output: The prediction label ht (xt ) for each xt ∈ U .
Procedure TLRisk train
1: for each c ∈ C do
Estimate the model ˆθc based on Equation (6).
2:
3: end for
Procedure TLRisk test
1: for each xt ∈ U do
Estimate the model ˆθxt based on Equation (7).
2:
Predict the label ht (xt ) for xt based on Equations (1) and (5).
3:
4: end for

(5)

(6)

R(c, xt ) ∝ ∆( ˆθc , ˆθxt ),
where ∆( ˆθc , ˆθxt ) denotes the dissimilarity between two models ˆθc and ˆθxt . To achieve this objective,
as in [5], we formulate these two models in the target feature space Yt ; speci ﬁcally, if we use KL
divergence as the distance function, ∆( ˆθc , ˆθxt ) can be measured by KL(p(Yt | ˆθc )||p(Yt | ˆθxt )).
Our estimation is based on the Markov chain assumption where ˆθc → c → ys → yt → xt → ˆθxt
and ˆθc → c → yt → xt → ˆθxt , so that
p(yt | ˆθc ) = ZYs Xc0∈C
p(yt |ys )p(ys |c0 )p(c0 | ˆθc ) dys + λ Xc0∈C
where p(yt |ys ) can be estimated using the translator φ; p(ys |c0 ) can be estimated based on the
statistical observations in the labeled text data set Ls in the source feature space Ys ; p(yt |c0 ) can be
estimated based on Lt in the target feature space Yt ; p(c0 | ˆθc ) can be calculated as: p(c0 | ˆθc ) = 1 if
c = c0 , and otherwise, p(c0 | ˆθc ) = 0; and λ is a trade-off parameter which controls the inﬂuence of
target space labeled data Lt .
For another model p(Yt | ˆθxt ), it can be estimated by
p(yt | ˆθxt ) = ZXt
t | ˆθxt ) dx0
p(yt |x0
t )p(x0
t ,
t ) can be estimated using the feature extractor in the target feature space Yt , and
where p(yt |x0
t | ˆθxt ) = 0.
t | ˆθxt ) = 1 if x0
t | ˆθxt ) can be calculated as p(x0
t = xt ; otherwise p(x0
p(x0
Integrating Equations (1), (5), (6) and (7), our translated learning framework is summarized as
algorithm TLRisk, an abbreviation for Translated Learning via Risk Minimization, which is shown
in Algorithm 1.

p(yt |c0 )p(c0 | ˆθc ),

(7)

Considering the computational cost of Algorithm 1, due to the Markov chain assumption, our al-
gorithm TLRisk can be implemented using dynamic programming. Therefore, in the worst case,
the time complexity of TLRisk is O(|C ||Yt | + |Yt ||Ys |) in training, and O(|C ||Yt |) for predicting
an instance. In practice, the data are quite sparse, and good feature mappings (or translator) should
also be sparse, otherwise it will consist of many ambiguous cases. Therefore, TLRisk can perform
much faster than the worst cases generally, and the computational cost of TLRisk is linear in the
non-zero occurrences in feature mappings.

2.4 Translator φ
We now explain in particular how to build the translator φ(yt , ys ) ∝ p(yt |ys ) to connect two dif-
ferent feature spaces. As mentioned before, to estimate the translator p(yt |ys ), we need some co-
occurrence data across the two feature spaces: source and target. Formally, we need co-occurrence
data in the form of p(yt , ys ), p(yt , xs ), p(xt , ys ), or p(xt , xs ). In cross-language problems, dictio-
naries can be considered as data in the form of p(yt , ys ) (feature-level co-occurrence). On the Web,

DATA S ET

horse vs coin
kayak vs bear
electric-guitar vs snake
cake vs binoculars
laptop vs sword
bonsai vs comet

DATA S I ZE
DOCUM ENT S
+
1610
1045
335
265
210
166

−
1345
885
326
320
203
164

IMAGE S
+
270
102
122
104
128
122

−
123
101
112
216
102
120

DATA S ET

dog vs canoe
greyhound vs cd
stained-glass vs microwave
rainbow vs sheet-music
tomato vs llama
frog vs saddle

DATA S I ZE
DOCUM ENT S
+
1084
380
331
261
175
150

−
1047
362
267
256
172
148

IMAGE S
+
102
94
99
102
102
115

−
103
102
107
84
119
110

Table 1: The description for each data set. Here, horse vs coin indicates all the positive in-
stances are about horse while all the negative instances are about coin.
“ +” means positive
instance; “ −” means negative instances.
social annotations on images (e.g. Flickr, images associated with keywords) and search-engine re-
sults in response to queries are examples for correlational data in the forms of p(yt , xs ) and p(xt , ys )
(feature-instance co-occurrence). Moreover, multi-view data (e.g. Web pages including both text
and pictures) is an example for data in the form of p(xt , xs ) (instance-level co-occurrence). Where
there is a pool of such co-occurrence data available, we can build the translator φ for estimating the
Markov chains in the previous subsections.
In particular, to estimate the translator φ, at ﬁrst, the feature-instance co-occurrence data ( p(yt , xs )
or p(xt , ys )) can be used to estimate the probabilities for feature-level co-occurrence p(yt , ys );
p(xt , ys )p(yt |xt ) dxt . The
p(yt , xs )p(ys |xs ) dxs and p(yt , ys ) = RXt
formally, p(yt , ys ) = RXs
instance-level co-occurrence data can also be converted to feature-level co-occurrence; formally,
p(xt , xs )p(ys |xs )p(yt |xt ) dxsdxt . Here, p(ys |xs ) and p(yt |xt ) are two feature
p(yt , ys ) = RXt RXs
extractors in Ys and Yt . Using the feature-level co-occurrence probability p(yt , ys ), we can estimate
t .
the translator as p(yt |ys ) = p(yt , ys )/ RYt
p(y 0
t , ys )dy 0
3 Evaluation: Text-aided Image Classiﬁcation
In this section, we apply our framework TLRisk to a text-aided image classi ﬁcation problem, which
uses binary labeled text documents as auxiliary data to enhance the image classi ﬁcation. This prob-
lem is derived from the application where a user or a group of users may have expressed preferences
over some text documents, and we wish to translate these preferences to images for the same group
of users. We will show the effectiveness of TLRisk on text-aided image classi ﬁcation. Our ob-
jective is to demonstrate that even with a small amount of labeled image training data, we can still
build a high-quality translated learning solution for image classi ﬁcation by leveraging the text doc-
uments, even if the co-occurrence data themselves are not sufﬁcient when directly used for training
a classi ﬁcation model in the target space.

3.1 Data Sets
The data sets of Caltech-2561 and Open Directory Project (ODP, http://www.dmoz.org/)
were used in our evaluation, as the image and text corpora. Our ODP collection was crawled during
August 2006, and involves 1,271,106 English Web pages. We generated 12 binary text-to-image
classi ﬁcation tasks from the above corpora. The descriptio n for each data set is presented in Table
1. The ﬁrst column presents the name of each data set, e.g. horse vs coin indicates all the
positive instances are about horse while all the negative instances are about coin. We collected
the corresponding documents from ODP for each category. However, due to space limitation, we
omit the detailed ODP directory information with respect to each data set here. In the table, we
also listed the data sizes for each task, including documents and images. Generally, the number of
documents is much larger than the number of images.

For data preprocessing, the SIFT descriptor [6] was used to ﬁ nd and describe the interesting points
in the images, and then clustered the extracted interest points into 800 clusters to obtain the code-
book. Based on the code-book, each image can be converted to a corresponding feature vector. For
text documents, we ﬁrst extracted and stemmed all the tokens
from the ODP Web pages, and then
information gain [12] was used to select the most important features for further learning process.
We collected the co-occurrence data from a commercial image search engine during April 2008.
The collected data are in the form of feature-instance co-occurrence p(ys , xt ), so that we have to
convert them to feature-level co-occurrence p(ys , yt ) as discussed in Section 2.4.

1http://www.vision.caltech.edu/Image Datasets/Caltech256/

0.40

0.35

0.30

0.25

0.20

e
t
a
R
 
r
o
r
r
E

Cosine

Image Only
Search+Image
TLRisk
Lowerbound

 

0.40

e
t
a
R
 
r
o
r
r
E

0.35

0.30

0.25

0.20

Kullback−Leibler Divergence

Pearson’s Correlation Coefficient

Image Only
Search+Image
TLRisk
Lowerbound

 

0.40

e
t
a
R
 
r
o
r
r
E

0.35

0.30

0.25

0.20

 

Image Only
Search+Image
TLRisk
Lowerbound

0.15

 

0.15

 

0.15

 

32

12 4
16
8
1 2 4
16
8
1 2 4
16
8
number of labeled images per category
number of labeled images per category
number of labeled images per category
(c)
(b)
(a)
Figure 2: The average error rates over 12 data sets for text-aided image classi ﬁcation with different
number of labeled images Lt .
Cosine
0.35

Pearson’s Correlation Coefficient

Kullback−Liebler Divergence

0.35

0.35

32

32

 

 

 

average over 12 data sets

average over 12 data sets

average over 12 data sets

e
t
a
R
 
r
o
r
r
E

0.30

0.25

0.20

e
t
a
R
 
r
o
r
r
E

0.30

0.25

0.20

e
t
a
R
 
r
o
r
r
E

0.30

0.25

0.20

0.15

 

0.15

 

0.15

 

4

16

0.0625

0.25
1
0.25
1
0.25
1
λ (in log scale)
λ (in log scale)
λ (in log scale)
(c)
(b)
(a)
Figure 3: The average error rates over 12 data sets for text-aided image classi ﬁcation with different
trade-off λ.

0.0625

0.0625

16

16

4

4

3.2 Evaluation Methods

Few existing research works addressed the text-aided image classi ﬁcation problem, so that for the
baseline methods in our experiments, we ﬁrst simply used the labeled data Lt as the training data in
the target space to train a classi ﬁcation model; we refer to t his model as Image Only. The second
baseline is to use the category name (in this case, there are two names for binary classi ﬁcation
problems) to search for training images and then to train classi ﬁers together with labeled images in
Lt ; we refer to this model as Search+Image.
Our framework TLRisk was evaluated under three different dissimilarity functions: (1) Kullback-
p(yt |θC ) log p(yt |θC )
p(yt |θXt ) dyt ; (2) Negative of cosine function
Leibler divergence (named KL): RYt
R Yt
p(yt |θC )p(yt |θXt )dyt
(named NCOS): −
; (3) Negative of the Pearson’s correlation co-
p2 (yt |θC )dytq R Yt
q R Yt
p2 (yt |θXt )dyt
efﬁcient (named NPCC): − cov(p(Yt |θC ),p(Yt |θXt ))
√var(p(Yt |θC ))var(p(Yt |θXt ))
We also evaluated the lower bound of the error rate with respect to each data set. To estimate the
lower bound, we conducted a 5-fold cross-validation on the test data U . Note that this strategy, which
is referred to as Lowerbound, is unavailable in our problem setting, since it uses a large amount of
labeled data in the target space. In our experiments, this lower bound is used just for reference. We
also note that on some data sets, the performance of Lowerbound may be slightly worse than that
of TLRisk, because Lowerbound was trained based on images in Caltech-256, while TLRisk
was based on the co-occurrence data. These models used different supervisory knowledge.

.

3.3 Experimental Results

The experimental results were evaluated in terms of error rates, and are shown in Figure 2. On
one hand, from the table, we can see that our framework TLRisk greatly outperforms the baseline
methods Image Only and Search+Image, no matter which dissimilarity function is applied.
On the other hand, compared with Lowerbound, TLRisk also shows comparable performance.
It indicates that our framework TLRisk can effectively learn knowledge across different feature
spaces in the case of text-to-image classi ﬁcation.

Moreover, when the number of target space labeled images decreases, the performance of Image
Only declines rapidly, while the performances of Search+Image and TLRisk stay very sta-

DATA S ET

1

2

3

4

5

ENGL I SH

LOCAT ION
Top: Sport: Ballsport
Top: Computers: Internet
Top: Arts: Architecture: Building Types
Top: Home: Cooking: Recipe Collections
Top: Science: Agriculture
Top: Society: Crime
Top: Sports: Skating: Roller Skating
Top: Health: Public Health and Safety
Top: Recreation: Outdoors: Hunting
Top: Society: Holidays

S I ZE
2000
2000
1259
475
1886
1843
926
2361
2919
2258

G ERMAN

LOCAT ION
Top: World: Deutsch: Sport: Ballsport
Top: World: Deutsch: Computer: Internet
Top: World: Deutsch: Kultur: Architektur: Geb ¨audetypen
Top: World: Deutsch: Zuhause: Kochen: Rezeptesammlungen
Top: World: Deutsch: Wissenschaft: Agrarwissenschaften
Top: World: Deutsch: Gesellschaft: Kriminalit ¨at
Top: World: Deutsch: Sport: Rollsport
Top: World: Deutsch: Gesundheit: Public Health
Top: World: Deutsch: Freizeit: Outdoor: Jagd
Top: World: Deutsch: Gesellschaft: Fest ´und Feiertage

S I ZE
128
126
71
72
71
69
70
71
70
72

Table 2: The description for each cross-language classi ﬁca tion data set.
ble. This indicates that TLRisk is not quite sensitive to the size of Lt ; in other words, TLRisk
has good robustness. We also want to note that, sometimes TLRisk performs slightly better than
Lowerbound. This is not a mistake, because these two methods use different supervisory knowl-
edge: Lowerbound is based on images in the Caltech-256 corpus; TLRisk is based on the co-
occurrence data. In these experiments, Lowerbound is just for reference.

In TLRisk, a parameter to tune is the trade off parameter λ (refer to Equation (6)). Figure 3 shows
the average error rate curves on all the 12 data sets, when λ gradually changes from 2−5 to 25 .
In this experiment, we ﬁxed the number of target training ima ges per category to one, and set the
threshold K (which is the number of images to collect for each text keyword, when collecting the
co-occurrence data) to 40. From the ﬁgure, we can see that, on one hand, when λ is very large, which
means the classi ﬁcation model mainly builds on the target sp ace training images Lt , the performance
is rather poor. On the other hand, when λ is small such that the classi ﬁcation model relies more on
the auxiliary text training data Ls , the classi ﬁcation performance is relatively stable. Ther efore, we
suggest to set the trade-off parameter λ to a small value, and in these experiments, all the λs are set
to 1, based on Figure 3.

4 Evaluation: Cross-language Classiﬁcation
In this section, we apply our framework TLRisk to another scenario, the cross-language classi ﬁ-
cation. We focused on English-to-German classi ﬁcation, wh ere English documents are used as the
source data to help classify German documents, which are target data.

In these experiments, we collected the documents from corresponding categories from ODP English
pages and ODP German pages, and generated ﬁve cross-languag e classi ﬁcation tasks, as shown in
Table 2. For the co-occurrence data, we used the English-German dictionary from the Internet Dic-
tionary Project2 (IDP). The dictionary data are in the form of feature-level co-occurrence p(yt , ys ).
We note that while most cross-language classi ﬁcation works
rely on machine translation [1], our
assumption is that the machine translation is unavailable and we rely on dictionary only.

We evaluated TLRisk with the negative of cosine (named NCOS) as the dissimilarity function. Our
framework TLRisk was compared to classi ﬁcation using only very few German lab eled documents
as a baseline, called German Labels Only. We also present the lower bound of error rates by
performing 5-fold cross-validation on the test data U , which we refer to as Lowerbound. The
performances of the evaluated methods are presented in Table 3. In this experiment, we have only
sixteen German labeled documents in each category. The error rates in Table 3 were evaluated
by averaging the results of 20 random repeats. From the ﬁgure , we can see that TLRisk always
shows marked improvements compared with the baseline method German Labels Only, al-
though there are still gaps between TLRisk and the ideal case Lowerbound. This indicates our
algorithm TLRisk is effective on the cross-language classi ﬁcation problem.

DATA S E T
German Labels Only
TLRisk
Lowerbound

1
0.246 ± 0.061
0.191 ± 0.045
0.170 ± 0.000

2
0.133 ± 0.037
0.122 ± 0.043
0.116 ± 0.000

3
0.301 ± 0.067
0.253 ± 0.062
0.157 ± 0.000

4
0.257 ± 0.053
0.247 ± 0.059
0.176 ± 0.000

5
0.277 ± 0.068
0.183 ± 0.072
0.166 ± 0.000

Table 3: The average error rate and variance on each data set, given by all the evaluation methods,
for English-to-German cross-language classi ﬁcation.

We have empirically tuned the trade-off parameter λ. Similar to the results of the text-aided image
classi ﬁcation experiments, when λ is small, the performance of TLRisk is better and stable. In

2http://www.ilovelanguages.com/idp/index.html

these experiments, we set λ to 2−4 . However, due to space limitation, we cannot present the curves
for λ tuning here.

5 Related Work
We review several prior works related to our work. To solve the label sparsity problem, researchers
proposed several learning strategies, e.g. semi-supervised learning [13] and transfer learning [3,
11, 10, 9, 4]. Transfer learning mainly focuses on training and testing processes being in different
scenarios, e.g. multi-task learning [3], learning with auxiliary data sources [11], learning from
irrelevant categories [10], and self-taught learning [9, 4]. The translated learning proposed in this
paper can be considered as an instance of general transfer learning; that is, transfer learning from
data in different feature spaces.

Multi-view learning addresses learning across different feature spaces. Co-training [2] established
the foundation of multi-view learning, in which the classi ﬁ ers in two views learn from each other
to enhance the learning process. Nigam and Ghani [8] proposed co-EM to apply EM algorithm to
each view, and interchange probabilistic labels between different views. Co-EMT [7] is an active
learning multi-view learning algorithm, and has shown more robustness empirically. However, as
discussed before, multi-view learning requires that each instance should contain two views, while in
translated learning, this requirement is relaxed. Translated learning can accept training data in one
view and test data in another view.

6 Conclusions
In this paper, we proposed a translated learning framework for classifying target data using data
from another feature space. We have shown that in translated learning, even though we have very
little labeled data in the target space, if we can ﬁnd a bridge to link the two spaces through feature
translation, we can achieve good performance by leveraging the knowledge from the source data.
We formally formulated our translated learning framework using risk minimization, and presented
an approximation method for model estimation. In our experiments, we have demonstrated how this
can be done effectively through the co-occurrence data in TLRisk. The experimental results on
the text-aided image classi ﬁcation and the cross-language classi ﬁcation show that our algorithm can
greatly outperform the state-of-the-art baseline methods.

Acknowledgement We thank the anonymous reviewers for their greatly helpful comments.
Wenyuan Dai and Gui-Rong Xue are supported by the grants from National Natural Science Foun-
dation of China (NO. 60873211) and the MSRA-SJTU joint lab project “Transfer Learning and its
Application on the Web”. Qiang Yang thanks the support of Hon g Kong CERG Project 621307.

References
[1] N. Bel, C. Koster, and M. Villegas. Cross-lingual text categorization. In ECDL, 2003.
[2] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998.
[3] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
[4] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Self-taught clustering. In ICML, 2008.
[5] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information
retrieval. In SIGIR, 2001.
[6] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer
Vision, 60(2):91–110, 2004.
[7] I. Muslea, S. Minton, and C. Knoblock. Active + semi-supervised learning = robust multi-view learning.
In ICML, 2002.
[8] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In CIKM, 2000.
[9] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: transfer learning from unlabeled
data. In ICML, 2007.
[10] R. Raina, A. Ng, and D. Koller. Constructing informative priors using transfer learning. In ICML, 2006.
[11] P. Wu and T. Dietterich. Improving svm accuracy by training on auxiliary data sources. In ICML, 2004.
[12] Y. Yang and J. Pedersen. A comparative study on feature selection in text categorization. In ICML, 1997.
[13] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, University of Wisconsin-
Madison, 2007.


Reconciling Real Scores with Binary Comparisons:
A Uni ﬁed Logistic Model for Ranking

Nir Ailon
Google Research NY
111 8th Ave, 4th FL New York NY 10011 nailon@gmail.com

Abstract

The problem of ranking arises ubiquitously in almost every aspect of life, and
in particular in Machine Learning/Information Retrieval. A statistical model for
ranking predicts how humans rank subsets V of some universe U . In this work we
deﬁne a statistical model for ranking that satis ﬁes certain
desirable properties.
The model automatically gives rise to a logistic regression based approach to
learning how to rank, for which the score and comparison based approaches are
dual views. This offers a new generative approach to ranking which can be used
for IR.
There are two main contexts for this work. The ﬁrst is the theo ry of econometrics
and study of statistical models explaining human choice of alternatives. In this
context, we will compare our model with other well known models. The second
context is the problem of ranking in machine learning, usually arising in the con-
text of information retrieval. Here, much work has been done in the discriminative
setting, where different heuristics are used to deﬁne ranki ng risk functions.
Our model is built rigorously and axiomatically based on very simple desirable
properties deﬁned locally for comparisons, and automatica lly implies the exis-
tence of a global score function serving as a natural model parameter which can
be efﬁciently ﬁtted to pairwise comparison judgment data by
solving a convex
optimization problem.

1

Introduction

Ranking is an important task in information sciences. The most notable application is information
retrieval (IR), where it is crucial to return results in a sorted order for the querier. The subject of
preference and ranking has been thoroughly studied in the context of statistics and econometric
theory [8, 7, 29, 36, 34, 31], combinatorial optimization [26, 37, 20, 3, 4, 14] and machine learning
[6, 9, 33, 21, 19, 35, 23, 22, 25, 16, 17, 1, 13, 15, 28, 18].

Recently Ailon and Mehryar [5] following Balcan et al [9] have made signi ﬁcant progress in reduc-
ing the task of learning ranking to the binary classi ﬁcation problem of learning preferences. This
comparison based approach is in contrast with a score based approach which tries to regress to a
score function on the elements we wish to rank, and sort the elements based on this score as a ﬁnal
step.

The difference between the score based and comparison approaches is an example of ”local vs.
global ” views: A comparison is local (how do two elements com pare with each other), and a score
is global (how do we embed the universe on a scale). The score based approach seems reasonable
in cases where the score can be deﬁned naturally in terms of me asurable utility. In some real world
scenarios, either (i) an interpretable score is difﬁcult to deﬁne (e.g. a relevance score in information
retrieval) and (ii) an interpretable score is easy to deﬁne ( e.g. how much a random person is willing

to pay for product X in some population) but learning the score is difﬁcult due to noisy or costly
label acquisition for scores on individual points [7].

A well known phenomenon in the psychological study of human choice seems to potentially offer an
elegant solution to the above difﬁculties: Human response t o comparison questions is more stable in
the sense that it is not easily affected by irrelevant alternatives. This phenomenon makes acquisition
of comparison labels for learning tasks more appealing, but raises the question of how to go back
and ﬁt a latent score function that explains the comparisons . Moreover, the score parameter ﬁtting
must be computationally efﬁcient. Much effort has been rece ntly put in this subject from a machine
learning perspective [6, 9, 33, 21, 19, 35, 23, 22, 25, 16, 17, 1, 13, 15, 28, 18].

2 Ranking in Context

The study of ranking alternatives has not been introduced by ML/IR, and has been studied throughly
from the early years of the 20th century in the context of statistics and econometrics. We mention
work in ML/IR by Lebanon and Lafferty [27] and Cao et al. [12] who also draw from the classic
work for information retrieval purposes.

ML/IR is usually interested in the question of how a machine should correctly rank alternatives
based on experience from human feedback, whereas in statistics and econometrics the focus is on
the question of how a human chooses from alternatives (for the purpose of e.g. effective marketing
or policy making). Therefore, there are notable differences between the modern and classic foci.
Notwithstanding these differences, the classic foci is relevant to modern applications, and vice versa.
For example, any attempt to correctly choose from a set (predominantly asked in the classic context)
can be converted into a ranking algorithm by repeatedly choosing and removing from the set.

Deﬁnition 2.1 A ranking model for U is a function D mapping any ﬁnite subset V ⊆ U to a
distribution on rankings of V . In other words, D(V ) is a probability distribution on the |V |! possible
orderings of V .

A Thurstonian model for ranking (so named after L. Thurstone [36]) is one in which an independent
random real valued variable Zv is associated with each v ∈ V , and the ranking is obtained by sorting
the elements if V in decreasing order (assuming the value represents utility). Often the distributions
governing the Zv ’s are members of a parametric family, with a location parameter representing an
intrinsic ”value”. The source of variability in Zv is beyond the scope of this work. This model is
related to the more general random utility model (RUM) approach studied in econometrics.

A purely comparison based model is due to Babington and Smith: The parameter of the model is a
matrix {puv }u,v∈U . Given items u, v , a subject would prefer u over v with probability puv = 1−pvu .
Given a subset V , the subject ﬂips a corresponding biased coin independentl y to decide on the
preference of all pairs u, v ∈ V , and repeats the process until the set of preferences is transitive. This
model is unwieldy in full generality, and more succinct representations were proposed. Mallows [30]
following Bradley and Terry [11] proposed to take puv as α(u)/(α(u) + α(v)), where the α(v)’s
are constants attached to each element. Note that the marginal probability of u being preferred over
v in the context of a set V ⊃ {u, v} in the Babington-Smith model is in general not puv , even in
Mallows’s special case.

In distance based models it is assumed that there is a ”modal ”
ranking of the set V , and the prob-
ability of any ranking decreases with its distance from the mode. Several deﬁnitions of distances
between permutations. Often the probability density itself is deﬁned as an exponential model. We
refer the reader to [31] for in depth analysis of such models.
The Plackett-Luce model. The classic model most related to this work is Plackett and Luce’s [29,
34] multistage model for ranking. Each element v ∈ U has an assigned ”value” parameter α(v). At
each stage a choice is made. Given a set V , item u ∈ V wins with probability α(u)/ Pv∈V α(v).1
The winner is removed from V and the process is repeated for the remaining elements, until a
ranking is obtained. Yellott [38] made the surprising observation that the Luce-Plackett model is
exactly Thurstone’s model where the Zu ’s are translated Gumbel (doubly-exponential) distributed

1This choice function is known as the multinomial logit (MNL) and is equivalent to the standard (dichoto-
mous) logit when only two alternatives are available.

variables. The underlying winner choice model satis ﬁes Luc e’s choice axiom [29] which, roughly
speaking, stipulates that the probability of an element u winning in V is the same as the product of
the probability of the winner contained in V ′ ⊆ V and the probability of u winning in V ′ . It turns
out that this axiom (often used as criticism of the model) implies the underlying choice function of
the Plackett-Luce model.

An interesting property of Plackett-Luce for our purpose is that it is asymmetric in the sense that it
is winner-centric and not loser-centric. The model cannot explain both ranking by successive loser
choice and successive winner choice simultaneously unless it is trivial (this point was noticed by
McCullagh [32]). It is clear however that breaking down the process of ranking by humans to an
iterated choice of winners ignores the process of elimination (placing alternatives at the bottom of
the list). In the following sections we propose a new symmetric model for ranking, in which the basic
discrete task is a comparison of pairs of elements, and not choice of an element from arbitrarily large
sets (as in Plackett-Luce).

3 An Axiomatic Approach for De ﬁning a Pairwise-Stable Model f or Ranking

For a ranking π of some subset V ⊆ U , we use the notation u ≺π v to denote that u precedes2 v
according to π . We let π(v) ∈ {1, . . . , n} denote the rank of v ∈ V , where lower numbers designate
precedence (hence u ≺π v if π(u) < π(v)). The inverse π−1 (i) is the unique element v of V with
π(v) = i. We overload notation and let π(u, v) denote the indicator variable taking the value of 1 if
u ≺ v and 0 otherwise.

Deﬁnition 3.1 A ranking model D for U satis ﬁes pairwise stability if for any u, v ∈ U and for any
V1 , V2 ⊇ {u, v}, Prπ∼D(V1 ) [u ≺π v ] = Prπ∼D(V2 ) [u ≺ v ].

Pairwise stability means that the preference (or comparison) of u, v is statistically independent of
the context (subset) they are ranked in. Note that Plackett-Luce is pairwise stable (this follows
from the fact that the model is Thurstonian) but Babington-Smith/Mallows is not.
If a ranking
model D satis ﬁes pairwise stability, then the probability PrD [u ≺ v ] is naturally deﬁned and equals
Prπ∼D(V ) [u ≺π v ] for any V ⊇ {u, v}.
Pairwise stability is a weak property which permits a very wide family of ranking distributions. In
particular, if the universe U is a ﬁnite set then any distribution Π on rankings on the entire universe
U gives rise to a model DΠ with DΠ (V ) deﬁned as the restriction of Π to V . This model clearly
satis ﬁes pairwise stability but does not have a succint desc ription and hence undesirable.

We strengthen the conditions on our model by considering triplets of elements. Assume that a
model D satis ﬁes pairwise stability. Fix three elements u, v , w . Consider a process in which we
randomly and independently decide how u and w should compare with v . What would be the
induced distribution on the order of u and w , conditioned on them being placed on opposite sides
of v? If we sample from the distributions D({u, v}) and D({v , w}) to independently decide how to
compare u with v and w with v (respectively), then we get

Pr[u ≺ w |( u ≺ v ≺ w) ∨ (w ≺ v ≺ u)] =
PrD [u ≺ v ] PrD [v ≺ w]
PrD [u ≺ v ] PrD [v ≺ w] + PrD [w ≺ v ] PrD [v ≺ u]

.

What happens if we force this to equal PrD [u ≺ w]? In words, this would mean that the comparison
of u with w conditioned on the comparison being determined by pivoting around v is distributed like
D({u, w}). We write this desired property as follows (the second line follows from the ﬁrst):

2We choose in this work to use the convention that an element u precedes v if u is in a more favorable
position. When a score function is introduced later, the convention will be that higher scores correspond to
more favorable positions. We will use the symbol < (resp. >) to compare scores, which is semantically
opposite to ≺ (resp. ≻) by our convention.

Pr
[u ≺ w] =
D

Pr
[w ≺ u] =
D

PrD [u ≺ v ] PrD [v ≺ w]
PrD [u ≺ v ] PrD [v ≺ w] + PrD [w ≺ v ] PrD [v ≺ u]
PrD [w ≺ v ] PrD [v ≺ u]
PrD [w ≺ v ] PrD [v ≺ u] + PrD [u ≺ v ] PrD [v ≺ w]

.

(1)

Deﬁnition 3.2 Assume D is a ranking model for U satisfying pairwise stability. For a pair u, w ∈ U
and another element v ∈ U we say that u and w satisfy the pivot condition with respect to v if (1)
holds.

Dividing the two desired equalities in (1), we get (assuming the ratio exists):

PrD [u ≺ w]
PrD [w ≺ u]

=

PrD [u ≺ v ] PrD [v ≺ w]
PrD [w ≺ v ] PrD [v ≺ u]

.

(2)

If we denote by ∆D (a, b) the ”comparison logit 3 ”: ∆D (a, b) = log(PrD [a ≺ b]/ PrD [b ≺ a]) , then
(2) implies ∆D (u, v) + ∆D (v , w) + ∆D (w, u) = 0 . This in turn implies that there exist numbers
s1 , s2 , s3 such that ∆(u, v) = s1 − s2 , ∆(v , w) = s2 − s3 and ∆(w, u) = s1 − s3 . These numbers,
deﬁned up to any additive constant, should be called (additi ve) scores. We will see in what follows
that the score function can be extended to a larger set by patching scores on triplets.

By the symmetry it is now clear that the pivoting condition of u and w with respect to v implies the
pivoting condition of u and v with respect to w and of v and w with respect to u. In other words, the
pivoting condition is a property of the triplet {u, v , w}.

Deﬁnition 3.3 Assume a ranking model D for U satis ﬁes pairwise stability, and let ∆D : U × U →
R denote the comparison logit as deﬁned above. A triplet {u, v , w} ⊆ U is said to satisfy the pivot
condition in D if ∆D (u, v) + ∆D (v , w) + ∆D (w, u) = 0 . We say that U satis ﬁes the pivot condition
in D if {u, v , w} satis ﬁes the pivot condition for all {u, v , w} ⊆ U .

Lemma 3.1 If U satis ﬁes the pivot condition in a pairwise stability model D for U , then there exists
a real valued score function s : V → R such that for all a, b ∈ V , ∆D (a, b) = s(a) − s(b) .

Proof Fix some element v ∈ U and set s(v) = 0. For every other element u ∈ V \ {v} set
s(v) = ∆D (v , u). It is now immediate to verify that for all a, b ⊆ V one has ∆D (a, b) = s(a)−s(b).
Indeed, by construction s(a) − s(b) = ∆D (a, u) − ∆D (b, u) but by the pivot property this equals
exactly ∆D (a, b), as required (remember that ∆D (a, b) = −∆D (a, b) by deﬁnition of ∆D ).

By starting with local assumptions (pairwise stability and the pivoting property), we obtained a nat-
ural global score function s on the universe of elements. The score function governs the probability
of u preceding v via the difference s(u) − s(v) passed through the inverse logit. Note that we used
the assumption that the comparison logit is ﬁnite on all u, v (equivalently, that 0 < PrD (u ≺ v) < 1
for all u, v ), but this assumption can be dropped if we allow the score function to obtain values in
R + ωZ, where ω is the limit ordinal of R.
The Plackett-Luce model satis ﬁes both pairwise stability a nd the pivot condition with s(u) =
log α(u). Hence our deﬁnitions are non empty. Inspired by recent work on the QuickSort algo-
rithm [24] as a random process [4, 3, 5, 37], we deﬁne a new symmetric model based on a series of
comparisons rather than choices from sets.

4 The New Ranking Model

We deﬁne a model called QSs (short for QuickSort), parametrized by a score function s : U 7→ R
as follows. Given a ﬁnite subset V ⊂ U :

1. Pick a ”pivot element ”

v uniformly at random from V .

3The ”logit of p” is standard shorthand for the log-odds, or

log(p/(1 − p)).

2. For all u ∈ V \ {v}, place u to the left of v with probability 1/(1 + es(v)−s(u) ), and to the
right with the remaining probability 1/(1 + es(u)−s(v) ), independently of all other choices.
3. Recurse on the left and on the right sides, and output the ranking of V obtained by joining
the results in an obvious way (left ≺ pivot ≺ right).

(The function 1/(1 + e−x ) is the inverse logit function.) We shall soon see that QuickSort gives
us back all the desired statistical local properties of a ranking models. That the model QSs can
be sampled efﬁciently is a simple consequence of the fact tha t QuickSort runs in expected time
O(n log n) (some attention needs to be paid the fact that unlike in the textbook proofs for QuickSort
[5]).
the pivoting process is randomized, but this is not difﬁcult

Theorem 4.1 The ranking model QSs for U satis ﬁes both pairwise stability and the pivoting condi-
tion. Additionally, for any subset V ⊆ U the mode of QSs (V ) is any ranking π∗ satisfying u ≺π∗ v
whenever s(u) > s(v).

Proof (of Theorem 4.1): First we note that if QSs satis ﬁes pairwise stability, then the pivot prop-
erty will be implied as well. Indeed, by taking V = {u, v} we would get from the model that
PrQSs (u ≺ v) = 1/(1 + es(v)−s(u) ), immediately implying the pivot property.
To see that QSs satis ﬁes pairwise stability, we show that for any u, v and V ⊇ {u, v}, the probability
of the event u ≺π v is exactly 1/(1 + es(v)−s(u) ), where π ∼ QSs (V ). Indeed, the order of u, v
can be determined in one of two ways. (i) Directly: u or v are chosen as pivot when the other is
present in the same recursive call. We call this event E{u,v} . Conditioned on this event, clearly the
probability that u ≺π v is exactly the required probability 1/(1 + es(v)−s(u) ) by step 2 of QuickSort
(note that it doesn’t matter which one of v or u is the pivot). (ii) Indirectly: A third element w ∈ V
is the pivot when both u and v are present in the recursive call, and w sends u and v to opposite
recursion sides. We denote this event by E ′
{u,v},w . Conditioned on this event, the probability that
u ≺π v , is exactly as required (by using the same logit calculus we used in Section 3).
To conclude the proof of pairwise stability, it remains to observe that the collection of events
{E{u,v} } ∪ nE ′
{u,v},w : w ∈ V \ {u, v}o is a pairwise disjoint cover of the probability space. This
implies that Prπ∼QSs (V ) (u ≺π v) is the desired quantity 1/(1 + es(v)−s(u) ), concluding the proof
of pairwise stability.

We need to work harder to prove the intuitive mode argument. Let τ , σ be two permutations on V
such that

a1 ≺τ a2 ≺τ · · · ≺τ ak ≺τ u ≺τ v ≺τ ak+1 ≺τ · · · ≺τ an−2
a1 ≺ a2 ≺σ · · · ≺σ ak ≺σ v ≺σ u ≺σ ak+1 ≺σ · · · ≺σ an−2 ,
where V = {u, v}∪{a1 , . . . , an−2 }. In words, τ and σ differ on the order of exactly two consecutive
elements u, v . Assume that s(u) > s(v) (so τ , placing u in a more favorable position than v , is
intuitively more ”correct ”). We will prove that the probabi
lity of getting τ is strictly higher than the
probability of getting σ from QSs . Since π∗ , the permutation sorting by s, can be obtained from
any permutation by a sequence of swapping incorrectly ordered (according to s) adjacent pairs, this
would prove the theorem by a standard inductive argument.
Let qτ = Prπ∼QS [π = τ ], and similarly deﬁne qσ . To prove that qτ > qσ we need extra notation.
Our QuickSort generative model gives rise to a random integer node-labeled ordered binary tree4
implicitly constructed as an execution side effect. This tree records the ﬁnal position of the pivots
chosen in each step as follows: The label L of the root of the tree is the rank of the pivot in the ﬁnal
solution (which equals the size of the left recursion plus 1). The left subtree is the tree recursively
constructed on the left, and the right subtree is the tree recursively constructed on the right with
L added to the labels of all the vertices. Clearly the resulting tree has exactly n nodes with each
label in {1 . . . n} appearing exactly once. Let pπ ,T denote the probability that QuickSort outputs a
permutation π and (implicitly) constructs a pivot selection tree T . Let T denote the collection of
all ordered labeled binary trees with node labels in {1, . . . , n}. For T ∈ T and a node x ∈ T let
ℓ(x) denote the integer label on x. Let Tx denote the subtree rooted by x and let ℓ(Tx ) denote the

4By that we mean a tree in which each node has at most one left child node and at most one right child node,
and the nodes are labeled with integers.

collection of labels on those nodes. By construction, if QuickSort outputted a ranking π with an
(implicitly constructed) tree T , then at some point the recursive call to QuickSort took π−1 (ℓ(Tx ))
as input and chose π−1 (ℓ(x)) as pivot, for any node x of T . By a standard probability argument
(summing over a disjoint cover of events): qτ = PT ∈T qτ ,T and qσ = PT ∈T qσ,T . It sufﬁces to
show now that for any ﬁxed T ∈ T , qτ ,T > qσ,T . To compute qπ ,T for π = τ , σ we proceed as
follows: At each node x of T we will attach a number Pπ (x) which is the likelihood of the decisions
made at that level, namely, the choice of the pivot itself and the separation of the rest of the elements
to its right and left.

Pπ (x) =

1
|Tx | Y
y∈TL (x)

[π−1 (ℓ(y)) ≺ π−1 (ℓ(x))] × Y
Pr
QS
y∈TR (x)

[π−1 (ℓ(x)) ≺ π−1 (ℓ(y))] ,
Pr
QS

Where |Tx | is the number of nodes in Tx , TR (x) is the set of vertices in the left subtree of x and sim-
ilarly for TL (x). The factor 1/|Tx | comes from the likelihood of uniformly at random having chosen
the pivot π−1 (ℓ(x)) from the set of nodes of Tx . The ﬁrst product corresponds to the random com-
parison decisions made on the elements thrown to the left, and the second to right. By construction,
pτ ,T = Qx∈T Pτ (x) and similarly pσ,T = Qx∈T Pσ (x). Since u, v are adjacent in both τ and σ , it is
clear that the two nodes x1 , x2 ∈ T labeled τ (u) and τ (v) respectively have an ancestor-descendent
relation in T (otherwise their least common ancestor in T would have been placed between them,
violating the consecutiveness of u and v in our construction and implying pτ ,T = qτ ,T = 0). Also
recall that σ(u) = τ (v) and σ(v) = τ (u). By our assumption that τ and σ differ only on the order
of the adjacent elements u, v , Pτ (x) and Pσ (x) could differ only on nodes x on the path between
x1 and x2 . Assume w.l.o.g. that x1 is an ancestor of x2 , and that x2 is a node in the left subtree of
x1 . By our construction, x2 is the rightmost node5 in TL (x1 ). Let Y denote the set of nodes on the
path from x1 to x2 (exclusive) in T . Let W denote the set of nodes in the left (and only) subtree
of x2 , and let Z denote the set of remaining nodes in TL (x1 ): Z = TL (x1 ) \ (W ∪ Y ∪ {x2}).
Since τ −1 (ℓ(z )) = σ−1 (ℓ(z )) for all z ∈ Z we can deﬁne elt(z ) = τ −1 (ℓ(z )) = σ−1 (ℓ(z )) and
similarly we can correspond each y ∈ Y with a single element elt(y) and each w ∈ W with a single
elements elt(w) of V . As claimed above, we only need to compare between Pτ (x1 ) and Pσ (x1 ),
between Pτ (x2 ) and Pσ (x2 ) and Pτ (y) and Pσ (y) for y ∈ Y . Carefully unfolding these products
node by node, we see that it sufﬁces to notice that for all y ∈ Y , the probability of throwing elt(y)
to the left of u (pivoting on u) times the probability of throwing v to the right of elt(y) (pivoting
on elt(y)) as appears inside the product Pσ (x1 )Pσ (y) is exactly the probability of throwing elt(y)
to the left of v (pivoting on v ) times the probability of throwing u to the right of elt(y) (pivoting on
elt(y)) as appears inside the product Pτ (x1 )Pτ (y). Also for all w ∈ W the probability of throw-
ing elt(w) to the left of u (pivoting on u) times the probability of throwing elt(w) to the left of v
(pivoting on v ) appears exactly once in both Pτ (x1 )Pτ (x2 ) and Pσ (x1 )Pσ (x2 ) (though in reversed
order). Following these observations one can be convinced by the desired result of the theorem by
noting that in virtue of s(u) > s(v): (i) PrQS [v ≺ u] > PrQS [u ≺ v ], and (ii) for all z ∈ Z ,
PrQS [elt(z ) ≺ u] > PrQS [elt(z ) ≺ v ].

5 Comparison of Models

The stochastic QuickSort model as just deﬁned as well as Plac kett-Luce share much in common,
but they are not identical for strictly more than 2 elements. Both satisfy the intuitive property that
the mode of the distribution corresponding to a set V is any ranking which sorts the elements of
V in decreasing s(v) = log α(v) value. The stochastic QuickSort model, however, does not suffer
from the asymmetry problem which is often stated as a criticism of Plackett-Luce.
Indeed, the
distributions QSs (V ) has the following property: If we draw from QSs (V ) and ﬂip the resulting
permutation, the resulting distribution is QS−s (V ). This property does not hold in general for
Plackett-Luce, and hence serves as proof of their nonequivalence.
Assume we want to ﬁt s in the MLE sense by drawing random permutations from QSs (V ). This
seems to be difﬁcult due to the unknown choice of pivot. On the other hand, the log-likelihood
function corresponding to Plackett-Luce is globally concave in the values of the function s on V ,
and hence a global maximum can be efﬁciently found. This also holds true in a generalized linear
model, in which s(v) is given as the dot product of a feature vector φ(v) with an unknown weight

5The rightmost node of T is the root if it has no right descendent, or the rightmost node of its right subtree.

vector which we estimate (as done in [10] in the context of predicting demand for electric cars).
Hence, for the purpose of learning given full permutations of strictly more than two elements, the
Plackett-Luce model is easier to work with.

In practical IR settings, however, it is rare that training data is obtained as full permutations: such
a task is tiresome. In most applications, the observables used for training are in the form of bi-
nary response vectors (either relevant or irrelevant for each alternative) or comparison of pairs of
alternatives (either A better or B better given A,B). For the latter, Plackett-Luce is identical to Quick-
Sort, and hence efﬁcient ﬁtting of parameters is easy (using
logistic regression). As for the former,
the process of generating a binary response vector can be viewed as the task performed at a single
QuickSort recursive level. It turns out that by deﬁning a nui sance parameter to represent the value s
of an unknown pivot, MLE estimation can be performed efﬁcien tly and exactly [2].

References

[1] Shivani Agarwal and Partha Niyogi. Stability and generalization of bipartite ranking algo-
rithms. In COLT, pages 32–47, 2005.
[2] N. Ailon. A simple linear ranking algorithm using query dependent intercept variables.
arXiv:0810.2764v1.
[3] Nir Ailon. Aggregation of partial rankings, p-ratings and top-m lists. In SODA, 2007.
[4] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: rank-
ing and clustering. In Proceedings of the 37th Annual ACM Symposium on Theory of Comput-
ing, Baltimore, MD, USA, May 22-24, 2005, pages 684–693. ACM, 2005.
[5] Nir Ailon and Mehryar Mohri. An efﬁcient reduction of ran king to classi ﬁcation. In COLT,
2008.
[6] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A
unifying approach for margin classi ﬁers.
Journal of Machine Learning Research, 1:113–141,
2000.
[7] D. Ariely, G. Loewenstein, and D. Prelec. Coherent arbitrariness: Stable demand curves with-
out stable preferences. The Quarterly Journal of Economics, 118(1):73–105, 2008.
[8] K. J. Arrow. A difﬁculty in the concept of social welfare.
Journal of Political Economy,
58(4):328–346, August 1950.
[9] Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford,
and Gregory B. Sorkin. Robust reductions from ranking to classi ﬁcation. In Nader H. Bshouty
and Claudio Gentile, editors, COLT, volume 4539 of Lecture Notes in Computer Science, pages
604–619. Springer, 2007.
[10] S. Beggs and S. Cardell. Assessing the potential demand for electric cars. Journal of Econo-
metrics, 17:1–19, 1981.
[11] R.A. Bradley and M.A. Terry. Rank analysis of incomplete block designs. Biometrika, 39:324–
345, 1952.
[12] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise
approach to listwise approach. In ICML ’07: Proceedings of the 24th international conference
on Machine learning, pages 129–136, New York, NY, USA, 2007. ACM.
[13] William W. Cohen, Robert E. Schapire, and Yoram Singer. Learning to order things. J. Artif.
Intell. Res. (JAIR), 10:243–270, 1999.
[14] D. Coppersmith, Lisa Fleischer, and Atri Rudra. Ordering by weighted number of wins gives
a good ranking for weighted tournamnets.
In Proceedings of the 17th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), 2006.
[15] Corinna Cortes and Mehryar Mohri. AUC Optimization vs. Error Rate Minimization.
In
Advances in Neural Information Processing Systems (NIPS 2003), volume 16, Vancouver,
Canada, 2004. MIT Press.
[16] Corinna Cortes, Mehryar Mohri, and Ashish Rastogi. An Alternative Ranking Problem for
Search Engines. In Proceedings of the 6th Workshop on Experimental Algorithms (WEA 2007),
volume 4525 of Lecture Notes in Computer Science, pages 1–21, Rome, Italy, June 2007.
Springer-Verlag, Heidelberg, Germany.

[17] Corinna Cortes, Mehryar Mohri, and Ashish Rastogi. Magnitude-Preserving Ranking Algo-
rithms. In Proceedings of the Twenty-fourth International Conference on Machine Learning
(ICML 2007), Oregon State University, Corvallis, OR, June 2007.
[18] David Cossock and Tong Zhang. Subset ranking using regression. In COLT, pages 605–619,
2006.
[19] Koby Crammer and Yoram Singer. Pranking with ranking. In Thomas G. Dietterich, Suzanna
Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems
14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December
3-8, 2001, Vancouver, British Columbia, Canada], pages 641–647. MIT Press, 2001.
[20] Ronald Fagin, Ravi Kumar, Mohammad Mahdian, D. Sivakumar, and Erik Vee. Comparing
and aggregating rankings with ties. In Alin Deutsch, editor, Proceedings of the Twenty-third
ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, June 14-16,
2004, Paris, France, pages 47–58. ACM, 2004.
[21] Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An efﬁcient boosting algo-
rithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.
[22] Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra, and Klaus Obermayer. Learning a pref-
erence relation in ir. In In Proceedings Workshop Text Categorization and Machine Learning,
International Conference on Machine Learning, pages 80–84, 1998.
[23] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries for ordinal
regression. In Advances in Large Margin Classi ﬁers , pages 115–132, 2000.
[24] C.A.R. Hoare. Quicksort: Algorithm 64. Comm. ACM, 4(7):321–322, 1961.
[25] Thorsten Joachims. Optimizing search engines using clickthrough data. In KDD ’02: Pro-
ceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 133–142, New York, NY, USA, 2002. ACM Press.
In STOC ’07:
[26] Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors.
Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 95–
103, New York, NY, USA, 2007. ACM Press.
[27] Guy Lebanon and John D. Lafferty. Cranking: Combining rankings using conditional prob-
ability models on permutations. In ICML ’02: Proceedings of the Nineteenth International
Conference on Machine Learning, pages 363–370, San Francisco, CA, USA, 2002. Morgan
Kaufmann Publishers Inc.
[28] Erich L. Lehmann. Nonparametrics: Statistical Methods Based on Ranks. Holden-Day, San
Francisco, California, 1975.
[29] R.D. Luce. Individual choice behaviour. Wiley, 1959.
[30] C.L. Mallows. Non-null ranking models. Biometrika, 44:113–130, 1957.
[31] John I. Marden. Analyzing and modeling rank data. Chapman & Hall, 1995.
[32] P. McCullagh. Permutations and regression models. Probability models and statistical analyses
for ranking data, pages 196–215, 1993.
[33] Mark H. Montague and Javed A. Aslam. Condorcet fusion for improved retrieval. In Pro-
ceedings of the 2002 ACM CIKM International Conference on Information and Knowledge
Management, McLean, VA, USA, November 4-9, 2002, pages 538–548. ACM, 2002.
[34] R. L. Plackett. The analysis of permutations. Applied Statistics, 24:193–202.
[35] Cynthia Rudin, Corinna Cortes, Mehryar Mohri, and Robert E. Schapire. Margin-based rank-
ing meets boosting in the middle.
In Peter Auer and Ron Meir, editors, Learning Theory,
18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005,
Proceedings, pages 63–78. Springer, 2005.
[36] L. L. Thurstone. A law of comparative judgement. Psychological Reviews, 34:273–286.
[37] David P. Williamson and Anke van Zuylen. ”deterministi c algorithms for rank aggregation and
other ranking and clustering problems ”. In Proceedings of the 5th Workshop on Approximation
and Online Algorithms (WAOA) (to appear), 2007.
[38] J. Yellott. The relationship between luce’s choice axiom, thurstone’s theory of comparat-
ice judgment, and the double exponential distribution. Journal of Mathematical Psychology,
15:109–144, 1977.


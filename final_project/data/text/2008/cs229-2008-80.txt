CS229 Final Pro ject
One Click: Ob ject Removal

Ming Jiang

Nicolas Meunier

December 12, 2008

1 Introduction

In this pro ject, our goal is to come up with an algorithm that can automatically detect the contour
of an ob ject selected by the user and remove this ob ject from the image by replacing it with a
plausible estimate of the background. The algorithm mainly consists of two steps of tasks. The
ﬁrst one is to detect the contour of an ob ject in an image. Given a pixel in the image chosen by the
user, the algorithm should be able to deﬁne a mask surrounding the entire ob ject containing the
selected pixel. We do not need to ﬁnd the boundaries of all ob jects in the image but just to ﬁnd
a mask that contains at least all the pixels of an ob ject that we would like to remove. The second
important task is to remove properly this ob ject from the image. For this task, given an entire
mask containing pixels to be replaced, we aim to produce an algorithm that can precisely replace
the ob ject by a visually pleasant estimate of the background behind it.

2 Foreground Detection

Our entire goal being to remove an ob ject from an image, we ﬁrst need to be able to deﬁne a
mask that represent our ob ject and that we would like to remove. In other terms, given a simple
click by the user, we need to ﬁgure out what ob ject he wants to remove and automatically deﬁne
a mask that surrounds this ob ject. As we want to work on the widest possible class of ob jects,
we decided not to implement any kind of ob ject detection that would limit our algorithm to the
learnt ob jects. We ﬁnally came up with the idea to deﬁne the mask by preprocessing our image in
superpixels (cluster of coherent pixels) and then asking the user to choose one of them that is inside
the ob ject he wants to remove. It then remains the task to deﬁne all the superpixels that are inside
the ob ject. For the superpixel extraction, we used the method of Ren and Malik with the available
code at http://www.cs.sfu.ca/~mori/research/superpixels/code/ and we then developed the
following algorithm based on a simpliﬁed segmentation classiﬁer.

2.1 Principles of the algorithm

In order to compute the entire mask, we decided to build a classiﬁer that learns if a superpixel
is part of the foreground (so is potentially part of an ob ject we would like to remove) or part of
the background (and then we don’t want to add this superpixel to the mask as it could contain
important information for the image inpainting task). This is a simple segmentation problem where
we only have to distinguish between two possibilities for a superpixel.
To build this classiﬁer, we ﬁrst need to extract features from the superpixel that will help us
distinguish between background and foreground. The features we used in our algorithm contain
some color statistics over the superpixels, texture information based on diﬀerent ﬁlterings (wavelet
ﬁlters), geometry information about the superixels and their spatial location in the image. Then

1

we implemented two learning methods based on these features to classify the superpixels: logistic
regression and SVM. We used groundtruth information about the foreground and background of
images as our training data (around 15000 groundtruth superpixels taken from 100 images). After
training, we evaluate our algorithm on a separate set of testing data (around 15000 superpixels)
and produce the estimated foreground mask for each image. First the mask contains all predicted
foreground superpixels. Then based on the user’s selection and adjacency information of the su-
perpixels, we create the ﬁnal mask which contains only the connected components around the user
selection. Figure 1 illustrates the algorithm process. You can ﬁnd another example in Figure 8.

Figure 1: Process of generating foreground mask

2.2 Comparison and use of the diﬀerent classiﬁers

We used two classiﬁers for learning: logistic regression and SVM (with diﬀerent kernels). To compare
the performance of these two methods, we used the following three measures:

1. Accuracy: the rate of correct predictions made by the model over a data set.

2. Precision: the number of true positives (i.e. the number of items correctly labeled as belonging
to the class) divided by the total number of elements labeled as belonging to the class (i.e.
the sum of true positives and false positives).

3. Recall: the number of true positives divided by the total number of elements that actually
belong to the class (i.e. the sum of true positives and false negatives).

There are two big issues that are worth noticing for our algorithm. One is that there will be
many more negative data (i.e. the superpixels belonging to background) than positive data (i.e.
the superpixels belonging to foreground). That means we cannot judge the performance of our
classiﬁers only by looking at its accuracy. As our goal is to detect the foreground ob jects, it is more
important to focus on the precision and recall measures than accuracy. Table 1 gives the comparison
result between logistic regression and SVM with a linear kernel for a non-biased decision threshold.

Table 1: Comparison between logistic regression and SVM
Logistic
SVM
0.8804
0.9016
0.7555
0.6414
0.4662
0.4655

Accuracy
Precision
Recall

The other important issue is that for the image inpainting algorithm, we can tolerate a mask
bigger than the original ob ject. That means we can sacriﬁce the accuracy and precision to some
extent for a higher recall. To change the recall, we can adjust the threshold parameter T of

2

boundary decision. We will label foreground superpixels with a probability/margin larger than T
and background otherwise. Figure 2 illustrates the precision-recall curve.

Figure 2: Precision-Recall Curve

Finally we choose a threshold parameter 0.3 as opposed to the normal 0.5 for testing in our
algorithm, which produces a precision of 53.51% and a recall of 73.45%.

3 Image Inpainting

For the inpainting part itself, we now have a mask that has been ﬁlled in by the previous algorithm
and that we want to remove from the image. The mask is considered as a missing region of the
image that we want to ﬁll in such a way that the result will seem visually natural. We do not intend
to restore any hidden ob ject in our pro ject, just to replace an ob ject by a plausible background.
Many algorithms have been developed over the last years, using:

1. statistical-based methods - given the texture surrounding the mask, this approach tries to
extract interesting statistics for those texture and then reproduce them inside the mask area.
The typical problem of this kind of approach is that it works well when we are only working
with textures.

2. PDE-based methods - given information surrounding the mask, we try to propagate it inside
the mask using a diﬀusion process. This process is generally simulated by solving a partial
diﬀerential equation (PDE). A problem of this approach is that the mask generally has to be
quite thin and the information quite smooth to give a visual natural solution. So, when the
missing region is highly textured or corresponds to a large ob ject, those methods would lead
to a blurry solution.

3. Examplar-based methods - This is the most known successful techniques for inpainting pur-
pose. These methods ﬁll the mask by copying content from the known part of the image,
thus considering that the missing information inside the mask can be found elsewhere in the
image.

For this pro ject, we decided to use an examplar-based inpainting approach and to improve it by
using machine learning to deﬁne optimal metrics used in the algorithm. The challenge here is to be

3

0.10.20.30.40.50.60.70.80.90.40.450.50.550.60.650.70.750.80.850.90.20.250.30.350.40.450.50.550.60.650.70.750.8RecallPrecisionPrecision−Recall Curveable to ﬁll the removed ob ject by a plausible background that ﬁts well with the rest of the image.
We want it to preserve edges inside the region and we want it to use appropriate information from
the background to do so. For this purpose, we implemented the algorithm in [1].

Figure 3: principle of the algorithm

We have a mask Ω that we want to remove from the image. We will progressively ﬁll the mask
with information coming from the rest of the image. To do so, we will use a template window that
will ﬁnd the closest patch in the image (outside the mask) to a given patch ψp on the border of
our mask. This closest patch in the image will be used to ﬁll the missing information of the border
patch inside the mask. To compute this closest patch, we will deﬁne our own metric that is deﬁned
in the next section.
For every patch ψp around the mask we deﬁne a priority order in which we will ﬁll the mask
based on the parameters deﬁned in ﬁgure 3. We modiﬁed the original priority function in [1]
using diﬀerent features, like presence of edges around a patch, and presence of diﬀerent kind of
segmentation, using the algorithm in [3] and the Stair Vision Library [2].

3.1 Comparing patch and propagating information inside the mask

If we want to choose a correct patch to replace part of the mask, we need a way to compare patches.
In the original algorithm, only color diﬀerence is used to compute distinction between diﬀerent
patches. We purpose here to use a learning algorithm to deﬁne a new metric to pick up patches
for replacing. For two given patches, we can deﬁne a small set of diﬀerent features that are some
characteristics of their diﬀerence:
1. the sum of absolute diﬀerence of each color channel

2. the sum of square diﬀerence of each color channel

3. the number of pixels with a diﬀerent segmentation

4. the diﬀerence of entropy in the segmentation
We then have a set of features S = {Fi}1≤i≤n and we can deﬁne a function that permits us to
(cid:80)n
compare two patches as follows :
i=1 wiFi (P1 , P2 )
f (P1 , P2 ) =
For a given starting patch around the mask we can select a potential replacing patch and run
our inpainting algorithm starting with this patch. We then score the ﬁnal result comparing its
color and segmentation with the original image. For each patch tried as a potential solution we
can then attribute a score. By this method, we can then create a training database on which to
run a learning algorithm (here a logistic regression) to maximize our ﬁnal metric. The results of
this approach gives almost equal importance to the diﬀerent features and with the learnt metric,
it permits to avoid reconstructing unwanted ob jects inside the mask as it could sometimes happen
with the original algorithm. This is why using the segmentation features to compare patches was
useful in our approach. You can see some examples of our inpainting algorithm in ﬁgure 6 and 9.

4

Figure 4: Original image

Figure 5: Mask

Figure 6: Inpainted image

Figure 7: Original image

Figure 8: Mask

Figure 9: Inpainted image

4 Conclusion

We have presented an algorithm to detect the contour of an ob ject selected by the user and remove
it from the image in a visually pleasant way. For foreground detection, we applied and compared
logistic regression and SVM for the learning process, and also adjusted the threshold parameter
for decision boundary to trade oﬀ between the precision and recall rate. For image inpainting, our
results always do at least as good as the original algorithm and improves it in some diﬃcult cases
where the original algorithm fails. We think that improving the results can not be done without
using a more global approach, or without having access to higher level information, concerning
context or ob jects surrounding the mask.

Acknowledgements

We would like to thank John Bauer, Geremy Heitz, Stephen Gould and Daphne Koller for their
contribution to the inpainting algorithm. We would also like to thank Professor Andrew Ng and
the TAs for their advice and suggestions.

References

[1] A.Criminisi and P.Perez. Ob ject removal by exemplar-based inpainting. Computer Vision and
Pattern Recognition, 2003.

[2] Stephen Gould, Andrew Y. Ng,
and Daphne Koller.
http://ai.stanford.edu/ sgould/svl. 2008.

The

stair vision library,

[3] Stephen Gould, Jim Rodgers, David Cohen, Gal Elidan, and Daphne Koller. Multi-class seg-
mentation with relative location prior. International Journal of Computer Vision (IJCV), 2008.

5


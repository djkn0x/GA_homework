On Bootstrapping the ROC Curve

Patrice Bertail
CREST (INSEE) & MODAL’X - Universit ´e Paris 10
pbertail@u-paris10.fr

St ´ephan Cl ´emenc¸ on
Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141
stephan.clemencon@telecom-paristech.fr

Nicolas Vayatis
ENS Cachan & UniverSud - CMLA UMR CNRS 8536
vayatis@cmla.ens-cachan.fr

Abstract

This paper is devoted to thoroughly investigating how to bootstrap the ROC curve,
a widely used visual tool for evaluating the accuracy of test/scoring statistics in
the bipartite setup. The issue of conﬁdence bands for the ROC curve is considered
and a resampling procedure based on a smooth version of the empirical distribu-
tion called the ”smoothed bootstrap” is introduced. Theoretical arguments and
simulation results are presented to show that the ”smoothed bootstrap” is prefer-
able to a ”naive” bootstrap in order to construct accurate conﬁdence bands.

1

Introduction

Since the seminal contribution of [14], so-called ROC curves (ROC standing for Receiving Oper-
ator Characteristic) have been extensively used in a wide variety of applications (anomaly detec-
tion in signal analysis, medical diagnosis, search engines, credit-risk screening) as a visual tool for
evaluating the performance of a test statistic regarding its capacity of discrimination between two
populations, see [8]. Whereas the statistical properties of their empirical counterparts have been
only lately studied from the asymptotic angle, see [18, 13, 11, 16], ROC curves also have recently
received much attention in the machine-learning literature through the development of statistical
learning procedures tailored for the ranking problem, see [10, 2]. The latter consists of determining,
based on training data, a test statistic s(X ) (also called a scoring function) with a ROC curve ”as
high as possible” at all points of the ROC space. Given a candidate s(X ), it is thus of prime impor-
tance to assess its performance by computing a conﬁdence band for the corresponding ROC curve,
in a data-driven fashion preferably. Indeed, in such a functional setup, resampling-based procedures
should naturally be preferred to those relying on computing/simulating the (gaussian) limiting dis-
tribution, as ﬁrst observed in [19, 21, 20], where the use of the bootstrap is promoted for building
conﬁdence bands in the ROC space.
By building on recent works, see [17, 12], it is the purpose of this paper to investigate how the
bootstrap approach should be practically implemented based on a thorough analysis of the asymp-
totic properties of empirical ROC curves. Beyond the pointwise analysis developed in the studies
mentioned above, here we tackle the problem from a functional angle, considering the entire ROC
curve or parts of it. This viewpoint indeed appears as particularly relevant in scoring applications.
Although the asymptotic results established in this paper are of a theoretical nature, they are con-
siderably meaningful from a computational perspective. It turns out indeed that smoothing is the

1

key ingredient for the bootstrap conﬁdence band to be accurate, whereas a naive bootstrap approach
would yield bands of low coverage probability in this case and should be consequently avoided by
practicioners for analyzing ROC curves.
The rest of the paper is organized as follows. In Section 2, notations are ﬁrst set out and certain
key notions of ROC analysis are brieﬂy recalled. The choice of an adequate (pseudo-)metric on the
ROC space, a crucial point of the analysis, is also considered. The smoothed bootstrap algorithm
is presented in Section 3, together with the theoretical results establishing its asymptotic accuracy
as well as preliminary simulation results illustrating the impact of smoothing on the bootstrap per-
formance. In Section 4, the gain in terms of convergence rate acquired by the smoothing step is
thoroughly discussed. We refer to [1] for technical proofs.

2 Background

Here we brieﬂy recall basic concepts of the bipartite ranking problem as well as key results related
to the statistical estimation of ROC curves. We also set out the notations that shall be needed
throughout the paper. Although the results contained in this paper can be formulated without
referring to the bipartite ranking framework, in the purpose of motivating the present analysis
we intentionally connected them to this major statistical learning problem, which has recently
revitalized the interest for the problem of assessing the accuracy of empirical ROC curves, see [4].

2.1 Assumptions and notation
In the bipartite ranking problem, the problem is to order all the elements X of a set X by degree
of relevance, when relevancy may be observed through some binary indicator variable Y . Precisely,
one has a system consisting of a binary random output Y , taking its values in {−1, 1} say, and a
random input X , taking its values in a (generally high-dimensional) feature space X , which models
some observation for predicting Y . The probabilistic model is the same as for standard binary
classiﬁcation but the prediction task is different. In the case of information retrieval for instance, the
goal is to order all documents x of the list X by degree of relevance for a particular request (rather
than simply classifying them as relevant or not as in classiﬁcation). This amounts to assigning to
each document x in X a score s(x) indicating its degree of relevance for this speciﬁc query. The
challenge is thus to build a scoring function s : X → R from sampling data, so as to rank the
observations x by increasing order of their score s(x) as accurately as possible: the higher the score
s(X ) is, the more likely one should observe Y = +1.
True ROC curves. A standard way of measuring the ranking performance consists of plotting the
ROC curve, namely the graph of the mapping
ROCs : α ∈ (0, 1) (cid:55)→ 1 − (Gs ◦ H −1
s )(1 − α),
where Gs (respectively Hs ) denotes s(X )’s cdf conditioned on Y = +1 (resp. conditioned on
Y = −1) and F −1 (α) = inf {x ∈ R/ F (x) ≥ α} the generalized inverse of any cdf F on R. It
boils down to plotting the true positive rate versus the false positive rate when testing the assumption
”H0 : Y = −1” based on the statistic s(X ). This functional performance measure induces a partial
order on the set of scoring functions, according to which it may be shown, by standard Neyman-
Pearson’s arguments, that increasing transforms of the regression function η(x) = P(Y = +1 |
X = x) are the optimal scoring functions (the test statistic η(X ) is uniformly more powerful, i.e.
∀α ∈ (0, 1), ROCη (α) ≥ ROCs (α), for any scoring function s(x)).
Empirical ROC curve estimates. Practical learning strategies for selecting a good scoring func-
tion are based on training data Dn = {(Xi , Yi )}1≤i≤n and should thus rely on accurate empirical
estimates of the true ROC curves. Let p = P(Y = +1). For any scoring function candidate s(X ),
∀α ∈ (0, 1), (cid:91)ROCs (α) = 1 − (cid:98)Gs ◦ (cid:98)H −1
an empirical counterpart of ROCs is naturally obtained by computing
s (1 − α)
n(cid:88)
n(cid:88)
from empirical cdf estimates:
(cid:98)Gs (x) =
I{Yi=+1}K (x − s(Xi )) and (cid:98)Hs (x) =
1
n+
i=1
i=1

I{Yi=−1}K (x − s(Xi )),

1
n−

2

where n+ = (cid:80)n
I{Yi = +1} = n − n− is the (random) number of positive instances among
order to obtain smoothed versions (cid:101)Gs (x) and (cid:101)Fs (x) of the latter cdfs, a typical choice consists of
picking instead a function K (u) of the form (cid:82)
i=1
the sample (distributed as the binomial B in(n, p)) and K (u) denotes the step function I{u≥0} . In
such that (cid:82) K (v)dv = 1) and h > 0 is the smoothing bandwidth, see Remark 1 for a practical view
v≥0 Kh (u − v)dv , with Kh (u) = h−1K(h−1 · u)
where K ≥ 0 is a regularizing Parzen-Rosenblatt kernel (i.e. a bounded square integrable function
of smoothing. Here and throughout, I{A} denotes the indicator function of any event A.
Metrics on the ROC space. When it comes to measure closeness between curves in the ROC
space, various metrics may be used, see [9]. Viewing the ROC space as a subset of the Skorohod’s
space D([0, 1]) of c `ad-l `ag functions f : [0, 1] → R, the standard metric induced by the sup norm
||.||∞ appears as a natural choice. As shall be seen below, asymptotic arguments for grounding
the bootstrapping of the empirical ROC curve ﬂuctuations, when measured in terms of the sup
norm ||.||∞ , are rather straightforward. However, given the geometry of empirical ROC curves,
this metric is not always convenient for our purpose and may produce very wide, and thus non
informative conﬁdence bands. For analyzing stepwise graphs, such as empirical ROC curves, we
shall consider the closely related pseudo-metric deﬁned as follows:
∀(f1 , f2 ) ∈ D([0, 1])2 , dB (f1 , f2 ) = sup
dB (f1 , f2 ; t),
t∈[0,1]
where dB (f1 , f2 ; t) = min{|f1 (t) − f2 (t)|, |f −1
◦ f2 (t) − t|. We clearly have
◦ f1 (t) − t|, |f −1
dB (f1 , f2 ) ≤ ||f1 − f2 ||∞ . The major advantage of considering this pseudo-metric is that it provides
2
1
a control on vertical and horizontal jumps of ROC curves both at the same time, treating both
types of error in a symmetric fashion. Equipped with this pseudo-metric, two piecewise constant
ROC curves may be close to each other, even if their jumps do not exactly match. This is clearly
appropriate for describing the ﬂuctuations of the empirical ROC curve (and the deviation between
the latter and its bootstrap counterpart as well). This way, dB permits to construct builds bands of
reasonable size, well adapted to the stepwise shape of empirical ROC curves, with better coverage
probabilities. In this respect, the closely related Hausdorff distance (i.e. the distance between the
graphs completed by linear segments at jump points) would also be a pertinent choice. However,
providing a theoretical basis in the case of the Hausdorff distance is very challenging and will not
be addressed in this paper, owing to space limitations.
As the goal pursued in the present paper is to build, in the ROC space viewed as a subspace of
the Skorohod’s space D([0, 1]) equipped with a proper (pseudo-) metric, a conﬁdence band for
the ROC curve of a given diagnosis test statistic s(X ), we shall omit to index by s the quan-
tities considered and denote by Z the r.v. s(X ) (and by Zi , 1 ≤ i ≤ n, the s(Xi )’s) for
notational simplicity. Throughout the paper, we assume that H (dx) and G(dx) are continuous
probability distributions, with densities h(x) and g(x) respectively. Eventually, denote by P the
joint distribution of (Z, Y ) on R × {−1, +1} and by Pn its empirical version based on the sam-
ple Dn = {(Zi , Yi )}1≤i≤n . Equipped with the notations above, one may write P (dz , y) =
pI{y=+1}G(dz ) + (1 − p)I{y=−1}H (dz ).

2.2 Asymptotic law - Gaussian approximation

√
In the situation described above, the next theorem establishes the strong consistency of the empirical
ROC curve in sup norm and provides a strong approximation at the rate 1/
n, up to logarithmic
factors, for the ﬂuctuation process:
√

n((cid:91)ROCn (α) − ROC(α)), α ∈ [0, 1].
rn (α) =
This (gaussian) approximation plays a crucial role in understanding the asymptotic behavior of the
empirical ROC curve and of its bootstrap counterpart. The following assumptions are required.
H1 The slope of the ROC curve is bounded: supα∈[0,1]{g(H −1 (α))/h(H −1 (α))} < ∞.
H2 H is twice differentiable on [0, 1]. Furthermore, ∀α ∈ [0, 1], h(α) > 0 and there exists
γ > 0 such that supα∈[0,1]{α(1 − α) · d log(h ◦ H −1 (α))/dα} ≤ γ < ∞.
Theorem. 1 (FUNC T IONA L L IM I T TH EOR EM) Suppose that H1 − H2 are fulﬁlled. Then,

3

(i) the empirical ROC curve is strongly consistent:
|(cid:91)ROCn (α) − ROC(α)| → 0 a.s. as n → ∞,
sup
α∈[0,1]
(ii) there exist a sequence of two independent brownian bridges {(B (n)
(cid:16)
1
such that we almost surely have, uniformly over [0, 1],

(α), B (n)
(cid:17)
2
√

n

,

(α))}α∈[0,1]

(1)

rn (α) = z (n) (α) + o

(log log n)ρ1 (γ ) (log n)ρ2 (γ ) )/

where

and

(ROC(α))

z (n) (α) = (1 − p)−1/2 g(H −1 (1 − α))
(α) + p−1/2B (n)
h(H −1 (1 − α)) B (n)
(cid:40) ρ1 (γ ) = 0, ρ2 (γ ) = 1, if γ < 1
1
2
ρ1 (γ ) = 0, ρ2 (γ ) = 2, if γ = 1
ρ1 (γ ) = γ , ρ2 (γ ) = γ − 1 + ε, ε > 0, if γ > 1
These results may be immediately derived from classical strong approximations for the empirical
√
and quantile processes, see [5, 18]). Incidentally, we mention that the approximation rate is not
always log2 (n)/
n, contrarily to what is claimed in [18].
We point out that, owing to the presence of the term (g/h)(H −1 (1 − α)) in it, the gaussian approx-
imant can hardly be used for constructing ROC conﬁdence bands. To avoid explicit computation of
density estimates, bootstrap conﬁdence sets should be certainly preferred in practice.

.

3 Bootstrapping empirical ROC curves

Beyond consistency of the empirical curve in sup norm and the asymptotic normality of the ﬂuctu-
ation process, we now tackle the question of constructing conﬁdence bands for the true ROC curve
via the bootstrap approach introduced by [6], extending pointwise results established in [17]. The
latter suggests to consider, as an estimate of the law of the ﬂuctuation process rn = {rn (α)}α∈[0,1] ,
the conditional law given Dn of the bootstrapped ﬂuctuation process
n = {√
n(ROC∗ (α) − (cid:91)ROC(α))}α∈[0,1] ,
r∗
(2)
pairs with a common distribution (cid:101)Pn close to Pn . We shall also consider
where ROC ∗ is the ROC curve corresponding to a sample D∗
n = {(Z ∗
i )}1≤i≤n of i.i.d. random
i , Y ∗
√
ndB (ROC∗ , (cid:91)ROC),
d∗
n =
(3)
√
ndB ((cid:91)ROC, ROC).
whose random ﬂuctuations, given Dn , are expected to mimic those of dn =
The difﬁculty is twofold. Firstly, the target of the bootstrap procedure is here a distribution on a path
dB (., .). Secondly, both rn and dn are functionals of the quantile process { (cid:98)H −1 (α)}α∈[0,1] . It is
space, the ROC space being viewed as a subspace of Dn ([0, 1]), equipped with either ||.||∞ or else
well-known that the naive bootstrap (i.e. resampling from the raw empirical distribution) generally
provides bad approximations of the distribution of empirical quantiles in practice: the rate of con-
vergence for a given quantile is indeed of order OP (n−1/4 ), see [7], whereas the rate of the gaussian
approximation is n−1/2 . As shall be seen below, the same phenomenon may be naturally observed
for ROC curves. In a similar fashion to what is generally recommended for empirical quantiles, we
suggest to implement a smoothed version of the bootstrap algorithm in order to improve the approx-
imation rate of ||rn ||∞ ’s distribution, respectively of dn ’s distribution . In short, this boils down to
resampling the data from a smoothed version of the empirical distribution Pn .
3.1 The Algorithm
sampling data Dn = {(Zi , Yi ); 1 ≤ i ≤ n}. Set n+ = (cid:80)
Here we describe the algorithm for building a conﬁdence band at level 1 −  in the ROC space from
I{Yi=1} = n − n− . It is performed
1≤i≤n
in four steps as follows.

4

1. Based on Dn , compute the empirical class cdf estimates (cid:98)G and (cid:98)H , as well as their
A LGOR I THM - SMOOTH ED ROC BOOT STRA P
smoothed versions (cid:101)G and (cid:101)H . Plot the ROC curve estimate:
(cid:92)ROC (α) = 1 − (cid:98)G ◦ (cid:98)H −1 (1 − α), α ∈ [0, 1].
I{y=+1} (cid:101)G(dz ) + n+
I{y=−1} (cid:101)H (dz ),
(cid:101)Pn (dz , y) = n−
2. From the smooth distribution estimate
n
n
draw a bootstrap sample D∗
n = {(Z ∗
i )}1≤i≤n conditioned on Dn .
i , Y ∗
3. Based on D∗
n , compute the bootstrap versions of the empirical class cdf estimates G∗
and H ∗ . Plot the bootstrap ROC curve
ROC∗ (α) = 1 − G∗ ◦ H ∗−1 (1 − α), α ∈ [0, 1].
4. Eventually, get the bootstrap conﬁdence bands at level 1− deﬁned by the ball of center
√
(cid:91)ROC and radius δ/
n ||∞ ≤ δ ) = 1 − 
n in D([0, 1]), where δ is deﬁned by P∗ (||r∗
n ≤ δ ) = 1 − , when considering the dB
in the case of the sup norm or by P∗ (d∗
distance, denoting by P∗ (.) the conditional probability given the original data Dn .

Before turning to the theoretical properties of this algorithm and related numerical experiments, a
few remarks are in order.

Remark 1 (MONT E -CARLO A P PROX IMAT ION) From a computational angle, the true smoothed
bootstrap distribution must be approximated in its turn, using a Monte-Carlo approximation scheme.
A convenient way of doing this in practice, while reproducing theoretical advantages of smoothing,
(this procedure is equivalent to drawing bootstrap data from a smooth estimate (cid:101)Pn (dz , dy) com-
consists of drawing B bootstrap samples, of size n, with replacement in the original data and then
perturbating each drawn data by independent centered gaussian random variables of variance h2
puted using a gaussian kernel Kh (u) = (2πh2 )−1/2 exp(−u2 /(2h2 ))), see [22]. Regarding the
choice of the number of bootstrap replications, picking B = n does not modify the rate of conver-
gence. However, choosing B of magnitude comparable to n so that (1 + B ) is an integer may be
more appropriate: the -quantile of the approximate bootstrap distribution is the uniquely deﬁned
and this will not modify the rate of convergence neither, see [15].

Remark 2 (ON TUN ING PARAM ET ER S) The primary tuning parameters of the Algorithm are those
related to the smoothing stage. When using a gaussian regularizing kernel, one should typically
choose a bandwidth hn of order n−1/5 in order to minimize the mean square error.
lent to recenter by a smoothed version of the original empirical curve (cid:93)ROC(.) = 1 − (cid:101)G ◦ (cid:101)H −1 (1 − .)
Remark 3 (ON R ECEN TER ING) From the asymptotic analysis viewpoint, it would be fairly equiva-
in the computation of the bootstrap ﬂuctuation process. However, numerically speaking, computing
the sup norm of the estimate (2) is much more tractable, insofar as it solely requires to evaluate the
distance between piecewise constant curves over the pooled set of jump points. It should also be
noticed that smoothing the original curve, as proposed in [17], should be also avoided in practice,
since it hides the jump locations, which constitute the essential part of the information.

3.2 Asymptotic analysis

We now investigate the accuracy of the bootstrap estimate output by the Algorithm. The result stated
in the next theorem extend those established in [17] in the pointwise framework. The functional
nature of the approximation result below is essential, since it should be enhanced that, in most
ranking applications, assessing the uncertainty about the whole estimated ROC curve, or some part
of it at least, is what really matters. In the sequel, we assume that the kernel K used in the smoothing
step is ”pyramidal” (e.g. gaussian or of the form I{u∈[−1,+1]} ).

5

Assume further that smoothed versions of the cdf ’s (cid:101)G and (cid:101)H are computed at step 1 using a scaled
Theorem. 2 (A SYM P TOT IC ACCURACY) Suppose that the hypotheses of Theorem 1 are fulﬁlled.
kernel Khn (u) with hn ↓ 0 as n → ∞ in a way that nh3
n → ∞ and nh5
n log2 n → 0. Then, the
(cid:18) log(h−1
bootstrap distribution estimates output by the Algorithm are such that
n )
√
|P∗ (d∗
n ≤ t) − P(dn ≤ t)| are of order oP
n ||∞ ≤ t) − P(||rn ||∞ ≤ t)| and sup
|P∗ (||r∗
sup
t∈R
t∈R
nhn
Hence, up to logarithmic factors, choosing hn ∼ 1/(log2+η n1/5 ) with η > 0 yields an approxima-
tion error of order n−2/5 for the bootstrap estimate. Although its rate is slower than the one of the
gaussian approximation (1), the smoothed bootstrap method remains very appealing from a compu-
tational perspective, the construction of conﬁdence bands from simulated brownian bridges being
very difﬁcult to implement in practice. As shall be seen below, the rate reached by the smoothed
bootstrap distribution is nevertheless a great improvement, compared to the naive bootstrap approach
(see the discussion below).

(cid:19)

.

Remark 4 (BOOT STRA P P ING SUMMARY STAT I S T IC S) From Theorem 1 above, asymptotic validity
of the smooth bootstrap method for estimating the distribution of the ﬂuctuations of a functional
Φ((cid:91)ROC) of the empirical ROC curve may be deduced, as soon as the function Φ deﬁned on D([0, 1])
is sufﬁciently smooth (namely continuously Hadamard differentiable). For instance, it could be
applied to summary statistics involving a speciﬁc piece of the ROC curve only in order to focus on
the ”best instances” [3], or more classically to the area under the ROC curve (AUC). However, in
the latter case, due to the fact that this particular summary statistic is of the form of a U -statistic
[2], the naive bootstrap rate is faster than the one we obtained here (of order n−1 ).

3.3 Simulation results

The striking advantage of the smoothed bootstrap is the improved rate of convergence of the resulting
estimator. Furthermore, choosing dB for measuring the magnitude order of curve ﬂuctuations has an
even larger impact on the accuracy of the empirical bands. As an illustration of this theoretical result,
we now display simulation results, emphasizing the gain acquired by smoothing and considering the
pseudo-metric dB .
We present conﬁdence bands for a single trajectory and the estimation of the coverage probability
of the bands for a simple binormal model:
Yi = +1 if β0 + β1X + ε > 0, and Yi = −1 otherwise,
where ε and X are independent standard normal r.v.’s. In this example, the scoring function s(x)
is the maximum likelihood estimator of the probit model on the training set. We choose here
β0 = β1 = 1, n = 1000, B = 999 and γ = 0.95 for the targeted coverage probability. Cov-
erage probabilities are obtained over 2000 replications of the procedure, using the package ROCR of
statistical software R. As mentioned before, choosing ||.||∞ yields very large bands with coverage
probability close to 1! Though still large, bands based on the pseudo-metric dB are clearly much
more informative (see Fig. 1). It should be noticed that the coverage improvement obtained by
smoothing is clearer in the pontwise estimation setup (here α = 0.2) but much more difﬁcult to
evidence for conﬁdence bands.

Table 1: Empirical coverage probabilities for 95% empirical bands/intervals.

M E THOD
NA IVE BOOT STRA P ||rn ||∞
SMOOTH ED BOOT STRA P ||rn ||∞
NA IVE BOOT STRA P dn
SMOOTH ED BOOT STRA P dn
NA IVE BOOT STRA P rn (0.2)
SMOOTH ED BOOT STRA P rn (0.2)

COVERAG E (%)
100
100
90 .3
93 .1
89 .7
92 .5

6

Figure 1: ROC conﬁdence bands.

4 Discussion

Let us now give an insight into the reason why the smoothed bootstrap procedure outperforms the
bootstrap without smoothing.
In most statistical problems where the nonparametric bootstrap is
useful, there is no particular reason for implementing it from a smoothed version of the empirical df
from the raw cdf ’s (cid:98)G and (cid:98)H instead of their smoothed versions at step 2 of the Algorithm. Then, for
rather from the raw empirical distribution itself, see [22]. However, in the present case, smoothing
affects the rate of convergence. Suppose indeed that the bootstrap process (2) is built by drawing
n (α) ≤ t) − P(rn (α) ≤ t)| = OP (n−1/4 ). Hence, the naive bootstrap
any α ∈]0, 1[, supt∈R |P∗ (r∗
rate induces an error of order O(n−1/4 ) which cannot be improved, whereas it may be shown that
the rate n−2/5 is attained by the smoothed bootstrap (in a similar fashion to the functional setup),
provided that the amount of smoothing is properly chosen. Heuristically, this is a consequence of the
value (cid:98)H −1 (1 − α) given the data Dn , due to the fact that the step cdf (cid:98)H is not regular around
oscillation behavior of the deviation between the bootstrap quantile H ∗−1 (1 − α) and its expected
(cid:98)H −1 (1 − α): this corresponds to a jump with probability one.
Higher-order accuracy. A classical way of improving the pointwise approximation rate consists of
bootstrapping a standardized version of the r.v. rn (α). It is natural to consider, as standardization
factor, the square root of an estimate of the asymptotic variance:
σ2 (α) = var(z (n) (α)) = α(1 − α)
ROC(α)(1 − ROC(α))
g(H −1 (1 − α))2
h(H −1 (1 − α))2 +
(4)
1 − p
.
An estimate (cid:98)σ2
p
smoothed density estimators ˜h = (cid:101)H (cid:48) and ˜g = (cid:101)G(cid:48) into (4) instead of their (unknown) theoretical
n of plug-in type could be considered, obtained by plugging n+/n, (cid:93)ROC and
counterparts. More interestingly, from a computational viewpoint, a bootstrap estimator of the vari-
ance could also be used. Following the argument used in [17] for a smoothed original estimate of
√
the ROC curve, one may show that a smoothed bootstrap of the studentized statistic rn (α)/σn (α)
yields a better pointwise rate of convergence than 1/
n, the one of the gaussian approximation in
the Central Limit Theorem. Precisely, for a given α ∈]0, 1[, if the bandwidth used in the computation

7

False positive rateTrue positive rate0.00.20.40.60.81.00.00.20.40.60.81.00.020.220.410.610.81Figure1:||.||∞conﬁdencebandFalse positive rateTrue positive rate0.00.20.40.60.81.00.00.20.40.60.81.00.010.210.410.60.81Figure2:dBconﬁdencebandFalse positive rateTrue positive rate0.00.20.40.60.81.00.00.20.40.60.81.00.010.210.410.60.81Figure3:Ponctualsmoothbootstrapconﬁdenceinterval,

(5)

(cid:19)

(cid:12)(cid:12)(cid:12)(cid:12)P∗ (cid:18) r∗
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) = OP
(cid:18) 1
(cid:19)
(cid:18) rn (α)
n (α) is chosen of order n−1/3 , we have:
of σ2
n (α)
≤ t
− P
≤ t
sup
σ∗
n (α)
σn (α)
t∈R
n2/3
n (α)’s bootstrap counterpart by σ∗2
n (α). Notice that the bandwidth used in the standard-
denoting σ2
ization step (i.e. for estimating the variance) is not the same as the one used at the resampling stage
of the procedure. This is a key point for achieving second-order accuracy. This time, the smoothed
(studentized) bootstrap method widely outperforms the gaussian approach, when the matter is to
build conﬁdence intervals for the ordinate (cid:91)ROC(α) of a point of abciss α on the empirical ROC
curve. However, it is not clear yet, whether this result remains true for conﬁdence bands, when
for the supremum ||rn/(cid:98)σn ||∞ ). This will be the scope of further research.
considering the whole ROC curve (this would actually require to establish an Edgeworth expansion
References
[1] P. Bertail, S. Cl ´emenc¸ on, and N. Vayatis. On constructing accurate conﬁdence bands for ROC curves
through smooth resampling, http://hal.archives-ouvertes.fr/hal-00335232/fr/. Technical report, 2008.
[2] S. Cl ´emenc¸ on, G. Lugosi, and N. Vayatis. Ranking and scoring using empirical risk minimization. Pro-
ceedings of COLT 2005, Eds P. Auer and R. Meir, LNAI 3559, Springer, 2005.
[3] S. Cl ´emenc¸ on and N. Vayatis. Ranking the best instances. Journal of Machine Learning Research,
5:197–227, 2007.
[4] W. Cohen, R. Schapire, and Y. Singer. Learning to order things. Journal of Artiﬁcial Intelligence Research,
10:243–270, 1999.
[5] M. Csorgo and P. Revesz. Strong approximations in probability and statistics. Academic Press, 1981.
[6] B. Efron. Bootstrap methods: another look at the jacknife. Annals of Statistics, 7:1–26, 1979.
[7] M. Falk and R. Reiss. Weak convergence of smoothed and nonsmoothed bootstrap quantile estimates.
Annals of Probability, 17:362–371, 1989.
[8] T. Fawcett. ROC graphs: Notes and practical considerations for data mining researchers. Technical Report
HPL 2003-4), 5:197–227, 2003.
[9] P. Flach. The geometry of roc space: understanding machine learning metrics through roc isometrics. In
T. Fawcett and N. Mishra, editors, Proc. 20th International Conference on Machine Learning (ICML’03),
AAAI Press, 86:194–201, 2003.
[10] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences.
Journal of Machine Learning Research, 4:933–969, 2003.
[11] P. Ghosal and J. Gu. Bayesian ROC curve estimation under binormality using a partial likelihood based
on ranks. Submitted for publication, 2007.
[12] P. Ghosal and J. Gu. Strong approximations for resample quantile process and application to ROC method-
ology. Submitted for publication, 2007.
[13] A. Girling. ROC conﬁdence bands: An empirical evaluation. Journal of the Royal Statistical Society,
Series B, 62:367–382, 2000.
[14] D. Green and J. Swets. Signal detection theory and psychophysics. Wiley, NY, 1966.
[15] P. Hall. On the number of bootstrap simulations required to construct a conﬁdence interval. Annals of
Statistics, 14:1453–1462, 1986.
[16] P. Hall and R. Hyndman.
Improved methods for bandwidth selection when estimating ROC curves.
Statistics and Probability Letters, 64:181–189, 2003.
[17] P. Hall, R. Hyndman, and Y. Fan. Nonparametric conﬁdence intervals for receiver operating characteristic
curves. Biometrika, 91:743–750, 2004.
[18] F. Hsieh and B. Turnbull. Nonparametric and semi-parametric statistical estimation of the ROC curve.
The Annals of Statistics, 24:25–40, 1996.
[19] S. Macskassy and F. Provost. Conﬁdence bands for ROC curves: methods and an empirical study. In
Proceedings of the ﬁrst Workshop on ROC Analysis in AI (ROCAI-2004) at ECAI-2004, 2004.
[20] S. Macskassy, F. Provost, and S. Rosset. Bootstrapping the ROC curve: an empirical evaluation.
Proceedings of ICML-2005 Workshop on ROC Analysis in Machine Learning (ROCML-2005), 2005.
[21] S. Macskassy, F. Provost, and S. Rosset. ROC conﬁdence bands: An empirical evaluation. In Proceedings
of the 22nd International Conference on Machine Learning (ICML-2005), 2005.
[22] B. Silverman and G. Young. The bootstrap: to smooth or not to smooth? Biometrika, 74:469–479, 1987.

In

8


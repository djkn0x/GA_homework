Near-Minimax Recursive Density Estimation
on the Binary Hypercube

Maxim Raginsky
Duke University
Durham, NC 27708
m.raginsky@duke.edu

Svetlana Lazebnik
UNC Chapel Hill
Chapel Hill, NC 27599
lazebnik@cs.unc.edu

Rebecca Willett
Duke University
Durham, NC 27708
willett@duke.edu

Jorge Silva
Duke University
Durham, NC 27708
jg.silva@duke.edu

Abstract

This paper describes a recursive estimation procedure for multivariate binary den-
sities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients
to estimate, which renders conventional approaches computationally prohibitive
when d is large. However, for a wide class of densities that satisfy a certain spar-
sity condition, our estimator runs in probabilistic polynomial time and adapts to
the unknown sparsity of the underlying density in two key ways: (1) it attains
near-minimax mean-squared error, and (2) the computational complexity is lower
for sparser densities. Our method also allows for ﬂexible co ntrol of the trade-off
between mean-squared error and computational complexity.

1 Introduction

Multivariate binary data arise in a variety of ﬁelds, such as biostatistics [1], econometrics [2] or
artiﬁcial intelligence [3].
In these and other settings, it
is often necessary to estimate a proba-
bility density from a number of independent observations. Formally, we have n i.i.d. samples
from a probability density f (with respect to the counting measure) on the d-dimensional bi-
= {0, 1}, and seek an estimate bf of f with a small mean-squared error
△
nary hypercube Bd , B
MSE(f , bf ) = E (cid:8) Px∈Bd (f (x) − bf (x))2 (cid:9).
In many cases of practical interest, the number of covariates d is much larger than log n, so direct
estimation of f as a multinomial density with 2d parameters is both unreliable and impractical. Thus,
one has to resort to “nonparametric ” methods and search for g ood estimators in a suitably de ﬁned
class whose complexity grows with n. Some nonparametric methods proposed in the literature, such
as kernels [4] and orthogonal expansions [5, 6], either have very slow rates of MSE convergence or
are computationally prohibitive for large d. For example, the kernel method [4] requires O(n2 d)
operations to compute the estimate at any x ∈ Bd , yet its MSE decays as O(n−4/(4+d) ) [7], which is
extremely slow when d is large. In contrast, orthogonal function methods generally have much better
MSE decay rates, but rely on estimating 2d coefﬁcients in a ﬁxed basis, which requires enormous
computational resources for large d. For instance, using the Fast Hadamard Transform to estimate
the coefﬁcients in the so-called Walsh basis using n samples requires O(nd2d ) operations [5].
In this paper we take up the problem of accurate, computationally tractable estimation of a density
on the binary hypercube. We take the minimax point of view, where we assume that f comes from
a particular function class F and seek an estimator that approximately attains the minimax MSE
MSE(f , bf ),
△
R∗
sup
= inf
n (F )
bf
f ∈F
where the in ﬁmum is over all estimators based on n i.i.d. samples. We will de ﬁne our function class
to re ﬂect another feature often encountered in situations i nvolving multivariate binary data: namely,

that the shape of the underlying density is strongly in ﬂuenc ed by small constellations of the d co-
variates. For example, when working with panel data [2], it may be the case that the answers to some
speciﬁc subset of questions are highly correlated among a pa rticular group of the panel participants,
and the responses of these participants to other questions are nearly random; moreover, there may
be several such distinct groups in the panel. To model such “c onstellation effects” mathematically,
we will consider classes of densities that satisfy a particular sparsity condition.

Our contribution consists in developing a thresholding density estimator that adapts to the unknown
sparsity of the underlying density in two key ways: (1) it is near-minimax optimal, with the error
decay rate depending upon the sparsity, and (2) it can be implemented using a recursive algorithm
that runs in probabilistic polynomial time and whose computational complexity is lower for sparser
densities. The algorithm entails recursively examining empirical estimates of whole blocks of the
2d basis coefﬁcients. At each stage of the algorithm, the weigh ts of the coefﬁcients estimated at
previous stages are used to decide which remaining coefﬁcie nts are most likely to be signiﬁcant,
and computing resources are allocated accordingly. We show that this decision is accurate with high
probability. An additional attractive feature of our approach is that it gives us a principled way of
trading off MSE against computational complexity by controlling the decay of the threshold as a
function of the recursion depth.

2 Preliminaries

. Throughout the paper, C and c denote
We ﬁrst list some de ﬁnitions and results needed in the sequel
generic constants whose values may change from line to line. For two real numbers a and b, a ∧ b
and a ∨ b denote, respectively, the smaller and the larger of the two.
Biased Walsh bases. Let µd denote the counting measure on the d-dimensional binary hypercube
Bd . Then the space of all real-valued functions on Bd is the real Hilbert space L2 (µd ) with the
= Px∈Bd f (x)g (x). Given any η ∈ (0, 1), we can construct an
△
standard inner product hf , g i
orthonormal system Φd,η in L2 (µd ) as follows. De ﬁne two functions ϕ0,η , ϕ1,η : B → R by
△
△
= (1 − η)x/2η (1−x)/2
= (−1)x ηx/2 (1 − η)(1−x)/2 ,
and ϕ1,η (x)
(1)
x ∈ {0, 1}.
ϕ0,η (x)
Now, for any s = (s(1), . . . , s(d)) ∈ Bd de ﬁne the function ϕs,η : Bd → R by
dYi=1
∀x = (x(1), . . . , x(d)) ∈ Bd
(this is written more succinctly as ϕs,η = ϕs(1),η ⊗ . . . ⊗ ϕs(d),η , where ⊗ is the tensor product).
The set Φd,η = {ϕs,η : s ∈ Bd} is an orthonormal system in L2(µd ), which is referred to as the
Walsh system with bias η [8, 9]. Any function f ∈ L2 (µd ) can be uniquely represented as
f = Xs∈Bd
θs,η ϕs,η ,
where θs,η = hf , ϕs,η i. When η = 1/2, we get the standard Walsh system used in [5, 6]; in that
case, we shall omit the index η = 1/2 for simplicity. The product structure of the biased Walsh
bases makes them especially convenient for statistical applications as it allows for a computation-
ally efﬁcient recursive method for computing accurate esti mates of squared coefﬁcients in certain
hierarchically structured sets.

ϕs(i),η (x(i)),

ϕs,η (x) △=

(2)

Sparsity and weak-ℓp balls. We are interested in densities whose representations in some biased
Walsh basis satisfy a certain sparsity constraint. Given η ∈ (0, 1) and a function f ∈ L2(µd ), let
θ(f ) denote the list of its coefﬁcients in Φd,η . We are interested in cases when the components
of θ(f ) decay according to a power law. Formally, let θ(1) , . . . , θ(M ) , where M = 2d , be the
components of θ(f ) arranged in decreasing order of magnitude: |θ(1) | ≥ |θ(2) | ≥ . . . ≥ |θ(M ) |.
Given some 0 < p < ∞, we say that θ(f ) belongs to the weak-ℓp ball of radius R [10], and write
θ(f ) ∈ wℓp (R), if
|θ(m) | ≤ R · m−1/p ,
(3)
1 ≤ m ≤ M .

It is not hard to show that the coefﬁcients of any probability density on Bd in Φd,η are bounded by
R(η) = [η ∨ (1 − η)]d/2 . With this in mind, let us de ﬁne the class Fd (p, η) of all functions f on Bd
satisfying θ(f ) ∈ wℓp (R(η)) in RM . We are particularly interested in the case 0 < p < 2. When
η = 1/2, with R(η) = 2−d/2 , we shall write simply Fd (p).
We will need approximation properties of weak-ℓp balls as listed, e.g., in [11]. The basic fact is that
the power-law condition (3) is equivalent to the concentration estimate
(cid:12)(cid:12)(cid:8)s ∈ Bd : |θs | ≥ λ(cid:9)(cid:12)(cid:12) ≤ (R/λ)p ,
(4)
∀λ > 0.
For any 1 ≤ k ≤ M , let θk (f ) denote the vector θ(f ) with θ(k+1) , . . . , θ(M ) set to zero. Then it
△
follows from (3) that kθ(f ) − θk (f )kℓ2
M ≤ CRk−r , where r
= 1/p − 1/2, and C is some constant
that depends only on p. Given any f ∈ Fd (p, η) and denoting by fk the function obtained from it
by retaining only the k largest coefﬁcients, we get from Parseval’s identity that
kf − fk kL2 (µd ) ≤ CRk−r .
(5)
To get a feeling for what the classes Fd (p, η) could model in practice, we note that, for a ﬁxed
= √η/(√η + √1 − η) is the unique
η ∈ (0, 1), the product of d Bernoulli(η∗ ) densities with η∗ △
sparsest density in the entire scale of Fd (p, η) spaces with 0 < p < 2: all of its coefﬁcients in
Fd,η are zero, except for θs,η with s = (0, . . . , 0), which is equal to (η∗ /√η)d . Other densities in
{Φd (p, η) : 0 < p < 2} include, for example, mixtures of components that, up to a permutation
of {1, . . . , d}, can be written as a tensor product of a large number of Bernoulli(η∗ ) densities and
some other density. The parameter η can be interpreted either as the default noise level in measuring
an individual covariate or as a smoothness parameter that interpolates between the point masses
δ(0,...,0) and δ(1,...,1) . We assume that η is known (e.g., from some preliminary exploration of the
data or from domain-speciﬁc prior information) and ﬁxed.

η = 1/2 with R(1/2) = 2−d/2 . Our
In the following, we limit ourselves to the “noisiest” case
theory can be easily modiﬁed to cover any other η ∈ (0, 1): one would need to replace R = 2−d/2
with the corresponding R(η) and use the bound kϕs,η k∞ ≤ R(η) instead of kϕsk∞ ≤ 2−d/2 when
estimating variances and higher moments.

3 Density estimation via recursive Walsh thresholding

We now turn to our problem of estimating a density f on Bd from a sample {Xi}n
i=1 when f ∈ Fd (p)
for some unknown 0 < p < 2. The minimax theory for weak-ℓp balls [10] says that
R∗
n (Fd (p)) ≥ CM −p/2n−2r/(2r+1) ,
r = 1/p − 1/2
where M = 2d . We shall construct an estimator that adapts to unknown sparsity of f in the sense
that it achieves this minimax rate up to a logarithmic factor without prior knowledge of p and that
its computational complexity improves as p → 0.
Our method is based on the thresholding of empirical Walsh coefﬁcients. A thresholding estimator
is any estimator of the form
bf = Xs∈Bd
I{T (bθs )≥λn } bθsϕs ,
where bθs = (1/n) Pn
i=1 ϕs (Xi ) are empirical estimates of the Walsh coefﬁcients of f , T (·) is
some statistic, and I{·} is an indicator function. The threshold λn depends on the sample size. For
example, in [5, 6] the statistic T (bθs ) = bθ2
s was used with the threshold λn = 1/M (n + 1). This
choice was motivated by the considerations of bias-variance trade-off for each individual coefﬁcient.
The main disadvantage of such direct methods is the need to estimate all M = 2d Walsh coefﬁcients.
While this is not an issue when d ≍ log n, it is clearly impractical when d ≫ log n. To deal with this
issue, we will consider a recursive thresholding approach that will allow us to reject whole groups
of coefﬁcients based on efﬁciently computable statistics.
This approach is motivated as follows. For
any 1 ≤ k ≤ d, we can write any f ∈ L2 (µd ) with the Walsh coefﬁcients θ(f ) as
f = Xu∈Bk Xv∈Bd−k
θuv ϕuv = Xu∈Bk
fu ⊗ ϕu ,

△
where uv denotes the concatenation of u ∈ Bk and v ∈ Bd−k and, for each u ∈ Bk , fu
=
L2 (µd−k ) = Pv∈Bd−k θ2
Pv∈Bd−k θuv ϕv lies in L2(µd−k ). By Parseval’s identity, Wu
△= kfuk2
uv .
This means that if Wu < λ for some u ∈ Bk , then θ2
uv < λ for every v ∈ Bd−k . Thus, we could
start at u = 0 and u = 1 and check whether Wu ≥ λ. If not, then we would discard all θuv with
v ∈ Bd−1; otherwise, we would proceed on to u0 and u1. At the end of this process, we will be left
only with those s ∈ Bd for which θ2
s ≥ λ. Let fλ denote the resulting function. If f ∈ Fd (p) for
some p, then we will have kf − fλk2
L2 (µd ) ≤ CM −1 (M λ)−2r/(2r+1) .
We will follow this reasoning in constructing our estimator. We begin by developing an estimator
for Wu . We will use the following fact, easily proved using the de ﬁn itions (1) and (2) of the Walsh
functions: for any density f on Bd , any k and u ∈ Bk , we have
fu (y ) = Ef (cid:8)ϕu (πk (X ))I{σk (X )=y}(cid:9) , ∀y ∈ Bd−k
and Wu = Ef {ϕu (πk (X ))fu (σk (X ))} ,
△
△
= (x(k + 1), . . . , x(d)) for any x ∈ Bd . This suggests
where πk (x)
= (x(1), . . . , x(k)) and σk (x)
that we can estimate Wu by
nXi2=1
nXi1=1
1
cWu =
ϕu (πk (Xi1 ))ϕu (πk (Xi2 ))I{σk (Xi1 )=σk (Xi2 )} .
n2
Using induction and Eqs. (1) and (2), we can prove that cWu = Pv∈Bd−k bθ2
uv . An advantage of
computing cWu indirectly via (6) rather than as a sum of bθ2
uv , v ∈ Bd−k , is that, while the latter
has O(2d−k n) complexity, the former has only O(n2 d) complexity. This can lead to signiﬁcant
computational savings for small k . When k ≥ d − log(nd), it becomes more efﬁcient to use the
direct estimator.
Now we can de ﬁne our density estimation procedure. Instead o f using a single threshold for all
1 ≤ k ≤ d, we consider a more ﬂexible strategy: for every k , we shall compare each cWu to a
threshold that depends not only on n, but also on k . Speciﬁcally, we will let
αk log n
(7)
λk,n =
,
1 ≤ k ≤ d
n
k=1 satisﬁes α1 ≥ αk ≥ αd > 0. (This k-dependent scaling will allow us to trade
where α = {αk }d
△
off MSE and computational complexity.) Given λ = {λk,n }d
k=1 , de ﬁne the set A(λ)
= {s ∈ Bd :
cWπk (s) ≥ λk,n , ∀1 ≤ k ≤ d} and the corresponding estimator
= Xs∈Bd
I{s∈A(λ)} bθsϕs ,
bfRWT
△
where RWT stands for “recursive Walsh thresholding.” To imp lement bfRWT on a computer, we adapt
the algorithm of Goldreich and Levin [12], originally developed for cryptography and later applied
to the problem of learning Boolean functions from membership queries [13]: we call the routine
RECUR S IVEWAL SH, shown in Algorithm 1, with u = ∅ (the empty string) and with λ from (7).

(6)

(8)

sup
f ∈Fd (p)

Analysis of the estimator. We now turn to the asymptotic analysis of the MSE and the computa-
tional complexity of bfRWT . We ﬁrst prove that bfRWT adapts to unknown sparsity of f :
Theorem 3.1 Suppose the threshold sequence λ = {λk }d
k=1 is such that αd ≥ (20d + 25)2/2d .
Then for all 0 < p < 2 the estimator (8) satisﬁes
(cid:19)2r/(2r+1)
2d (cid:18) 2dα1 log n
C
Ef kf − bfRWTk2
MSE(f , bfRWT ) = sup
L2 (µd ) ≤
n
f ∈Fd (p)
where the constant C depends only on p.
Proof: Let us decompose the squared L2 error of bfRWT as
L2 (µd ) = Xs
I{s∈A(λ)} (θs − bθs )2 + Xs
kf − bfRWTk2

I{s∈A(λ)c } θ2
s ≡ T1 + T2 .

,

(9)

Algorithm 1 RECUR S IVEWAL SH(u, λ)
k ← length(u)
if k = d then
nPi=1
ϕu (Xi ); if bθ2
u ≥ λd,n then output u, bθu ; return
compute bθu ← 1
n
end if
nPi1=1
nPi2=1
compute cWu0 ← 1
ϕu0 (πk+1 (Xi1 ))ϕu0 (πk+1 (Xi2 ))I{σk+1 (Xi1 )=σk+1 (Xi2 )}
n2
nPi1=1
nPi2=1
compute cWu1 ← 1
ϕu1 (πk+1 (Xi1 ))ϕu1 (πk+1 (Xi2 ))I{σk+1 (Xi1 )=σk+1 (Xi2 )}
n2
if cWu0 ≤ λk+1,n then return else RECUR S IVEWAL SH(u0, λ); end if
if cWu1 ≤ λk+1,n then return else RECUR S IVEWAL SH(u1, λ); end if
We start by observing that s ∈ A(λ) only if bθ2
s ≥ λd,n , while for any s ∈ A(λ)c there exists some
s < λk,n ≤ λ1,n . De ﬁning the sets A1 = {s ∈ Bd : bθ2
1 ≤ k ≤ d such that bθ2
s ≥ λd,n} and
s < λ1,n }, we get T1 ≤ Ps I{s∈A1 } (θs − bθs )2 and T2 ≤ Ps I{s∈A2 } θ2
A2 = {s ∈ Bd : bθ2
s . Further,
s ≥ 3λ1,n/2}, we can write
s < λd,n/2} and S = {s ∈ Bd : θ2
de ﬁning B = {s ∈ Bd : θ2
T1 = Xs
I{s∈A1∩B} (θs − bθs )2 + Xs
I{s∈A1 ∩B c } (θs − bθs )2 ≡ T11 + T12 ,
T2 = Xs
s + Xs
I{s∈A2∩S} θ2
I{s∈A2 ∩S c } θ2
s ≡ T21 + T22 .
First we deal with the easy terms T12 , T22 . Applying (4), (5) and a bit of algebra, we get
M n (cid:18) 2
M λd,n (cid:19)p/2
M n (cid:12)(cid:12)(cid:8)s : θ2
s ≥ λd,n/2(cid:9)(cid:12)(cid:12) ≤
1
1
1
n−2r/(2r+1) ,
≤
E T12 ≤
M
M (cid:18) M α1 log n
(cid:19)2r/(2r+1)
E T22 ≤ Xs∈Bd
C
s <(3α1 /2) log n/n} θ2
I{θ2
s ≤
n
Next we deal with the large-deviation terms T11 and T21 . Using Cauchy –Schwarz, we get
E T11 ≤ Xs hE(θs − bθs )4 · P(s ∈ A1 ∩ B )i1/2
(12)
.
To estimate the fourth moment in (12), we use Rosenthal’s inequality [14] to get E(θs − bθs )4 ≤
c/M 2n2 . To bound the probability that s ∈ A1 ∩ B , we observe that s ∈ A1 ∩ B implies that
|bθs − θs | ≥ (1/5)pλd,n , and then use Bernstein’s inequality [14] to get
2(1 + 2β/3) (cid:19) = 2n−β 2/[2(1+2β/3)] ≤ 2n−(β−1)/2
P (cid:0)|bθs − θs | ≥ (1/5)pλd,n (cid:1) ≤ 2 exp (cid:18)−
β 2 log n
with β = (1/5)√M αd ≥ 4d + 5. Since n−(β−1)/2 ≤ n−2(d+1) , we have
E T11 ≤ C n−(d+1) ≤ C/(M n).
(13)
Finally, E T21 ≤ Ps
s . Using the same argument as above, we get P(s ∈ A2 ∩ S ) ≤
P(s ∈ A2 ∩ S )θ2
2n−(γ−1)/2 , where γ = (1/5)√M α1 . Since θ2
s ≤ 1/M for all s ∈ Bd and since γ ≥ β , this gives
E T21 ≤ 2n−2(d+1) ≤ 2/(M n).
(14)
Putting together Eqs. (10), (11), (13), and (14), we get (9), and the theorem is proved.
Our second result concerns the running time of Algorithm 1. Let K (α, p) △= Pd
k=1 α−p/2
k
Theorem 3.2 Given any δ ∈ (0, 1), provided each αk is chosen so that
p2kαk n log n ≥ 5 (cid:2)C2√n + (log(d/δ) + k)/ log e(cid:3) ,
Algorithm 1 runs in O(n2 d(n/M log n)p/2K (α, p)) time with probability at least 1 − δ .

(15)

(cid:4)

.

(10)

(11)

.

Proof: The complexity is determined by the number of calls to RECUR S IVEWAL SH. For each k ,
a call to RECUR S IVEWAL SH is made at every u ∈ Bk with cWu ≥ λk,n . Let us say that a call to
RECUR S IVEWAL SH(u, λ) is correct if Wu ≥ λk,n /2. We will show that, with probability at least
1 − δ , only the correct calls are made. The probability of making at least one incorrect call is
P   d[k=1 [u∈Bk {cWu ≥ λk,n , Wu < λk,n /2}! ≤
P (cid:16)cWu ≥ λk,n , Wu < λk,n /2(cid:17) .
dXk=1 Xu∈Bk
For a given u ∈ Bk , cWu ≥ λk,n and Wu < λk,n /2 together imply that kfu − bfuk2
L2 (µd−k ) ≥
(1/5)pλk,n , where bfu
= Pv∈Bd−k bθuv ϕv . Now, it can be shown that, for every u ∈ Bk , the norm
△
kfu − bfukL2 (µd−k ) can be expressed as a supremum of an empirical process [15] over a certain
function class that depends on k (details are omitted for lack of space). We can then use Talagrand’s
concentration-of-measure inequality for empirical processes [16] to get
P(cWu ≥ λk,n , Wu < λk,n /2) ≤ exp (cid:8) − nC1 (2k a2
k,n ∧ 2k/2ak,n )(cid:9),
where ak,n = (1/5)pαk log n/n − C2/√2kn, and C1 , C2 are the absolute constants in Talagrand’s
bound. If we choose αk as in (15), then P(cWu ≥ λk,n , Wu < λk,n /2) ≤ δ/(d2d−k ) for all u ∈ Bk .
Summing over k , u ∈ Bk , we see that, with probability ≥ 1 − δ , only the correct calls will be made.
It remains to bound the number of the correct calls. For each k , Wu ≥ λk,n /2 implies that there
exists at least one v ∈ Bd−k such that θ2
uv ≥ λk,n /2. Since for every 1 ≤ k ≤ d each θs contributes
to exactly one Wu , we have by the pigeonhole principle that
(cid:12)(cid:12)(cid:8)u ∈ Bk : Wu ≥ λk,n /2(cid:9)(cid:12)(cid:12) ≤ (cid:12)(cid:12)(cid:8)s ∈ Bd : θ2
s ≥ λk,n /2(cid:9)(cid:12)(cid:12) ≤ (2/M λk,n )p/2 ,
where in the second inequality we used (4) with R = 1/√M . Hence, the number of correct
recursive calls is bounded by N = Pd
k=1 (2/M λk,n )p/2 = (2n/M log n)p/2K (α, p). At each call,
we compute an estimate of the corresponding Wu0 and Wu1 , which requires O(n2 d) operations.
Therefore, with probability at least 1 − δ , the time complexity will be as stated in the theorem. (cid:4)

MSE vs. complexity. By controlling the rate at which the sequence αk decays with k , we can
trade off MSE against complexity. Consider the following two extreme cases: (1) α1 = . . . =
αd ∼ 1/M and (2) αk ∼ 2d−k /M . The ﬁrst case, which reduces to term-by-term threshold-
ing, achieves the best bias-variance trade-off with the MSE O((log n/n)2r/(2r+1)(1/M )). How-
ever, it has K (α, p) = O(M p/2 d), resulting in O(d2 n2 (n/ log n)p/2 ) complexity. The second
case, which leads to a very severe estimator that will tend to reject a lot of coefﬁcients, has MSE
of O((log n/n)2r/(2r+1)M −1/(2r+1) ), but K (α, p) = O(M p/2 ), leading to a considerably better
O(dn2 (n/ log n)p/2 ) complexity. From the computational viewpoint, it is preferable to use rapidly
decaying thresholds. However, this reduction in complexity will be offset by a corresponding in-
crease in MSE. In fact, using exponentially decaying αk ’s in practice is not advisable as its low
complexity is mainly due to the fact that it will tend to reject even the big coefﬁcients very early on,
especially when d is large. To achieve a good balance between complexity and MSE, a moderately
decaying threshold sequence might be best, e.g., αk ∼ (d − k + 1)m /M for some m ≥ 1. As p → 0,
the effect of λ on complexity becomes negligible, and the complexity tends to O(n2 d).
Positivity and normalization issues. As is the case with orthogonal series estimators, bfRWT may
not necessarily be a bona ﬁde density. In particular, there m ay be some x ∈ Bd such that bfRWT (x) <
0, and it may happen that R bfRWTdµd 6= 1. In principle, this can be handled by clipping the negative
values at zero and renormalizing, which can only improve the MSE. In practice renormalization may
be computationally expensive when d is very large. If the estimate is suitably sparse, however, the
renormalization can be carried out approximately using Monte-Carlo methods.

4 Simulations

The focus of our work is theoretical, consisting in the derivation of a recursive thresholding proce-
dure for estimating multivariate binary densities (Algorithm 1), with a proof of its near-minimaxity

and an asymptotic analysis of its complexity. Although an extensive empirical evaluation is outside
the scope of this paper, we have implemented the proposed estimator, and now present some simula-
tion results to demonstrate its small-sample performance. We generated synthetic observations from
a mixture density f on a 15-dimensional binary hypercube. The mixture has 10 components, where
each component is a product density with 12 randomly chosen covariates having Bernoulli(1/2)
distributions, and the other three having Bernoulli(0.9) distributions. For d = 15, it is still feasible
to quickly compute the ground truth, consisting of 32768 values of f and its Walsh coefﬁcients.
These values are shown in Fig. 1 (left). As can be seen from the coefﬁcient pro ﬁle in the bottom of
the ﬁgure, this density is clearly sparse. Fig. 1 also shows t he estimated probabilities and the Walsh
coefﬁcients for sample sizes n = 5000 (middle) and n = 10000 (right).

Ground truth (f )

bfRWT , n = 5000

bfRWT , n = 10000

Figure 1: Ground truth (left) and estimated density for n = 5000 (middle) and n = 10000 (right) with
constant thresholding. Top: true and estimated probabilities (clipped at zero and renormalized) arranged in
lexicographic order. Bottom: absolute values of true and estimated Walsh coefﬁcients arranged in lexicographic
order. For the estimated densities, the coefﬁcient plots al so show the threshold level (dotted line) and absolute
values of the rejected coefﬁcients (lighter color).

)
d
2
 
×
(
 
E
S
M

0.6

0.5

0.4

0.3

0.2

0.1

 

constant
log
linear

)
s
(
 
e
m
i
T

1400
1200
1000
800
600
400
200

 

2000

4000
6000
Sample size (n)
(a)

8000

10000

2000

4000
8000 10000
6000
Sample size (n)
(b)

s
l
l
a
c
 
e
v
i
s
r
u
c
e
R

3500

3000

2500

2000

1500

1000

500

40

35

30

25

20

15

10

d
e
t
a
m
i
t
s
e
 
.
s
f
f
e
o
C

2000

4000
8000 10000
6000
Sample size (n)
(c)

2000

4000
6000
Sample size (n)
(d)

8000

10000

Figure 2: Small-sample performance of bfRWT in estimating f wth three different thresholding schemes:
(a) MSE; (b) running time (in seconds); (c) number of recursive calls; (d) number of coefﬁcients retained by
the algorithm. All results are averaged over ﬁve independen t runs for each sample size (the error bars show the
standard deviations).

To study the trade-off between MSE and complexity, we implemented three different thresholding
schemes: (1) constant, λk,n = 2 log n/(2dn), (2) logarithmic, λk,n = 2 log(d − k + 2) log n/(2dn),
and (3) linear, λk,n = 2(d − k + 1) log n/(2dn). Up to the log n factor (dictated by the theory),
the thresholds at k = d are set to twice the variance of the empirical estimate of any coefﬁcient
whose value is zero; this forces the estimator to reject empirical coefﬁcients whose values cannot
be reliably distinguished from zero. Occasionally, spurious coefﬁcients get retained, as can be seen
in Fig. 1 (middle) for the estimate for n = 5000. Fig 2 shows the performance of bfRWT . Fig. 2(a)
is a plot of MSE vs. sample size. In agreement with the theory, MSE is the smallest for the con-
stant thresholding scheme [which is simply an efﬁcient recu rsive implementation of a term-by-term
thresholding estimator with λn ∼ log n/(M n)], and then it increases for the logarithmic and for
the linear schemes. Fig. 2(b,c) shows the running time (in seconds) and the number of recursive

calls made to RECUR S IVEWAL SH vs. sample size. The number of recursive calls is a platform-
independent way of gauging the computational complexity of the algorithm, although it should be
kept in mind that each recursive call has O(n2 d) overhead. The running time increases polynomi-
ally with n, and is the largest for the constant scheme, followed by the logarithmic and the linear
schemes. We see that, while the MSE of the logarithmic scheme is fairly close to that of the constant
scheme, its complexity is considerably lower, in terms of both the number of recursive calls and the
running time. In all three cases, the number of recursive calls decreases with n due to the fact that
weight estimates become increasingly accurate with n, which causes the expected number of false
discoveries (i.e., making a recursive call at an internal node of the tree only to reject its descendants
later) to decrease. Finally, Fig. 2(d) shows the number of coefﬁcients retained in the estimate. This
number grows with n as a consequence of the fact that the threshold decreases with n, while the
number of accurately estimated coefﬁcients increases. The true density f has 40 parameters: 9 to
specify the weights of the components, 3 per component to locate the indices of the nonuniform
covariates, and the single Bernoulli parameter of the nonuniform covariates. It is interesting to note
that the maximal number of coefﬁcients returned by our algor ithm approaches 40.

Overall, these preliminary simulation results show that our implemented estimator behaves in accor-
dance with the theory even in the small-sample regime. The performance of the logarithmic thresh-
olding scheme is especially encouraging, suggesting that it may be possible to trade off MSE against
complexity in a way that will scale to large values of d. In the future, we plan to test our method
on high-dimensional real data sets. Our particular interest is in social network data, e.g., records of
meetings among large groups of individuals. These are represented by binary strings most of whose
entries are zero (i.e., only a very small number of people are present at any given meeting). To model
their densities, we plan to experiment with Walsh bases with η biased toward unity.

Acknowledgments

This work was supported by NSF CAREER Award No. CCF-06-43947 and DARPA Grant No. HR0011-07-1-
003.

References

J. Econometrics

[1] I. Shmulevich and W. Zhang. Binary analysis and optimization-based normalization of gene expression
data. Bioinformatics 18(4):555–565, 2002.
[2] J.M. Carro. Estimating dynamic panel data discrete choice models with ﬁxed effects.
140:503–528, 2007.
[3] Z. Ghahramani and K. Heller. Bayesian sets. NIPS 18:435–442, 2006.
[4] J. Aitchison and C.G.G. Aitken. Multivariate binary discrimination by the kernel method. Biometrika
63(3):413–420, 1976.
[5] J. Ott and R.A. Kronmal. Some classi ﬁcation procedures f or multivariate binary data using orthogonal
functions. J. Amer. Stat. Assoc. 71(354):391–399, 1976.
[6] W.-Q. Liang and P.R. Krishnaiah. Nonparametric iterative estimation of multivariate binary density. J.
Multivariate Anal. 16:162–172, 1985.
[7] J.S. Simonoff. Smoothing categorical data. J. Statist. Planning and Inference 47:41–60, 1995.
[8] M. Talagrand. On Russo’s approximate zero-one law. Ann. Probab. 22:1576–1587, 1994.
[9] I. Dinur, E. Friedgut, G. Kindler and R. O’Donnell. On the Fourier tails of bounded functions over the
discrete cube. Israel J. Math. 160:389–421, 2007.
[10] I.M. Johnstone. Minimax Bayes, asymptotic minimax and sparse wavelet priors. In S.S. Gupta and
J.O. Berger, eds., Statistical Decision Theory and Related Topics V, pp. 303–326, Springer, 1994.
[11] E.J. Cand `es and T. Tao. Near-optimal signal recovery f rom random projections: universal encoding strate-
gies? IEEE Trans. Inf. Theory 52(12):5406–5425, 2006.
[12] O. Goldreich and L. Levin. A hard-core predicate for all one-way functions. STOC, pp. 25–32, 1989.
[13] E. Kushilevitz and Y. Mansour. Learning decision trees using the Fourier spectrum. SIAM J. Comput.
22(6):1331-1348, 1993.
[14] W. H ¨ardle, G. Kerkyacharian, D. Picard and A.B. Tsybakov. Wavelets, Approximation, and Statistical
Applications, Springer, 1998.
[15] S.A. van de Geer. Empirical Processes in M-Estimation, Cambridge Univ. Press, 2000.
[16] M. Talagrand. Sharper bounds for Gaussian and empirical processes. Ann. Probab. 22:28–76, 1994.


Accurate and Cheap Robot Range Finder ∗

Ivan Papusha

December 12, 2008

Abstract
A novel high-quality distance sensor for robotics applications is proposed. The sensor relies on
triangulation with the oﬀset of a laser line as it is reﬂected oﬀ an ob ject into a cheap webcam. An
ML approach to ﬁnding the error model showed that the sensor was very accurate when the laser
line was found. It is suggested that better line-ﬁnding methods than simple color ﬁltering would
make the sensor more viable.

1 Introduction

Any self-respecting mobile robot requires some sort of distance measurement sensor or range ﬁnding
mechanism. Among the most accessible are ultrasonic range ﬁnders, which time the propagation and
reﬂection of a sonic pulse. While cheap and ubiquitous, ultrasonic rangers, especially when placed in
ring conﬁguration, are plagued with fundamental diﬃculties that make them unattractive for serious
localization.
At the other end of the spectrum are high-end laser rangeﬁnders, which use light instead of sound to
determine distance from time-of-ﬂight. These are often bundled with precision optics and mechanisms
that allow one to infer 3D point clouds over large and small “human” distances. Their precision and
reliability makes them ideal for industrial and research applications, but their cost is prohibitive to the
hobbyist or researcher on a low budget.
To solve the problem of optimizing user cost vs. sensor precision and reliability, we consider a
diﬀerent sensor design. Here, the only materials needed are a laser line generator and a camera (e.g.,
a cheap webcam). The principle of operation is simple: one points the line generator and the camera
in the same direction, but oﬀsets them some known distance dof f (∼ 10cm) The camera sees the laser
line, ﬁltered from the rest of the environment based on brightness and color, which is oﬀset whenever
it strikes an ob ject. By calculating the oﬀset distance from the expected position of the laser line, the
distance to the ob ject can be inferred.
This sensor, of course, is not perfect and is sub ject to its own errors. In particular, its error model
and corresponding error parameters are quite diﬀerent from the error parameters of time-of-ﬂight
based rangers such as the ones discussed above. This paper will formulate the error model and use an
automatic Maximum Likelihood (ML) method that will learn the fundamental parameters of the error
model from a proposed calibration scheme [3] [1]. Results of the sensor and sensor model in action are
also discussed.
∗ I thank Morgan Quigley for his direction, as well as the STAIR Pro ject for the materials.

1

2 Range Sensor Design

2.1 Geometry and Raycasting

The relations among the fundamental lengths and angles described in Figure 1 are derived using a
simple ray casting scheme. The sensor is extremely sensitive to construction parameters, so to eliminate
systematic error, a sturdy construction is necessary. Note that the camera must be appropriately
calibrated for radial distortion (using e.g., a standard checkerboard algorithm [2]), so that these ﬁgures
make sense.

Figure 1: Single-Point simpliﬁed ranger geometry, side view. The camera is tilted such that the actual
horizon corresponds to the bottom ﬁeld of view limit. The ob ject is dobj away from the laser line
generator, which is separated from the camera by dof f , a known distance. The camera sees the point
of intersection yrc/yres of the way from the bottom of its viewing plane.

In order to triangulate the distance to given ob ject, we use a standard line-plane intersection
method. The camera is assumed to be at the origin of a 3-D spherical coordinate system. The plane
of intersection is the one generated by the laser line, oﬀset a distance dof f below the camera. After
calibration [2] and laser line detection (see below), horizontal and vertical angles, θhoriz and θvert
respectively are inferred from the location of the laser line for each column of the display. Finally, the
distance dobj to the ob ject is calculated by ﬁnding the intersection of a line and a plane.

2.2 Detecting the Laser Line

2.2.1 Color Filtering

Assume that the laser line has color represented by the RGB vector v = (r, g , b)T , where r, g , and b
are intensity values for the red, green, and blue components of the color respectively. Experimentally,
typical values for the components of v are (68, 127, 12)T on a scale from 0 to 255. For every pixel with
color w, we can pro ject w onto v to see how “strongly” w corresponds to v using the dot product.
w · v
(cid:107)v(cid:107)2
where y ∈ R is the strength of the pro jection onto v and ˆv is the unit vector in the direction of v .
Colors strongly correlated with v will give large values of y and colors weakly correlated with v will
give small values.

y =

(1)

ˆv

2

obstructioncameralaser linehorizon (bottom ﬁeld of view)object-laserintersectioncamera horizontop ﬁeld of viewθφdoffdobjypθrot2.2.2 Fitting a Gaussian

Since each column of pixels corresponds to one oﬀset, and hence one measurement, we can ﬁnd where
the line is by a simple ﬁt to a gaussian. To make this more concrete, suppose that in a given column,
the pro jections onto v are the scalars {y (1) , y (2) , . . . , y (r)}, where r is the camera’s vertical resolution
in pixels. We can therefore treat the values y (1) , . . . , y (r) as entries in a histogram with r bins of size
(cid:32) r(cid:88)
(cid:33)−1 r(cid:88)
1. The expected position of the laser line is simply the weighted mean
i=1
i=1

i · yi .

(2)

E [y ] =

yi

Notice that E [y ] can be a fraction, which eﬀectively allows a little extra resolution beyond the camera’s
pixel resolution. In practice, detection works better if the camera is taken slightly out of focus, which
blurs out some noise at the expense of a wider distribution over the y values.

3 Sensor Calibration

3.1 Gathering Data

(a) Raw data

(b) Histogram

Figure 2: Raw data and histogram for 10,000 measurements of a box 2.48 meters from the sensor.
About 63% of the measurements are centered around the true distance. A small number of values are
scattered randomly through the entire measurement space, and the rest return zmax , when the laser
line was not found.
A box is placed a known distance dobj ∼ 1m in the center third of the camera’s ﬁeld of view.
Only those pixels which correspond to the ob ject are considered. After the laser line is found and
raycasting is used to extract the distance to the ob ject, one should expect all the measurements in
the point cloud to center around the actual distance dobj = µ to the box. Label all the measurements
z (1) , z (2) , . . . , z (m) , where in this case m = 10, 000.

3.2 Error Model

We use a modiﬁed decomposition of the measurement density discussed in [3].
In particular, the
probability for each measurement z (i) , the probability p(z (i) |µ) of that measurement given that the
actual distance is µ, is split into three distributions

3

1000200030004000500060007000800090001000000.511.522.533.544.555.510000 Raw Samples, Object at 2.48 metersSampleRaw distance00.511.522.533.544.555.5050010001500200025003000Histogram, Object at 2.48 metersDistanceFrequency• Correct measurement: A single-variable gaussian distribution phit (z (i) |µ; σ2
hit ) centered around
the correct measurement. Its variance, σ2
hit is a parameter of the model that measures noise. We
(cid:40)ηN (µ, σ2
include in the model only those values corresponding to the measurement space that were not
failures, i.e.,
if 0 ≤ z (i) < zmax ,
hit )
phit (z (i) |µ; σ2
where η is a normalization parameter equal to the cumulative density η = (cid:82) zmax
hit ) =
otherwise.
0
N (µ, σ2
hit ) dz
0
• Random Measurement: A uniform distribution prand (z (i) |µ) corresponding to measurement
(cid:40)1/zmax
noise through the entire measurement space, i.e.,
0

if 0 ≤ z < zmax ,
otherwise.

prand (z (i) |µ) =

p(z (i) |µ; σ2
hit

, αhit , αrand , αmax ) =

• Failure: A point-mass distribution pmax (z (i) |µ) = 1{z = zmax} equal to 1 only if the measure-
ment is a failure - e.g., the camera failed to detect a line.
The probabilities are mixed using the mixing parameters αhit , αrand , αmax , where we constrain αhit +

 phit (z (i) |µ; σ2
 αhit
T
αrand + αmax = 1
hit )
prand (z (i) |µ)
αrand
pmax (z (i) |µ)}
αmax
3.3 Learning from Data to Infer Error Parameters
We formulate the learning problem of ﬁnding the probability density function by maximizing the total
m(cid:89)
, αhit , αrand , αmax )T . That is,
likelihood of the parameter θ = (σ2
hit
i=1
sub ject to αhit + αrand + αmax = 1, where m is the number of training examples, in this case 10,000.

p(z (i) |µ; σ2
hit

, αhit , αrand , αmax )

(3)

(4)

·

θ = arg max
θ

3.4 Results

The Expectation Maximization (EM) algorithm [1] was used to solve the constrained optimization
problem of Equation 4. The results are summarized in Figure 3. Having run the algorithm for several
distances, the mixing parameters remain about the same for the same lighting conditions, while the
gaussian width σhit increases with distance. This makes sense, because as the distance to the ob ject
increases, the oﬀsets approach the resolution of the camera.

4 Conclusion

4.1 Advantages Over Ultrasonic Rangers

The horizontal resolution of the sensor is limited only by the horizontal resolution of the camera.
That is, the distances provided by the sensor are much more granular than a single or even several
sonar elements. In addition, the webcam sensor does not suﬀer from sonar cross-talk, and can make
measurements as quickly as the camera can take pictures and hardware can process them. In ultrasonic

4

Figure 3: Plot of the probability density function for measurements of distances to a box 2.48 m away.
Notice the uniform density spread throughout the entire space and the gaussian centered around the
actual distance. The mixing parameters show that about 64% of the results come from the gaussian,
and 27% belong to the point-mass failure function. The rest are random readings.

sensors, one needs to wait a long time in clock cycles after sending a ping before the ping returns (or
fails to return). During this time, no more measurements can be made. The slow measurement-to-
measurement time can be detrimental in a dynamic environment, as the robot can fail to see ob jects
that move quickly in and out of view. Because the webcam sensor relies on light, the camera sees the
laser and can calculate oﬀsets almost instantaneously.

4.2 Disadvantages
The main problem with the sensor is the sensitivity to the laser line detection. The measurement zmax
is returned whenever the laser line was not found, which happens quite often. Because the scheme
used was based on simple color ﬁltering, any environment containing colors similar to the laser’s make
it hard to ﬁnd the line. In particular, the sensor is currently unsuitable in use in bright environments,
e.g., outside in daylight. It is possible to make the laser line detection more robust by using knowledge
about what it should look like in the camera. We know, for instance, that the line is horizontal or
almost horizontal. One can train a classiﬁcation algorithm to look for horizontal lines in an image,
and then decide whether the horizontal line is part of the environment or the laser line. Indeed, this
can be the sub ject of further research.

References

[1] Andrew Ng. Em algorithm. CS229 Class Notes, 2008.

[2] Hai Nguyen. Ros wiki: Camera calibration, 2008.

[3] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic Robotics, chapter 6.3, pages
124–139. MIT Press, 2005.

5

00.511.522.533.544.555.500.10.20.30.40.50.60.70.80.91Fit probability density, Object at 2.48 metersDistance (meters)Probability Densityσ = 0.036536µ = 2.4785zhit = 0.63889zmax = 0.2727zrand = 0.08841
TRAIN YOUR TV

Zhi Li, Borja Peleato

CS229 Class Project, Autumn 2008/2009

ABSTRACT

We study the problem of predicting the viewer’s behavior in
an Interactive TV application, using soccer matches as an ex-
ample. Based on the information extracted from the video
frames and the user’s Region of Interest (RoI) trajectory his-
tory, we make the prediction of the viewer’s RoI ahead of
time. We start with a generic probabilistic model, make sim-
pliﬁcations and develop a tractable system. Lastly, we verify
its performance through a set of experimental results and a
live demo.

1. INTRODUCTION

Consider you are watching a live soccer match on TV. The
latest Interactive TV technology enables customized viewing
experiences for users. You are enabled to pan/tilt/zoom the
video according to your viewing habits. For example, you
may want to have a clear view of where the ball is; alter-
natively, you may choose to view the details of a particular
player you like. In this project, our goal is to make the Interac-
tive TV even smarter – it can automatically learn the viewer’s
habits and adapt accordingly. There are at least two advan-
tages of this approach. First, if we can successfully predict the
viewer’s Region of Interest (RoI), we can do a pre-fetching in
the streaming of the video and hence reduce the latency or
the image distortions. Second, when the viewer is tired of
changing the RoI, the smart TV can do this on his behalf.
Prior work on coding and streaming for Interactive TV ap-
plications has been studied in [1, 2]. In [3], the authors have
also experimented with predicting the viewer’s behavior us-
ing various tools, such as ARMA model, Kalman ﬁlter and
motion vectors extracted from the compressed video. How-
ever, none of the above approaches involve any learning algo-
rithms, thus they are non-adaptive to each individual viewer’s
behavior. In this project, we develop a learning module which
allows the system to learn and adapt to each viewer’s behav-
ior, thereby enhancing the viewing experiences.
This report is organized as follows. In Section 2, we in-
troduce the setup of the Interactive TV system. In Section 3,
we formulate the RoI prediction problem and present our so-
lutions. Section 4 presents a set of experimental results to
demonstrate the effectiveness of our learning algorithm.

Fig. 1. User interface for Interactive TV viewing.

2. ENVIRONMENT SETUP

We consider the following system environment. The full-
frame video is captured by a high-resolution camera and is
available at the server. The video streamed from the server
to the client includes two parts – a base-layer overview video
and the enhancement-layer RoI video (refer to Fig. 1). At the
client side, the viewer indicates his RoI. If the RoI matches
the predicted one, then both the base- and enhancement-layer
streams are decoded and rendered as a high-resolution video
and displayed. If they do not match, then only the RoI of the
base layer video is decoded and rendered as a low-resolution
video. Hence, better RoI prediction would lead to higher
video quality.
Our main objective in this project is to implement a mod-
ule which can learn and accurately predict the viewer’s RoI. In
order to facilitate such prediction, we will allow some stream-
ing start-up delay and send some overview video frames ahead
of time. The inputs to this module are the viewer’s RoI tra-
jectory history and the overview video up to the frame of pre-
diction.
The performance will be evaluated based on the Euclidean
distance between the predicted RoI trajectory and the actual
one. To evaluate the prediction subjectively, we have built up

Fig. 2. Timeline of the RoI prediction problem.

a live demo. The demo lets the user specify his RoI during
the ﬁrst 300 frames of the video, uses that information for
training, and makes predictions for the rest 3000 frames.
In this project, we only focus on the streaming of a live
soccer match.

3. PROBLEM FORMULATION AND SOLUTION

In this section, we start with a generic probablistic model. We
show that by making proper assumptions we can decompose
the problem into two parts – prediction using overview video
frames (Section 3.2) and prediction using viewer RoI trajec-
tory history (Section 3.3). In the end, we present an overall
block diagram of the proposed solution.

3.1. Probablistic Model
Suppose we have a sequence of overview video frames {fi }.
On each frame the viewer can indicate a RoI φi . According
to the viewer’s indication, the RoI video is rendered. The
parameters used to characterize the RoI are: the position of
the RoI center (φx , φy ) and the RoI zoom (i.e., region size)
φz . Assume the RoI region has ﬁxed aspect ratio α, then the
width and height of the region are φz and αφz , respectively.
Overall, φi = [φxi φyi φz i ]T .
We derive a probabilistic model for the RoI prediction
problem. Suppose currently we are at time t, we want to make
a prediction of the viewer’s RoI n frames later, i.e., φt+n . The
information available for our prediction includes the follow-
ing: the overview video up to frame (t + n), i.e., f t+n =
{. . . , ft+n−2 , ft+n−1 , ft+n}, and the viewer’s RoI trajectory
history up to frame t, i.e., φt = {. . . , φt−2 , φt−1 , φt}. Based
p (cid:0)φt+n | f t+n , φt (cid:1) .
on all the information available, we make a prediction using:
ˆφt+n = arg max
(1)
φt+n
We want to decompose p(φt+n | f t+n , φt ) into factors that
only depend on f and φ separately. A heuristic way to do this
is to make the Markovity assumption below:
f t+n ↔ φt+n ↔ φt .
That is, the information from the overview video f t+n and
the trajectory history φt can be considered independent given
the viewer’s RoI at (t + n). A generative (but non-causal) in-
terpretation of this assumption is that we generate φt+n ﬁrst,

(2)

(3)

then based on the value of φt+n we generate φt and f t+n
separately. According to this assumption, we have:
p(f t+n , φt | φt+n ) = p(f t+n | φt+n )p(φt | φt+n ).
p (cid:0)φt+n | f t+n , φt (cid:1)
We can then write:
=p (cid:0)f t+n , φt | φt+n
(cid:1) p(φt+n )
=p (cid:0)f t+n | φt+n
(cid:1) p (cid:0)φt | φt+n
(cid:1) p(φt+n )
p(f t+n , φt )
p(f t+n , φt )
= p (φt+n | f t+n ) p (φt+n | φt )
p(f t+n )p(φt )
p(f t+n , φt ) .
p(φt+n )

(4)

(5)

Notice that p(φt+n ) is the prior probability of the RoI we
want to predict. Here we adapt the frequentist’s point of view
and do not make any assumptions on it. Therefore, we treat
p(φt+n ) as uniformly distributed (i.e., a constant). Our pre-
p (cid:0)φt+n | f t+n (cid:1) p (cid:0)φt+n | φt (cid:1) .
diction rule simpliﬁes to:
ˆφt+n = arg max
φt+n
The ﬁrst term p (φt+n | f t+n ) can be interpreted as the pre-
diction based on the available overview video frames and the
second term p (φt+n | φt ) as the prediction based on the viewer’s
trajectory history.
Direct prediction of φ = [φx φy φz ] using regression is
difﬁcult. Instead, we discretize each video frame into blocks
(say, 10 × 40) and predict the probability that each block
is in the RoI. Writing this formally, let p(x, y), where 1 ≤
x ≤ N ,1 ≤ y ≤ M be the position of the block, and let
It+n (p) be the indicator function of block p being in the RoI
at frame (t + n). We want to ﬁnd out Pr (It+n (p) | f t+n )
and Pr (It+n (p) | φt ). After they are found for every block,
we obtain a probability map. Then we ﬁnd [φx φy φz ] by
ﬁtting the probability map with a rectangular area.
In the next two subsections, we derive models to compute
Pr (It+n (p) | f t+n ) and Pr (It+n (p) | φt ), respectively.

3.2. Prediction Using Video Frames
Pr (cid:0)It+n (p) | f t+n (cid:1) = Pr (It+n (p) | ft+n )
We make the following simpliﬁcations:
= Pr (It+n (p) | ξt+n (p), p)

(6)

The ﬁrst equation is due to the assumption that It+n (p) is con-
ditionally independent of the previous frames f t+n−1 given
frame ft+n . 1 The second equation makes the assumption
that It+n (p) depends only on a local patch ξt+n around p and

1Actually not quite. As we will see next, the features we extract from
the frames include movement intensity, which is also determined by previous
frames. But for notational simplicity, let us write as such.

Current TimeTime of PredictionTraining WindowLook-aheadWindow...the actual location p (e.g., whether the RoI is in the ﬁeld or at
the audience do matter).
To compute Pr (It+n (p) | ξt+n (p), p), we select features
to reﬂect ξt+n (p) and p, and use a logistic regression model.
We select the following local patch features:
• DI ST BALL(p) – Euclidean distance from the local
patch center to the ball. If the viewer’s RoI follows the
ball, then the larger DI ST BALL(p), the less likely
p is in RoI. Ball detector for soccer sequence is readily
available in the literature [4]. In this project, we assume
we know the ground truth where the ball is by manually
marking the ball position throughout the frames.
• M OV (p) – Local patch movement intensity. We com-
pute this by measuring the difference between adjacent
frames. Our assumption is that typical viewers would
be interested in viewing areas which invovle intense
movements.
• N U M P LAY ERS (p) – Number of players within a
local patch. We ﬁrst segment the players from each
video frame, and then count how many players are in
a local patch. Our assumption is that the viewer would
be interested in areas with more players.
To capture the location information, we give each block
of the frame a label, and measure the percentage of each label
within a neighborhood area (say 3 × 3) of p. The constructed

feature vector is
P ERC F I ELD(p)
P ERC GOALM OU T H (p)
P ERC AU DI EN CE (p)
P ERC SCOREBOARD(p)
P ERC ADS (p)
P ERC CE I LIN G(p)
Overall, the feature vector x(i) ∈ R9 extracted from a frame

 .
at location p is:
Given φi , the target variable y (i) = It (p) is straightforward to
compute. After obtaining a set of training examples (x(i) , y (i) ),
we run logistic regression to obtain parameter θ .

DI ST BALL(p)
M OV (p)
N U M P LAY ERS (p)
LOCAL LABEL(p)

LOCAL LABEL(p) =

take this scheme as a benchmark for comparison, and we will
try to improve upon it using learning techniques.
The available information is the viewer’s recent RoI tra-
jectory history φt = (..., φt−2 , φt−1 , φt ) and his behavior
from watching previous matches Φ1 , Φ2 , Φ3 , ..., ΦN . Alter-
natively, we could obtain the later set of trajectories from
other viewers watching similar programs, in our case, soc-
cer matches. Our hypothesis is that there exist some short
“typical trajectories” that viewers very commonly follow. Ex-
amples of these would be moving to the scoreboard and back
to the ﬁeld, parabolic following of goal kicks, U-shape when
scanning through the audience, etc. Some of these trajecto-
ries are not linear, and it would be difﬁcult to capture them all
under a single set of linear regression parameters. Hence, we
group similar trajectory segments into a single class and then
ﬁnd a different set of regression parameters for each group.
Past trajectories Φ1 , Φ2 , Φ3 , ..., ΦN are cut into overlap-
ping segments of 300 frames (i.e., 12 seconds for frame rate
25). This gives us a very large number segments on which to
run the k-means algorithm. By means of cross-validation we
found out that 10 was the optimal number of clusters for our
data.2
For each cluster we chose the regression parameters to
minimize the mean square error, given by the normal equa-
tions. Let ψ1 , ψ2 , ..., ψN be the set of segments that fall into
the i-th cluster. Build matrix

A = [ψ1,[1:10:201] , ψ2,[1:10:201] , ..., ψN ,[1:10:201] ]T

with 21 of their ﬁrst 200 samples, and vector

b = [ψ1,250 , ψ2,250 , ..., ψN ,250 ]T

with their 250-th sample. The parameters for a prediction
with training window of 200 samples (i.e., 4 seconds) and
lookahead of 50 samples (i.e., 2 seconds) are given by ci =
(AT A)−1AT b. The parameters and centroids of each cluster
are computed during an ofﬂine training phase and stored to be
used at the prediction time.
While the viewer is watching the match, the position of
the RoI during the last 200 frames is kept, and compared once
per second with the centroids of all the clusters. The viewer’s
trajectory is assumed to belong in the cluster whose centroid
is closest. Once decided the cluster, the corresponding co-
efﬁcients are used to center our Gaussian model for the pre-
dicted RoI center. Zoom factor was modeled by convolving
this gaussian distribution with a rectangle of the same size as
the current RoI window.

2 The number of parameters increases with the number of clusters. Too
many clusters causes overﬁtting, too few underﬁtting. With more data, this
number would have been much larger. For the milestone report we used a
larger trajectory dataset recorded on a shorter clip. The optimal number of
clusters was 50.

 .

x(i) =

3.3. Prediction Using Viewer’s Trajectory History
The second term Pr (It+n (p) | φt ) in our probabilistic model
represents the information given by the viewer’s trajectory
history. The straight-forward option would be to perform a
linear extrapolation based on the last available positions. We

Fig. 3. Illustration of the overall RoI prediction scheme.

3.4. Prediction Using Combined Information

Each of the above prediction schemes results in a probability
map over the frame blocks. According to (5), we can estimate
the probability of a certain block being in the RoI as the prod-
uct of the individual probabilities in each of those maps. The
RoI center is predicted as the center of mass of the resulting
probability map, and the region size as its standard deviation.
Fig. 3 gives a schematic illustration of the overall scheme.

4. EXPERIMENTAL RESULTS

The performance of our scheme is evaluated in terms of the
Euclidean distance between the predicted RoI and the ground
truth. The Euclidean distance is computed based on vectors of
[φx φy φz ]T . We generated 60 user trajectories along 3000
frames (i.e., 60x120 seconds) and labeled them according to
the viewer behaviors. Viewers could be either following the
ball, scanning the audience, looking at the players or hav-
ing varying behaviors. For each of the experiments we ran-
domly picked 20% of the candidate trajectories for testing and
used the rest for training. While playing the test trajectories,

we performed one prediction per second, obtaining over 100
samples per trajectory. Unless otherwise noted, the training
window and lookahead duration are set to 4 and 3 seconds,
respectively.
Fig. 4 shows the mean distance between the predicted and
the actual RoI for different prediction schemes and viewing
behaviors. For each behavior, we show four bars correspond-
ing to the prediction using video features (good for well lo-
calized behaviors, such as ball or players), recent trajectory
(good for varying behaviors), combination of the previous
two, and linear extrapolation (comparison benchmark). From
the plot, it is observed that the combined scheme always has
better performance than the linear extrapolation scheme.
In order to evaluate how our schemes escalate to more
challenging requirements, we also tested their performance
for different lookahead and training window durations. Fig. 5
shows the relation between lookahead duration and perfor-
mance. For short lookahead durations, trajectory predictions
are better. However, as we try to predict further in time, the
video features offer a more reliable source of information. We
ﬁnd that as long as the algorithm has correctly identiﬁed the
viewer’s interest, it can lock on to them for a long period of

Prediction Using Trajectory History510152025303540246810Video Frame Partioned Into Blocks (Frame #182)Region Map510152025303540246810AudienceScoreboardCeilingFieldAdsGoalmouthBallDetectionDistanceTo BallMovementIntensityNumber ofPlayersPlayerDetectionRegion Map510152025303540246810AudienceScoreboardCeilingFieldAdsGoalmouthLocalLabelsTraining Feature VectorLogisticRegressionLabelQuery Feature VectorProbability Map510152025303540246810SegmentK-MeansClusteringLeast SquaresLeast SquaresLeast SquaresLeast SquaresCentroidsMinimum Distance...LSE CoeffsTraining Offline510152025303540246810LinearRegressionCombiningInformationTrajectoryPrediction Using Trajectory HistoryPrediction Using VideoFig. 4. Performance under different viewer behaviors.

Fig. 5. Performance under different lookahead duration.

time. It is also worth noting that the combined scheme cor-
responding to our probabilistic model is able to maintain a
lower error than each individual scheme for most of the tested
lookahead durations.
Finally, Fig. 6 shows the relation between training win-
dow length and performance. Longer training durations slightly
improve the accuracy of trajectory predictions, but at the cost
of increased complexity and reduced feature reliability.

5. CONCLUSIONS

Although it might seem hard to perform a real-time RoI pre-
diction for a viewer watching a soccer match, it turns out that
by analyzing his past behavior, and extracting some very sim-
ple features from a low-resolution overview video, we can
perform much better than a simple linear extrapolation model.
Furthermore, by using learning-based techniques we are able
to make the system adaptive to individual viewer’s behavior.
One interesting observation is that when we increase the

Fig. 6. Performance under different training window dura-
tion.

lookahead time, the performance of the combined scheme
does not degrade signiﬁcantly. This implies that our learning
algorithm will be very useful for delay-sensitive video stream-
ing applications.

6. ACKNOWLEDGMENTS

We would like to thank Aditya Mavlankar for sharing the soc-
cer sequence and the initial user interface. We would also like
to thank Prof. Andrew Ng, Ian Goodfellow, Honglak Lee,
Chuong (Tom) Do and Tianshi Gao for really helpful discus-
sions.

7. REFERENCES

[1] Aditya Mavlankar, Pierpaolo Baccichet, David Varo-
dayan, and Bernd Girod,
“Optimal slice size for
streaming regions of high resolution video with virtual
pan/tilt/zoom functionality,” in Proc. of 15th European
Signal Processing Conference (EUSIPCO), Sept. 2007.

[2] Aditya Mavlankar, Jeonghun Noh, Pierpaolo Baccichet,
and Bernd Girod,
“Peer-to-peer multicast live video
streaming with interactive virtual pan/tilt/zoom function-
ality,” in Proc. of International Conference on Image Pro-
cessing (ICIP), Oct. 2008.

[3] Aditya Mavlankar, David Varodayan, and Bernd Girod,
“Region-of-interest prediction for interactively streaming
regions of high resolution video,” in Proc. of 16th IEEE
International Packet Video Workshop (PV), Nov. 2007.

[4] X. Yu, C. Xu, H.W. Leong, Q. Tian, Q. Tang, and
K.W. Wan, “Trajectory-based ball detection and track-
ing with applications to semantic analysis of broadcast
soccer video,” in ACM Conference on Multimedia, Nov.
2003.

012345678910RoI Euclidean DistanceBallAudiencePlayersMixedVideo InformationTrajectory HistoryVideo Combined with TrajectorySimple Extrapolation012345678012345678lookahead time (sec.)RoI Euclidean DistanceVideo InformationTrajectory HistoryVideo Combined with TrajectorySimple Extrapolation34567891011121.522.533.54Training Window Duration (sec.)RoI Euclidean DistanceVideo InformationTrajectory HistoryVideo Combined with TrajectorySimple Extrapolation
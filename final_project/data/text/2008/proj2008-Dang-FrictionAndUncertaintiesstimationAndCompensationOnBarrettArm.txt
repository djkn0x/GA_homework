Uncertainties estimation and compensation on a robotic manipulator.
Application on Barrett arm

Duong Dang

December 12, 2008

Abstract

In this pro ject, two diﬀerent regression methods
have been used to estimate the joint friction and
gravity, inertia estimation error on a 7 degree of
freedom Barrett robotic arm. Using these estima-
tions, appropriate compensation terms have been
added to the servo loop to achieve an improvement
of a factor of 10 in tracking error.

Acknowledgement

I would like to thank Quoc Le for his advice and
a number of discussions on this problem and on
machine learning in general during the quarter. I
also would like to thank the AI Lab for providing
the Barrett arm for this pro ject.

1 Problem statement

The ob jective of this pro ject is using learning to
estimate the friction and uncertainties in dynamic
parameter of the robot. This estimation will be
used as a compensation term to be added to the
PD controller.
The equation of motion of a manipulators con-
sisting of n links is given by [1]

The control term is given from the traditional PD
controller

u = ¨qd + Kv ( ˙qd − ˙q) + Kp (qd − q)

(3)

where qd is the desired tra jectory. If there is no
friction and all the terms are estimated precisely
(i.e: ˆA = A, ˆb = b, ˆG = G), from (??), (??) and
(??) we will have

¨e + Kv ˙e + Kpe = 0

(4)

Therefore the error e = qd − q will be asymptoti-
cally zero. In the real world however, it is diﬃcult
to estimate all the dynamic parameters precisely,
a frictional term F must be added to the right
hand side of (??). We then have

¨e+Kv ˙e+Kpe = f (q , ˙q , ¨q) = ˆA−1 (∆A ¨q+∆b+∆G+Ff )
(5)
where ∆A, ∆b and ∆G are estimation errors of
dynamic parameters A, b, G.
To learn the function f (q , ˙q , ¨q), we can give the
robot a desired tra jectory. At each time stamp
t(i) , we record {q (i) , ˙q (i) , ¨q (i)} and compute f (i) by
formula (??). We then run a non parametric re-
gression on the training data. The output will be
the prediction f ∗ at any state {q∗ , ˙q∗ , ¨q∗}

τ = A(q) ¨q + G(q) + b(q , ˙q)

(1)

2 Uncertainty characteristics

Where q , ˙q and ¨q are a n-dimension vector rep-
resenting the joint angles,velocities and accelera-
tions. A, G, b represent the inertia matrix, grav-
ity term and centrifugal and Coriolis terms. The
torque τ is computed based on a control signal u
and the estimations of A , G and b

τ = ˆA(q)u + ˆG(q) + ˆb(q , ˙q)

(2)

Friction is a non linear, discontinuous function
of ˙q . Several frictional models exist in the liter-
ature [?]. The simplest way is a constant force
(Coulomb friction) always against the movement.
A more complicated model of the friction [?] in-
clude Coulomb friction, viscous and Stribeck eﬀect
(Figure 2.). One thing to be noted is that all the

1

frictional model shares a same propety. That is
friction force (and therefore torque) is a asymetric
function of ˙q

Figure 1: A complex model friction

come
Gravity and inertia estimation error
from incorrect estimate of joint masses and po-
sitions of center of mass. ∆G and ∆A are only
function of q

Coriolis and centrifugal forces estimation
error depend upon both q and ˙q .

3 Experiment setup

3.1 Barrett arm

Figure 2: Sources of uncertainties on Barrett arm

Two main sources of uncertainties on the Bar-
rett arm in this experiment are several sensors
with unknown mass mounted at the hand and joint
frictions.

2

3.2 Choice of tra jectory

In this pro ject, the goal is to learn the function
f in equation (??) and extract individual uncer-
tainty terms to ﬁnd the joint friction proﬁle and
mass oﬀsets. In general, f is fairly complicated.
Especially, the ﬁrst uncertainty term depends on
both q and ¨q . That would make it diﬃcult to ex-
tract separate uncertainty terms in the end.
The trick here is to choose a sinusoidal function
qd = C1 + C2 sin(ωt) as desired tra jectory. With
this setup we have ¨qd = −C2ω2 sin(ωt), we then
can approximate ¨q = −ω2 (q − C1 ). The uncer-
tainty term ∆ ˆA ¨q now only depends on q .
For the sake of simplicity in the problem we as-
sume that ∆b is negligible compare to other Un-
certainties. This is not a bad assumption since b
itself is usually small compared to other terms.
With the choice of tra jectory and assumption
above, equation (??) can be written as

ˆA(¨e + Kv ˙e + Kpe) = f ( ˙q , q) = Ff ( ˙q) + Fm (q) (6)

Where both Ff and Fm are non linear function,
non parametric of ˙q and Fm . Ff is a asymmetric
function of
˙q . This fact help us to separate the
terms in the right hand side of (??) after doing
regression.

4 Results

4.1 Gaussian process regression

Given noisy observations

D = {X, y},

(7)

Gaussian process regression [?] predicts the mean
µ and covariance Σ at patterns X ∗ as follow
µ = K (X, X ∗ ) (cid:2)K (X, X ) + σ2 I (cid:3) y and
Σ = K (X ∗ , X ∗ )
− K (X, X ∗ )) (cid:2)K (X, X ) + σ2 I (cid:3)−1
K (X, X ∗ )
(9)
In our problem, X and X ∗ are 2n × m and 2n × k
matrices represent the training set and the query

(8)

points.

X = [x(1) . . . x(m) ],
X ∗ = [x∗ (1) . . . x∗ (m) ],

x = (q1 . . . qn ˙q1 . . . ˙qn )T
1 . . . ˙q∗
˙q∗
n )T

1 . . . q∗
x∗ = (q∗
n

the label vector y is the column vector of the value
found in (??).

y = (y (1) . . . y (m) )T , y = f +ε where ε = N (0, σ2 I )

K (X, X ), K (X, X ∗ , and K (X ∗ , X ∗ ) are respec-
tively m × m, k × m and k × k matrices corre-
sponding to a Gaussian kernel.

K (X, X )ij = exp((x(i) − x(j ) )T Θ(x(i) − x(j ) ))
K (X, X ∗ )ij = exp((x∗ ((i) − x(j ) )T Θ(x∗ (i) − x(j ) ))
K (X ∗ , X ∗ )ij = exp((x∗ (i) − x∗ (j ) )T Θ(x∗ (i) − x∗ (j ) ))

Kernel covariance matrix Θ is a diagonal matrix
that we choose.

4.1.1 Robust regression, outliers detection

 

Training examples
Gaussian proccess prediction

1

0.5

 

0

−0.5

dq

−1

−1.5

1.7

2.1

2

1.8

1.9

q

F

3

2

1

0

−1

−2

−3
1.5

Figure 4: Regression after detecting and removing
outliers (4500 training examples)

error |µ(i) − F (i)| at all points. All the outliers
should have a large error. We can then discard the
training examples with highest error, say 1/10 of
the total data set. Figure ?? show the improve-
ment after removing outliers.

Training examples
Gaussian proccess prediction

 

4.1.2 Hyperparameters selection

3

2

1

0

−1

−2

F

−3
1.5

 

1

0.5

0

−0.5

dq

−1

−1.5

1.7

2.1

2

1.8

1.9

q

Figure 3: Naive Gaussian process regression on
initial data set (5000 training examples)

To optimize the hyperparameters, the leave-one-
out-cross validation technique has been used [?].
In this experiment, only joint 4 is chosen to move,
˙q4 )T ). We need
all other joints are ﬁxed (x = (q4
to optimize three hyperparametters:
the noise
level σ , and the bandwidths θ1 and θ2 . For each
set {σ, θ1 , θ2}, the data is randomly divided into
10 trunks then choose each of those trunks as test-
ing points with the other nine as training data
set. The total error is computed with σ rang-
ing from 0.01 to 0.2 and θ1 ,θ2 ranging from 0.02
to 0.5. The approximate optimal parameters are
σ = 0.05, θ1 = 0.1 and θ2 = 0.2

Apply the naive Gaussian process regression
does not provide a good prediction of uncertainty
f ∗ (Figure ??). The reason is outliers aﬀect
greatly Gaussian process. As a result, we need
to clean up the data before doing regression. To
detect outliers, we set X ∗ = X and do regression
with large bandwidth. After that we compute the

4.2 Support vector regression

The technique in section ?? does a decent job re-
moving outliers in our particular data set. How-
ever, in general we do not know a priori the por-
tion of outliers. Therefore, in ﬁxing the portion of
data to be removed, we might not remove all the
outliers in some cases or discard too much useful

3

data in others. Instead of Gaussian process, a sup-
port vector regression can be used to reduce the
inﬂuence of outliers.

Figure 5: The soft margin loss setting for a linear
SVM (from [?])

The key idea is ﬁnding the optimal line (in fea-
ture space) through the data set, with a loss func-
tion deﬁne in ﬁgure ?? Writing the optimization
problem and using the Kernel trick, the dual prob-
lem can be written as
maximize (− 1
2 Pl
i )(αj − α∗
i,j=1 (αi − α∗
j k(xi , xj )
i ) + Pl
−ǫ Pl
i=1 yi (αi − α∗
i )
i=1 (αi + α∗
(10)
l
Xi=1
(αi − α∗
i ∈ [0, C ]
i ) = 0 and αi , α∗
where x and x∗ are training examples and testing
points k is a Gaussian Kernel as in section ??;ǫ
and C is the parametters of our choice. The pre-
dictions are

sub ject to

(11)

f (x) =

(αi − α∗
i )k(xi , x) + b

l
Xi=1
where b is determined by the support vectors [?].
To solve problem (??) I used CVX, a package for
specifying and solving convex programs [?, ?].
Figure ?? shows that support vector regression
(red line) is much less sensitive to noise than Gaus-
sian process regression (green line).

 

Training examples
Gaussian proccess
Support vector regression

F

3

2

1

0

−1

−2

−3

−4
1

0.5

 

0

dq

−0.5

1.8

1.7

−1

1.6

1.9

q

2.3

2.2

2.1

2

Figure 6: Comparison between Gaussian process
regression and support vector regression on a noisy
data set.

Ff is an asymmetric function of ˙q , therefore

Fm (q) =

1
2
Ff ( ˙q) = F (q , ˙q) − Fm (q)

[F (q , ˙q) + F (q , − ˙q)]

(12)

(13)

Use equations (??) and (??) to the prediction
in section ?? and ??, we obtain Figure ?? and ??.
Each line in Figure ??, correspond to a ﬁx value
of ˙q in equation (??).

Gravity and inertia estimation error

 

−0.4

−0.5

−0.6

−0.7

−0.8

−0.9

−1

]
m
.
N
[
 
G
 
a
t
l
e
D

−1.1

 

1.7

1.8

1.9
q [rad]

2

2.1

Figure 7: Gravity and inertia estimation error

4.3 Friction proﬁle and gravity, inertial
estimation error

To extract the contribution of Ff and Fm to the
total uncertainty in (??), we can use the fact that

One should note that if the friction proﬁle found
in Figure ?? is inherent to this particular joint
and will not depend on tra jectory, the gravity -
inertia estimation error found here is.It is based

4

]
m
.
N
[
 
G
 
a
t
l
e
D

1

0.5

0

−0.5

−1
 
−1

 

Friction profile

−0.5

0
dq [rad/s]

0.5

1

Figure 8: Friction proﬁle

on assumption of sinusoidal tra jectory and is valid
only for this class of tra jectory.
In our particular case, it is possible to make
these estimations tra jectory independent. We
know that the ∆G and ∆A come mainly from the
mass of various sensors mounted on the robot’s
hand. We can then estimated this mass by the pre-
diction of Fm at ¨q = 0: Fm (q , ˙q)(cid:12)(cid:12) ¨q=0 = ∆G(q) =
∆mg l, where ∆m = − P msensor and l is the
moment arm from the gravity force to joint 4 at
the given point. The resulting oﬀset is ∆m =
0.254 ± 0.009 kg.

4.4 Validation

Having found the friction proﬁle and the mass oﬀ-
set of the arm, we can run a tra jectory with the
appropriate compensation terms. Figure ?? show
the improvement made by the compensation term.
A sinusoidal tra jectory at diﬀerent a frequency
(ω = 2rad/s) has been used for validation. The
maximum tracking error has been reduced from
0.06 rad to 0.005 rad, or a factor of 10.

5 Future works

Both regression methods work well with the prob-
lem. Support vector regression is not sensitive to
noise so does not requires preprocessing data. The
compensation terms reduces signiﬁcant tracking
error. The next steps will be estimating frictions
and mass error for all joints and use the compensa-
tion terms in the next version of the Barrett Arm
controller that I am helping develop.

5

0.02

0

]
d
a
r
[
 
r
o
r
r
E
 
n
o
i
t
i
s
o
P

−0.02

−0.04

−0.06

−0.08

−0.1

−0.12
 

 

Without compensation
With compensation terms

5

10
t [s]

15

20

Figure 9: Tracking error with compensation terms

References

[1] Andrew Ng, Machine Learning CS229 Lecture
Notes.

[2] Alex J. Smola, Bernhard Scholkopf A Tutorial
on Support Vector Regression statistics and
Computing, 2001.

[3] Oussama Khatib, Introduction to Robotics.
CS223A Course Reader.

[4] C. E. Rasmussen & C. K. I. Williams, Gaus-
sian Processes for Machine Learning the MIT
Press, 2006

[5] H. Olsson K.J. Astrom C. Canudas de Wit M.
Gafvert P. Lischinsk Friction Models and Fric-
tion Compensation Eur. J. Control, vol. 4, no.
3, pp. 176-195, 1998.

[6] M. Grant and S. Boyd. CVX: Matlab software
for disciplined convex programming (web page
and software). http://stanford.edu/ boyd/cvx,
December 2008.

[7] M. Grant and S. Boyd. Graph implementations
for nonsmooth convex programs, Recent Ad-
vances in Learning and Control (a tribute to
M. Vidyasagar)

[8] V. Blondel, S. Boyd, and H. Kimura, ed-
itors, pages 95-110, Lecture Notes in Con-
trol and Information Sciences, Springer, 2008.
http://stanford.edu/ boyd/graph dcp.html.


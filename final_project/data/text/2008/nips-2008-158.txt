Estimation of Information Theoretic Measures for
Continuous Random Variables

Fernando P ´erez-Cruz
Princeton University, Electrical Engineering Department
B-311 Engineering Quadrangle, 08544 Princeton (NJ)
fp@princeton.edu

Abstract

We analyze the estimation of information theoretic measures of continuous ran-
dom variables such as: differential entropy, mutual information or Kullback-
Leibler divergence. The objective of this paper is two-fold. First, we prove that the
information theoretic measure estimates using the k-nearest-neighbor density es-
timation with ﬁxed k converge almost surely, even though the k-nearest-neighbor
density estimation with ﬁxed k does not converge to its true measure. Second,
we show that the information theoretic measure estimates do not converge for k
growing linearly with the number of samples. Nevertheless, these nonconvergent
estimates can be used for solving the two-sample problem and assessing if two
random variables are independent. We show that the two-sample and indepen-
dence tests based on these nonconvergent estimates compare favorably with the
maximum mean discrepancy test and the Hilbert Schmidt independence criterion.

1

Introduction

Kullback-Leibler divergence, mutual information and differential entropy are central to information
theory [5]. The divergence [17] measures the ‘distance’ between two density distributions while
mutual information measures the information one random variable contains about a related random
variable [23]. In machine learning, statistics and neuroscience the information theoretic measures
also play a leading role. For instance, the divergence is the error exponent in large deviation theory
[5] and the divergence can be directly applied to solving the two-sample problem [1]. Mutual infor-
mation is extensively used to assess whether two random variables are independent [2] and has been
proposed to solve the all-relevant feature selection problem [8, 24]. Information-theoretic analysis of
neural data is unavoidable given the questions neurophysiologists are interested in1 . There are other
relevant applications in different research areas in which divergence estimation is used to measure
the difference between two density functions, such as multimedia [19] and text [13] classiﬁcation,
among others.
The estimation of information theoretic quantities can be traced back to the late ﬁfties [7], when Do-
brushin estimated the differential entropy for one-dimensional random variables. The review paper
by Beirlant et al. [4] analyzes the different contributions of nonparametric differential entropy esti-
mation for continuous random variables. The estimation of the divergence and mutual information
for continuous random variables has been addressed by many different authors [25, 6, 26, 18, 20, 16],
see also the references therein. Most of these approaches are based on estimating the densities ﬁrst.
For example, in [25], the authors propose to estimate the densities based on data-dependent his-
tograms with a ﬁxed number of samples from q(x) in each bin. The authors of [6] compute relative
frequencies on data-driven partitions achieving local independence for estimating mutual informa-
tion. Also, in [20, 21], the authors compute the divergence using a variational approach, in which

1See [22] for a detailed discussion on mutual information estimation in neuroscience.

1

convergence is proven ensuring that the estimate for p(x)/q(x) or log p(x)/q(x) converges to the
true measure ratio or its log ratio.
There are only a handful of approaches that use k-nearest-neighbors (k-nn) density estimation [26,
18, 16] for estimating the divergence and mutual information for ﬁnite k . Although ﬁnite k-nn
density estimation does not converge to the true measure, the authors are able to prove mean-square
consistency of their divergence estimators imposing some regularity constraint over the densities.
These proofs are based on the results reported in [15] for estimating the differential entropy with
k-nn density estimation.
The results in this paper are two-fold. First, we prove almost sure convergence of our divergence es-
of p(x)/ (cid:98)p(x) as a waiting time distribution independent of p(x). We can readily apply this result to
timate based on k-nn density estimation with ﬁnite k . Our result is based on describing the statistics
the estimation of the differential entropy and mutual information.
Second, we show that for k linearly growing with the number of samples, our estimates do not con-
verge nor present known statistics. But they can be reliably used for solving the two-sample problem
or assessing if two random variables are independent. We show that for this choice of k , the esti-
mates of the divergence or mutual information perform, respectively, as well as the maximum mean
discrepancy (MMD) test in [9] and the Hilbert Schmidt independence criterion (HSIC) proposed in
[10].
The rest of the paper is organized as follows. We prove in Section 2 the almost sure convergence
of the divergence estimate based on k-nn density estimation with ﬁxed k . We extend this result
for differential entropy and mutual information in Section 3. In Section 4 we present some exam-
ples to illustrate the convergence of our estimates and to show how can they be used to assess the
independence of related random variables. Section 5 concludes the paper with some ﬁnal remarks.

D(P ||Q) =

2 Estimation of the Kullback-Leibler Divergence
(cid:90)
If the densities P and Q exist with respect to a Lebesgue measure, the Kullback-Leibler divergence
is given by:
p(x) log p(x)
q(x) dx ≥ 0.
Rd
This divergence is ﬁnite whenever P is absolutely continuous with respect to Q and it is zero only
if P = Q.
The idea of using k-nn density estimation to estimate the divergence was put forward in [26, 18],
where they prove mean-square consistency of their estimator for ﬁnite k . In this paper, we prove
the almost sure convergence of this divergence estimator, using waiting-times distributions without
needing to impose additional conditions over the density models. Given a set with n i.i.d. samples
from p(x), X = {xi }n
i=1 , and m i.i.d. samples from q(x), X (cid:48) = {x(cid:48)
j }m
j=1 , we estimate D(P ||Q)
n(cid:88)
n(cid:88)
log (cid:98)pk (xi )
from a k-nn density estimate of p(x) and q(x) as follows:
(cid:98)Dk (P ||Q) =
(cid:98)qk (xi )
1
n
i=1
i=1
(cid:98)pk (xi ) =
Γ(d/2 + 1)
k
(n − 1)
(cid:98)qk (xi ) = k
πd/2
Γ(d/2 + 1)
1
(4)
sk (xi )d
πd/2
m
rk (xi ) and sk (xi ) are, respectively, the Euclidean distances to the k-nn of xi in X \xi and X (cid:48) , and
πd/2/Γ(d/2 + 1) is the volume of the unit-ball in Rd . Before proving (2) converges almost surely
to D(P ||Q), let us show an intermediate necessary result.
bution P , the limiting distribution of p(x)/ (cid:98)p1 (x) is exponentially distributed with unit mean for any
Lemma 1. Given n i.i.d. samples, X = {xi}n
i=1 , from an absolutely continuous probability distri-
x in the support of p(x).

+ log m
n − 1

log sk (xi )
kk (xi )

(1)

(2)

(3)

where

1
rk (xi )d

= d
n

2

Proof. Let’s initially assume p(x) is a d-dimensional uniform distribution with a given support. The
set Sx,R = {xi | (cid:107)xi − x(cid:107)2 ≤ R, xi ∈ X } contains all the samples from X inside the ball centered
in x of radius R. The radius R has to be small enough for the ball centered in x to be contained
within the support of p(x).
2 | xi ∈ Sx,R } are consequently uniformly distributed between 0 and Rd .
The samples in {(cid:107)xi − x(cid:107)d
Thereby, the limiting distribution of r1 (x)d = minxj ∈Sx,R ((cid:107)xj − x(cid:107)d
2 ) is exponentially distributed,
as it measures the waiting time between the origin and the ﬁrst event of a uniformly-spaced sample
ball centered in x, p(x)/ (cid:98)p1 (x) is distributed as a unit-mean exponential distribution as n tends to
(see Theorem 2.4 in [3]). Since p(x)nπd/2/Γ(d/2 + 1) is the mean number of samples per unit
inﬁnity.
For non-uniform absolutely-continuous P , P(r1 (x) > ε) → 0 as n → ∞ for any x in the support
and the limiting distribution of p(x)/ (cid:98)p1 (x) is a unit-mean exponential distribution.
of p(x) and any ε > 0. Therefore, as n tends to inﬁnity p(arg minxj ∈Sx,R ((cid:107)xj − x(cid:107)d
2 )) → p(x)
bution P , the limiting distribution of p(x)/ (cid:98)pk (x) is a unit-mean 1/k-variance gamma distribution
Corolary 1. Given n i.i.d. samples, X = {xi }n
i=1 , from an absolutely continuous probability distri-
for any x in the support of p(x).

Proof. In the previous proof, instead of measuring the waiting time to the ﬁrst event, we compute the
waiting time to the k th event of a uniformly-spaced sample. This waiting-time limiting distribution
is a unit-mean and 1/k-variance Erlang (gamma) distribution [14].
distribution P , then (cid:98)pk (x) P→ p(x) for any x in the support of p(x), if k → ∞ and k/n → 0, as
Corolary 2. Given n i.i.d. samples, X = {xi }n
i=1 , from an absolutely continuous probability
n → ∞.
p(x)/ (cid:98)pk (x) is a unit-mean 1/k-variance gamma distribution. As k → ∞ the variance of the gamma
Proof. The k-nn in X tends to x as k/n → 0 and n → ∞. Thereby the limiting distribution of
distribution goes to zero and consequently (cid:98)pk (x) converges to p(x).
The second corollary is the widely known result that k-nn density estimation converges to the true
k grows linearly with n, the k-nn sample in X does not converge to x, which precludes p(x)/ (cid:98)pk (x)
measure if k → ∞ and k/n → 0. We have just include it in the paper for clarity and completeness. If
to present known statistics. For this growth on k , the divergence estimate does not converge to
D(P ||Q).
Now we can prove the almost surely convergence to (1) of the estimate in (2) based on the k-nn
density estimation.
Theorem 1. Let P and Q be absolutely continuous probability measures and let P be absolutely
continuous with respect to Q. Let X = {xi}n
i=1 and X (cid:48) = {x(cid:48)
i }m
(cid:98)Dk (P ||Q)
i=1 be i.i.d. samples, respectively,
from P and Q, then
D(P ||Q)
a.s.−→
Proof. We can rearrange (cid:98)Dk (P ||Q) in (2) as follows:
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
log (cid:98)pk (xi )
(cid:98)Dk (P ||Q) =
(cid:98)qk (xi )
(cid:98)pk (xi )
(cid:98)qk (xi )
1
log q(xi )
log p(xi )
log p(xi )
1
− 1
1
=
q(xi )
n
n
n
n
i=1
i=1
i=1
i=1
The ﬁrst term is the empirical estimate of (1) and, by the law of large numbers [11], it converges
almost surely to its mean, D(P ||Q).
The limiting distributions of p(xi )/ (cid:98)pk (xi ) and q(xi )/(cid:98)qk (xi ) are unit-mean 1/k-variance gamma
(cid:90) ∞
n(cid:88)
distributions, independent of i, p(x) and q(x) (see Corollary 1). In the large sample limit:
(cid:98)pk (xi )
log p(xi )
1
kk
a.s.−→
log(z )z k−1 e−kz dz
(k − 1)!
n
0
i=1
by the law of large numbers [11].

(5)

+

(6)

(7)

3

Finally, the sum of almost surely convergent terms also converges almost surely [11], which com-
pletes our proof.
The k-nn based divergence estimator is biased, because the convergence rate of p(xi )/ (cid:98)pk (xi ) and
q(xi )/(cid:98)qk (xi ) to the unit-mean 1/k-variance gamma distribution depends on the density models and
we should not expect them to be identical. If p(x) = q(x), the divergence is zero and our estimate
is unbiased for any k (even if k/n does not tend to zero), since the statistics of the second and
third term in (6) are identical and they cancel each other out for any n (their expected mean is the
same). We use the Monte Carlo based test described in [9] with our divergence estimator to solve
the two-sample problem and decide if the samples from X and X (cid:48) actually came from the same
distribution.

3 Differential Entropy and Mutual Information Estimation

The results obtained for the divergence can be readily applied to estimate the differential entropy of
a random variable or the mutual information between two correlated random variables.
(cid:90)
The differential entropy for an absolutely continuous random variable P is given by:
h(x) = −

p(x) log p(x)dx

(8)

(9)

(10)

(11)

(12)

1
n

where

h(x) + γk

We can estimate the differential entropy given a set with n i.i.d. samples from P , X = {xi }n
i=1 ,
(cid:88)
(cid:98)hk (x) = − 1
using k-nn density estimation as follows:
log (cid:98)pk (xi )
where (cid:98)pk (xi ) is given by (3).
n
i=1
Theorem 2. Let P be an absolutely continuous probability measure and let X = {xi }n
(cid:98)hk (x)
i=1 be i.i.d.
samples from P , then
a.s.−→
(cid:90) ∞
γk = − kk
log(z )z k−1 e−kz dz
(k − 1)!
∼=0.5772 and it is known as the Euler-Mascheroni constant [12].
0
and γ1
Proof. We can rearrange (cid:98)hk (x) in (9) as follows:
n(cid:88)
n(cid:88)
n(cid:88)
(cid:98)hk (x) = − 1
log (cid:98)pk (xi ) = − 1
(cid:98)pk (xi )
log p(xi )
n
n
i=1
i=1
i=1
The ﬁrst term is the empirical estimate of (9) and, by the law of large numbers [11], it converges
The limiting distributions of p(xi )/ (cid:98)pk (xi ) is a unit-mean 1/k-variance gamma distribution, inde-
almost surely to its mean, h(x).
(cid:90) ∞
n(cid:88)
pendent of i and p(x) (see Corollary 1). In the large sample limit:
(cid:98)pk (xi )
log p(xi )
1
kk
a.s.−→
log(z )z k−1 e−kz dz = −γk
(k − 1)!
n
0
i=1
by the law of large numbers [11].
Finally, the sum of almost surely convergent terms also converges almost surely [11], which com-
pletes our proof.

log p(xi ) +

(13)

Now, we can use the expansion of the conditional differential entropy, mutual information and con-
ditional mutual information to prove the convergence of their estimates based on k-nn density esti-
mation to their values.

4

p(x, y) log p(y, x)
n(cid:88)
p(x) dxdy
log p(yi , xi )
a.s.−→
p(xi )
i=1

(cid:90)
• Conditional differential entropy:
h(y|x) = −
(cid:98)h(y|x) = − 1
n
(cid:90)
• Mutual Information:
p(x, y) log p(y, x)
I (x; y) = −
n(cid:88)
p(x)p(y) dxdy
(cid:98)I (x; |y) =
log p(yi , xi )
1
a.s.−→
p(xi )p(yi )
n
(cid:90)
i=1
• Conditional Mutual Information:
p(x, y, z) log p(y, x, z)p(z)
I (x; y|z) =
n(cid:88)
p(x, z)p(y, z) dxdydz
(cid:98)I (x; y|z) =
log p(yi , xi , zi )p(zi )
a.s.−→
p(xi , zi )p(yi , zi )
i=1

1
n

h(y|x)

I (x; y) + γk

I (x; y|z)

(14)

(15)

(16)

(17)

(18)

(19)

4 Experiments

We have carried out two sets of experiments.
In the ﬁrst one, we show the convergence of the
divergence to their limiting value as the number of samples tends to inﬁnity and we compare the
divergence estimation to the MMD test in [9] for MNIST dataset. In the second experiment, we
compute if two random variables are independent and compare the obtained results to the HSIC
proposed in [10].
We ﬁrst compare the divergence between a uniform distribution between 0 and 1 in d-dimension and
√
a zero-mean Gaussian distribution with identity covariance matrix. We plot the divergence estimates
n and k = n/2 with m = n.
for d = 1 and d = 5 in Figure 1 as a function of n, for k = 1, k =

(a)

(b)

Figure 1: We plot the divergence for d = 1 in (a) and d = 5 in (b). The solid line with ’(cid:63)’ represents
the divergence estimate for k = 1, the solid line with ’∗’ represents the divergence estimate for
√
n, the solid line with ’◦’ represents the divergence estimate for k = n/2 and the dashed-
k =
dotted line represents the divergence. The dashed-lines represent ±3 standard deviation for each
divergence estimate. We have not added symbols to them to avoid cluttering the images further and
from the plots it should be clear which conﬁdence interval is assigned to what estimate.
limiting distributions of p(x)/ (cid:98)pk (x) and q(x)/(cid:98)qk (x) are unknown and they depend on p(x) and
As expected, the divergence estimate for k = n/2 does not converge to the true divergence as the
5

1021031040.90.9511.051.11.15nKLDd=1  k=0.5nk=n0.5k=1KLD1021031044.74.84.955.15.25.35.45.5nKLDd=5  k=0.5nk=n0.5k=1KLD√
q(x), respectively. Nevertheless, this estimate converges faster to its limiting value and its variance
is much smaller than that provided by the estimates of the divergence with k =
n or k = 1. This
may indicate that using k = n/2 might be a better option for solving the two-sample problem than
actually trying to estimate the true divergence, as theorized in [9].
√
Both divergence estimates for k = 1 and k =
n converge to the true divergence as the number
n, because p(x)/ (cid:98)p1 (x) converges much faster to its limiting distribution
√
of samples tends to inﬁnity. The convergence of the divergence estimate for k = 1 is signiﬁcantly
than p(x)/ (cid:98)p√
n (x). p(x)/ (cid:98)p1 (x) converges faster because the nearest neighbor to x is much closer
faster than that with k =
n-nearest-neighbor and we need that the k-nn to be close enough to x for p(x)/ (cid:98)pk (x) to
√
than the
be close to its limiting distribution. As d grows the divergence estimates need many more samples
to converge and even for small dimensions the number of samples can be enormously large.
Nevertheless, we can still use this divergence estimate to assess whether two sets of samples come
from the same distribution, because the divergence estimate for p(x) = q(x) is unbiased for any
2(a) we plot the divergence estimator for (cid:98)D1 (3, 2) (solid line) and (cid:98)D1 (3, 3) (dashed line) mean
k . In Figure 2(a) we plot the divergence estimate between the three’s and two’s handwritten digits
in the MNIST dataset (http://yann.lecun.com/exdb/mnist/) in a 784 dimensional space. In Figure
values for 100 experiments together with their 90% conﬁdence interval. For comparison purposes
we plot the MMD test from [9], in which a kernel method was proposed for solving the two-sample
problem. We use the code available in http://www.kyb.mpg.de/bs/people/arthur/mmd.htm and use
√
its bootstrap estimate for our comparisons. For n = 5 the error rate for the test using k = 1 is 1%, for
n is 7% and for k = n/2 is 43% and for the MMD test is 34%. For n ≥ 10 all tests reported
k =
zero error rate. It seems than the k = 1 test is more powerful than the MMD test in this case, at
least for small n. But we can see that the conﬁdence interval for the MMD test decreases faster than
the test based on the divergence estimate with k = 1 and we should expect better performance for
larger n, similar to the divergence estimate with k = n/2.

Figure 2: In (a) we plot (cid:98)D1 (3||2) (solid), (cid:98)D1 (3||3) (dashed) and their 90% conﬁdence interval
(b)
(a)
(dotted). In (b) we repeat the same plots using the MMD test from [9].
(cid:21) (cid:20)x1
(cid:20) cos(θ)
(cid:20)y1
(cid:21)
(cid:21)
In the second example we compute the mutual information between y1 and y2 , which are given by:
sin(θ)
=
− sin(θ)
(20)
cos(θ)
y2
x2
where x1 and x2 are independent and uniformly distributed between 0 and 1, and θ ∈ [0, π/4]. If
θ is zero, y1 and y2 are independent. Otherwise they are not independent, but still uncorrelated for
any θ .
We carry out a test for describing if y1 and y2 are independent. The test is identical to the one
described in [10] and we use the Mote Carlo resampling technique proposed in that paper with a
In Figure 3 we report the acceptance of the null
95% conﬁdence interval and 1000 repetitions.
√
hypothesis (y1 and y2 are independent) as a function of θ for n = 100 in (a) and as a function of n
for θ = π/8 in (b). We compute the mutual information with k = 1, k =
n and k = n/2 for our
test, and compare it to the HSIC in [10].

6

101102−200−1000100200300400500nD(3||3)   D(3||2)Divergence101102−0.2−0.15−0.1−0.0500.050.10.150.20.250.3nMMD(3||3)   MMD(3||2)Maximum Mean Discrepancy(a)

(b)

Figure 3: We plot the acceptance of the null hypothesis (y1 and y2 are independent) for a 95%
conﬁdence interval in (a) as a function of θ and in (b) as a function on (n). The solid line uses the
√
mutual information estimate with k = n/2 and the dash-dotted line uses the HSIC. The dashed and
dotted lines, respectively, use the mutual information estimate with k =
n and k = 1.

The HSIC test and the mutual information estimate based test with k = n/2 perform equally well
√
at predicting whether y1 and y2 are independent, while the test based on the mutual information
estimates with k = 1 and k =
n clearly underperforms. This example shows that if our goal is
to predict whether two random variables are independent we are better off using HSIC or a noncon-
vergent estimate of the mutual information rather than trying to compute the mutual information as
accurately as possible. Furthermore, in our test, the computational complexity of computing HSIC
for n = 5000 is over 10 times more computationally costly (running time) than computing the
mutual information for k = n/22 .
As we saw in the case of the divergence estimate in Figure 1, mutual information is more accurately
estimated when k = 1, but at the cost of a higher variance. If our objective is to estimate the mutual
information (or the divergence), we should use a small value of k , ideally k = 1. However, if we are
interested in assessing whether two random variables are independent, it is better to use k = n/2,
because the variance of the estimate is much lower, even though it does not converge to the mutual
information (or the divergence).

5 Conclusions

We have proved that the estimates of the differential entropy, mutual information and divergence
based on k-nn density estimation for ﬁnite k converge almost surely, even though the density esti-
mate does not converge. The previous literature could only prove mean-squared consistency and it
scribing the limiting distribution of p(x)/ (cid:98)pk (x). This limiting distribution can be easily described
required imposing some constraints over the density models. The proof in this paper relies on de-
using waiting-times distributions, such as the exponential or the Erlang distributions.
We have shown, experimentally, that ﬁxing k = 1 achieves the fastest convergence rate, at the
√
expense of a higher variance for our estimator. The divergence, mutual information and differential
n we can prove that (cid:98)pk (x) converges to p(x) while for ﬁnite k this convergences does not
√
entropy estimates using k = 1 are much better than the estimates using k =
n, even though for
k =
occur.
Finally, if we are interested in solving the two-sample problem or assessing if two random variables
are independent, it is best to ﬁx k to a fraction of n (we have used k = n/2 in our experiments),
although in this case the estimates do not converge to the true value. Nevertheless, their variances
are signiﬁcantly lower, which allows our tests to perform better. The tests with k = n/2 perform as
well as the MMD test for solving the two sample problem and the HSIC for assessing independence.

2For computing HSIC test we use A. Gretton code in http://www.kyb.mpg.de/bs/people/arthur/indep.htm
and for ﬁnding the k-nn we use the sort function in Matlab.

7

00.10.20.30.40.50.60.700.10.20.30.40.50.60.70.80.91qAcept H0n=100  k=0.5nHSICk=n0.5k=110210300.10.20.30.40.50.60.70.80.91nAcept H0q=p/8  k=0.5nHSICk=n0.5k=1Acknowledgment

Fernando Prez-Cruz is supported by Marie Curie Fellowship 040883-AI-COM. This work was par-
tially funded by Spanish government (Ministerio de Educaci ´on y Ciencia TEC2006-13514-C02-
01/TCM.

References
[1] N. Anderson, P. Hall, and D. Titterington. Two-sample test statistics for measuring discrepancies be-
tween two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis, 50(1):41–54, 7 1994.
[2] F. R. Bach and M. I. Jordan. Kernel independent component analysis. JMLR, 3:1–48, 2004.
[3] K. Balakrishnan and A. P. Basu. The Exponential Distribution: Theory, Methods and Applications. Gor-
don and Breach Publishers, Amsterdam, Netherlands, 1996.
[4] J. Beirlant, E. Dudewicz, L. Gyorﬁ, and E. van der Meulen. Nonparametric entropy estimation: An
overview. nternational Journal of the Mathematical Statistics Sciences, pages 17–39, 1997.
[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, USA, 1991.
[6] G. A. Darbellay and I. Vajda. Estimation of the information by an adaptive partitioning of the observation
space. IEEE Trans. Information Theory, 45(4):1315–1321, 5 1999.
[7] R. L. Dobrushin. A simpliﬁed method for experimental estimate of the entropy of a stationary sequence.
Theory of Probability and its Applications, (4):428–430, 1958.
[8] F. Fleuret. Fast binary feature selection with conditional mutual information. JMLR, 5:1531–1555, 2004.
[9] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch ¨olkopf, and A. Smola. A kernel method for the two-
In B. Sch ¨olkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information
sample-problem.
Processing Systems 19, Cambridge, MA, 2007. MIT Press.
[10] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch ¨olkopf, and A. Smola. A kernel statistical test of
independence. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information
Processing Systems 20, Cambridge, MA, 2008. MIT Press.
[11] G.R. Grimmett and D.R. Stirzaker. Probability and Random Processes. Oxford University Press, Oxford,
UK, 3 edition, 2001.
[12] Julian Havil. Gamma: Exploring Euler’s Constant. Princeton University Press, New York, USA, 2003.
[13] S. Mallela I. S. Dhillon and R. Kumar. A divisive information-theoretic feature clustering algorithm for
text classiﬁcation. JMLR, 3:1265–1287, 3 2003.
[14] Leonard Kleinrock. Queueing Systems. Volume 1: Theory. Wiley, New York, USA, 1975.
[15] L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problems
Inform. Transmission, 23(2):95–101, 4 1987.
[16] A. Kraskov, H. St ¨ogbauer, and P. Grassberger. Estimating mutual information. Physical Review E,
69(6):1–16, 6 2004.
[17] S. Kullback and R. A. Leibler. On information and sufﬁciency. Ann. Math. Stats., 22(1):79–86, 3 1951.
[18] N. N. Leonenko, L. Pronzato, and V. Savani. A class of renyi information estimators for multidimensional
densities. Annals of Statistics, 2007. Submitted.
[19] P. J. Moreno, P. P. Ho, and N. Vasconcelos. A kullback-leibler divergence based kernel for svm classiﬁ-
cation in multimedia applications. Technical Report HPL-2004-4, HP Laboratories, 2004.
[20] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Nonparametric estimation of the likelihood ratio and
divergence functionals. In IEEE Int. Symp. Information Theory, Nice, France, 6 2007.
[21] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems 20, Cambridge, MA, 2008. MIT Press.
[22] L. Paninski. Estimation of entropy and mutual information. Neural Compt, 15(6):1191–1253, 6 2003.
[23] C. E. Shannon. A mathematical theory of communication. Bell System Tech. J., pages 379–423, 1948.
[24] K. Torkkola. Feature extraction by non parametric mutual information maximization. JMLR, 3:1415–
1438, 2003.
[25] Q. Wang, S. Kulkarni, and S. Verd ´u. Divergence estimation of continuous distributions based on data-
dependent partitions. IEEE Trans. Information Theory, 51(9):3064–3074, 9 2005.
[26] Q. Wang, S. Kulkarni, and S. Verd ´u. A nearest-neighbor approach to estimating divergence between
continuous random vectors. In IEEE Int. Symp. Information Theory, Seattle, USA, 7 2006.

8


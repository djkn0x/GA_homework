Policy Search for Motor Primitives in Robotics

Jens Kober, Jan Peters
Max Planck Institute for Biological Cybernetics
Spemannstr. 38
72076 Tübingen, Germany
{jens.kober,jan.peters}@tuebingen.mpg.de

Abstract

Many motor skills in humanoid robotics can be learned using parametrized motor
primitives as done in imitation learning. However, most interesting motor learn-
ing problems are high-dimensional reinforcement learning problems often beyond
the reach of current methods. In this paper, we extend previous work on policy
learning from the immediate reward case to episodic reinforcement learning. We
show that this results in a general, common framework also connected to pol-
icy gradient methods and yielding a novel algorithm for policy learning that is
particularly well-suited for dynamic motor primitives. The resulting algorithm is
an EM-inspired algorithm applicable to complex motor learning tasks. We com-
pare this algorithm to several well-known parametrized policy search methods and
show that it outperforms them. We apply it in the context of motor learning and
show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAMTM
robot arm.

1
Introduction
Policy search, also known as policy learning, has become an accepted alternative of value function-
based reinforcement learning [2]. In high-dimensional domains with continuous states and actions,
such as robotics, this approach has previously proven successful as it allows the usage of domain-
appropriate pre-structured policies, the straightforward integration of a teacher’s presentation as
well as fast online learning [2, 3, 10, 18, 5, 6, 4]. In this paper, we will extend the previous work
in [17, 18] from the immediate reward case to episodic reinforcement learning and show how it
relates to policy gradient methods [7, 8, 11, 10]. Despite that many real-world motor learning tasks
are essentially episodic [14], episodic reinforcement learning [1] is a largely undersubscribed topic.
The resulting framework allows us to derive a new algorithm called Policy Learning by Weighting
Exploration with the Returns (PoWER) which is particularly well-suited for learning of trial-based
tasks in motor control. We are especially interested in a particular kind of motor control policies also
known as dynamic motor primitives [22, 23]. In this approach, dynamical systems are being used in
order to encode a policy, i.e., we have a special kind of parametrized policy which is well-suited for
robotics problems.
We show that the presented algorithm works well when employed in the context of learning dynamic
motor primitives in four different settings, i.e., the two benchmark problems from [10], the Under-
actuated Swing-Up [21] and the complex task of Ball-in-a-Cup [24, 20]. Both the Underactuated
Swing-Up as well as the Ball-in-a-Cup are achieved on a real Barrett WAMTM robot arm. Please also
refer to the video on the ﬁrst author’s website. Looking at these tasks from a human motor learning
perspective, we have a human acting as teacher presenting an example for imitation learning and,
subsequently, the policy will be improved by reinforcement learning. Since such tasks are inherently
single-stroke movements, we focus on the special class of episodic reinforcement learning. In our
experiments, we show how a presented movement is recorded using kinesthetic teach-in and, subse-
quently, how a Barrett WAMTM robot arm is learning the behavior by a combination of imitation and
reinforcement learning.

1

2 Policy Search for Parameterized Motor Primitives
Our goal is to ﬁnd reinforcement learning techniques that can be applied to a special kind of pre-
structured parametrized policies called motor primitives [22, 23], in the context of learning high-
dimensional motor control tasks.
In order to do so, we ﬁrst discuss our problem in the general
context of reinforcement learning and introduce the required notation in Section 2.1. Using a gener-
alization of the approach in [17, 18], we derive a new EM-inspired algorithm called Policy Learning
by Weighting Exploration with the Returns (PoWER) in Section 2.3 and show how the general
framework is related to policy gradients methods in 2.2. [12] extends the [17] algorithm to episodic
reinforcement learning for discrete states; we use continuous states. Subsequently, we discuss how
we can turn the parametrized motor primitives [22, 23] into explorative [19], stochastic policies.
2.1 Problem Statement & Notation
In this paper, we treat motor primitive learning problems in the framework of reinforcement learning
with a strong focus on the episodic case [1]. We assume that at time t there is an actor in a state
st and chooses an appropriate action at according to a stochastic policy π(at |st , t). Such a policy
is a probability distribution over actions given the current state. The stochastic formulation allows
a natural incorporation of exploration and, in the case of hidden state variables, the optimal time-
invariant policy has been shown to be stochastic [8]. Upon the completion of the action, the actor
transfers to a state st+1 and receives a reward rt . As we are interested in learning complex motor
tasks consisting of a single stroke [23], we focus on ﬁnite horizons of length T with episodic restarts
[1] and learn the optimal parametrized, stochastic policy for such reinforcement learning problems.
We assume an explorative version of the dynamic motor primitives [22, 23] as parametrized policy π
with parameters θ ∈ Rn . However, in this section, we will keep most derivations sufﬁciently general
that they would transfer to various other parametrized policies. The general goal in reinforcement
learning is to optimize the expected return of the policy π with parameters θ deﬁned by
J (θ) =  Tp(τ )R(τ )dτ ,
(1)
where T is the set of all possible paths, rollout τ = [s1:T +1 , a1:T ] (also called episode or trial)
denotes a path of states s1:T +1 = [s1 , s2 , . . ., sT +1 ] and actions a1:T = [a1 , a2 , . . ., aT ]. The
p(τ ) = p(s1 )QT
R(τ ) = T −1PT
probability of rollout τ is denoted by p(τ ) while R(τ ) refers to its return. Using the standard
assumptions of Markovness and additive accumulated rewards, we can write
t=1 p(st+1 |st , at )π(at |st , t),
t=1 r(st , at , st+1 , t),
(2)
where p(s1 ) denotes the initial state distribution, p(st+1 |st , at ) the next state distribution condi-
tioned on last state and action, and r(st , at , st+1 , t) denotes the immediate reward.
While episodic Reinforcement Learning (RL) problems with ﬁnite horizons are common in mo-
tor control, few methods exist in the RL literature, e.g., Episodic REINFORCE [7], the Episodic
Natural Actor Critic eNAC [10] and model-based methods using differential-dynamic programming
[21]. Nevertheless, in the analytically tractable cases, it has been studied deeply in the optimal
control community where it is well-known that for a ﬁnite horizon problem, the optimal solution
is non-stationary [15] and, in general, cannot be represented by a time-independent policy. The
motor primitives based on dynamical systems [22, 23] are a particular type of time-variant policy
representation as they have an internal phase which corresponds to a clock with additional ﬂexibility
(e.g., for incorporating coupling effects, perceptual inﬂuences, etc.), thus, they can represent optimal
solutions for ﬁnite horizons. We embed this internal clock or movement phase into our state and,
thus, from optimal control perspective have ensured that the optimal solution can be represented.
2.2 Episodic Policy Learning
In this section, we discuss episodic reinforcement learning in policy space which we will refer to
as Episodic Policy Learning. For doing so, we ﬁrst discuss the lower bound on the expected return
suggested in [17] for guaranteeing that policy update steps are improvements. In [17, 18] only the
immediate reward case is being discussed, we extend their framework to episodic reinforcement
learning and, subsequently, derive a general update rule which yields the policy gradient theorem
[8], a generalization of the reward-weighted regression [18] as well as the novel Policy learning by
Weighting Exploration with the Returns (PoWER) algorithm.
2.2.1 Bounds on Policy Improvements
Unlike in reinforcement learning, other machine learning branches have focused on optimizing lower
bounds, e.g., resulting in expectation-maximization (EM) algorithms [16]. The reasons for this pref-
erence apply in policy learning: if the lower bound also becomes an equality for the sampling policy,

2

we can guarantee that the policy will be improved by optimizing the lower bound. Surprisingly, re-
sults from supervised learning can be transferred with ease. For doing so, we follow the scenario
suggested in [17], i.e., generate rollouts τ using the current policy with parameters θ which we
0 .
weight with the returns R (τ ) and subsequently match it with a new policy parametrized by θ
This matching of the success-weighted path distribution is equivalent to minimizing the Kullback-
Leibler divergence D (pθ 0 (τ ) kpθ (τ ) R (τ )) between the new path distribution pθ 0 (τ ) and the
reward-weighted previous one pθ (τ ) R (τ ). As shown in [17, 18], this results in a lower bound on
the expected return using Jensen’s inequality and the concavity of the logarithm, i.e.,
pθ (τ )
pθ (τ ) R (τ ) log pθ 0 (τ )
pθ (τ ) pθ 0 (τ ) R (τ ) dτ ≥ T
0 ) = log T
log J (θ
pθ (τ ) dτ + const,
∝ −D (pθ (τ ) R (τ ) kpθ 0 (τ )) = Lθ (θ
0 ),
(4)
where D (p (τ ) kq (τ )) =  p (τ ) log(p (τ ) /q (τ ))dτ is the Kullback-Leibler divergence which is
considered a natural distance measure between probability distributions, and the constant is needed
for tightness of the bound. Note that pθ (τ ) R (τ ) is an improper probability distribution as pointed
out in [17]. The policy improvement step is equivalent to maximizing the lower bound on the
0 ) and we show how it relates to previous policy learning methods.
expected return Lθ (θ
2.2.2 Resulting Policy Updates
In the following part, we will discuss three different policy updates which directly result from Sec-
tion 2.2.1. First, we show that policy gradients [7, 8, 11, 10] can be derived from the lower bound
0 ) (as was to be expected from supervised learning, see [13]). Subsequently, we show that
Lθ (θ
natural policy gradients can be seen as an additional constraint regularizing the change in the path
distribution resulting from a policy update when improving the policy incrementally. Finally, we
will show how expectation-maximization (EM) algorithms for policy learning can be generated.
0 ) that deﬁnes the lower bound on the
Policy Gradients. When differentiating the function Lθ (θ
where T is the set of all possible paths and ∂θ 0 log pθ 0 (τ ) = PT
expected return, we directly obtain
0 ) =  Tpθ (τ ) R (τ ) ∂θ 0 log pθ 0 (τ ) dτ ,
∂θ 0 Lθ (θ
(5)
t=1 ∂θ 0 log π(at |st , t) denotes the
log-derivative of the path distribution. As this log-derivative only depends on the policy, we can
estimate a gradient from rollouts without having a model by simply replacing the expectation by
0 is close to θ , we have the policy gradient estimator which is widely known as
a sum; when θ
0 ) = ∂θ J (θ). Obviously, a reward which
Episodic REINFORCE [7], i.e., we have limθ 0→θ ∂θ 0 Lθ (θ
precedes an action in an rollout, can neither be caused by the action nor cause an action in the same
rollout. Thus, when inserting Equations (2) into Equation (5), all cross-products between rt and
o
nPT
∂θ log π(at+δt |st+δt , t + δ t) for δ t > 0 become zero in expectation [10]. Therefore, we can omit
these terms and rewrite the estimator as
where Qπ (s, a, t) = E {PT
t=1∂θ 0 log π(at |st , t)Qπ (s, a, t)
0 ) = E
∂θ 0 Lθ (θ
(6)
,
˜t=t r(s˜t , a˜t , s˜t+1 , ˜t)|st = s, at = a} is called the state-action value
0 → θ in the inﬁnite
function [1]. Equation (6) is equivalent to the policy gradient theorem [8] for θ
horizon case where the dependence on time t can be dropped.
The derivation results in the Natural Actor Critic as discussed in [9, 10] when adding an additional
punishment to prevent large steps away from the observed path distribution. This can be achieved by
restricting the amount of change in the path distribution and, subsequently, determining the steepest
descent for a ﬁxed step away from the observed trajectories. Change in probability distributions
is naturally measured using the Kullback-Leibler divergence, thus, after adding the additional con-
straint of D(pθ 0 (τ )kpθ (τ )) ≈ 0.5(θ
0 − θ)TF(θ)(θ
0 − θ) = δ using a second-order expansion as
approximation where F(θ) denotes the Fisher information matrix [9, 10].
Policy Search via Expectation Maximization. One major drawback of gradient-based ap-
proaches is the learning rate, an open parameter which can be hard to tune in control problems
but is essential for good performance. Expectation-Maximization algorithms are well-known to
avoid this problem in supervised learning while even yielding faster convergence [16]. Previously,
similar ideas have been explored in immediate reinforcement learning [17, 18]. In general, an EM-
0 ). In
algorithm would choose the next policy parameters θn+1 such that θn+1 = argmaxθ 0 Lθ (θ
nPT
o
the case where π(at |st , t) belongs to the exponential family, the next policy can be determined
analytically by setting Equation (6) to zero, i.e.,
t=1∂θ 0 log π(at |st , t)Qπ (s, a, t)
E

= 0,

(3)

(7)

3

Algorithm 1 Policy learning by Weighting Exploration with the Returns for Motor Primitives
Input: initial policy parameters θ0
repeat
Sample: Perform rollout(s) using a = (θ + εt )Tφ(s, t) with [εt ]ij ∼ N (0, σ2
Estimate: Use unbiased estimate ˆQπ (s, a, t) = PT
ij ) as stochastic
policy and collect all (t, st , at , st+1 , εt , rt+1 ) for t = {1, 2, . . . , T + 1}.
˜t=t r(s˜t , a˜t , s˜t+1 , ˜t).
DPT
E
.DPT
E
Reweight: Compute importance weights and reweight rollouts, discard low-importance roll-
outs.
Update policy using θk+1 = θk +
t=1εtQπ (s, a, t)
t=1Qπ (s, a, t)
until Convergence θk+1 ≈ θk

w(τ )

.

w(τ )

0 . Depending on the choice of a stochastic policy, we will obtain different solutions
and solving for θ
and different learning algorithms. It allows the extension of the reward-weighted regression to larger
horizons as well as the introduction of the Policy learning by Weighting Exploration with the Returns
(PoWER) algorithm.

2.3 Policy learning by Weighting Exploration with the Returns (PoWER)
In most learning control problems, we attempt to have a deterministic mean policy ¯a = θTφ(s, t)
with parameters θ and basis functions φ. In Section 3, we will introduce the basis functions of
the motor primitives. When learning motor primitives, we turn this deterministic mean policy
¯a = θTφ(s, t) into a stochastic policy using additive exploration ε(s, t) in order to make model-
free reinforcement learning possible, i.e., we always intend to have a policy π(at |st , t) which can be
brought into the form a = θTφ(s, t) + (φ(s, t)). Previous work in this context [7, 4, 10, 18], with
the notable exception of [19], has focused on state-independent, white Gaussian exploration, i.e.,
(φ(s, t)) ∼ N (0, Σ). It is straightforward to obtain the Reward-Weighted Regression for episodic
0 which naturally yields a weighted regression method with the
RL by solving Equation (7) for θ
state-action values Qπ (s, a, t) as weights. This form of exploration has resulted into various ap-
plications in robotics such as T-Ball batting, Peg-In-Hole, humanoid robot locomotion, constrained
reaching movements and operational space control, see [4, 10, 18] for both reviews and their own
applications.
However, such unstructured exploration at every step has a multitude of disadvantages: it causes a
large variance which grows with the number of time-steps [19, 10], it perturbs actions too frequently
‘washing’ out their effects and can damage the system executing the trajectory. As a result, all
methods relying on this state-independent exploration have proven too fragile for learning the Ball-
in-a-Cup task on a real robot system. Alternatively, as introduced by [19], one could generate a form
t φ(s, t) with [εt ]ij ∼ N (0, σ2
ij ), where
of structured, state-dependent exploration (φ(s, t)) = εT
ij are meta-parameters of the exploration that can also be optimized. This argument results into
σ2
the policy a ∼ π(at |st , t) = N (a|θTφ(s, t), ˆΣ(s, t)). Inserting the resulting policy into Equation
nPT
o−1
nPT
o
(7), we obtain the optimality condition in the sense of Equation (7) and can derive the update rule
0 = θ + E
t=1Qπ (s, a, t)W(s, t)εt
t=1Qπ (s, a, t)W(s, t)
(8)
θ
E
with W(s, t) = φ(s, t)φ(s, t)T /(φ(s, t)Tφ(s, t)). Note that for our motor primitives W reduces
to a diagonal, constant matrix and cancels out. Hence the simpliﬁed form in Algorithm 1.
In
order to reduce the number of rollouts in this on-policy scenario, we reuse the rollouts through
importance sampling as described in the context of reinforcement learning in [1]. To avoid the
fragility sometimes resulting from importance sampling in reinforcement learning, samples with
very small importance weights are discarded. The expectations E {·} are replaced by the importance
sampler denoted by h·iw(τ ) . The resulting algorithm is shown in Algorithm 1. As we will see in
Section 3, this PoWER method outperforms all other described methods signiﬁcantly.

3 Application to Motor Primitive Learning for Robotics
In this section, we demonstrate the effectiveness of the algorithm presented in Section 2.3 in the
context of motor primitive learning for robotics. For doing so, we will ﬁrst give a quick overview
how the motor primitives work and how the algorithm can be used to adapt them. As ﬁrst evaluation,
we will show that the novel presented PoWER algorithm outperforms many previous well-known

4

methods, i.e., ‘Vanilla’ Policy Gradients, Finite Difference Gradients, the Episodic Natural Actor
Critic and the generalized Reward-Weighted Regression on the two simulated benchmark problems
suggested in [10] and a simulated Underactuated Swing-Up [21]. Real robot applications are done
with our best benchmarked method, the PoWER method. Here, we ﬁrst show PoWER can learn
the Underactuated Swing-Up [21] even on a real robot. As a signiﬁcantly more complex motor
learning task, we show how the robot can learn a high-speed Ball-in-a-Cup [24] movement with
motor primitives for all seven degrees of freedom of our Barrett WAMTM robot arm.

3.1 Using the Motor Primitives in Policy Search
The motor primitive framework [22, 23] can be described as two coupled differential equations, i.e.,
we have a canonical system ˙y = f (y, z) with movement phase y and possible external coupling to z
as well as a nonlinear system ¨x = g(x, ˙x, y, θ) which yields the current action for the system. Both
dynamical systems are chosen to be stable and to have the right properties so that they are useful for
the desired class of motor control problems. In this paper, we focus on single stroke movements as
they frequently appear in human motor control [14, 23] and, thus, we will always choose the point
attractor version of the motor primitives exactly as presented in [23] and not the older one in [22].
The biggest advantage of the motor primitive framework of [22, 23] is that the function g is linear
in the policy parameters θ and, thus, well-suited for imitation learning as well as for our presented
reinforcement learning algorithm. For example, if we would have to learn only a motor primitive for
a single degree of freedom qi , then we could use a motor primitive in the form ¯¨qi = g(qi , ˙qi , y , θ) =
φ(s)Tθ where s = [qi , ˙qi , y ] is the state and where time is implicitly embedded in y . We use the
output of ¯¨qi = φ(s)Tθ = ¯a as the policy mean. The perturbed accelerations ¨qi = a = ¯a + ε is given
to the system. The details of φ are given in [23].
In Sections 3.3 and 3.4, we use im-
itation learning for the initialization.
For imitations, we follow [22]: ﬁrst,
extract the duration of the movement
from initial and ﬁnal zero velocity
and use it to adjust the time constants.
Second, use locally-weighted regres-
sion to solve for an imitation from a
single example.

Figure 1: This ﬁgure shows the mean performance of all
compared methods in two benchmark tasks averaged over
twenty learning runs with the error bars indicating the stan-
dard deviation. Policy learning by Weighting Exploration
with the Returns (PoWER) clearly outperforms Finite Dif-
ference Gradients (FDG), ‘Vanilla’ Policy Gradients (VPG),
the Episodic Natural Actor Critic (eNAC) and the adapted
Reward-Weighted Regression (RWR) for both tasks.

3.2 Benchmark Comparison
As benchmark comparison, we in-
tend to follow a previously studied
scenario in order to evaluate which
method is best-suited for our prob-
lem class. For doing so, we perform
our evaluations on the exact same
benchmark problems as [10] and use
two tasks commonly studied in mo-
tor control literature for which the analytic solutions are known, i.e., a reaching task where a goal
has to be reached at a certain time while the used motor commands have to be minimized and a
reaching task of the same style with an additional via-point. In this comparison, we mainly want to
show the suitability of our algorithm and show that it outperforms previous methods such as Finite
Difference Gradient (FDG) methods [10], ‘Vanilla’ Policy Gradients (VPG) with optimal baselines
[7, 8, 11, 10], the Episodic Natural Actor Critic (eNAC) [9, 10], and the episodic version of the
Reward-Weighted Regression (RWR) algorithm [18]. For both tasks, we use the same rewards as in
[10] but we use the newer form of the motor primitives from [23]. All open parameters were manu-
ally optimized for each algorithm in order to maximize the performance while not destabilizing the
convergence of the learning process.
When applied in the episodic scenario, Policy learning by Weighting Exploration with the Returns
(PoWER) clearly outperformed the Episodic Natural Actor Critic (eNAC), ‘Vanilla’ Policy Gradient
(VPG), Finite Difference Gradient (FDG) and the adapted Reward-Weighted Regression (RWR)
for both tasks. The episodic Reward-Weighted Regression (RWR) is outperformed by all other
algorithms suggesting that this algorithm does not generalize well from the immediate reward case.

5

102103−1000−500−250number of rolloutsaverage return(a) minimum motor command102103−102−101number of rolloutsaverage return(b) passing through a point  FDGVPGeNACRWRPoWERFigure 2: This ﬁgure shows the time series of the Underactuated Swing-Up where only a single joint
of the robot is moved with a torque limit ensured by limiting the maximal motor current of that joint.
The resulting motion requires the robot to (i) ﬁrst move away from the target to limit the maximal
required torque during the swing-up in (ii-iv) and subsequent stabilization (v). The performance of
the PoWER method on the real robot is shown in (vi).

While FDG gets stuck on a plateau, both eNAC and VPG converge to the same, good ﬁnal solution.
PoWER ﬁnds the same (or even slightly better) solution while achieving it noticeably faster. The
results are presented in Figure 1. Note that this plot has logarithmic scales on both axes, thus a
unit difference corresponds to an order of magnitude. The omission of the ﬁrst twenty rollouts was
necessary to cope with the log-log presentation.
3.3 Underactuated Swing-Up
As additional simulated benchmark and for the real-
robot evaluations, we employed the Underactuated
Swing-Up [21]. Here, only a single degree of free-
dom is represented by the motor primitive as described
in Section 3.1. The goal is to move a hanging heavy
pendulum to an upright position and stabilize it there
in minimum time and with minimal motor torques.
Figure 3: This ﬁgure shows the perfor-
By limiting the motor current for that degree of free-
mance of all compared methods for the
dom, we can ensure that the torque limits described in
swing-up in simulation and show the mean
[21] are maintained and directly moving the joint to
performance averaged over 20 learning
the right position is not possible. Under these torque
runs with the error bars indicating the stan-
limits, the robot needs to (i) ﬁrst move away from the
dard deviation. PoWER outperforms the
target to limit the maximal required torque during the
other algorithms from 50 rollouts on and
swing-up in (ii-iv) and subsequent stabilization (v) as
ﬁnds a signiﬁcantly better policy.
illustrated in Figure 2 (i-v). This problem is similar to
a mountain-car problem where the car would have to stop on top or experience a failure.
The applied torque limits were the same as in [21] and so was the reward function was the except
that the complete return of the trajectory was transformed by an exp(·) to ensure positivity. Again all
open parameters were manually optimized. The motor primitive with nine shape parameters and one
goal parameter was initialized by imitation learning from a kinesthetic teach-in. Subsequently, we
compared the other algorithms as previously considered in Section 3.2 and could show that PoWER
would again outperform them. The results are given in Figure 3. As it turned out to be the best
performing method, we then used it successfully for learning optimal swing-ups on a real robot. See
Figure 2 (vi) for the resulting real-robot performance.
3.4 Ball-in-a-Cup on a Barrett WAMTM
The most challenging application in this paper is the children’s game Ball-in-a-Cup [24] where a
small cup is attached at the robot’s end-effector and this cup has a small wooden ball hanging down
from the cup on a 40cm string.
Initially, the ball is hanging down vertically. The robot needs
to move fast in order to induce a motion at the ball through the string, swing it up and catch it
with the cup, a possible movement is illustrated in Figure 4 (top row). The state of the system is
described in joint angles and velocities of the robot and the Cartesian coordinates of the ball. The
actions are the joint space accelerations where each of the seven joints is represented by a motor
primitive. All motor primitives are perturbed separately but employ the same joint ﬁnal reward
given by r(tc ) = exp(−α(xc − xb )2 − α(yc − yb )2 ) while r(t) = 0 for all other t 6= tc where tc
is the moment where the ball passes the rim of the cup with a downward direction, the cup position
denoted by [xc , yc , zc ] ∈ R3 , the ball position [xb , yb , zb ] ∈ R3 and a scaling parameter α = 100.
The task is quite complex as the reward is not modiﬁed solely by the movements of the cup but
foremost by the movements of the ball and the movements of the ball are very sensitive to changes
in the movement. A small perturbation of the initial condition or during the trajectory will drastically
change the movement of the ball and hence the outcome of the rollout.

6

501001502000.60.70.80.91number of rolloutsaverage return  RWRPoWERFDGVPGeNACFigure 4: This ﬁgure shows schematic drawings of the Ball-in-a-Cup motion, the ﬁnal learned robot
motion as well as a kinesthetic teach-in. The green arrows show the directions of the current move-
ments in that frame. The human cup motion was taught to the robot by imitation learning with
31 parameters per joint for an approximately 3 seconds long trajectory. The robot manages to re-
produce the imitated motion quite accurately, but the ball misses the cup by several centimeters.
After ca. 75 iterations of our Policy learning by Weighting Exploration with the Returns (PoWER)
algorithm the robot has improved its motion so that the ball goes in the cup. Also see Figure 5.

Due to the complexity of the task, Ball-in-a-Cup is
even a hard motor learning task for children who usu-
ally only succeed at it by observing another person
playing and a lot of improvement by trial-and-error.
Mimicking how children learn to play Ball-in-a-Cup,
we ﬁrst initialize the motor primitives by imitation and,
subsequently, improve them by reinforcement learn-
ing. We recorded the motions of a human player by
kinesthetic teach-in in order to obtain an example for
imitation as shown in Figure 4 (middle row). From the
Figure 5: This ﬁgure shows the expected
imitation, it can be determined by cross-validation that
return of the learned policy in the Ball-in-
31 parameters per motor primitive are needed. As ex-
a-Cup evaluation averaged over 20 runs.
pected, the robot fails to reproduce the the presented
behavior and reinforcement learning is needed for self-improvement. Figure 5 shows the expected
return over the number of rollouts where convergence to a maximum is clearly recognizable. The
robot regularly succeeds at bringing the ball into the cup after approximately 75 iterations.
4 Conclusion
In this paper, we have presented a new perspective on policy learning methods and an application
to a highly complex motor learning task on a real Barrett WAMTM robot arm. We have generalized
the previous work in [17, 18] from the immediate reward case to the episodic case. In the process,
we could show that policy gradient methods are a special case of this more general framework.
During initial experiments, we realized that the form of exploration highly inﬂuences the speed of
the policy learning method. This empirical insight resulted in a novel policy learning algorithm,
Policy learning by Weighting Exploration with the Returns (PoWER), an EM-inspired algorithm
that outperforms several other policy search methods both on standard benchmarks as well as on a
simulated Underactuated Swing-Up.
We successfully applied this novel PoWER algorithm in the context of learning two tasks on a
physical robot, i.e., the Underacted Swing-Up and Ball-in-a-Cup. Due to the curse of dimensionality,
we cannot start with an arbitrary solution. Instead, we mimic the way children learn Ball-in-a-Cup
and ﬁrst present an example for imitation learning which is recorded using kinesthetic teach-in.
Subsequently, our reinforcement learning algorithm takes over and learns how to move the ball into

7

02040608010000.20.40.60.81number of rolloutsaverage returnthe cup reliably. After only realistically few episodes, the task can be regularly fulﬁlled and the
robot shows very good average performance.
References
[1] R. Sutton and A. Barto. Reinforcement Learning. MIT Press, 1998.
[2] J. Bagnell, S. Kadade, A. Ng, and J. Schneider. Policy search by dynamic programming. In
Advances in Neural Information Processing Systems (NIPS), 2003.
[3] A. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In
International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2000.
[4] F. Guenter, M. Hersch, S. Calinon, and A. Billard. Reinforcement learning for imitating con-
strained reaching movements. RSJ Advanced Robotics, 21, 1521-1544, 2007.
[5] M. Toussaint and C. Goerick. Probabilistic inference for structured planning in robotics. In
International Conference on Intelligent Robots and Systems (IROS), 2007.
[6] M. Hoffman, A. Doucet, N. de Freitas, and A. Jasra. Bayesian policy learning with trans-
dimensional MCMC. In Advances in Neural Information Processing Systems (NIPS), 2007.
[7] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8:229–256, 1992.
[8] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. In Advances in Neural Information Processing
Systems (NIPS), 2000.
[9] J. Bagnell and J. Schneider. Covariant policy search. In International Joint Conference on
Artiﬁcial Intelligence (IJCAI), 2003.
[10] J. Peters and S. Schaal. Policy gradient methods for robotics. In International Conference on
Intelligent Robots and Systems (IROS), 2006.
[11] G. Lawrence, N. Cowan, and S. Russell. Efﬁcient gradient estimation for motor control learn-
ing. In International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2003.
[12] H. Attias. Planning by probabilistic inference. In Ninth International Workshop on Artiﬁcial
Intelligence and Statistics (AISTATS), 2003.
[13] J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden
variables. Machine Learning, 29:213–244, 1997.
[14] G. Wulf. Attention and motor skill learning. Human Kinetics, Champaign, IL, 2007.
[15] D. E. Kirk. Optimal control theory. Prentice-Hall, Englewood Cliffs, New Jersey, 1970.
[16] G. J. McLachan and T. Krishnan. The EM Algorithm and Extensions. Wiley Series in Proba-
bility and Statistics. John Wiley & Sons, 1997.
[17] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neu-
ral Computation, 9(2):271–278, 1997.
[18] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In International Conference on Machine Learning (ICML), 2007.
[19] T. Rückstieß, M. Felder, and J. Schmidhuber. State-dependent exploration for policy gradient
methods. In European Conference on Machine Learning (ECML), 2008.
[20] M. Kawato, F. Gandolfo, H. Gomi, and Y. Wada. Teaching by showing in kendama based on
optimization principle. In International Conference on Artiﬁcial Neural Networks, 1994.
[21] C. G. Atkeson. Using local trajectory optimizers to speed up global optimization in dynamic
programming. In Advances in Neural Information Processing Systems (NIPS), 1994.
[22] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor
primitives. In Advances in Neural Information Processing Systems (NIPS), 2003.
[23] S. Schaal, P. Mohajerian, and A. Ijspeert. Dynamics systems vs. optimal control — a unifying
view. Progress in Brain Research, 165(1):425–445, 2007.
[24] Wikipedia, May 31, 2008. http://en.wikipedia.org/wiki/Ball_in_a_cup
[25] J. Kober, B. Mohler, and J. Peters. Learning perceptual coupling for motor primitives. In
International Conference on Intelligent RObots and Systems (IROS), 2008.

8


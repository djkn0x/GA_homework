Inferring rankings under constrained sensing

Srikanth Jagabathula

Devavrat Shah

Laboratory of Information and Decision Systems,
Massachusetts Institute of Technology,
Cambridge, MA 02139.
{jskanth, devavrat}@mit.edu

Abstract

Motivated by applications like elections, web-page ranking, revenue maximiza-
tion etc., we consider the question of inferring popular rankings using constrained
data. More speci ﬁcally, we consider the problem of inferrin g a probability distri-
bution over the group of permutations using its ﬁrst order ma rginals. We ﬁrst
prove that it is not possible to recover more than O(n) permutations over n ele-
ments with the given information. We then provide a simple and novel algorithm
that can recover up to O(n) permutations under a natural stochastic model; in this
sense, the algorithm is optimal. In certain applications, the interest is in recov-
ering only the most popular (or mode) ranking. As a second result, we provide
an algorithm based on the Fourier Transform over the symmetric group to recover
the mode under a natural majority condition; the algorithm turns out to be a max-
imum weight matching on an appropriately deﬁned weighted bi partite graph. The
questions considered are also thematically related to Fourier Transforms over the
symmetric group and the currently popular topic of compressed sensing.

1

Introduction

We consider the question of determining a real-valued function on the space of permutations of
n elements with very limited observations. Such a question naturally arises in many applications
including efﬁcient web-page rank aggregation, choosing th e winner in a sport season, setting odds
in gambling for revenue maximization, estimating popularity of candidates pre-election and the list
goes on (for example, see references [1], [2], [3]). In what follows, we give a motivating example
for the pursuit of this quest.
A motivating example. Consider a pre-election scenario in a democratic country with n potential
candidates. Each person (or voter) has certain ranking of these candidates in mind (consciously or
sub-consciously). For example, let n = 3 and the candidates be A, B and C . Each person believes
in one of the 3! = 6 possible ordering of these candidates. For example, let 50% of people believe
in A > B > C , 30% of people believe in B > A > C and 20% of people believe in C > A > B .
We wish to infer these preferences of population by means of a limited set of questions.

Speci ﬁcally, suppose we can interview a representative col
lection (i.e.
reasonably large random
collection) of people for this purpose. However, in the interview we may not be able to ask them
their complete ranking of all candidates. This may be because a person may not be able to articulate
it clearly. Or, in situations (e.g. gambling) where there is a ﬁnancial signi ﬁcance associated with
information of complete ranking, an individual may not be ready to provide that information. In
such a situation, we will have to settle with restricted questions of the following type: what will be
the rank of candidate A in your opinion? or, whom would you rank second?
Given answers to such restricted questions, we would like to infer what fraction of the population
prefers which ordering of candidates. Clearly, such restricted information cannot lead to any useful

1

inference of prevalent ordering of candidates in the population if there are too many of them (for
large n). Now, in a real world scenario, it is likely that people decide rankings of candidates based on
a few issues such as war, abortion, economy and gay marriage. That is, an individual will decide the
ranking of the candidates based on the opinions of candidates on these issues. Therefore, irrespective
of the number of candidates, the number of distinct rankings that prevail in the population are likely
to be very few.

In this paper, we are interested in inferring such few prevalent rankings of candidates and their
popularity based on the restricted (or partial) information as explained above. Thematically, this
question is similar to the pursuit of compressed sensing. However, as we explain in Section 2,
standard compressed sensing does not apply under this setting. We also discuss a natural relation
between the available information and the Fourier coefﬁcie nts of the Fourier transformation based
on group representation (see Proposition 1). It turns out that the problem we consider is equivalent
to that of recovery of a function over a symmetric group using the ﬁrst order Fourier coefﬁcients.
Thus, our problem is thematically related to the recovery of functions over non-commutative groups
ion 2, a naive recovery by setting the
using a limited set of Fourier coefﬁcients. As we show in Sect
unknown Fourier coefﬁcients to zero yields a very bad result
. Hence, our approach has potential
applications to yielding a better recovery.

In many applications, one is speci ﬁcally interested in ﬁndi
ng out the most popular ranking (or mode)
rather than all the prevalent rankings. For this, we consider an approximation based on Fourier trans-
formation as a surrogate to ﬁnd the mode. We establish that un der the natural majority condition,
our algorithm ﬁnds the correct mode (see Theorem 2). Interes tingly enough, our algorithm to ﬁnd
an estimate of the mode corresponds to ﬁnding a maximum weigh t matching in a weighted bipartite
graph of n nodes.
Organization. We start describing the setup, the problem statement, and the relation to compressed
sensing and Fourier transform based approaches in Section 2.
In Section 3, we provide precise
statements of the main results. In the remaining Sections, we prove these results and discuss the
relevant algorithms.

2 Background and preliminaries

Setup. Let Sn = {σ1 , . . . , σN } denote set of all possible N = n! permutations (orderings) of
n elements. Sn is also known as the symmetric group of degree n. Let f : Sn → [0, 1] denote a
mapping from the symmetric group to the interval [0, 1]. We assume that the function f is normalized
i.e., kf kℓ1 = 1, where k·kℓ1 denotes the ℓ1 norm. Let pk denote the value f (σk ), for 1 ≤ k ≤ N .
Without loss of generality we assume that the permutations are labeled such that pk ≤ pm for
k < m. We write f (·) to denote the function and f to denote the vector (f (σk ))N ×1 . The set of
permutations for which f (·) is non-zero will be called the support of f (·); also, the cardinality of the
support will be called sparsity of f and is denoted by K i.e., K = kf kℓ0 . Each permutation σ will
be represented by its corresponding permutation matrix denoted by P σ i.e., P σ
ij = 1{σ(j )=i} , where
1E is the indicator variable of the event E . For brevity, we write P σ to mean both the n × n matrix
and the n2 × 1 vector. We use the terms permutation and permutation matrix interchangeably. We
think of permutations as complete matchings in a bipartite graph. Speci ﬁcally, we consider an n × n
bipartite graph and each permutation corresponds to a complete matching in the graph. The edges
in a permutation will refer to the edges in the corresponding bipartite matching. For 1 ≤ i, j ≤ n,
let

(1)

qij := X
σ∈Sn :σ(j )=i

f (σ)

Let Q denote both the matrix (qij )n×n and the vector (qij )n2×1 . It is easy to note that Q can be
equivalently written as Pσ∈Sn f (σ)P σ . From the deﬁnition, it also follows that Q is a doubly
stochastic matrix. The matrix Q corresponds to the ﬁrst order
information about the function f (·).
In the election example, it is easy to see that qij denotes the fraction of voters that have ranked
candidate j in the ith position.
Problem statement and result. The basic objective is to determine the values of the function f (·)
precisely, using only the values of the matrix Q. We will ﬁrst prove, using information theoretic
techniques, that recovery is asymptotically reliable (average probability of error goes to zero as

2

n → ∞) only if K = O(n). We then provide a novel algorithm that recovers prevalent rankings and
their popularity exactly under minimal (essentially necessary) conditions; under a natural stochastic
model, this algorithm recovers up to O(n) permutations. In this sense, our algorithm is optimal.
It is often the case that the full knowledge of functional values at all permutations is not required.
Speci ﬁcally, in scenarios such as ranked elections, intere st is in ﬁnding the most likely permuta-
tion i.e., arg max f (σ). Theorem 2 proves that the max-weight matching yields the most likely
permutation under natural majority assumption.

2.1 Relation to Fourier Transform

The question we consider is thematically related to harmonic analysis of functions over non-
commutative groups. As we shall show soon, the matrix Q is related to the ﬁrst two Fourier coef-
ﬁcients of the Fourier Transform of the distribution over th e permutation group. Thus, the problem
we are considering can be restated as that of reconstructing a distribution over the permutation group
from its ﬁrst two Fourier coefﬁcients. Reconstructing dist
ributions over the permutation group from
a limited number of Fourier coefﬁcients has several applica tions. Speci ﬁcally, there has been some
recent work on multi-object tracking (see [4] and [3]), in which they approach the daunting task of
maintaining a distribution over the permutation group by approximating it using the ﬁrst few Fourier
coefﬁcients. This requires reconstructing the function fr om a limited number of Fourier coefﬁcients,
where our solution can be potentially applied.

We will now discuss the Fourier Transform of a function on the permutation group, which provides
another possible approach for recovery of f . Interestingly enough, the ﬁrst order Fourier transform
of f can be constructed using information based on Q = (qij ). As we shall ﬁnd, this approach fails
to recover sparse f as it has tendency to “spread” the mass on all n! elements given Q. However,
as established in Theorem 2 this leads to recovery of mode or most likely assignment of f under
natural majority condition.

h(σ) =

(2)

Next, some details on what the Fourier transform (an interested reader is requested to check [5] for
missing details) based approach is, how Q can be used to obtain an approximation of f and why it
does not recover f exactly. The details relevant to recovery of mode of f will be associated with
Theorem 2.
Fourier Transform: Deﬁnition. We can obtain a solution to the set of linear equations in (8)
using the Fourier Transforms at symmetric group representations. For a function h : G → R on
group G, its Fourier Transform at a representation ρ of G is deﬁned as ˆhρ = Pσ h(σ)ρ(σ). The
collection of Fourier Transforms of h(·) at a complete set of inequivalent irreducible representations
of G completely determine the function. This follows from the following expression for the inverse
Fourier Transform:

1
ρk ρk (σ)i
dρk Tr hˆhT
|G| X
k
where |G| denotes the cardinality of G, dρk denotes the degree of representation ρk and k indexes
over the complete set of inequivalent irreducible representations of G. The trivial representation of
a group is the 1-dimensional representation ρ0 (σ) = 1, ∀ σ ∈ G. Therefore, the Fourier Transform
of h(·) at ρ0 is Pσ h(σ).
Fourier Transform: Approximation. The above naturally suggests an approximation based on a
limited number of Fourier coefﬁcients with respect to a certain subset of irreducible representations.
We will show that, indeed, the information matrix Q corresponds to the Fourier coefﬁcient with
respect to the ﬁrst-order
representation of the symmetric group Sn . Therefore, it yields a natural
approximation.
It is known that [5] the ﬁrst order permutation representation of Sn , denoted by τ1 , has a degree
n and maps every permutation σ to its corresponding permutation matrix P σ . In other words, we
have τ1 (σ) = P σ . Thus, ˆf (σ) = Pσ∈Sn f (σ)τ1 (σ) = Q. Reconstruction of f requires Fourier
Transforms at irreducible representations. Even though τ1 is not an irreducible representation, it is
known that [5] that every representation of a group is equivalent to the direct sum of irreducible rep-
resentations. In particular, τ1 can be decomposed into τ1 = ρ0 ⊕ ρ1 ; where ρ0 is the aforementioned
trivial representation of degree 1 and ρ1 is an irreducible representation of degree n − 1. It is worth
pointing out to a familiar reader that what we call ρ1 is more appropriately denoted by ρ(n−1,1) in

3

the literature; but we will stick to ρ1 for brevity. Thus, Q is related to the Fourier Transforms of the
irreducible representations ρ0 and ρ1 . We now have the following proposition:
Proposition 1. Consider a function f : Sn → R. Suppose that kf kℓ1 = 1 and we are given
its corresponding Q. Then, its natural Fourier approximation obtained by looking at the Fourier
coefﬁcients of the relevant irreducible representations i s given by the function ˜f : Sn → R deﬁned
as:

hQ, P σ i
˜f (σ) = (n − 1)
N
for σ ∈ Sn , with N = n!, kf kℓ1 = k ˜f kℓ1 and Pσ∈Sn
Proof. We have:

−

n − 2
N
˜f (σ)P σ = Q.

Q = X
σ∈Sn

f (σ)τ1 = X
σ∈Sn

f (σ)(τ0 ⊕ τ1 ) = ˆfρ0 ⊕ ˆfρ1 .

Therefore,

(3)

(4)

(5)

ρ1 (cid:17) (ρ0 (σ) ⊕ ρ1 (σ))i
hQ, P σ i = Tr (cid:2)QT P σ (cid:3) = Tr h(cid:16) ˆf T
ρ0 ⊕ ˆf T
Since Tr is independent of the basis, choosing an appropriate basis we can write:
hQ, P σ i = Tr h ˆf T
ρ0 ρ0 (σ)i + Tr h ˆf T
ρ1 ρ1 (σ)i = 1 + Tr h ˆf T
ρ1 ρ1 (σ)i
(6) is true because ρ0 (σ) = 1, ∀σ ∈ Sn , and kf kℓ1 = 1.
˜f is obtained by truncating the Inverse Fourier Transform expression to the ﬁrst two terms. Thus,
from (2), it follows that:

(6)

˜f (σ) =

1
ρ1 ρ1 (σ)i
N h ˆf T
ρo ρ0 (σ) + (n − 1) ˆf T

(7)

Using the fact that ρ0 (σ) = 1 ∀σ ∈ Sn , ˆfρ0 = 1, and plugging (6) into (7) gives the result of the
proposition.

Summary. Thus, the Fourier Transform technique yields a solution to the problem. Unfortunately,
the solution is not sparse and the “mass ” is distributed over
all the permutations yielding values
of O(1/N ) for all permutations. In summary, a naive approach to the reconstruction of a sparse
distribution gives unsatisfactory results and requires a different approach.

2.2 Relation to Compressed Sensing

Here we discuss the relation of the above stated question to the recently popular topic of compressed
sensing. Indeed, both share the commonality in the sense that the ultimate goal is to recover a sparse
function (or vector) based on few samples. However, as we shall show, the setup of our work here
is quite different. This is primarily because in the standard compressed sensing setup, samples are
chosen as “random projections ” while here samples are highl y constrained and provide information
matrix Q. Next, we provide details of this.
Our problem can be formulated as a solution to a set of linear equations by deﬁning a matrix A as
the n2 × N matrix with column vectors as P σk , 1 ≤ k ≤ N . Then, f is a solution to the following
set of linear equations:

Ax = Q

(8)

Candes and Tao (2005) [6] provide an approach to solve this problem. They require the vector f to
be sparse i.e., kf kℓ0 = ρN , for some ρ > 0. As discussed earlier, this is a reasonable assumption
in our case because: (a) the total number of permutations N can be very large even for a reasonably
sized n and (b) most functions f (·) that arise in practice are determined by a small (when compared
to N ) number of parameters. Under a restriction on the isometry constants of the matrix A, Candes
and Tao prove that the solution f is the unique minimizer to the LP:

minkxkℓ1

s.t. Ax = Q

(9)

4

Unfortunately, the approach of Candes and Tao cannot be directly applied to our problem because
the isometry constants of the matrix A do not satisfy the required conditions.
We now take a closer look at the isometry constants of A. Gaussian random matrices form an
important class of matrices with good isometry constants. Unfortunately, neither is our matrix A
random nor is there a straightforward random formulation of our problem. To see why the matrix
A has bad isometry constants, we take a simple example. For any n ≥ 4 consider the following 4
permutations: σ1 = id, σ2 = (12), σ3 = (34) and σ4 = (12)(34). Here, id refers to the identity
permutation and the permutations are represented using the cycle notation. It is easy to see that:

P σ1 + P σ4 = P σ2 + P σ3

(10)

For any integer 1 ≤ S ≤ N , the S restricted isometry constant of A is deﬁned as the smallest
quantity such that AT c obeys:
(1 − δS )kck2
ℓ2 ≤ kAT ck2
ℓ2 ≤ (1 + δS )kck2
ℓ2
∀ T ⊆ {1, 2, . . . , N } of cardinality at most S and all real vectors c. Here, AT c denotes
Pk∈T ck P σk . From this deﬁnition and (10), it follows that
δS = 1 ∀ S ≥ 4. Theorem 1.4 re-
quires δS < 1 for perfect reconstruction of f when kf kℓ0 ≤ S . Therefore, the compressed sensing
approach of Candes and Tao does not guarantee the unique reconstruction of f if kf kℓ0 ≥ 4.

(11)

3 Main results

Exact recovery. The main result of this paper is about the exact recovery of f from the given
constrained information matrix Q = (qij ) under the hypothesis that f is sparse or has small kf kℓ0 .
We provide an algorithm that recovers f exactly if the underlying support and probabilities have the
following two properties:
Property 1 (P1). Suppose the function f (·) is K sparse. Let p1 , p2 , . . . , pK be the function values.
The following is true:
X
j∈J

pj ∀ J, J ′ ⊆ {1, 2, . . . , K } s.t J ∩ J ′ = ∅

pj 6= X
j∈J ′

Property 2 (P2). Let {σ1 , σ2 , . . . , σK } be the support of f (·). For each 1 ≤ i ≤ K , ∃ an 1 ≤ ηi ≤
n such that σi (ηi ) 6= σj (ηi ) ∀ j 6= i. In other words, each permutation has at least one edge that is
different from all the others.

When properties P1 and P2 are satis ﬁed, the equation Q = Af has a unique solution and can indeed
be recovered; we will provide an algorithm for such recovery. The following is the formal statement
of this result and will be proved later.
Theorem 1. Consider a function f : Sn → [0, 1] such that kf kℓ0 = L, kf kℓ1 = 1, and the func-
tional values and the support possess properties P1 and P2. Then, matrix Q is sufﬁcient to recon-
struct f (·) precisely.

Random model, Sparsity and Theorem 1.

Theorem 1 asserts that when properties P1 and P2 are satis ﬁed , exact recovery is possible. However,
it is not clear why they are reasonable. We will now provide some motivation and prove that the
algorithm is indeed optimal in terms of the maximum sparsity it can recover.

Let’s go back to the counter-example we mentioned before: For any n ≥ 4 consider the 4 permuta-
tions σ1 = id, σ2 = (12), σ3 = (34) and σ4 = (12)(34). We have P σ1 + P σ4 = P σ2 + P σ3 . Now,
consider 4 values p1 , p2 , p3 and p4 . Without loss of generality suppose that p1 ≤ p4 and p2 ≤ p3 .
Using the equation P σ1 + P σ4 = P σ2 + P σ3 , we can write the following:

Q = p1P σ1 + p2P σ2 + p3P σ3 + p4P σ4
= (p1 + p2 )P σ1 + (p1 + p2 )P σ4 + (p3 − p2 )P σ3
= (p1 + p2 )P σ2 + (p1 + p3 )P σ3 + (p4 − p1 )P σ4 .

5

Thus, under the above setup, there is no unique solution to Q = Af . In addition, from the last two
equalities, we can conclude that even the sparsest solution is not unique. Hence, there is no hope of
recovering f given only Q in this setup.
The question we now ask is whether the above counter example is contrived and specially con-
structed, or is it more prevalent. For that, we consider a random model which puts a uniform mea-
sure on all the permutations. The hope is that under this model, situations like the counter example
occur with a vanishing probability. We will now describe the random model and then state important
results on the sparsity of f that can be recovered from Q.
Random Model. Under the random model, we assume that the function f with sparsity K is con-
structed as follows: Choose K permutations uniformly at random and let them have any non-trivial
real functional values chosen uniformly at random from a bounded interval and then normalized.
We call an algorithm producing an estimate ˆf of f as asymptotically reliable if Pr hf 6= ˆf i = ε(n)
where ε(n) → 0 as n → ∞. We now have the following two important results:
Lemma 1. Consider a function f : Sn → R with sparsity K . Given the matrix Q = Af , and no
additional information, the recovery will be asymptotically reliable only if K ≤ 4n.

First note that a trivial bound of (n − 1)2 can be readily obtained as follows: Since Q is doubly
stochastic, it can be written as a convex combination of permutation matrices [7], which form a
space of dimension (n − 1)2 . Lemma 1 says that this bound is loose. It can be proved using standard
arguments in Information Theory by considering A as a channel with input f and output Q.
Lemma 2. Consider a function f : Sn → R with sparsity K constructed according to the random
model described above. Then, the support and functional values of f possess properties P1 and P2
with probability 1 − o(1) as long as K ≤ 0.6n.

It follows from Lemma 2 and Theorem 1 that f can be recovered exactly from Q if the sparsity
K = O(n). Coupled with Lemma 1 we conclude that our algorithm is optimal in the sense that it
achieves the sparsity bound of O(n).
Recovery of Mode. As mentioned before, often we are interested in obtaining only limited informa-
tion about f (·). One such scenario is when we would like to ﬁnd just the most li kely permutation.
For this purpose, we use the Fourier approximation ˜f (cf. Proposition 1) in place of f : that is, the
mode of f is estimated as mode of ˜f . The following result states the correctness of this approxima-
tion under majority.
Theorem 2. Consider a function f : Sn → [0, 1] such that kf kℓ0 = L and kf kℓ1 = 1. Suppose the
majority condition holds, that is maxσ∈Sn f (σ) > 1/2. Then,
˜f (σ) = arg max
σ∈Sn

f (σ) = arg max
σ∈Sn

arg max
σ∈Sn

hP σ , Qi .

The mode of ˜f , or maximizer of hP σ , Qi is essentially the maximum weight matching in a weighted
bipartite graph: consider a complete bipartite graph G = ((V1 , V2 ), E ) with V1 = V2 = {1, . . . , n}
and E = V1 × V2 with edge (i, j ) ∈ E having weight qij . Then, weight of a matching (equivalently
permutation σ ) is indeed hP σ , Qi. The problem of ﬁnding maximum weight matching is classical
.
It can be solved in O(n3 ) using algorithm due to Edmond and Karp [8] or max-product belief
propagation by Bayati, Shah and Sharma [9]. Thus, this is an approximation that can be evaluated.

4 Theorem 1: Proof and Algorithm

Here, we present a constructive proof of Theorem 1. Speci ﬁca lly, we will describe an algorithm to
determine the function values from Q which will be the original f as long as properties P1 and P2
are satis ﬁed.
Let p1 , p2 , . . . , pL denote the non-zero functional values. Let σ1 , σ2 , . . . , σL denote the correspond-
ing permutations i.e., f (σk ) = pk . Without loss of generality assume that the permutations are
labeled such that pi ≤ pj for i < j . Let q1 , q2 , . . . , qM , where M = n2 , denote the values of matrix
Q arranged in ascending order.

6

Given this sorted version, we have qi ≤ qj for i < j . Let ei denote the edge (u, v) such that
qi = qei = quv , where recall that
quv = X
k:σk (u)=v

f (σk ) = X
k:σk (u)=v

pk .

Let Ak denote the set of edges corresponding to permutation σk , 1 ≤ k ≤ L. That is, Ak =
{(u, σk (u)) : 1 ≤ u ≤ n}. The algorithm stated below will itself determine L, and (Ak , pk )1 ≤
k ≤ L using information Q. The algorithm works when properties P1 and P2 are satis ﬁed.

Algorithm:
initialization: p0 = 0, k(0) = 0 and Ak = ∅, 1 ≤ k ≤ M .
i = 1 to M
for
qi = Pj∈J pj for some J ⊆ {0, 1, . . . , k(i − 1)}
if
k(i) = k(i − 1)
Aj = Aj ∪ {ei } ∀ j ∈ J
else
k(i) = k(i − 1) + 1
pk(i) = qi
Ak(i) = Ak(i) ∪ {ei }
end if
end for
Output L = k(i) and (pk , Ak ), 1 ≤ k ≤ L.
By property P2, there exists at least one qi such that it is equal to pk , for each 1 ≤ k ≤ L. The
property P1 ensures that whenever qi = pk(i) , the condition in the “if ” statement of the pseudocode
is not satis ﬁed. Therefore, the algorithm correctly assign s values to each of the pk ’s.
Note that the condition in the “if ” statement being true impl
ies that edge ei is present in all the
permutations σj such that j ∈ J . Property P1 ensures that such a J , if exists, is unique. Therefore,
when the condition is satis ﬁed, the only permutations that c ontain edge ei are σj , j ∈ J .
When the condition in the “if ” statement fails, again from pro perties P1 and P2 it follows that edge
ei is contained only in permutation σk(i) . From this discussion we can conclude that at the end of the
iterations, each of the Ai ’s contain complete information about their corresponding permutations.
The algorithm thus completely determines the function f (·). Finally, note that the algorithm does
not require the knowledge of kf kℓ0 .

5 Theorem 2: Proof and Algorithm

Here, our interest is in ﬁnding the mode of f . The algorithm we have proposed is use the mode of
˜f , as an estimate of mode of f . We wish to establish that when maxσ∈Sn f (σ) > 1/2 then
˜f (σ);

˜σ∗ = σ∗ , where

˜σ∗ = arg max
σ∈Sn

σ∗ = arg max
σ∈Sn

f (σ).

Since we have assumed that f (σ∗ ) > 1/2 and kf kℓ1 = 1, we should have Pσ∈S f (σ) < 1/2,
where S ⊂ Sn such that σ∗ /∈ S . Therefore, there is exactly one entry in each column of matrix Q
that is > 1/2, and the corresponding edge should be a part of σ∗ . Thus, keeping only those edges
(i, j ) such that Qi,j > 1/2, we should the matching σ∗ . It is clear from the construction that σ∗
indeed has the maximum weight of all the other matchings. The result now follows.

6 Conclusion

In summary, we considered the problem of inferring popular rankings from highly constrained in-
formation. Since raking data naturally arises in several diverse practical situations, an answer to this
question has wide ranging implications.

7

Speci ﬁcally, we considered the problem of inferring a spars e normalized function on the symmetric
group using only the ﬁrst order
information about the function. In the election example this ﬁrst
order information corresponds to the fraction of people who have ranked candidate i in the j th
position. We provide a novel algorithm to precisely recover the permutations and the associated
popularity under minimal, and essentially necessary, conditions. We provide justi ﬁcation to the
necessity of our assumptions and consider a natural random model to quantify the sparsity that can
be supported.

We also provide an algorithm, based on Fourier transform approximation, to determine the most
popular ranking (mode of the function). The algorithm is essentially a max-weight matching with
weights as the q.. values. Under a natural majority assumption, the algorithm ﬁnds the correct mode.
The question considered is thematically related to harmonic analysis of functions over the symmet-
ric group and also the currently popular topic of compressed sensing. The problem we consider can
be restated as the reconstruction of a function using its ﬁrst order Fourier representation, which has
several applications particularly in the multi-object tracking problem. On the other hand, the paral-
lels to the to the standard compressed sensing setup are limited because the available information is
highly constrained. Thus, the existing approaches of compressed sensing cannot be applied to the
problem.
Next Steps. We concentrated on the recovery of the distribution from its ﬁrst order marginals. A
possible next step would be to consider recovery under different forms of partial information. More
speci ﬁcally, practical applications motivate considerin g the recovery of distribution from pair-wise
information: probability of candidate i being ranked above candidate j . Another natural practical
consideration would be to address the presence of noise in the available information. Understanding
recovery of distributions with the above considerations are natural next steps.

References

[1] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation revisited. In Proceedings
of WWW10, 2001.
[2] Yiling Chen, Lance Fortnow, Evdokia Nikolova, and David M. Pennock. Betting on permu-
tations.
In EC ’07: Proceedings of the 8th ACM conference on Electronic commerce, pages
326–335, New York, NY, USA, 2007. ACM.
[3] J. Huang, C. Guestrin, and L. Guibas. Efﬁcient Inference for Distributions on Permutations. In
Advances in Neural Information Processing Systems (NIPS), 2007.
[4] R. Kondor, A. Howard, and T. Jebara. Multi-object tracking with representations of the sym-
metric group. In Proceedings of the Eleventh International Conference on Arti ﬁcial Intelligence
and Statistics, 2007.
[5] P. Diaconis. Group Representations in Probability and Statistics. IMS Lecture Notes-Monograph
Series, 11, 1988.
[6] E.J. Candes and T. Tao. Decoding by linear programming. Information Theory, IEEE Transac-
tions on, 51(12):4203–4215, Dec. 2005.
[7] G. Birkhoff. Tres observaciones sobre el algebra lineal. Univ. Nac. Tucuman Rev. Ser. A, 5:147–
151, 1946.
[8] J. Edmonds and R. Karp. Theoretical improvements in algorithmic efﬁciency for network ﬂow
problems. Jour. of the ACM, 19:248–264, 1972.
[9] M. Bayati, D. Shah, and M. Sharma. Max-product for maximum weight matching: convergence,
correctness and lp duality. IEEE Transactions on Information Theory, March 2008.

8


Sparsity of SVMs that use the -insensitive loss

Ingo Steinwart
Information Sciences Group CCS-3
Los Alamos National Laboratory
Los Alamos, NM 87545, USA
ingo@lanl.gov

Andreas Christmann
University of Bayreuth
Department of Mathematics
D-95440 Bayreuth
Andreas.Christmann@uni-bayreuth.de

Abstract

In this paper lower and upper bounds for the number of support vectors are derived
for support vector machines (SVMs) based on the -insensitive loss function. It
turns out that these bounds are asymptotically tight under mild assumptions on the
data generating distribution. Finally, we brieﬂy discuss a trade-off in  between
sparsity and accuracy if the SVM is used to estimate the conditional median.

1
Introduction
Given a reproducing kernel Hilbert space (RKHS) of a kernel k : X × X → R and training set
D := ((x1 , y1 ), . . . , (xn , yn )) ∈ (X × R)n , the -insensitive SVM proposed by Vapnik and his
co-workers [10, 11] for regression tasks ﬁnds the unique minimizer fD,λ ∈ H of the regularized
n(cid:88)
empirical risk
1
λ(cid:107)f (cid:107)2
H +
L (yi , f (xi )) ,
(1)
n
i=1
where L denotes the -insensitive loss deﬁned by L (y , t) := max{0, |y − t| − } for all y , t ∈ R
and some ﬁxed  ≥ 0. It is well known, see e.g. [2, Proposition 6.21], that the solution is of the form
n(cid:88)
i k(xi , · ) ,
β ∗
fD,λ =
i=1
n(cid:88)
n(cid:88)
n(cid:88)
where the coefﬁcients β ∗
i are a solution of the optimization problem
|βi | − 1
yiβi − 
maximize
2
Here we set C := 1/(2λn). Note that the equality constraint (cid:80)n
i,j=1
i=1
i=1
−C ≤ βi ≤ C
for all i = 1, . . . , n.
(4)
subject to
i=1 βi = 0 needed in [2, Proposition
6.21] is superﬂuous since we do not include an offset term b in the primal problem (1).
In the
following, we write SV (fD,λ ) := {i : β ∗
(cid:54)= 0} for the set of indices that belong to the support
i
vectors of fD,λ . Furthermore, we write # for the counting measure, and hence #SV (fD,λ ) denotes
the number of support vectors of fD,λ .
It is obvious from (2) that #SV (fD,λ ) has a crucial inﬂuence on the time needed to compute
fD,λ (x). Due to this fact, the -insensitive loss was originally motivated by the goal to achieve
sparse decision functions, i.e., decision functions fD,λ with #SV (fD,λ ) < n. Although empiri-
cally it is well-known that the -insensitive SVM achieves this sparsity, there is, so far, no theo-
retical explanation in the sense of [5]. The goal of this work is to provide such an explanation by
establishing asymptotically tight lower and upper bounds for the number of support vectors. Based
on these bounds we then investigate the trade-off between sparsity and estimation accuracy of the
-insensitive SVM.

βiβj k(xi , xj )

(2)

(3)

2 Main results

D ∈ (X × R)n :

#SV (fD,λ )
n

and

#SV (fD,λ )
n

Before we can formulate our main results we need to introduce some more notations. To this end,
let P be a probability measure on X × R, where X is some measurable space. Given a measurable
f : X → R, we then deﬁne the L -risk of f by RL ,P (f ) := E(x,y)∼PL (y , f (x)). Moreover, recall
that P can be split into the marginal distribution PX on X and the regular conditional probability
P( · |x). Given a RKHS H of a bounded kernel k , [1] then showed that
λ(cid:107)f (cid:107)2
H + RL ,P (f )
fP,λ := arg inf
f ∈H
(cid:80)n
exists and is uniquely determined whenever RL ,P (0) < ∞. Let us write δ(x,y) for the Dirac
measure at some (x, y) ∈ X × R. By considering the empirical measure D := 1
i=1 δ(xi ,yi ) of a
training set D := ((x1 , y1 ), . . . , (xn , yn )) ∈ (X × R)n , we then see that the corresponding fD,λ is
n
:= (cid:8)(x, y) ∈ X × R : |f (x) − y | >  + δ(cid:9)
the solution of (1). Finally, we need to introduce the sets
:= (cid:8)(x, y) ∈ X × R : |f (x) − y | ≥  − δ(cid:9) ,
low (f )
Aδ
up (f )
Aδ
where f : X → R is an arbitrary function and δ ∈ R. Moreover, we use the short forms Alow (f ) :=
low (f ) and Aup (f ) := A0
up (f ). Now we can formulate our ﬁrst main result.
A0
Theorem 2.1 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Then, for all n ≥ 1, ρ > 0, δ > 0, and λ > 0 satisfying
(cid:17) ≥ 1 − 3e− δ2 λ2 n
Pn(cid:16)
> P(cid:0)Aδ
low (fP,λ )(cid:1) − ρ
δλ ≤ 4, we have
D ∈ (X × R)n :
16 − e−2ρ2 n
(cid:17) ≥ 1 − 3e− δ2 λ2 n
Pn(cid:16)
< P(cid:0)Aδ
up (fP,λ )(cid:1) + ρ
16 − e−2ρ2 n .
Before we present our second main result, we brieﬂy illustrate Theorem 2.1 for the case where we
ﬁx the regularization parameter λ and let n → ∞.
Corollary 2.2 Let P be a probability measure on X × R and H be a separable RKHS with bounded
(cid:17)
n→∞ Pn(cid:16)
≤ P(cid:0)Aup (fP,λ )(cid:1) + ρ
D ∈ (X × R)n : P(cid:0)Alow (fP,λ )(cid:1) − ρ ≤ #SV (fD,λ )
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Then, for all ρ > 0 and λ > 0, we have
= 1 .
lim
n
Aup (fP,λ ) \Alow (fP,λ ) = (cid:8)(x, fP,λ (x) − ) : x ∈ X (cid:9) ∪ (cid:8)(x, fP,λ (x) + ) : x ∈ X (cid:9) .
Note that the above corollary exactly describes the asymptotic behavior of the fraction of support
vectors modulo the probability of the set
In particular, if the conditional distributions P( · |x), x ∈ X , have no discrete components, then the
above corollary gives an exact description.
Of course, in almost no situation it is realistic to assume that λ stays ﬁxed if the sample size n grows.
Instead, it is well-known, see [1], that the regularization parameter should vanish in order to achieve
consistency. To investigate this case, we need to introduce some additional notations from [6] that
are related to the L -risk. Let us begin by denoting the Bayes L -risk by R∗
L ,P := inf RL ,P (f ),
where P is a distribution and the inﬁmum is taken over all measurable functions f : X → R. In
(cid:90)
addition, given a distribution Q on R, [6] and [7, Chapter 3] deﬁned the inner L -risks by
CL ,Q (t) :=
t ∈ R,
L (y , t) dQ(y) ,
(cid:90)
R
L ,Q := inf t∈R CL ,Q (t). Obviously, we have
and the minimal inner L -risks were denoted by C ∗
(cid:0)f (x)(cid:1) dPX (x) ,
RL ,P (f ) =
CL ,P( · |x)
X

(5)

(cid:82)
and [6, Lemma 2.5], see also [7, Lemma 3.4], further established the intuitive formula R∗
L ,P =
M∗ (x) := (cid:8)t ∈ R : CL ,P( · |x) (t) = C ∗
(cid:9).
X C ∗
L ,P( · |x) dPX (x). Moreover, we need the sets of conditional minimizers
L ,P( · |x)
The following lemma collects some useful properties of these sets.
Lemma 2.3 Let P be a probability measure on X × R with R∗
L ,P < ∞. Then M∗ (x) is a non-
empty and compact interval for PX -almost all x ∈ X .
Given a function f : X → R, Lemma 2.3 shows that for PX -almost all x ∈ X there exists a unique
(cid:12)(cid:12)t∗ (x) − f (x)(cid:12)(cid:12) ≤ (cid:12)(cid:12)t − f (x)(cid:12)(cid:12)
t∗ (x) ∈ M∗ (x) such that
for all t ∈ M∗ (x) .
(6)
In other words, t∗ (x) is the element in M∗ (x) that has the smallest distance to f (x). In the follow-
ing, we sometimes write t∗
λ (x) := t∗ (x) if f = fP,λ and we wish to emphasize the dependence of
:= (cid:8)(x, y) ∈ X × R : |t∗ (x) − y | >  + δ(cid:9)
t∗ (x) on λ. With the help of these elements, we ﬁnally introduce the sets
:= (cid:8)(x, y) ∈ X × R : |t∗ (x) − y | ≥  − δ(cid:9) ,
low (f )
M δ
up (f )
M δ
where δ ∈ R. Moreover, we again use the short forms Mlow (f ) := M 0
low (f ) and Mup (f ) :=
up (f ). Now we can formulate our second main result.
M 0
Theorem 2.4 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL ,P (0) < ∞ and that H is dense in
L1 (PX ). Then, for all ρ > 0, there exist a δρ > 0 and a λρ > 0 such that for all λ ∈ (0, λρ ] and all
Pn(cid:16)
(cid:17) ≥ 1 − 8e−δ2
D ∈ (X × R)n : P(cid:0)Mlow (fP,λ )(cid:1) − ρ ≤ #SV (fD,λ )
≤ P(cid:0)Mup (fP,λ )(cid:1) + ρ
n ≥ 1 we have
ρ λ2 n .
n
nn → ∞, then the
If we choose a sequence of regularization parameters λn such that λn → 0 and λ2
resulting SVM is L -risk consistent under the assumptions of Theorem 2.4, see [1]. For this case,
the following obvious corollary of Theorem 2.4 establishes lower and upper bounds on the number
of support vectors.
Corollary 2.5 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL ,P (0) < ∞ and that H is dense in
nn → ∞. Then, for all
L1 (PX ). Furthermore, let (λn ) ∈ (0, ∞) be a sequence with λn → 0 and λ2
ρ > 0, the probability Pn of D ∈ (X × R)n satisfying
m→∞ P(cid:0)Mlow (fP,λm )(cid:1) − ρ ≤ #SV (fD,λn )
P(cid:0)Mup (fP,λm )(cid:1) + ρ
lim inf
n
converges to 1 for n → ∞.

≤ lim sup
m→∞

In general, the probabilities of the sets Mlow (fP,λ ) and Mup (fP,λ ) are hard to control since, e.g.,
for ﬁxed x ∈ X and λ → 0 it seems difﬁcult to show that fP,λ (x) is not “ﬂipping” from the left
hand side of M∗ (x) to the right hand side. Indeed, for general M∗ (x), such ﬂipping would give
λ (x) ∈ M∗ (x) for λ → 0, and hence would result in signiﬁcantly different sets
different values t∗
Mlow (fP,λ ) and Mup (fP,λ ). As a consequence, it seems hard to show that, for probability measures
P(cid:0)Mlow (fP,λ )(cid:1) = lim sup
P(cid:0)Mup (fP,λ )(cid:1) .
P whose conditional distributions P( · |x), x ∈ X , have no discrete components, we always have
lim inf
(7)
λ→0
λ→0
However, there are situations in which this equality can easily be established. For example, assume
that the sets M∗ (x) are PX -almost surely singletons. In this case, t∗
λ (x) is in fact independent of λ,
and hence so are Mlow (fP,λ ) and Mup (fP,λ ). Namely, in this case these sets contain the pairs (x, y)

for which y is not contained in the closed or open -tube around M∗ (x), respectively. Consequently,
(7) holds provided that the conditional distributions P( · |x), x ∈ X , have no discrete components,
and hence Corollary 2.5 gives a tight bound on the number of support vectors. Moreover, if in
this case we additionally assume  = 0, i.e., we consider the absolute loss, then we easily ﬁnd
P(Mlow (fP,λ )) = P(Mup (fP,λ )) = 1, and hence Corollary 2.5 shows that the corresponding SVM
does not tend to produce sparse decision functions. Finally, recall that for this speciﬁc loss function,
M∗ (x) equals the median of P( · |x), and hence M∗ (x) is a singleton whenever the median of
P( · |x) is unique.
Let us now illustrate Corollary 2.5 for  > 0. To this end, we assume in the following that the
conditional distributions P( · |x) are symmetric, i.e., for PX -almost all x ∈ X there exists a con-
ditional center c(x) ∈ R such that P(c(x) + A|x) = P(c(x) − A|x) for all measurable A ⊂ R.
Note that by considering A := [0, ∞) it is easy to see that c(x) is a median of P( · |x). Further-
more, the assumption RL ,P (0) < ∞ imposed in the results above ensures that the conditional
P (x) := E(Y |x) of P( · |x) exists PX -almost surely, and from this it is easy to conclude that
mean f ∗
P (x) for PX -almost all x ∈ X . Moreover, from [8, Proposition 3.2 and Lemma 3.3] we
c(x) = f ∗
immediately obtain the following lemma.
Lemma 2.6 Let P be a probability measure on X × R such that RL ,P (0) < ∞. Assume that the
conditional distributions P( · |x), x ∈ X , are symmetric and that for PX -almost all x ∈ X there
P (x) + [−δ, δ ](cid:12)(cid:12)x(cid:1) > 0 ,
P(cid:0)f ∗
exists a δ(x) > 0 such that for all δ ∈ (0, δ(x)] we have
P (x) + [ − δ,  + δ ](cid:12)(cid:12)x(cid:1) > 0 .
P(cid:0)f ∗
(8)
(9)
P (x)} and f ∗
Then, for PX -almost all x ∈ X , we have M∗ (x) = {f ∗
P (x) equals PX -almost surely
the unique median of P( · |x).

Obviously, condition (8) means that the conditional distributions have some mass around their me-
P ± . More-
dian f ∗
P , whereas (9) means that the conditional distributions have some mass around f ∗
over, [8] showed that under the assumptions of Lemma 2.6, the corresponding -insensitive SVM can
be used to estimate the conditional median. Let us now illustrate how the value of  inﬂuences both
the accuracy of this estimate and the sparsity. To this end, let us assume for the sake of simplicity
that the conditional distributions P( · |x) have continuous Lebesgue densities p( · |x) : R → [0, ∞).
By the symmetry of the conditional distributions it is then easy to see that these densities are sym-
metric around f ∗
P (x). Now, it follows from the continuity of the densities, that (8) is satisﬁed if
P (x) + |x) > 0. Let us ﬁrst consider the case where
P (x)|x) > 0, whereas (9) is satisﬁed if p(f ∗
p(f ∗
the conditional distributions are equal modulo translations. In other words, we assume that there
exists a continuous Lebesgue density q : R → [0, ∞) which is symmetric around 0 such that for
PX -almost all x ∈ X we have
y ∈ R.
P (x) + y |x) ,
q(y) = p(f ∗
Note that this assumption is essentially identical to a classical “signal plus noise” assumption. In
the following we further assume that q is unimodal, i.e., q has its only local and global maximum
at 0. From this we easily see that (8) is satisﬁed, and (9) is satisﬁed if q() > 0. By Lemma
2.6 and the discussion around (7) we then conclude that under the assumptions of Corollary 2.5
the fraction of support vectors asymptotically approaches 2Q([, ∞)), where Q is the probability
measure deﬁned by q . This conﬁrms the intuition that larger values of  lead to sparser decision
functions. In particular, if Q([, ∞)) = 0, the corresponding SVM produces super sparse decision
functions, i.e., decision functions whose number of support vectors does not grow linearly in the
sample size. However, not surprisingly, there is a price to be paid for this sparsity. Indeed, [8,
Lemma 3.3] indicates that the size of q() has a direct inﬂuence on the ability of fD,λ to estimate
the conditional median f ∗
(cid:26)t2/2
P . Let us describe this in a little more detail. To this end, we ﬁrst ﬁnd by
[8, Lemma 3.3] and the convexity of t (cid:55)→ CL ,Q (t) that
if t ∈ [0, ]
CL ,Q (t) − C ∗
L ,Q ≥ q() ·
t − 2/2
if t ≥ .
(cid:113)RL ,P (f ) − R∗
P(cid:107)L1 (PX ) ≤ (cid:112)2/q()
By a literal repetition of the proof of [8, Theorem 2.5] we then ﬁnd the self-calibration inequality
(cid:107)f − f ∗
L ,P ,

(10)

which holds for all f : X → R with RL ,P (f ) − R∗
L ,P ≤ 2/2. Now, if we are in the situation
L ,P in probability for n → ∞, and thus
of Corollary 2.5, then we know that RL ,P (fD,λn ) → R∗
(10) shows that fD,λn approximates the conditional median f ∗
P with respect to the L1 (PX )-norm.
However, the guarantee for this approximation becomes worse the smaller q() becomes, i.e., the
In other words, the sparsity of the decision functions may be paid by less accurate
larger  is.
estimates of the conditional median. On the other hand, our results also show that moderate values
for  can lead to both reasonable estimates of the conditional median and relatively sparse decision
functions. In this regard we further note that one can also use [8, Lemma 3.3] to establish self-
calibration inequalities that measure the distance of f to f ∗
P only up to . In this case, however, it
is obvious that such self-calibration inequalities are worse the larger  is, and hence the informal
conclusions above remain unchanged.
Finally, we like to mention that, if the conditional distributions are not equal modulo transla-
tions, then the situation may become more involved.
In particular, if we are in a situation with
P (x)|x) = inf x p(f ∗
P (x) + |x) > 0 but inf x p(f ∗
P (x)|x) > 0 and p(f ∗
P (x) + |x) = 0, self-
p(f ∗
calibration inequalities of the form (10) are in general impossible, and weaker self-calibration in-
equalities require additional assumptions on P. We refer to [8] where the case  = 0 is considered.

3 Proofs

minimize

subject to

maximize

n(cid:88)
Setting C := 1
2λn and introducing slack variables, we can restate the optimization problem (1) as
1
(cid:107)f (cid:107)2
(ξi + ˜ξi )
H + C
2
i=1
f (xi ) − yi ≤  + ξi ,
yi − f (xi ) ≤  + ˜ξi ,
ξi , ˜ξi ≥ 0
for all i = 1, . . . , n.
In the following we denote the (unique) solution of (11) by (f ∗ , ξ ∗ , ˜ξ ∗ ), where we note that we have
f ∗ = fD,λ . It is well-known, see e.g. [2, p. 117], that the dual optimization problem of (11) is
n(cid:88)
n(cid:88)
n(cid:88)
( ˜αi + αi ) − 1
yi ( ˜αi − αi ) − 
( ˜αi − αi )( ˜αj − αj )k(xi , xj )
2
i=1
i=1
i,j=1
0 ≤ αi , ˜αi ≤ C
for all i = 1, . . . , n,
subject to
n , ˜α∗
where k is the kernel of the RKHS H . Furthermore, if (α∗
1 , . . . , α∗
1 , ˜α∗
n ) denotes a solution of
(12), then we can recover the primal solution (f ∗ , ξ ∗ , ˜ξ ∗ ) by
n(cid:88)
i − α∗
i )k(xi , · ) ,
f ∗ =
( ˜α∗
i=1
i = max{0, f ∗ (xi ) − yi − } ,
ξ ∗
i = max{0, yi − f ∗ (xi ) − } ,
˜ξ ∗
for all i = 1, . . . , n. Moreover, the Karush-Kuhn-Tucker conditions of (12) are
i (f ∗ (xi ) − yi −  − ξ ∗
α∗
i ) = 0 ,
(16)
i (yi − f ∗ (xi ) −  − ˜ξ ∗
˜α∗
i ) = 0 ,
(17)
i − C )ξ ∗
(α∗
i = 0 ,
(18)
i − C ) ˜ξ ∗
( ˜α∗
i = 0 ,
(19)
ξ ∗
˜ξ ∗
i = 0 ,
(20)
i
i ˜α∗
α∗
i = 0 ,
(21)
where i = 1, . . . , n. Finally, note that by setting βi := ˜αi − αi the problem (12) can be simpliﬁed
to (3), and consequently, a solution β ∗ of (3) is of the form β ∗ = ˜α∗ − α∗ . The following simple
lemma provides lower and upper bounds for the set of support vectors.

(11)

(12)

(13)

(14)
(15)

(cid:8)i : |fD,λ (xi ) − yi | > (cid:9) ⊂ (cid:8)i : β ∗
i (cid:54)= 0(cid:9) ⊂ (cid:8)i : |fD,λ (xi ) − yi | ≥ (cid:9) .
Lemma 3.1 Using the above notations we have
Proof: Let us ﬁrst prove the inclusion on the left hand side. To this end, we begin by ﬁxing an index
i with fD,λ (xi ) − yi > . By fD,λ = f ∗ and (14), we then ﬁnd ξ ∗
i > 0, and hence (18) implies
i − α∗
i = −C (cid:54)= 0. The case
i = ˜α∗
i = 0 and hence we have β ∗
α∗
i = C . From (21) we conclude ˜α∗
yi − fD,λ (xi ) >  can be shown analogously, and hence we obtain the ﬁrst inclusion. In order to
i (cid:54)= 0. By β ∗
i − α∗
show the second inclusion we ﬁx an index i with β ∗
i = ˜α∗
i and (21) we then have
i (cid:54)= 0 or ˜α∗
i (cid:54)= 0. Let us ﬁrst consider the case α∗
i (cid:54)= 0 and ˜α∗
either α∗
i = 0. The KKT condition (16)
together with fD,λ = f ∗ implies fD,λ (xi ) − yi −  = ξ ∗
i ≥ 0 we get fD,λ (xi ) − yi ≥ .
i and since ξ ∗
The second case ˜α∗
i = 0 can be shown analogously.

ηi

We further need the following Hilbert space version of Hoeffding’s inequality from [12, Chapter 3],
see also [7, Chapter 6.2] for a slightly sharper inequality.
Theorem 3.2 Let (Ω, A, P) be a probability space and H be a separable Hilbert space. Moreover,
let η1 , . . . , ηn : Ω → H be independent random variables satisfying EP ηi = 0 and (cid:107)ηi (cid:107)∞ ≤ 1 for
(cid:114) τ
(cid:13)(cid:13)(cid:13)H
(cid:16)(cid:13)(cid:13)(cid:13) 1
(cid:17) ≥ 1 − 3e−τ .
n(cid:88)
all i = 1, . . . , n. Then, for all τ ≥ 1 and all n ≥ τ , we have
< 4
P
n
n
i=1
Finally, we need the following theorem, see [7, Corollary 5.10], which was essentially shown by
[13, 5, 3] .
Theorem 3.3 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. We write Φ : X → H for the canonical feature map of H ,
i.e., Φ(x) := k( · , x), x ∈ X . Then for all λ > 0 there exists a function h : X × R → [−1, 1] such
that for all n ≥ 1 and all D ∈ (X × R)n we have
(cid:107)fD,λ − fP,λ(cid:107)H ≤ λ−1(cid:107)EDhΦ − EPhΦ(cid:107)H ,
where ED denotes the empirical average with respect to D .
Proof of of Theorem 2.1: In order to show the ﬁrst estimate we ﬁx a δ > 0 and a λ > 0 such that
1 − 3e−τ ≤ Pn (cid:0)D ∈ (X × R)n : (cid:107)EDhΦ − EPhΦ(cid:107)H ≤ 4(cid:112)τ /n(cid:1)
δλ ≤ 4. Let τ := λ2 δ2n/16 which implies n ≥ τ . Combining Theorems 3.2 and 3.3 we then obtain
≤ Pn (cid:0)D ∈ (X × R)n : (cid:107)fD,λ − fP,λ(cid:107)H ≤ δ(cid:1) .
(22)
Let us now assume that we have a training set D ∈ (X × R)n such that (cid:107)fP,λ − fD,λ(cid:107)H ≤ δ . Given
a pair (x, y) ∈ Aδ
low (fP,λ ), we then have
 + δ < |fP,λ (x) − y | ≤ |fD,λ (x) − y | + |fP,λ (x) − fD,λ (x)| ≤ |fD,λ (x) − y | + δ
by the triangle inequality and (cid:107)k(cid:107)∞ ≤ 1 which implies (cid:107) · (cid:107)∞ ≤ (cid:107) · (cid:107)H . In other words, we have
#SV (fD,λ ) ≥ #(cid:8)i : |fD,λ (xi ) − yi | > (cid:9) ≥ #(cid:8)i : |fP,λ (xi ) − yi | >  + δ(cid:9)
low (fP,λ ) ⊂ Alow (fD,λ ). Consequently, Lemma 3.1 yields
Aδ
n(cid:88)
=
low (fP,λ ) (xi , yi ) .
1Aδ
i=1
(cid:17) ≥ 1 − 3e− δ2 λ2 n
Pn(cid:16)
n(cid:88)
Combining this estimate with (22) we then obtain
≥ 1
#SV (fD,λ )
D ∈ (X × R)n :
low (fP,λ ) (xi , yi )
1Aδ
.
16
n
n
i=1
Pn(cid:16)
(cid:17) ≥ 1 − e−2ρ2 n
n(cid:88)
low (fP,λ ) (xi , yi ) > P(cid:0)Aδ
low (fP,λ )(cid:1) − ρ
Moreover, Hoeffding’s inequality, see, e.g. [4, Theorem 8.1], shows
1
D ∈ (X × R)n :
1Aδ
n
i=1

≤ 1
n

.

up (fP,λ ) (xi , yi )
1Aδ

#SV (fD,λ )
n

for all ρ > 0 and n ≥ 1. From these estimates and a union bound we conclude the ﬁrst inequality.
In order to show the second estimate we ﬁrst observe that for training sets D ∈ (X × R)n with
(cid:107)fP,λ − fD,λ (cid:107)H ≤ δ we have Aup (fD,λ ) ⊂ Aδ
#SV (fD,λ ) ≤ n(cid:88)
up (fP,λ ). Lemma 3.1 then shows
up (fP,λ ) (xi , yi ) ,
1Aδ
i=1
Pn(cid:16)
(cid:17) ≥ 1 − 3e− δ2 λ2 n
n(cid:88)
and hence (22) yields
D ∈ (X × R)n :
16
i=1
Using Hoeffding’s inequality analogously to the proof of the ﬁrst estimate we then obtain the second
estimate.
(cid:91)
low (fP,λ ) ⊂ Aδ (cid:48)
low (fP,λ ) for 0 ≤ δ (cid:48) ≤ δ .
Proof of of Corollary 2.2: We ﬁrst observe that we have Aδ
Let us show
low (fP,λ ) = Alow (fP,λ ) .
(23)
Aδ
δ>0
Obviously, the inclusion “⊂” directly follows from the above monotonicity. Conversely, for (x, y) ∈
Alow (fP,λ ) we have |f (x) − y | >  and hence |f (x) − y | >  + δ for some δ > 0, i.e., we have
P(cid:0)Aδ
low (fP,λ )(cid:1) = P(cid:0)Alow (fP,λ )(cid:1) .
shown (x, y) ∈ Aδ
low (fP,λ ). From (23) we now conclude
lim
δ(cid:38)0
(cid:92)
up (fP,λ ) for 0 ≤ δ (cid:48) ≤ δ , and it is easy to check that
up (fP,λ ) ⊂ Aδ
In addition, we have Aδ (cid:48)
up (fP,λ ) = Aup (fP,λ ) .
Aδ
δ>0
up (fP,λ ) for all δ > 0 we have |f (x) − y | ≥  − δ for all δ > 0, from which
Indeed, if (x, y) ∈ Aδ
we conclude |f (x) − y | ≥ , i.e. (x, y) ∈ Aup (fP,λ ). Conversely, the inclusion “⊃” directly follows
P(cid:0)Aδ
up (fP,λ )(cid:1) = P(cid:0)Aup (fP,λ )(cid:1) .
from the above monotonicity of the sets Aup . From (25) we then conclude
lim
(26)
δ(cid:38)0
nn → ∞. Combining (24)
Let us now ﬁx a decreasing sequence (δn ) ⊂ (0, 1) with δn → 0 and δ2
and (26) with the estimates of Theorem 2.1, we then obtain the assertion.

(24)

(25)

Proof of Lemma 2.3: Since the loss function L is Lipschitz continuous and convex in t, it is easy
to verify that t (cid:55)→ CL ,P( · |x) (t) is Lipschitz continuous and convex for PX -almost all x ∈ X , and
hence M∗ (x) is a closed interval. In order to prove the remaining assertions it sufﬁces to show
that limt→±∞ CL ,P( · |x) (t) = ∞ for PX -almost all x ∈ X . To this end, we ﬁrst observe that
R∗
L ,P < ∞ implies C ∗
L ,P( · |x) < ∞ for PX -almost all x ∈ X . Let us ﬁx such an x, a B > 0,
and a sequence (tn ) ⊂ R with tn → −∞. By the shape of L , there then exists an r0 > 0 such
that L (y , t) ≥ 2B for all y , t ∈ R with |y − t| ≥ r0 . Furthermore, there exists an M > 0 with
P([−M , M ] | x) ≥ 1/2, and since tn → −∞ there further exists an n0 ≥ 1 such that tn ≤ −M − r0
(cid:90)
for all n ≥ n0 . For y ∈ [−M , M ] we thus have y − tn ≥ r0 , and hence we ﬁnally ﬁnd
CL ,P( · |x) (tn ) ≥
L (y , tn ) dP(y |x) ≥ B
[−M ,M ]
for all n ≥ n0 . The case tn → ∞ can be shown analogously.
For the proof of Theorem 2.4 we need the following two intermediate results.
Theorem 3.4 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL ,P (0) < ∞ and that H is dense in
(cid:0)(cid:8)x ∈ X : |fP,λ (x) − t| > δ for all t ∈ M∗ (x)(cid:9)(cid:1) < ρ .
L1 (PX ). Then, for all δ > 0 and ρ > 0, there exists a λ0 > 0 such that for all λ ∈ (0, λ0 ] we have
PX

Proof: Since H is dense in L1 (PX ) we have inf f ∈H RL ,P (f ) = R∗
L ,P by [9, Theorem 3], and
hence limλ→0 RL ,P (fP,λ ) = R∗
L ,P . Now we obtain the assertion from [6, Theorem 3.16].
Lemma 3.5 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL ,P (0) < ∞ and that H is dense in
P(cid:0)M 2δ
low (fP,λ )(cid:1) ≤ P(cid:0)Aδ
low (fP,λ )(cid:1) + ρ
P(cid:0)M 2δ
up (fP,λ )(cid:1) ≥ P(cid:0)Aδ
up (fP,λ )(cid:1) − ρ .
L1 (PX ). Then, for all δ > 0 and ρ > 0, there exists a λ0 > 0 such that for all λ ∈ (0, λ0 ] we have
and
λ (x)| ≤ δ(cid:9)(cid:17)
low (fP,λ ) ⊂ (cid:16)
low (fP,λ ) ∩ (cid:8)(x, y) ∈ X × R : |fP,λ (x) − t∗
Proof: We write t∗
λ (x) for the real number deﬁned by (6) for f (x) := fP,λ (x). Then we have
∪ (cid:8)(x, y) ∈ X × R : |fP,λ (x) − t(x)| > δ for all t(x) ∈ M∗ (x)(cid:9) .
M 2δ
M 2δ
Moreover, given an (x, y) ∈ M 2δ
low (fP,λ ) ∩ {(x, y) ∈ X × R : |fP,λ (x) − t∗
λ (x)| ≤ δ}, we ﬁnd
λ (x) − y | ≤ |fP,λ (x) − t∗
 + 2δ < |t∗
λ (x)| + |fP,λ (x) − y | ≤ δ + |fP,λ (x) − y | ,
i.e., we have (x, y) ∈ Aδ
up (fP,λ ) ⊂ (cid:16)
λ (x)| ≤ δ(cid:9)(cid:17)
up (fP,λ ) ∩ (cid:8)(x, y) ∈ X × R : |fP,λ (x) − t∗
low (fP,λ ). Estimating the probability of the remaining set by Theorem 3.4
then yields the ﬁrst assertion. In order to prove the second estimate we ﬁrst observe that
∪ (cid:8)(x, y) ∈ X × R : |fP,λ (x) − t(x)| > δ for all t(x) ∈ M∗ (x)(cid:9) .
Aδ
Aδ
For (x, y) ∈ Aδ
up (fP,λ ) ∩ {(x, y) ∈ X × R : |fP,λ (x) − t∗
λ (x)| ≤ δ} we further have
λ (x) − y | ≤ δ + |t∗
λ (x)| + |t∗
 − δ ≤ |fP,λ (x) − y | ≤ |fP,λ (x) − t∗
λ (x) − y | ,
i.e., we have (x, y) ∈ M 2δ
up (fP,λ ). Again, the assertion now follows from Theorem 3.4 .
P(cid:0)M δ
low (fP,λ )(cid:1) = P(cid:0)Mlow (fP,λ )(cid:1)
P(cid:0)M δ
up (fP,λ )(cid:1) = P(cid:0)Mup (fP,λ )(cid:1)
Proof of Theorem 2.4: Analogously to the proofs of (24) and (26), we ﬁnd
lim
lim
and
δ(cid:38)0
δ(cid:38)0
Combining these equations with Theorem 2.1 and Lemma 3.5, we then obtain the assertion.

References
[1] A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression. Bernoulli,
13:799–819, 2007.
[2] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University
Press, Cambridge, 2000.
[3] E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel
methods. J. Mach. Learn. Res., 5:1363–1390, 2004.
[4] L. Devroye, L. Gy ¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New
York, 1996.
[5] I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105, 2003.
[6] I. Steinwart. How to compare different loss functions. Constr. Approx., 26:225–287, 2007.
[7] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008.
[8] I. Steinwart and A. Christmann. How SVMs can estimate quantiles and the median. In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 305–312.
MIT Press, Cambridge, MA, 2008.
[9] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In G. Lugosi
and H. U. Simon, editors, Proceedings of the 19th Annual Conference on Learning Theory, pages 79–93.
Springer, New York, 2006.
[10] V. Vapnik, S. Golowich, and A. Smola. Support vector method for function approximation, regression
estimation, and signal processing. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural
Information Processing Systems 9, pages 81–287. MIT Press, Cambridge, MA, 1997.
[11] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998.
[12] V. Yurinsky. Sums and Gaussian Vectors. Lecture Notes in Math. 1617. Springer, Berlin, 1995.
[13] T. Zhang. Convergence of large margin separable linear classiﬁcation. In T. K. Leen, T. G. Dietterich, and
V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 357–363. MIT Press,
Cambridge, MA, 2001.


Differentiable Sparse Coding

David M. Bradley
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dbradley@cs.cmu.edu

J. Andrew Bagnell
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dbagnell@ri.cmu.edu

Abstract

Prior work has shown that features which appear to be biologically plausible as
well as empirically useful can be found by sparse coding with a prior such as
a laplacian (L1 ) that promotes sparsity. We show how smoother priors can pre-
serve the beneﬁts of these sparse priors while adding stability to the Maximum
A-Posteriori (MAP) estimate that makes it more useful for prediction problems.
Additionally, we show how to calculate the derivative of the MAP estimate efﬁ-
ciently with implicit differentiation. One prior that can be differentiated this way
is KL-regularization. We demonstrate its effectiveness on a wide variety of appli-
cations, and ﬁnd that online optimization of the parameters of the KL-regularized
model can signiﬁcantly improve prediction performance.

1

Introduction

Sparse approximation is a key technique developed in engineering and the sciences which approxi-
mates an input signal, X , in terms of a “sparse” combination of ﬁxed bases B . Sparse approximation
relies on an optimization algorithm to infer the Maximum A-Posteriori (MAP) weights ˆW that best
reconstruct the signal, given the model X ≈ f (BW ). In this notation, each input signal forms
a column of an input matrix X , and is generated by multiplying a set of basis vectors B , and a
column from a coefﬁcient matrix W , while f (z ) is an optional transfer function. This relationship
is only approximate, as the input data is assumed to be corrupted by random noise. Priors which
produce sparse solutions for W , especially L1 regularization, have gained attention because of their
usefulness in ill-posed engineering problems [1], their ability to elucidate certain neuro-biological
phenomena, [2, 3], and their ability to identify useful features for classiﬁcation from related unla-
beled data [4].
Sparse coding [2] is closely connected to Independent Component Analysis as well as to certain
approaches to matrix factorization. It extends sparse approximation by learning a basis matrix B
which represents well a collection of related input signals–the input matrix X –in addition to per-
forming optimization to compute the best set of weights ˆW . Unfortunately, existing sparse coding
algorithms that leverage an efﬁcient, convex sparse approximation step to perform inference on the
latent weight vector [4] are difﬁcult to integrate into a larger learning architecture.
It has been
convincingly demonstrated that back-propagation is a crucial tool for tuning an existing generative
model’s output in order to improve supervised performance on a discriminative task. For example,
greedy layer-wise strategies for building deep generative models rely upon a back-propagation step
to achieve excellent model performance [5]. Unfortunately, existing sparse coding architectures pro-
duce a latent representation ˆW that is an unstable, discontinuous function of the inputs and bases;
an arbitrarily small change in input can lead to the selection of a completely different set of latent
weights.
We present an advantageous new approach to coding that uses smoother priors which preserve the
sparsity beneﬁts of L1 -regularization while allowing efﬁcient convex inference and producing stable
latent representations ˆW . In particular we examine a prior based on minimizing KL-divergence to

1

the uniform distribution which has long been used for approximation problems [6, 7]. We show this
increased stability leads to better semi-supervised classiﬁcation performance across a wide variety
of applications for classiﬁers using the latent representation ˆW as input. Additionally, because of
the smoothness of the KL-divergence prior, B can be optimized discriminatively for a particular
application by gradient descent, leading to outstanding empirical performance.

2 Notation

Uppercase letters, X , denote matrices and lowercase letters, x, denote vectors. For matrices, super-
scripts and subscripts denote rows and columns respectively. Xj is the jth column of X , X i is the
j is the element in the ith row and jth column. Elements of vectors are indicated
ith row of X , and X i
by subscripts, xj , and superscripts on vectors are used for time indexing xt . X T is the transpose of
matrix X .

3 Generative Model

Sparse coding ﬁts a generative model (1) to unlabeled data, and the MAP estimates of the latent
variables of this model have been shown to be useful as input for prediction problems [4].
(1)
divides the latent variables into two independent groups, the coefﬁcients W and the basis B , which
combine to form the matrix of input examples X . Different examples (columns of X ) are assumed
to be independent of each other. The Maximum A Posteriori (MAP) approximation replaces the
integration over W and B in (1) with the maximum value of P (X |W, B )P (W )P (B ), and the
values of the latent variables at the maximum, ˆW and ˆB , are the MAP estimates.
Finding ˆW given B is an approximation problem, solving for ˆW and ˆB simultaneously over a set
(cid:90)
(cid:90)
(cid:90)
(cid:90)
of independent examples is a coding problem.
(cid:89)
W
B
W
B
i
Given B , the negative log of the generative model can be optimized independently for each example,
and it is denoted for a generic example x by L in (2). L decomposes into the sum of two terms, a loss
function DL (x(cid:107)f (Bw)) between an input example and the reconstruction produced by the transfer
function f , and a regularization function DP (w(cid:107)p) that measures a distance between the coefﬁcients
for the example w and a parameter vector p. A regularization constant λ controls the relative weight
of these two terms. For ﬁxed B , minimizing (2) with respect to w separately for each example is
equivalent to maximizing (1).

P (X |W, B )P (W )P (B )dW dB =

P (X ) =

P (B )

P (Xi |Wi , B )P (Wi )dW dB (1)

L = DL (x(cid:107)f (Bw)) + λDP (w(cid:107)p)
ˆw = arg min
L
w
In many applications, the anticipated distribution of x after being corrupted by noise can be modeled
by an exponential family distribution. Every exponential family distribution deﬁnes a Bregman di-
linear transfer function, DL (x(cid:107)f (Bw)) = (cid:80)
vergence which serves as a matching loss function for estimating the parameters of the distribution1 .
One common choice for the loss/transfer functions is the squared loss function with its matching
i (xi − B iw)2 , which is the matching Bregman Diver-
gence for x drawn from a multidimensional gaussian distribution.
The regularization function DP (w(cid:107)p) is also often a Bregman divergence, but may be chosen for
other features such as the sparsity of the resulting MAP estimate ˆw . A vector is commonly called
p -norm2 , p ≤ 1 regularization
sparse if many elements are exactly zero. The entropy [9, 10], and Lp
functions [2, 3, 4] promote this form of sparsity, and all of them have shown the ability to learn bases

(2)
(3)

1The maximum likelihood parameter estimate for any regular exponential family distribution can be found
by minimizing the corresponding Bregman divergence for that family, and every Bregman divergence has a
matching transfer function which leads to a convex minimization problem [8]. That matching transfer function
p (x) = P
is the gradient ∇φ of the function φ which is associated with the Bregman divergence Dφ (x(cid:107)y) = φ(x) −
φ(y) − (cid:104)x − y , ∇φ(y)(cid:105).
i |xi |p corresponds to the negative log of a generalized gaussian prior.
2Lp

2

containing interesting structure from unlabeled data. However, of these only L1 leads to an efﬁcient,
convex procedure for inference, and even this prior does not produce differentiable MAP estimates.
We argue that if the latent weight vector ˆw is to be used as input to a classiﬁer, a better deﬁnition of
“sparsity” is that most elements in ˆw can be replaced by elements in a constant vector p without sig-
niﬁcantly increasing the loss. One regularization function that produces this form of pseudo-sparsity
is the KL-divergence K L(w(cid:107)p). This regularization function has long been used for approximation
problems in Geophysics, Crystallography, Astronomy, and Physics, where it is commonly referred to
as Maximum Entropy on the Mean (MEM) [7], and has been shown in the online setting to compete
with low L1 -norm solutions in terms of regret [11, 12].
tion to maxi xi is a sum of exponentials, (cid:80)
L1 regularization provides sparse solutions because its Fenchel dual [13] is the max function, mean-
ing only the most useful basis vectors participate in the reconstruction. A differentiable approxima-
i , whose dual is the KL-divergence (4). Regularization
i ex
with KL has proven useful in online learning, where it is the implicit prior of the exponentiated gra-
dient descent (EGD) algorithm. EGD has been shown to be “sparse” in the sense that it can select a
few relevant features to use for a prediction task from many irrelevant ones.
The form of KL we use (4) is the full Bregman divergence of the negative entropy function3 . Often
KL is used to compute distances between probability distributions, and for this case the KL we
use reduces to the standard form. For sparse coding however, it is inconvenient to assume that
(cid:21)
(cid:20)
(cid:107) ˆw(cid:107)1 = (cid:107)p(cid:107)1 = 1, so we use the full unnormalized KL instead.
DP (w(cid:107)p) = (cid:88)
wi log wi
pi
i
For the prior vector p we use a uniform vector whose L1 magnitude equals the expected L1 mag-
nitude of w . p has an analogous effect to the q parameter in Lq -norm regularization. p → 0
approximates L1 and p → ∞ approximates L2 . Changing p affects the magnitude of the KL term,
so λ in (2) must be adjusted to balance the loss term in the sparse coding objective function (small
values of p require small values of λ).
Below we provide a) an efﬁcient procedure for inferring ˆw in this model; b) an algorithm for itera-
tively updating the bases B , and c) show that this model leads to differentiable estimates of ˆw . We
also provide the general form of the derivative for arbitrary Bregman losses.

− wi + pi

(4)

Implementation
4
To compute ˆw with KL-regularization, we minimize (3) using exponentiated gradient descent (EGD)
with backtracking until convergence (5). EGD automatically enforces positivity constraints on the
coefﬁcient vector w , and is particularly efﬁcient for optimization because it is the natural mirror
descent rule for KL-regularization [12]. The gradient of the objective function (2) with respect to
the coefﬁcient for the jth basis vector wj is given in (6) for matching loss/transfer function pairs.

j = wt
wt+1
j e

−α ∂L
∂wj

(5)

(6)

= (f (Bw) − x)T Bj + λ log wj
∂L
pj
∂wj
This iterative update is run until the maximum gradient element is less than a threshold, which
is estimated by periodically running a random set of examples to the limits of machine precision,
and selecting the largest gradient threshold that produces ˆw within  of the exact solution. The
α parameter is continuously updated to balance the number of sucessful steps and the number of
backtracking steps4 . Because L1 -regularization produces both positive and negative weights, to
compare L1 and KL regularization on the same basis we expand the basis used for KL by adding the
negation of each basis vector, which is equivalent to allowing negative weights (see Appendix B).
During sparse coding the basis matrix B is updated by Stochastic Gradient Descent (SGD), giving
the update rule Bt+1 = Bt − η ∂L
. This update equation does not depend on the prior chosen
∂B i
j
3−H (x) = x log(x)
4 In our experiments, if the ratio of backtracking steps to total steps was more than 0.6, α was decreased by
10%. Similarly α was increased by 10% if the ratio fell below 0.3.

3

for w and is given in (7) for matching loss/transfer function pairs. SGD implements an implicit
L2 regularizer and is suitable for online learning, however because the magnitude of w is explicitly
penalized, the columns of B were constrained to have unit L2 norm to prevent the trivial solution of
√
inﬁnitely large B and inﬁnitely small w . The step size was adjusted for the magnitude of ˆw in each
application, and was then decayed over time as η ∝ 1/
t. The same SGD procedure was also used
to optimize B through backpropagation, as explained in the next section.

= wj (f (B iw) − xi )

∂L
∂B i
j

(7)

5 Modifying a Generative Model For A Discriminative Task

Sparse Coding builds a generative model from unlabeled data that captures structure in that data by
learning a basis B . Our hope is that the MAP estimate of basis coefﬁcients ˆw produced for each
input vector x will be useful for predicting a response y associated with x. However, the sparse
coding objective function only cares about reconstructing the input well, and does not attempt to
make ˆw useful as input for any particular task. Fortunately, since priors such as KL-divergence
regularization produce solutions that are smooth with respect to small changes in B and x, B can be
modiﬁed through back-propagation to make ˆw more useful for prediction.
The key to computing the derivatives required for backpropagation is noting that the gradient with
respect to w of the optimization (3) at its minimum ˆw can be written as a set of ﬁxed point equations
where the gradients of the loss term equal the gradient of the regularization:

∇DL (x(cid:107)f (B ˆw)) .

∇DP ( ˆw(cid:107)p) = − 1
λ
Then if the regularization function is twice differentiable with respect to w , we can use implicit dif-
ferentiation on (8) to compute the gradient of ˆw with respect to B , and x [14]. For KL-regularization
and the simple case of a linear transfer function with squared loss, ∂ ˆw
∂B is given in (9), where (cid:126)ei is a
unit vector whose ith element is 1. A general derivation for matched loss/transfer function pairs as
∂x means that multiple
deﬁned before is provided in appendix C. Note that the ability to compute ∂ ˆw
(cid:18)
(cid:19)−1 (cid:0)(B k ˆwi )T + (cid:126)ei (f (B k ˆw) − xk )(cid:1)
layers of sparse coding could be used.
B T B + diag( λ
ˆw

(8)

(9)

= −

∂ ˆw
∂B k
i

)

6 Experiments

We verify the performance of KL-sparse coding on several benchmark tasks including the MNIST
handwritten digit recognition data-set, handwritten lowercase English characters classiﬁcation,
movie review sentiment regression, and music genre classiﬁcation (Appendix E). In each applica-
tion, the ˆw produced using KL-regularization were more useful for prediction than those produced
with L1 regularization due to the stability and differentiability provided by KL.

6.1 Sparsity
KL-regularization retained the desirable pseudo-sparsity characteristics of L1 , namely that each
example, x, produces only a few large elements in ˆw . Figure 1 compares the mean sorted and
normalized coefﬁcient distribution over the 10,000 digit MNIST test set for KL-divergence and
several Lp
p regularization functions, and shows that although the KL regularization function is not
sparse in the traditional sense of setting many elements of ˆw to zero, it is sparse in the sense that ˆw
contains only a few large elements in each example, lending support to the idea that this sense of
sparsity is more important for classiﬁcation.

6.2 Stability
Because the gradient of the KL-divergence regularization function goes to ∞ with increasing w , it
produces MAP estimates ˆw that change smoothly with x and B (see Appendix A for more details).

4

Figure 1: Left: Mean coefﬁcient distribution over the 10,000 digit MNIST test set for various regularization
functions. Each example ˆw was sorted by magnitude and normalized by (cid:107) ˆw(cid:107)∞ before computing the mean
over all examples. Right: test set classiﬁcation performance. Regularization functions that produced few large
values in each examples (such as KL and L1) performed the best. Forcing small coefﬁcients to be exactly 0
was not necessary for good performance. Note the log scale on the horizontal axis.

Regularization
L1
KL

Gaussian Noise (Standard Deviation) Random Translations (pixels)
0.1
0.01
1
0.1
1.211±0.213
0.285±0.056
0.0283±0.0069
0.138±0.026
0.0172±0.0016
0.164±0.015
0.070±0.011
0.671±0.080

Table 1: The 10,000 images of handwritten digits in the MNIST test set were used to show the stability
beneﬁts of KL-regularization. Distance (in L1 ) between the representation for x, ˆw , and the representation
after adding noise, divided by (cid:107) ˆw(cid:107)1 . KL-regularization provides representations that are signiﬁcantly more
stable with respect to both uncorrelated additive Gaussian noise (Left), and correlated noise from translating
the digit image in a random direction (Right).

Table 1 quantiﬁes how KL regularization signiﬁcantly reduces the effect on ˆw of adding noise to the
input x.
This stability improves the usefulness of ˆw for prediction. Figure 2 shows the most-discriminative
2-D subspace (as calculated by Multiple Discriminant Analysis [15]) for the input space, the L1 and
KL coefﬁcient space, and the KL coefﬁcient space after it has been specialized by back-propagation.
The L1 coefﬁcients tame the disorder of the input space so that clusters for each class are apparent,
although noisy and overlapping. The switch to KL regularization makes these clusters more distinct,
and applying back-propagation further separates the clusters.

Figure 2: Shown is the distribution of the eight most confusable digit classes in the input space and in the
coefﬁcient spaces produced by sparse approximation. Multiple Discriminant Analysis was used to compute the
most discriminative 2-D projection of each space. The PCA-whitened input space (left) contains a lot of overlap
between the classes. L1 regularization (center) discovers structure in the unlabeled data, but still produces more
overlap between classes than KL sparse approximation (right) does with the same basis trained with L1 sparse
coding. Figure best seen in color.

6.3
Improved Prediction Performance
On all applications, the stability provided by KL-regularization improved performance over L1 , and
back-propagation further improved performance when the training set had residual error after an
output classiﬁer was trained.

5

6.3.1 Handwritten Digit Classiﬁcation
We tested our algorithm on the benchmark MNIST handwritten digits dataset [16]. 10,000 of the
60,000 training examples were reserved for validation, and classiﬁcation performance was evaluated
on the separate 10,000 example test set. Each example was ﬁrst reduced to 180D from 768D by
PCA, and then sparse coding was performed using a linear transfer function and squared loss5 . The
validation set was used to pick the regularization constant, λ, and the prior mean for KL, p.
Maxent classiﬁers6 [17] were then learned on randomly sampled subsets of the training set of vari-
ous sizes. Switching from L1 -regularized to KL-regularized sparse approximation improved perfor-
mance in all cases (Table 2). When trained on all 50,000 training examples, the test set classiﬁcation
error of KL coefﬁcients, 2.21%, was 37% lower than the 3.53% error rate obtained on the L1 -
regularized coefﬁcients. As shown in Table 3, this increase in performance was consistent across
a diverse set of classiﬁcation algorithms. After running back-propagation with the KL-prior, the
test set error was reduced to 1.30%, which improves on the best results reported7 for other shallow-
architecture permutation-invariant classiﬁers operating on the same data set without prior knowledge
about the problem8 , (see Table 4).

Training Set Size
L1 (Test Set)
KL (Test set)
KL After Backprop (Test Set)
Improvement from Backprop
KL (Training Set)

1000
50000
20000
10000
2000
7.72% 6.63% 4.74% 4.16% 3.53%
5.87% 5.06% 3.00% 2.51% 2.21%
5.66
4.46% 2.31% 1.78% 1.30%
3.6% 11.9% 23.0% 29.1% 43.0%
0.00% 0.05% 1.01% 1.50% 1.65%

Table 2: The ability to optimize the generative model with back-propagation leads to signiﬁcant performance
increases when the training set is not separable by the model learned on the unlabeled data. Shown is the
misclassiﬁcation rate on the MNIST digit classiﬁcation task. Larger training sets with higher residual error
beneﬁt more from back-propagation.

Classiﬁer
Maxent
2-layer NN
SVM (Linear)
SVM (RBF)

PCA
KL
L1
7.49% 3.53% 2.21%
2.23% 2.13% 1.40%
5.55% 3.95% 2.16%
1.54% 1.94% 1.28%

KL+backprop
1.30%
1.36%
1.34%
1.31%

Table 3: The stability afforded by the KL-prior improves the performance of all classiﬁer types over the
L1 prior. In addition back-propagation allows linear classiﬁers to do as well as more complicated non-linear
classiﬁers.

Algorithm
Test Set Error

L1
KL
3.53% 2.21%

KL+backprop
1.30%

SVM 2-layer NN [18]
1.4%
1.6%

3-layer NN
1.53%

Table 4: Test set error of various classiﬁers on the MNIST handwritten digits database.

6.3.2 Transfer to Handwritten Character Classiﬁcation
In [4], a basis learned by L1 -regularized sparse coding on handwritten digits was shown to improve
classiﬁcation performance when used for the related problem of handwritten character recognition

5This methodology was chosen to match [4]
6Also known as multi-class logistic regression
7An extensive comparison of classiﬁcation algorithms for this dataset can be found on the MNIST website,
http://yann.lecun.com/exdb/mnist/
8Better results have been reported when more prior knowledge about the digit recognition problem is pro-
vided to the classiﬁer, either through specialized preprocessing or by giving the classiﬁer a model of how digits
are likely to be distorted by expanding the data set with random afﬁne and elastic distortions of the training
examples or training with vicinal risk minimization. Convolutional Neural Networks produce the best results
on this problem, but they are not invariant to permutations in the input since they contain a strong prior about
how pixels are connected.

6

with small training data sets (< 5000 examples). The handwritten English characters dataset9 they
used consists of 16x8 pixel images of lowercase letters. In keeping with their work, we padded
and scaled the images to match the 28x28 pixel size of the MNIST data, projected onto the same
PCA basis that was used for the MNIST digits, and learned a basis from the MNIST digits by
L1 -regularized sparse coding. This basis was then used for sparse approximation of the English
characters, along with a linear transfer function and squared loss.
In this application as well, Table 5 shows that simply switching to a KL prior from L1 for sparse
approximation signiﬁcantly improves the performance of a maxent classiﬁer. Furthermore, the KL
prior allows online improvement of the sparse coding basis as more labeled data for the character-
recognition task becomes available. This improvement increases with the size of the training set, as
more information becomes available about the target character recognition task.

Training Set Size Raw PCA
100
44.3
46.9
61.2
60.4
500
66.7
66.3
1000
76.0
75.1
5000
20000
79.3
79.7

L1
44.0
63.7
69.5
78.9
83.3

KL KL+backprop
50.7
49.4
69.9
69.2
76.4
75.0
84.2
82.5
89.1
86.0

Table 5: Classiﬁcation Accuracy on 26-way English Character classiﬁcation task.

6.3.3 Comparison to sLDA: Movie Review Sentiment Regression
KL-regularized sparse coding bears some similarities to the supervised LDA (sLDA) model intro-
duced in [19], and we provide results for the movie review sentiment classiﬁcation task [20] used
in that work. To match [19] we use vectors of normalized counts for the 5000 words with the high-
est tf-idf score among the 5006 movie reviews in the data set, use 5-fold cross validation, compute
tions ˆy : pR2 := 1 − ((cid:80)(y − ˆy)2 )/((cid:80)(y − ¯y)2 )). Since the input is a probability distribution, we use
predictions with linear regression on ˆw , and report our performance in terms of predictive R2 (the
fraction of variability in the out-of-fold response values which is captured by the out-of-fold predic-
a normalized exponential transfer function, f (B , w) = eBw
, to compute the reconstruction of the
(cid:107)eBw (cid:107)1
input. For sparse coding we use KL-divergence for both the loss and the regularization functions,
as minimizing the KL-divergence between the empirical probability distribution of the document
given by each input vector x and f (B , w) is equivalent to maximizing the “constrained Poisson
distribution” used to model documents in [21] (details given in appendix D). Table 6 shows that the
sparse coding generative model we use is competitive with and perhaps slightly better than LDA.
After back-propagation, its performance is superior to the supervised version of LDA, sLDA10 .

predictive R2
0.263
0.264
0.281
0.457
0.500
0.507
0.534

Algorithm
LDA [19]
64D unsupervised KL sparse coding
256D unsupervised KL sparse coding
L1 -regularized regression [19]
sLDA [19]
L2 -regularized regression
256D KL-regularized coding with backprop

Table 6: Movie review sentiment prediction task. KL-regularized sparse coding compares favorably with LDA
and sLDA.

7 Conclusion
This paper demonstrates on a diverse set of applications the advantages of using a differentiable,
smooth prior for sparse coding. In particular, a KL-divergence regularization function has signiﬁcant
9Available at http://ai.stanford.edu/˜btaskar/ocr/
10Given that the word counts used as input are very sparse to begin with, classiﬁers whose regret bounds de-
pend on the L2 norm of the gradient of the input (such as L2 -regularized least squares) do quite well, achieving
a predictive R2 value on this application of 0.507.

7

advantages over other sparse priors such as L1 because it retains the important aspects of sparsity,
while adding stability and differentiability to the MAP estimate ˆw . Differentiability in particular
is shown to lead to state-of-the-art performance by allowing the generative model learned from
unlabeled data by sparse-coding to be adapted to a supervised loss function.

Acknowledgments
David M. Bradley is supported by an NDSEG fellowship provided by the Army Research Ofﬁce.
The authors would also like to thank David Blei, Rajat Raina, and Honglak Lee for their help.

References
[1] J. A. Tropp, “Algorithms for simultaneous sparse approximation: part ii: Convex relaxation,” Signal
Process., vol. 86, no. 3, pp. 589–602, 2006.
[2] B. Olshausen and D. Field, “Sparse coding with an overcomplete basis set: A strategy employed by v1?”
Vision Research, 1997.
[3] Y. Karklin and M. S. Lewicki, “A hierarchical bayesian model for learning non-linear statistical regulari-
ties in non-stationary natural signals,” Neural Computation, vol. 17, no. 2, pp. 397–423, 2005.
[4] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng, “Self-taught learning: Transfer learning from
unlabeled data,” in ICML ’07: Proceedings of the 24th international conference on Machine learning,
2007.
[5] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy layer-wise training of deep networks,”
in Advances in Neural Information Processing Systems 19, B. Sch ¨olkopf, J. Platt, and T. Hoffman, Eds.
Cambridge, MA: MIT Press, 2007, pp. 153–160.
[6] E. Rietsch, “The maximum entropy approach to inverse problems,” Journal of Geophysics, vol. 42, pp.
489–506, 1977.
[7] G. Besnerais, J. Bercher, and G. Demoment, “A new look at entropy for solving linear inverse problems,”
IEEE Trans. on Information Theory, vol. 45, no. 5, pp. 1565–1578, July 1999.
[8] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh, “Clustering with bregman divergences,” Journal of
Machine Learning Research, vol. 6, pp. 1705–1749, 2005.
[9] M. Brand, “Pattern discovery via entropy minimization,” in AISTATS 99, 1999.
[10] M. Shashanka, B. Raj, and P. Smaragdis, “Sparse overcomplete latent variable decomposition of counts
data,” in NIPS, 2007.
[11] J. Kivinen and M. Warmuth, “Exponentiated gradient versus gradient descent for linear predictors,” In-
formation and Computation, pp. 1–63, 1997.
[12] N. Cesa-Bianchi and G. Lugosi, Prediction, Learning, and Games. Cambridge University Press, 2006.
[13] R. Rifkin and R. Lippert, “Value regularization and fenchel duality,” The Journal of Machine Learning
Research, vol. 8, pp. 441–479, 2007.
[14] D. Widder, Advanced Calculus, 2nd ed. Dover Publications, 1989.
[15] R. Duda, P. Hart, and D. Stork, Pattern classiﬁcation. Wiley New York, 2001.
[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recogni-
tion,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
[17] K. Nigam, J. Lafferty, and A. McCallum, “Using maximum entropy for text classiﬁcation,” 1999.
[Online]. Available: citeseer.ist.psu.edu/article/nigam99using.html
[18] P. Y. Simard, D. Steinkraus, and J. C. Platt, “Best practices for convolutional neural networks applied
to visual document analysis,” in ICDAR ’03: Proceedings of the Seventh International Conference on
Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2003, p. 958.
[19] D. M. Blei and J. D. McAuliffe, “Supervised topic models,” in NIPS 19, 2007.
[20] B. Pang and L. Lee, “Seeing stars: Exploiting class relationships for sentiment categorization with respect
to rating scales,” in Proceedings of the ACL, 2005, pp. 115–124.
[21] R. Salakhutdinov and G. Hinton, “Semantic hashing,” in SIGIR workshop on Information Retrieval and
applications of Graphical Models, 2007.

8


Bayesian Experimental Design of Magnetic
Resonance Imaging Sequences

Matthias W. Seeger, Hannes Nickisch, Rolf Pohmann and Bernhard Sch ¨olkopf
Max Planck Institute for Biological Cybernetics
Spemannstraße 38
72012 T ¨ubingen, Germany
{seeger,hn,rolf.pohmann,bs}@tuebingen.mpg.de

Abstract

We show how improved sequences for magnetic resonance imaging can be
found through optimization of Bayesian design scores. Combining approximate
Bayesian inference and natural image statistics with high-performance numeri-
cal computation, we propose the ﬁrst Bayesian experimental design framework
for this problem of high relevance to clinical and brain research. Our solution
requires large-scale approximate inference for dense, non-Gaussian models. We
propose a novel scalable variational inference algorithm, and show how powerful
methods of numerical mathematics can be modiﬁed to compute primitives in our
framework. Our approach is evaluated on raw data from a 3T MR scanner.

1 Introduction

Magnetic resonance imaging (MRI) [7, 2] is a key diagnostic technique in healthcare nowadays, and
of central importance for experimental research of the brain. Without applying any harmful ioniz-
ing radiation, this technique stands out by its amazing versatility: by combining different types of
radiofrequency irradiation and rapidly switched spatially varying magnetic ﬁelds (called gradients)
superimposing the homogeneous main ﬁeld, a large variety of different parameters can be recorded,
ranging from basic anatomy to imaging blood ﬂow, brain function or metabolite distribution. For
this large spectrum of applications, a huge number of sequences has been developed that describe
the temporal ﬂow of the measurement, ranging from a relatively low number of multi-purpose tech-
niques like FLASH [5], RARE [6], or EPI [9], to specialized methods for visualizing bones or
perfusion. To select the optimum sequence for a given problem, and to tune its parameters, is a dif-
ﬁcult task even for experts, and even more challenging is the design of new, customized sequences
to address a particular question, making sequence development an entire ﬁeld of research [1]. The
main drawbacks of MRI are high initial and running costs, since a very strong homogeneous mag-
netic ﬁeld has to be maintained, moreover long scanning times due to weak signals and limits to
gradient amplitude. With this in mind, by far the majority of scientiﬁc work on improving MRI
is motivated by obtaining diagnostically useful images in less time. Beyond reduced costs, faster
imaging also leads to higher temporal resolution in dynamic sequences for functional MRI (fMRI),
less annoyance to patients, and fewer artifacts due to patient motion.
In this paper, we employ Bayesian experimental design to optimize MRI sequences. Image recon-
struction from MRI raw data is viewed as a problem of inference from incomplete observations. In
contrast, current reconstruction techniques are non-iterative. For most sequences used in hospitals
today, reconstruction is done by a single fast Fourier transform (FFT). However, natural and MR
images show stable low-level statistical properties,1 which allows them to be reconstructed from

1These come from the presence of edges and smooth areas, which on a low level deﬁne image structure, and
which are not present in Gaussian data (noise).

1

fewer observations. In our work, a non-Gaussian prior distribution represents low-level spectral and
local natural image statistics. A similar idea is known as compressed sensing (CS), which has been
applied to MRI [8].
A different and more difﬁcult problem is to improve the sequence itself. In our Bayesian method,
a posterior distribution over images is maintained, which is essential for judging the quality of the
sequence: the latter can be modiﬁed so as to decrease uncertainty in regions or along directions of
interest, where uncertainty is quantiﬁed by the posterior. Importantly, this is done without the need
to run many MRI experiments in random a priori data collections. It has been proposed to design
sequences by blindly randomizing aspects thereof [8], based on CS theoretical results. Beyond being
hard to achieve on a scanner, our results indicate that random measurements do not work well for
real MR images. Similar negative ﬁndings for a variety of natural images are given in [12].
Our proposal requires efﬁcient Bayesian inference for MR images of realistic resolution. We present
a novel scalable variational approximate inference algorithm inspired by [16]. The problem is re-
duced to numerical mathematics primitives, and further to matrix-vector multiplications (MVM)
with large, structured matrices, which are computed by efﬁcient signal processing code. Most pre-
vious algorithms [3, 14, 11] iterate over single non-Gaussian potentials, which renders them of no
use for our problem here.2 Our solutions for primitives required here should be useful for other ma-
chine learning applications as well. Finally, we are not aware of Bayesian or classical experimental
design methods for dense non-Gaussian models, scaling comparably to ours. The framework of
[11] is similar, but could not be applied to the scale of interest here. Our model and experimental
design framework are described in Section 2, a novel scalable approximate inference algorithm is
developed in Section 3, and our framework is evaluated on a large-scale realistic setup with scanner
raw data in Section 4.

2 Sparse Linear Model. Experimental Design
Denote the desired MR image by u ∈ Rn , where n is the number of pixels. Under ideal conditions,
the raw data y ∈ Rm from the scanner is a linear map3 of u , motivating the likelihood
ε ∼ N (0, σ2I ).
y = Xu + ε ,
Here, each row of X is a single Fourier ﬁlter, determined by the sequence. In the context of this
paper, the problem of experimental design is how to choose X within a space of technically feasible
sequences, so that u can be best recovered given y . As motivated in Section 1, we need to specify
a prior P (u ) which represents low-level statistics of (MR) images, distinctly super-Gaussian distri-
butions — a Gaussian prior would not be a sensible choice. We use the one proposed in [12]. The
qY
posterior has the form
j=1
the prior being a product of Laplacians on linear projections sj of u , among them the image gradient
and wavelet coefﬁcients. The Laplace distribution encourages sparsity of s . Further details are given
in [12]. MVMs with B cost O(q) with q ≈ 3n. MAP estimation for the same model was used in
[8].
Bayesian inference for (1) is analytically not tractable, and an efﬁcient deterministic approximation
is discussed in Section 3. In the variant of Bayesian sequential experimental design used here, an
extension of X by X∗ ∈ Rd,n is scored by the entropy difference
∆(X∗ ) := H[P (u |y )] − EP (y∗ |y ) [H[P (u |y , y∗ )]] ,
(2)
where P (u |y , y∗ ) is the posterior after including (X∗ , y∗ ). This criterion measures the decrease in
uncertainty about u , averaged over the posterior P (y∗ |y ). Our approach is sequential: a sequence
is combined from parts, each extension being chosen by maximizing the entropy difference over a

P (u |y ) ∝ N (y |Xu , σ2I )

(1)

e− ˜τj |sj | ,

s = Bu , ˜τj = τj /σ,

2The model we use has q = 196096 potentials and n = 65536 latent variables. Any algorithm that iterates
over single potentials, has to solve at least q linear systems of size n, while our method often converges after
solving less than 50 of these.
3Phase contributions in u are discussed in Section 4.

2

candidate set {X∗ }. After each extension, a new scanner measurement is obtained for the single
extended sequence only. Our Bayesian predictive approach allows us to score many candidates
(X∗ , y∗ ) without performing costly MR measurements for them. The sequential restriction makes
sense for several reasons. First, MR sequences naturally decompose in a sequential fashion: they
describe a discontinuous path of several smooth trajectories (see Section 4). Also, a non-sequential
approach would never make use of any real measurements, relying much more on the correctness
of the model. Finally, the computational complexity of optimizing over complete sequences is
staggering. Our sequential approach seems also better suited for dynamic MRI applications.

3 Scalable Approximate Inference

(3)

(4)

In this section, we propose a novel scalable algorithm for the variational inference approx-
First, e− ˜τj |sj | =
imation proposed in [3]. We make use of ideas presented in [16].
−1
j /(2σ2 ) e−(τ 2
maxπj >0 e−πj s2
j /2)π
, using Legendre duality (the Laplace site is log-convex in s2
j )
j
[3]. Let π = (πj ) and Π = diag π . To simplify the derivation, assume that B T ΠB is invertible,4
and let Q(u ) ∝ exp(−uT B T ΠBu /(2σ2 )), Q(y , u ) := P (y |u )Q(u ). The joint distribution is
Gaussian, and
2 (τ 2 )T (π−1 ) |B T ΠB /(2πσ2 )|−1/2 R P (y |u )Q(u ) du , and
Q(u |y ) = N (u |h , σ2Σ ), Σ−1 = A := X T X + B T ΠB , h = ΣX T y .
Z
We have that P (y ) ≥ e− 1
P (y |u )Q(u ) du = |2πσ2Σ |1/2 max
P (y |u )Q(u ),
Q(u |y )Q(y ) = |2πσ2Σ |1/2 max
u
u
where the maximum is attained at u = h . Therefore, P (y ) ≥ C1 (σ2 )e−φ(π )/2 with
φ(π ) := log |A | + (τ 2 )T (π−1 ) + min
σ−2 ky − Xu k2 + σ−2sT Πs ,
s = Bu ,
u
and the bound is tightened by minimizing φ(π ). Now, g(π ) := log |A | is concave, so we can
use another Legendre duality, g(π ) = minz(cid:23)0 zT π − g∗ (z ), to obtain an upper bound φz (π ) =
minu φz (u , π ) ≥ φ(π ). In the outer loop steps of our algorithm, we need to ﬁnd the minimizer
z ∈ Rq
+ ; the inner loop consists of minimizing the upper bound w.r.t. π for ﬁxed z . Introducing
γ := π−1 , we ﬁnd that (u , γ ) 7→ φz (u , γ −1 ) is jointly convex, which follows just as in [16], and
because zT (γ −1 ) is convex (all zj ≥ 0). Minimizing over γ gives the convex problem
qX
√
σ−2 ky − Xu k2 + 2
j=1
which is of standard form and can be solved very efﬁciently by the iteratively reweighted least
squares (IRLS) algorithm, a special case of Newton-Raphson. In every iteration, we have to solve
(X T X + B T (diag e )B )d = r , where r , e are simple functions of u . We use the linear conjugate
gradients (LCG) algorithm [4], requiring a MVM with X , X T , B , and B T per iteration. The
line search along the Newton direction d can be done in O(q), no further MVMs are required.
In our experiments, IRLS converged rapidly. At convergence, π 0
j = τj (p0
j )−1/2 , p0 = p0 (u0 ).
For updating z → z 0 given π , note that πT z 0 − g(π ) = g∗ (z 0 ) = min ˜π ˜π T z 0 − g( ˜π ), so that
z 0 = diag−1 (cid:0)BA−1B T (cid:1) = σ−2 (VarQ [sj | y ]).
0 = ∇π πT z 0 − g(π ) = z 0 − ∇π g(π ), and
(5)
z 0 cannot be computed by a few LCG runs. Since A has no sparse graphical structure, we cannot
use belief propagation either. However, the Lanczos algorithm can be used to estimate z 0 [10].
This algorithm is also essential for scoring many candidates in each design step of our method (see
Section 3.1).
Our algorithm iterates between updates of z (outer loop steps) and inner loop convex optimization
of (u , π ). We show in [13] that minπ φ(π ) is a convex problem, whenever all model sites are
log-concave (as is the case for Laplacians), a ﬁnding which is novel to the best of our knowledge.

pj := zj + σ−2 s2
j , s = Bu ,

min
u

τj

pj ,

4The end result is valid for singular BT ΠB , by a continuity argument.

3

Once converged to the global optimum of φ(π ), the posterior is approximated by Q(·|y ) of (3),
whose mean is given by u . The main idea is to decouple φ(π ) by upper bounding the critical
term log |A |. If the z updates are done exactly, the algorithm is globally convergent [16]. Our
algorithm is inspired by [16], where a different problem is addressed. Their method produces very
sparse solutions of Xu ≈ y , while our focus is on close approximate inference, especially w.r.t.
the posterior covariance matrix. It was found in [12] that aggressive sparsiﬁcation, notwithstanding
being computationally convenient, hurts experimental design (and even reconstruction) for natural
images. Their update of z requires (5) as well, but can be done more cheaply, since most πj = +∞,
and A can be replaced by a much smaller matrix. Finally, note that MAP estimation [8] is solving
(4) once for z = 0, so can be seen as special case of our method.

3.1 Lanczos Algorithm. Efﬁcient Design

The Lanczos algorithm [4] is typically used to ﬁnd extremal eigenvectors of large, positive deﬁnite
matrices A . Requiring an MVM with A in each iteration, it produces QT AQ = T ∈ Rk,k
after k iterations, where QT Q = I , T tridiagonal. Lanczos estimates of expressions linear in
Σ = A−1 are obtained by plugging in the low-rank approximation QT −1QT ≈ Σ [10]. In our
case, z (k) := diag−1 (BQT −1QT B T ) → z 0 , L(k) := log |T | → g(π ). We also use Lanczos
to compute entropy difference scores, approximating (2) by using Q(u |y ) instead of P (u |y ), and
Q0 (u |y ) ∝ Q(u |y )P (y∗ |u ) instead of P (u |y , y∗ ), with π 0 = π . The expectation over P (y∗ |y )
∆(X∗ ) ≈ − log |A | + log (cid:12)(cid:12)A + X T∗ X∗ (cid:12)(cid:12) = log (cid:12)(cid:12)I + X∗ΣX T∗
(cid:12)(cid:12) .
need not be done then, and
For nc candidates of d rows, computing scores would need d · nc LCG runs, which is not feasible.
Using the Lanczos approximation of Σ , we need k MVMs with X∗ for each candidate, then nc
Cholesky decompositions of min{k , d} × min{k , d} matrices. Both computations can readily be
parallelized, as is done in our implementation. Note that we can compute ∂∆(X∗ )/∂α for X∗ =
X∗ (α), if ∂X∗ /∂α is known, so that gradient-based score optimization can be used.
The basic recurrence of the Lanczos method is treacherously simple. The loss of orthogonality
in Q has to be countered, thus typical Lanczos codes are intricate. Q has to be maintained in
memory. The matrices A we encounter here, have an almost linearly decaying spectrum, so standard
Lanczos codes, designed for geometrically decaying spectra, have to be modiﬁed. Our A have no
close low rank approximations, and eigenvalues from both ends of the spectrum converge rapidly in
Lanczos. Therefore, our estimate z (k) is not very close to the true z 0 even for quite large k . However,
z (k) (cid:22) z 0 , since zk−1,j ≤ zk,j for all j . Since the sparsity penalty on sj in (4) is stronger for smaller
zj , underestimations from the Lanczos algorithm entail more sparsity (although still zk,j > 0). In
practice, a smaller k often leads to somewhat better results, besides running much faster. While the
global convergence proof for our algorithm hinges on exact updates of z , which cannot be done to
the best of our knowledge, the empirical success of Section 4 may be due to this observation, noting
that natural image statistics are typically more super-Gaussian than the Laplacian. In conclusion,
approximate inference requires the computation of marginal variances, which for general models
cannot be approximated closely with generic techniques. In the context of sparse linear models, it
seems to be sufﬁcient to estimate the dominating covariance eigendirections, for which the Lanczos
algorithm with a moderate number of steps can be used. More generally, the Lanczos method is a
powerful tool for approximate inference in Gaussian models, an insight which does not seem to be
widely known in machine learning.

4 Experiments

We start with some MRI terminology. An MR scanner acquires Fourier coefﬁcients Y (k ) at spatial
frequencies5 k (the 2d Fourier domain is called k-space), along smooth trajectories k (t) determined
by magnetic ﬁeld gradients g (t). The control ﬂow is called sequence. Its cost is determined by how
long it takes to obtain a complete image, depending on the number of trajectories and their shapes.
Gradient amplitude and slew rate constraints enforce smooth trajectories. In Cartesian sampling,
trajectories are parallel equispaced lines in k-space, so the FFT can be used for image reconstruc-
tion. Spiral sampling offers a better coverage of k-space for given gradient power, leading to faster
5Both k and spatial locations r are seen as ∈ R2 or ∈ C.

4

Figure 1: MR signal acquisition: r -space and k-space represen-
tation of the signal on a rectangular grid as well as the trajectory
obtained by means of magnetic ﬁeld gradients

acquisition. It is often used for dynamic studies, such as cardiac imaging and fMRI. A trajectory
k (t) leads to data y = Xk u , where Xk = [e−i2πr T
j k (t‘ ) ]‘j . We use gridding interpolation6 with a
Kaiser-Bessel kernel [1, ch. 13.2] to approximate the multiplication with X k , which would be too
expensive otherwise. As for other reconstruction methods, most of our running time is spent in the
gridding (MVMs with X , X T , and X∗ ).
For our experiments, we acquired
data on an equispaced grid.7
In
theory, the image u is real-valued;
in reality, due to resonance fre-
quency offsets, magnetic ﬁeld inho-
mogeneities, and eddy currents [1,
the reconstruction con-
ch. 13.4],
tains a phase ϕ(r ).
It
is com-
mon practice to discard ϕ after re-
construction. Short of modelling a
complex-valued u , we correct for
low-frequency phase contributions by
a cheap pre-measurement.8 Note
|utrue |, against which recon-
that
structions are judged below, is not al-
tered by this correction. From the
corrected raw data, we simulate all further measurements under different sequences using grid-
ding interpolation. While no noise is added to these measurements, there remain signiﬁcant high-
frequency erroneous phase contributions in utrue .
Interleaved outgoing Archimedian spirals employ trajectories k (t) ∝ θ(t)ei2π [θ(t)+θ0 ] , θ(0) = 0,
where the gradient g (t) ∝ dk/dt grows to maximum strength at the slew rate, then stays there [1,
ch. 17.6]. Sampling along an interleave respects the Nyquist limit. The number of revolutions Nr
and interleaves Nshot determine the radial spacing. The scan time is proportional to Nshot . In our
setup, Nr = 8, resulting in 3216 complex samples per interleave. For equispaced offset angles θ0 ,
the Nyquist spiral (respecting the limit radially) has Nshot = 16. Our goal is to design spiral se-
quences with smaller Nshot , reducing scan time by a factor 16/Nshot . We use the sequential method
described in Section 2, where {X∗ ∈ Rn×d} is a set of potential interleaves, d = 6432. The image
resolution is 256 × 256, so n = 65536. Since utrue is approximately real-valued, measurements at
k and −k are quite redundant, which is why we restrict9 ourselves to offset angles θ0 ∈ [0, π). We
score candidates (π/256)[0 : 255] in each round, comparing to equispaced placements jπ/Nshot ,
and to drawing θ0 uniformly at random. For the former, favoured by MRI practitioners right now,
the maximum k-space distance between samples is minimized, while the latter is aligned with com-
pressed sensing recommendations [8].
For a given sequence, we consider different image reconstructions: the posterior mode (convex MAP
estimation) [8], linear least squares (LS; linear conjugate gradients), and zero ﬁlling with density
compensation (ZFDC; based on Voronoi diagram) [1, ch. 13.2.4]. The latter requires a single MVM
with X T only, and is most commonly used in practice. We selected the τ scale parameters (there
are two of them, as in [12]) optimally for the Nyquist spiral Xnyq , and set σ2 to the variance of
Xnyq (utrue − |utrue |). We worked on two slices (8,12) and used 750 Lanczos iterations in our
method.10 We report L2 distances between reconstruction and true image |utrue |. Results are given
in Table 3, and some reconstructions (slice 8) are shown in Figure 2.

6NFFT: http://www-user.tu-chemnitz.de/˜potts/nfft/
7Field of view (FOV) 260mm (256 × 256 voxels, 1mm2 ), 16 brain slices with a turbo-spin sequence, 23
echoes per excitation. Train of 120◦ refocusing pulses, each phase encoded differently. Slices are 4mm thick.
8We sample the center of k-space on a p × p Cartesian grid, obtaining a low-resolution reconstruction
by FFT, whose phase ˜ϕ we use to correct the raw data. We tried p ∈ {16, 32, 64} (larger p means better
correction), results below are for p = 32 only. While reconstruction errors generally decrease somewhat with
larger p, the relative differences between all settings below are insensitive to p.
9Dropping this restriction disfavours equispaced {θ0 } setups with even Nshot .
10This seems small, given that n = 65536. We also tried 1250 iterations, which needed more memory, ran
almost twice as long, and gave slightly worse results (see end of Section 3.1).

5

r−space: U(r)1nn1k−space: Y(k)−1/201/21/20−1/20246−50050gradients: g(t)gx in [mT/m]0246−50050t in [ms]gy in [mT/m]Figure 2: Reconstruction results. Differences to true image (a; scale [0, 1]) in (b-f), scale [−0.1, 0.1].

Nshot img MAPop MAPrd MAPeq LSop
ZFDCeq
ZFDCrd
LSeq ZFDCop
LSrd
12.99 16.01 ± 2.49 14.18 17.23 19.97 ± 1.33 16.80 25.13 38.04 ± 6.14 23.51
8
5
8.31 12.46 ± 2.46 10.06 12.67 16.24 ± 1.13 13.19 18.79 33.29 ± 4.71 18.16
8
6
3.95 11.81 ± 2.71 4.40
7.80 13.71 ± 2.25 7.80
14.55 33.67 ± 5.90 12.73
8
7
2.94 6.86 ± 2.00 2.84
3.77 7.43 ± 2.48 3.31
13.08 26.96 ± 4.47 6.20
8
8
8.01 10.17 ± 1.63 9.32 12.77 14.95 ± 1.08 12.01 20.58 28.88 ± 4.25 19.74
12
5
4.94 7.74 ± 1.75 5.21
9.77 11.89 ± 0.95 9.77
16.33 25.47 ± 3.15 15.36
6
12
12.34 26.02 ± 3.44 10.62
6.40 9.95 ± 1.73 6.18
2.84 7.46 ± 1.80 3.18
12
7
2.20 4.60 ± 1.26 2.09
3.32 5.33 ± 1.73 2.27
10.07 21.47 ± 3.67 4.28
8
12

slices 2,4,6,10,12,14 from design of slice 8
LSop
Nshot MAPop
LSeq
MAPeq
14.70 ± 1.6 14.57 ± 2.1
10.67 ± 2.1
9.01 ± 1.3
5
10.80 ± 1.5 10.95 ± 1.8
6.51 ± 2.1
5.43 ± 1.1
6
3.00 ± 0.5
3.27 ± 0.8
7.08 ± 1.1
6.45 ± 1.4
7
3.16 ± 0.6
2.42 ± 0.3
2.34 ± 0.3
2.70 ± 0.6
8
img MAPeq , Nshot = 16, (Nyq)
LSeq , Nshot = 16, (Nyq)
3.31
2.75
8
12
1.96
2.27

Figure 3: Results for spiral interleaves on slices 8, 12 (table left). Reconstruction: MAP (posterior mode [8]),
LS (least squares), ZFDC (zero ﬁlling, density compensation). Offset angles θ0 ∈ [0, π): op (optimized; our
method), rd (uniformly random; avg. 10 runs), eq (equispaced). Nshot : Number of interleaves.
Table upper right: Avg. errors for slices 2,4,6,10,14, measured with sequences optimized on slice 8.
Table lower right: Results for Nyquist spiral eq[Nshot = 16].

The standard reconstruction method ZFDC is improved upon strongly by LS (both are linear, but LS
is iterative), which in turn is improved upon signiﬁcantly by MAP. This is true even for the Nyquist
spiral (Nshot = 16). While the strongest errors of ZFDC lie outside the “effective ﬁeld of view”
(roughly circular for spiral), panel f of Figure 2 shows that ZFDC errors contain important structures
all over the image. Modern implementations of LS and MAP are more expensive than ZFDC by
moderate constant factors. Results such as ours, together with the availability of affordable high-
performance digital computation, strongly motivate the transition away from direct signal processing
reconstruction algorithms to modern iterative statistical estimators. Note that ZFDC (and, to a lesser
extent, LS) copes best with equispaced designs, while MAP works best with optimized angles. This
is because the optimized designs leave larger gaps in k-space (see Figure 4). Nonlinear estimators
can interpolate across such gaps to some extent, using image sparsity priors. Methods like ZFDC
merely interpolate locally in k-space, uninformed about image statistics, so that violations of the
Nyquist limit anywhere necessarily translate into errors.
It is clearly evident that drawing the spiral offset angles at random does not work well, even if
MAP reconstruction is used as in [8]. The ratio MAPrd /MAPop in L2 error is 1.23, 1.45, 2.99,
2.33 in Table 3, upper left. While both MAPop and MAPeq essentially attain Nyquist performance
with Nshot = 8, MAPrd does not decrease to that level even with Nshot = 16 (not shown). Our

6

(a)   Slice8(b)   MAP−op, Nshot=7, E=3.95(c)   MAP−eq, Nshot=7, E=4.40(d)   MAP−rd, Nshot=7, E=12.08(e)   MAP−eq, Nshot=8, E=2.84(f)   ZFDC−eq, Nshot=8, E=6.20results strongly suggest that randomizing MR sequences is not a useful design principle.11 Similar
shortcomings of randomly drawn designs were reported in [12], in a more idealized setup. Reasons
why CS theory as yet fails to guide measurement design for real images, are reviewed there, see
also [15]. Beyond the rather bad average performance of random designs, the large variance across
trials in Table 3 means that in practice, a randomized sequence scan is much like a gamble. The
outcome of our Bayesian optimized design is stable, in that sequences found in several repetitions
gave almost identical reconstruction performance.
The closest competitors in Table 3 are MAPop
and MAPeq . Since utrue is close to real, both
attain close to Nyquist performance up from
Nshot = 8. In the true undersampling regime
Nshot ∈ {5, 6, 7}, MAPop improves signiﬁ-
cantly12 upon MAPeq . Comparing panels b,c
of Figure 2, the artifact across the lower right
leads to distortions in the mouth area. Under-
sampling artifacts are generally ampliﬁed by
regular sampling, which is avoided in the op-
timized designs. Breaking up such regular de-
signs seems to be the major role of random-
ization in CS theory, but our results show that
much is lost in the process. We see that approx-
imate Bayesian experimental design is useful to
optimize measurement architectures for subsequent MAP reconstruction. To our knowledge, no
similar design optimization method based purely on MAP estimation has been proposed (ours needs
approximate inference), rendering the beneﬁcial interplay between our framework and subsequent
MAP estimation all the more interesting. The computational primitives required for MAP estima-
tion and our method are the same. Our implementation requires about 5 hours on a single standard
desktop machine to optimize 11 angles sequentially, 256 candidates per extension, with n and d as
above. The score computations dominate the running time, but can readily be parallelized.
It is neither feasible nor desirable on most current MR scanners to optimize the sequence during the
measurement, so an important question is whether sequences optimized on some slices work better
in general as well (for the same contrast and similar objects). We tested transferability by measuring
ﬁve other slices not seen by the optimization method. The results (Table 3, upper right) indicate
that the main improvements are not speciﬁc to the object the sequence was optimized for.13 Two
spirals found by our method are shown in Figure 4 (2 of 8 interleaves, Nshot = 8). The spacing
is not equidistant, and as noted above, only nonlinear MAP estimation can successfully interpolate
across resulting larger k-space gaps. On the other hand, the spacing is more regular than is typically
achieved by random sampling.

Figure 4: Spirals found by our algorithm. The ordering
is color-coded: dark spirals selected ﬁrst.

5 Discussion

We have presented the ﬁrst scalable Bayesian experimental design framework for automatically
optimizing MRI sequences, a problem of high impact on clinical diagnostics and brain research. The
high demands on image resolution and processing time which come with this application are met in
principle by our novel variational inference algorithm, reducing computations to signal processing

11 Images exhibit a decay in power as function of spatial frequence (distance to k-space origin), and the most
evident failure of uniform random sampling is the ignorance of this fact [15]. While this point is noted in [8],
the variable-density weighting suggested there is built in to all designs compared here. Any spiral interleave
samples more closely around the origin. In fact, the sampling density as a function of spatial frequency |k (t)|
does not depend on the offset angles θ0 .
12 In another set of experiments (not shown), we compared optimization, randomization, and equispacing of
θ0 ∈ [0, 2π), in disregard of the approximate real-valuedness of utrue . In this setting, equispacing performs
poorly (worse than randomization).
13However, it is important that the object exhibits realistic natural image statistics. Artiﬁcial phantoms of
extremely simple structure, often used in MR sequence design, are not suitable in that respect. Real MR images
are much more complicated than simple phantoms, even in low level statistics, and results obtained on phantoms
only should not be given overly high attendance.

7

−0.0300.03−0.0300.03Slice 8, Nshot=8−0.0300.03−0.0300.03Slice 12, Nshot=8primitives such as FFT and gridding. We demonstrated the power of our approach in a study with
spiral sequences, using raw data from a 3T MR scanner. The sequences found by our method lead to
reconstructions of high quality, even though they are faster than traditionally used Nyquist setups by
a factor up to two. They improve strongly on sequences obtained by blind randomization. Moreover,
across all designs, nonlinear Bayesian MAP estimation was found to be essential for reconstructions
from undersamplings, and our design optimization framework is especially useful for subsequent
MAP reconstruction.
Our results strongly suggest that modiﬁcations to standard sequences can be found which produce
similar images at lower cost. Namely, with so many handles to turn in sequence design nowadays,
this is a high-dimensional optimization problem dealing with signals (images) of high complexity,
and human experts can greatly beneﬁt from goal-directed machine exploration. Randomizing param-
eters of a sequence, as suggested by compressed sensing theory, helps to break wasteful symmetries
in regular standard sequences. As our results show, many of the advantages of regular sequences
are lost by randomization though. The optimization of Bayesian information leads to irregular se-
quences as well, improving on regular, and especially on randomized designs. Our insights should
be especially valuable in MR applications where a high temporal resolution is essential (such as
fMRI studies), so that dense spatial sampling is not even an option. An extension to 3d volume
reconstruction, making use of non-Gaussian hidden Markov models, is work in progress. Finally,
our framework seems also promising for real-time imaging [1, ch. 11.4], where the scanner allows
for on-line adaptations of the sequence depending on measurement feedback. It could be used to
help an operator homing in on regions of interest, or could even run without human intervention.
We intend to test our proposal directly on an MR scanner, using the sequential setup described in
Section 2. This will come with new problems not addressed in Section 4, such as phase or image
errors that depend on the sequence employed14 (which could be accounted for by a more elaborate
noise model). In our experiments in Section 4, the choice of different offset angles is cost-neutral,
but when a larger set of candidates is used, respective costs have to be quantiﬁed in terms of real
scan time, error-proneness, heating due to rapid gradient switching, and other factors.

Acknowledgments

We thank Stefan Kunis for help and support with NFFT.

References
[1] M.A. Bernstein, K.F. King, and X.J. Zhou. Handbook of MRI Pulse Sequences. Elsevier Academic Press,
1st edition, 2004.
[2] A. Garroway, P. Grannell, and P. Mansﬁeld. Image formation in NMR by a selective irradiative pulse. J.
Phys. C: Solid State Phys., 7:L457–L462, 1974.
[3] M. Girolami. A variational method for learning sparse and overcomplete representations. N. Comp.,
13:2517–2532, 2001.
[4] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press, 3rd edition, 1996.
[5] A. Haase, J. Frahm, D. Matthaei, W. H ¨anicke, and K. Merboldt. FLASH imaging: Rapid NMR imaging
using low ﬂip-angle pulses. J. Magn. Reson., 67:258–266, 1986.
[6] J. Hennig, A. Nauerth, and H. Friedburg. RARE imaging: A fast imaging method for clinical MR. Magn.
Reson. Med., 3(6):823–833, 1986.
[7] P. Lauterbur.
Image formation by induced local interactions: Examples employing nuclear magnetic
resonance. Nature, 242:190–191, 1973.
[8] M. Lustig, D. Donoho, and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR
imaging. Magn. Reson. Med., 85(6):1182–1195, 2007.
[9] P. Mansﬁeld. Multi-planar image formation using NMR spin-echoes. J. Phys. C, 10:L50–L58, 1977.
[10] M. Schneider and A. Willsky. Krylov subspace estimation. SIAM J. Comp., 22(5):1840–1864, 2001.
[11] M. Seeger. Bayesian inference and optimal design for the sparse linear model. JMLR, 9:759–813, 2008.
[12] M. Seeger and H. Nickisch. Compressed sensing and Bayesian experimental design. In ICML 25, 2008.
[13] M. Seeger and H. Nickisch. Large scale variational inference and experimental design for sparse general-
ized linear models. Technical Report TR-175, Max Planck Institute for Biological Cybernetics, T ¨ubingen,
Germany, September 2008.
[14] M. Tipping and A. Faul. Fast marginal likelihood maximisation for sparse Bayesian models. In AI and
Statistics 9, 2003.
[15] Y. Weiss, H. Chang, and W. Freeman. Learning compressed sensing. Snowbird Learning Workshop,
Allerton, CA, 2007.
[16] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In NIPS 20, 2008.

14Some common problems with spirals are discussed in [1, ch. 17.6.3], together with remedies.

8


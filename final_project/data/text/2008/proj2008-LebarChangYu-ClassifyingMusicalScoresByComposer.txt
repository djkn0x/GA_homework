Classifying Musical Scores by Composer:
A machine learning approach

David Yu
Gary Chang
Justin Lebar
{jlebar, gwchang, yud}@stanford.edu

November 14, 2008

Abstract
We apply various machine learning algorithms to classify
musical scores in the **kern format by their composer.
Our algorithms were able to distinguish some composers
well, but had diﬃculty distinguishing between composers
from similar time periods. We conclude that categoriza-
tion problems among certain sets of composers likely have
a higher inherent error rate than others.

1 Introduction
Understanding the features that demarcate musical gen-
res and distinguish the works of various composers is im-
portant for a number of applications, including organizing
musical databases and building music recommendation
engines. Previous work has shown that classifying audio
recordings of music is a diﬃcult machine learning prob-
lem, not in the least because one must employ sophisti-
cated audio processing techniques to extract features from
a noisy audio waveform (see examples of such complexity
at [1]).
We avoided the challenges of audio processing by taking
a diﬀerent approach: Instead of analyzing audio ﬁles, we
used musical scores in the plain-text **kern format (full
speciﬁcation available at [2]).
Classifying music by its waveform remains the primal
task in this ﬁeld, but we hope that our analysis of fea-
tures and algorithms for classifying scores may suggest
which techniques for analyzing audio might be more ef-
fective and may hint at the inherent error associated with
distinguishing between certain composers.

2 Process
We used the The Humdrum Pro ject’s library of **kern-
encoded classical scores, available at http://kern.
humdrum.org, as our source of training and testing data.
**kern scores are essentially a lossless representation of
the original printed music and include information about
articulations, dynamics, and even note stem directions
in addition to the notes themselves. For simplicity, our
model ignores everything except pitches and their dura-
tions.
Each line in a **kern ﬁle lists one or more notes
which begin simultaneously. We call each of these lines a
“chord”, and they form the basic token for our analysis of

scores.
We run each score through a **kern parser written
in Java before applying our machine learning algorithms
to the scores. The parser transposes the score from its
original key to the key of C ma jor and generates features
from each score’s chords.

3 Data
Figure 1 lists the composers we analyzed. For each com-
poser except Beethoven, we were only able to access sig-
niﬁcant numbers of pieces of one type—either pieces for
a keyboard instrument or pieces for string quartets. To
maintain this symmetry, we consider Beethoven’s string
quartets and piano sonatas as being written by two dif-
ferent composers. For pieces with multiple movements,
we analyzed each movement as a separate piece.

Composer
Key
Bach, J. S.
JSB
SCA
Scarlatti, D.
HDN Haydn
MOZ Mozart
LVBp Beethoven (kbd)
LVBq Beethoven (sqt)
CHO Chopin
JOP
Joplin, Scott

Lived
1685-1750
1685-1757
1732-1809
1756-1791
1770-1827
1770-1827
1810-1849
1867-1917

Type
kbd
kbd
sqt
sqt
kbd
sqt
kbd
kbd

n
111
58
212
82
78
70
88
46

Figure 1: Composers analyzed. Type is either “kbd” for key-
board music or “sqt” for string quartets. n is the number of
pieces by each composer in our training set.

4 Methodologies

4.1 Naive Bayes
Naive Bayes showed little success when we considered two
chords equal only if they contained the same pitches for
the same durations, but we improved upon this result
by relaxing the conditions under which two chords were
considered equal. The best equality function we tested
is pitch-count. Two chords are equal under pitch-count
if they both contain the same number of notes of each
pitch, ignoring octave and duration. That is, two chords
each containing two Cs and a D in some octave have the
same pitch-count, but a chord containing one C and two
Ds has a diﬀerent pitch-count than the other two.

1

Requiring that two chords have similar rhythms in or-
der to be considered equal decreased the eﬃcacy of our
classiﬁer dramatically. If we ignored the number of times
a note appeared in a chord and considered a chord con-
taining three Cs and a D to be equal to a chord containing
one C and four Ds, our Naive Bayes implementation per-
formed with only slightly less accuracy than pitch-count.
If we created n-chord tokens by grouping together the
ﬁrst n chords into one token, chords 2 through n + 1 into
a second token, etc., our model’s train error fell to nearly
zero for all composers and the testing error increased,
even for n = 2. This indicates that even 2-chord tokens
cause our algorithm to overﬁt the data.

4.2 Support Vector Machines
Our second approach to the problem was to apply a stan-
dard Support Vector Machine using the same features we
had extracted for Naive Bayes. To this end, we used the
libSVM library, available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm/.
The accuracy of our SVM varied greatly depending on
the kernel we used and the parameters we passed to the
SVM. We tried three kernels:

1. linear: k(u, v) = uT v

2. sigmoid: k(u, v) = tanh(γuT v + c0 )

3. Gaussian radial basis function:
k(u, v) = e−γ |u−v |2

and found that the last signiﬁcantly outperformed the
ﬁrst two. As a result, we spent most of our eﬀorts tuning
and trying diﬀerent features for the Gaussian radial basis
function kernel.

4.3 Linear and Quadratic Discriminant
Analysis
We performed LDA classiﬁcation using the same pitch-
count features as Naive Bayes and SVM since it was
shown it give better accuracy. LDA assumes that the
input features are distributed according to a multivari-
ate Gaussian; if this assumption holds, then LDA should
require fewer training samples to reach performance sim-
ilar to NB and SVM. Although we didn’t observe that
our data was closely distributed according to a multivari-
ate Gaussian, we hoped that our LDA might nevertheless
perform well given a limited training set. To maintain
symmetry with our other algorithms, we trained on 360
samples overall (45 per composer), and thus we rank-
reduced our data by selecting the 360 largest principal
components and performed LDA on those.
Supposing that the linearity of the classiﬁers in LDA
might be a limitation to its performance, we also per-
formed quadratic discriminant analysis on our data, using

the same features as LDA. To maintain the nonsingular-
ity of the class-speciﬁc covariance matrices, we classiﬁed
on the 45 largest principal components.

4.4 K-Nearest Neighbors
Our implementations of Naive Bayes, SVM, LDA, and
QDA include no metric for measuring the similarity of two
tokens aside from strict equality. As a result, we could
not train on full chords—instead, we reduced chords to
their pitch-count and learned the frequency that a com-
poser wrote chords with various pitch-count values. This
reduction throws out a great deal of information—in par-
ticular, it ignores the notes’ octaves and gives long and
short notes equal weight. We devised a scheme for classi-
fying scores using nearest-neighbor techniques which at-
tempts to overcome these deﬁciencies.
Our algorithm works as follows: For every measure m
in every score, we create the set Nm containing the k
measures in other pieces which are most similar to m,
as measured by some distance function d. To classify a
score S , we compute for each measure m ∈ S a function
C (m, Nm ) of m’s neighbors which classiﬁes m as being
most similar to one composer. We then take a ma jority
vote of all the measures to classify S .
We tried a number of diﬀerent parameters to this al-
gorithm. In the end, we represented each measure as a
vector where each element of the vector was a weighted
sum of the notes sounding at a given pitch in that mea-
sure. Longer notes received greater weight, in proportion
to their length. We found that taking octaves into ac-
count did improve the accuracy of our algorithm, as we’d
hoped.
We found that the algorithm was not very sensitive to
(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13) x
the distance function used, but we had best performance
when using
(cid:107)x(cid:107) − y
d(x, y) =
(cid:107)y(cid:107)
as opposed to d(x, y) = (cid:107)x−y(cid:107) or d(x, y) = (cid:107)x−y(cid:107)/((cid:107)x(cid:107)+
(cid:107)y(cid:107)). Our algorithm was somewhat sensitive to our choice
of C (m, Nm ), the function mapping a measure m and
its neighbors Nm to a composer. The KNN algorithm
(cid:19)
(cid:18)
(cid:88)
performed best overall for C (m, Nm ) deﬁned as
1
1{c composed n} exp
d(n, m)2
n∈Nm
This choice of C decreases very quickly as d(m, n) in-
creases, suggesting that our KNN classiﬁcation performs
best when C (m, Nm ) outputs the single nearest neighbor
of m, using a ma jority vote when m has many neighbors
at approximately the same distance.

arg max
composer c

.

5 Results
We see the following consistencies across classiﬁers: First,
all classiﬁcation methods had diﬃculty distinguishing be-

2

Figure 2: Test error for the four main algorithms used.
In each case, we trained on 45 scores from each composer
and tested on the remaining scores. For NB, SVM, and
LDA, we used the pitch-count feature. For KNN, we used
the measure-count feature. See Figure 1 for the full names
of the composers listed.

Figure 3: Confusion plot for Naive Bayes classiﬁcation.
The small bars represent the percentage of a composer’s
works which were misclassiﬁed as the composer listed on
the vertical axis. The large bars represent the sum of the
small bars they contain.

tween composers belonging to the same musical period.
For example, in the SVM confusion graph (Figure 4),
we see that most pieces misclassiﬁed as Bach were by
Scarlatti and vice versa. This is unsurprising, since Bach
and Scarlatti were contemporaries and both wrote in the
Baroque style. We see the same pattern can be across
Classical (Haydn, Mozart, and Beethoven) and Roman-
tic (Beethoven piano and Chopin) composers in all our
confusion plots.
Second, all classiﬁers were able to distinguish Joplin’s
works to a high degree of accuracy. This is consistent with
the fact that Joplin’s ragtime style that was a distinct
departure from the European classical traditions.

5.1 Naive Bayes
We initially conducted our Naive Bayes modeling using
70% of each composer’s scores for training and the re-
maining for cross-validation testing, but we found that
this method biased the model towards composers with
more scores. To remove this bias, we trained on a constant
number of scores from each composer (45) and tested on
the remaining scores; doing so, we obtained much more
balanced results. We used this cross-validation scheme
for all of our other classiﬁers as well.

5.2 Support Vector Machine
Using the same training and cross validation scheme
as Naive Bayes, we tried features including pitch-count,
note-count (which reports only the number notes in a
chord and ignores the notes’ pitches) and pitch-present
(which is similar to pitch-count, except that for each

pitch, the feature only reports a 1 if it appeared once
or more, or a 0 if it never appeared). For each set
of features, we performed a grid search to approximate
the optimal values for the SVM parameters: We mea-
sured the performance of the classiﬁer for values of the
slack constant C ∈ {2−5 , 2−4 , ..., 215} and the radial basis
function parameter γ ∈ {2−15 , 2−14 , ..., 23} by perform-
ing 10-fold cross validation on the training set. We found
that the pitch-count feature set yielded the highest cross-
validation accuracy of 67.5%, followed by pitch-present at
63.9% and note-count at 60.6%. It is interesting to note
that pitch-count was also the feature set that yielded the
highest accuracy in the Naive Bayes classiﬁer. However,
we still achieved a reasonably high level of accuracy when
given only boolean values for each pitch (in the case of
pitch-present ) or when given only the number of notes in
each chord (in the case of note-count ).
Our last step was to perform a ﬁne-grained search for
C and γ around the local maximum found previously.
With these parameters and the pitch-count feature, our
cross-validation accuracy increased from 67.5% to 68.3%.
Our overall accuracy on the test set using these optimal
parameters is shown in Figure 2, and the confusion matrix
is plotted in Figure 4.

5.3 K-Nearest Neighbors
Even with a high degree of tuning, our KNN algorithm
performed on par with LDA and worse NB and SVM. We
had hoped that the larger features we used would allow
us to extract more information from the scores, but this
turned out not to be the case. We suspect that the criti-
cal piece of information lost in the measure-count feature

3

JSBSCAHDNMOZLVBpLVBqCHOJOP020406080100ComposerError (%)  NBSVMLDAKNN0102030405060708090100JOPCHOLVBqLVBpMOZHDNSCAJSBError (%)Composer  JOPCHOLVBqLVBpMOZHDNSCAJSBFigure 4: Confusion plot for SVM classiﬁcation.

Figure 6: Confusion plot for LDA classiﬁcation.

Meanwhile, QDA had an error rate upwards of 75%.
QDA requires signiﬁcantly more samples than LDA to
classify data of the same dimension, so we had to run
QDA on a lower-dimensional vector space than LDA, es-
sentially giving QDA less training data. Furthermore, the
fact that our data is not multivariate Gaussian (see Figure
8) made errors even more pronounced in QDA.

6 Analysis
In order to understand how well our classiﬁcation meth-
ods have performed, we need to estimate the Bayes er-
ror that is inherent to our classiﬁcation problem. The
Bayes classiﬁer is ambiguous when there is overlap in the
posterior probability distributions of two or more distinct
classes. Therefore, we can estimate the Bayes error by ap-
proximating the degree of overlap between the posterior
distributions. We do this through PCA and a historical
analysis.

6.1 Principal Component Analysis
Our ﬁrst approach to estimating the degree of overlap
between features of distinct classes was to create a 2-
dimensional visualization of the data. This was done by
mapping the pitch-count features to their ﬁrst two prin-
cipal components. We observed that composers belong-
ing to the same musical period exhibit signiﬁcant overlap
in the ﬁrst two principal components (see 8). On the
other hand, composers belonging to distinct musical pe-
riod show a high degree of separability (see 7). These
observations help explain the patterns in confusion errors
discussed above.

6.2 Historical Analysis
The degree of similarity in the music of distinct com-
posers can also be traced historically. Haydn and Mozart

Figure 5: Confusion plot for KNN classiﬁcation.

as compared to pitch-count is the number of notes being
played at once. This information is likely key to success-
ful diﬀerentiation between string quartets and keyboard
pieces, something which our KNN algorithm had diﬃculty
doing (see Figure 5).

5.4 Linear and Quadratic Discriminant
Analysis
We achieved training error of 0.8% and cross-validation
accuracy of 51.3% using linear discriminant analysis.
While this seems substantially lower than SVM, we see
from Figure 2 that LDA is quite comparable to SVM for
all composers except for the Romantics (Beethoven and
Chopin). The diﬃculty LDA had classifying the Roman-
tics is likely due to the linearity of its decision boundary
or and the fact that our data is not distributed according
to a multivariate Gaussian.

4

0102030405060708090100JOPCHOLVBqLVBpMOZHDNSCAJSBError (%)Composer  JOPCHOLVBqLVBpMOZHDNSCAJSB0102030405060708090100JOPCHOLVBqLVBpMOZHDNSCAJSBError (%)Composer  JOPCHOLVBqLVBpMOZHDNSCAJSB0102030405060708090100JOPCHOLVBqLVBpMOZHDNSCAJSBError (%)Composer  JOPCHOLVBqLVBpMOZHDNSCAJSBFigure 7: PCA of Romantic vs. Ragtime composers.

Figure 8: PCA of Classical composers.

were both in Vienna from 1781–1784 and were admirers
of each other’s works. Haydn’s Opus 20 string quartets
are believed to have been inspired by Mozart’s K168–
173, while Mozart’s K387, 421, 428, 458, 464, and 465
are widely known as the “Haydn quartets” and were in-
spired by Haydn’s Opus 33 [4]. Similarly, Beethoven was
Haydn’s pupil in Vienna from 1792–1795 and Chopin’s
late contemporary. Beethoven’s later works (after 1815)
are widely considered to be the beginning of the Romantic
period, a canon to which Chopin belonged.

The similarities in Haydn and Mozart string quartets
have been quantitatively accessed by Carig Sapp and Yi-
Wen Liu of Stanford University. In an online quiz that
asks listeners to distinguish between movements from
Mozart and Haydn string quartets, the accuracy rates
ranged from 52 to 66% for self-identiﬁed novices and
experts respectively [5].
In light of the diﬃculty even
experts have with this classiﬁcation problem, our algo-
rithms’ diﬃculty here is not surprising.

7 Conclusions

We see that SVM is consistently the more accurate clas-
siﬁer across all composers we analyzed. This is likely
because SVM makes no assumptions on the probabilis-
tic distribution of features, allows for nonlinear deci-
sion boundaries, and can train on a sparse yet high-
dimensional data. These properties make the SVM the
superior classiﬁer for this particular problem. Although
our error rates for classifying some composers were large,
errors we saw are in line with what we’d expect from a
historical analysis, PCA, and human trials.

8 Further work
In this paper, we laid a framework for applying machine
learning techniques to **kern scores. Our analysis here
has been context-free: Our NB, SVM, and LDA algo-
rithms consider only how many times a given chord ap-
pears in a score, and our KNN algorithm classiﬁes mea-
sures with no consideration given to the surrounding mea-
sures. Future work might involve attempting to add more
context to these models, either by adding context to the
features used, or by using a context-full model, such as
an HMM.

9 Acknowledgments
We’d like to thank Prof. Jonathan Berger for his valu-
able guidance at the beginning of this pro ject. We’d also
like to thank the Humdrum Pro ject and the Center for
Computer Assisted Research in the Humanities for mak-
ing their collections of **kern scores available online for
free.

References

[1] “MIREX 2008,” International Music Information Retrieval Sys-
tems Evaluation Laboratory, University of Illinois at Urbana-
Champaign, http://www.music- ir.org/mirex/2008/index.php

[2] “Everything You Need to Know About the Humdrum ‘**kern’ Rep-
resentation,” Ohio State University School of Music, http://dactyl.
som.ohio- state.edu/Humdrum/representations/kern.html

[3] Saunders, C., Hardoon, D. R., Shawe-Taylor, J. and Widmer, G.
(2004) Using String Kernels to Identify Famous Performers from
their Playing Style. In: The 15th European Conference on Machine
Learning (ECML) and the 8th European Conference on Principles
and Practice of Knowledge Discovery in Databases (PKDD), 20-24
September, Pisa, Italy.

[4] Rosen, Charles. “The Classical Style: Haydn, Mozart, Beethoven.”
W. W. Norton Company, 1998.

[5] Sapp, Craig and Yi-Wen Liu. “The Haydn Mozart String Quartet
Quiz.” Stanford University, http://qq.themefinder.org/.

5

−2−10123456−3−2−10123456  chopinjoplin0123456−10123456  beethoven/quartetmozarthaydn
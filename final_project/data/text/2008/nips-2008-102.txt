On the Generalization Ability of
Online Strongly Convex Programming Algorithms

Sham M. Kakade
TTI Chicago
Chicago, IL 60637
sham@tti-c.org

Ambuj Tewari
TTI Chicago
Chicago, IL 60637
tewari@tti-c.org

Abstract

This paper examines the generalization properties of online convex programming
algorithms when the loss function is Lipschitz and strongly convex. Our main
result is a sharp bound, that holds with high probability, on the excess risk of the
output of an online algorithm in terms of the average regret. This allows one to
use recent algorithms with logarithmic cumulative regret guarantees to achieve
fast convergence rates for the excess risk with high probability. As a corollary, we
characterize the convergence rate of P EGA SO S (with high probability), a recently
proposed method for solving the SVM optimization problem.

1

Introduction

Online regret minimizing algorithms provide some of the most successful algorithms for many ma-
chine learning problems, both in terms of the speed of optimization and the quality of generalization.
Notable examples include efﬁcient learning algorithms for
structured prediction [Collins, 2002] (an
algorithm now widely used) and for ranking problems [Crammer et al., 2006] (providing competitive
results with a fast implementation).

Online convex optimization is a sequential paradigm in which at each round, the learner predicts a
vector wt ∈ S ⊂ Rn , nature responds with a convex loss function, `t , and the learner suffers loss
`t (wt ). In this setting, the goal of the learner is to minimize the regret:
TXt=1
TXt=1
`t (wt ) − min
w∈S
which is the difference between his cumulative loss and the cumulative loss of the optimal ﬁxed
vector.

`t (w)

Typically, these algorithms are used to train a learning algorithm incrementally, by sequentially
feeding the algorithm a data sequence, (X1 , Y1 ), . . . , (XT , YT ) (generated in an i.i.d. manner). In
essence, the loss function used in the above paradigm at time t is `(w; (Xt , Yt )), and this leads to a
guaranteed bound on the regret:
TXt=1
TXt=1
RegT =
`(wt ; (Xt , Yt )) − min
`(w; (Xt , Yt ))
w∈S
However, in the batch setting, we are typically interested in ﬁnding a parameter bw with good gener-
alization ability, i.e. we would like:
R( bw) − min
R(w)
w∈S
to be small, where R(w) := E [`(w; (X, Y ))] is the risk.

Intuitively, it seems plausible that low regret on an i.i.d. sequence, should imply good generaliza-
tion performance. In fact, for most of the empirically successful online algorithms, we have a set of
techniques to understand the generalization performance of these algorithms on new data via ‘online
to batch’ conversions — the conversions relate the regret of
the algorithm (on past data) to the gen-
eralization performance (on future data). These include cases which are tailored to general convex
functions [Cesa-Bianchi et al., 2004] (whose regret is O(√T )) and mistake bound settings [Cesa-
Bianchi and Gentile, 2008] (where the the regret could be O(1) under separability assumptions).
In these conversions, we typically choose bw to be the average of the wt produced by our online
algorithm.
Recently, there has been a growing body of work providing online algorithms for strongly convex
loss functions (i.e. `t is strongly convex), with regret guarantees that are merely O(ln T ). Such
algorithms have the potential to be highly applicable since many machine learning optimization
problems are in fact strongly convex — either with strongly c onvex loss functions (e.g. log loss,
square loss) or, indirectly, via strongly convex regularizers (e.g. L2 or K L based regularization).
Note that in the latter case, the loss function itself may only be just convex but a strongly convex reg-
ularizer effectively makes this a strongly convex optimization problem; e.g. the SVM optimization
problem uses the hinge loss with L2 regularization. In fact, for this case, the P EGA SO S algorithm
of Shalev-Shwartz et al. [2007] — based on the online strongl
y convex programming algorithm of
Hazan et al. [2006] — is a state-of-the-art SVM solver. Also, Ratliff et al. [2007] provide a similar
subgradient method for max-margin based structured prediction, which also has favorable empirical
performance.

The aim of this paper is to examine the generalization properties of online convex programming
algorithms when the loss function is strongly convex (where strong convexity can be deﬁned in a
general sense, with respect to some arbitrary norm || · ||). Suppose we have an online algorithm
which has some guaranteed cumulative regret bound RegT (e.g. say RegT ≤ ln T with T samples).
Then a corollary of our main result shows that with probability greater than 1 − δ ln T , we obtain a
parameter bw from our online algorithm such that:
qRegT ln 1
+ O 
T  .
ln 1
RegT
δ
δ
R( bw) − min
R(w) ≤
T
T
w
Here, the constants hidden in the O-notation are determined by the Lipschitz constant and the strong
convexity parameter of the loss `. Importantly, note that the correction term is of lower order than
√ln T
the regret — if the regret is
ln T then the additional penalty is O(
). If one naively uses the
T
Hoeffding-Azuma methods in Cesa-Bianchi et al. [2004], one would obtain a signi ﬁcantly worse
penalty of O(1/√T ).
This result solves an open problem in Shalev-Shwartz et al. [2007], which was on characterizing the
convergence rate of the P EGA SO S algorithm, with high probability. P EGA SO S is an online strongly
convex programming algorithm for the SVM objective function — it repeatedly (and randomly)
subsamples the training set in order to minimize the empirical SVM objective function. A corollary
to this work essentially shows the convergence rate of P EGA SO S (as a randomized optimization
algorithm) is concentrated rather sharply.

+

Ratliff et al. [2007] also provide an online algorithm (based on Hazan et al. [2006]) for max-margin
based structured prediction. Our results are also directly applicable in providing a sharper concen-
tration result in their setting (In particular, see the regret bound in Equation 15, for which our results
can be applied to).

This paper continues the line of research initiated by several researchers [Littlestone, 1989, Cesa-
Bianchi et al., 2004, Zhang, 2005, Cesa-Bianchi and Gentile, 2008] which looks at how to convert
online algorithms into batch algorithms with provable guarantees. Cesa-Bianchi and Gentile [2008]
prove faster rates in the case when the cumulative loss of the online algorithm is small. Here,
we are interested in the case where the cumulative regret is small. The work of Zhang [2005] is
closest to ours. Zhang [2005] explicitly goes via the exponential moment method to derive sharper
concentration results. In particular, for the regression problem with squared loss, Zhang [2005] gives
a result similar to ours (see Theorem 8 therein). The present work can also be seen as generalizing
his result to the case where we have strong convexity with respect to a general norm. Coupled with

recent advances in low regret algorithms in this setting, we are able to provide a result that holds
more generally.

Our key technical tool is a probabilistic inequality due to Freedman [Freedman, 1975]. This, com-
bined with a variance bound (Lemma 1) that follows from our assumptions about the loss function,
allows us to derive our main result (Theorem 2). We then apply it to statistical learning with bounded
loss, and to P EGA SO S in Section 4.

2 Setting

Fix a compact convex subset S of some space equipped with a norm k · k. Let k · k∗ be the dual norm
deﬁned by kvk∗ := supw : kwk≤1
v · w. Let Z be a random variable taking values in some space
Z . Our goal is to minimize F (w) := E [f (w; Z )] over w ∈ S . Here, f : S × Z → [0, B ] is some
function satisfying the following assumption.

Assumption LIST.
(LIpschitz and STrongly convex assumption) For all z ∈ Z , the function
fz (w) = f (w; z ) is convex in w and satis ﬁes:
1. fz has Lipschitz constant L w.r.t. to the norm k · k, i.e. ∀w ∈ S , ∀λ ∈ ∂ fz (w) (∂ fz denotes
the subdifferential of fz ), kλk∗ ≤ L. Note that this assumption implies ∀w, w0 ∈ S ,
|fz (w) − fz (w0 )| ≤ Lkw − w0 k.
2. fz is ν -strongly convex w.r.t. k · k, i.e. ∀θ ∈ [0, 1], ∀w, w0 ∈ S ,
ν
fz (θw + (1 − θ)w0 ) ≤ θfz (w) + (1 − θ)fz (w0 ) −
θ(1 − θ)kw − w0k2 .
2
Denote the minimizer of F by w? , w? := arg minw∈S F (w). We consider an online setting in
which independent (but not necessarily identically distributed) random variables Z1 , . . . , ZT be-
come available to us in that order. These have the property that
∀t, ∀w ∈ S, E [f (w; Zt )] = F (w) .
Now consider an algorithm that starts out with some w1 and at time t, having seen Zt , updates the
parameter wt to wt+1 . Let Et−1 [·] denote conditional expectation w.r.t. Z1 , . . . , Zt−1 . Note that
wt is measurable w.r.t. Z1 , . . . , Zt−1 and hence Et−1 [f (wt ; Zt )] = F (wt ).
Deﬁne the statistics,
TXt=1
TXt=1
f (w; Zt ) ,
TXt=1
TXt=1
(F (wt ) − F (w? )) =
Deﬁne the sequence of random variables
ξt := F (wt ) − F (w? ) − (f (wt ; Zt ) − f (w? ; Zt )) .
(1)
Since Et−1 [f (wt ; Zt )] = F (wt ) and Et−1 [f (w? ; Zt )] = F (w? ), ξt is a martingale difference
sequence. This deﬁnition needs some explanation as it is imp ortant to look at the right martingale
T Pt f (wt ; Zt )
difference sequence to derive the results we want. Even under assumption LIST, 1
T Pt f (w? ; Zt ) will not be concentrated around 1
T Pt F (wt ) and F (w? ) respectively at a
and 1
rate better then O(1/√T ) in general. But if we look at the difference, we are able to get sharper
concentration.

F (wt ) − T F (w? ) .

Diﬀ T :=

RegT :=

f (wt ; Zt ) − min
w∈S

3 A General Online to Batch Conversion

The following simple lemma is crucial for us. It says that under assumption LIST, the variance
of the increment in the regret f (wt ; Zt ) − f (w? ; Zt ) is bounded by its (conditional) expectation
F (wt ) − F (w? ). Such a control on the variance is often the main ingredient in obtaining sharper
concentration results.

Lemma 1. Suppose assumption LIST holds and let ξt be the martingale difference sequence deﬁned
in (1). Let
t (cid:3)
Vart−1 ξt := Et−1 (cid:2)ξ 2
be the conditional variance of ξt given Z1 , . . . , Zt−1 . Then, under assumption LIST, we have,
4L2
(F (wt ) − F (w? )) .
Vart−1 ξt ≤
ν
The variance bound given by the above lemma allows us to prove our main theorem.
Theorem 2. Under assumption LIST, we have, with probability at least 1 − 4 ln(T )δ ,
+ 4r L2 ln(1/δ)
pRegT
, 6B(cid:27) ln(1/δ)
+ max (cid:26) 16L2
TXt=1
1
RegT
F (wt ) − F (w? ) ≤
T
T
ν
T
ν
T
T Pt F (wt ) can be replaced by F ( ¯w) where ¯w := 1
T Pt
Further, using Jensen’s inequality, 1
wt .

3.1 Proofs

Proof of Lemma 1. We have,

Vart−1 ξt ≤ Et−1 h(f (wt ; Zt ) − f (w? ; Zt ))2 i
≤ Et−1 (cid:2)L2 kwt − w? k2 (cid:3)
[ Assumption LIST, part 1 ]
= L2 kwt − w? k2 .
On the other hand, using part 2 of assumption LIST, we also have for any w, w0 ∈ S ,
; Z(cid:19) +
≥ f (cid:18) w + w0
f (w; Z ) + f (w0 ; Z )
ν
8 kw − w0k2 .
2
2
Taking expectation this gives, for any w, w0 ∈ S ,
2 (cid:19) +
≥ F (cid:18) w + w0
F (w) + F (w0 )
ν
8 kw − w0 k2 .
2
Now using this with w = wt , w0 = w? , we get
(cid:19) +
≥ F (cid:18) wt + w?
F (wt ) + F (w? )
ν
8 kwt − w? k2
2
2
ν
8 kwt − w? k2 .
≥ F (w? ) +
[∵ w? minimizes F ]

This implies that

Combining (2) and (3) we get,

kwt − w? k2 ≤

4(F (wt ) − F (w? ))
ν

Vart−1 ξt ≤

4L2
ν

(F (wt ) − F (w? ))

(2)

(3)

The proof of Theorem 2 relies on the following inequality for martingales which is an easy conse-
quence of Freedman’s inequality [Freedman, 1975, Theorem 1.6]. The proof of this lemma can be
found in the appendix.
Lemma 3. Suppose X1 , . . . , XT is a martingale difference sequence with |Xt | ≤ b. Let
VartXt = Var (Xt | X1 , . . . , Xt−1 ) .
t=1 VartXt be the sum of conditional variances of Xt ’s. Further, let σ = √V . Then we
Let V = PT
have, for any δ < 1/e and T ≥ 3,
Xt > max n2σ, 3bpln(1/δ)o pln(1/δ)! ≤ 4 ln(T )δ .
Prob   TXt=1

By deﬁnition of RegT ,

Proof of Theorem 2. By Lemma 1, we have σ := qPT
t=1 Vart ξt ≤ q 4L2
ν Diﬀ T . Note that |ξt | ≤
2B because our f has range [0, B ]. Therefore, Lemma 3 gives us that with probability at least
1 − 4 ln(T )δ , we have
ξt ≤ max n2σ, 6Bpln(1/δ)o pln(1/δ) .
TXt=1
TXt=1
ξt
Diﬀ T − RegT ≤
and therefore, with probability, 1 − 4 ln(T )δ , we have
Diﬀ T − RegT ≤ max (4r L2
Diﬀ T , 6Bpln(1/δ)) pln(1/δ) .
ν
Using Lemma 4 below to solve the above quadratic inequality for Diﬀ T , gives
+ 4r L2 ln(1/δ)
pRegT
PT
, 6B(cid:27) ln(1/δ)
+ max (cid:26) 16L2
t=1 F (wt )
T
ν
T
ν
T

− F (w? ) ≤

RegT
T

The following elementary lemma was required to solve a recursive inequality in the proof of the
above theorem. Its proof can be found in the appendix.
Lemma 4. Suppose s, r, d, b, ∆ ≥ 0 and we have
s − r ≤ max{4√ds, 6b∆}∆ .
s ≤ r + 4√dr∆ + max{16d, 6b}∆2 .

Then, it follows that

4 Applications

4.1 Online to Batch Conversion for Learning with Bounded Loss

Suppose (X1 , Y1 ), . . . , (XT , YT ) are drawn i.i.d. from a distribution. The pairs (Xi , Yi ) belong
to X × Y and our algorithm are allowed to make predictions in a space D ⊇ Y . A loss function
` : D × Y → [0, 1] measures quality of predictions. Fix a convex set S of some normed space and a
function h : X × S → D . Let our hypotheses class be {x 7→ h(x; w) | w ∈ S }.
On input x, the hypothesis parameterized by w predicts h(x; w) and incurs loss `(h(x; w), y) if the
correct prediction is y . The risk of w is deﬁned by
R(w) := E [`(h(X ; w), Y )]
and let w? := arg minw∈S R(w) denote the (parameter for) the hypothesis with minimum risk. It
is easy to see that this setting falls under the general framework given above by thinking of the pair
(X, Y ) as Z and setting f (w; Z ) = f (w; (X, Y )) to be `(h(X ; w), Y ). Note that F (w) becomes
the risk R(w). The range of f is [0, 1] by our assumption about the loss functions so B = 1.
Suppose we run an online algorithm on our data that generates a sequence of hypotheses w0 , . . . , wT
such that wt is measurable w.r.t. X<t , Y<t . Deﬁne the statistics,
TXt=1
TXt=1
`(h(Xt ; wt ), Yt ) − min
w∈S
TXt=1
TXt=1
R(wt ) − T R(w? ) .
Diﬀ T :=
At the end, we output ¯w := (PT
wt )/T . The following corollary then follows immediately from
t=1
Theorem 2. It bounds the excess risk R( ¯w) − R(w? ).

(R(wt ) − R(w? )) =

`(h(Xt ; w), Yt ) ,

RegT :=

Corollary 5. Suppose assumption LIST is satis ﬁed for f (w; (x, y)) := `(h(x; w), y). Then we
have, with probability at least 1 − 4 ln(T )δ ,
+ 4r L2 ln(1/δ)
pRegT
, 6(cid:27) ln(1/δ)
+ max (cid:26) 16L2
RegT
R( ¯w) − R(w? ) ≤
T
ν
T
ν
T
Recently, it has been proved [Kakade and Shalev-Shwartz, 2008] that if assumption LIST is satis ﬁed
for w 7→ `(h(x; w), y) then there is an online algorithm that generates w1 , . . . , wT such that
L2 (1 + ln T )
RegT ≤
.
2ν
Plugging it in the corollary above gives the following result.
Corollary 6. Suppose assumption LIST is satis ﬁed for f (w; (x, y)) := `(h(x; w), y). Then there is
an online algorithm that generates w1 , . . . , wT and in the end outputs ¯w such that, with probability
at least 1 − 4 ln(T )δ ,
ν T sln (cid:18) 1
4L2√ln T
δ (cid:19) + max (cid:26) 16L2
, 6(cid:27) ln(1/δ)
T
ν

L2 ln T
ν T

,

+

R( ¯w) − R(w? ) ≤
for any T ≥ 3.
4.2 High Probability Bound for P EGA SO S

F (w) =

(4)

f (w; Zt ) =

`(w; (x, y)) ,

`(w; (xi , yi ))

λ
2 kwk2 +

P EGA SO S [Shalev-Shwartz et al., 2007] is a recently proposed method for solving the primal SVM
problem. Recall that in the SVM optimization problem we are given m example, label pairs
(xi , yi ) ∈ Rd × {±1}. Assume that kxi k ≤ R for all i where k · k is the standard L2 norm.
Let
mXi=1
λ
1
2 kwk2 +
m
be the SVM objective function. The loss function `(w; (x, y)) = [1 − y(w · x)]+ is the hinge loss.
At time t, P EGA SO S takes a (random) approximation
k X(x,y)∈Zt
1
of the SVM objective function to estimate the gradient and updates the current weight vector wt to
wt+1 . Here Zt is a random subset of the data set of size k . Note that F (w) can be written as
2 kwk2 + `(w; Z )(cid:21)
F (w) = E (cid:20) λ2
where Z is an example (xi , yi ) drawn uniformly at random from the m data points. It is also easy
to verify that
∀w, E [f (w; Zt )] = F (w) .
It can be shown that w? := arg min F (w) will satisfy kw? k ≤ 1/√λ so we set
√λ (cid:27) .
S = (cid:26)w ∈ Rd : kwk ≤
1
For any z that is a subset of the data set, the function
|z | X(x,y)∈z
λ
1
2 kwk2 +
w 7→ f (w; z ) =
is Lipschitz on S with Lipschitz constant L = √λ + R and is λ-strongly convex. Also f (w; z ) ∈
[0, 3/2 + R/√λ]. So, the P EGA SO S setting falls under our general framework and satis ﬁes assu mp-
tion LIST.

`(w; (x, y))

f (w; Zt ) +

,

(5)

.

L2 ln T
λ

Theorem 1 in Shalev-Shwartz et al. [2007] says, for any w, T ≥ 3,
TXt=1
TXt=1
L2 ln T
f (wt ; Zt ) ≤
λ
where L = √λ + R. It was noted in that paper that plugging in w = w? and taking expectations,
we easily get
EZ1 ,...,ZT " TXt=1
F (wt )# ≤ T F (w? ) +
Here we use Theorem 2 to prove an inequality that holds with high probability, not just in expecta-
tion.
Corollary 7. Let F be the SVM objective function deﬁned in (4) and w1 , . . . , wT be the sequence
of weight vectors generated by the P EGA SO S algorithm. Further, let w? denote the minimizer of the
SVM objective. Then, with probability 1 − 4δ ln(T ), we have
λ sln (cid:18) 1
4L2√ln T
√λ (cid:27) ln (cid:18) 1
δ (cid:19)+max (cid:26) 16L2
δ (cid:19) , (6)
TXt=1
L2 ln T
6R
F (wt )−T F (w? ) ≤
λ
λ
for any T ≥ 3. Therefore, assuming R = 1, we have, for λ small enough, with probability at least
1 − δ ,
λT ! .
F (wt ) − F (w? ) = O   ln T
TXt=1
δ
Proof. Note that (5) implies that RegT ≤ L2 ln T
. The corollary then follows immediately from
Theorem 2 by plugging in ν = λ and B = 3/2 + R/√λ.
λ

, 9 +

1
T

+

IEEE Transactions on

References
N. Cesa-Bianchi and C. Gentile.
Improved risk tail bounds for on-line algorithms.
Information Theory, 54(1):286–390, 2008.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms.
IEEE Transactions on Information Theory, 50(9):2050–2057, September 2004.
M. Collins. Discriminative training methods for hidden Markov models: Theory and experiments with percep-
tron algorithms. In Conference on Empirical Methods in Natural Language Processing, 2002.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms.
Journal of Machine Learning Research, 7:551–585, Mar 2006.
David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100–118, Feb 1975.
E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In
Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2006.
S. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for online optimization.
Advances in Neural Information Processing Systems, 2008.
N. Littlestone. Mistake bounds and logarithmic linear-threshold learning algorithms. PhD thesis, U. C. Santa
Cruz, March 1989.
Nathan Ratliff, James (Drew) Bagnell, and Martin Zinkevich.
(online) subgradient methods for structured
prediction. In Eleventh International Conference on Artiﬁcial Intelligence and Statistics (A IStats), March
2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for
SVM. In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML), pages
807–814, 2007.
T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In Proceedings of the
Eighteenth Annual Conference on Computational Learning Theory, pages 173–187, 2005.

≤

≤

Appendix
Proof of Lemma 3. Note that a crude upper bound on VartXt is b2 . Thus, σ ≤ b√T . We choose a
discretization 0 = α−1 < α0 < . . . < αl such that αi+1 = rαi for i ≥ 0 and αl ≥ b√T . We will
specify the choice of α0 and r shortly. We then have, for any c > 0,
Prob  Xt
Xt > c max{rσ, α0 }pln(1/δ)!
(cid:19)
Prob (cid:18)Pt Xt > c max{rσ, α0 }pln(1/δ)
lXj=0
=
& αj−1 < σ ≤ αj
j (cid:19)
Prob (cid:18)Pt Xt > cαjpln(1/δ)
lXj=0
& α2
j−1 < V ≤ α2
Prob  Xt
j !
lXj=0
Xt > cαjpln(1/δ) & V ≤ α2
exp 
3 (cid:16)cαjpln(1/δ)(cid:17) b 
lXj=0
−c2α2
j ln(1/δ)
j + 2
2α2
exp 
3 (cid:16)cpln(1/δ)(cid:17) b 
lXj=0
−c2αj ln(1/δ)
=
2αj + 2
where the inequality (?) follows from Freedman’s inequality. If we now choose α0 = bcpln(1/δ)
then αj ≥ bcpln(1/δ) for all j and hence every term in the above summation is bounded by
exp (cid:16) −c2 ln(1/δ)
2+2/3 (cid:17) which is less then δ if we choose c = 5/3. Set r = 2/c = 6/5. We want
α0 r l ≥ b√T . Since cpln(1/δ) ≥ 1, choosing l = logr (√T ) ensures that. Thus we have
Prob   TXt=1
bpln(1/δ)}pln(1/δ)!
6
5
5
max{
σ,
Xt >
3
5
3
Xt > c max{rσ, α0 }pln(1/δ)!
= Prob  Xt
≤ (l + 1)δ = (log6/5 (√T ) + 1)δ
≤ (6 ln(√T ) + 1)δ ≤ 4 ln(T )δ .

(∵ T ≥ 3)

(?)
≤

Proof of Lemma 4. The assumption of the lemma implies that one of the following inequalities
holds:
s − r ≤ 4√ds∆ .
s − r ≤ 6b∆2
In the second case, we have
− (4√d∆)√s − r ≤ 0
(cid:0)√s(cid:1)2
which means that √s should be smaller than the larger root of the above quadratic. This gives us,
s = (√s)2 ≤ (cid:16)2√d∆ + p4d∆2 + r(cid:17)2
≤ 4d∆2 + 4d∆2 + r + 4p4d2∆4 + d∆2 r
≤ 8d∆2 + r + 8d∆2 + 4√dr∆
[∵ √x + y ≤ √x + √y ]
≤ r + 4√dr∆ + 16d∆2 .

(8)

(7)

Combining (7) and (8) ﬁnishes the proof.


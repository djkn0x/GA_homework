A Transductive Bound for the Voted Classiﬁer with an
Application to Semi-supervised Learning

Massih R. Amini
Laboratoire d’Informatique de Paris 6
Universit ´e Pierre et Marie Curie, Paris, France
massih-reza.amini@lip6.fr

Franc¸ ois Laviolette
Universit ´e Laval
Qu ´ebec (QC), Canada
francois.laviolette@ift.ulaval.ca

Nicolas Usunier
Laboratoire d’Informatique de Paris 6
Universit ´e Pierre et Marie Curie, Paris, France
nicolas.usunier@lip6.fr

Abstract

We propose two transductive bounds on the risk of majority votes that are estimated over
partially labeled training sets. The ﬁrst one involves the margin distribution of the clas-
siﬁer and a risk bound on its associate Gibbs classiﬁer. The bound is tight when so is
the Gibbs’s bound and when the errors of the majority vote classiﬁer is concentrated on a
zone of low margin. In semi-supervised learning, considering the margin as an indicator
of conﬁdence constitutes the working hypothesis of algorithms which search the decision
boundary on low density regions. Following this assumption, we propose to bound the er-
ror probability of the voted classiﬁer on the examples for whose margins are above a ﬁxed
threshold. As an application, we propose a self-learning algorithm which iteratively as-
signs pseudo-labels to the set of unlabeled training examples that have their margin above
a threshold obtained from this bound. Empirical results on different datasets show the
effectiveness of our approach compared to the same algorithm and the TSVM in which
the threshold is ﬁxed manually.

1

Introduction

Ensemble methods [5] return a weighted vote of baseline classiﬁers. It is well known that under the PAC-
Bayes framework [9], one can obtain an estimation of the generalization error (also called risk) of such
majority votes (referred as Bayes classiﬁer). Unfortunately, those bounds are generally not tight, mainly
because they are indirectly obtain via a bound on a randomized combination of the baseline classiﬁers
(called the Gibbs classiﬁer). Although the PAC-Bayes theorem gives tight risk bounds of Gibbs classiﬁers,
the bounds of their associate Bayes classiﬁers come at a cost of worse risk (trivially a factor of 2, or under
some margin assumption, a factor of 1+). In practice the Bayes risk is often smaller than the Gibbs risk.
In this paper we present a transductive bound over the Bayes risk. This bound is also based on the risk of
the associated Gibbs classiﬁer, but it takes as an additional information the exact knowledge of the margin
distribution of unlabeled data. This bound is obtained by analytically solving a linear program. The intuitive
idea here is that given the risk of the Gibbs classiﬁer and the margin distribution, the risk of the majority
vote classiﬁer is maximized when all its errors are located on low margin examples. We show that our
bound is tight when the associated Gibbs risk can accurately be estimated and when the Bayes classiﬁer
makes most of its errors on low margin examples.
The proof of this transductive bound makes use of the (joint) probability over an unlabeled data set that the
majority vote classiﬁer makes an error and the margin is above a given threshold. This second result natu-
rally leads to consider the conditional probability that the majority vote classiﬁer makes an error knowing
that the margin is above a given threshold.

This conditional probability is related to the concept that the margin is an indicator of conﬁdence which is
recurrent in semi-supervised self-learning algorithms [3,6,10,11,12]. These methods ﬁrst train a classiﬁer
on the labeled training examples. The classiﬁer outputs serve then to assign pseudo-class labels to unlabeled
data having margin above a given threshold. The supervised method is retrained using the initial labeled set
and its previous predictions on unlabeled data as additional labeled examples. Practical algorithms almost
ﬁx the margin threshold manually.
In the second part of the paper, we propose to ﬁnd this margin threshold by minimizing the bound on
the conditional probability. Empirical results on different datasets show the effectiveness of our approach
compared to TSVM [7] and the same algorithm but with a manually ﬁxed threshold as in [11]
In the remainder of the paper, we present, in section 2, our transductive bounds and show their outcomes
in terms of sufﬁcient conditions under which unlabeled data may be of help in the learning process and a
linear programming method to estimate these bounds. In section 4, we present experimental results obtained
with a self-learning algorithm on different datasets in which we use the bound presented in section 2.2 for
choosing the threshold which serve in the label assignment step of the algorithm. Finally, in section 5 we
discuss the outcomes of this study and give some pointers to further research.

2 Transductive Bounds on the Risk of the Voted Classiﬁer
We are interested in the study of binary classiﬁcation problems where the input space X is a subset of Rd
and the output space is Y = {−1, +1}. We furthermore suppose that the training set is composed of a
i=1 ∈ Z l and an unlabeled set XU = (x(cid:48)
i=l+1 ∈ X u , where Z represents the
labeled set Z(cid:96) = ((xi , yi ))l
i )l+u
set of X × Y . We suppose that each pair (x, y) ∈ Z(cid:96) is drawn i.i.d. with respect to a ﬁxed, but unknown,
probability distribution D over X × Y and we denote the marginal distribution over X by DX .
To simplify the notation and the proofs, we restrict ourselves to the deterministic labeling case, that is, for
each x(cid:48) ∈ XU , there is exactly one possible label that we will denote by y (cid:48) .1
In this study, we consider learning algorithms that work in a ﬁxed hypothesis space H of binary classiﬁers
(deﬁned without reference to the training data). After observing the training set S = Z(cid:96) ∪ XU , the task of
the learner is to choose a posterior distribution Q over H such that the Q-weighted majority vote classiﬁer
BQ (also called the Bayes classiﬁer) will have the smallest possible risk on examples of XU . Recall that the
Bayes classiﬁer is deﬁned by
∀x ∈ X .
BQ (x) = sgn [Eh∼Qh(x)]
(1)
where, sgn(x)=+1 if the real number x > 0 and −1 otherwise. We further denote by GQ the associated
Gibbs classiﬁer which for classifying any example x ∈ X chooses randomly a classiﬁer h according to the
(cid:88)
distribution Q. We accordingly deﬁne the transductive risk of GQ over an unlabeled set by:
1
Eh∼Q [[h(x(cid:48) ) (cid:54)= y (cid:48) ]]
Ru (GQ ) def=
(2)
u
x(cid:48)∈XU
Where, [[π ]] = 1 if predicate π holds and 0 otherwise, and for every unlabeled example x(cid:48) ∈ XU we refer to
y (cid:48) as its true unknown class label. In section 2.1 we show that if we consider the margin as an indicator of
u (GQ ) of the risk of GQ which holds with probability
conﬁdence and that we dispose a tight upper bound Rδ
1 − δ over the random choice of Z(cid:96) and XU (for example using Theorem 17 or 18 of Derbelo et al. [4]), we
(cid:88)
are then able to accurately bound the transductive risk of the Bayes classiﬁer:
1
[[BQ (x(cid:48) ) (cid:54)= y (cid:48) ]]
Ru (BQ ) def=
u
x(cid:48)∈XU
(cid:88)
This result follows from a bound on the joint Bayes risk:
1
[[BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) > θ]]
Ru∧θ (BQ ) def=
(4)
u
x(cid:48)∈XU
Where mQ (·) = |Eh∼Qh(·)| denotes the unsigned margin function. One of the practical issues that arises
from this result is the possibility to deﬁne a threshold θ for which the bound is optimal and that we use
in a self-learning algorithm by iteratively assigning pseudo-labels to unlabeled examples having margin
above this threshold. We ﬁnally denote by Eu z the expectation of a random variable z with respect to
the uniform distribution over XU and for notation convenience we equivalently deﬁne Pu the uniform
1The proofs can be inferred to the more general noisy case, but one has to replace the summation (cid:80)
u card(A).
probability distribution over XU i.e. For any subset A, P (A) = 1
(cid:80)
(x(cid:48) ,y(cid:48) )∈XU ×{−1,+1}. P(x(cid:48) ,y(cid:48) )∼D (y (cid:48) |x(cid:48) ) in the deﬁnitions of equations (3) and (4).

x(cid:48) ∈XU by

(3)

2.1 Main Result

Our main result is the following theorem which provides two bounds on the transductive risks of the Bayes
classiﬁer (3) and the joint Bayes risk (4).
(cid:27)
(cid:26)
Theorem 1 Suppose that BQ is as in (1). Then for all Q and all δ ∈ (0, 1] with probability at least 1 − δ :
Q (γ )(cid:5)
(cid:4)K δ
1
Ru (BQ ) ≤ inf
u (Q) − M <
Pu (mQ (x(cid:48) ) < γ ) +
(5)
γ∈(0,1]
γ
+
(cid:67)
2 (EumQ (x(cid:48) ) − 1), M
Q (t) = EumQ (x(cid:48) )[[mQ (x(cid:48) ) (cid:67) t]] for (cid:67) being < or ≤
u (Q) = Rδ
u (GQ ) + 1
Where K δ
and (cid:98).(cid:99)+ denotes the positive part (i.e. (cid:98)x(cid:99)+ = [[x > 0]]x).
(cid:27)
(cid:26)
(cid:107)
(cid:106)
More generally, with probability at least 1 − δ , for all Q and all θ ≥ 0:
1
≤
Q (θ) − M <
Ru∧θ (BQ ) ≤ inf
Pu (θ < mQ (x(cid:48) ) < γ ) +
Q (γ )
u (Q) + M
K δ
γ∈(θ,1]
γ
+
(cid:26)
(cid:27)
In section 2.2 we will prove that the bound (5) simply follows from (6). In order to better understand the
(cid:4)K δ
Q (γ )(cid:5)
u (Q) the right hand side of equation (5):
former bound on the risk of the Bayes classiﬁer, denote by F δ
1
u (Q) − M <
Pu (mQ (x(cid:48) ) < γ ) +
u (Q) def= inf
F δ
γ∈(0,1]
γ
and consider the following special case where the classiﬁer makes most of its errors on unlabeled examples
with low margin. Proposition 2, together with the explanations that follow, makes this idea clearer.
Proposition 2 Assume that ∀x ∈ XU , mQ (x) > 0 and that ∃C ∈ (0, 1] such that ∀γ > 0:

(6)

+

Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) = γ ) (cid:54)= 0 ⇒ Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) < γ ) ≥ C · Pu (mQ (x(cid:48) ) < γ )
Then, with probability at least 1 − δ :
u (Q) − Ru (BQ ) ≤ 1 − C
u (GQ ) − Ru (GQ )
Ru (BQ ) + Rδ
F δ
γ ∗
C
Where γ ∗ = sup {γ |Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) = γ ) (cid:54)= 0}
Now, suppose that the margin is an indicator of conﬁdence. Then, a Bayes classiﬁer that makes its error
mostly on low margin regions will admit a coefﬁcient C in inequality (7) close to 1 and the bound of (5)
u (GQ ) ). In the next section we provide proofs
becomes tight (provided we have an accurate upper bound Rδ
of all the statements above and show in lemma 4 a simple way to compute the best margin threshold for
which the general bound on the joint Bayes risk is the lowest.

(7)

2.2 Proofs

All our proofs are based on the relationship between Ru (GQ ) and Ru (BQ ) and the following lemma:

Ru (GQ ) =

(1 − EumQ (x(cid:48) ))

Lemma 3 Let (γ1 , .., γN ) be the ordered sequence of the different strictly positive values of the margin on
XU , that is {γi , i = 1..N } = {mQ (x(cid:48) )|x(cid:48) ∈ XU ∧ mQ (x(cid:48) ) > 0} and ∀i ∈ {1, . . . , N − 1}, γi < γi+1 .
Denote moreover bi = Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) = γi ) for i ∈ {1, . . . , N }. Then,
N(cid:88)
1
2
i=1

biγi +
N(cid:88)
i=k+1
Proof Equation (9) follows the deﬁnition Ru∧θ (BQ ) = Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) > θ).
Equation (8) is obtained from the deﬁnition of the margin mQ which writes as
∀x(cid:48) ∈ XU , mQ (x(cid:48) ) = |Eh∼Q [[h(x(cid:48) ) = 1]] − Eh∼Q [[h(x(cid:48) ) = −1]]| = |1 − 2Eh∼Q [[h(x(cid:48) ) (cid:54)= y (cid:48) ]]|

bi with k = max{i|γi ≤ θ}

∀θ ∈ [0, 1], Ru∧θ (BQ ) =

(8)

(9)

By noticing that for all x(cid:48) ∈ XU the condition Eh∼Q [[h(x(cid:48) ) (cid:54)= y (cid:48) ]] > 1
2 is equivalent to the statement
y (cid:48)Eh∼Qh(x(cid:48) ) < 0 or BQ (x(cid:48) ) (cid:54)= y (cid:48) , we can rewrite mQ without absolute values and hence get:
1
1
(1 − mQ (x(cid:48) ))[[BQ (x(cid:48) ) = y (cid:48) ]]
(1 + mQ (x(cid:48) ))[[BQ (x(cid:48) ) (cid:54)= y (cid:48) ]] +
Eh∼Q [[h(x(cid:48) ) (cid:54)= y (cid:48) ]] =
(10)
2
2
Finally equation (8) yields by taking the mean over x(cid:48) ∈ XU and by reorganizing the equation using the
notations of bi and γi . Recall that the values the x(cid:48) for which mQ (x(cid:48) ) = 0 counts for 0 in the sum that
deﬁned the Gibbs risk (see equation 2 and the deﬁnition of mQ ). (cid:3)

≤
Q (0) = 0

bi u.c. ∀i, 0 ≤ bi ≤ Pu (mQ (x(cid:48) ) = γi ) and

Proof of Theorem 1 First, we notice that equation (5) follows equation (6) from the fact that M
and the following inequality:
Ru (BQ ) = Ru∧0 (BQ ) + Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) = 0) ≤ Ru∧0 (BQ ) + Pu (mQ (x(cid:48) ) = 0)
For proving equation (6), we know from lemma 3 that for a ﬁx θ ∈ [0, 1] there exist (b1 , . . . , bN ) such that
0 ≤ bi ≤ Pu (mQ (x(cid:48) ) = γi ) and which satisfy equations (8) and (9).
Let k = max{i | γi ≤ θ}, assuming now that we can obtain an upper bound Rδ
u (GQ ) of Ru (GQ ) which
holds with probability 1 − δ over the random choices of Z(cid:96) and XU , from the deﬁnition (4) of Ru∧θ (BQ )
with probability 1 − δ we have then
N(cid:88)
N(cid:88)
Ru∧θ (BQ ) ≤ max
b1 ,..,bN
i=1
i=k+1
2 (1 − EumQ (x(cid:48) )). It turns out that the right hand side in equation (11) is the
u (GQ ) − 1
u (Q) = Rδ
Where K δ
0
solution of a linear program that can be solved analytically and which is attained for:
(cid:32)
(cid:33)
(cid:22) K δ
(cid:23)
if i ≤ k ,
u (Q)−(cid:80)
k<j<i γj Pu (mQ (x(cid:48) )=γj )
bi =
Pu (mQ (x(cid:48) ) = γi ) ,
min
γi
+
Using the notations deﬁned in Theorem 1, we rewrite (cid:80)
For clarity, we defer the proof of equation (12) to lemma 4, and continue the proof of equation (6).
(cid:111)
(cid:110)
which implies (cid:80)N
i=k+1 bi = (cid:80)I
≤
Q (γi ) − M
k<j<i γj Pu (mQ (x(cid:48) ) = γj ) as M <
Q (θ).
≤
Q (θ) − M <
i|K δ
Q (γi ) > 0
u (Q) + M
We further deﬁne I = max
i=k+1 bi
≤
Q (θ)−M <
from equations (11) and (12) with bI = K δ
u (Q)+M
Q (γI )
. From this inequality we hence obtain a
γI
bound on Ru∧θ (BQ ):

biγi ≤ K δ
u (Q)

elsewhere.

(11)

(12)

(13)

u (Q) + M
K δ

Ru∧θ (BQ ) ≤ Pu (θ < mQ (x(cid:48) ) < γI ) +

≤
Q (θ) − M <
Q (γI )
γI
The proof of the second point in theorem 1 is just a rewriting of this result as from the deﬁnition of γI , for any
γ > γI , the right-hand side of equation (6) is equal to Pu (mQ (x(cid:48) ) < γ ), which is greater than the right-hand
≤
Q (θ)−M <
side of equation (13). Moreover, for γ < γI , we notice that γ (cid:55)→ Pu (mQ (x(cid:48) ) < γ ) + K δ
u (Q)+M
Q (γ )
γ
decreases. (cid:3)
Lemma 4 (equation (12)) Let gi , i = 1...N be such that 0 < gi < gi+1 , pi ≥ 0, i = 1...N , B ≥ 0 and
k ∈ {1, . . . , N }. Then, the optimal value of the linear program:
N(cid:88)
N(cid:88)
qi gi ≤ B
max
(cid:16)
q1 ,...,qN
pi , (cid:98) B−(cid:80)
i=1
i=k+1
j<i q∗
i = 0 and ∀i > k , q∗
is attained for q∗ deﬁned by: ∀i ≤ k : q∗
j gj
Proof Deﬁne O = {0}k × (cid:81)N
i = min
gi
in O , and that this solution is q∗ . In the rest of the proof, we denote F (q) = (cid:80)N
i=k+1 [0, pi ]. We will show that problem (14) has a unique optimal solution
i=k+1 qi .

u.c. ∀i, 0 ≤ qi ≤ pi and

(cid:17)

(cid:99)+

qi

(14)

qopt ∈ (cid:81)N
First, the problem is convex, feasible (take ∀i, qi = 0) and bounded. Therefore there is an optimal solution
if i > k and qopt,O
i=1 [0, pi ]. Deﬁne qopt,O by qopt,O
= 0 otherwise. Then, qopt,O ∈ O ,
= qopt
it is clearly feasible, and F (qopt,O ) = F (qopt ). Therefore, there is an optimal solution in O .
i
i
i
Now, for (q , q (cid:48) ) ∈ RN × RN , deﬁne I(q , q (cid:48) ) = {i|qi > q (cid:48)
i }, and consider the lexicographic order (cid:23):
∀(q , q (cid:48) ) ∈ RN × RN , q (cid:23) q (cid:48) ⇔ I(q (cid:48) , q) = ∅ or (I(q , q (cid:48) ) (cid:54)= ∅ and min I(q , q (cid:48) ) < min I(q (cid:48) , q))
necessarily feasible and (cid:80)N
The crucial point is that q∗ is the greatest feasible solution in O for (cid:23).
Indeed, notice ﬁrst that q∗ is
i = B . To see this result let M be the set {i > k | : q∗
i < pi }, we then have
i=1 q∗
two possibilities to consider. (a) M = ∅. In this case q∗ is simply the maximal element for (cid:23) in O . (b)
M (cid:54)= ∅. In this case, let K = min{i > k |q∗
i < pi } and M = I(q , q∗ ).
We claim that there are no feasible q ∈ RN such that q (cid:31) q∗ . By way of contradiction, suppose such a q
which yields the same result; and ﬁnally, if M ≥ K , we have (cid:80)N
i=1 qi > (cid:80)N
exists. Then, if M ≤ k , we have qM > 0, and therefore q is not in O; if k < M < K , we have qM > pM ,
i=1 q∗
i = B , and q is not
feasible.
We now show that if q ∈ O is feasible and q∗ (cid:31) q , then q is not optimal (which is equivalent to show that
an optimal solution in O must be the greatest feasible solution for (cid:23)).
Let q ∈ O be a feasible solution such that q∗ (cid:31) q . Since q (cid:31) q∗ , I(q∗ , q) is not empty. If I(q , q∗ ) = ∅, we
have F (q∗ ) > F (q), and therefore q is not optimal. We now treat the case where I(q , q∗ ) (cid:54)= ∅.
(cid:16)
(cid:17)
Let K = min I(q∗ , q) and M = min I(q , q∗ ). We have qM > 0 by deﬁnition, and K < M because q∗ (cid:31) q
and q ∈ O . Let λ = min
(pK − qK )
and deﬁne q (cid:48) by:
qM , gM
gK
We can see that q (cid:48) is feasible by the deﬁnition of λ, that it satisﬁes the box constraints, and (cid:80)
M = qM − λ.
i = qi if i (cid:54)∈ {K, M } ,
q (cid:48)
q (cid:48)
q (cid:48)
K = qK + gM
and
λ
λ ∗ gK − λ ∗ gM = (cid:80)
(cid:80)
gK
i q (cid:48)
i gi =
− 1) > F (q) since
i qi gi ≤ B . Moreover F (q (cid:48) ) = F (q) + λ( gM
i qi gi + gM
gK
gK
gK < gM and λ > 0. Thus, q is not optimal.
In summary, we have shown that there is an optimal solution in O , and that a feasible solution in O must be
the greatest feasible solution for the lexicographic order in O to be optimal and which is q∗ . (cid:3)
Proof of Proposition 2 First let us claim that
Q (γ ∗ )(cid:5)
(cid:4)Ku (Q) − M <
1
Ru (BQ ) ≥ Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) < γ ∗ ) +
(15)
γ ∗
+
where γ ∗ = sup {γ |Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) = γ ) (cid:54)= 0} and Ku (Q) = Ru (GQ ) + 1
2 (EumQ (x(cid:48) ) − 1).
Q (γ ∗ )(cid:5)
(cid:4)Ku (Q) − M <
Indeed, assume for now that equation (15) is true. Then, by assumption we have:
1
Ru (BQ ) ≥ C · Pu (mQ (x(cid:48) ) < γ ∗ ) +
(cid:107)
(cid:106)
γ ∗
u (Q) − M <
, with probability at least 1 − δ we obtain:
Q (γ ∗ )
K δ
+
u (GQ ) − Ru (GQ )
u (Q) − Ru (BQ ) ≤ (1 − C )Pu (mQ (x(cid:48) ) < γ ∗ ) + Rδ
(17)
F δ
γ ∗
u (GQ ) − Ru (GQ ) when
This is due to the fact that (cid:98)K δ
Q (γ ∗ )(cid:99)+ ≤ Rδ
Q (γ ∗ )(cid:99)+ − (cid:98)Ku (Q) − M <
u (Q) − M <
u (GQ ) ≥ Ru (GQ ). Taking once again equation (16), we have Pu (mQ (x(cid:48) ) < γ ∗ ) ≤ 1
C Ru (BQ ). Plug-
Rδ
ging back this result in equation (17) yields Proposition 2.
Now, let us prove the claim (equation (15)). Since ∀x(cid:48) ∈ XU , mQ (x(cid:48) ) > 0, we have Ru (BQ ) = Ru∧0 (BQ ).
of lemma 3 that Ru (GQ ) = (cid:80)K
Using the notations of lemma 3, denote K the index such that γK = γ ∗ . Then, it follows from equation (8)
Ku (Q)−(cid:80)K−1
2 (1 − EumQ (x(cid:48) )). Solving for bK in this equality yields bK =
i=1 biγi + 1
Pu (mQ (x(cid:48) ) = γi ). Finally, from equation (9), we have Ru (BQ ) = (cid:80)K
Q (γ ∗ )(cid:99)+ since bK ≥ 0 and ∀i, bi ≤
and we therefore have bK ≥ 1
(cid:98)Ku (Q) − M <
i=1 bi γi
by using the lower bound on bK and the fact that (cid:80)K−1
γK
γK
i=1 bi , which implies equation (15)
i=1 bi = Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) ∧ mQ (x(cid:48) ) < γ ∗ ). (cid:3)

u (Q) ≤ Pu (mQ (x(cid:48) ) < γ ∗ ) + 1
Since F δ
γ ∗

+

(16)

In general, good PAC-Bayesian approximations of Ru (GQ ) are difﬁcult to carry out in supervised learning
[4] mostly due to the huge number of needed instances to obtain accurate approximations of the distribution
of the absolute values of the margin. In this section we have shown that the transductive setting allows for
high precision on the bounds from the risk Ru (GQ ) of the Gibbs classiﬁer to the risk Ru (BQ ) if we suppose
that the Bayes classiﬁer makes its errors mostly on low margin regions.

3 Relationship with margin-based self-learning algorithms

(18)

In Proposition 2 we have considered the hypothesis that the margin is an indicator of conﬁdence as one of
the sufﬁcient conditions which leads to a tight approximation of the risk of the Bayes classiﬁer, Ru (BQ ).
This assumption constitutes the working hypothesis of margin-based self-learning algorithms in which a
classiﬁer is ﬁrst built on the labeled training set. The output of the learner can then be used to assign
pseudo-labels to unlabeled examples having a margin above a ﬁxed threshold (denoted by the set ZU\ in
what follows) and the supervised method is repeatedly retrained upon the set of the initial labeled and
unlabeled examples that have been classiﬁed in the previous steps. The idea behind this pseudo-labeling
is that unlabeled examples having a margin above a threshold are less subject to error prone labels, or
equivalently, are those which have a small conditional Bayes error deﬁned as:
Ru∧θ (BQ )
Ru|θ (BQ ) def= Pu (BQ (x(cid:48) ) (cid:54)= y (cid:48) | mQ (x(cid:48) ) > θ) =
Pu (mQ (x(cid:48) ) > θ)
In this case the label assignation of unlabeled examples upon a margin criterion has the ef-
fect
This strategy follows the
to push away the decision boundary from the unlabeled data.
cluster assumption [10] used in the design of some semi-supervised learning algorithms where
the decision boundary is supposed to pass through a region of low pattern density.
Though
margin-based self-learning algorithms are inductive in essence,
their learning phase is nearly re-
lated to transductive learning which predicts the labels of a given unlabeled set.
Indeed,
in both
cases the pseudo class-label assignation of unlabeled examples is interrelated to their margin.
For all these algorithms the choice
of the threshold is a crucial point,
as with a low threshold the risk
to assign false labels to exam-
ples is high and a higher value of
the threshold would not provide
enough examples to enhance the
current decision function. In order
to examine the effect of ﬁxing the
threshold or computing it automat-
ically we considered the margin-
based self-training algorithm pro-
posed by T ¨ur et al. [10, Figure 6]
(referred as SLA in the following),
in which unlabeled examples hav-
ing margin above a ﬁxed threshold
are iteratively added to the labeled
set and are not considered in next rounds for label distribution. In our approach, the best threshold mini-
mizing the conditional Bayes error (18) from equation (6) of theorem 1 is computed at each round of the
algorithm (line 3, ﬁgure 1 - SLA∗ ) while the threshold is kept ﬁxed in [10, Figure 6] (line 3 is outside of the
Q (G), of the risk of the Gibbs classiﬁer which is involved in the computation of
repeat loop). The bound Rδ
the threshold in equation (18) was ﬁxed to its worst value 0.5.

Input: Labeled and Unlabeled training sets: Z(cid:96) , XU
Initialize
(1) Train a classiﬁer H on Z(cid:96)
(2) Set ZU\ ← ∅
repeat
(3) Compute the margin threshold θ∗ minimizing (18) from (6)
(4) S ← {(x(cid:48) , y (cid:48) ) | x(cid:48) ∈ XU ; mQ (x(cid:48) ) ≥ θ∗ ∧ y (cid:48) = sgn(H (x(cid:48) ))}
(5) ZU\ ← ZU\ ∪ S, XU = XU \S
(6) Learn a classiﬁer H by optimizing a global loss function on
Z(cid:96) and ZU\
until XU is empty or that there are no adds to ZU\ ;
Output The ﬁnal classiﬁer H

Figure 1: Self-learning algorithm (SLA∗ )

4 Experiments and Results

(cid:88)
(cid:88)
In our experiments, we employed a Boosting algorithm optimizing the following exponential loss2 as the
baseline learner (line (6), ﬁgure 1):
Lc (H, Z(cid:96) , ZU\ ) =
e−yH (x) +
x∈Z(cid:96)
x(cid:48)∈ZU\
(cid:80)
(cid:80)
2Bennett et al. [1] have shown that the minimization of (19) allows to reach a local minima of the margin loss
x(cid:48)∈ZU\ e|H (x(cid:48) )| .
function LM (H, Z(cid:96) , ZU\ ) = 1
e−yH (x) + 1|ZU\ |
x∈Z(cid:96)
l

e−y (cid:48)H (x(cid:48) )

1
|ZU\ |

1
l

(19)

Where H = (cid:80)
t αtht is a linear weighted sum of decision stumps ht which are uniquely deﬁned by an
input feature jt ∈ {1, . . . , d} and a threshold λt as:
ht (x) = 2[[ϕjt (x) > λt ]] − 1
With ϕj (x) the j th feature characteristic of x. Within this setting, the Gibbs classiﬁer is deﬁned as a
t=1 according to Q such that ∀t, PQ (ht ) = |αt |(cid:80)
random choice from the set of baseline classiﬁers {ht}T
t |αt | .
Accordingly the Bayes classiﬁer is simply the weighted voting classiﬁer BQ = sign(H ). Although the
self-learning model (SLA∗ ) is an inductive algorithm we carried out experiments in a transductive setting
in order to compare results with the transductive SVM of Joachims [7] and the self-learning algorithm
(SLA) described in [11, Figure 6]. For the latter, after training a classiﬁer H on Z(cid:96) (ﬁgure 1, step 1) we
ﬁxed different margin thresholds considering the lowest and the highest output values of H over the labeled
training examples. We evaluated the performance of the algorithms on 4 collections from the benchmark
data sets3 used in [3] as well as 2 data sets from the UCI repository [2]. In this case, we chose sets large
enough for reasonable labeled/unlabeled partitioning, and that represent binary classiﬁcation problems.
Each experiment was repeated 20 times by partitioning, at each time, the data set into two random labeled
and unlabeled training sets.

Table 1: Means and standard deviations of the classiﬁcation error on unlabeled training data over the 20
trials for each data set. d denotes the dimension, l and u refer respectively to the number of labeled and
unlabeled examples in each data set.

l + u
Dataset
d
l
241 1500 10
CO I L2
241 1500 10
D IG I T
241 1500 10
G241c
241 1500 10
USPS
P IMA 8
768
10
569
WDBC 30
10

SLA
.302↓±.042
.201↓±.038
.314↓±.037
.342↓±.024
.379↓±.026
.168↓±.016

SLA∗
.255±.019
.149±.012
.248±.018
.278↓±.022
.305±.021
.124±.011

TSVM
.286↓±.031
.156±.014
.252±.021
.261±.019
.318↓±.018
.141↓±.016

SLA
l
100 .148↓±.015
.091↓±.01
100
100 .201↓±.017
100 .114↓±.012
.284↓±.019
50
.112↓±.011
50

SLA∗
.134±.011
.071±.005
.191±.014
.112↓±.012
.266±.018
.079±.007

TSVM
.152↓±.016
.087↓±.009
.196±.022
.103±.011
.276±.021
.108↓±.01

For each data set, means and standard deviations of the classiﬁcation error on unlabeled training data over
the 20 trials are shown in Table 1 for 2 different splits of the labeled and unlabeled sets. The symbol ↓
indicates that performance is signiﬁcantly worse than the best result, according to a Wilcoxon rank sum test
used at a p-value threshold of 0.01 [8]. In addition, we show in ﬁgure 2 the evolutions on the CO I L2 , D IG I T
and USPS data sets of the classiﬁcation and both risks of the Gibbs classiﬁer (on the labeled and unlabeled
training sets) for different number of rounds in the SLA∗ algorithm. These ﬁgures are obtained from one of
the 20 trials that we ran for these collections.
The most important conclusion from these empirical results is that for all data sets, the self-learning al-
gorithm becomes competitive when the margin threshold is found automatically rather than if it is ﬁxed
manually. The augmented self-learning algorithm achieves performance statistically better or equivalent to
that of TSVM in most cases, while it outperforms the initial method over all runs.

Figure 2: Classiﬁcation error, train and test Gibbs errors with respect to the iterations of the SLA∗ algorithm
for a ﬁxed number of labeled training data l = 10.

3 http://www.kyb.tuebingen.mpg.de/ssl-book/benchmarks.html

In SLA∗ the automatic choice of the margin-threshold has the effect to select, at the ﬁrst rounds of the
algorithm, many unlabeled examples for which their class labels can be predicted with high conﬁdence by
the voted classiﬁer. The exponential fall of the classiﬁcation rate in ﬁgure 2 can be explained by the addition
of these highly informative pseudo-labeled examples at the ﬁrst steps of the learning process (ﬁgure 1).
After this fall, few examples are added because the learning algorithm does not increase the margin on
unlabeled data. Hence, the number of additional pseudo-labeled examples decreases resulting in a plateau
in the classiﬁcation error curves in ﬁgure 2. We further notice that the error of the Gibbs classiﬁer on labeled
data increases fastly to a stationary error point and that on the unlabeled examples does not vary in time.

5 Conclusions

The contribution of this paper is two fold. First, we proposed a bound on the risk of the voted classiﬁer
using the margin distribution of unlabeled examples and an estimation of the risk of the Gibbs classiﬁer.
We have shown that our bound is a good approximation of the true risk when the errors of the associated
Gibbs classiﬁer can accurately be estimated and that the voted classiﬁer makes most its errors on low margin
examples.
The proof of the bound passed through a second bound on the joint probability that the voted classiﬁer
makes an error and that the margin is above a given threshold. This tool led to the conditional probability
that the voted classiﬁer makes an error knowing that the margin is above a given threshold. We showed that
the search of a margin threshold minimizing this conditional probability can be obtained by analytically
solving a linear program.
This resolution conducted to our second contribution which is to ﬁnd automatically the margin threshold in
a self-learning algorithm. Empirical results on a number of data sets have shown that the adaptive threshold
allows to enhance the performance of a self-learning algorithm.

References

[1] Bennett, K., Demiriz, A. & Maclin, R. (2002) Expoliting unlabeled data in ensemble methods. In Proc. ACM Int.
Conf. Knowledge Discovery and Data Mining, 289-296.
[2] Blake, C., Keogh, E. & Merz, C.J. (1998) UCI repository of machine learning databases. University of California,
Irvine. [on-line] http://www.ics.uci.edu/ mlearn/MLRepository.html
[3] Chapelle, O., Sch ¨olkopf, B. & Zien, A. (2006) Semi-supervised learning. MA: MIT Press.
[4] Derbeko, P., El-Yaniv, R. & Meir, R. (2004) Explicit learning curves for transduction and application to clustering
and compression algorithms. Journal of Artiﬁcial Intelligence Research 22:117-142.
In First International Workshop on Multiple
[5] Dietterich, T.G. (2000) Ensemble Methods in Machine Learning.
Classiﬁer Systems, 1-15.
[6] Grandvalet, Y. & Bengio, Y. (2005) Semi-supervised learning by entropy minimization. In Advances in Neural
Information Processing Systems 17, 529-536. Cambridge, MA: MIT Press.
[7] Joachims, T. (1999) Transductive Inference for Text Classiﬁcation using Support Vector Machines. In Proceedings
of the 16th International Conference on Machine Learning, 200-209.
[8] Lehmann, E.L. (1975) Nonparametric Statistical Methods Based on Ranks. McGraw-Hill, New York.
In Proc. od the 16th Annual Conference on
[9] McAllester, D. (2003) Simpliﬁed PAC-Bayesian margin bounds.
Learning Theory, Lecture Notes in Artiﬁcial Intelligence, 203-215.
[10] Seeger, M. (2002) Learning with labeled and unlabeled data. Technical report, Institute for Adaptive and Neural
Computation, University of Edinburgh.
[11] T ¨ur, G., Hakkani-T ¨ur, D.Z. & Schapire, R.E. (2005) Combining active and semi-supervised learning for spoken
language understanding. Journal of Speech Communication 45(2):171-186.
[12] Vittaut, J.-N., Amini, M.-R. & Gallinari, P. (2002) Learning Classiﬁcation with Both Labeled and Unlabeled Data.
In European Conference on Machine Learning, 468-476.


High-dimensional support union recovery in
multivariate regression

Guillaume Obozinski
Department of Statistics
UC Berkeley
gobo@stat.berkeley.edu

Martin J. Wainwright
Department of Statistics
Dept. of Electrical Engineering and Computer Science
UC Berkeley
wainwright@stat.berkeley.edu
Michael I. Jordan
Department of Statistics
Department of Electrical Engineering and Computer Science
UC Berkeley
jordan@stat.berkeley.edu

Abstract
We study the behavior of block (cid:96)1 /(cid:96)2 regularization for multivariate regression,
where a K -dimensional response vector is regressed upon a ﬁxed set of p co-
variates. The problem of support union recovery is to recover the subset of
covariates that are active in at least one of the regression problems. Study-
ing this problem under high-dimensional scaling (where the problem parame-
ters as well as sample size n tend to inﬁnity simultaneously), our main result
is to show that exact recovery is possible once the order parameter given by
θ(cid:96)1 /(cid:96)2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is
the sample size, p is the ambient dimension of the regression model, s is the size
of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a
combination of the sparsities and overlaps of the K -regression coefﬁcient vectors
that constitute the model. This sparsity-overlap function reveals that block (cid:96)1 /(cid:96)2
regularization for multivariate regression never harms performance relative to a
naive (cid:96)1 -approach, and can yield substantial improvements in sample complexity
(up to a factor of K ) when the regression vectors are suitably orthogonal rela-
tive to the design. We complement our theoretical results with simulations that
demonstrate the sharpness of the result, even for relatively small problems.

1 Introduction
A recent line of research in machine learning has focused on regularization based on block-structured
norms. Such structured norms are well motivated in various settings, among them kernel learn-
ing [3, 8], grouped variable selection [12], hierarchical model selection [13], simultaneous sparse
approximation [10], and simultaneous feature selection in multi-task learning [7]. Block-norms that
compose an (cid:96)1 -norm with other norms yield solutions that tend to be sparse like the Lasso, but the
structured norm also enforces blockwise sparsity, in the sense that parameters within blocks are
more likely to be zero (or non-zero) simultaneously.
The focus of this paper is the model selection consistency of block-structured regularization in the
setting of multivariate regression. Our goal is to perform model or variable selection, by which we
mean extracting the subset of relevant covariates that are active in at least one regression. We refer
to this problem as the support union problem. In line with a large body of recent work in statistical
machine learning (e.g., [2, 9, 14, 11]), our analysis is high-dimensional in nature, meaning that we
allow the model dimension p (as well as other structural parameters) to grow along with the sample
size n. A great deal of work has focused on the case of ordinary (cid:96)1 -regularization (Lasso) [2, 11, 14],
showing for instance that the Lasso can recover the support of a sparse signal even when p (cid:192) n.

1

Some more recent work has studied consistency issues for block-regularization schemes, including
classical analysis (p ﬁxed) of the group Lasso [1], and high-dimensional analysis of the predic-
tive risk of block-regularized logistic regression [5]. Although there have been various empirical
demonstrations of the beneﬁts of block regularization, the generalizations of the result of [11] ob-
tained by [6, 4] fail to capture the improvements observed in practice. In this paper, our goal is to
understand the following question: under what conditions does block regularization lead to a quan-
tiﬁable improvement in statistical efﬁciency, relative to more naive regularization schemes? Here
statistical efﬁciency is assessed in terms of the sample complexity, meaning the minimal sample size
n required to recover the support union; we wish to know how this scales as a function of prob-
lem parameters. Our main contribution is to provide a function quantifying the beneﬁts of block
regularization schemes for the problem of multivariate linear regression, showing in particular that,
under suitable structural conditions on the data, the block-norm regularization we consider never
harms performance relative to naive (cid:96)1 -regularization and can lead to substantial gains in sample
complexity.
More speciﬁcally, we consider the following problem of multivariate linear regression: a group of
K scalar outputs are regressed on the same design matrix X ∈ Rn×p . Representing the regression
coefﬁcients as a p × K matrix B ∗ , the regression model takes the form
Y = X B ∗ + W,
(1)
where Y ∈ Rn×K and W ∈ Rn×K are matrices of observations and zero-mean noise respectively
(cid:111)
(cid:110)
and B ∗ has columns β ∗(1) , . . . , β ∗(K ) which are the parameter vectors of each univariate regression.
We are interested in recovering the unionofthesupports of individual regressions, more speciﬁcally
∗(k)
we would like to recover S = ∪k Sk . The Lasso is often
(cid:54)= 0
i ∈ {1, . . . , p}, β
if Sk =
i
presented as a relaxation of the so-called (cid:96)0 regularization, i.e., the count of the number of non-zero
parameter coefﬁcients, an intractable non-convex function. More generally, block-norm regulariza-
tions can be thought of as the relaxation of a non-convex regularization which counts the number of
∗(k)
is non-zero. More
covariates i for which at least one of the univariate regression parameters β
p(cid:88)
i denote the ith row of B ∗ , and deﬁne, for q ≥ 1,
i
speciﬁcally, let β ∗
= |{i ∈ {1, . . . , p}, (cid:107)β ∗
(cid:107)B ∗ (cid:107)(cid:96)0 /(cid:96)q
i (cid:107)q > 0}|
i=1
All (cid:96)0 /(cid:96)q norms deﬁne the same function, but differ conceptually in that they lead to different (cid:96)1 /(cid:96)q
relaxations. In particular the (cid:96)1/(cid:96)1 regularization is the same as the usual Lasso. The other conceptu-
ally most natural block-norms are (cid:96)1/(cid:96)2 and (cid:96)1 /(cid:96)∞ . While (cid:96)1/(cid:96)∞ is of interest, it seems intuitively
to be relevant essentially to situations where the support is exactly the same for all regressions, an
In the current paper, we focus on the (cid:96)1 /(cid:96)2 case and consider the estimator (cid:98)B obtained by solving
assumption that we are not willing to make.
(cid:189)
(cid:190)
the following disguised second-order cone program:
1
(cid:80)
|||Y − X B |||2
F + λn (cid:107)B (cid:107)(cid:96)1 /(cid:96)2
2n
where |||M |||F : = (
ij )1/2 denotes the Frobenius norm. We study the support union problem
i,j m2
under high-dimensional scaling, meaning that the number of observations n, the ambient dimen-
sion p and the size of the union of supports s can all tend to inﬁnity. The main contribution of
this paper is to show that under certain technical conditions on the design and noise matrices, the
model selection performance of block-regularized (cid:96)1 /(cid:96)2 regression (2) is governed by the control
parameter θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) : =
2 ψ(B∗ ,ΣSS ) log(p−s) , where n is the sample size, p is the ambient
n
dimension, s = |S | is the size of the union of the supports, and ψ(·) is a sparsity-overlap function
deﬁned below. More precisely, the probability of correct support union recovery converges to one for
all sequences (n, p, s, B ∗ ) such that the control parameter θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) exceeds a ﬁxed critical
threshold θcrit < +∞. Note that θ(cid:96)1 /(cid:96)2 is a measure of the sample complexity of the problem—that
is, the sample size required for exact recovery as a function of the problem parameters. Whereas
the ratio (n/ log p) is standard for high-dimensional theory on (cid:96)1 -regularization (essentially due to
covering numberings of (cid:96)1 balls), the function ψ(B ∗ , ΣSS ) is a novel and interesting quantity, which

(cid:107)B ∗ (cid:107)(cid:96)1 /(cid:96)q

min
B∈Rp×K

=

(cid:107)β ∗
i (cid:107)q

,

(2)

and

2

measures both the sparsity of the matrix B ∗ , as well as the overlap between the different regression
tasks (columns of B ∗ ).
In Section 2, we introduce the models and assumptions, deﬁne key characteristics of the problem and
state our main result and its consequences. Section 3 is devoted to the proof of this main result, with
most technical results deferred to the appendix. Section 4 illustrates with simulations the sharpness
of our analysis and how quickly the asymptotic regime arises.

1.1 Notations
For a (possibly random) matrix M ∈ Rp×K , and for parameters 1 ≤ a ≤ b ≤ ∞, we distinguish
(cid:190) 1
(cid:182) a
(cid:189) p(cid:88)
(cid:181) K(cid:88)
the (cid:96)a /(cid:96)b block norms from the (a, b)-operator norms, deﬁned respectively as
(cid:107)M (cid:107)(cid:96)a /(cid:96)b
|||M |||a, b : = sup
(cid:107)M x(cid:107)a ,
b
a
(cid:80)
(cid:107)x(cid:107)b=1
i=1
k=1
although (cid:96)∞ /(cid:96)p norms belong to both families (see Lemma B.0.1). For brevity, we denote the
spectral norm |||M |||2, 2 as |||M |||2 , and the (cid:96)∞ -operator norm |||M |||∞, ∞ = maxi
j |Mij | as |||M |||∞ .

|mik |b

(3)

: =

and

2 Main result and some consequences

The analysis of this paper applies to multivariate linear regression problems of the form (1), in which
the noise matrix W ∈ Rn×K is assumed to consist of i.i.d. elements Wij ∼ N (0, σ2 ). In addition,
we assume that the measurement or design matrices X have rows drawn in an i.i.d. manner from a
zero-mean Gaussian N (0, Σ), where Σ (cid:194) 0 is a p × p covariance matrix.
Suppose that we partition the full set of covariates into the support set S and its complement S c , with
|S | = s, |S c | = p − s. Consider the following block decompositions of the regression coefﬁcient
(cid:184)
(cid:183)
(cid:183)
(cid:184)
matrix, the design matrix and its covariance matrix:
B ∗
ΣSS
ΣSS c
B ∗ =
, X = [XS XS c ] ,
and Σ =
B ∗
S
.
ΣS c S ΣS c S c
S c
i to denote the ith row of B ∗ , and assume that the sparsity of B ∗ is assessed as follows:
We use β ∗
(A0) Sparsity: The matrix B ∗ has row support S : = {i ∈ {1, . . . , p} | β ∗
i (cid:54)= 0}, with s = |S |.
In addition, we make the following assumptions about the covariance Σ of the design matrix:
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)ΣS c S (ΣSS )−1
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)∞ ≤ 1 − γ .
(A1) Bounded eigenspectrum: There exist a constant Cmin > 0 (resp. Cmax < +∞) such that all
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)∞ ≤ Dmax .
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(ΣSS )−1
eigenvalues of ΣSS (resp. Σ) are greater than Cmin (resp. smaller than Cmax ).
(A2) Mutual incoherence: There exists γ ∈ (0, 1] such that
(A3) Self incoherence: There exists a constant Dmax such that
Assumption A1 is a standard condition required to prevent excess dependence among elements of
the design matrix associated with the support S . The mutual incoherence assumption A2 is also
well known from previous work on model selection with the Lasso [10, 14]. These assumptions are
trivially satisﬁed by the standard Gaussian ensemble (Σ = Ip ) with Cmin = Cmax = Dmax = γ = 1.
More generally, it can be shown that various matrix classes satisfy these conditions [14, 11].

2.1 Statement of main result
eter λn > 0, thereby obtaining a solution (cid:98)B = (cid:98)B (λn ). Use this solution to compute an estimate
With the goal of estimating the union of supports S , our main result is a set of sufﬁcient conditions
(cid:111)
(cid:110)
using the following procedure. Solve the block-regularized problem (2) with regularization param-
i ∈ {1, . . . , p} | (cid:98)βi (cid:54)= 0
of the support union as (cid:98)S ( (cid:98)B ) : =
deﬁned if the solution (cid:98)B is unique, and as part of our analysis, we show that the solution (cid:98)B is indeed
. This estimator is unambiguously
unique with high probability in the regime of interest. We study the behavior of this estimator for a

3

A3. As (n, p, s) tends to inﬁnity, we give conditions on the triplet and properties of B ∗ for which (cid:98)B
sequence of linear regressions indexed by the triplet (n, p, s), for which the data follows the general
model presented in the previous section with deﬁning parameters B ∗ (n) and Σ(n) satisfying A0-
is unique, and such that P[ (cid:98)S = S ] → 1.
The central objects in our main result are the sparsity-overlap function, and the sample complexity
parameter, which we deﬁne here. For any vector βi (cid:54)= 0, deﬁne ζ (βi ) : = βi(cid:107)βi (cid:107)2
. We extend the
function ζ to any matrix BS ∈ Rs×K with non-zero rows by deﬁning the matrix ζ (BS ) ∈ Rs×K
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) ζ (BS )T (ΣSS )−1 ζ (BS )
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
with ith row [ζ (BS )]i = ζ (βi ). With this notation, we deﬁne the sparsity-overlap function ψ(B )
and the sample complexity parameter θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) as
θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) : =
n
ψ(B ) : =
and
2 ψ(B ∗ ) log(p− s) .
2
i (cid:107)2 to denote the minimal (cid:96)2 row-norm of the matrix B ∗
min : = mini∈S (cid:107)β ∗
Finally, we use b∗
S . With
this notation, we have the following result:
Theorem 1. Consider a random design matrix X drawn with i.i.d. N (0, Σ) row vectors, an obser-
(cid:179)(cid:112)
(cid:180)
vation matrix Y speciﬁed by model (1), and a regression matrix B ∗ such that (b∗
min )2 decays strictly
n max {s, log(p − s)}, for any function f (p) → +∞. Suppose that we solve
more slowly than f (p)
the block-regularized program (2) with regularization parameter λn = Θ
f (p) log(p)/n
.
For any sequence (n, p, B ∗ ) such that the (cid:96)1 /(cid:96)2 control parameter θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) exceeds the
γ 2 , then with probability greater than 1 − exp(−Θ(log p)),
(a) the block-regularized program (2) has a unique solution (cid:98)B , and
critical threshold θcrit (Σ) : = Cmax
(b) its support set (cid:98)S ( (cid:98)B ) is equal to the true support union S .
Remarks:
(i) For the standard Gaussian ensemble (Σ = Ip ), the critical threshold is simply
θcrit (Σ) = 1. (ii) A technical condition that we require on the regularization parameter is
λ2
→ ∞
nn
log(p − s)
which is satisﬁed by the choice given in the statement.

(5)

(4)

2.2 Some consequences of Theorem 1

It is interesting to consider some special cases of our main result. The simplest special case is the
univariate regression problem (K = 1), in which case the function ζ (β ∗ ) outputs an s-dimensional
sign vector with elements z ∗
i = sign(β ∗
i ), so that ψ(β ∗ ) = z ∗ T (ΣSS )−1 z ∗ = Θ(s). Consequently,
the order parameter of block (cid:96)1 /(cid:96)2 -regression for univariate regresion is given by Θ(n/(2s log(p −
s)), which matches the scaling established in previous work on the Lasso [11].
More generally, given our assumption (A1) on ΣSS , the sparsity overlap ψ(B ∗ ) always lies in the
K—that is, B ∗
]. At the most pessimistic extreme, suppose that B ∗ : = β ∗(cid:126)1 T
interval [
s
s
,
consists of K copies of the same coefﬁcient vector β ∗ ∈ Rp , with support of cardinality |S | = s.
KCmax
Cmin
√
K , from which we see that ψ(B ∗ ) = z ∗ T (ΣSS )−1 z ∗ , with
We then have [ζ (B ∗ )]ij = sign(β ∗
i )/
z ∗ again the s-dimensional sign vector with elements z ∗
i = sign(β ∗
i ), so that there is no beneﬁt in
sample complexity relative to the naive strategy of solving separate Lasso problems and construct-
ing the union of individually estimated supports. This might seem a pessimistic result, since under
model (1), we essentially have K n observations of the coefﬁcient vector β ∗ with the same design
matrix but K independent noise realizations. However, the thresholds as well as the rates of conver-
gence in high-dimensional results such as Theorem 1 are not determined by the noise variance, but
rather by the number of interfering variables (p − s).
At the most optimistic extreme, consider the case where ΣSS = Is and (for s > K ) suppose that
B ∗ is constructed such that the columns of the s × K matrix ζ (B ∗ ) are all orthogonal and of equal
length. Under this condition, we have

4

Corollary 1 (Orthonormal tasks). If the columns of the matrix ζ (B ∗ ) are all orthogonal with equal
length and ΣSS = Is×s then the block-regularized problem (2) succeeds in union support recovery
K log(p − s)) is larger than 1.
once the sample complexity parameter n/(2 s
For the standard Gaussian ensemble, it is known [11] that the Lasso fails with probability one for
all sequences such that n < (2 − ν )s log(p − s) for any arbitrarily small ν > 0. Consequently,
Corollary 1 shows that under suitable conditions on the regression coefﬁcient matrix B ∗ , (cid:96)1 /(cid:96)2 can
provides a K -fold reduction in the number of samples required for exact support recovery.
As a third illustration, consider, for ΣSS = Is×s , the case where the supports Sk of individual
regression problems are all disjoint. The sample complexity parameter for each of the individual
Lassos is n/(2sk log(p − sk )) where |Sk | = sk , so that the sample size required to recover the
support union from individual Lassos scales as n = Θ(maxk [sk log(p − sk )]). However, if the
S ) are orthogonal, and Z ∗
supports are all disjoint, then the columns of the matrix Z ∗
S = ζ (B ∗
TZ ∗
S =
diag(s1 , . . . , sK ) so that ψ(B ∗ ) = maxk sk and the sample complexity is the same. In other words,
S
even though there is no sharing of variables at all there is surprisingly no penalty from regularizing
jointly with the (cid:96)1 /(cid:96)2 -norm. However, this is not always true if ΣSS (cid:54)= Is×s and in many situations
(cid:96)1 /(cid:96)2 -regularization can have higher sample complexity than separate Lassos.
In addition to previous notations, the proofs use the shorthands: (cid:98)ΣSS = 1
S XS , (cid:98)ΣS c S = 1
3 Proof of Theorem 1
and ΠS = XS ( (cid:98)ΣSS )−1X T
n X T
n X T
S c XS
S denotes the orthogonal projection onto the range of XS .
High-level proof outline: At a high level, our proof is based on the notion of what we refer to as
with the subgradient of the (cid:96)1 /(cid:96)2 norm. We then construct a primal matrix (cid:98)B along with a dual
a primal-dual witness: we ﬁrst formulate the problem (2) as a second-order cone program (SOCP),
matrix (cid:98)Z such that, under the conditions of Theorem 1, with probability converging to 1:
with the same primal variable B as in (2) and a dual variable Z whose rows coincide at optimality
(a) The pair ( (cid:98)B , (cid:98)Z ) satisﬁes the Karush-Kuhn-Tucker (KKT) conditions of the SOCP.
not have a unique solution a priori, a strict feasibility condition satisﬁed by the dual variables (cid:98)Z
In spite of the fact that for general high-dimensional problems (with p (cid:192) n), the SOCP need
guarantees that (cid:98)B is the unique optimal solution of (2).
(b)
(c) The support union ˆS of (cid:98)B is identical to the support union S of B ∗ .
an optimal primal-dual pair for which the primal solution (cid:98)B correctly recovers the support set S :
At the core of our constructive procedure is the following convex-analytic result, which characterizes
Lemma 1. Suppose that there exists a primal-dual pair ( (cid:98)B , (cid:98)Z ) that satisfy the conditions:
(cid:98)ZS = ζ ( (cid:98)BS )
(cid:98)ΣSS ( (cid:98)BS − B ∗
S W = −λn (cid:98)ZS
(cid:176)(cid:176)(cid:176)(cid:176)
(cid:176)(cid:176)(cid:176)(cid:176) (cid:98)ΣS c S ( (cid:98)BS − B ∗
(cid:176)(cid:176)(cid:176) (cid:98)ZS c
(cid:176)(cid:176)(cid:176)
S ) − 1
X T
n
S ) − 1
(cid:98)BS c = 0.
: =
(6c)
X T
S c W
λn
< λn
n
(cid:96)∞ /(cid:96)2
(cid:96)∞ /(cid:96)2
Then ( (cid:98)B , (cid:98)Z ) is the unique optimal solution to the block-regularized problem, with (cid:98)S ( (cid:98)B ) = S by
(6d)
Appendix A proves Lemma 1, with the strict feasibility of (cid:98)ZS c given by (6c) to certify uniqueness.
construction.
Based on Lemma 1, we construct the primal dual pair ( (cid:98)B , (cid:98)Z ) as follows. First, we set (cid:98)BS c = 0, to
3.1 Construction of primal-dual witness
satisfy condition (6d). Next, we obtain the pair ( (cid:98)BS , (cid:98)ZS ) by solving a restricted version of (2):
(cid:40)
(cid:41)
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)Y − X
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)2
(cid:183)
(cid:98)BS = arg min
1
+ λn(cid:107)BS (cid:107)(cid:96)1 /(cid:96)2
BS
.
0S c
BS ∈Rs×K
2n
F
5

(6b)

(6a)

(7)

.

: =

(9)

(10)

(11)

Vj

: = X T
j

Since s < n, the empirical covariance (sub)matrix (cid:98)ΣSS = 1
has a unique optimum (cid:98)BS . We then choose (cid:98)ZS to be the solution of equation (6b). Since any
S XS is strictly positive deﬁnite
n X T
such matrix (cid:98)ZS is also a dual solution to the SOCP (7), it must be an element of the subdifferential
with probability one, which implies that the restricted problem (7) is strictly convex and therefore
∂ (cid:107) (cid:98)BS (cid:107)(cid:96)1 /(cid:96)2 . It remains to show that this construction satisﬁes conditions (6a) and (6c). In order to
satisfy condition (6a), it sufﬁces to show that (cid:98)βi (cid:54)= 0, i ∈ S . From equation (6b) and since (cid:98)ΣSS is
(cid:184)
(cid:183)
(cid:179) (cid:98)ΣSS
(cid:180)−1
− λn (cid:98)ZS
( (cid:98)BS − B ∗
invertible, we may solve as follows
X T
S W
For any row i ∈ S , we have (cid:107) (cid:98)βi(cid:107)2 ≥ (cid:107)β ∗
S ) =
= : US .
(8)
n
i (cid:107)2 − (cid:107)US (cid:107)(cid:96)∞ /(cid:96)2 . Thus, it sufﬁces to show that the
(cid:189)
(cid:190)
following event occurs with high probability
(cid:107)US (cid:107)(cid:96)∞ /(cid:96)2 ≤ 1
E (US )
2 b∗
to show that no row of (cid:98)BS is identically zero. We establish this result later in this section.
: =
min
Turning to condition (6c), by substituting expression (8) for the difference ( (cid:98)BS − B ∗
(cid:181)
(cid:182)
S ) into equa-
tion (6c), we obtain a (p − s) × K random matrix VS c , whose row j ∈ S c is given by
( (cid:98)ΣSS )−1 (cid:98)ZS
[ΠS − In ] W
− λn
XS
(cid:110)
(cid:111)
n
n
In order for condition (6c) to hold, it is necessary and sufﬁcient that the probability of the event
E (VS c )
(cid:107)VS c (cid:107)(cid:96)∞ /(cid:96)2 < λn
converges to one as n tends to inﬁnity.
Correct inclusion of supporting covariates: We begin by analyzing the probability of E (US ).
(cid:179)(cid:112)
(cid:179)(cid:112)
(cid:180)
(cid:179)
(cid:180)(cid:180)
Lemma 2. Under assumption A3 and conditions (5) of Theorem 1, with probability 1 −
exp(−Θ(log s)), we have
(cid:107)US (cid:107)(cid:96)∞ /(cid:96)2 ≤ O
Dmax + O
(log s)/n
+ λn
s2 /n
.
This lemma is proved in in the Appendix. With the assumed scaling n = Ω (s log(p − s)), and the
f (p) max{s,log(p−s)}
min )2 ≥ 1
min , which we write explicitly as (b∗
assumed slow decrease of b∗
for
ε2
n
some εn → 0, we have
n
(cid:107)US (cid:107)(cid:96)∞ /(cid:96)2
b∗
min
so that the conditions of Theorem 1 ensure that E (US ) occurs with probability converging to one.
Correct exclusion of non-support: Next we analyze the event E (VS c ). For simplicity, in the
following arguments, we drop the index S c and write V for VS c . In order to show that (cid:107)V (cid:107)(cid:96)∞ /(cid:96)2 <
(cid:107)V (cid:107)(cid:96)∞ /(cid:96)2 ≤ 3(cid:88)
λn with probability converging to one, we make use of the decomposition
1
1
(cid:107)E [V | XS ](cid:107)(cid:96)∞ /(cid:96)2 ,
T (cid:48)
T (cid:48)
1 : =
i
λn
λn
i=1
1
1
(cid:107)V − E [V |XS , W ](cid:107)(cid:96)∞ /(cid:96)2 .
(cid:107)E [V |XS , W ] − E [V |XS ](cid:107)(cid:96)∞ /(cid:96)2
T (cid:48)
T (cid:48)
3 : =
2 : =
and
λn
λn
1 ≤ 1 − γ . Under conditions (5) of Theorem 1, T (cid:48)
Lemma 3. Under assumption A2, T (cid:48)
2 = op (1).
(cid:107)V (cid:107)(cid:96)∞ /(cid:96)2 < 1 with high probability, it sufﬁces to show that T (cid:48)
Therefore, to show that 1
3 < γ
λn
with high probability. Until now, we haven’t appealed to the sample complexity parameter
θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ). In the next section, we prove that θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) > θcrit (Σ) implies that T (cid:48)
3 < γ
with high probability.

≤ O(εn ),

where

(12)

6

(13)

and

(cid:107)ξj (cid:107)2
2

(cid:162)
(cid:161)
(cid:162)
(cid:161) (cid:107)Vj − E [Vj | XS , W ](cid:107)2
Lemma 4. Conditionally on W and XS , we have
2 | W, XS
d=
ΣS c | S
j j ξT
j Mn ξj ,
where ξj ∼ N ((cid:126)0K , IK ) and where the K × K matrix Mn = Mn (XS , W ) is given by
(cid:98)Z T
S ( (cid:98)ΣSS )−1 (cid:98)ZS +
1
: = λ2
n2 W T (ΠS − In )W.
n
Mn
n
But the covariance matrix Mn is itself concentrated. Indeed,
(cid:111)
(cid:110)
Lemma 5. Under the conditions (5) of Theorem 1, for any δ > 0, the following event T (δ) has
probability converging to 1:
ψ(B ∗ )
|||Mn |||2 ≤ λ2
T (δ) : =
(1 + δ)
(14)
.
n
n
3 ≥ γ | T (δ)] + P[T (δ)c ], but, from lemma 5,
3 ≥ γ ] ≤ P[T (cid:48)
For any ﬁxed δ > 0, we have P[T (cid:48)
P[T (δ)c ] → 0, so that it sufﬁces to deal with the ﬁrst term.
Given that (ΣS c | S )j j ≤ (ΣS c S c )j j ≤ Cmax for all j , on the event T (δ), we have
(cid:183)
(cid:184)
ψ(B ∗ )
2 ≤ Cmax λ2
j Mn ξj ≤ Cmax |||Mn |||2 max
(cid:107)ξj (cid:107)2
max
max
(ΣS c | S )j j ξT
j∈S c
j∈S c
j∈S c
n
n
1
γ 2
(cid:107)ξj (cid:107)2
2 ≥ 2t∗ (n, B ∗ )
3 ≥ γ | T (δ)] ≤ P
with t∗ (n, B ∗ ) : =
P[T (cid:48)
n
max
ψ(B ∗ ) (1 + δ) .
j∈S c
2
Cmax
(cid:183)
(cid:184)
Finally using the union bound and a large deviation bound for χ2 variates we get the following
condition which is equivalent to the condition of Theorem 1: θ(cid:96)1 /(cid:96)2 (n, p ; B ∗ ) > θcrit (Σ):
→ 0 if t∗ (n, B ∗ ) > (1 + ν ) log(p − s) for some ν > 0.
2 ≥ 2t∗ (n, B ∗ )
(cid:107)ξj (cid:107)2
Lemma 6. P
max
j∈S c
4 Simulations
In this section, we illustrate the sharpness of Theorem 1 and furthermore ascertain how quickly
the predicted behavior is observed as n, p, s grow in different regimes, for two regression tasks
(i.e., K = 2). In the following simulations, the matrix B ∗ of regression coefﬁcients is designed
√
√
ij in {−1/
2} to yield a desired value of ψ(B ∗ ). The design matrix X is
with entries β ∗
2, 1/
√
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) ζ (B ∗ )T ζ (B ∗ )
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
sampled from the standard Gaussian ensemble. Since |β ∗
ij | = 1/
2 in this construction, we have
min = 1. Moreover, since Σ = Ip , the sparsity-overlap ψ(B ∗ ) is simply
S ), and b∗
B ∗
S = ζ (B ∗
2 . From our analysis, the sample complexity parameter θ(cid:96)1 /(cid:96)2 is controlled by the
“interference” of irrelevant covariates, and not by the variance of a noise component.
We consider linear sparsity with s = αp, for α = 1/8, for various ambient model dimen-
sions p ∈ {32, 256, 1024}. For each value of p, we perform simulations varying the sample
size n to match corresponding values of the basic Lasso sample complexity parameter, given
(cid:112)
: = n/(2s log(p − s)), in the interval [0.25, 1.5].
by θLas
In each case, we solve the block-
regularized problem (2) with sample size n = 2θLas s log(p − s) using the regularization parameter
log(p − s) (log s)/n. In all cases, the noise level is set at σ = 0.1.
λn =
For our construction of matrices B ∗ , we choose both p and the scalings for the sparsity so that the
obtained values for s that are multiples of four, and construct the columns Z (1)∗ and Z (2)∗ of the
matrix B ∗ = ζ (B ∗ ) from copies of vectors of length 4. Denoting by ⊗ the usual matrix tensor
product, we consider:
(cid:126)1s , so that the sparsity-overlap is ψ(B ∗ ) = s.
Identical regressions: We set Z (1)∗ = Z (2)∗ = 1√
2
Orthogonal regression: Here B ∗ is constructed with Z (1)∗ ⊥ Z (2)∗ , so that ψ(B ∗ ) = s
2 , the most
(cid:126)1s/2 ⊗ (1, −1)T .
favorable situation. To achieve this, we set Z (1)∗ = 1√
(cid:126)1s and Z (2)∗ = 1√
2
2
Intermediate angles: In this intermediate case, the columns Z (1)∗ and Z (2)∗ are at a 60◦ angle,
(cid:126)1s/4 ⊗ (1, 1, 1, −1)T .
(cid:126)1s and Z (2)∗ = 1√
4 s. We set Z (1)∗ = 1√
which leads to ψ(B ∗ ) = 3
2
2
Figure 1 shows plots of all three cases and the reference Lasso case for the three different values
of the ambient dimension and the two types of sparsity described above. Note how the curves all
undergo a threshold phenomenon, with the location consistent with the predictions of Theorem 1.

7

Figure 1. Plots of support recovery probability P[ (cid:98)S = S ] versus the basic (cid:96)1 control parameter
θLas=n/[2s log(p − s)] for linear sparsity s=p/8, and for increasing values of p ∈ {32, 256, 1024}
from left to right. Each graph shows four curves corresponding to the case of independent (cid:96)1 regular-
ization (pluses), and for (cid:96)1 /(cid:96)2 regularization, the cases of identical regression (crosses), intermediate
angles (nablas), and orthogonal regressions (squares). As plotted in dotted vertical lines, Theorem 1
predicts that identical case should succeed for θLas>1 (same as ordinary Lasso), intermediate case for
θLas>0.75, and orthogonal case for θLas>0.50. The shift of these curves conﬁrms this prediction.
5 Discussion
We studied support union recovery under high-dimensional scaling with the (cid:96)1 /(cid:96)2 regularization,
and shown that its sample complexity is determined by the function ψ(B ∗ ). The latter integrates
the sparsity of each univariate regression with the overlap of all the supports and the discrepancies
between each of the vectors of parameter estimated.
In favorable cases, for K regressions, the
sample complexity for (cid:96)1 /(cid:96)2 is K times smaller than that of the Lasso. Moreover, this gain is not
obtained at the expense of an assumption of shared support over the data.
In fact, for standard
Gaussian designs, the regularization seems “adaptive” in sense that it doesn’t perform worse than
the Lasso for disjoint supports. This is not necessarily the case for more general designs and in some
situations, which need to be characterized in future work, it could do worse than the Lasso.
References
[1] F. Bach. Consistency of the group Lasso and multiple kernel learning. Technical report, INRIA -
D ´epartement d’Informatique, Ecole Normale Sup ´erieure, 2008.
[2] F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In
Proc. Int. Conf. Machine Learning (ICML). Morgan Kaufmann, 2004.
[3] D. Donoho, M. Elad, and V. M. Temlyakov. Stable recovery of sparse overcomplete representations in
the presence of noise. IEEE Trans. Info Theory, 52(1):6–18, January 2006.
[4] H. Liu and J. Zhang. On the (cid:96)1−(cid:96)q regularized regression. Technical Report arXiv:0802.1517v1, Carnegie
Mellon University, 2008.
[5] L. Meier, S. van de Geer, and P. B ¨uhlmann. The group lasso for logistic regression. Technical report,
Mathematics Department, Swiss Federal Institute of Technology Z ¨urich, 2007.
[6] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models.
Electronic Journal of Statistics, 2:605–633, 2008.
[7] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection and joint subspace selection for multiple
classiﬁcation problems. Statistics and Computing, 2009. To appear.
[8] M. Pontil and C.A. Michelli. Learning the kernel function via regularization. Journal of Machine Learning
Research, 6:1099–1125, 2005.
[9] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. SpAM: sparse additive models. In Neural Info.
Proc. Systems (NIPS) 21, Vancouver, Canada, December 2007.
[10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Trans.
Info Theory, 52(3):1030–1051, March 2006.
[11] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity using using
(cid:96)1 -constrained quadratic programs. Technical Report 709, Department of Statistics, UC Berkeley, 2006.
[12] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society B, 1(68):4967, 2006.
[13] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute
penalties. Technical report, Statistics Department, UC Berkeley, 2007.
[14] P. Zhao and B. Yu. Model selection with the lasso. J. of Machine Learning Research, pages 2541–2567,
2007.

8

00.511.500.20.40.60.81qP(support correct)p=32  s=p/8=4  L1Z1=Z2Ð (Z1,Z2)=60oZ1^ Z200.511.500.20.40.60.81qP(support correct)p=256  s=p/8=3200.511.500.20.40.60.81qP(support correct)p=1024  s=p/8=128
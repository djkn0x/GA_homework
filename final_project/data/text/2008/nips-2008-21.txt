G o a l - d i r e c t e d   d e c i s i o n  m a k i n g   i n   p r e f r o n t a l
c o r t e x :  A   c om p u t a t i o n a l   f r a m e w o r k

                                                                   James  An
                          Ma tthew  Bo t v in ick    
            Pr inceton  Neu roscience I nstitu te and                         Compu ter  Science Depar tm en t
      Depar tmen t o f  Psycho logy, Pr inceton  Un iversity                    Pr inceton  Un iversi ty
                          Pr inceton , N J  08540
                         Pr inceton , N J  08540
                     ma tthewb@princeton .edu
                           an@princeton .edu

Ab stract

Research   in   an imal  learn ing   and   behav io ral  neu roscience  has   d istingu ished
between   two   fo rms   o f   action   con tro l:   a  hab it-based   fo rm ,  wh ich   relies   on
sto red   actio n   values ,  and   a  goal-d ir ected   fo rm ,  wh ich   fo reca sts   and
compares   act ion   ou tcomes   based   on   a  model  o f   the  env ironmen t .    Wh ile
hab it-based   con tro l  has   been   the  sub ject  o f   ex tensive  compu tational
research ,  th e  compu tational  p r incip les   under ly ing   goal-d irec ted   con tro l  in
an imals   have   so   far   received   less   at ten tion .    In   the  p resen t   paper,  we
advance  a  compu tational  framewo rk   fo r   goal-d irected   con tro l  in   an imals
and   humans .    We  take  th ree   emp ir ically   mo tivated   p o in ts   as   found ing
p remises:  (1 )   Neu rons   in   do rso latera l  p ref ron tal  co r tex   rep r esen t  action
po licies ,  (2 )   Neu rons   in   o rb ito f ron tal  co r tex   rep resen t  rewa rds ,  and   (3 )
Neu ral  compu tation ,  across   domains ,  can   be  app rop r iately   und erstood   as
perfo rming   s tructu red   p robab ilistic  in ference.    On   a  pu rely   compu tational
level,  the  r esu lting   accoun t  relates   closely   to   p rev ious   wo r k   using   Bayesian
in ference  to   so lve  Markov   decision   p rob lems ,  bu t  ex tends   th is   wo rk   by
in troducing   a  new   algo r ithm ,  wh ich   p rovab ly   converges   on   op tim al  p lans .
On   a  cogn itive  and   neu roscien tif ic  level,  the  theo ry   p rov ide s   a  un ify ing
f ramewo rk   fo r   several  d ifferen t  fo rms   o f   goal-d irected   action   selection ,
p lacing   emph asis   on   a  novel  fo rm ,  w i th in   wh ich   o rb ito f ron tal   reward
rep resen tations  d irectly  d r ive po licy  selection .

1

G o a l - d i r e c t e d   a c t i o n   c o n t r o l

In   the  study   o f   human   and   an imal  beh av io r,  it  is   a  lo ng -stand ing   idea   that  reward -based
decision   mak ing   may   rely   on   two   qual itatively   d ifferen t  mechan isms .    In   hab it-based
decision   mak ing ,  stimu li  elicit  ref lex - like  responses ,  shaped   by   past  rein fo rcemen t  [1 ] .    In
goa l-d irected   o r   purposive  decision   mak ing ,  on   the  o ther   hand ,  actions   are  selected   bas ed   on
a  p rospectiv e  consideration   o f  possib le  ou tcomes   and   fu tu re  lines   o f   action   [2 ] . Over   the  p ast
twen ty   years   o r   so ,  the  atten tion   o f   cogn itive  neu roscien tis ts   and   compu tationally   m inded
psycho log ist s   has   tended   to   focus   on   hab it-based   con tro l,  du e  in   large  par t  to   in terest  in
po ten tial  links   between   dopaminerg ic  function   an d   tempo ral-d ifference  algo r ithms   fo r
rein fo rcemen t  learn ing .    However,  a  resu rgence  o f   in terest  in   pu rposive  action   selection   is
now   being   d r iven   by   innovations   in   an imal  behav io r   research ,   wh ich   have  y ielded   powe rfu l
new   behav io r al  assays   [3 ] ,  and   revealed   specif ic  effects   o f   focal  neu ral  damage  on   goal -
d irected  beh av io r  [4 ] .

In   d iscussin g   some  o f   the  relevan t  d ata,  Daw,  N iv   and   Dayan   [5 ]   recen tly   po in ted   ou t  the
close  relationsh ip   between   pu rposive   decision   mak ing ,  as   und erstood   in   the  behav io ra l
sciences ,  an d   model-based   methods   fo r   the  so lu tion   o f   Markov   d ecision   p rob lems   (MDPs) ,
where  action   po licies   are  der ived   f r om   a  jo in t  analysis   o f   a   transition   function   (a  mapp ing

f rom   states   and   actions   to   ou tcomes)   and   a  reward   function   ( a  mapp ing   f rom   states   to
rewards) .    Beyond   th is   impo r tan t  ins igh t,  little  wo rk   has   ye t  been   done  to   character ize  the
compu tations   under ly ing   goal-d irected   action   selection   ( though   see  [6 ,  7 ] ) .    As   d iscu ssed
below,  a  g reat  deal  o f   ev idence  ind icate s   that  pu rposive  action   selection   depends   cr itically   on
a  par ticu lar   reg ion   o f   the  b rain ,  th e  p refron tal  co r tex .    However,  it  is   cu rr en tly   a  cr itical,  and
qu ite open , question  what the relevan t compu tations  w ith in  th is  par t o f  the b rain  migh t be.

O f   cou rse,  the  basic  compu tational  p rob lem   o f   fo rmu lating   an   op timal  po licy   g iven   a  model
o f   an   MDP  has   been   ex tensively   stud ied ,  and   there  is   no   sho r tage  o f   a lgo r ithms   one  migh t
consider   as   po ten tially   relevan t  to   p ref ron tal  function   (e.g .,  value  iteration ,  po licy   iteration ,
backward   ind uction ,  linear   p rog ramming ,  and   o thers) .    Howeve r,  from   a  cog n itive  and
neu roscien ti f ic  perspective,  there  i s   one  app roach   to   so lv in g   MDPs   that  it   seems   par ticu lar ly
appealing   to   consider.    In   par ticu lar,  several  re searchers   have  suggested   methods   fo r   so lv ing
MDPs   th rough   probab ilistic  in ference  [8 -12 ] .    The  in ter est  o f   th is   idea,  in   the   p resen t
con tex t,  der ives   from   a  recen t  movem en t  toward   framing   human   and   an imal  in fo rmation
p rocessing ,  as   well  as   the  under ly in g   neu ral  compu tations , 
in   terms   o f   structu red
p robab ilistic  in ference  [13 ,  14 ] .    G iven   th is   perspective,  it  is   inv iting   to   consider   whethe r
goal-d irected   action   selection ,  and   the  neu ral  mechan isms   th at  under lie  it,  migh t  be
understood  in  those same terms .

One  challeng e  in   investigating   th is   possib ility   is   that  p rev ious   research   fu rn ishes   no   ‘o ff- the-
shelf ’   algo r ithm   fo r   so lv ing   MDPs   th rough   p robab ilistic  in ference   that  bo th   p rovab ly   y ields
op timal  po licies   and   aligns   w ith   wha t  is   known   abou t  action   selection   in   the  b rain .    We
endeavo r   her e  to   star t  f illing   in   th at  gap .    In   the  fo llow in g   section ,  we  in troduce  an   accoun t
o f   how   goal- d irected   action   selectio n   can   be  per fo rmed   based   on   p robab ilisit ic  in ference,
w ith in   a  netwo rk   whose  componen ts   map   g rossly   on to   specif ic  b rain   structu res .    As   pa r t  o f
th is   accoun t ,  we  in troduce  a  new   alg o r ithm   fo r   so lv ing   MDPs   th rough   Bayesian   in ference,
along   w ith   a   convergence  p roo f .     We  then   p resen t  resu lts   f rom   a  set  o f   simu lations
illustrating   how   the  framewo rk   wou ld   accoun t  fo r   a  var iety   o f   behav io ral  phenomena    that
are though t  to  invo lve pu rposive  ac tion  selection .

2

C o m p u t a t i o n a l   m o d e l

As   no ted   ear lier,  the  p refro n tal  co r tex   (PFC)   is   bel ieved   to   p lay   a  p ivo tal  ro le  in   pu rposive
behav io r.  Th is   is   in d icated   by   a  b road   assoc iation   between   p ref ron ta l  lesions   and
impairmen ts   in   goal-d irected   action   in   bo th   humans   (see  [15 ] )   and   an imal s   [4 ] .    S ing le-un it
reco rd ing   an d   o ther   data  suggest  tha t  d ifferen t  secto rs   o f   PFC  make  d istinct  con tr ibu tions .
In   par ticu la r,  neu rons   in   do rso lateral  p ref ron tal   co r tex   (DLPFC)   appear   to   encode  task -
specif ic  map p ings   f rom   stimu li  to   re sponses   (e.g .,  [16 ] ) :  “task   rep resen tations ,”  in   the
language  o f   psycho logy,  o r   “po licies”  in   the  language  o f   d ynamic  p rog ramming .    A lthough
there  is   som e  understand ing   o f   how   p o licy   rep resen tations   in   DLPFC  may   gu ide  action
execu tion   [15 ] ,  little  is   yet  known   abou t  how   thes e  rep resen tations   are  th emselves   selected .
Ou r   most  bas ic  p roposal  is   that  DLPFC  po licy   rep resen tations   are  selected   in   a  p rosp ective,
model-based   fash ion ,  leverag ing   in fo rmation   abou t  action -ou tcome  con tingencies   ( i.e. ,  the
transition   f unction )   and   abou t  the  incen tive  value  associated   w ith   specif ic  ou tcomes   o r   states
( the  reward   function ) .    There  is   ex tensive  ev idence  to   sugge st  that  state- reward   associations
are  rep resen ted   in   ano ther   area  o f   the  PFC,  the  o rb ito f ron ta l  co r tex   (OFC)   [17 ,  18 ] .    As   fo r
the  transition   function ,  although   it   is   clear   that  the  b rain   con tains   detailed   rep re sen tations   o f
action -ou tcome  associations   [19 ] ,  their   anatomical  localization   is   n o t  yet  en tirely   clear.
However,  some  ev idence  suggests   that  the  env iromen tal   effects   o f   sim p le  actions   may   be
rep resen ted   in   in fer io r   f ron to -par ie tal  co r tex   [20 ] ,  and   there  is   also   ev idence  suggest ing   that
med ial tempo ral structu res  may  be im po r tan t in  fo recasting  action  ou tcomes  [21 ] .

As   detailed   in   the  nex t  section ,  ou r   model  assumes   that  po licy   rep resen tations   in   DLPFC,
reward   rep re sen tations   in   OFC,  and   r ep resen tations   o f   states   and   actions   in   o ther   b r ain
reg ions ,  are   coo rd inated   w ith in   a  ne two rk   structu re  that  rep resen ts   their   causal  o r   statistical
in terdependencies ,  and   that po licy   s election   occu rs , w ith in   th is   netwo rk ,  th rough   a  p rocess   o f
p robab ilistic in ference.

2 . 1  

A r c h i t e c t u r e

The  imp lemen tation   takes   the  fo rm  o f   a  d irected  g raph ical mo del  [22 ] , w ith   the  layou t  shown
in   F igu re  1 .     Each   node  rep resen ts   a   d iscrete  random   var iab le.    S ta te  var iab les   ( s) ,

rep resen ting   the  set  o f   m   possib le  wo r ld   states ,  serve  the  ro le  p layed   by   par ietal  and   med ial
tempo ral  co r tices   in   rep resen ting   ac tion   ou tcomes .  Action   var iab les   (a )   rep resen ting   the  set
o f   availab le   actions ,  p lay   the  ro le
o f   h igh - leve l  co r tical  mo to r   areas
invo lved   in   the  p rog ramming   o f
action   sequences .  Po licy  var iab les
((cid:1)) ,  each   rep re-sen tin g   the  set  o f
all 
determin istic 
po licies
associated   w ith   a  specif ic  state,
cap tu re  the  rep resen tational  ro le
o f   DLPFC. 
  Local  and   g lobal
u tility  var iab les ,  d escr ibed   fu r ther
below,  cap tu re  th e  ro le  o f   OFC  in
rep resen ting   incen tive  value.    A
separate set  o f  nodes  is  included  fo r  each  d iscrete time-step  up  to  the p lann ing  ho r izon .

   F ig  1 . Le ft: S ing le-step  decision . R igh t: Sequen tial deci sion .
   Each  time -slice includes a set o f  m  po licy  node s.

)

1
2

(cid:2)
(cid:6)
(cid:3)

p u si
(

) =

(cid:4)
+ 1
(cid:7),
(cid:5)

The  cond itio nal  p robab ilities   associated   w ith   each   var iab le  are  rep resen ted   in   tabu lar   fo rm .
S tate  p robab ilities   are  based   on   the   state  and   action   var iab les   in   the  p reced ing   tim e-step ,  and
thus   encode  the  transition   function .     Action   p robab ilities   d epend   on   the  cu r ren t  sta te  and   its
associated   p o licy   var iab le.    U tilities   depend   on ly   on   the  cu rren t  state.    Rather   than
rep resen ting   reward  magn itude as   a con tinuous   var iab le, we adop t  an  app roach   in trodu ced  by
[23 ] ,  rep resen ting   reward   th rough   the  po ster io r   p robab ility   o f   a   b inary   var iab le  (u ) .    S tates
associated   w ith   large  positive  reward   raise  p (u )   ( i.e,  p (u=1 |s))   near   to   o ne;  states   associated
w ith   large  negative  rewards   reduce  p (u )   to   near   ze ro .      In   the  simu lations   repo r ted   below, we
used  a simp le linear  transfo rmation   to  map  from  scalar  rewar d  values  to  p (u ) :
R si(
rmax
      
In   situation s   invo lv ing   sequen tial  actions ,  expected   retu rns   f rom   d ifferen t  time- steps  must  be
in teg rated   in to   a  g lobal  rep resen tat ion   o f   expected   value.    In   o rder   to   accomp lish   th is ,  we
emp loy   a  techn ique  p roposed   by   [8 ] ,  in troducin g  a  “g lobal”  u tility   var iab le  (uG) .   Like  u ,  th is
is  a b inary   random  var iab le, bu t associated  w ith  a poster io r  p robab ility  determined  as:1
1
N

                                                    
where  N  is   the  numbe r   o f   u   nodes .    The   netwo rk   as   who le  embod ies   a  generative  model  fo r
instrumen tal   action .    The  basic  idea   is   to   use  th is   model  as   a  substrate  fo r   p robab i listic
in ference,  in   o rder   to   ar r ive  at  op t imal  po licies .    There  ar e  th ree  general  methods   fo r
accomp lish in g   th is ,  wh ich   co rrespond   th ree  fo rms   o f   query.    F irst,  a  desired   ou tcome  state
can   be  iden t if ied ,  by   treating   one  o f   the  state  var iab les   (a s   well  as   the  in itial  state  var iab le)
as   observed   (see  [9 ]   fo r   an   app l ication   o f   th is   app roach ) .    Second ,  the  expected   retu rn   fo r
specif ic  p lans   can   be  evaluated   and   compared   by   cond ition ing   on   specif ic  sets   o f   values   over
the  po licy   n odes   (see  [5 ,  21 ] ) .    However,  ou r   focus   here  is   on   a  less   obv iou s   possib ility,
wh ich  is  to  cond ition  d irectly  on  th e u tility  var iab le uG  , as  exp lain ed  nex t.

rmax (cid:1) max j R s j(

p uG(

) =

p(u i )

(cid:1)
i

)

     (1)

     (2)

2 . 2

P o l i c y   s e l e c t i o n   b y   p r o b a b i l i s t i c   i n f e r e n c e :   a n   i t e r a t i v e   a l g o r i t h m

Cooper   [23 ]   in troduced   the  idea  o f   in fer r ing   op timal  decisions   in   in f lu ence  d iag rams   by
treating   u ti lity   nodes   in to   b inary   r andom   var iab les   and   then   cond ition ing   on   these  var iab les .
A lthough   th is   techn ique  has  been   ado p ted   in   some mo re  recen t  wo rk   [9 , 12 ] , we are  awa re o f
no   app lication   that  guaran tees   op tim al  decisions ,  in   the  exp ected -reward   sense,  in   m u lti-step
tasks .    We  in troduce  here  a  simp le  algo r ithm   that  does   fu rn ish   such   a   guaran tee.    The
p rocedu re  is   as   fo llows:  (1 )   In itialize  the  po licy   nodes   w ith   any   set  o f   non -determin istic
p r io rs .  (2 )   Treating   the  in it ial  state  and   uG   as   observed   var iab les   (uG   =  1 ) ,2  use  standar d   belief
                                                  
1 Note that temporal discounting can be incorporated into the framework through minimal
modifications to Equation 2.
2  In the single-action situation, where there is only one  u node, it is this variable that is treated as
observed (u = 1).

p ropagation   (o r   a  comparab le  algo r ithm)   to   in fer   the  poster io r   d istr ibu tions   over   al l  po licy
nodes .    (3 )   Set  the  prior   d istr ibu tio ns   over   the  po licy   nodes   to   the  values   (poster io rs)
ob tained   in   step   2 .    (4 )   Go   to   step   2 .    The  nex t  two   section s   p resen t  p roo fs   o f   mono ton icity
and  convergence fo r  th is  algo r ithm .

2 . 2 . 1 M o n o t o n i c i t y

We show  first  that, at each policy node,  the probability associated with  the optimal policy will  rise
on every iteration. Define (cid:1)* as follows:

* ,(cid:2)+
) , (cid:4) (cid:3)(cid:2) (cid:1) (cid:2)
) > p uG
(
(
                                        p uG (cid:2)
+  is  the current set of probability distributions at all policy nodes on subsequent  time-steps.
where (cid:1)
(Note  that we  assume  here,  for  simplicity,  that  there  is  a  unique  optimal  policy.)  The  objective  is
to establish that:

         (3)

(cid:3)(cid:2) ,(cid:2)+

*

                                                     

*
p (cid:1)t
(

*
) > p (cid:1)t (cid:2)1
(

)

         (4)

where t indexes processing iterations.  The dynamics of the network entail that

                                                    p (cid:1)t(
where (cid:1) represents any value  (i.e., policy) of the decision node being considered.   Substituting  this
into (4) gives

) = p (cid:1)t (cid:2)1 uG
(

         (5)

)

                                                  
From  this  point  on  the  focus  is  on  a  single  iteration,  which  permits  us  to  omit  the  relevant
subscripts.   Applying Bayes’ law to (6) yields

        (6)

* uG
(
p (cid:1)t (cid:2)1

*
) > p (cid:1)t (cid:2)1
(

)

                                                
Canceling, and bringing the denominator up, this becomes

p uG (cid:2)*
) p (cid:2)*
(
(
)
p (cid:2)(
p uG (cid:2)(
(cid:1)
)
)
(cid:2)

> p (cid:2)*
(

)

                                             
Rewriting the left hand side, we obtain

p uG (cid:2)*
(

) >

(cid:1)
(cid:2)

) p (cid:2)(
p uG (cid:2)(
)

                                         
Subtracting and further rearranging:

p uG (cid:2)*
(

(cid:1)
(cid:2)

) p (cid:2)(
)

>

(cid:1)
(cid:2)

) p (cid:2)(
p uG (cid:2)(
)

(cid:1)
                                           
(cid:2)
(cid:6)
p uG (cid:3)*
) (cid:4) p uG (cid:3)*
(
(
(cid:7)

(cid:4)
(cid:5)

                         

                                         

p uG (cid:2)*
(

) (cid:3) p uG (cid:2)(
)

(cid:6)
(cid:7) p (cid:2)(
)

> 0

(cid:8)
(cid:9) p (cid:3)*
)
(

(cid:6)
(cid:7)

p uG (cid:3)*
(

) +

(cid:2)
(cid:5)(cid:3) (cid:1)(cid:3)*
(cid:8)
(cid:9) p
) (cid:4) p uG
)
(cid:5)(cid:3)(

(cid:5)(cid:3)(
)

> 0

(cid:6)
(cid:7)

p uG (cid:3)*
(

(cid:2)
(cid:5)(cid:3) (cid:1)(cid:3)*

) (cid:4) p uG
(cid:5)(cid:3)(
)

(cid:8)
(cid:9) p

(cid:5)(cid:3)(
)

> 0

         (7)

         (8)

         (9)

       (10)

       (11)

       (12)

Note that this last inequality (12) follows from the definition of (cid:1)*.

+.   In particular, the policy (cid:1)* will only be part
Remark:  Of course, the identity of (cid:1)*  depends on (cid:1)
+  is  optimal.   Fortunately,  this  requirement  is
of  a  globally  optimal  plan  if  the  set  of  choices (cid:1)
guaranteed  to  be  met,  as  long  as  no  upper  bound  is  placed  on  the  number  of  processing  cycles.
Recalling  that  we  are  considering  only  finite-horizon  problems,  note  that  for  policies  leading  to
+   is  empty.   Thus (cid:1)*  at  the  relevant  policy  nodes  is  fixed,  and  is
states  with  no  successors, (cid:1)
guaranteed  to be part of  the optimal policy.   The proof above shows  that (cid:1)* will continuously rise.
Once  it  reaches  a  maximum,  (cid:1)*  at  immediately  preceding  decisions  will  perforce  fit  with  the
globally optimal policy.  The process works backward, in the fashion of backward induction.

2 . 2 . 2 C o n v e r g e n c e

Continuing with the same notation, we show now that

                                                 
Note that, if we apply Bayes’ law recursively,

* u G
lim t (cid:3)(cid:1) pt (cid:2)
(

) = 1

pt (cid:1)(cid:3) uG
(
            
Thus,

) =

) p t (cid:1)(cid:3)
p uG (cid:1)(cid:3)
(
(
p i uG(
)

)

=

2

pt (cid:2)1 (cid:1)(cid:3)
p uG (cid:1)(cid:3)
)
(
(
) pt (cid:2)1 uG(
pi uG(
)

)

=

3

pt (cid:2) 2 (cid:1)(cid:3)
p uG (cid:1)(cid:3)
)
(
)
(
) pt (cid:2) 2 uG(
) pt (cid:2)1 uG(
pt uG(

…

)

       (13)

       (14)

) =

p1 (cid:1)(cid:2) uG
(

) p1 (cid:1)(cid:2)
(
p uG (cid:1)(cid:2)
(
p1 (cid:1)(cid:2)
p uG (cid:1)(cid:2)
)
(
(
p1 uG(
) p1 uG(
p2 uG(
)
)
 
and so forth.  Thus, what we wish to prove is

p2 (cid:1)(cid:2) uG
(

) =

)

,

2

)

,

 

p3 (cid:1)(cid:2) uG
(

) =

3

p1 (cid:1)(cid:2)
p uG (cid:1)(cid:2)
)
(
)
(
) p1 uG(
) p2 uG(
p3 uG(

,

)

(15)

                                                 
or, rearranging,

p1 (cid:3)*
(

(cid:1)
)
pt uG(

p uG (cid:3)*
(
(cid:1)
(cid:2)
t =1

)

)

= 1

      (16)

pt uG(
)
)
(
p uG (cid:3)(cid:4)
                                                
Note  that, given  the  stipulated  relationship between p((cid:1)) on  each processing  iteration  and p((cid:1) | uG)
on the previous iteration,

= p1 (cid:3)(cid:4)
(

       (17)

(cid:1)
(cid:2)
t =1

) .

pt uG(

) =

(cid:1)
(cid:2)

p uG (cid:2)(
)

pt (cid:2)(
) =

(cid:1)
(cid:2)

p uG (cid:2)(
)

(
pt (cid:3) 1 (cid:2) uG

) =

(cid:1)
(cid:2)

pt (cid:3)1 (cid:2)(

p uG (cid:2)(

)2
pt (cid:3)1 uG(

)

)

       (18)

                   

p uG (cid:2)(

)3
)4
(cid:1)
(cid:1)
(cid:2)
(cid:2)
=
pt (cid:3)1 uG(
) pt (cid:3) 2 uG(
pt (cid:3)1 uG(
) pt (cid:3) 2 uG(
) pt (cid:3) 3 uG(
                                 
With this in mind, we can rewrite the left hand side product in (17) as follows:
4
3
2

p uG (cid:2)(

pt (cid:3) 1 (cid:2)(
)

pt (cid:3) 1 (cid:2)(
)

…

=

)

)

(cid:3)

p1 (cid:2)(
)

p uG (cid:2)(
)

p uG (cid:2)(
)

(cid:1)
(cid:1)
(cid:1)
p1 uG(
)
(cid:2)
(cid:2)
(cid:2)
) p1 uG(
(
) p1 uG(
(
) p1 uG(
(
)
(
p uG (cid:2)(cid:4)
p uG (cid:2)(cid:4)
p uG (cid:2)(cid:4)
p uG (cid:2)(cid:4)
         
Note  that,  given  (18),  the  numerator  in  each  factor  of  (19)  cancels  with  the  denominator  in  the
subsequent  factor,  leaving only p(uG|(cid:1)*)  in  that denominator. The expression can  thus be  rewritten
as

…        (19)

p uG (cid:2)(
)

) p3 uG(

) p2 uG(

) p2 uG(

p1 (cid:2)(
)

p1 (cid:2)(
)

)

)

)

(cid:3)

(cid:3)

1
(
p uG (cid:2)(cid:4)
                    

)

(cid:3)

1
(
p uG (cid:2)(cid:4)

)

(cid:3)

1
(
p uG (cid:2)(cid:4)

)

(cid:3)

p uG (cid:2)(
)

4

p1 (cid:2)(
)

(cid:1)
(cid:2)

(
p uG (cid:2)(cid:4)

)

)(cid:1)
p uG (cid:3)(
.
(cid:1) p1 (cid:3)(
)
(
)
p uG (cid:3)(cid:4)

=

(cid:2)
(cid:3)

…

 

       (20)

The objective  is  then  to show  that  the above equals p((cid:1)*).    It proceeds directly from  the definition
of (cid:1)* that, for all (cid:1) other than (cid:1)*,

p uG (cid:1)(
)
(
)
p uG (cid:1)(cid:2)
                                                        
Thus,  all  but  one  of  the  terms  in  the  sum  above  approach  zero,  and  the  remaining  term  equals
p1((cid:1)*).  Thus,

       (21)

< 1

                                            

(cid:2)
(cid:3)

)(cid:1)
p uG (cid:3)(
(cid:1)
)
(
p uG (cid:3)(cid:5)

) = p1 (cid:3)(cid:5)
p1 (cid:3)(
(

)

       (22)

3

S i m u l a t i o n s

3 . 1

B i n a r y   c h o i c e

We  beg in   w ith   a  simu lation   o f   a  simp le  incen tive  cho ice  situa tion .    Here,  an   an imal  faces
two   levers .    Pressing   the  lef t  lever   reliab ly   y ields   a  p refe r red   food   (r   =  2 ) ,  the  r igh t  a  less
p refer red   fo od   (r  =  1 ) .    Rep resen ting   these  con tingencie s   in   a  netwo rk   structu red   as   in   F ig .  1
( lef t)   and   emp loy ing   the  iterative  a lgo r ithm   descr ibed   in   section   2 .2   y ields   the  resu lts   in
F igu re  2A .    Shown   here  are  the  poste r io r   p robab ilities   fo r   the  po licies   press   left  and   press
righ t,  along   w ith   the  marg inal  value  o f   p (u   =  1 )   under   these  poster io rs   ( labeled   EV  fo r
expected   value) .    The  dashed   ho r izon tal  line  ind icates   the  expected   value  fo r   the  op timal
p lan , to  wh ich  the model obv iously  converges .

A  key   emp ir ical  assay   fo r   pu rposive  b ehav io r   invo lves   ou tcome  deva lua tion .  Here,  action s
y ield ing   a  p rev iously   valued  ou tcome   are  abandoned   af ter   the   incen tive value  o f   the  ou tcome
is   reduced ,  fo r   examp le  by   pair ing  w ith   an   aversive  even t  (e.g .,  [4 ]) .    To   simu late  th is  w ith in
the  b inary   cho ice  scenar io   just  desc r ibed ,  we  reduced   to   zer o   the  reward   value  o f   th e  food
y ielded   by   the  lef t  lever   ( fL) ,  by   mak ing   the  app rop r iate  change  to   p (u |fL) .    Th is   y ie lded   a
reversal in   lever  cho ice (F ig . 2B) .

Ano ther   sign atu re  o f   pu rposive  actio ns   is   that  they   are  aban doned   when   their   causal
connection   w ith   reward ing   ou tcomes   is   removed   (con tingency  degrada tion ,  see  [4 ]) .    We
simu lated   th is   by   star ting   w ith   the  model  f rom   F ig .  2A  and   chang in g   cond itional
p robab ilitie s   at  s   fo r   t=2   to   ref lect   a  decoup ling   o f   the  left  action   from   the  fL   ou tcome.    Th e
resu lting  behav io r  is  shown  in  F ig . 2C.

 

F ig  2 . S imu lation  resu lts, b inary  cho ice.

3 . 2

S t o c h a s t i c   o u t c o m e s

A  cr itical  aspect  o f   the  p resen t  mode ling   parad igm   is   that  it   y ields   reward -max imizing
cho ices   in   s tochastic  domains ,  a  p ro per ty   that  d istingu ishes   it  f rom   some  o ther   recen t
app roaches   u sing   g raph ical  models   to   do   p lann ing   (e.g .,  [9 ]) .    To   illustrate ,  we  used   the
arch itectu re   in   F igu re  1   ( lef t)   to   s imu late  a  cho ice  between   two   fair   co ins .    A  ‘ lef t’   co in
y ields   $1   fo r   heads ,  $0   fo r   tails;  a  ‘ r igh t’   co in   $2   fo r   heads   bu t  fo r   tails   a  $3   loss .    As
illustrated   in  F ig . 2D , the model max imizes  expected  value b y  op ting  fo r  the lef t co in .

F ig  3 . S imu lation  resu lts, two -step  sequen t ial cho ice.

3 . 3

S e q u e n t i a l   d e c i s i o n

Here,  we  ado p t  the  two -step   T-maze  scenar io   used   by   [24 ]   (F ig .  3A ) .    Rep resen ting   the  task
con tingencie s   in   a  g raph ical  model  b ased   on   the  temp late  f rom   F ig   1   ( r igh t) ,  and   using   the
reward   value s   ind icated   in   F ig .  3A ,  y ields   the  cho ice  behav io r   shown   in   F igu re  3B.
Fo llow ing   [24 ] ,  a  sh if t  in   mo tivational  state  f rom   hunger   to   th irst  can   be   rep resen ted   in   the

g raph ical  mo del  by   chang ing   the  rewa rd   function   (R(cheese)   =  2 ,  R(X )   =  0 ,  R(water )   =  4 ,
R(carro ts)   =  1 ) .    Imposing   th is   chang e  at  the  level  o f   the  u   var iab les   y ields   the  cho ice
behav io r   shown   in   F ig .  3C.    The  mode l  can   also   be  used   to   simu late  effo r t-based   d ecision .
S tar ting   w ith   the  scenar io   in   F ig .  2A ,  we  simu lated   the  inse r tion   o f   an   effo r t-demand ing
scalab le  bar r ier   at  S 2   (R(S 2 )   =  -2 )   by   m ak ing   app rop r iate  change s   p (u |s) .    The  resu lting
behav io r  is   shown  in  F ig . 3D .

A  famous   emp i r ical  demonstration   o f   p u rposive  con tro l  invo lve s   detour   behav ior.  Using   a
maze  like  th e  one  shown   in   F ig .  4A ,  w ith   a  food   reward   p laced   at  s5 ,  To lman   [2 ]   found   that
rats   reacted   to  a barr ier   at  locatio n   A by   tak ing   th e  upper   rou te,  bu t  to   a  bar r ier   at B  by   tak ing
the  longer   lower   rou te.    We  simu lated   th is   exper imen t  by   rep resen ting   the  co r respond ing
transition   and   reward   functions   in   a   g raph ical  model  o f   the  fo rm   shown   in   F ig .  1   ( r igh t) ,3
rep resen ting   the  inser tion   o f   bar r ie rs   by   app rop r iate  change s   to   the  transition   func tion .    The
resu lting  ch o ice behav io r  at the cr i tical junctu re s2  is  shown  in  F ig . 4 .

F ig  4 . S imu lation  resu lts, detou r behav io r. B : No  barri er. C : Barrier  at A . D : Barrier  at B .

Ano ther   clas sic  emp ir ical  demonstrat ion   invo lves   la ten t
learn ing .    Blodgett  [25 ]   allowed   rats   to   exp lo re  the  maze
shown   in   F ig .  5 .    Later   inser tion   o f   a  food   reward   at  s1 3
was   fo llowed   immed iately   by   d ramatic   reductions   in   the
runn ing   time ,  ref lecting   a  reduction   in   en tr ies   in to   b lind
alleys .    We  simu lated   th is   effect  in   a  mo del  based   on   the
temp late  in   F ig .  1   (r igh t) ,  rep resen ting   the  maze  layou t
v ia  an   app ro p r iate  transition   function .    In   the  absence  o f
a  reward   at  s1 2 ,  random   cho ices   occu r red   at  each
in tersection .    However,  setting   R(s1 3 )   =  1   resu lted   in   the
set o f  cho ices  ind icated  by  the heav ier  ar rows  in  F ig . 5 .

4

R e l a t i o n   t o   p r e v i o u s   w o r k

         F ig  5 . Laten t learn ing .

In itial  p rop osals   fo r   how   to   so lve  d ecision   p rob lems   th rough   p robab ilistic  in ference   in
g raph ical  mo dels ,  includ ing   the  idea   o f   encod ing   reward   as   the  poster io r   p robab ility   o f   a
random   u tili ty   var iab le,  were  pu t  fo r th   by   Cooper   [23 ] .    Related   ideas   were  p resen ted   by
Shach ter   and   Peo t  [12 ] ,  includ ing   the  use  o f   nodes   that  in teg rate  in fo rmation   f rom   mu ltip le
u tility   node s .    Mo re  recen tly,  A ttias   [11 ]   and  Verma  and   Rao   [9 ]   have  used   g raph ical  models
to   so lve  sho r test-path   p rob lems ,  lev erag ing   p robab ilistic  rep resen tations   o f   rewards ,  though
no t  in   a  way   that  guaran teed   convergence  on   op t imal  ( reward   max imizing )   p lans .    Mo re
closely   rela ted   to   the  p resen t  resea rch   is   wo rk   by   Toussain t  and   S to rkey   [10 ] ,  emp loy ing   the
EM  algo r ithm .    The  iterative  app roach   we  have  in troduced   her e  has   a  cer tain   resemb lance  to
the  EM  p rocedu re, wh ich   becomes   ev id en t  if   one  v iews   the  po l icy   var iab les   in   ou r  mod els   as
parameters   o n   the  mapp ing   f rom   state s   to   actions .    I t  seems   possib le  that  there  may   be  a
fo rmal equ iv alence between  the algo r ithm  we have p roposed  an d  the one repo r ted  by  [10 ] .

As   a  cogn itive  and   neu roscien tif ic  p roposal,  the  p resen t wo r k   bears   a  close  relation   to   recen t
wo rk   by   Hasselmo   [6 ] ,  add ressing   the  p ref ron tal  compu tat ions   under ly ing   goal-d ir ected
action   selec tion   (see  also   [7 ]) .    The  p res en t  effo r ts   are  tied   mo re  closely   to   no rma tive
p r incip les   o f   decision -mak ing ,  whereas   the  wo rk   in   [6 ]   is   tied   mo r e  closely   to   the  details   o f
neu ral  circu itry.    In   th is   r espect,  the  two   app roach es   may   p rove  comp lemen ta ry,  and   it  w ill
be in teresting  to  fu r ther  consider  their  in ter relations .
                                                  
3 In this simulation and the next, the set of states associated with each state node was limited to the
set of reachable states for the relevant time-step, assuming an initial state of s1 .

A c k n o w l e d g m e n t s

Thanks   to   An d rew   Ledv ina,  Dav id   Blei,  Yael  N iv,  Nathan iel  Daw,  and   Franci sco   Pereira  fo r
usefu l commen ts .

R e f e r e n c e s

[1 ] Hu ll, C .L ., P rincip les o f Behavio r. 1943 , New  Yo rk : App leto n -Cen tu ry.

[2 ] To lman , E .C ., Pu rpo sive Behavio r in  An ima ls and  Men . 1932 , New  Yo rk : Cen tu ry.

[3 ]  D ick in so n ,  A .,  Action s  and   hab its:  the  developmen t  o f  behavio ra l  au tonomy.  Ph ilo soph ica l
Tran saction s o f the Royal Society  (Lo ndon ), Series B , 1985 . 308 : p . 67 -78 .

[4 ]  Balleine,  B .W.  and   A .  D ick in son ,  Goa l-d irected   in strumen ta l  action :  con tingen cy  and   incen tive
lea rn ing  and  their co rtica l sub stra t es. Neu ropharmaco logy, 1998 . 37 : p . 407 -419 .

[5 ]  Daw,  N .D .,  Y.  N iv,  and   P.  Dayan ,  Uncerta in ty-ba sed   competition   between   p refron ta l  and   stria ta l
systems fo r behavio ra l con tro l. Natu re Neu ro science, 2005 . 8 : p . 1704 -17 11 .

[6 ]  Hasselmo ,  M .E .,  A  model  o f  p refron ta l  co rtica l  mechan isms  fo r  goa l-d irected   behavio r.  Jou rnal  o f
Cogn itive Neu ro science, 2005 . 17 : p . 1115 -1129 .

[7 ]  Schmajuk ,  N .A .  and   A .D .  Th ieme,  Pu rpo sive  behavio r  and   cogn itive  map p ing .    A  neu ra l  netw o rk
model. B io log ical C ybernetics, 1992 . 67 : p . 165 -174 .

[8 ]  Tatman ,  J.A .  and   R .D .  Shach ter,  Dynam ic  p rog ramm ing   an d  
Tran saction s on  Sy stems, Man  and  Cybe rnetics, 1990 . 20 : p . 365 -379 .

in fluence  d iag rams.  IEEE

[9 ]  Verma,  D .  and   R .P.N .  Rao .  P lann ing   and   acting   in   uncerta in   enviromen ts  u sing   p robab ilistic
in ference. in  IEEE /RSJ In t erna tiona l Con ference on  In te lligen t Robo ts and  Systems. 2006 .

[10 ]  Tou ssain t,  M .   and   A .  S to rkey.  P robab ilistic  in ference  fo r  so l ving   d iscrete  and   con t inuou s  sta te
ma rkov  decision   p rocesses.  in   Proceed ing s  o f   the  23 rd   In terna tio na l  Con ference  on   Mach ine
Lea rn ing . 2006 . P itt sbu rgh , PA .

[11 ]  A ttias,  H .  P lann ing   by  p robab ilistic  in f erence.  in   Proceed ing s  o f   the  9 th   In t.  Wo rkshop   on
Artificia l In telligence and  S ta tisti cs. 2003 .

[12 ]  Shach ter,  R .D .  and   M .A .  Peo t.  Decision   mak ing   u sing   p robab ilistic  in ference  method s .  in
Uncerta in ty  in   a rtificia l  in telligen ce:  Proceed ing s  o f   the  E igh th   Con ference  (1992 ).  1992 .  S tan fo rd
Un iversity : M . Kau fmann .

[13 ]  Chater,  N .,  J.B .  Tenenbaum ,  and   A .  Yu ille,  P robab ilistic  models  o f  cogn ition :  con cep tua l
founda tion s.  Trend s in  Cog n itive Sciences, 2006 . 10 (7 ): p . 287 -291 .

[14 ]  Doya, K .,  et   al.,  ed s.  The Bayesian   B ra in : P robab ilistic  App roaches  to  Neu ra l  Cod ing . 2006 , MIT
P ress: Camb r idge, MA .

[15 ]  M iller,  E .K .  and   J .D .  Cohen ,  An   in teg ra ti ve  theo ry  o f  p refron ta l  co rtex   function .  Annual  Rev iew
o f Neu ro science, 2001 . 24 : p . 167 -202 .

[16 ]  Asaad ,  W.F.,  G .  Rainer,  and   E .K .  M iller,  Ta sk-specific   neu ra l  activity  in   the  p rima te  p refron ta l
co rtex. Jou rnal o f  Neu rophy sio logy, 2000 . 84 : p . 451 -459 .

[17 ] Ro lls, E .T., The function s o f the o rb ito fron ta l co rtex . B rain  and  Co gn ition , 2004 . 55 : p . 11 -29 .

[18 ]  Padoa-Sch iop pa,  C .  and   J.A .  Assad ,  Neu ron s  in   the  o rb ito fron ta l  co rtex   encode  econom ic  va lue.
Natu re, 2006 . 441 : p . 223 -226 .

[19 ]  Gopn ik ,  A .,  et  al.,  A  theo ry  o f  cau sa l  lea rn ing   in   ch ild ren :  cau sa l  m ap s  and   Bayes  nets.
P sycho log ica l Rev iew, 2004 . 111 : p . 1 -31 .

[20 ]  Ham ilto n ,  A .F.d .C .  and   S .T.  G rafton ,  Action   ou tcomes  a re  rep resen ted   in   h uman   in ferio r
fron topa rieta l  co rtex. Cereb ral Co r tex , 2008 . 18 : p . 1160 -1168 .

[21 ]  John son ,  A .,  M .A .A .  van   der  Meer,  and   D .A .  Red ish ,  In teg ra ting   h ippocampu s  and   stria tum   in
decision -mak ing . Cu rren t Op in ion  in  Neu rob io logy, 2008 . 17 : p . 692 -697 .

[22 ] Jen sen ,  F.V., Bayesian  Netwo rks and  Decision  Graph s. 2001 , New  Yo rk : Sp ringe r Verlag .

[23 ]  Cooper,  G .F.  A  method   fo r  u sing   belief  netwo rks  a s   in fluence  d iag rams.  in   Fou rth   Wo rkshop   on
Uncerta in ty  in  Artificia l In telligen ce. 1988 . Un iv ersity  o f M inneso ta, M in neapo lis.

[24 ]  N iv,  Y.,  D .  Joel,  and   P.  Dayan ,  A  no rma tive  p erspective  on   mo tiva tion .  Trend s  in   Cog n itive
Sciences, 20 06 . 10 : p . 375 -381 .

[25 ]  B lodget t,  H .C .,  The  effect  o f  the  in troduction   o f  rewa rd   upon   the  m a ze  perfo rmance  o f  ra ts.
Un iversity  o f Califo rn ia Pub lication s in  P sycho logy, 1929 . 4 : p . 113 -134 .


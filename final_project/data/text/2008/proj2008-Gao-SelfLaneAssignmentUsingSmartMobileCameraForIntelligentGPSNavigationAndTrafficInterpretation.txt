Self Lane Assignment Using Smart Mobile Camera For Intelligent
GPS Navigation and Traﬃc Interpretation

Tianshi Gao
Stanford University

1. Introduction

Imagine that you are driving on the highway at 70
mph and trying to ﬁgure out which lane should follow
to exit in front. The precision of the current GPS can-
not tell on which lane you are on, so the instruction
given by the GPS is just as simple as ”keep right” re-
sulting in the driver’s panic caused by searching from
multiple signs. However, if we have a smart camera
mounted inside the vehicle which is capable of infer-
ring the current lane the vehicle is on and feeding this
information into GPS, then more intelligent instruc-
tions like ”stay on the current lane” or ”turn to your
next right lane” can be achieved.
Another potential application using smart mobile cam-
era is to estimate the speed and density of the vehi-
cles surrounding a prob vehicle with camera mounted
inside. As a result, real-time traﬃc status can be in-
ferred from data collected by actively running vehicles
instead of limited static loop detectors beneath the
road. Moreover, if the prob vehicle knows which lane
it is on, then lane by lane traﬃc ﬂow can be obtained
by reporting the self lane number of the prob vehicle.
In this report, I proposed a novel and very eﬀective
algorithm consisting of both computer vision and ma-
chine learning techniques to solve the problem which is
deﬁned as given an image taken by a camera mounted
inside a vehicle, infer on which lane the vehicle is.
I form this problem as a scene classiﬁcation problem
where diﬀerent classes correspond to diﬀerent scenes
seen from diﬀerent lanes as shown in Figure 1. In the
proposed algorithm, horizon is detected, a set of fea-
tures at diﬀerent positions on the image is obtained
from a ﬁlter bank consisting of oriented steerable ﬁl-
ters at two scales and ﬁnally three diﬀerent learning
algorithms are applied to train the classiﬁers and the
results are compared. For each class, both the preci-
sion and recall rates are around or above 90%.

tianshig@stanford.edu

(a) Lane 1

(b) Lane 2

(c) Lane 3

(d) Lane 4

Figure 1. Sample images from four diﬀerent classes

2. Approach

2.1. Problem Formation

In this work I focused on the highway situation where
lanes exhibit low curvature and the number of lanes is
four which is given as prior information. In practice,
this is a reasonable assumption because we can get the
number of lanes from the GPS/digital map and four
lanes is a very common case.
Since perfectly detecting/ﬁtting all lanes is unreliable
or even impossible due to occlusion caused by other ve-
hicles, I tried to ﬁnd a global representation of the im-
age directly from low-level features like oriented edges
and infer the self lane number by the overall appear-
ance. Therefore, we can think of this problem as a
scene classiﬁcation problem in which given a test im-
age we want to classify it into four categories corre-
sponding to four scenes seen on lane 1, 2, 3 and 4
respectively (lane 1 corresponds to the leftmost lane,
and others are indexed from the left to right).
The scene classiﬁcation problem considered here is dif-
ferent from the traditional one which is about classify-
ing images into very broad categories like oﬃce, high-
way, forest, mountain, etc. Therefore, the problem
here is more challenging in terms of understanding the
scenes in a ﬁner granularity. It is also diﬀerent from
ob ject recognition which I’d like to bypass, since each
ob ject detector for the lane marker, vehicle and road

will have certain error rate and it’s very likely that er-
rors from each detector will accumulate at later stage.

Self Lane Assignment Using Smart Mobile Camera
m = 1, 2, · · · , 12 corresponding to each ﬁlter turned
to diﬀerent angles with even or odd phases. In the ex-
periment, I’ve partitioned the images into M ×N cells,
where M = 3 and N = 6. Let the grayscale image be
I and then the response of the image to each of the
ﬁlter is incorporated into:
(cid:2)

|(I ∗ Fk,m )(x, y )|

(1)

xi,j,k,m =

(x,y)∈Ci,j

2.2. Feature

2.2.1. Feature Design

Based on the intuition of how people distinguish diﬀer-
ent lane situations, there are three desired properties
for a good feature design to capture:
• Lane markers. Lanes are separated and deﬁned by
lane markers, so they are the most discriminative
information.
• Vehicles.
If there are multiple vehicles on your
right, then it’s less likely that you are on the right-
most lane.
• Spatial distribution. Besides the presence of lane
markers and vehicles, the more important infor-
mation is how they are spatially distributed on
the image plane.

To capture the information above, as shown in Fig-
ure 2(a) a ﬁlter bank consisting of oriented steerable
ﬁlters is used to implicitly capture the textures for
both lane markers and vehicles. Due to perspective
pro jection, the scales of edges at diﬀerent positions on
the image plane varies, so I used two sets of 12 ﬁlters
which are at two diﬀerent scales. To keep the spa-
tial information, I partitioned the image below horizon
into multiple cells which is shown in Figure 2(b).

(a)

(b)

Figure 2. (a) Filter bank consisting of 12 oriented steerable
ﬁlters per 30 degree with even and odd phases (b) Spatial
partition below horizon

2.2.2. Feature Representation

Denote the ﬁlters as Fk,m where k = 1, 2 correspond-
ing to two sets of ﬁlters with diﬀerent scales and

where Ci,j
ith row
to the cell at
corresponds
and j th column and i = 1, · · · , M and j =
1, · · · , N .
In order to make the response less sen-
sitive to the illumination and contrast of the im-
age, I normalize the 12-dimensional vector xi,j,k,· =
[xi,j,k,1 , xi,j,k,2 , · · · , xi,j,k,12 ]T to have energy 1, i.e.,
(cid:3)xi,j,k,· (cid:3)2 = 1. But if the energy of xi,j,k,· is too small,
I’ll not normalize it in order to capture those uniform
road regions. Furthermore, three statistics used in [1]
of the response for a set of 12 ﬁlters within one cell are
also incorporated into the feature vector. These three
statistics include mean, argmax and max−median of
the components of xi,j,k,· . So each cell has a (12+3)×2
dimensional descriptor, and all the 3×6 descriptors are
stacked into a single 15 × 2 × 3 × 6 = 540 dimensional
feature vector for each image.

2.3. Horizon Detection

As mentioned in the above subsection, the image is
partitioned into multiple cells below horizon. I detect
the horizon by detecting the vanishing point in two
steps as follows:

1. Lone line detection. I used the same method men-
tioned in [2] to detect lone lines in the image and
ﬁltered out those nearly vertical or horizontal lines
to reduce outliers. Denote the number of lone
lines after ﬁltering as L and these lines are pa-
rameterized by θi and ri as follows:
i = 1, 2, · · · , L

x sin θi + y cos θi = ri

(2)

2. Robust ﬁtting in Hough domain. Since the van-
ishing point is located at the intersection of the L
lines, we can get the estimation of the vanishing
point by minimize the norm of the residual:
minimiz e (cid:3)Ax − r(cid:3)1

(3)

where,

A =

⎛
⎜⎜⎜⎝

⎞
⎟⎟⎟⎠

sin θ1
cos θ1
cos θ2
sin θ2
...
...
sin θL cos θL

⎞
⎟⎟⎟⎠

⎛
⎜⎜⎜⎝

r1
r2
...
rL

r =

Self Lane Assignment Using Smart Mobile Camera

The reason I used 1-norm instead of ordinary least
square is to make the estimation less sensitive to
outliers. This convex optimization problem (3) is
solved using CVX [3]. Figure 3 shows some detec-
tion results.

Figure 3. Sample horizon detection results. Color lines are
the detected long lines and the cross is the detected van-
ishing point.

2.4. Learning Algorithm

Each dimension in the feature vector doesn’t provide
equal information. For example, those features from
the cell that is in front of the vehicle contain little
information. To avoid overﬁtting, i.e., reducing the
model complexity, I have chosen three diﬀerent learn-
ing algorithms:

1. Adaboost with decision trees.
I used logistic re-
gression version of Adaboost [1, 4] with weak
learners based on decision trees. Decision trees
make good weak learners, since they provide ex-
plicit feature selection and limited modeling of the
joint statistics of features.

2. Bayesian logistic regression. By assuming a prior
on the coeﬃcients in logistic regression, Bayesian
logistic regression is capable of shrinking the coef-
ﬁcients to avoid overﬁtting. The parameters can
be learned by MAP and I used zero mean Gaus-
sian prior and implementation from [5].

3. SVM. Although SVM doesn’t provide explicit fea-
ture selection or shrinkage, it’s still possible to
have relatively low error rate. I used the [6] im-
plementation with linear kernel.

Table 1. Horizon Detection Error Rate
std of relative error
mean of relative error
1.33%
3.85%

3. Experiment and Result

3.1. Data

I sampled frames from 12 short sequences and the
numbers of images collected for lane 1 to lane 4 are
112, 500, 750 and 750 respectively. Since the dataset
is unbalanced, I set the initial weights for Adaboost
according to the proportion of the number of images
for each class, and also adjust the penalty parameters
of relaxation for diﬀerent classes in SVM according to
the ratio between the numbers of diﬀerent classes.

3.2. Horizon Detection

I estimated the horizon position for all the 2112 im-
ages, and Table 1 and Figure 4 show the detection
result. Around 98% of the detected horizon lies in the
5% relative error band. The mean of the relative error
is only 1.33% which is 3 pixels in this case.

400

350

300

250

200

150

100

0

estimated horizon position
5% relative error band

500

1000

1500

2000

2500

Figure 4. Estimated horizon positions.

3.3. Classiﬁcation

The classiﬁer for each class is trained in one vs. all
fashion. At the test stage, the classiﬁcation result is
chosen as the one with the highest probability. I used
3-fold cross validation to evaluate the performances
of three learning algorithms. The fold number is 3
instead of most common 5 or 10 is because I want to
reduce the correlation between the training and test
data.
The confusion table for each algorithm is shown in

Self Lane Assignment Using Smart Mobile Camera

Table 2. Confusion Table for Adaboost with Decision Trees
Recall
Lane 2
Lane 1
Lane 4
Lane 3
97.32%
3
109
10
53
87.00%
435
2
91.73%
26
688
36
699
43
8
93.20%
98.20% 90.25% 87.76% 95.10%

Lane 1
Lane 2
Lane 3
Lane 4
Precision

Table 3. Confusion Table for Bayesian Logistic Regression

Lane 1

Lane 2

Lane 3

Lane 4

110
3

Lane 1
Lane 2
Lane 3
Lane 4

2
464
23
3

30
689
40

3
38
707

Precision

97.35%

94.31%

90.78%

94.52%

Recall

98.21%
92.80%
91.87%
94.27%

Table 2, 3, 4. All three methods give comparable
results, and Bayesian logistic regression has slightly
better accuracy for lane 2 and lane 3. In general, the
precision and recall rate for each class is around or
above 90%. In addition, the results are consistent with
the intuitions in terms of:
i) lane 1 and lane 4 have
better accuracy than lane 2 and lane 3; ii) lane 2 and
lane 3 are more likely to be confused by each other;
iii) and lane 1 is more likely to be confused by lane 2
than lane 3,4, and lane 4 is more likely to be confused
by lane 3 than lane 1,2.
Moreover, top features selected by Adaboost with deci-
sion trees are shown in Figure 5. The oriented ﬁlters at
diﬀerent cells correspond to the selected features. The
positions and orientations of these ﬁlters are more or
less consistent with the positions and orientations with
the lane markers or the vehicles like the vertical edge
response at the 2nd row and 3rd column for classiﬁer
lane 3.

HMM, making spatial partition more robust maybe by
drawing rays from vanishing point.

(a) Lane 1

(b) Lane 2

(c) Lane 3

(d) Lane 4

Figure 5. Selected top features by Adaboost with decision
trees for diﬀerent classes.

Acknowledgments

I’d like to thank Honglak Lee and Zhi Li for some
helpful discussions.

4. Conclusion and Discussion

References

In this report, I have proposed a novel and eﬀective
algorithm to infer the lane number from a single im-
age. The results show that bypassing explicit ob ject
recognition and achieving the inference goal directly
from low-level representation with the spatial distribu-
tion works well in this scene classiﬁcation problem re-
quiring ﬁner granularity. Some further improvements
could include incorporating temporal dimension using

Table 4. Confusion Table for SVM
Lane 1
Lane 2
Lane 3
Lane 4

110
2

Lane 1
Lane 2
Lane 3
Lane 4

2
453
46
6

37
670
44

8
33
700

Precision

98.21%

89.35%

89.21%

94.47%

Recall

98.21%
90.60%
89.33%
93.33%

[1] D. Hoiem, A. Efros, and M. Herbert, ”Geometric con-
text from a single image,” International Conference on
Computer Vision (ICCV), 2005.

[2] J. Kosecka and W. Zhang, ”Video compass,” Proc.
ECCV, Springer-Verlag, 2002.

[3] M. Grant and S. Boyd. CVX: Matlab software for
disciplined convex programming (web page and software).
http://stanford.edu/ boyd/cvx, December 2008.

[4] M. Collins, R. Schapire, and Y. Singer, ”Logistic regres-
sion, adaboost and bregman distances,” Machine Learning,
vol. 48, no. 1-3, 2002

and D. Madigan,
[5] A. Genkin, D. D.Lewis
”BBR:
Regression
Bayesian
Logistic
Software,”
http://www.stat.rutgers.edu/ madigan/BBR/

[6]
A

Chang
C.
Library
for

and
C.
Support

Lin,
”LIBSVM –
Vector Machines,”

Self Lane Assignment Using Smart Mobile Camera

http://www.csie.ntu.edu.tw/ cjlin/libsvm


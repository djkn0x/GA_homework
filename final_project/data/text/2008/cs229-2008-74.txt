Recommendation System Based on Collaborative Filtering

Zheng Wen

December 12, 2008

1

Introduction

Recommendation system is a speciﬁc type of information ﬁltering technique that attempts to present
information items (such as movies, music, web sites, news) that are likely of interest to the user.
It is of great importance for the success of e-commerce and IT industry nowadays, and gradually
gains popularity in various applications (e.g. Netﬂix pro ject, Google news, Amazon). Intuitively, a
recommendation system builds up a user’s proﬁle based on his/her past records, and compares it
with some reference characteristics, and seeks to predict the ‘rating’ that a user would give to an
item he/she had not yet evaluated. In most cases, the recommendation system corresponds to a
large-scale data mining problem.
Based on the choice of reference characteristics, a recommendation system could be based on
content-based approach or col laborative ﬁltering (CF) approach (see [1]) or both. As their names
indicate, content-based approach is based on the “matching” of user proﬁle and some speciﬁc charac-
teristics of an item (e.g.the occurrence of speciﬁc words in a document) while collaborative ﬁltering
approach is a process of ﬁltering information or pattern based on the collaboration of users, or the
similarity between items. In this pro ject, we build a recommendation system based on multiple
collaborative ﬁltering (CF) approaches and their mixture, using part of Netﬂix pro ject data as an
example.
The remaining part of this report is organized as follows: in Section 2, we reformulate the Netﬂix
pro ject and use it to test the proposed algorithm in Section 3; in Section 3, we propose various
CF algorithms to solve this problem; the experimental results are demonstrated in Section 4. We
conclude the current results and propose future work at last.

2 Problem Formulation

We use the Netﬂix movie recommendation system as a speciﬁc example of recommendation system.
Speciﬁcally, assume there are Nu users and Nm movies, given a set of training examples (i.e. a set
of triples (user, mov ie, rating)), deﬁne the user-movie matrix A ∈ (cid:60)Nm×Nu as:
(cid:40)

(1)

Amu =

Rm,u , User u’s rating on Movie m
?

if such rating exists
if no such rating

Our task is to predict all the users’ ratings on all the movies based on the training set (i.e. existent
ratings). In other words, we try to replace all the “question marks” in A by some optimal guesses.
In this problem, each rating is an integer between 1 and 5, and the goal is to minimize the RMSE
(root mean square error) when predicting the ratings on a test set (which is of course unknown

1

Implementation
RMSE

Naive
around 1.25

Cinematch Top in the current leaderboard of the contest
0.9514
0.8604

rmse =

(Rm,u − Pm,u )2 ,

Table 1: Performance of “Benchmarks”
(cid:118)(cid:117)(cid:117)(cid:116) 1
during the training phase), that is, to minimize
(cid:88)
|Stest |
(m,u)∈Stest
where (m, u) ∈ Stest if User u rates Movie m in the test set, |Stest | is its cardinality, Rm,u is the
true rating and Pm,u is the prediction based on the recommendation system.
Due to the recommendation system’s importance in improving service quality, Netﬂix has started
a contest with a grand prize to attract the researchers worldwide to work on this problem. Cur-
rently, many researchers, including many experts, are focusing on this problem and have made great
progress. The performances of some important implementations can serve as the “measures” (or
“benchmarks”) for the quality of diﬀerent algorithms, as shown in Table 1, where Naive imple-
mentation corresponds to using a constant rating (such as the average of all the available ratings),
Cinematch is Netﬂix’s original recommendation system (baseline).
Due to the limited computation power of PC and MATLAB, we only use part of the available
data to build the recommendation system. Speciﬁcally, we use a data set include 20,000 users, and
1,500 movies.

(2)

3 Collaborative Filtering Algorithms

3.1

Item-Based K Nearest Neighbor (KNN) Algorithm

The ﬁrst approach is the item-based K -nearest neighbor (KNN) algorithm. Its philosophy is as
follows: in order to determine the rating of User u on Movie m, we can ﬁnd other movies that are
similar to Movie m, and based on User u’s ratings on those similar movies we infer his rating on
Movie m, see [2] for more detail. In order to determine which movies are “similar”, we need to
deﬁne a similarity function (similarity metric), as in [2], we use the adjusted cosine similarity
(cid:0)Ra,u − ¯Ru
(cid:1) (cid:0)Rb,u − ¯Ru
(cid:1)
(cid:80)
u∈U (a) (cid:84) U (b)
between Movie a and b:
(cid:114)(cid:80)
(cid:1)2
(cid:0)Rb,u − ¯Ru
(cid:1)2 (cid:80)
(cid:0)Ra,u − ¯Ru
u∈U (a) (cid:84) U (b)
u∈U (a) (cid:84) U (b)
sim(a, b) =
(3)
,
have rated Movie a and hence U (a) (cid:84) U (b) is the set of users that have rated both Movie a and b.
where Ra,u is User u’s rating on Movie a, ¯Ru is User u’s average rating, U (a) is the set of users that
The advantage of the above-deﬁned adjusted cosine similarity over standard similarity is that the
diﬀerences in the rating scale between diﬀerent users are taken into consideration.
As its name indicates, KNN ﬁnds the nearest K neighbors of each movie under the above-
deﬁned similarity function, and use the weighted means to predict the rating. For example, the
(cid:80)
KNN algorithm for movies leads to the following formula:
(cid:80)
u (m) sim(m, j )Rj,u
j∈N K
u (m) |sim(m, j )|
j∈N K

Pm,u =

(4)

,

2

u (m) = {j : j belongs to the K most similar movies to Movie m and User u has rated j },
where N K
and sim(m, j ) is the adjusted cosine similarity deﬁned in (3), Rj,u are the existent ratings (of User
u on Movie j ) and Pm,u is the prediction.
It should be pointed out that there also exists a user-based KNN algorithm. However, in most
cases of the Netﬂix pro ject, its performance is poorer than item-based KNN algorithm. This is
because the user-based data appear to be sparser (i.e. it is very unlikely that a movie has only been
rated by 1 or 2 users, and highly possible that a user only rates 1 or 2 movies). In addition, since
typically there are much more users than movies, the user-based KNN also leads to a computational
challenge.

3.2 Item-Based EM Algorithm

An alternative approach to solve this problem is Item-based EM algorithm. In this algorithm, we
classify movies into G groups, and each Movie m belongs to Group g with probability Qm (g). For
a given Group g of movies, we assume the ratings of diﬀerent users are independent and Gaussian,
that is, P (Ru,m |m ∈ g) ∼ N (µg ,u , σ2
g ,u ). The conditional independence is known as Naive Bayes
assumption in machine learning literature and is widely used in many diﬀerent applications. In this
pro ject, intuitively, if we know Movie 1 is a horror movie, then whether User 1 likes this movie
should be independent of whether User 2 likes this movie, hence their ratings on this movie are
conditionally independent. The Gaussian assumption is assumed to simplify the calculation in the
M -step. The unboundedness issue related to the Gaussian assumption can be solved by truncation.
To formulate the EM algorithm formula in this case, let latent random variable Gm ∼ Qm (·)
denotes the group of Movie m. Deﬁne U (m) as the set of users that have rated Movie m. Thus,
the E-step formula is described as follows:
(cid:16)− (Ru,m−µg,u )2
(cid:17)
m = g |Ru,m ; µg ,u , σ2
m (g) = P (G(t+1)
g ,u )
Q(t+1)
(cid:18)
(cid:80)
1√
Q(t)
m (g)Πu∈U (m)
exp
2σ2
2πσg,u
g,u
− (Ru,m−µg(cid:48) ,u )2
m (g (cid:48) )Πu∈U (m)
1√
g (cid:48) Q(t)
exp
2πσg(cid:48) ,u
2σ2
g(cid:48) ,u
where superscripts (t) and (t + 1) are used to denote distributions of latent random variables in
(cid:34)
(cid:40)
(cid:32)
(cid:35)
(cid:33)(cid:41)
diﬀerent iterations. For M-step, we need to solve
(cid:88)
(cid:88)
− (Ru,m − µg ,u )2
− log (Qm (g))
2σ2
g ,u
g
m

Πu∈U (m)
Qm (g) (cid:88)
u∈U (m)
Deﬁne M (u) as the set of movies User u has rated, by changing the order of summation and a little
(cid:80)
bit of algebra, we have
(cid:80)
m∈M (u) Qm (g)Ru,m
m∈M (u) Qm (g)

−log(σg ,u ) − (Ru,m − µg ,u )2
2σ2
g ,u

1√
2πσg ,u
(cid:34)

which is equivalent to

Qm (g)

log

exp

(5)

,

(cid:88)
m,g

max
µg,u ,σ2
g,u

(cid:19) ,

(cid:35)

.

=

max
µg,u ,σ2
g,u

µg ,u =

3

g ,u =
σ2

(cid:80)
(cid:80)
m∈M (u) Qm (g) (Ru,m − µg ,u )2
m∈M (u) Qm (g)
Pu,m = (cid:88)
We repeat E-step and M-step until convergence. And the ﬁnal prediction is given by
g
Similarly, there also exists a user-based EM algorithm. Due to the reasons discussed in the
previous subsection about the advantage of item-based KNN over user-based KNN, for this pro ject,
the item-based EM will work better than user-based EM in most cases.

Qm (g)µg ,u .

(6)

3.3 Sparse SVD Algorithm

Another algorithm to solve this problem is based upon sparse matrix SVD. This approach models
both users and movies by giving them coordinates in a low dimensional feature space i.e. each user
and each movie has a feature vector. And each rating (known or unknown) is modeled as the inner
product of the corresponding user and movie feature vectors. In other words, we assume there exist
a small number of (unknown) factors that determine (or dominate) ratings, and try to determine
the values (instead of their meanings) of these factors based on training data. Mathematically,
based on the training data (sparse data of a huge matrix), we try to ﬁnd a low-rank approximation
of the user-movie matrix A. This approach is called sparse SVD algorithm.
Let ui ∈ (cid:60)Nf , i = 1, 2, · · · , Nu be all the users’ feature vectors, and mj ∈ (cid:60)Nf , j = 1, 2, · · · , Nm
be all the movies’ feature vectors. This problem can be formulated as the following optimization
problem:
(cid:88)
 ,
nui (cid:107)ui(cid:107)2 + (cid:88)
(cid:88)
(i,j )∈I
i
j
where I = {(i, j )|if user i has rated movie j }, nui is the number of movies user i has rated, and nmj
is the number of users that have rated movie j . We notice the weighted L2 regularization term is
added to avoid potential overﬁtting.
One should notice that to solve (7) directly is nontrivial. As has been proposed in [3], this
problem can be solved as follows: ﬁrst, we ﬁx the users’ feature vectors ui , and solve for the movies’
feature vectors mj ; then we ﬁx movies’ feature vectors mj , and solve for users’ feature vectors ui .
We repeat this process until convergence. Notice in each step, we are solving a regularized least
square problem, for which eﬃcient algorithms exist and are handy to use.

nmj (cid:107)mj(cid:107)2

(Ri,j − ui

T mj )2 + λ

min
ui ,mj

(7)

3.4 Tricks in Postprocessing

In addition to the collaborative ﬁltering algorithms discussed in the previous subsections, multiple
“tricks” in data postprocessing could also be applied simultaneously to enhance the performance
of the recommendation system. Some of the “tricks” we have used in our implementation include
newcomer prediction, prediction truncation, item-based correction and near-integer round-oﬀ.

4

Newcomer Prediction

When there exists a newcomer, i.e. a user without any existent ratings, it is very diﬃcult to predict
his/her rating on any item. In order to minimize the RMSE in this case, we use the item mean as
his/her prediction. That is, the prediction of User u’s rating on Movie m is the average value of
(cid:88)
existent ratings on Movie m given by other users,
u(cid:48)∈U (m)

1
|U (m)|

Pm,u =

Rm,u(cid:48) ,

as deﬁned above, U (m) is the set of users that have rated Movie m.

Prediction Truncation

For some algorithms such as sparse SVD, it is possible that the prediction Pm,u is above 5 or
below 1. In this case, we “truncate” the prediction. That is, we set Pm,u = 5 if Pm,u > 5 and
Pm,u = 1 if Pm,u < 1. Of course, prediction truncation will strictly improve the performance of the
recommendation system and reduce the RMSE.

Item-Based Correction

The third and most important “trick” is item-base correction. Speciﬁcally, deﬁne the item-based
(cid:88)
rating mean as
u(cid:48)∈U (m)

1
|U (m)|

Sm =

Rm,u(cid:48) ,

where U (m) is deﬁned above. For a given collaborative ﬁltering algorithm, deﬁne the item-based
(cid:88)
prediction mean as
1
Nu
u(cid:48)
where Nu is the number of users. We correct the prediction by ∆m = Sm − ˜Sm . That is

˜Sm =

Pm,u(cid:48) ,

Pm,u

:= Pm,u + ∆m .

(8)

The above equation gives the formula for item-based correction. Based on our experiment, the
item-based correction improves the RMSE by about 0.01.

Near-Integer Round-oﬀ

For Netﬂix pro ject, one interesting question is that whether rounding oﬀ the prediction will reduce
the RMSE. While reducing the errors of some predictions to 0, this approach will also increase the
errors of other predictions. In practice, it is observed that in most cases the naive round-oﬀ will
increase the RMSE.
In this pro ject, we use an approach called near-integer round-oﬀ. Speciﬁcally, we round oﬀ
the prediction if it is close enough to an integer. During the implementation, we round oﬀ the
prediction if its distance to the nearest integer is less than or equal to 0.1. It has been observed
that this approach can improve the performance of the recommendation system.

5

Sequence of Implementation

In our implementation, given predictions as outcomes of some collaborative ﬁltering algorithms, we
further enhance the performance by ﬁrst applying the newcomer prediction algorithm, then carry
out the item-based correction, then prediction truncation, and at last near-integer round-oﬀ. It has
been observed that with this sequence of implementation of postprocessing tricks, the RMSE of the
recommendation system can be reduced by approximate 0.02.

3.5 Parameter Adjustment and Algorithm Mixture

As is classical in machine learning and collaborative ﬁltering, we need to adjust the parameters
in order to achieve the optimal performance of the recommendation system. For example, in the
item-based KNN algorithm, we need to adjust the number of neighbors (i.e. K ); in the item-based
EM algorithm, we need to adjust the number of groups of movies; in the sparse SVD algorithm, we
need to adjust the dimension of the feature vectors and the regularization weight. All the parameter
adjustment is carried out through K -fold cross-validation.
It is also noticed that as expected, “blending” the algorithms could improve the performance.
For ”blending”, we mean that we use the convex combination of the predictions from diﬀerent col-
is the prediction
laborative ﬁltering algorithms as the ﬁnal predictions. For example, assume P KN N
m,u
is the prediction from the sparse SVD algorithm,
from the item-based KNN algorithm, and P SV D
m,u
then the ﬁnal prediction is

m,u + (1 − λ)P KN N
= λP SV D
P f inal
,
m,u
m,u
where λ ∈ (0, 1) is the weight of sparse SVD algorithm in the “blending”.

(9)

3.6 Our New Contribution

It should be noticed that most of the algorithms used in this pro ject have been presented in previous
literatures (e.g. [2, 3]) in this ﬁeld. To the best of our knowledge, our new contributions include:

1. We use the item-based correction and near-integer roundoﬀ in both sparse SVD and EM
algorithm.

2. We change the EM algorithm from user-based to item-based.

4 Experimental Results

We carry out the algorithms proposed in Section 3 on part of Netﬂix data with 20, 000 users
and 1, 500 movies. For each CF algorithm and their mixture, we use the postprocessing “tricks”
presented above to enhance the performance and use K -fold cross-validation to choose the best
parameters (models).
In particular, we choose K = 10 in our implementation. The results are
listed as follows:

4.1 Item-Based KNN Algorithm

For the item-based KNN algorithm, we use the approach described in Subsection 3.1. The cross-
validation RMSE is shown in Figure 1. From the cross validation result, we notice the optimal
number of nearest neighbors is K = 600. With this particular choice of K , the RMSE of test data
is 0.9508.

6

Figure 1: Cross Validation for Item-Based KNN Algorithm

Although the performance of the item-based KNN algorithm is NOT as good as alternative algo-
rithms, such as the sparse SVD and item-based EM algorithm, it does reach a performance similar
to Cinematch. In addition, the prediction results of the KNN algorithm can be used to initialize
other algorithms, which will usually lead to a much better performance and faster convergence than
random initialization.

4.2 Item-Based EM Algorithm

For the item-based EM algorithm, we apply the algorithm described in Subsection 3.2 and use the
cross validation approach to choose the optimal number of groups of movies. The cross validation
indicates the optimal number of groups is 30, and the resulting RMSE for test data is 0.9140.
During the experiment, we notice that item-based EM algorithm has a better performance than
user-based EM algorithm. However, its performance is poorer than the sparse SVD algorithm.

4.3 Sparse SVD Algorithm

For the sparse SVD algorithm, we implement the algorithm as described in Subsection 3.3. The
cross-validation RMSE for diﬀerent feature vector dimensions Nf and L2 regularization weights λ is
shown in Figure 2,3 and 4. From the cross validation, we notice the optimal feature space dimension
is Nf = 150 and the associated optimal weight is λ = 0.106. With these parameters, the RMSE on
test data is 0.89674.
We notice that sparse SVD has a better performance than item-based KNN algorithm and item-
based EM algorithm. This is primarily due to: (1) the sparse SVD simultaneously treat both user
and item features, while other algorithms always concentrate on one aspect (hence can be classiﬁed
as “user-based” or “item-based”); (2) L2 regularization is implemented to avoid overﬁtting.

7

Figure 2: Cross Validation for Sparse SVD Algorithm: NF = 6, 8, 10, 12

Figure 3: Cross Validation for Sparse SVD Algorithm: NF = 30, 50, 70, 90, 110

8

Figure 4: Cross Validation for Sparse SVD Algorithm:NF = 150, 200, 250, 300

Algorithm Item-Based KNN Item-Based EM Sparse SVD Blending of Item-Based EM and Sparse SVD
0.8930
0.89674
0.9140
0.9508
RMSE

Table 2: Performance of Diﬀerent Collaborative Filtering Algorithms

4.4 Blend Item-Based KNN and Sparse SVD

At last, we “blend” the predictions of item-based KNN and sparse SVD, as described in Equation
(9). We use the cross validation to choose the optimal weight λ, and the cross-validation RMSE is
shown in Figure 5. From the cross validation, the optimal “blending” weight is λ = 0.78. With this
optimal weight, the RMSE for test data is 0.8930.
In summary, the performance of diﬀerent algorithms are illustrated in Table 2.

5 Conclusion and Future Work

In this pro ject, we present several collaborative ﬁltering algorithms for recommendation system and
test the performance of each algorithm and their mixtures on part of the Netﬂix data. At last,
a RMSE of 0.8930 is achieved, which corresponds to a 6.14% improvement of the performance of
Cinematch (baseline).
As to the future work, we plan to

1. Try other important algorithms on this pro ject, such as Bayes Network;

2. Work more on the algorithm ”blending”. Since diﬀerent algorithms might characterize dif-
ferent aspects of the problem, and cooperation between multiple algorithms should lead to a
better performance. Of course, model selection approaches such as cross validation will be
widely used in this scenario.

9

Figure 5: Cross Validation for “Blending” Weight of Item-Based KNN and Sparse SVD

Acknowledgment

We are grateful for Prof. Ng and TAs’ help during this course and pro ject. We also thank Mr.
Robbie Yan for helping us get started on this pro ject.

References

[1] J. Breese, D. Heckerman and C. Kadie, “Empirical Analysis of Predictive Algorithms for Col-
laborative Filtering”, Technical Report of Microsoft Research, 1998.

[2] B. Sarwar, G. Karypis, J. Konstan and John Riedl, “Item-Based Collaborative Filtering Rec-
ommendation Algorithms”, Proceedings of the 10th international conference on World Wide
Web 2001: 285-295.

[3] Y. Zhou, D. Wilkinson, R. Schreiber, R. Pan. “Large-Scale Parallel Collaborative Filtering for
the Netﬂix Prize”, AAIM 2008: 337-348.

[4] Andrew Ng, CS229 Lecture Notes.

10


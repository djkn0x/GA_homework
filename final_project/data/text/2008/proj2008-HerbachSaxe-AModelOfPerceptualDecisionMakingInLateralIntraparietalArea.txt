A model of perceptual decision making in lateral intraparietal
area

Andrew Saxe

Josh Herbach

Advised by J. McClelland

1

Introduction

We model an oculomotor decision-making experiment in which monkeys are shown a cloud of moving dots and must
decide which way the dots are moving on average. In each trial of the experiment a subset of the dots moves coherently
to the left or right, while the remaining dots move randomly. The monkey indicates its choice by saccading to the left or
right, and if correct it receives a juice reward. This standard paradigm has recently been extended to include differing
reward conditions [1]: Before the onset of the motion stimulus, a reward stimulus is presented that indicates how many
drops of juice the monkey will receive for a correct answer in each direction.
Analysis of the behavioral data from this experiment shows that the monkeys achieve a near-optimal reward rate on
this task [1]. However, the neural circuits underlying this decision process remain unclear.
We attempt to explain the neural data using a computational neuroscience approach. We hypothesize the computation
that we believe LIP performs, ﬁnd the optimal solution to that computation, and see if the optimal solution predicts
the experimental data. In particular, we hypothesize that LIP implements an optimal decision rule: it receives noisy
observations of the motion direction as input, and produces the decision that will maximize the monkey’s expected
reward rate as output. In section 2 we formulate a probabilistic model of the task. In section 3 we ﬁnd the optimal
decision rule. In section 4 we set the parameters of our model to closely approximate previous modeling efforts. In
section 5, we present a stochastic neural network representing LIP that we train to implement this optimal decision
rule. Finally, section 6 makes comparisons to experimental data.

2 Modeling the Computation

Because we intend to train a recurrent neural network to implement the optimal decision rule, we ﬁrst formulate the
computational problem solved by the monkey in a discrete-time setting.
In a given trial of the experiment, a certain fraction of dots move coherently to the left or right and the rest move
randomly. Deﬁne the signed coherence c ∈ [−1, 1] of a trial to be the fraction of dots moving coherently with the sign
indicating if the movement is to the left or right respectively. Thus the correct answer y = sign(c). Correct responses
are rewarded with either one or two drops of juice. Deﬁne r+ , r− ∈ {1, 2}, where r+ is the reward magnitude for a
correct response with y = 1 and r− is the reward magnitude for a correct response with y = −1.
During a trial, LIP receives input from lower-level visual processing areas sensitive to motion direction. We describe
this input signal as a series of noisy inputs X = {x(t) , t = 1, ..., T }, where LIP receives input x(t) at time t. Here
the time period during which the monkey views the moving dots has been discretized into T instants. In the simplest
model, let x(t) ∈ {−1, 1} represent the observed stimulus direction. We take the x(t) to be mutually independent
given y , and will specify p(x(t) ) by deﬁning a measurement model p(x(t) |c) and a prior over coherences for a trial
p(c). The experiment consists of blocks of trials with ﬁxed coherence magnitude |c| = cmag . For each trial the
leftward or rightward direction is chosen randomly, i.e. p(y = 1) is known and set by the experimenter. Thus
p(c) = δ(c − cmag )p(y = 1) + δ(c + cmag )p(y = −1).
To ﬁnd p(x(t) |c) = p(x(t) |y , cmag ), we will require that the probability of error under the optimal decision function
in the equal reward case r+ = r− be identical to that of previous modeling approaches based on the continuous time

drift diffusion process [1,2]. Since previous approaches have treated the equal reward case, this requirement will make
our formulation match previous work. To do so we must know the optimal decision rule.

(1)

= r+αp(y = 1)

3 The Optimal Decision Rule
We now derive the optimal decision rule as a function of p(x(t) |c) and the other parameters. Let p(cid:48) = p(x(t) = 1|y =
1, cmag ). We can now compute the expected reward R for a single trial given a response D(X, r+ , r− , cmag ) = 1,
T(cid:89)
E [R|D(X, r+ , r− , cmag ) = 1] = r+p(y = 1|X, cmag )
p(x(t) |y = 1, cmag ).
t=1
(the formula for the expected reward is similar when D(X, r+ , r− , cmag ) = −1). Since we have deﬁned D∗ to
be a function maximizing the expected reward rate, D∗ should choose 1 when E [R|D(X, r+ , r− , cmag ) = −1] <
E [R|D(X, r+ , r− , cmag ) = 1] or alternatively (since E [R|D(X, r+ , r− , cmag ) = i] ≥ 0), it chooses 1 when
E [R|D(X, r+ , r− , cmag ) = 1]
E [R|D(X, r+ , r− , cmag ) = −1] .
x(t) (1 − p(cid:48) )− (cid:80)
(cid:48) (cid:80)
p(y = 1)
r+
x(t)
1 <
p(y = −1) p
.
t
t
r−
(cid:18) p(cid:48)
(cid:19)
(cid:18) p(y = 1)
(cid:18) r+
(cid:19)
(cid:19) (cid:88)
Now we take the log of both sides to get the condition D∗ (X, r+ , r− , cmag ) = 1 if
p(y = −1)
1 − p(cid:48)
r−
t

Manipulating this eventually yields

0 < log

1 <

+ log

+ log

x(t) ,

(3)

(2)

(4)

(5)

and -1 otherwise.

∗ 2 − 1.

D∗ (X, r+ , r− , cmag ) = 1

In the equal reward, p(y = 1) = 1/2 case (5) becomes

4 A Measurement Model in Agreement with the Drift Diffusion Process
(cid:40)(cid:88)
(cid:41)
x(t) > 0
Let N = (cid:80)T
t
t=1 1{x(t) = 1}. Then N ∼ Binomial(T , p(cid:48) ). Assuming y = 1, the probability of error is the probability
that the sum of T Bernoulli trials – each with probability of success p(x(t) = 1|y = 1; r+ , r− , cmag ) – is less than
T /2. To ﬁnd p(cid:48) we can require that as T → ∞, the error matches the error of the drift diffusion model. By applying
the de Moivre-Laplace theorem and setting the result equal to the error of the drift diffusion model, we ﬁnd that
(cid:113) k2 T
1
1
p(cid:48) =
2
2
A2 tf
A is a drift rate parameter representing the strength of the motion signal, and k is a parameter controlling the amount of
noise. Unsurprisingly, as the inﬂuence of noise (k) increases, the signal portion of p(cid:48) decreases and as the meaningful
drift increases (A) the signal has a larger impact. In [1,2] the drift rate A is assumed to be proportional to coherence,
A = ac where a is a parameter. Substituting this into (6) yields
(cid:113) k2 T
1
1
p(cid:48) =
2
2
+ 1
a2 c2 tf
from which p(x(t) |y , cmag ) can be recovered. Thus our discrete time model and the continuous models in [1,2] will be
close approximations of each other for large T . Figure 1a shows the quality of approximation for T = 15, the number
of samples used subsequently.

+ 1

(6)

(7)

+

+

.

,

Figure 1: (a) Comparison of psychometric functions under a DDM and the discrete time optimal decision function (5),
with p(x(t)|c) as given in (7) and T = 15, tf = 5, a = 1, k = 1; (b) Recurrent network architecture.

5
Implementing the decision rule in a recurrent neural network
The optimal decision rule given in (5) can only predict behavioral data. To make the connection to neural data, we
1 = 1 (cid:8)x(t) = 1(cid:9), u(t)
2 = 1 (cid:8)x(t) = −1(cid:9), u(t)
train a recurrent neural network to implement the optimal decision rule and see if the ﬁring rates of the network match
those recorded in LIP. Our network consists of three layers (Figure 1b). The input layer presents the vector u(t) where
u(t)
3 = r+ , u(t)
4 = r− , and u(t)
5 = 1. The components u(t)
and u(t)
1
2
split the input x(t) into two processes so that neurons in the network can become responsive to only the leftward or
rightward directions (or to a mixture of both). It projects to a hidden layer with weights W u . Each neuron in the
hidden layer receives input from all other hidden neurons through recurrent connections W r , and sends output to the
output layer through weights W y . The output layer consists of a single neuron which outputs the decision ˆy (t) .
In other contexts noise has been found to be crucial to reproducing observed properties of experimental data [3] and
so we have trained networks in both a noiseless, deterministic setting, and a noisy, stochastic one.
(cid:88)
 ,
ij ) + (cid:88)
Let o(t) be the hidden neuron activation at time t. This activation is updated according to
(cid:32)
(cid:33)
j + η (t)
j (u(t)
j + ζ (t)
ij (o(t)
i + νi )
W u
W r
(cid:1), and all
(cid:1) , νi ∼ Gaussian (cid:0)0, σ2
i ∼ Gaussian (cid:0)0, σ2
(cid:1)2
(cid:0)1+ao
j
j
ij ∼ Gaussian
(t)
o
, η (t)
1+exp −x , ζ (t)
0, α
1
j
ν
η
(t)
j
random variables are mutually independent. The noise terms were proposed by Todorov in [3]. The random variable
ζ (t)
ij models the poisson ﬁring characteristics of neurons in vivo and synaptic depression effects. The random variables
η (t)
and νi model input noise, with η (t)
representing ﬂuctuating noise and νi representing slowly varying noise that
i
i
remains constant over the course of a trial. We took α = .1, a = .1, and σ2
η = σ2
ν = .001.
 − 1.
(cid:88)
j
To train the network we minimize the expected error of the network over a set of training data. We use the back
propagation through time algorithm [4] to calculate the gradient of the error with respect to the network parameters,
and make use of noise freezing tricks described in [3] to speed the computations.

The output of the network ˆy (t) is calculated as

where f (x) =

o(t+1)
i

= f

ˆy (t) = 2f

j o(t)
W y
j

6 Comparison to data

The ﬁring rates of hidden units in the trained network show two typical patterns. In the ﬁrst, the unit begins each trial
with a low activation, and its activation increases as it receives information favoring one direction. We interpret this

−1−0.500.5100.20.40.60.81DDM vs. Binomial Psychometric Functions, T=15Coherence cProbability of response 1, p(D*(X,r+,r−,cmag)=1)  DDMBinomialWxr(cid:31)(t)r(cid:30)(t)x(t)1WyInputHiddenOutputWrŷ(t)Figure 2: Firing rates. (a) Monkey data; (b) Model

Figure 3: Psychometric functions. (a) Monkey data; (b) Model

Figure 4

010203040506070−10−5051015202530ms (aligned to 200ms after motion period onset)Deviation from initial firing rate (Hz)Average firing rates of Neuron 42, 24% coherence  −1,2,21,2,2−1,2,11,2,1−1,1,21,1,2−1,1,11,1,1y,r−,r+10121416182022−0.100.10.20.30.40.50.60.70.80.9Integrator neuron average responsetime step (t=10 is beginning of motion period)Activation (% of maximum firing rate)  1,2,21,1,21,2,11,1,1−1,2,2−1,1,2−1,2,1−1,1,1y,r−,r+−0.500.500.10.20.30.40.50.60.70.80.91Signed coherence vs % of rightward responsesSigned Coherence% Rightward responses  1,11,22,12,2r−,r+−0.500.500.10.20.30.40.50.60.70.80.91Signed coherence vs % of rightward responsesSigned coherence% Rightward responses  1,11,22,12,2−1−0.500.5−0.8−0.6−0.4−0.200.20.40.60.8u1u2Input preference for trained neuronsneuron as tracking the sum of x(t) , and hence call it an integrator neuron. The average activity of a typical integrating
neuron is shown in Figure 2b. In the second pattern, the unit begins near its maximum activation, and its activation
decreases as it receives information favoring one direction. We call this type of unit an reverse integrator neuron.
We examined neural data collected and provided to us by A. Rorie, W.T. Newsome, and J. Guan for neurons exhibiting
these patterns in monkeys performing the task. We found four neurons of ﬁfty-two that robustly showed this integrating
response pattern. One such neuron is shown in Figure 2a (compare to Figure 2b). A number of other neurons showed
robust integrating processes in some but not all of the reward conditions. However, no reverse integrators were found
in the monkey data. This observation is consistent with the general ﬁnding of sparse ﬁring rates in the brain. Although
we expected reverse integrators in our deterministic network, we had hoped that they would not be present in the
stochastic network since the variance of the noise added to each hidden neuron scales linearly with that neuron’s
activation. We reasoned that this would push the network toward lower ﬁring rates to reduce the impact of this noise,
as reported by [3]. However this was not observed, and our stochastic network was very close in character to the
deterministic one.
To compare the behavioral data to that of the model, we analyze the percentage of rightward responses as a function
of coherence and reward condition. The resulting curve is called a psychometric function, and is shown in Figure
3a for the monkey data. The psychometric function shows intuitive behavior:
the equal reward conditions yield
indistinguishable curves, and the asymmetric reward conditions shift the curves in the direction of higher reward.
Figure 3b shows the psychometric function calculated based on ﬁve trained networks, each one trained for a different
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) > T the optimal decision
since the sum of the inputs is bounded by T , i.e. (cid:12)(cid:12)(cid:80)
t x(t) (cid:12)(cid:12) ≤ T , so for
coherence level. The curves show the same general pattern, except that the asymmetric reward conditions for the
model go to zero or one at coherences near zero. This pattern is actually optimal for our discrete setting of the task,
log(r+ /r− )
log(p(cid:48) /(1−p(cid:48) ))
will be ﬁxed regardless of the input sequence. Since these bumps are clearly not observed in the monkey data, our
computational model of the task is too crude to properly capture monkey behavior near low coherences. To mitigate
log(r+ /r− )
log(p(cid:48) /(1−p(cid:48) )) is smaller than T .
these problems in the future, the parameters could be chosen so that
Finally, we examine the structure of the learned solution. Models of the equal reward case often posit one group of
neurons sensitive to motion in one direction and another group sensitive to motion in the other direction. Our model
had the freedom to become selective to any mixture of the two input components u1 and u2 . To see if neurons in the
model become sensitive to one direction of motion or the other, we plot the unit vector in the direction of the weights
to these input components in Figure 4. A weight vector aligned with one axis or the other would indicate a neuron
selective only to one direction of motion. Instead, we see weight vectors aligned with the negative diagonal, indicating
opposed input weights of equal magnitude. That is, our neurons become selective to both directions of motion.

7 Conclusion

The model presented in this paper exhibits a number of features of the neural and behavioral data for monkeys, yet the
discrepancies between the model and data remain large. We believe that the successes of the model argue for continued
investigation. If the basic hypothesis is correct, i.e., the neural responses are indeed the result of optimizing the neural
system subject to constraints imposed by its architecture and biological substrate, then we can set conditions under
which we would expect the model to reproduce the experimental data. First, we can ask if we have the proper model of
the computation. The optimal solution to the proper model should match the behavioral data. Next, we can ask if we
have successfully trained a neural network to implement the optimal decision rule. Since we know the optimal rule,
it is easy to determine whether the problem lies in the training process. Finally, we can ask if we have successfully
captured the constraints of the biological system.

References

[1] S. Feng, P. Holmes, A. Rorie, and W.T. Newsome. “Can monkeys choose optimally when faced with noisy stimuli and unequal
rewards?” Submitted, PLOS Computational Biology, Aug 2008.
[2] R. Bogacz, E. Brown, J. Moehlis, P. Holmes, J.D. Cohen. “The physics of optimal decision making: A formal analysis of
models of performance in two-alternative forced choice tasks.” Psychological Review, Vol 113(4), Oct 2006, 700-765.
[3] E. Todorov. “Mixed muscle-movement representations emerge from optimization of stochastic sensorimotor transformations.”
Submitted 2007.
[4] P.J. Werbos. “Back Propagation Through Time: What it Does and How to Do it.” Proceedings of the IEEE, Vol 78(10), 1990,
1550-1560.


Relative Margin Machines

Pannagadatta K Shivaswamy and Tony Jebara
Department of Computer Science, Columbia University, New York, NY
pks2103,jebara@cs.columbia.edu

Abstract

In classiﬁcation problems, Support Vector Machines maximize the margin
of separation between two classes. While the paradigm has been success-
ful, the solution obtained by SVMs is dominated by the directions with
large data spread and biased to separate the classes by cutting along large
spread directions. This article proposes a novel formulation to overcome
such sensitivity and maximizes the margin relative to the spread of the
data. The proposed formulation can be eﬃciently solved and experiments
on digit datasets show drastic performance improvements over SVMs.

1 Introduction

The goal of most machine learning problems is to generalize from a limited number of
training examples. For example, in support vector machines [10] (SVMs) a hyperplane 1
of the form w⊤x + b = 0, w ∈ Rm , x ∈ Rm , b ∈ R is recovered as a decision boundary
after observing a limited number of training examples. The parameters of the hyperplane
(w, b) are estimated by maximizing the margin (the distance between w⊤x + b = 1 and
w⊤x + b = −1) while minimizing a weighted upper bound on the misclassiﬁcation rate on
the training data (the so called slack variables). In practice, the margin is maximized by
minimizing 1
2 w⊤w.
While this works well in practice, we point out that merely changing the scale of the data
can give a diﬀerent solution. On one hand, an adversary can exploit this shortcoming to
transform the data so as to give bad performance. More distressingly, this shortcoming
can naturally lead to a bad performance especially in high dimensional settings. The key
problem is that SVMs simply ﬁnd a large margin solution giving no attention to the spread
of the data. An excellent discriminator lying in a dimension with relatively small data
spread may be easily overlooked by the SVM solution.
In this paper, we propose novel
formulations to overcome such a limitation. The crux here is to ﬁnd the maximum margin
solution with respect to the spread of the data in a relative sense rather than ﬁnding the
absolute large margin solution.

Linear discriminant analysis ﬁnds a pro jection of the data so that the inter-class separation
is large while within class scatter is small. However, it only makes use of the ﬁrst and
the second order statistics of the data. Feature selection with SVMs [12] remove that have
low discriminative value. Ellipsoidal kernel machines [9] normalize data in feature space
by estimating bounding ellipsoids. While these previous methods showed performance im-
provements, both relied on multiple-step locally optimal algorithms for interleaving spread
information with margin estimation. Recently, additional examples were used to improve
the generalization of the SVMs with so called “Universum” samples [11]. Instead of leverag-
ing additional data or additional model assumptions such as axis-aligned feature selection,

1 In this paper we use the dot product w
an inner product.

⊤

x with the understanding that it can be replaced with

1

the proposed method overcomes what seems to be a fundamental limitation of the SVMs
and subsequently yield improvements in the same supervised setting. In addition, the for-
mulations derived in this paper are convex, can be eﬃciently solved and admit some useful
generalization bounds.

Notation Boldface letters indicate vectors/matrices. For two vectors u ∈ Rm and v ∈ Rm ,
u ≤ v indicates that ui ≤ vi for all i from 1 to m. 1, 0 and I denote the vectors of all ones,
all zeros and the identity matrix respectively. Their dimensions are clear from the context.

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

Figure 1: Top: As the data is scaled along the x-axis, the SVM solution (red or dark shade)
deviates from the maximum relative margin solution (green or light shade). Bottom: The
pro jections of the examples in the top row on the real line for the SVM solution (red or
dark shade) and the proposed classiﬁer (green or light shade) in each case.

2 Motivation with a two dimensional example

Let us start with a simple two dimensional toy dataset to illustrate a problem with the
SVM solution. Consider the binary classiﬁcation example shown in the top row of Figure
1 where squares denote examples from one class and triangles denote examples from the
other class. Consider the leftmost plot in the top row of Figure 1. One possible decision
boundary separating the two classes is shown in green (or light shade). The solution shown
in red (or dark shade) is the SVM estimate; it achieves the largest margin possible while
still separating both the classes. Is this necessarily “the best” solution?

Let us now consider the same set of points after scaling the x-axis in the second and the
third plots. With progressive scaling, the SVM increasingly deviates from the green solution,
clearly indicating that the SVM decision boundary is sensitive to aﬃne transformations of
the data and produces a family of diﬀerent solutions as a result. This sensitivity to scaling
and aﬃne transformations is worrisome. If there is a best and a worst solution in the family
of SVM estimates, there is always the possibility that an adversary exploits this scaling such
that the SVM solution we recover is poor. Meanwhile, an algorithm producing the green
decision boundary remains resilient to such adversarial scalings.

In the previous example, a direction with a small spread in the data produced a good
discriminator. Merely ﬁnding a large margin solution, on the other hand, does not recover
the best possible discriminator. This particular weakness in large margin estimation has
only received limited attention in previous work. In the above example, suppose each class is
generated from a one dimensional distribution on a line with the two classes on two parallel
lines. In this case, the green decision boundary should obtain zero test error even if it is
estimated from a ﬁnite number of samples. However, for ﬁnite training data, the SVM
solution will make errors and will do so increasingly as the data is scaled along the x-axis.
Using kernels and nonlinear mappings may help in some cases but might also exacerbate
such problems. Similarly, simple prepossessing of the data (aﬃne “whitening” to make the

2

dataset zero mean and unit covariance or scaling to place the data into a zero-one box) may
fail to resolve such problems.

For more insight, consider the uni-dimensional pro jections of the data given by the green and
red solutions in the bottom row of Figure 1. In the green solution, all points in the ﬁrst class
are mapped to a single coordinate and all points in the other class are mapped to another
(distinct) coordinate. Meanwhile, the red solution produces more dispersed pro jections of
the two classes. As the adversarial scaling is increased, the spread of the pro jection in the
SVM solution increases correspondingly. Large margins are not suﬃcient on their own and
what is needed is a way to also control the spread of the data after pro jection. Therefore,
rather than just maximizing the margin, a trade-oﬀ regularizer should also be used to
minimize the spread of the pro jected data.
In other words, we will couple large margin
estimation with regularization which seeks to bound the spread |w⊤x + b| of the data. This
will allow the linear classiﬁer to recover large margin solutions not in the absolute sense but
rather relative to the spread of the data in that pro jection direction.

3 Formulations

Given (xi , yi )n
i=1 where xi ∈ Rm and yi ∈ {±1} drawn independent and identically dis-
tributed from a distribution Pr(x, y ), the Support Vector Machine primal formulation 2 is
as follows:

min
w,b,ξ≥0

1
2 kwk2 + C ξ⊤1 s.t. yi (w⊤ xi + b) ≥ 1 − ξi , ∀1 ≤ i ≤ n.
The above formulation minimizes an upper bound on the misclassiﬁcation while maximizing
the margin (the two quantities are traded oﬀ by C ). In practice, the following dual of the
formulation (1) is solved:

(1)

1
2

(2)

0≤α≤C 1 −
max

αi s.t. α⊤y = 0.

αiαj yiyj x⊤
i xj +

n
n
n
Xi=1
Xi=1
Xj=1
It is easy to see that the above formulation (2) is rotation invariant; if all the xi are replaced
by Axi where A ∈ Rm×m , A⊤A = I, then the solution remains the same. However, the
solution is not guaranteed to be the same when A is not a rotation matrix. In addition, the
solution is sensitive to translations as well.
Typically, the dot product between the examples is replaced by a kernel function k : Rm ×
Rm → R such that k(xi , xj ) = φ(xi )⊤φ(xj ), where φ : Rm → H is a mapping to a Hilbert
space to obtain non-linear decision boundaries in the input space. Thus, in (2), x⊤
i xj is
replaced by k(xi , xj ) to obtain non-linear solutions. In rest of this paper, we denote by
K ∈ Rn×n the Gram matrix, whose individual entries are given by Kij = k(xi , xj ).
Next, we consider the formulation which corresponds to whitening the data with the covari-
i=1 xi Pn
n2 Pn
n Pn
n Pn
ance matrix. Denote by Σ = 1
j , and µ = 1
i − 1
j=1 x⊤
i=1 xix⊤
i=1 xi , the
sample covariance and mean respectively. Consider the following formulation which we call
Σ-SVM:
1 − D
D
1
2 wk2 + C ξ⊤1 s.t. yi (w⊤ (xi − µ) + b) ≥ 1 − ξi ,
kwk2 +
2 kΣ
2
where 0 ≤ D ≤ 1 is an additional parameter that trades oﬀ between the two regularization
terms.

min
w,b,ξ≥0

(3)

The dual of (3) can be shown to be:

max
0≤α≤C 1,y⊤α=0

n
n
n
Xj=1
Xi=1
Xi=1
2After this formulation, we stop explicitly writing ∀1 ≤ i ≤ n since it will be obvious from the
context.

αi yi (xi − µ)⊤ ((1 − D)I + DΣ)−1

αj yj (xj − µ).

αi −

(4)

1
2

3

It is easy to see that the above formulation (4) is translation invariant and tends to an aﬃne
invariant solution when D tends to one. When 0 < D < 1, it can be shown, by using the
Woodbury matrix inversion formula, that the above formulation can be “kernelized” simply
by replacing the dot products x⊤
i xj in (2) by:
1 − D  k(xi , xj ) −
n2 !
K⊤
K⊤
1⊤K1
j 1
1
i 1
n −
n
1 − D  (cid:18)Ki −
n (cid:19)! ,
n (cid:19)⊤ (cid:18) I
11⊤
11⊤
(cid:18)Kj −
n2 (cid:19)(cid:21)
n2 (cid:19) (cid:20) 1 − D
I + K (cid:18) I
1
K1
K1
n −
n −
D
where Ki is the ith column of K. For D = 0 and D = 1, it is much easier to obtain the
kernelized formulations. Note that the above formula involves a matrix inversion of size n,
making the kernel computation alone O(n3 ).
3.1 RMM and its geometrical interpretation

−

+

−1

From Section 2, it is clear that large margin in the absolute sense might be deceptive and
could merely be a by product of bad scaling of the data. To overcome this limitation, as
we pointed out earlier, we need to bound the pro jections of the training examples as well.
As in the two dimensional example, it is necessary to trade oﬀ between the margin and the
spread of the data. We propose a slightly modiﬁed formulation in the next section that
can be solved eﬃciently. For now, we write the following formulation, mainly to show how
it compares with the Σ-SVM. In addition, writing the dual of the following formulation
gives some geometric intuition. Since we trade oﬀ between the pro jections and the margin,
implicitly, we ﬁnd large relative margin. Thus we call the following formulation the Relative
Margin Machine (RMM):

min
w,b,ξ≥0

1
(w⊤xi + b)2 ≤
2 kwk2 + C ξ⊤1 s.t. yi (w⊤ xi + b) ≥ 1 − ξi ,
This is a quadratically constrained quadratic problem (QCQP). This formulation has one
extra parameter B in addition to the SVM parameter. Note that B ≥ 1 since having a
B less than one would mean none of the examples would satisfy yi (w⊤ xi + b) ≥ 1. Let
wC and bC be the solutions obtained by solving the SVM (1) for a particular value of C ,
then B > maxi |w⊤
C xi + bC |, makes the constraint on the second line in the formulation (5)
inactive for each i and the solution obtained is the same as the SVM estimate.

(5)

1
2

.

B 2
2

For smaller B values, we start getting diﬀerent solutions. Speciﬁcally, with a smaller B , we
still ﬁnd a large margin solution such that all the pro jections of the training examples are
bounded by B . Thus by trying out diﬀerent B values, we explore diﬀerent large margin
solutions with respect to the pro jection and spread of the data.

In the following, we assume that the value of B is smaller than the threshold mentioned
above. The Lagrangian of (5) is given by:
n
n
λi (cid:18) 1
B 2(cid:19) ,
1
Xi=1
Xi=1
αi (cid:0)yi (w⊤ xi + b) − 1 + ξi (cid:1) − β⊤ξ +
2 kwk2 + C ξ⊤1 −
(w⊤ xi + b)2 −
2
where α, β , λ ≥ 0 are the Lagrange multipliers corresponding to the constraints. Diﬀer-
entiating with respect to the primal variables and equating them to zero, it can be shown
that:
n
n
n
n
n
1
Xi=1
Xi=1
Xi=1
Xi=1
Xi=1
λiw⊤xi ), C 1 = α + β .
λixix⊤
i )w − b
αi yixi , b =
(I +
(
λ⊤1
Denoting by Σλ = Pn
i=1 λixi Pn
λ⊤ 1 Pn
λ⊤ 1 Pn
i − 1
j , and by µλ = 1
i=1 λixix⊤
j=1 λj x⊤
j=1 λj xj
the dual of (5) can be shown to be:
n
n
1
Xi=1
Xi=1
αi yi (xi − µλ )⊤ (I + Σλ )−1
B 2λ⊤1 (6)
αj yj (xj − µλ ) −
αi −
max
2
0≤α≤C 1,λ≥0

n
Xj=1

αi yi −

λixi =

1
2

1
2

4

Note that the above formulation is translation invariant since µλ is subtracted from each xi .
Σλ corresponds to a “shape matrix” (potentially low rank) determined by xi ’s that have
2 (w⊤ xi + b)2 − B 2
non-zero λi . From the KKT conditions of (5), λi ( 1
2 ) = 0. Consequently
2 (w⊤ xi + b)2 − B 2
λi > 0 implies ( 1
2 ) = 0.
Geometrically, in the above formulation (6), the data is whitened with the matrix (I + Σλ )
while solving SVM. While this is similar to what is done by the Σ-SVM, the matrix (I + Σλ )
is determined jointly considering both the margin of the data and the spread. In contrast,
in Σ-SVM, whitening is simply a prepossessing step which can be done independently of the
margin. Note that the constraint 1
2 (w⊤ xi + b)2 ≤ 1
2 B 2 can be relaxed with slack variables at
the expense of one additional parameter however this will not be investigated in this paper.

The proposed formulation is of limited use unless it can be solved eﬃciently. Solving (6)
amounts to solving a semi-deﬁnite program; it cannot scale beyond a few hundred data
points. Thus, for eﬃcient solution, we consider a diﬀerent but equivalent formulation.
2 (w⊤ xi + b)2 ≤ 1
Note that the constraint 1
2 B 2 can be equivalently posed as two linear
constraints : (w⊤ xi + b) ≤ B and −(w⊤xi + b) ≤ B . With these constraints replacing
the quadratic constraint, we have a quadratic program to solve. In the primal, we have 4n
constraints (including ξ ≥ 0 ) instead of the 2n constraints in the SVM. Thus, solving RMM
as a standard QP has the same order of complexity as the SVM. In the next section, we
brieﬂy explain how the RMM can be solved eﬃciently from the dual.

3.2 Fast algorithm

The main idea for the fast algorithm is to have linear constraints bounding the pro jections
rather than quadratic constraints. The fast algorithm that we developed is based on SVMlight
[5]. We ﬁrst write the equivalent of (5) with linear constraints:

min
w,b,ξ≥0

1
2 kwk2 + C ξ⊤1 s.t. yi (w⊤xi + b) ≥ 1 − ξi , w⊤xi + b ≤ B , − w⊤ xi − b ≤ B . (7)

The dual of (7) can be shown to be the following:

1
(α ⊗ y − λ + λ∗ )⊤ K (α ⊗ y − λ + λ∗ ) + α⊤1 − Bλ⊤1 − Bλ∗⊤1
α,λ,λ∗ −
max
2
s.t. α⊤y − λ⊤1 + λ∗⊤1 = 0, 0 ≤ α ≤ C 1, λ, λ∗ ≥ 0,
where, the operator ⊗ denotes the element-wise product of two vectors.
The above QP (8) is solved in an iterative way. In each step, only a subset of the dual
variables are optimized. Let us say, q , r and s ( ˜q , ˜r and ˜s) are the indices to the free (ﬁxed)
variables in α, λ and λ∗ respectively (such that q ∪ ˜q = {1, 2, · · · n} and q ∩ ˜q = ∅, similarly
for the other two indices) in a particular iteration. Then the optimization over the free
variables in that step can be expressed as:
⊤
Ksq −Ksr Kss # " αq ⊗ yq
2 " αq ⊗ yq
" Kqq −Kqr Kqs
#
#
1
−Krq Krr −Krs
λr
λr
λ∗
λ∗
s
s
⊤
Ks ˜q −Ks˜r Ks˜s # " α ˜q ⊗ y ˜q
2 " αq ⊗ yq
" Kq ˜q −Kq ˜r Kq ˜s
# + α⊤
#
1
r 1 − Bλ∗⊤
q 1 − Bλ⊤
−Kr ˜q Kr ˜r −Kr ˜s
−
λ˜r
λr
s 1
λ∗
λ∗
s
˜s
˜s 1, 0 ≤ αq ≤ C 1, λr , λ∗
˜r 1 − λ∗⊤
˜q y ˜q + λ⊤
s 1 = −α⊤
r 1 + λ∗⊤
q yq − λ⊤
s.t. α⊤
s ≥ 0.
Note that while the ﬁrst term in the ob jective above is quadratic in the free variables (over
which it is optimized), the second term is only linear.

s −
max
αq ,λr ,λ∗

(8)

(9)

The algorithm, solves a small sub-problem like (9) in each step until the KKT conditions
of the formulation (8) are satisﬁed to a given tolerance. In each step, the free variables are
selected using heuristics similar to those in SVMlight but slightly adapted to our formulation.

5

We omit the details due to lack of space. Since only a small subset of the variables is
optimized, book-keeping can be done eﬃciently in each step. Moreover, the algorithm can
be warm-started with a previous solution just like SVMlight .

4 Experiments

Experiments were carried out on three sets of digits - optical digits from the UCI machine
learning repository [1], USPS digits [6] and MNIST digits [7]. These datasets have diﬀerent
number of features (64 in optical digits, 256 in USPS and 784 in MNIST) and training
examples (3823 in optical digits, 7291 in USPS and 60000 in MNIST). In all these multi-
class experiments one versus one classiﬁcation strategy was used. We start by noting that,
on the MNIST test set, an improvement of 0.1% is statistically signiﬁcant [3, 4]. This
corresponds to 10 or fewer errors by one method over another on the MNIST test set.

All the parameters were tuned by splitting the training data in each case in the ratio 80:20
and using the smaller split for validation and the larger split for training. The process
was repeated ﬁve times over random splits to pick best parameters (C for SVM, C and
D for Σ-SVM and C and B for RMM). A ﬁnal classiﬁer was trained for each of the 45
classiﬁcation problems with the best parameters found from cross validation using all the
training examples in those classes.

In the case of MNIST digits, training Σ-SVM and KLDA are prohibitive since they involve
inverting a matrix. So, to compare all the methods, we conducted an experiment with 1000
examples per training. For the larger experiments we simply excluded Σ-SVM and KLDA.
The larger experiment on MNIST consisted of training with two thirds of the digits (note
that this amounts to training with 8000 examples on an average for each pair of digits) for
each binary classiﬁcation task. In both the experiments, the remaining training data was
used as a validation set. The classiﬁer that performed the best on the validation set was
used for testing.

Once we had 45 classiﬁers for each pair of digits, testing was done on the separate test set
available in each of these three datasets (1797 examples in the case of optical digits, 2007
examples in USPS and 10000 examples in MNIST). The ﬁnal prediction given for each test
example was based on the ma jority of predictions made by the 45 classiﬁers on the test
example with ties broken uniformly at random.

Table 1 shows the result on all the three datasets for polynomial kernel with various degrees
and the RBF kernel. For each dataset, we report the number of misclassiﬁed examples using
the ma jority voting scheme mentioned above. It can be seen that while Σ-SVM usually
performs much better compared to SVM, RMM performs even better than Σ-SVM in most
cases. Interestingly, with higher degree kernels, Σ-SVM seems to match the performance
of the RMM, but in most of the lower degree kernels, RMM outperforms both SVM and
Σ-SVM convincingly. Since, Σ-SVM is prohibitive to run on large scale datasets, the RMM
was clearly the most competitive method in these experiments.

Training with entire MNIST We used the best parameters found by crossvalidation
in the previous experiments on MNIST and trained 45 classiﬁers for both SVM and RMM
with al l the training examples for each class in MNIST for various kernels. The test results
are reported in Table 1; the advantage still carries over to the full MNIST dataset.

4

3.5

3

2.5

2

1.5

1

0.5

0

 
3

SVM
RMM B1
RMM B2
RMM B3

 

3.1

3.2

3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

Figure 2: Log run time versus log number of examples from 1000 to 10000 in steps of 1000.

6

1
SVM
71
Σ-SVM 61
71
KLDA
RMM
71
145
SVM
Σ-SVM 132
132
KLDA
153
RMM
SVM
696
Σ-SVM 671
1663
KLDA
689
RMM
552
SVM
RMM
534
536
SVM
RMM
521

2
57
48
57
36
109
108
119
109
511
470
848
342
237
164
198
146

3
54
41
54
32
109
99
121
94
422
373
591
319
200
148
170
140

4
47
36
47
31
103
94
117
91
380
341
481
301
183
140
156
130

5
40
35
40
33
100
89
114
91
362
322
430
298
178
123
157
119

6
46
31
46
30
95
87
118
90
338
309
419
290
177
129
141
116

7
46
29
46
29
93
90
117
90
332
303
405
296
164
129
136
115

RBF
51
47
45
51
104
97
101
98
670
673
1597
613
166
144
146
129

OPT

USPS

1000-MNIST

2/3-MNIST

Full MNIST

Table 1: Number of digits misclassiﬁed with various kernels by SVM, Σ-SVM and RMM
for three diﬀerent datasets.

Run time comparison We studied the empirical run times using the MNIST digits 3 vs
8 and polynomial kernel with degree two. The tolerance was set to 0.001 in both the cases.
The size of the sub-problem (9) solved was 500 in all the cases. The number of training
examples were increased in steps of 1000 and the training time was noted. C value was
set at 1000. SVM was ﬁrst run on the training examples. The value of maximum absolute
prediction θ was noted. We then tried three diﬀerent values of B for RMM, B1 = 1+(θ−1)/2,
B2 = 1 + (θ − 1)/4 B3 = 1 + (θ − 1)/10. In all the cases, the run time was noted. We show
a log-log plot comparing the number of examples to the run time in Figure 2. Both SVM
and RMM have similar asymptotic behavior. However, in many cases, warm starting RMM
with previous solution signiﬁcantly helped in reducing the run times.

5 Conclusions

We identiﬁed a sensitivity of Support Vector Machines and maximum absolute margin cri-
teria to aﬃne scalings. These classiﬁers are biased towards producing decision boundaries
that separate data along directions with large data spread. The Relative Margin Machine
was proposed to overcome such a problem and optimizes the pro jection direction such that
the margin is large only relative to the spread of the data. By deriving the dual with
quadratic constraints, a geometrical interpretation was also formulated for RMMs. An im-
plementation for RMMs requiring only additional linear constraints in the SVM quadratic
program leads to a competitively fast implementation. Experiments showed that while aﬃne
transformations can improve over the SVMs, RMM performs even better in practice.

The maximization of relative margin is fairly promising as it is compatible with other popular
problems handled by the SVM framework such as ordinal regression, structured prediction
etc. These are valuable future extensions for the RMM. Furthermore, the constraints that
bound the pro jection are unsupervised; thus RMMs can readily work in semi-supervised
and transduction problems. We will study these extensions in detail in an extended version
of this paper.

References

[1] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.

[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463–482, 2002.

[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep
networks.
In Advances in Neural Information Processing Systems 19, pages 153–160. MIT
Press, Cambridge, MA, 2007.

7

[4] D. Decoste and B. Sch¨olkopf. Training invariant support vector machines. Machine Learning,
pages 161–190, 2002.

[5] T. Joachims. Making large-scale support vector machine learning practical. In Advances in
Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA, 1998.

[6] Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L. Jackel.
Back-propagation applied to handwritten zip code recognition. Neural Computation, 1:541–
551, 1989.

[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[8] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004.

[9] P. K. Shivaswamy and T. Jebara. Ellipsoidal kernel machines. In Proceedings of the Artiﬁcial
Intel ligence and Statistics, 2007.

[10] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995.

[11] J. Weston, R. Collobert, F. H. Sinz, L. Bottou, and V. Vapnik. Inference with the universum.
In Proceedings of the International Conference on Machine Learning, pages 1009–1016, 2006.

[12] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection
for SVMs. In Neural Information Processing Systems, pages 668–674, 2000.

A Generalization Bound

In this section, we give the empirical Rademacher complexity [2, 8] for function classes used
by the SVM, and modiﬁed versions of RMM and Σ-SVM which can be plugged into a
generalization bound.

Maximizing the margin can be seen as choosing a function f (x) = w⊤x from a bounded class
of functions FE := {x → w⊤ x| 1
2 kwk2 ≤ E }. For a technical reason, instead of bounding
the pro jection on the training examples as in (5), we consider bounding the pro jections
on an independent set of examples drawn from Pr(x), that is, a set U = {u1 , u2 , . . . unu }.
Note that if we have an iid training set, it can be split into two parts and one part can be
used exclusively to bound the pro jections and the other part can be used exclusively for
classiﬁcation constraints. Since the labels of the examples used to bound the pro jections
do not matter, we denote this set by U and the other part of the set by (xi , yi )n
i=1 We
now consider the following function class which is closely related to RMM: HE ,D := {x →
2 w⊤w + D
w⊤x| 1
2 (w⊤ui )2 ≤ E ∀1 ≤ i ≤ nu} where D > 0 trades oﬀ between large margin
and small bound on the pro jections. Similarly, consider: GE ,D := {x → w⊤x| 1
2 w⊤w +
2nu Pnu
D
i=1 (w⊤ui )2 ≤ E }, which is closely related to the class of functions considered by
Σ-SVM. The empirical Rademacher complexities of the three classes of functions are as
below:
2√2E
2√2E
n vuut
n vuut
n
ˆR(GE ,D ) ≤ UGE ,D :=
Xi=1
nu
n
2
1
ˆR(HE ,D ) ≤ UHE ,D := min
Xi=1
Xi=1
i Σ−1
x⊤
λi ,
λ,D xi +
n
n
λ≥0
i=1 λi I + D Pnu
i and Σλ,D = Pnu
nu Pnu
where ΣD = I + D
i=1 λiuiu⊤
i=1 uiu⊤
i . Note that the last
upper bound is not a closed form expression, but a semi-deﬁnite optimization. Now, the
upper bounds UFE , UGE ,D and UHE ,D can be plugged in the following theorem in place of
ˆR(F ) to obtain Rademacher type generalization bounds.
Theorem 1 Fix γ > 0, let F be the class of functions from Rm × {±1} → R given by
f (x, y ) = −yg (x). Let {(x1 , y1 ), . . . , (xn , yn )} be drawn iid from a probability distribution
D. Then, with probability at least 1 − δ over the samples of size n, the fol lowing bound holds:
PrD [y 6= sign(g (x))] ≤ ξ⊤1/n + 2 ˆR(F )/γ + 3p(ln(2/δ))/2n, where ξi = max(0, 1 − yi g (xi ))
are the so-cal led slack variables.

ˆR(FE ) ≤ UFE :=

x⊤
i xi ,

i Σ−1
x⊤
D xi ,

n
Xi=1

E

8


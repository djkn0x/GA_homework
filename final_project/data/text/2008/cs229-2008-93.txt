Chinese (Restaurant) Menu Translation
Ting Liao

1 Description of the problem

When you goes to a Chinese restaurant, especially when traveling in Asia, you will have no idea what 
the menu says even if there is an English translation on it. You will see names from “burn the spring 
chicken” to “cowboy meat”. Even the menu at Olympic Village during the 2008 Beijing Olympics has 
similar problems. The main reason is that a normal translator cannot generally handle the name of a 
dish due to the lack of sentence structure. When you simply group a set of nouns together,  as in most 
of the cases in names of Chinese dishes, it would be hard for the translator to decide what to do when 
the words have multiple meanings.

If we want to build a general translator that can handle food translation, it would be very hard. 
However, if we create a translator that is specifically for food translation, the problem becomes much 
easier to tackle. First of all we do not have to worry about the context, we know it has to do with food, 
and knowing that the numbers of useful vocabularies is easier to handle. This paper describes a 
supervised learning algorithm that trains the computer to translate the name of a dish in Chinese to 
English.

2 The idea

In the movie “Terminal”, Tom Hanks learned the English language by comparing a booklet in English 
and the same booklet in the fictional Krakozhian. The same idea applies here. The training set contains 
entries consisted of the Chinese name of a dish and the corresponding English name of the dish. The 
challenge to is break up to Chinese name such that we can find the Chinese word(s) that corresponds to 
a certain English word(s) in the name. Unlike the English Language, Chinese words are not made up by 
letters, but instead they are made up with characters. A Chinese word can be a single character or a 
multi-character one. (however a word with 5 or more characters is extremely rare). Another difference 
between the languages is that Chinese words are not separated by space. So when you see two Chinese 
characters together, it could be one word or it could be two, and we will have to account for that.

3 Model

We assume that there is a correct objective translation F(C) →+ E, which means whenever we see the 
word C, we can translate it to multiple E's, the different E's should only happen when the word C is 
used for different meanings. By objective I mean if two C's have the same meaning, they cannot return 
a different E. Consider a training set of size m (we have m pairs), the Chinese word C(i) is the Chinese 
translation of E(i). C(i) = C1
(i) C2
(i)....Cn
(i) where Cj
(i) is the j-th character for the Chinese word with length 
n and Ej
(i) is the j-th word in the English word. The goal is to create a set of translation rules F such that 
when we see a Chinese word C, we can apply F(C) → E for the English translation E.

Obviously, if we choose F that makes. F(C(i)) → E(i), then we will get a 100% match for the training set. 
But as you can imagine, if we get anything that is not in the set, we will fail to translate it, so this is not 
very useful. If we know the word “chicken”, we should be able to use this knowledge in other context. 
So if there is a word Cj we see multiple times in different context, and their English translations contain 
the same specific sequence E, the rule F(Cj) → E will be a useful one because the more appearances of 
this translation in the training set could mean that there is a higher probability we see this rule used in 
words outside of this training set. We assign a score to each translation guess. The score should reflect 

two things :
1. The more this guess appears, the higher the score should be
2. A rule that translates an n-length C is preferred to  a translation rule for a shorter C. Otherwise we 
will always translate only the substring of C because if the translation for F(C) → E has x appearance, 
the rule F(substring of C) → must have at least x appearance. But it would seem that F(C) → E should 
be considered if the number of appearance does not differ by too much.

The model I used (which is a bit arbitrary)  is for each rule F(C) → E, where the length of C is n, we 
give it a -n score, but we will treat the set of rules F(C(i)) → E(i) as the starting point, meaning it has 0 
points. Now if we find a rule F(C)->E with a length n C that appears x times, then these n * x 
characters can be represented by 1 rule, I thus treat this rule as scoring n * (x-1) points. We can see that 
this scoring system fits well with the 2 requirements I set if we use a greedy algorithm to try to 
maximize the score. (each Cj
(i) can only be used to help find one rule, after it is used, we delete that 
character. See below).

4 Training

The ideal case here would be maximizing the score, which is find a set of rules F such that
each  F(C) → E for a C with length n that appears x times has score n * (x-1)
find the set of non overlapping (we cannot use two different F for the same Cj
that will maximize the score for this training set.

(i) when keeping score) F's 

The problem is, the total number of possibility of F is enormous because of the multi-character and 
multi-word translation. So it would be hard to find the “real” maximum. Instead I used a greedy 
algorithm to find F. We went through the whole set and find out the single rule F that creates the 
highest scores. (in my dataset, the highest score rule is for the word “rice”, which has score 87.)

The way to do that is simply going through all the words with different lengths, then find out how 
many times the word has appeared in the training set, then we find out the English translation for these 
words and find out which English pattern has appeared most (and x = the number of times the English 
pattern appears). Once we have found the F(C) → E with the highest score at this point, we can remove 
C and E from the training pairs if C is in the Chinese name and E is in the English name of the training 
pair. We replace C and E with *, to make sure we won't check for the word AC after B is removed 
from ABC. We then repeat the process until we cannot find rule (from the left over words) F such that 
it scores higher than 0. This is the first phase of the algorithm

The second phase is to train something I called the “unbreakable”. Which represents a Chinese word 
that cannot be broken up into substring that makes sense, it happens when there is a special name of a 
dish. Therefore we simply add F(C(i)) → E(i) as one of the translation rules. The third phase is to find 
out all the useless words. Sometimes on a menu, there are some useless words (in the sense of 
translations) like “wonderful” and “tasty”. So after removing all our guesses. If the Chinese part of the 
left over pair is not empty but the English part is empty, we make a guess that these Chinese words are 
useless. That means F(C)-> “” . If at this point there are pairs that has only one consecutive sequence of 
characters in the Chinese part and only one sequence of words in the English part, we basically link 
these sequence of Chinese words to the sequence of English words. Since we do not have a great basis 
for these guesses, we simply give them a score of 0. There may be still some pairs that are still not 
completely mapped, so we will have to give up and find out what the pair originally looked like (before 
replacing words with *) and make a direct mapping from the Chinese name to the English name.

5 Testing

When given a Chinese name C, we try to find out what combinations of F(Cj)->E  give the highest 
scores. This method inherently has some problems. For a word that has multiple meaning, if one 
meaning dominates by having a much higher score, the other meaning does not have much chance of 
showing up (unless it can squeeze into the top 3, which means the rest of the words do not allow 
multiple meanings), even though it may be the correct meaning. We tested these translations on the 
original training set, trying to see what is the average hit rate (a word in the guess that is in the original 
English word), and the result turned out to be 90%. I tested it on different subsets of the training set and 
the result varies between 75% to 98%.

6 Problem
I) Positions
The biggest problem I have is with the position of the words. We may get F(C)->E right for a 
substring C, but where should E be put in our final guess? That is a major problem right now. I 
have a method in mind but I am not sure whether it will work. Given each F(C)->E, in addition 
to giving it a score, we assign to it a position score, which is the average “index of E / the length 
of the whole word” of all words where this rule F(C)->E applies. And when we are making the 
final guess, we sort the substring by their position score. This part I have not implemented.
II) Word inconsistency
The other problem is the inconsistency in words. For example, when to use “fried” and “deep 
fried”? They could mean the same thing most of the time. But using my algorithm, we will 
always translate the Chinese word to “fried”. That is not that big a problem by itself, the 
translation is reasonable, but when we are in the later phases of the training, it depends on the 
first phase removing the words accurately and in this case, “deep” is left over and may lead to 
unexpected result. For example a “useless” word will now be translated to “deep” instead of the 
empty string “”. That is not the only problem with inconsistency. We have different tenses and 
we have plural and singular forms. So is it meat slice? Or is it sliced meat? If C is the Chinese 
word for meat slice, F(C) → meat is the most likely result. Although we give weights to the 
length of the Chinese words, we do not really give weight to the length of the English word. 
Inconsistencies simply confuse the algorithm.
Perhaps what we can do here is do a reverse matching, meaning that we will use a similar 
algorithm to find a set of rules G(E)->C, which is supposed to be F-inverse. Training G will put 
weights on the length of the English word such that one mistake (e.g. typo) is not going to 
change F(C)->meat slice to F(C)->slice (if one of the set has “meet slice” instead). This is 
actually a quite important fix. But the problem is that this is not a 1 to 1 mapping. If it were, 
running this algorithm both ways would be a great way to remove inconsistencies. But since F 
is a one to many mapping, G will have to be many to one if it is F-inverse. That would be hard 
to achieve, so the most likely G will end up to be a many to many mapping. We can then try 
each of these rules in G(E)-> C and create a corresponding F(C)->E and see how the rule scores 
in the training set. 
III)One to many mapping
I guess the worst thing about this algorithm is that if the F(C)->E is a one-to-many mapping, 
they we get multiple results. As I have explained above, a less likely translation could be the 
right translation, but my algorithm does not know when is that the case. If somehow the 
translation got you a choice between “beef rice” and “pork rice”, you can't really decide even 
with the help of the translator.

    
8 Application

One of the main point of this algorithm is that we do not need to know much about the languages. We 
do need some minimal knowledge however such as the Chinese words are not separated by space but 
the English language is etc. If we have a perfectly objectively consistently translated menu (between 
languages), we can simply feed it to the machine, then the algorithm will learn whatever it can and be 
used to translate other menu. Provided we have proper character recognition software on cell phones, 
we can take pictures of the menu and find out the guessed meaning of the name, which would be 
interesting.

9 Conclusion

The algorithm itself is not an especially efficient one, for the training data I have 451 training pair 
(which by the way made me really hungry when I worked on this) it took about 30 minutes (after quite 
a bit of optimization).. The slowness is due to the fact that I do not know how to break up the words, 
which lead me to a lot of guessing and thus delaying the result. I got most of my data from restaurants 
around the bay area. But the problem remains : these menus are badly translated and extremely 
inconsistent (and creative), so unfortunately I have to put in my own translated names for the program 
to work. It is unfortunate because my goal is to feed the algorithm with menus that I do not understand 
and have the machine learn it for me. The algorithm can possibly be applied to other fields where the 
context does not involve too many words and where the grammar is not too important in the translation 
(I think “pork rice black bean sauce” is much better than “bone rice”, so even with bad grammar the 
translation here I think is quite useful to some people). I think the translations are helpful to a certain 
degree with a lot of room for improvement, but since it may be related to what you eat and thus maybe 
your health, use with care. Attached is a list with F's that has the highest score.

Training Data obtained from :
Hong Kong Restaurant
Washington Bakery and Restaurant
Tam Cafe

87
rice
飯
74
beef
牛肉
70
chicken
雞
51
fried
炒
46
noodle
麵
38
seafood
海鮮
37
soup
湯
31
vegetable
菜
31
beef
牛
30
c lay pot
煲
28
bean curd
豆腐
28
wonton
雲吞
XO醬
27
xo sauce
27
sandwich
三文治
26
pepper
椒
26
baked
焗
22
pork chop
豬扒
22
spaghetti
意粉
21
egg
蛋
20
curry
咖哩
18
sauce
汁
18
fish
魚
18
macaroni
通粉
17
shrimp
蝦
16
spare rib
排骨
15
black  bean
豉
15
shredded
絲
15
porridge
粥
14
mein
麵
14
instant
公仔
13
salt
鹽
13
pork
肉
12
rice
米
12
ginger
薑蔥
12
satay
沙爹
12
cod
斑球
11
fried
炸
11
chow fun
河
10
prawn
蝦
10
corn
粟米
10
tomato
茄
10
vermicelli
粉絲
10
kung pao
宮保
10
toast
多士
9
meat
肉
8
pork
叉燒
8
egg plant
茄子
8
scallop
瑤柱
8
mushroom
菇
8
satay sauce
沙茶
8
s tew
腩
8
black
黑
8
oyster sauce
蠔油
fig. 1 translation rules with the highest scores

火腿
烏冬
撈
豆
粒
鮮魷
煎
忌廉
芥菜
鴨
招牌
午餐
腸粉
腰肝
水餃
瀨粉
牛仔骨
什
蒜
式
味
柳
蘭
伊
生蠔
菜
羹
節瓜
上湯
蜆
牛
XO
腰果
蛋
炆
家鄉
河
豬
丸
免治
芋頭
三絲
油
蒸
酸
鮑
扒
雪
醬
味
炒
炒
咸

ham
udon
braised
pea
diced
squid
chow
cream
mus tard
duck
special
luncheon
rice
kidney
dumpling
lai fun
short rib
haslet
garlic
style
chinese
fillet
broccoli
yee
oyster
tender
soup
chinese melon
broth
clam
ox
xo sauce
cashew
ball
braised
country style
noodle
pork
ball
minced
taro
assorted meat
butter
steamed
sour
abalone
steak
snow
sauce
preserved
sauteed
sauce
salted

8
8
8
7
7
6
6
6
6
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3


Regularized Policy Iteration

Amir-massoud Farahmand1 , Mohammad Ghavamzadeh2 , Csaba Szepesv ´ari1 , Shie Mannor3 ∗
1Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada
2 INRIA Lille - Nord Europe, Team SequeL, France
3Department of ECE, McGill University, Canada - Department of EE, Technion, Israel

Abstract

In this paper we consider approximate policy-iteration-based reinforcement learn-
ing algorithms. In order to implement a ﬂexible function approximation scheme
we propose the use of non-parametric methods with regularization, providing a
convenient way to control the complexity of the function approximator. We pro-
pose two novel regularized policy iteration algorithms by adding L2 -regularization
to two widely-used policy evaluation methods: Bellman residual minimization
(BRM) and least-squares temporal difference learning (LSTD). We derive efﬁ-
cient implementation for our algorithms when the approximate value-functions
belong to a reproducing kernel Hilbert space. We also provide ﬁnite-sample per-
formance bounds for our algorithms and show that they are able to achieve optimal
rates of convergence under the studied conditions.

1 Introduction

A key idea in reinforcement learning (RL) is to learn an action-value function which can then be
used to derive a good control policy [15]. When the state space is large or inﬁnite, value-function
approximation techniques are necessary, and their quality has a major impact on the quality of the
learned policy. Existing techniques include linear function approximation (see, e.g., Chapter 8 of
[15]), kernel regression [12], regression tree methods [5], and neural networks (e.g., [13]). The user
of these techniques often has to make non-trivial design decisions such as what features to use in
the linear function approximator, when to stop growing trees, how many trees to grow, what kernel
bandwidth to use, or what neural network architecture to employ. Of course, the best answers to
these questions depend on the characteristics of the problem in hand. Hence, ideally, these questions
should be answered in an automated way, based on the training data.
A highly desirable requirement for any learning system is to adapt to the actual difﬁculty of the
learning problem. If the problem is easier (than some other problem), the method should deliver
better solution(s) with the same amount of data. In the supervised learning literature, such proce-
dures are called adaptive [7]. There are many factors that can make a problem easier, such as when
only a few of the inputs are relevant, when the input data lies on a low-dimensional submanifold of
the input space, when special noise conditions are met, when the expansion of the target function
is sparse in a basis, or when the target function is highly smooth. These are called the regularities
of the problem. An adaptive procedure is built in two steps: 1) designing ﬂexible methods with
a few tunable parameters that are able to deliver “optimal” performance for any targeted regular-
ity, provided that their parameters are chosen properly, and 2) tuning the parameters automatically
(automatic model-selection).
Smoothness is one of the most important regularities: In regression when the target function has
smoothness of order p the optimal rate of convergence of the squared L2 -error is n−2p/(2p+d) ,
∗Csaba Szepesv ´ari is on leave from MTA SZTAKI. This research was funded in part by the National Science
and Engineering Research Council (NSERC), iCore and the Alberta Ingenuity Fund. We acknowledge the
insightful comments by the reviewers.

where n is the number of data points and d is the dimension of the input space [7]. Hence, the rate
of convergence is higher for larger p’s. Methods that achieve the optimal rate are more desirable, at
least in the limit for large n, and seem to perform well in practice. However, only a few methods
in the regression literature are known to achieve the optimal rates. In fact, it is known that tree
methods with averaging in the leaves, linear methods with piecewise constant basis functions, and
kernel estimates do not achieve the optimal rate, while neural networks and regularized least-squares
estimators do [7]. An advantage of using a regularized least-squares estimator compared to neural
networks is that these estimators do not get stuck in local minima and therefore their training is more
reliable.
In this paper we study how to add L2 -regularization to value function approximation in RL. The
problem setting is to ﬁnd a good policy in a batch or active learning scenario for inﬁnite-horizon
expected total discounted reward Markovian decision problems with continuous state and ﬁnite ac-
tion spaces. We propose two novel policy evaluation algorithms by adding L2 -regularization to two
widely-used policy evaluation methods in RL: Bellman residual minimization (BRM) [16; 3] and
least-squares temporal difference learning (LSTD) [4]. We show how our algorithms can be imple-
mented efﬁciently when the value-function approximator belongs to a reproducing kernel Hilbert
space. We also prove ﬁnite-sample performance bounds for our algorithms. In particular, we show
that they are able to achieve a rate that is as good as the corresponding regression rate when the
value functions belong to a known smoothness class. We further show that this rate of convergence
carries through to the performance of a policy found by running policy iteration with our regularized
policy evaluation methods. The results indicate that from the point of view of convergence rates
RL is not harder than regression estimation, answering an open question of Antos et al. [2]. Due
to space limitations, we do not present the proofs of our theorems in the paper; they can be found,
along with some empirical results using our algorithms, in [6].
To our best knowledge this is the ﬁrst work that addresses ﬁnite-sample performance of a regularized
RL algorithm. While regularization in RL has not been thoroughly explored, there has been a few
works that used regularization. Xu et al. [17] used sparsiﬁcation in LSTD. Although sparsiﬁcation
does achieve some form of regularization, to the best of our knowledge the effect of sparsiﬁcation
on generalization error is not well-understood. Note that sparsiﬁcation is fundamentally different
from our approach. In our method the empirical error and the penalties jointly determine the solu-
tion, while in sparsiﬁcation ﬁrst a subset of points is selected independently of the empirical error,
which are then used to obtain a solution. Comparing the efﬁciency of these methods requires further
research, but the two methods can be combined, as was done in our experiments. Jung and Polani
[9] explored adding regularization to BRM, but this solution is restricted to deterministic problems.
The main contribution of that work was the development of fast incremental algorithms using sparsi-
ﬁcation techniques. L1 penalties have been considered by [11], who were similarly concerned with
incremental implementations and computational efﬁciency.

2 Preliminaries

As we shall work with continuous spaces, we ﬁrst introduce a few concepts from analysis. This is
followed by an introduction to Markovian Decision Processes (MDPs) and the associated concepts
and notation.
For a measurable space with domain S , we let M(S ) and B (S ; L) denote the set of probability
measures over S and the space of bounded measurable functions with domain S and bound 0 <
L < ∞, respectively. For a measure ν ∈ M(S ), and a measurable function f : S → R, we deﬁne
the L2 (ν )-norm of f , kf kν , and its empirical counterpart kf kν,n as follows:
Z
nX
1
n
t=1

f 2 (st ) , st ∼ ν.

|f (s)|2 ν (ds) ,

(1)

kf k2
ν =

kf k2
ν,n

def=

If {st} is ergodic, kf k2
ν,n converges to kf k2
ν as n → ∞.
A ﬁnite-action discounted MDP is a tuple (X , A, P , S, γ ), where X is the state space, A =
{a1 , a2 , . . . , aM } is the ﬁnite set of M actions, P : X × A → M(X ) is the transition probability
kernel with P (·|x, a) deﬁning the next-state distribution upon taking action a in state x, S (·|x, a)

gives the corresponding distribution of immediate rewards, and γ ∈ (0, 1) is a discount factor. We
make the following assumptions on MDP:
Euclidean space and the expected immediate rewards r(x, a) = R rS (dr |x, a) are bounded by
Assumption A1 (MDP Regularity) The set of states X is a compact subspace of the d-dimensional
Rmax .
We denote by π : X → M(A) a stationary Markov policy. A policy is deterministic if it is a
mapping from states to actions π : X → A. The value and the action-value functions of a policy π ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) X0 = x, A0 = a
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) X0 = x
#
#
" ∞X
" ∞X
denoted respectively by V π and Qπ , are deﬁned as the expected sum of discounted rewards that are
encountered when the policy π is executed:
Qπ (x, a) = Eπ
V π (x) = Eπ
γ tRt
γ tRt
,
.
t=0
t=0
Here Rt denotes the reward received at time step t; Rt ∼ S (·|Xt , At ), Xt evolves according to
Xt+1 ∼ P (·|Xt , At ), and At is sampled from the policy At ∼ π(·|Xt ). It is easy to see that for any
policy π , the functions V π and Qπ are bounded by Vmax = Qmax = Rmax /(1−γ ). The action-value
Z
function of a policy is the unique ﬁxed-point of the Bellman operator T π : B (X × A) → B (X × A)
deﬁned by
Q(y , π(y))P (dy |x, a).
(T πQ)(x, a) = r(x, a) + γ
Given an MDP, the goal is to ﬁnd a policy that attains the best possible values, V ∗ (x) =
supπ V π (x), ∀x ∈ X . Function V ∗ is called the optimal value function. Similarly the optimal
action-value function is deﬁned as Q∗ (x, a) = supπ Qπ (x, a), ∀x ∈ X , ∀a ∈ A. We say that
a deterministic policy π is greedy w.r.t. an action-value function Q and write π = ˆπ(·; Q), if,
π(x) ∈ argmaxa∈A Q(x, a), ∀x ∈ X , ∀a ∈ A. Greedy policies are important because any greedy
policy w.r.t. Q∗ is optimal. Hence, knowing Q∗ is sufﬁcient for behaving optimally. In this paper
we shall deal with a variant of the policy iteration algorithm [8]. In the basic version of policy
iteration an optimal policy is found by computing a series of policies, each being greedy w.r.t. the
action-value function of the previous one.
Throughout the paper we denote by F M ⊂ { f : X × A → R } some subset of real-valued func-
tions over the state-action space X × A, and use it as the set of admissible functions in the op-
timization problems of our algorithms. We will treat f ∈ F M as f ≡ (f1 , . . . , fM ), fj (x) =
PM
f (x, aj ), j = 1, . . . , M . For ν ∈ M(X ), we extend k·kν and k·kν,n deﬁned in Eq. (1) to F M by
kf k2
j=1 kfj k2
MX
nX
nX
ν = 1
ν , and
M
1
1
kf k2
I{At=at } f 2
ν,n =
j (Xt ) =
f 2 (Xt , At ),
nM
nM
t=1
t=1
j=1
where I{·} is the indicator function: for an event E , I{E} = 1 if and only if E holds and I{E} = 0,
otherwise.

(2)

3 Approximate Policy Evaluation

The ability to evaluate a given policy is the core requirement to run policy iteration. Loosely speak-
ing, in policy evaluation the goal is to ﬁnd a “close enough” approximation V (or Q) of the value
(or action-value) function of a ﬁxed target policy π , V π (or Qπ ). There are several interpretations to
the term “close enough” in this context and it does not necessarily refer to a minimization of some
norm. If Qπ (or noisy estimates of it) is available at a number of points (Xt , At ), one can form a
training set of examples of the form {(Xt , At ), Qt}1≤t≤n , where Qt is an estimate of Qπ (Xt , At )
and then use a supervised learning algorithm to infer a function Q that is meant to approximate Qπ .
Unfortunately, in the context of control, the target function, Qπ , is not known in advance and its
high quality samples are often very expensive to obtain if this option is available at all. Most often
these values have to be inferred from the observed system dynamics, where the observations do not
necessarily come from following the target policy π . This is referred to as the off-policy learning
problem in the RL literature. The difﬁculty arising is similar to the problem when training and test
distributions differ in supervised learning. Many policy evaluation techniques have been developed
in RL. Here we concentrate on the ones that are directly related to our proposed algorithms.

3.1 Bellman Residual Minimization

The idea of Bellman residual minimization (BRM) goes back at least to the work of Schweitzer and
Seidmann [14]. It was used later in the RL community by Williams and Baird [16] and Baird [3].
The basic idea of BRM comes from writing the ﬁxed-point equation for the Bellman operator in the
form Qπ − T πQπ = 0. When Qπ is replaced by some other function Q, the left-hand side becomes
non-zero. The resulting quantity, Q − T πQ, is called the Bellman residual of Q. If the magnitude
of the Bellman residual, kQ − T πQk, is small, then Q can be expected to be a good approximation
of Qπ . For an analysis using supremum norms see, e.g., [16]. It seems, however, more natural
to use a weighted L2 -norm to measure the magnitude of the Bellman residual as it leads to an
optimization problem with favorable characteristics and enables an easy connection to regression
function estimation [7]. Hence, we deﬁne the loss function LBRM (Q; π) = kQ − T πQk2
ν , where
ν is the stationary distribution of states in the input data. Using Eq. (2) with samples (Xt , At )
and by replacing (T πQ)(Xt , At ) with its sample-based approximation ( ˆT πQ)(Xt , At ) = Rt +
Rt + γQ(cid:0)Xt+1 , π(Xt+1 )(cid:1)(cid:17)i2
h
Q(Xt , At ) − (cid:16)
nX
γQ(Xt+1 , π(Xt+1 )), the empirical counterpart of LBRM (Q; π) can be written as
1
ˆLBRM (Q; π , n) =
(3)
.
nM
LBRM ; E h ˆLBRM (Q; π , n)
i 6= LBRM (Q; π). The reason is that stochastic transitions may lead to a
t=1
However, as it is well-known (e.g., see [15],[10]), in general, ˆLBRM is not an unbiased estimate of
non-vanishing variance term in Eq. (3). A common suggestion to deal with this problem is to use
uncorrelated or “double” samples in ˆLBRM . According to this proposal, for each state-action pair in
the sample, at least two next states should be generated (e.g., see [15]). This is neither realistic nor
sample-efﬁcient unless a generative model of the environment is available or the state transitions are
deterministic. Antos et al. [2] recently proposed a de-biasing procedure for this problem. We will
refer to it as modiﬁed BRM in this paper. The idea is to cancel the unwanted variance by introducing
an auxiliary function h and a new loss function LBRM (Q, h; π) = LBRM (Q; π) − kh − T πQk2
ν , and
approximating the action-value function Qπ by solving
ˆQBRM = argmin
sup
LBRM (Q, h; π),
h∈F M
Q∈F M
where the supremum comes from the negative sign of kh − T πQk2
ν . They showed that optimizing
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)h − ˆT πQ
h (cid:13)(cid:13)(cid:13)Q − ˆT πQ
− (cid:13)(cid:13)(cid:13)h∗
i
the new loss function still makes sense and the empirical version of this loss is unbiased. Solving
Eq. (4) requires solving the following nested optimization problems:
Q − ˆT πQ
h∗
ˆQBRM = argmin
Q = argmin
Q∈F M
h∈F M
ν
ν
ν
Of course in practice, T πQ is replaced by its sample-based approximation ˆT πQ.

(4)

,

.

(5)

3.2 Least-Squares Temporal Difference Learning

Least-squares temporal difference learning (LSTD) was ﬁrst proposed by Bradtke and Barto [4],
and later was extended to control by Lagoudakis and Parr [10]. They called the resulting algorithm
least-squares policy iteration (LSPI), which is an approximate policy iteration algorithm based on
LSTD. Unlike BRM that minimizes the distance of Q and T πQ, LSTD minimizes the distance of Q
and ΠT πQ, the back-projection of the image of Q under the Bellman operator, T πQ, onto the space
of admissible functions F M (see Figure 1). Formally, this means that LSTD minimizes the loss
function LLST D (Q; π) = kQ − ΠT πQk2
ν . It can also be seen as ﬁnding a good approximation for
the ﬁxed-point of operator ΠT π . The projection operator Π : B (X × A) → B (X × A) is deﬁned
by Πf = argminh∈F M kh − f k2
ν . In order to make this minimization problem computationally
feasible, it is usually assumed that F M is a linear subspace of B (X × A). The LSTD solution can
(cid:13)(cid:13)2
(cid:13)(cid:13)Q − h∗
therefore be written as the solution of the following nested optimization problems:
kh − T πQk2
h∗
ˆQLST D = argmin
Q = argmin
(6)
ν ,
,
Q
h∈F M
Q∈F M
ν
where the ﬁrst equation ﬁnds the projection of T πQ onto F M , and the second one minimizes the
distance of Q and the projection.

Figure 1: This ﬁgure shows the loss functions minimized by
BRM, modiﬁed BRM, and LSTD methods. The function space
F M is represented by the plane. The Bellman operator, T π , maps
an action-value function Q ∈ F M to a function T πQ. The vec-
tor connecting T πQ and its back-projection to F M , ΠT πQ, is
orthogonal to the function space F M . The BRM loss function is
the squared Bellman error, the distance of Q and T πQ. In order
to obtain the modiﬁed BRM loss, the squared distance of T πQ
and ΠT πQ is subtracted from the squared Bellman error. LSTD
aims at a function Q that has minimum distance to ΠT πQ.
Antos et al. [2] showed that when F M is linear, the solution of modiﬁed BRM (Eq. 4 or 5) coincides
Q − T πQ has to be perpendicular to F M , as a result ‚‚Q − h∗
‚‚2 =
kQ − T πQk2 − ‚‚h∗
Q − T πQ‚‚2 (Pythagorean theorem), and therefore the second equations in (5) and
with the LSTD solution (Eq. 6). A quick explanation for this is: the ﬁrst equations in (5) and (6) are
the same, the projected vector h∗
Q
(6) have the same solution.

4 Regularized Policy Iteration Algorithms

Table 1: The pseudo-code of policy-iteration algorithm

FittedPolicyQ(N ,Q(−1) ,PEval)
// N : number of iterations
// Q(−1) : Initial action-value function
// PEval: Fitting procedure
for i = 0 to N − 1 do
πi (·) ← ˆπ(·; Q(i−1) )
// the greedy policy w.r.t. Q(i−1) //
Generate training sample Di
Q(i) ← PEval(πi , Di )
end for
return Q(N −1) or πN (·) = ˆπ(·; Q(N −1) )

In this section, we introduce two regularized policy iteration algorithms. These algorithms are in-
stances of the generic policy-iteration method, whose pseudo-code is shown in Table 1. By assump-
tion, the training sample Di used at the ith (1 ≤ i ≤ N ) iteration of the algorithm is a ﬁnite trajectory
{(Xt , At , Rt )}1≤t≤n
generated
by a policy π , thus, At = π(Xt )
and Rt ∼ S (·|Xt , At ). Examples
of such policy π are πi plus some
exploration or some stochastic
stationary policy πb . The action-
value function Q(−1) is used to
initialize the ﬁrst policy. Alter-
natively, one may start with an
arbitrary initial policy. The proce-
dure PEval takes a policy πi (here
the greedy policy w.r.t. the current
function Q(i−1) )
action-value
along with training sample Di ,
and returns an approximation to
the action-value function of policy πi . There are many possibilities to design PEval. In this paper,
we propose two approaches, one based on regularized (modiﬁed) BRM (REG-BRM), and one based
on regularized LSTD (REG-LSTD). In REG-BRM, the next iteration is computed by solving the
following nested optimization problems:
‚‚‚2
‚‚‚2
‚‚‚2
−‚‚‚h
h ‚‚‚h − ˆT πi Q
h ‚‚‚Q − ˆT πi Q
i
(·; Q) − ˆT πi Q
(·; Q) = argmin
∗
∗
, Q(i) = argmin
+λQ,nJ (Q)
+λh,nJ (h)
h∈FM
Q∈FM
n
n
n
t = (cid:0)Xt+1 , πi (Xt+1 )(cid:1) represent state-action pairs, J (h) and J (Q) are penalty functions (e.g.,
(7)
where ( ˆT πi Q)(Zt ) = Rt + γQ(Z 0
t ) represents the empirical Bellman operator, Zt = (Xt , At ) and
Z 0
norms), and λh,n , λQ,n > 0 are regularization coefﬁcients.
In REG-LSTD, the next iteration is computed by solving the following nested optimization prob-
‚‚‚2
h ‚‚‚h − ˆT πi Q
i
i
h kQ − h
lems:
(·; Q) = argmin
(·; Q)k2
∗
, Q(i) = argmin
n + λQ,nJ (Q)
+ λh,nJ (h)
h∈FM
Q∈FM
n
It is important to note that unlike the non-regularized case described in Sections 3.1 and 3.2, REG-
BRM and REG-LSTD do not have the same solution. This is because, although the ﬁrst equations
in (7) and (8) are the same, the projected vector h∗ (·; Q) − ˆT πi Q is not necessarily perpendicular to
the admissible function space F M . This is due to the regularization term λh,nJ (h). As a result, the

h

h

∗

. (8)

i

,

FMQTπQΠTπQminimizedbyBRMminimizedbyLSTD‚‚‚2 − ‚‚‚h∗ (·; Q) − ˆT πi Q
‚‚‚2
‚‚‚Q − ˆT πi Q
Pythagorean theorem does not hold: kQ − h∗ (·; Q)k2 6=
, and
therefore the objective functions of the second equations in (7) and (8) are not equal and they do not
have the same solution.
We now present algorithmic solutions for REG-BRM and REG-LSTD problems described above.
We can obtain Q(i) by solving the regularization problems of Eqs. (7) and (8) in a reproducing
kernel Hilbert space (RKHS) deﬁned by a Mercer kernel K . In this case, we let the regularization
terms J (h) and J (Q) be the RKHS norms of h and Q, khk2H and kQk2H , respectively. Using
the Representer theorem, we can then obtain the following closed-form solutions for REG-BRM
Theorem 1. The optimizer Q ∈ H of Eqs. (7) and (8) can be written as Q(·) = P2n
and REG-LSTD. This is not immediate, because the solutions of these procedures are deﬁned with
nested optimization problems.
i=1 ˜αik( ˜Zi , ·),
where ˜Zi = Zi if i ≤ n and ˜Zi = Z 0
i−n , otherwise. The coefﬁcient vector ˜α = ( ˜α1 , . . . , ˜α2n )> can
be obtained by
˜α = (CK Q + λQ,nI )−1 (D> + γC >
2 B>B )r ,
REG-BRM:
˜α = (F >F K Q + λQ,nI )−1F >E r ,
REG-LSTD:
where r = (R1 , . . . , Rn )> , C = D>D − γ 2 (BC 2 )> (BC 2 ), B = K h (K h + λh,nI )−1 − I , D =
C 1 − γC 2 , F = C 1 − γEC 2 , E = K h (K h + λh,nI )−1 , and K h ∈ Rn×n , C 1 , C 2 ∈ Rn×2n ,
and K Q ∈ R2n×2n are deﬁned by [K h ]ij = k(Zi , Zj ), [C 1K Q ]ij = k(Zi , ˜Zj ), [C 2K Q ]ij =
k(Z 0
i , ˜Zj ), and [K Q ]ij = k( ˜Zi , ˜Zj ).

5 Theoretical Analysis of the Algorithms

In this section, we analyze the statistical properties of the policy iteration algorithms based on REG-
BRM and REG-LSTD. We provide ﬁnite-sample convergence results for the error between QπN , the
action-value function of policy πN , the policy resulted after N iterations of the algorithms, and Q∗ ,
the optimal action-value function. Due to space limitations, we only report assumptions and main
results here (Refer to [6] for more details). We make the following assumptions in our analysis,
some of which are only technical:
t )(cid:1), Xt ∼ ν ∈ M(X ), At ∼ πb (·|Xt ), X 0
t = (cid:0)X 0
Assumption A2 (1) At every iteration, samples are generated i.i.d. using a ﬁxed distribution over
states ν and a ﬁxed stochastic policy πb , i.e., {(Zt , Rt , Z 0
t )}n
t=1 are i.i.d. samples, where Zt =
t ∼ P (·|Xt , At ), and π is the
t , π(X 0
(Xt , At ), Z 0
policy being evaluated. We further assume that πb selects all actions with non-zero probability.
(2) The function space F used in the optimization problems (7) and (8) is a Sobolev space Wk (Rd )
with 2k > d. We denote by Jk (Q) the norm of Q in this Sobolev space.
(3) The selected function space F M contains the true action-value function, i.e., Qπ ∈ F M .
(4) For every function Q ∈ F M with bounded norm J (Q), its image under the Bellman operator,
T πQ, is in the same space, and we have J (T πQ) ≤ BJ (Q), for some positive and ﬁnite B , which
is independent of Q.
(5) We assume F M ⊂ B (X × A; Qmax ), for Qmax > 0.
(1) indicates that the training sample should be generated by an i.i.d. process. This assumption is
used mainly for simplifying the proofs and can be extended to the case where the training sample
is a single trajectory generated by a ﬁxed policy with appropriate mixing conditions as was done
in [2].
(2) Using Sobolev space allows us to explicitly show the effect of smoothness k on the
convergence rate of our algorithms and to make comparison with the regression learning settings.
Note that Sobolev spaces are large: In fact, Sobolev spaces are more ﬂexible than H ¨older spaces (a
generalization of Lipschitz spaces to higher order smoothness) in that in these spaces the norm mea-
sures the average smoothness of the functions as opposed to measuring their worst-case smoothness.
Thus, functions that are smooth most over the place except for some parts that have a small measure
will have small Sobolev-space norms, i.e., they will be looked as “simple”, while they would be
viewed as “complex” functions in H ¨older spaces. Actually, our results extend to other RKHS spaces

≤ c2

+

.

c3 log(n) + c4 log( 1
δ )
n

that have well-behaved metric entropy capacity, i.e., log N (ε, F ) ≤ Aε−α for some 0 < α < 2
and some ﬁnite positive A. In (3), we assume that the considered function space is large enough
to include the true action-value function. This is a standard assumption when studying the rate of
convergence in supervised learning [7]. (4) constrains the growth rate of the complexity of the norm
of Q under Bellman updates. We believe that this is a reasonable assumption that will hold in most
practical situations. Finally, (5) is about the uniform boundedness of the functions in the selected
function space. If the solutions of our optimization problems are not bounded, they must be trun-
cated, and thus, truncation arguments must be used in the analysis. Truncation does not change the
ﬁnal result, so we do not address it to avoid unnecessary clutter.
We now ﬁrst derive an upper bound on the policy evaluation error in Theorem 2. We then show how
the policy evaluation error propagates through the iterations of policy iteration in Lemma 3. Finally,
we state our main result in Theorem 4, which follows directly from the ﬁrst two results.
` log(n)
´ 2k
Theorem 2 (Policy Evaluation Error). Let Assumptions A1 and A2 hold. Choosing λQ,n =
2k+d and λh,n = Θ(λQ,n ), for any policy π , the following holds with probability at
c1
nJ 2
k (Qπ )
« 2k
„ log(n)
least 1 − δ, for c1 , c2 , c3 , c4 > 0.
‚‚‚2
‚‚‚ ˆQ − T π ˆQ
k (Qπ )´ d
`J 2
2k+d
2k+d
n
ν
Theorem 2 shows how the number of samples and the difﬁculty of the problem as characterized
k (Qπ ) inﬂuence the policy evaluation error. With a large number of samples, we expect || ˆQ −
by J 2
T π ˆQ||2
ν to be small with high probability, where π is the policy being evaluated and ˆQ is its estimated
action-value function using REG-BRM or REG-LSTD.
Let ˆQ(i) and εi = ˆQ(i) − T πi ˆQ(i) , i = 0, . . . , N − 1 denote the estimated action-value function and
the Bellman residual at the ith iteration of our algorithms. Theorem 2 indicates that at each iteration
i, the optimization procedure ﬁnds a function ˆQ(i) such that kεi k2
ν is small with high probability.
Lemma 3, which was stated as Lemma 12 in [2], bounds the ﬁnal error after N iterations as a
function of the intermediate errors. Note that no assumption is made on how the sequence ˆQ(i) is
generated in this lemma. In Lemma 3 and Theorem 4, ρ ∈ M(X ) is a measure used to evaluate the
performance of the algorithms, and Cρ,ν and Cν are the concentrability coefﬁcients deﬁned in [2].
Lemma 3 (Error Propagation). Let p ≥ 1 be a real and N be a positive integer. Then, for any
sequence of functions {Q(i) } ⊂ B (X × A; Qmax ), 0 ≤ i < N , and εi as deﬁned above, the
”
“
following inequalities hold:
”
“
∗ − QπN kp,ρ ≤
kQ
kεi kp,ν + γN/p Rmax
2γ
C 1/p
(1 − γ )2
ρ,ν max
,
0≤i<N
kQ
∗ − QπN k∞ ≤
kεi kp,ν + γN/p Rmax
2γ
C 1/p
(1 − γ )2
ν max
.
0≤i<N
Theorem 4 (Convergence Result). Let Assumptions A1 and A2 hold, λh,n and λQ,n use the same
schedules as in Theorem 2, and the number of samples n be large enough. The error between
the optimal action-value function, Q∗ , and the action-value function of the policy resulted after N
35 ,
0@„ log(n)
24c × C 1/2
2 1A + γN/2Rmax
! 1
 
iterations of the policy iteration algorithm based on REG-BRM or REG-LSTD, ˆQπN , is
« k
2k+d
2 1A + γN/2Rmax
24c × C 1/2
35 ,
0@„ log(n)
 
! 1
« k
ρ,ν
n
2k+d
2γ
(1 − γ )2
ν
n
with probability at least 1 − δ for some c > 0.

kQ
∗ − QπN k∞ ≤

log( N
δ )
n

kQ
∗ − QπN kρ ≤

2γ
(1 − γ )2

log( N
δ )
n

+

+

Theorem 4 shows the effect of number of samples n, degree of smoothness k , number of iterations
N , and concentrability coefﬁcients on the quality of the policy induced by the estimated action-
value function. Three important observations are: 1) the main term in the rate of convergence
− k
2k+d ), which is an optimal rate for regression up to a logarithmic factor and hence
is O(log(n)n

it is an optimal rate value-function estimation, 2) the effect of smoothness k is evident: for two
problems with different degrees of smoothness, learning the smoother one is easier – an intuitive,
but previously not rigorously proven result in the RL literature, and 3) increasing the number of
iterations N increases the error of the second term, but its effect is only logarithmic.
6 Conclusions and Future Work
In this paper we showed how L2 -regularization can be added to two widely-used policy evalua-
tion methods in RL: Bellman residual minimization (BRM) and least-squares temporal difference
learning (LSTD), and developed two regularized policy evaluation algorithms REG-BRM and REG-
LSTD. We then showed how these algorithms can be implemented efﬁciently when the value-
function approximation belongs to a reproducing kernel Hilbert space (RKHS). We also proved
ﬁnite-sample performance bounds for REG-BRM and REG-LSTD, and the regularized policy iter-
ation algorithms built on top of them. Our theoretical results indicate that our methods are able to
achieve the optimal rate of convergence under the studied conditions.
One of the remaining problems is how to ﬁnd the regularization parameters: λh,n and λQ,n . Us-
ing cross-validation may lead to a completely self-tuning process. Another issue is the type of
regularization. Here we used L2 -regularization, however, the idea can be extended naturally to L1 -
regularization in the style of Lasso, opening up the possibility of procedures that can handle a high
number of irrelevant features. Although the i.i.d. sampling assumption is technical, extending our
analysis to the case when samples are correlated requires generalizing quite a few results in super-
vised learning. However, we believe that this can be done without problem following the work of
[2]. Extending the results to continuous-action MDPs is another major challenge. Here the interest-
ing question is if it is possible to achieve better rates than the one currently available in the literature,
which scales quite unfavorably with the dimension of the action space [1].

References
[1] A. Antos, R. Munos, and Cs. Szepesv ´ari. Fitted Q-iteration in continuous action-space MDPs. In Ad-
vances in Neural Information Processing Systems 20 (NIPS-2007), pages 9–16, 2008.
[2] A. Antos, Cs. Szepesv ´ari, and R. Munos. Learning near-optimal policies with Bellman-residual mini-
mization based ﬁtted policy iteration and a single sample path. Machine Learning, 71:89–129, 2008.
[3] L.C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proceedings
of the Twelfth International Conference on Machine Learning, pages 30–37, 1995.
[4] S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine
Learning, 22:33–57, 1996.
[5] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. JMLR, 6:503–556,
2005.
[6] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv ´ari, and S. Mannor. L2-regularized policy iteration.
2009. (under preparation).
[7] L. Gy ¨orﬁ, M. Kohler, A. Krzy ˙zak, and H. Walk. A distribution-free theory of nonparametric regression.
Springer-Verlag, New York, 2002.
[8] R.A. Howard. Dynamic Programming and Markov Processes. The MIT Press, Cambridge, MA, 1960.
[9] T. Jung and D. Polani. Least squares SVM for least squares TD learning. In ECAI, pages 499–503, 2006.
[10] M. Lagoudakis and R. Parr. Least-squares policy iteration. JMLR, 4:1107–1149, 2003.
[11] M. Loth, M. Davy, and P. Preux. Sparse temporal difference learning using LASSO. In IEEE International
Symposium on Approximate Dynamic Programming and Reinforcement Learning, 2007.
[12] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning, 49:161–178, 2002.
[13] M. Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement
learning method. In 16th European Conference on Machine Learning, pages 317–328, 2005.
[14] P. J. Schweitzer and A. Seidmann. Generalized polynomial approximations in Markovian decision pro-
cesses. Journal of Mathematical Analysis and Applications, 110:568–582, 1985.
[15] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. Bradford Book. MIT Press, 1998.
[16] R. J. Williams and L.C. Baird. Tight performance bounds on greedy policies based on imperfect value
functions. In Proceedings of the Tenth Yale Workshop on Adaptive and Learning Systems, 1994.
[17] X. Xu, D. Hu, and X. Lu. Kernel-based least squares policy iteration for reinforcement learning. IEEE
Trans. on Neural Networks, 18:973–992, 2007.


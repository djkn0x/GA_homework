Counting Solution Clusters in Graph Coloring
Problems Using Belief Propagation

Lukas Kroc

Ashish Sabharwal
Department of Computer Science
Cornell University, Ithaca NY 14853-7501, U.S.A.
{kroc,sabhar,selman}@cs.cornell.edu ∗

Bart Selman

Abstract

We show that an important and computationally challenging solution space feature
of the graph coloring problem (COL), namely the number of clusters of solutions,
can be accurately estimated by a technique very similar to one for counting the
number of solutions. This cluster counting approach can be naturally written in
terms of a new factor graph derived from the factor graph representing the COL
instance. Using a variant of the Belief Propagation inference framework, we can
efﬁciently approximate cluster counts in random COL problems over a large range
of graph densities. We illustrate the algorithm on instances with up to 100, 000
vertices. Moreover, we supply a methodology for computing the number of clus-
ters exactly using advanced techniques from the knowledge compilation literature.
This methodology scales up to several hundred variables.

1

Introduction

Message passing algorithms, in particular Belief Propagation (BP), have been very successful in
efﬁciently computing interesting properties of succinctly represented large spaces, such as joint
probability distributions. Recently, these techniques have also been applied to compute properties
of discrete spaces, in particular, properties of the space of solutions of combinatorial problems. For
example, for propositional satisﬁability (SAT) and graph coloring (COL) problems, marginal prob-
ability information about the uniform distribution over solutions (or similar combinatorial objects)
has been the key ingredient in the success of BP-like algorithms. Most notably, the survey propa-
gation (SP) algorithm utilizes this information to solve very large hard random instances of these
problems [3, 11].

Earlier work on random ensembles of Constraint Satisfaction Problems (CSPs) has shown that the
computationally hardest instances occur near phase boundaries, where instances go from having
many globally satisfying solutions to having no solution at all (a “solution-focused picture”). In
recent years, this picture has been reﬁned and it was found that a key factor in determining the hard-
ness of instances in terms of search algorithm (or sampling algorithm) is the question: how are the
solutions spatially distributed within the search space? This has made the structure of the solution
space in terms of its clustering properties a key factor in determining the performance of combina-
torial search methods (a “cluster-focused picture”). Can BP-like algorithms be used to provide such
cluster-focused information? For example, how many clusters are there in a solution space? How
big are the clusters? How are they organized? Answers to such questions will shed further light into
our understanding of these hard combinatorial problems and lead to better algorithmic approaches
for reasoning about them, be it for ﬁnding one solution or answering queries of probabilistic infer-
ence about the set of solutions. The study of the solution space geometry has indeed been the focus

∗This work was supported by IISI, Cornell University (AFOSR grant FA9550-04-1-0151), DARPA (REAL
grant FA8750-04-2-0216), and NSF (grant 0514429).

of a number of recent papers [e.g. 1, 2, 3, 7, 9, 11], especially by the statistical physics community,
which has developed extensive theoretical tools to analyze such spaces under certain structural as-
sumptions and large size limits. We provide a purely combinatorial method for counting the number
of clusters, which is applicable even to small size problems and can be approximated very well by
message passing techniques.

Solutions can be thought of as ‘neighbors’ if they differ in the value of one variable, and the transitive
closure of the neighbor relation deﬁnes clusters in a natural manner. Counting the number of clusters
is a challenging problem. To begin with, it is not even clear what is the best succinct way to represent
clusters. One relatively crude but useful way is to represent a cluster by the set of ‘backbone’
variables in that cluster, i.e., variables that take a ﬁxed value in all solutions within the cluster.
Interestingly, while it is easy (polynomial time) to verify whether a variable assignment is indeed a
solution of CSP, the same check is much harder for a candidate cluster represented by the set of its
backbone variables.

We propose one of the ﬁrst scalable methods for estimating the number of clusters of solutions of
graph coloring problems using a belief propagation like algorithm. While the na¨ıve method, based
on enumeration of solutions and pairwise distances, scales to graph coloring problems with 50 or so
nodes and a recently proposed local search based method provides estimates up to a few hundred
node graphs [7], our approach—being based on BP—easily provides fast estimates for graphs with
100, 000 nodes. We validate the accuracy of our approach by also providing a fairly non-trivial
exact counting method for clusters, utilizing advanced knowledge compilation techniques. Our
approach works with the factor graph representation of the graph coloring problem. Yedidia et al.
[12] showed that if one can write the so-called “partition function”, Z , for a quantity of interest
in a factor graph with non-negative weights, then there is a fairly mechanical variational method
derivation that yields belief propagation equations for estimating Z . Under certain assumptions,
we derive a partition function style quantity, Z(−1) , to count the number of clusters. We then use
the variational method to obtain BP equations for estimating Z(−1) . Our experiments with random
graph coloring problems show that Z(−1) itself is an extremely accurate estimate of the number of
clusters, and so is its approximation, ZBP(−1) , obtained from our BP equations.

2 Preliminaries

The graph coloring problem can be expressed in the form of a factor graph, a bipartite graph with
two kinds of nodes. The variable nodes, ~x = (x1 , . . . , xn ), represent the variables in the problem (n
vertices to be colored) with their discrete domain Dom = {c1 , . . . , ck } (k colors). The factor nodes,
α, . . ., with associated factor functions fα , . . . , represent the constrains of the problem (no two
adjacent vertices have the same color). Each factor function is a Boolean function with arguments
~xα (a subset of variables from ~x) and range {0, 1}, and evaluates to 1 if and only if (iff) the associated
constraint is satisﬁed. An edge connects a variable xi with factor fα iff the variable appears in the
constraint represented by the factor node, which we denote by i ∈ α. In the graph coloring problem,
each factor function has exactly two variables.

In the factor representation, each variable assignment ~x is thought of as having a weight equal to the
product of the values that all factors evaluate to. We denote this product by F (~x) := Qα fα (~xα ).
In our case, the weight of an assignment ~x is 1 if all of the factors have value of 1, and 0 otherwise.
The assignments with weight 1 correspond precisely to legal colorings, or solutions to the problem.
The number of solutions can thus be expressed as the weighted sum across all possible assignments.
We denote this quantity by Z , the so-called partition function:
Z := X
F (~x) = X
Y
α
~x∈Domn
~x∈Domn

fα (~xα )

(1)

We deﬁne the solution space of a graph coloring problem to be the set of all its legal colorings. Two
legal colorings (or solutions) are called neighbors if they differ in the color of one vertex.
Deﬁnition 1 (Solution Cluster). A set of solutions C ⊆ S of a solution space S is a cluster if it is
a maximal subset such that any two solutions in C can be connected by a sequence from C where
consecutive solutions are neighbors.
In other words, clusters are connected components of the “solution graph” which has solutions as
nodes and an edge between two solutions if they differ in the value of exactly one variable.

3 A Partition Function Style Expression for Counting Clusters

In this section we consider a method for estimating the number of solution clusters of a graph
coloring problem. We brieﬂy describe the concepts here; a more in-depth treatment, including
formal results, may be found in [8]. First let us extend the deﬁnition of the function F so that it
may be evaluated on an extended domain DomE xt := P ({c1 , . . . , ck }) \ ∅ where c1 , . . . , ck are
the k domain values (colors) of each of the problem variables, and P is the power set operator
(so |DomE xt| = 2k − 1). Each generalized assignment ~y ∈ DomE xtn thus associates a (non-
empty) set of values with each original variable, deﬁning a hypercube in the search space for F .
We generalize F and fα to this extended domain in the natural way, F ′ (~y) := Q~x∈~y F (~x), and
α (~yα ) := Q~xα∈~yα
f ′
fα (~xα ), where the relation ∈ is applied point-wise, as will be the case with any
relational operators used on vectors in this text. This means that F ′ evaluates to 1 on a hypercube
iff F evaluates to 1 on all points within that hypercube.

Let us ﬁrst assume that the solution space we work with decomposes into a set of separated hy-
percubes, so clusters correspond exactly to the hypercubes; by separated hypercubes, we mean
that points in one hypercube differ from points in others in at least two values. E.g., ~y1 =
({c1 } , {c1 } , {c1 }) and ~y2 = ({c2 } , {c3 } , {c1 , c2 }) are separated hypercubes in three dimensions.
This allows us to develop a surprisingly simple expression for counting the number of clusters, and
we will later see that the same expression applies with high precision also to solution spaces of much
more complex instances of graph coloring problems. Consider the indicator function χ(~y) for the
property that ~y ∈ DomE xtn is a maximal solution hypercube contained in the solution space:
(cid:16)1 − F ′ (~y [yi ← yi ∪ {vi}])(cid:17)
· Y
Y
χ(~y) := F ′ (~y)
| {z }
i
vi /∈yi
~y is legal
{z
}
|
no point-wise generalization is legal
Here ~y [yi ← y ′
i ] denotes the substitution of y ′
i into yi in ~y . Note that if the solution clusters are in
fact hypercubes, then variable values that can be “extended” independently can also be extended all
at once, that is, F ′ (~y [yi ← yi ∪ {vi}]) = 1 and F ′ (~y [yj ← yj ∪ {vj }]) = 1 implies F (~y [yi ←
yi ∪ {vi} , yj ← yj ∪ {vj }]) = 1. Moreover, any F ′ (~y [yi ← yi ∪ {vi}]) implies F (~y). Using these
observations, χ(~y) can be reformulated by factoring out the product as follows. Here #o (~y) denotes
the number of odd-size elements of ~y , and #e (~y) the number of even-size ones.
χ(~y) = F ′ (~y)(cid:16) X
Y
(−1)#o (~y ′ ) Y
F ′ (~y [yi ← yi ∪ {vi}])
vi∈y ′
i
~y ′∈(P (Dom))n \~y
i
|
{z
=F ′ (~y∪~y ′ ) by hypercube assumption
(−1)#o (~z\~y)F ′ (~z ) = (−1)#e (~y) X
(−1)#e (~z)F ′ (~z )
~z⊇~y

~z :=~y∪~y ′
= X
~z⊇~y

(cid:17)

}

(−1)δe (yi )

(−1)#e (~y)(cid:17)

Finally, to count the number of maximal hypercubes ﬁtting into the set of solutions, we sum the
indicator function χ(~y) across all vectors ~y ∈ DomE xtn :
(−1)#e (~z)F ′ (~z )(cid:16) X
X
χ(~y) = X
(−1)#e (~y) X
(−1)#e (~z)F ′ (~z ) = X
~z⊇~y
~y
~y
~z
∅ /∈~y⊆~z
(cid:17) = X
(−1)#e (~z)F ′ (~z )(cid:16) Y
= X
X
i
~z
~z
∅ /∈~yi⊆~zi
{z
}
|
=1
The expression above is important for our study, and we denote it by Z(−1) :
Z(−1) := X
(−1)#e (~z)F ′ (~z ) = X
(−1)#e (~y) Y
α
~z∈DomExtn
~y∈DomExtn
The notation Z(−1) is chosen to emphasize its relatedness to the partition function (1) denoted by
Z , and indeed the two expressions differ only in the (−1) term. It is easily seen that if the solution
space consists of a set of separated hypercubes, then Z(−1) exactly captures the number of clusters
(each separated hypercube is a cluster). Surprisingly, this number is remarkably accurate even for
random coloring problems as we will see in Section 6, Figure 1.

(−1)#e (~z)F ′ (~z )

f ′
α (~yα )

(2)

4 Exact Computation of the Number of Clusters and Z(−1)

Obtaining the exact number of clusters for reasonable size problems is crucial for evaluating our
proposed approach based on Z(−1) and the corresponding BP equations to follow in Section 5. A
na¨ıve way is to explicitly enumerate all solutions, compute their pairwise Hamming distances, and
infer the cluster structure. Not surprisingly, this method does not scale well because the number of
solutions typically grows exponentially as the number of variables of the graph coloring problems
increases. We discuss here a much more scalable approach that uses two advanced techniques to
this effect: disjunctive negation normal form (DNNF) and binary decision diagrams (BDDs). Our
method scales to graph coloring problems with a few hundred variables (see experimental results)
for computing both the exact number of clusters and the exact value of Z(−1) .

Both DNNF [6] and BDD [4] are graph based data structures that have proven to be very effective
in “knowledge compilation”, i.e., in converting a 0-1 function F into a (potentially exponentially
long, but often reasonably sized) standard form from which various interesting properties of F can
be inferred easily, often in linear time in the size of the DNNF formula or BDD. For our purposes,
we use DNNF to succinctly represent all solutions of F and a set of BDDs to represent solution
clusters that we create as we traverse the DNNF representation. The only relevant details for us of
these two representations are the following: (1) DNNF is represented as an acyclic directed graph
with variables and their negations at the leaves and two kinds of internal nodes, “or” and “and”; “or”
nodes split the set of solutions such that they differ in the value of the variable labeling the node but
otherwise have identical variables; “and” nodes partition the space into disjoint sets of variables; (2)
BDDs represent arbitrary sets of solutions and support efﬁcient intersection and projection (onto a
subset of variables) operations on these sets.

We use the compiler c2d [5] to obtain the DNNF form for F . Since c2d works on Boolean formulas
and our F often has non-Boolean domains, we ﬁrst convert F to a Boolean function F ′ using a
unary encoding, i.e., by replacing each variable xi of F with domain size t with t Boolean variables
i,j , 1 ≤ j ≤ t, respecting the semantics: xi = j iff xi,j = 1. In order to ensure that F and F ′ have
x′
similar cluster structure of solutions, we relax the usual condition that only one of xi,1 , . . . , xi,t
may be 1, thus effectively allowing the original xi to take multiple values simultaneously. This
yields a generalized function: the domains of the variables of F ′ correspond to the power sets of the
domains of the respective variables of F . This generalization has the following useful property: if
two solutions ~x(1) and ~x(2) are neighbors in the solution space of F , then the corresponding solutions
~x′(1) and ~x′(2) are in the same cluster in the solution space of F ′ .

Computing the number of clusters. Given F ′ , we run c2d on it to obtain an implicit representation
of all solutions as a DNNF formula F ′′ . Next, we traverse F ′′ from the leaf nodes up, creating
clusters as we go along. Speciﬁcally, with each node U of F ′′ , we associate a set SU of BDDs,
one for each cluster in the sub-formula contained under U . The set of BDDs for the root node of
F ′′ then corresponds precisely to the set of solution clusters of F ′ , and thus of F . These BDDs are
computed as follows. If U is a leaf node of F ′′ , it represents a Boolean variable or its negation and
SU consists of the single one-node BDD corresponding to this Boolean literal. If U is an internal
node of F ′′ labeled with the variable xU and with children L and R, the set of BDDs SU is computed
as follows. If U is an “or” node, then we consider the union SL ∪ SR of the two sets of BDDs and
merge any two of these BDDs if they are adjacent, i.e., have two solutions that are neighbors in the
solution space (since the DNNF form guarantees that the BDDs in SL and SR already must differ
in the value of the variable xU labeling U , the adjacency check is equivalent to testing whether the
two BDDs, with xU projected out, have a solution in common; this is a straightforward projection
and intersection operation for BDDs); in the worst case, this leads to |SL | + |SR | cluster BDDs
in SU . Similarly, if U is an “and” node, then SU is constructed by considering the cross product
{bL and bR | bL ∈ SL , bR ∈ SR } of the two sets of BDDs and merging adjacent resulting BDDs as
before; in the worst case, this leads to |SL | · |SR | cluster BDDs in SU .
Evaluating Z(−1) . The exact value of Z(−1) on F ′ can also be evaluated easily once we have the
DNNF representation F ′′ . In fact, as is reﬂected in our experimental results, evaluation of Z(−1)
is a much more scalable process than counting clusters because it requires a simple traversal of F ′′
without the need for maintaining BDDs. With each node U of F ′′ , we associate a value VU which
equals precisely the difference between the number of solutions below U with an even number
of positive literals and those with an odd number of positive literals; Z(−1) then equals (−1)N

times the value thus associated with the root node of F ′′ . These values are computed bottom-
If U is a leaf node labeled with a positive (or negative) literal, then VU = −1
up as follows.
(or 1, resp.).
If U is an “or” node with children L and R, then VU = VL + VR . This works
because L and R have identical variables. Finally, if U is an “and” node with children L and R,
then VU = VLVR . This last computation works because L and R are on disjoint sets of variables
and because of the following observation. Suppose L has V e
L solutions with an even number of
positive literals and V o
L solutions with an odd number of positive literals; similarly for R. Then
VU = (V e
L V e
R + V o
L V o
R ) − (V e
L V o
R + V o
L V e
R ) = (V e
L − V o
L )(V e
R − V o
R ) = VLVR .

5 Belief Propagation Inference for Clusters

We present a version of the Belief Propagation algorithm that allows us to deal with the alternating
signs of Z(−1) . The derivation follows closely the one given by Yedidia et al. [12] for standard BP,
i.e., we will write equations for a stationary point of KL divergence of two sequences (not necessarily
probability distributions in our case). Since the Z(−1) expression involves both positive and negative
terms, we must appropriately generalize some of the steps.

Given a function p(~y) (the target function, with real numbers as its range) on DomE xtn that is
known up to a normalization constant but with unknown marginal sums, we seek a function b(~y)
(the trial function) to approximate p(~y), such that b’s marginal sums are known. The target function
(−1)#e (~y) Qα f ′
p(~y) is deﬁned as p(~y) := 1
α (~yα ). We adopt previously used notation [12]:
Z(−1)
~yα are values in ~y of variables that appear in factor (i.e. vertex) f ′
α ; ~y−i are values of all variables
in ~y except yi . The marginal sums can be extended in a similar way to allow for any number of
variables ﬁxed in ~y , speciﬁed by the subscript. When convenient, we treat the symbol α as a set of
indices of variables in f ′
α , to be able to index them. We begin by listing the assumptions used in the
derivation, both the ones that are used in the “standard” BP, and two additional ones needed for the
generalization. An assumption on b(~y) is legitimate if the corresponding condition holds for p(~y).

Assumptions: The standard assumptions, present in the derivation of standard BP [12], are:
b(~y) and bα (~yα ) = P~y−α
• Marginalization: bi (yi ) = P~y−i
b(~y). This condition is legitimate,
but cannot be enforced with a polynomial number of constraints. Moreover, it might happen that
the solution found by BP does not satisfy it, which is a known problem with BP [10].
bi (yi ) = P~yα
• Normalization: Pyi
bα (~yα ) = 1. This is legitimate and explicitly enforced.
• Consistency: ∀α, i ∈ α, yi : bi (yi ) = P~yα\i
bα (~yα ). This is legitimate and explicitly enforced.
• Tree-like decomposition: says that the weights b(~y) of each conﬁguration can be obtained from
the marginal sums as follows (di is the degree of the variable node yi in the factor graph):
|b(~y)| = Qα |bα (~yα )|
Qi |bi (yi )|di−1 . (The standard assumption is without the absolute values.) This assump-
tion is not legitimate, and it is built-in, i.e., it is used in the derivation of the BP equations.

To appropriately handle the signs of b(~y) and p(~y), we have two additional assumptions. These are
necessary for the BP derivation applicable to Z(−1) , but not for the standard BP equations.

• Sign-correspondence: For all conﬁgurations ~y , b(~y) and p(~y) have the same sign (zero, being a
singular case, is treated as having a positive sign). This is a built-in assumption and legitimate.
• Sign-alternation: bi (yi ) is negative iff |yi | is even, and bα (~yα ) is negative iff #e (~yα ) is odd.
This is also a built-in assumption, but not necessarily legitimate; whether or not it is legitimate
depends on the structure of the solution space of a particular problem.

The Sign-alternation assumption can be viewed as an application of the inclusion-exclusion prin-
ciple, and is easy to illustrate on a graph coloring problem with only two colors. In this case, if
F ′ (~y) = 1, then yi = {c1} means that yi can have color 1, yi = {c2 } that yi can have color 2,
and yi = {c1 , c2 } that yi can have both colors. The third event is included in the ﬁrst two, and its
probability must thus appear with a negative sign if the sum of probabilities is to be 1.

Kullback-Leibler divergence: The KL-divergence is traditionally deﬁned for probability distribu-
tions, for sequences of non-negative terms in particular. We need a more general measure, as our
sequences p(~y) and b(~y) have alternating signs. But using the Sign-correspondence assumption, we
observe that the usual deﬁnition of KL-divergence is still applicable, since the term in the logarithm

is non-negative: D(b k p) := P~y∈DomExtn b(~y) log b(~y)
p(~y) = P~y∈DomExtn b(~y) log |b(~y)|
|p(~y)| . More-
over, the following Lemma shows that the two properties of KL-divergence that make it suitable for
distance-minimization are still valid.

Lemma 1. Let b(.) and p(.) be (possibly negative) weight functions on the same domain D , with the
property that they agree on signs for all states (i.e., ∀~y ∈ D : sign(b(~y)) = sign(p(~y))), and that
they sum to the same constant (i.e., P~y b(~y) = P~y p(~y) = c). Then the KL-divergence D(b k p)
satisﬁes D(b k p) ≥ 0 and D(b k p) = 0 ⇔ b ≡ p.

The proof is essentially identical to the equivalent statement made about KL-divergence of proba-
bility distributions. We omit it here for lack of space.

Minimizing D(b k p): We write p(~y) = sign(p(~y)) · |p(~y)|, and analogously for b(~y). This allows
us to isolate the signs, and the minimization follows exactly the steps of standard BP derivation,
namely we write a set of equations characterizing stationary points of D(b k p). At the end, using
the Sign-alternation assumption, we are able to implant the signs back.

BP equations: The resulting modiﬁed BP updates (denoted BP(−1) ) are, for yi ∈ DomE xt:
ni→α (yi ) = Y
β∋i\α

mβ→i (yi )

mα→i (yi ) ∝

X
~yα\i∈DomExt|α|−1

α (~yα ) Y
f ′
j∈α\i

(−1)δ(|yj | is even)nj→α (yj )

(3)

(4)

(Almost equivalent to standard BP, except for the (−1) term.) One would iterate these equations
from a suitable starting point to ﬁnd a ﬁxed point, and then obtain the beliefs bi (yi ) and bα (~yα ) (i.e.,
estimates of marginal sums) using the Sign-alternation assumption and the standard BP relations:
bi (yi ) ∝(−1)δ(|yi | is even) Y
α (~yα ) Y
bα (~yα ) ∝(−1)#e (~yα ) f ′
α∋i
i∈α

mα→i (yi )

ni→α (yi )

(5)

To approximately count the number of clusters in large problems for which exact cluster count or
exact Z(−1) evaluation is infeasible, we employ the generic BP(−1) scheme derived above. We sub-
stitute the extended factors f ′ (~yα ) into Equations (3) and (4), iterate from a random initial starting
point to ﬁnd a ﬁxed point, and then use Equations (5) to compute the beliefs. The actual estimate of
Z(−1) is obtained with the standard BP formula (with signs properly taken care of), where di is the
degree of the variable node yi in the factor graph:
X
log ZBP(−1) := − X
bα (~yα ) log |bα (~yα )| + X
α
i
~yα

(di − 1) X
yi

bi (yi ) log |bi (yi )|

(6)

6 Experimental Evaluation

We empirically evaluate the accuracy of our Z(−1) and ZBP(−1) approximations on an ensemble of
random graph 3-coloring instances. The results are discussed in this section.

Z(−1) vs. the number of clusters. The left panel of Figure 1 compares the number of clusters (on the
x-axis, log-scale) with Z(−1) (on the y-axis, log-scale) for 2, 500 colorable random 3-COL instances
on graphs with 20, 50, and 100 vertices with average vertex degree ranging between 1.0 and 4.7 (the
threshold for 3-colorability). As can be seen, the Z(−1) expression captures the number of clusters
almost exactly. The inaccuracies come mostly from low graph density regions; in all instances we
tried with density > 3.0, the Z(−1) expression was exact. We remark that although uncolorable
instances were not considered in this comparison, Z(−1) = 0 = num-clusters by construction.

It is worth noting that for tree-structured graphs (with more than one vertex), the Z(−1) expression
gives 0 for any k ≥ 3 colors although there is exactly one solution cluster. Moreover, given a
disconnected graph with at least one tree component, Z(−1) also evaluates to 0 as it is the product
of Z(−1) values over different components. We have thus removed all tree components from the
generated graphs prior to computing Z(−1) ; tree components are easily identiﬁed and removing
them does not change the number of clusters. For low graph densities, there are still some instances

|V|= 20
|V|= 50
|V|= 100

0
0
0
5

0
0
0
1

0
0
2

0
5

0
2

5

)
1
−
(
Z

s
l
a
n
i
g
r
a
m
−
)
1
−
(
Z

0
3
.
0

0
2
.
0

0
1
.
0

0
0
.
0

N
/
)
Z
(
g
o
l
 
e
g
a
r
e
v
A

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

ZBP(−1), |V|=100K
ZBP(−1), |V|=100
Z(−1), |V|=100

5

20 50

200

1000

5000

0.00

0.10

0.20

0.30

Number of clusters

Cluster marginals

1

2

3

4

Average vertex degree

Figure 1: Left: Z(−1) vs. number of clusters in random 3-COL
problems with 20, 50 and 100 vertices, and average vertex degree
between 1.0 − 4.7. Right: cluster marginals vs. Z(−1) -marginals
for one instance of random 3-COL problem with 100 vertices.

Figure 2: Average ZBP(−1)
and Z(−1) for 3-COL vs. av-
erage vertex degrees for small
and large random graphs.

for which Z(−1) evaluates to 0; these instances are not visible in Figure 1 due to the log-log scale.
In fact, all our instances with fewer than 5 clusters have Z(−1) = 0. This is because of other
substructures for which Z(−1) evaluates to 0, e.g., cordless cycles of length not divisible by 3 (for
k = 3 coloring) with attached trees. These structures, however, become rare as the density increases.

Z(−1) marginals vs. clusters marginals. For a given problem instance, we can deﬁne the cluster
marginal of a variable xi to be the fraction of solution clusters in which xi only appears with one
particular value (i.e., xi is a backbone of the cluster). Since Z(−1) counts well the number of clusters,
it is natural to ask whether it is also possible to obtain the marginals information from it. Indeed,
Z(−1) does provide an estimate of the cluster marginals, and we call them Z(−1) -marginals. Recall
that the semantics of factors in the extended domain is such that a variable can assume a set of values
only if every value in the set yields a solution to the problem. This extends to the Z(−1) estimate of
the number of clusters, and one can therefore use the principle of inclusion-exclusion to compute the
number of clusters where a variable can only assume one particular value. The deﬁnition of Z(−1)
conveniently provides for correct signs, and the number of clusters where xi is ﬁxed to vi is thus
estimated by Pyi∋vi
Z(−1) (yi ), where Z(−1) (yi ) is the marginal sum of Z(−1) . The Z(−1) -marginal
is obtained by dividing this quantity by Z(−1) .

The right panel of Figure 1 shows the results on one random 3-COL problem with 100 vertices. The
plot shows cluster marginals and Z(−1) -marginals for one color; the points correspond to individ-
ual variables. The Z(−1) -marginals are close to perfect. This is a typical situation, although it is
important to mention that Z(−1) -marginals are not always correct, or even non-negative. They are
merely an estimate of the true cluster marginals, and how well they work depends on the solution
space structure at hand. They are exact if the solution space decomposes into separated hypercubes
and, as the ﬁgure shows, remarkably accurate also for random coloring instances.

The number of clusters vs. ZBP(−1) . Figure 3 depicts a comparison between ZBP(−1) and Z(−1)
for the 3-COL problem on colorable random graphs of various sizes and graph densities. It com-
pares Z(−1) (on the x-axis, log-scale) with ZBP(−1) (y-axis, log-scale) for 1, 300 colorable 3-COL
instances on random graphs with 50, 100, and 200 vertices, with average vertex degree ranging from
1.0 to 4.7. The plots shows that BP is quite accurate in estimating Z(−1) for individual instances,
which in turn captures the number of clusters. Instances which are not 3-colorable are not shown,
and BP in general incorrectly estimates a non-zero number of clusters for them.

Estimates on very large graphs and for various graph densities. Figure 2 shows similar data
from a different perspective: what is shown is a rescaled average estimate of the number of clusters
(y-axis) for average vertex degrees 1.0 to 4.7 (x-axis). The average is taken across different colorable
instances of a given size, and the rescaling assumes that the number of clusters = exp(|V | · Σ) where
Σ is a constant independent of the number of vertices [3]. The three curves show, respectively, BP’s
estimate for graphs with 100, 000 vertices, BP’s estimate for graphs with 100 vertices, and Z(−1) for
the same graphs of size 100. The averages are computed across 3, 000 instances of the small graphs,
and only 10 instances of the large ones where the instance-to-instance variability is practically non-
existent. The fact that the curves nicely overlay shows that BP(−1) computes Z(−1) very accurately

|V|= 50

)
1
−
(
P
B
Z

9
0
+
e
1

6
0
+
e
1

3
0
+
e
1

0
0
+
e
1

|V|= 100

)
1
−
(
P
B
Z

9
0
+
e
1

6
0
+
e
1

3
0
+
e
1

0
0
+
e
1

|V|= 200

)
1
−
(
P
B
Z

9
0
+
e
1

6
0
+
e
1

3
0
+
e
1

0
0
+
e
1

1e+00

1e+03

1e+06
Z(−1)

1e+09

1e+00

1e+03

1e+06
Z(−1)

1e+09

1e+00

1e+03

1e+06
Z(−1)

1e+09

Figure 3: ZBP(−1) compared to Z(−1) for 3-COL problem on random graphs with 50, 100 and 200
vertices and average vertex degree in the range 1.0 − 4.7.

on average for colorable instances (where we can compare it with exact values), and that the esti-
mate remains accurate for large problems. Note that the Survey Propagation algorithm developed
by Braunstein et al. [3] also aims at computing the number of certain clusters in the solution space.
However, SP counts only the number of clusters with a “typical size”, and would show non-zero
values in Figure 2 only for average vertex degrees between 4.42 and 4.7. Our algorithm counts
clusters of all sizes, and is very accurate in the entire range of graph densities.

7 Conclusion

We discuss a purely combinatorial construction for estimating the number of solution clusters in
graph coloring problems with very high accuracy. The technique uses a hypercube-based inclusion-
exclusion argument coupled with solution counting, and lends itself to an application of a modiﬁed
belief propagation algorithm. This way, the number of clusters in huge random graph coloring
instances can be accurately and efﬁciently estimated. Our preliminary investigation has revealed
that it is possible to use combinatorial arguments to formally prove that the cluster counts estimated
by Z(−1) are exact on certain kinds of solution spaces (not necessarily only for graph coloring). We
hope that such insights and the cluster-focused picture will lead to new techniques for solving hard
combinatorial problems and for bounding solvability transitions in random problem ensembles.

References

[1] D. Achlioptas and F. Ricci-Tersenghi. On the solution-space geometry of random constraint satisfaction
problems. In 38th STOC, pages 130–139, Seattle, WA, May 2006.
[2] J. Ardelius, E. Aurell, and S. Krishnamurthy. Clustering of solutions in hard satisﬁability problems. J.
Statistical Mechanics, P10012, 2007.
[3] A. Braunstein, R. Mulet, A. Pagnani, M. Weigt, and R. Zecchina. Polynomial iterative algorithms for
coloring and analyzing random graphs. Physical Review E, 68:036702, 2003.
[4] R. E. Bryant. Graph-based algorithms for Boolean function manipulation. IEEE Transactions on Com-
puters, 35(8):677–691, 1986.
[5] A. Darwiche. New advances in compiling CNF into decomposable negation normal form. In 16th Euro-
pean Conf. on AI, pages 328–332, Valencia, Spain, Aug. 2004.
[6] A. Darwiche. Decomposable negation normal form. J. ACM, 48(4):608–647, 2001.
[7] A. Hartmann, A. Mann, and W. Radenback. Clusters and solution landscapes for vertex-cover and SAT
problems. In Workshop on Physics of Distributed Systems, Stockholm, Sweden, May 2008.
[8] L. Kroc, A. Sabharwal, and B. Selman. Counting solution clusters of combinatorial problems using belief
propagation, 2008. (in preparation).
[9] F. Krzakala, A. Montanari, F. Ricci-Tersenghi, G. Semerjian, and L. Zdeborova. Gibbs states and the set
of solutions of random constraint satisfaction problems. PNAS, 104(25):10318–10323, June 2007.
[10] D. Mackay, J. Yedidia, W. Freeman, and Y. Weiss. A conversation about the Bethe free energy and
sum-product, 2001. URL citeseer.ist.psu.edu/mackay01conversation.html.
[11] M. M ´ezard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisﬁability prob-
lems. Science, 297(5582):812–815, 2002.
[12] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized
belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):2282–2312, 2005.


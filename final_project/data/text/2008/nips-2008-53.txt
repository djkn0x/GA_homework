ICA based on a Smooth Estimation of the Differential
Entropy

Lev Faivishevsky
School of Engineering, Bar-Ilan University
levtemp@gmail.com

Jacob Goldberger
School of Engineering, Bar-Ilan University
goldbej@eng.biu.ac.il

Abstract

In this paper we introduce the MeanNN approach for estimation of main infor-
mation theoretic measures such as differential entropy, mutual information and
divergence. As opposed to other nonparametric approaches the MeanNN results
in smooth differentiable functions of the data samples with clear geometrical inter-
pretation. Then we apply the proposed estimators to the ICA problem and obtain
a smooth expression for the mutual information that can be analytically optimized
by gradient descent methods. The improved performance of the proposed ICA
algorithm is demonstrated on several test examples in comparison with state-of-
the-art techniques.

1 Introduction

Independent component analysis (ICA) is the problem of recovering latent random vector from
observations of unknown linear functions of that vector. Assume a data S ∈ Rd is generated via d
independent sources. We observe X = AS where A is an unknown square matrix called the mixing
matrix. We are given repeated observation dataset {x1 , ..., xn} and our goal is to recover the linear
transformation A and the sources s1 , ..., sn that generated our data xi = Asi .
Given the minimal statement of the problem, it has been shown [6] that one can recover the origi-
nal sources up to a scaling and a permutation provided that at most one of the underlying sources is
Gaussian and the rest are non-Gaussian. Upon pre-whitening the observed data, the problem reduces
to a search over rotation matrices in order to recover the source and mixing matrix in the sense de-
scribed above [10]. We will assume henceforth that such pre-processing has been done. Specifying
distributions for the components of X , one obtains a parametric model that can be estimated via
maximum likelihood [3, 4]. Working with W = A−1 as the parametrization, one readily obtains
ˆW and provides estimates of the latent
a gradient or ﬁxed-point algorithm that yields an estimate
components via ˆS = ˆW X [10].
In practical applications the distributions of the d components of X are unknown. Therefore it is
preferable to consider the ICA model as a semiparametric model in which the distributions of the
components of X are left unspeciﬁed. The problem is then, obviously, to ﬁnd a suitable
contrast
function, i.e. a target function to be minimized in order to estimate the ICA model. The earliest
ICA algorithms were based on contrast functions deﬁned in terms of expectations of a single ﬁxed
nonlinear function, chosen in ad-hoc manner [5]. More sophisticated algorithms have been obtained
by careful choice of a single ﬁxed nonlinear function, such that the expectations of this function
yield a robust approximation to the mutual information [9].

Maximizing the likelihood in the semiparametric ICA model is essentially equivalent to minimizing
the mutual information between the components of the estimate ˆS = ˆW X [4]. The usage of the
mutual information as a contrast function to be minimized in estimating the ICA model is well
motivated, quite apart from the link to maximum likelihood [6].

1

Estimating MI from a given ﬁnite sample set is difﬁcult. Several modern approaches rely on
k-
nearest neighbor estimates of entropy and mutual information [12, 16]. Recently the Vasicek esti-
mator [17] for the differential entropy of 1D random variables, based on k-nearest neighbors statis-
tics, was applied to ICA [8, 13]. In addition ICA was studied by another recently introduced MI
estimator [16]. However, the derivative of the estimators that are based on order statistics can hardly
be computed and therefore the optimization of such numerical criteria can not be based on gradient
techniques. Also the result numerical criteria tend to have a non-smooth dependency on sample
values. The optimization therefore should involve computation of contrast function on a whole grid
of searched parameters.

In addition, such estimators do not utilize optimally the whole amount of data included in the sam-
ples of random vectors. Therefore they require signiﬁcant artiﬁcial enlargement of data sets by a
technique called data augmentation [13] that replaces each data point in sample with R-tuple (R is
usually 30) of points given by an statistical procedure with ad-hoc parameters. An alternative is the
Fourier ﬁltering of the estimated values of the evaluated MI estimators [16].

In the present paper we propose new smooth estimators for the differential entropy, the mutual in-
formation and the divergence. The estimators are obtained by a novel approach averaging k-nearest
neighbor statistics for the all possible values of order statistics k . The estimators are smooth, their
derivatives may be easily analytically calculated thus enabling fast gradient optimization techniques.
They fully utilize the amount of data comprised into a random variable sample. The estimators pro-
vide a novel geometrical interpretation for the entropy. When applied to ICA problem, the proposed
estimator leads to the most precise results for many distributions known at present.

The rest of the paper is organized as follows: Section 2 reviews the kNN approach for the entropy
and divergence estimation, Section 3 introduces the mean estimator for the differential entropy,
the mutual information and the divergence. Section 4 describes the application of the proposed
estimators to the ICA problem and Section 5 describes conducted numerical experiments.

2 kNN Estimators for the Differential Entropy
(cid:90)
We review the nearest neighbor technique for the Shannon entropy estimation. The differential
entropy of X is deﬁned as:

H (X ) = −

f (x) log f (x)dx

(1)

We describe the derivation of the Shannon differential entropy estimate of [11, 18]. Our aim is
to estimate H (X ) from a random sample (x1 , ..., xn ) of n random realizations of a d-dimensional
random variable X with unknown density function f (x). The entropy is the average of − log f (x).
If one had unbiased estimators for log f (xi ), one would arrive to an unbiased estimator for the
entropy. We will estimate log f (xi ) by considering the probability density function Pik () for the
distance between xi and its k-th nearest neighbor (the probability is computed over the positions
of all other n − 1 points, with xi kept ﬁxed). The probability Pik ()d is equal to the chance that
(cid:82)
there is one point within distance r ∈ [,  + d] from xi , that there are k − 1 other points at smaller
distances, and that the remaining n− k − 1 points have larger distances from xi . Denote the mass of
(cid:107)x−xi (cid:107)< f (x)dx. Applying the trinomial formula
the -ball centered at xi by pi (), i.e. pi () =
we obtain:
(n− 1)!
(cid:82)
dpi ()
(1 − pi )n−k−1
pk−1
1!(k − 1)!(n− k − 1)!
i
d
Pik ()d = 1. Hence, the expected value of the function
It can be easily veriﬁed that indeed
log pi () according to the distribution Pik () is:
(cid:182) (cid:90) 1
(cid:181)
(cid:90) ∞
n− 1
Pik () log pi ()d = k
k
0
0
= ψ(k) − ψ(n)
(cid:82) 1
where ψ(x) is the digamma function (the logarithmic derivative of the gamma function). To verify
0 xa−1 (1 − x)b−1 = Γ(a)Γ(b)/Γ(a + b) with respect to
the last equality, differentiate the identity

pk−1 (1 − p)n−k−1 log p dp

EPik () (log pi ()) =

(2)

(3)

Pik () =

2

(5)

log i

the parameter a and recall that Γ(cid:48) (x) = ψ(x)Γ(x). The expectation is taken over the positions of
all other n − 1 points, with xi kept ﬁxed. Assuming that f (x) is almost constant in the entire -ball
around xi , we obtain:
pi () ≈ cd d f (xi ).
(4)
where d is the dimension of x and cd is the volume of the d-dimensional unit ball (cd = πd/2 /Γ(1 +
d/2) for Euclidean norm). Substituting Eq. (4) into Eq. (3), we obtain:
− log f (xi ) ≈ ψ(n) − ψ(k) + log(cd ) + dE (log())
n(cid:88)
which ﬁnally leads to the unbiased kNN estimator for the differential entropy [11]:
Hk (X ) = ψ(n) − ψ(k) + log(cd ) + d
n
i=1
where i is the distance from xi to its k-th nearest neighbor. An alternative proof of the asymptotic
unbiasedness and consistency of the kNN estimator is found at [15].
A similar approach can be used to obtain a kNN estimator for the Kullback-Leibler divergence [19].
The estimator works as follows. Let {x1 , ..., xn} and {y1 , ..., ym} be i.i.d. d-dimensional samples
(cid:90)
drawn independently from the densities p and q respectively. By deﬁnition the divergence is given
by:
p(x) log p(x)
D(p(cid:107)q) =
q(x)
The distance of xi to its nearest neighbor in {xj }j (cid:54)=i is deﬁned as
d(xi , xj )
ρn (i) = min
j (cid:54)=i
We also deﬁne the distance of xi to its nearest neighbor in {yj }
νn (i) = min
d(xi , yj )
j=1,...,m
n(cid:88)
+ log m
n − 1
i=1
The authors established asymptotic unbiasedness and mean-square consistency of the estimator (10).
n(cid:88)
The same proofs could be applied to obtain k-nearest neighbor version of the estimator:
m (i)
log vk
n,m = d
ˆDk
n (i)
ρk
n
i=1

Then the estimator of [19] is given by
ˆDn,m = d
n

+ log m
n − 1

log νm (i)
ρn (i)

(10)

(11)

(6)

(7)

(8)

(9)

Being non-parametric, the kNN estimators (6, 11) rely on the order statistics. This makes the ana-
lytical calculation of the gradient hardly possible. Also it leads to a certain lack of smoothness of the
estimator value as a function of the sample coordinates. One also should mention that ﬁnding the
k-nearest neighbor is a computationally intensive problem. It becomes necessarily to use involved
approximate nearest neighbor techniques for large data sets.

3 The MeanNN Entropy Estimator

We propose a novel approach for the entropy estimation as a function of sample coordinates. It is
based on the fact that the kNN estimator (6) is valid for every k . Therefore the differential entropy
can be also extracted from a mean of several estimators corresponding to different values of k . Next
n−1(cid:88)
n−1(cid:88)
n(cid:88)
we consider all the possible values of order statistics k from 1 to n − 1:
1
1
n − 1
n − 1
i=1
k=1
k=1
where i,k is the k-th nearest neighbor of xi . Consider the double-summation last term in Eq. (12).
Exchanging the order of summation, the last sum adds for each sample point xi the sum of log of

Hk = log(cd ) + ψ(n) +

(−ψ(k) + d
n

Hmean =

log i,k )

(12)

3

its distances to all its nearest neighbors in the sample. It is of course equivalent to the sum of log of
(cid:88)
its distances to all other points in the sample set. Hence the mean estimator (12) for the differential
entropy can be written as:
i (cid:54)=j

log (cid:107)xi − xj (cid:107)

Hmean = const +

d
n(n − 1)

(13)

where the constant depends just on the sample size and dimensionality. We dub this estimator, the
MeanNN estimator for differential entropy. It follows that the differential entropy (approximation)
has a clear geometric meaning. It is proportional to log of the products of distances between each
two points in a random i.i.d. sample. It is an intuitive observation since a higher entropy would
lead to a larger scattering of the samples thus pairwise distances would grow resulting in a larger
product of all distances. Moreover, the MeanNN estimator (13) is a smooth function of the sample
coordinates. Its gradient can be easily found. The asymptotic unbiasedness and consistency of the
estimator follow from the same properties of the kNN estimator (6). Obviously, the same method
gives the mean estimator for the mutual information by usage of well known equality connecting the
mutual information and marginal and joint entropies:
Imean (X ; Y ) = Hmean (X ) + Hmean (Y ) − Hmean (X, Y )

(14)

We demonstrate the MeanNN estimator for the entropy in the case exponential distributed random
− x
variable f (x, µ) = 1
µ , x > 0, µ > 0. In this case case the entropy may be analytically calculated
µ e
as H = log µ + 1. We compared the performance of the MeanNN estimator with k-nearest neighbor
estimator (6) for various values of k . Results are given in Table 1. One may see that the mean
square error of the MeanNN estimator is the same or worse for the traditional kNN estimators. But
the standard deviation of the estimator values is best for the MeanNN estimator. Further we will
apply MeanNN for optimization of a certain criterion based on the entropy. In such cases the most
important characteristics of an estimator is its monotonic dependency on the estimated value and
the prediction of the exact value of the entropy is less important. Therefore one may conclude that
MeanNN is better applicable for optimization of entropy based numerical criteria.

Mean square error of entropy estimation
STD of estimator values

1NN
0.0290
0.1698

4NN
0.0136
0.1166

10NN MeanNN
0.0248
0.0117
0.1079
0.1029

Table 1: Performance of MeanNN entropy estimator in comparison with kNN entropy estimators.
100 samples of random variable, 10 various values of µ parameter, 100 repetitions.

(cid:88)
To obtain the estimator for the divergence we apply the same mean approach to estimator (11) setting
m = n − 1:
(cid:88)
n(cid:88)
n−1(cid:88)
log d(xi , xj )
i (cid:54)=j
i,j
i=1
k=1
(15)
The mean estimator for the divergence has a clear geometric interpretation. If the product of all
distances inside one sample is small in comparison with the product of pairwise distances between
the samples then one concludes that divergence is large and vice versa.

log d(xi , yj ) −

m (i)
log vk
n (i)
ρk

d
n(n − 1)

d
n(n − 1)

ˆDmean
n,n−1 =

=

4 The MeanNN ICA Algorithm
(cid:90)
dµ = D(q(y1 , .., yd )(cid:107) d(cid:89)
As many approaches do, we will use a contrast function
(cid:81)d
q(y1 , ..., yd ) log q(y1 , .., yd )
i=1 q(yi )
i=1

J (Y ) =

d(cid:88)
i=1

q(yi )) =

H (Yi )−H (Y1 , ..., Yd )
(16)

Considering Y as linear function of X , Y = W X , it is easily veri ﬁed [3, 7, 10] that

4

d(cid:88)
t=1

J (Y ) =

H (Yt ) − H (X1 , ..., Xd ) − log(|W |)

(17)

In particular, the change in the entropy of the joint distribution under linear transformation is simply
the logarithm of the Jacobian of the transformation. As we will assume the X ’s to be pre-whitened,
W will be restricted to rotation matrices, therefore log(|W |) = 0 and the minimization of J (Y )
reduces to ﬁnding
ˆW = arg min
H (Y1 ) + ... + H (Yd )
W
Denoting the rows of the matrix W by W = (w1 , ..., wd )(cid:62)
d(cid:88)
expression as a function of W :
(cid:62)
H (w
t X )
t=1
n(cid:88)
d(cid:88)
Then we can plug the MeanNN entropy estimator into Eq. (19) to obtain (after omitting irrelevant
constants) an explicit contrast function to minimize:
i (cid:54)=j
t=1

, we can explicitly write the minimization

t (xi − xj ))2 )
(cid:62)
log((w

S (W ) = arg min
W

ˆW = arg min
W

ˆW = arg min
W

(20)

(19)

(18)

Gst

(21)

(22)

W =

The gradient of the contrast function S (W ) with respect to a rotation matrix W may be found with
d−1(cid:89)
d(cid:89)
the assistance of the so-called Givens rotations (see e.g. [14]). In this parametrization a rotation
matrix W ∈ Rd×d is represented by a product of d(d − 1)/2 plane rotations:
t=s+1
s=1
where Gst is a rotation matrix corresponding to a rotation in the st plane by an angle λst . It is
(cid:183)
(cid:184)
(cid:183)
(cid:184)
the identity matrix except that its elements (s, s),(s, t),(t, s),(t, t) form a two-dimensional (2-D)
rotation matrix by

sin(λst )
cos(λst )
Gst (s, s) Gst (s, t)
=
− sin(λst )
cos(λst )
Gst (t, s) Gst (t, t)
(cid:184)
(cid:183) − sin(λst )
(cid:184)
(cid:183)
The gradient of a single rotation matrix Gst with respect to λst is a zero matrix except for elements
(s, s),(s, t),(t, s),(t, t) for which
cos(λst )
Gst (s, s) Gst (s, t)
∂
=
− cos(λst ) − sin(λst )
(cid:34)
Gst (t, s) Gst (t, t)
∂λst
d(cid:88)
d(cid:88)
d−1(cid:89)
n(cid:88)
d(cid:89)
It can easily veriﬁed that the gradient of the contrast function (20) is given by
(xir − xj r )
∂
∂wqr
∂S
|w
q (xi − xj )|
(cid:62)
∂λst
∂wqr
∂λst
i(cid:54)=j
q ,r=1
u=1
q ,r=1
v=u+1
Guv if both u = s and v = t, and ˜Guv = Guv otherwise.
where ˜Guv = ∂
∂λuv
The contrast function S (W ) and its gradient
∂
S may in theory suffer from discontinuities if a
row wt is perpendicular to a vector xi − xj . To overcome this numerical difﬁculty we utilize a
∂λst
smoothed version of the contrast function S (W, ) and give the expression for its gradient:
n(cid:88)
d(cid:88)
(cid:34)
(cid:35)
t (xi − xj ))2 + )
(cid:62)
log((w
n(cid:88)
d(cid:88)
d−1(cid:89)
i (cid:54)=j
t=1
(xir − xj r )
q (xi − xj ))2 + 
(cid:62)
(w
i(cid:54)=j
q ,r=1
u=1

d(cid:89)
v=u+1

d(cid:88)
q ,r=1

S (W, ) =

∂S
∂wqr

∂wqr
∂λst

∂
∂λst

(25)

(26)

(23)

(24)

(cid:35)

˜Guv

qr

S =

=

S =

= 2

For the optimization of the contrast function we apply the conjugate gradient method. The algorithm
is summarized in Figure 1.

5

˜Guv

qr

Input: Data vectors x1 , x2 , ..., xn ∈ Rd , assumed whitened
Output: Mixing matrix W
Method:
• Initialize d(d − 1)/2 rotation angles λst
• Apply the conjugate gradient optimization to the contrast function S (W (λ)) (25) to
ﬁnd the optimal angles
• Reconstruct the rotation matrix W from the found angles by Givens rotations (21)

Figure 1: The MeanNN ICA algorithm

5 Experiments

First we study the set of 9 problems proposed by [2]. Each problem corresponds to a 1D probability
distribution q(x). One thousand pairs of random numbers x and y are mixed as x(cid:48) = x cos φ +
y sin φ, y (cid:48) = −x sin φ + y cos φ with random angle φ common to all pairs (i.e. A is a pure rotation).
We applied the conjugate gradient methods for the optimization of the contrast function (25) with
 = 1/n = 0.001 in order to recover this rotation matrix. This was repeated 100 times with different
angles φ and with different random sets of pairs (x, y). To assess the quality of the estimator ˆA
(or, equivalently, of the back transformation ˆW = ˆA−1 ), we use the Amari performance index Perr
from [1].
d(cid:88)
|pij |
|pij |
1
maxk |pkj | ) − 1
maxk |pik | +
2d
i,j=1
where pij = ( ˆA−1A)ij . We compared our method with three state-of-the-art approaches: MILCA
[16], RADICAL [13] and KernelICA [2]. We used the ofﬁcial code proposed by authors 1 . For the
ﬁrst two techniques that utilize different information theoretic measures assessed by order statistics
it is highly recommended to use dataset augmentation. This is a computationally intensive technique
for the dataset enlargement by replacing each data set point with a ﬁxed number (usually 30) new
data points randomly generated in the small neighborhood of the original point. The proposed
method gives smooth results without any additional augmentation due to its smooth nature (see Eq.
(13)).

Perr =

(

(27)

pdfs MILCA MILCA Aug RADICAL RADICAL Aug KernelICA MeanNN ICA
2.4
3.3
2.8
3.6
2.5
3.3
a
2.6
3.0
3.3
3.6
3.0
3.4
b
4.2
c
7.5
4.4
7.6
5.4
4.9
1.4
1.4
1.4
1.6
1.7
1.8
d
1.4
1.5
1.7
1.5
1.6
1.7
e
1.3
1.4
1.4
1.4
1.6
1.4
f
1.3
g
1.4
1.6
1.4
1.4
1.4
1.4
1.5
1.7
1.6
2.0
1.7
h
1.5
i
1.9
2.1
1.8
1.8
1.8

Table 2: Amari performance (multiplied by 100) for two-component ICA. The distributions are: (a)
Student with 3 degrees of freedom; (b) double exponential; (c) Student with 5 degrees of freedom;
(d) exponential; (e) mixture of two double exponentials; (f) symmetric mixtures of two Gaussians;
(g) nonsymmetric mixtures of two Gaussians; (h) symmetric mixtures of four Gaussians; (i) non-
symmetric mixtures of four Gaussians.

In the explored cases the proposed method achieves the level of a state-of-the-art performance. This
is well explained by the inherent smoothness of MeanNN estimator, see Figure 2. Here we presented
1 http://www.klab.caltech.edu/∼kraskov/MILCA/,
https://www.cs.umass.edu/∼elm/ICA/,
http://www.di.ens.fr/∼fbach/kernel-ica/index.htm

6

the comparison of different contrast functions based on different order statistics estimators for a grid
of possible rotations angles for the mixture of two exponentially distributed random variables (case
MILCA approach. Also the contrast function corresponding to the order statistics k = 30 (cid:39) √
e). The contrast function corresponding to the order statistics k = 10 generally coincides with the
n
generally coincides with the RADICAL method. One may see that MeanNN ICA contrast function
leads to much more robust prediction of the rotation angle. One should mention that the gradient
based optimization enables to obtain the global optimum with high precision as opposed to MILCA
and RADICAL schemes which utilize subspace grid optimization.

Application of the gradient based optimization schemes also leads to a computational advantage.
The number of needed function evaluations was limited by 20 as opposed to 150 evaluations for grid
optimization schemes MILCA and RADICAL.

Figure 2: Convergence analysis for a mixture of two exponentially distributed random variables.
Contrast function dependence on a rotation angle for different entropy estimators. 1000 samples,
0.01 radian grid.

We also studied the application of MeanNN ICA to multidimensional problems. For that purpose
we chose at random D (generally) different distributions, then we mixed them by a random rotation
and ran the compared ICA algorithms to recover the rotation matrix. The results are presented at
Table 3. MeanNN ICA achieved the best performance.

dims MILCA MILCA Aug RADICAL RADICAL Aug KernelICA MeanNN ICA
2.5
2.9
3.0
3.1
3.3
3.0
2
2.2
4
2.7
2.7
2.8
2.3
2.6

Table 3: Amari index (multiplied by 100) for multidimensional ICA. 1000 samples, 10 repetitions

6 Conclusion

We proposed a novel approach for estimation of main information theoretic measures such as dif-
ferential entropy, mutual information and divergence. The estimators represent smooth differential
functions with clear geometrical meaning. Next this novel estimation technique was applied to the
ICA problem. Compared to state-of-the-art ICA methods the proposed method demonstrated supe-
rior results in the conducted tests.

Studied state-of-the-art approaches can be divided in two groups. The ﬁrst group is based on exact
entropy estimation, that usually leads to high performance as demonstrated by MILCA and RADI-
CAL. The drawback of such estimators is the lack of the gradient and therefore numerical difﬁculties
in optimization. The second group apply different from entropy criteria, that beneﬁt easy calcula-
tion of gradient (KernelICA). However such methods may suffer from deteriorated performance.

7

00.20.40.60.811.21.41.622.12.22.32.42.52.62.72.82.9Rotation angle φContrast function S(W(φ))  MeanNN10NN30NNMeanNN ICA comprises the advantages of these two kinds of estimators. It represents a contrast
function based on an accurate entropy estimation and its gradient is given analytically therefore it
may be readily optimized.

Finally we mention that the proposed estimation method may further be applied to various problems
in the ﬁeld of machine learning and beyond.

References

[1] S. Amari, A. Cichoki, and H.H.Yang. A new learning algorithm for blind signal separation. Advances in
Neural Information Processing Systems, 8, 1996.
[2] F. Bach and M. Jordan. Kernel independent component analysis. Journal of Machine Learning Research,
3, 2002.
[3] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind
deconvolution. Neural Computatiuon, 7, 1995.
[4] J.-F. Cardoso. Multidimensional independent component analysis. Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Processing (ICASSP’98), 1998.
[5] C.Jutten and J.Herault. Blind separation of sources, part 1: An adaptive algorithm based on neuromimetic
architecture. Signal Processing, 1991.
[6] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3), 1994.
[7] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, August
1991.
[8] D.T.Pham and P.Garat. Blind separation of mixtures of independent signals through a quasi-maximum
likelihood approach. IEEE transactions on Signal Processing 45(7), 1997.
[9] A. Hyvarinen and E.Oja. A fast ﬁxed point algorithm for independent component analysis.
computation, 9(7), 1997.
[10] A. Hyvarinen, J. Karhunen, and E. Oja. Independent component analysis. 2001.
[11] L. Kozachenko and N. Leonenko. On statistical estimation of entropy of random vector. Problems Infor.
Transmiss., 23 (2), 1987.
[12] A. Kraskov, H. St ¨ogbauer, and P. Grassberger. Estimating mutual information. Physical Review E,
69:066138, 2004.
[13] E. Miller and J. Fisher. Ica using spacing estimates of entropy. Proc. Fourth International Symposium on
,
Independent Component Analysis and Blind Signal Separation, Nara, Japan, Apr. 2003, pp. 1047–1052.
2003.
[14] J. Peltonen and S. Kaski. Discriminative components of data. IEEE Transactions on Neural Networks,
16(1), 2005.
[15] H. Singh, N. Misra, V. Hnizdo, A. Fedorowicz, and Eugene Demchuk. Nearest neighbor estimates of
entropy. American Journal of Mathematical and Management Sciences, 2003.
[16] H. St ¨ogbauer, A. Kraskov, S. Astakhov, and P. Grassberger. Least-dependent-component analysis based
on mutual information. Phys. Rev. E, 70(6):066123, Dec 2004.
[17] O. Vasicek. A test for normality based on sample entropy. J. Royal Stat. Soc. B, 38 (1):54–59, 1976.
[18] J. D. Victor. Binless strategies for estimation of information from neural data. Physical Review, 2002.
[19] Q. Wang, S. R. Kulkarni, and S. Verdu. A nearest-neighbor approach to estimating divergence between
continuous random vectors. IEEE Int. Symp. Information Theory, Seattle, WA, 2006.

Neural

8


Deep Learning with Kernel Regularization
for Visual Recognition

Kai Yu
Yihong Gong
Wei Xu
NEC Laboratories America, Cupertino, CA 95014, USA
{kyu, wx, ygong}@sv.nec-labs.com

Abstract

In this paper we aim to train deep neural networks for rapid visual recognition.
The task is highly challenging, largely due to the lack of a meaningful regular-
izer on the functions realized by the networks. We propose a novel regularization
method that takes advantage of kernel methods, where an oracle kernel function
represents prior knowledge about the recognition task of interest. We derive an ef-
ﬁcient algorithm using stochastic gradient descent, and demonstrate encouraging
results on a wide range of recognition tasks, in terms of both accuracy and speed.

1 Introduction

Visual recognition remains a challenging task for machines. This difﬁculty stems from the large
pattern variations under which a recognition system must operate. The task is extremely easy for a
human, largely due to the expressive deep architecture employed by human visual cortex systems.
Deep neural networks (DNNs) are argued to have a greater capacity to recognize a larger variety of
visual patterns than shallow models, because they are considered biologically plausible.

However, training deep architectures is difﬁcult because the large number of parameters to be tuned
necessitates an enormous amount of labeled training data that is often unavailable. Several authors
have recently proposed training methods by using unlabeled data. These methods perform a greedy
layer-wise pre-training using unlabeled data, followed by a supervised ﬁne-tuning [9, 4, 15]. Even
though the strategy notably improves the performance, to date, the best reported recognition accu-
racy on popular benchmarks such as Caltech101 by deep models is still largely behind the results of
shallow models.

Beside using unlabeled data, in this paper we tackle the problem by leveraging additional prior
knowledge. In the last few decades, researchers have developed successful kernel-based systems
for a wide range of visual recognition tasks. Those sensibly-designed kernel functions provide
an extremely valuable source of prior knowledge, which we believe should be exploited in deep
learning. In this paper, we propose an informative kernel-based regularizer, which makes it possible
to train DNNs with prior knowledge about the recognition task.

Computationally, we propose to solve the learning problem using stochastic gradient descent (SGD),
as it is the de facto method for neural network training. To this end we transform the kernel regu-
larizer into a loss function represented as a sum of costs by individual examples. This results in a
simple multi-task architecture where a number of extra nodes at the output layer are added to ﬁt a
set of auxiliary functions automatically constructed from the kernel function.

We apply the described method to train convolutional neural networks (CNNs) for a wide range of
visual recognition tasks, including handwritten digit recognition, gender classiﬁcation, ethnic origin
recognition, and object recognition. Overall our approach exhibits excellent accuracy and speed on
all of these tasks. Our results show that incorporation of prior knowledge can boost the performance
of CNNs by a large margin when the training set is small or the learning problem is difﬁcult.

1

2 DNNs with Kernel Regularization

L(β , θ) =

In our setting, the learning model, a deep neural network (DNN), aims to learn a predictive function
f : X → R that can achieve a low expected discrepancy E [‘(y , f (x))] over the distribution p(x, y).
In the simplest case Y = {−1, 1} and ‘(·, ·) is a differentiable hinge loss. Based on a set of labeled
nX
‘ (cid:0)yi , β>
(cid:1) + λkβ1 k2
examples [(xi , yi )]n
i=1 , the learning is by minimizing a regularized loss
1 φi + β0
i=1
where φi = φ(xi ; θ) maps xi to q -dimensional hidden units via a nonlinear deep architecture
with parameters θ , including the connection weights and biases of all the intermediate layers,
β = {β1 , β0 }, β1 includes all the parameters of the transformation from the last hidden layer to
the output layer, β0 is a bias term, λ > 0, and kak2 = tr(a>a) is the usual weight decay reg-
yi ,
 + λ
ularization. Applying the well-known representor theorem, we derive the equivalence to a kernel
nX
nX
nX
system1
j=1
i=1
i,j=1

L(α, β0 , θ) =

αj Ki,j + β0

(1)

‘

αiαj Ki,j

(2)

where the kernel is computed by
Ki,j = hφ(xi ; θ), φ(xj ; θ)i = φ>
i φj
We assume the network is provided with some prior knowledge, in the form of an m × m kernel
matrix Σ, computed on n labeled training data, plus possibly additional m−n unlabeled data if m >
n. We exploit this prior knowledge via imposing a kernel regularization on K (θ) = [Ki,j ]m
i,j=1 , such
that the learning problem seeks
Problem 2.1.

L(β , θ) + γΩ(θ)
min
β ,θ
Ω(θ) = tr (cid:2)K (θ)−1Σ(cid:3) + log det[K (θ)]
where γ > 0 and Ω(θ) is deﬁned by
This is a case of semi-supervised learning if m > n. Though Ω is non-convex w.r.t. K , it has a
unique minimum at K = Σ if Σ (cid:31) 0, suggesting that minimizing Ω(θ) encourages K to approach
Σ. The regularization can be explained from an information-theoretic perspective. Let p(f |K ) and
p(f |Σ) be two Gaussian distributions N (0, K ) and N (0, Σ).2 Ω(θ) is related to the KL-divergence
DKL [p(f |Σ)kp(f |K )]. Therefore, minimizing Ω(θ) forces the two distributions to be close. We
note that the regularization does not require Σ to be positive deﬁnite — it can be semideﬁnite.

(4)

(3)

3 Kernel Regularization via Stochastic Gradient Descent

The learning problem in Eq. (3) can be solved by using gradient-based methods. In this paper we
emphasize large-scale optimizations using stochastic gradient descent (SGD), because the method
is fast when the size m of total data is large and backpropagation, a typical SGD, has been the de
facto method to train neural networks for large-scale learning tasks.

SGD considers the problem where the optimization cost is the sum of the local cost of each indi-
vidual training example. A standard batch gradient descent updates the model parameters by using
the true gradient summed over the whole training set, while SGD approximates the true gradient by
the gradient caused by a single random training example. Therefore, the parameters of the model

1 In this paper we slightly abuse the notation, i.e., we use L to denote different loss functions. However their
meanings should be uniquely identiﬁed by checking the input parameters.
2From a Gaussian process point of view, a kernel function deﬁnes the prior distribution of a function f , such
that the marginal distribution of the function values f on any ﬁnite set of inputs is a multivariate Gaussian.

2

are updated after each training example. For large data sets, SGD is often much faster than batch
gradient descent.

However, because the regularization term de ﬁned by Eq. (4) does not consist of a cost function that
can be expressed as a sum (or an average) over data examples, SGD is not directly applicable. Our
idea is to transform the problem into an equivalent formulation that can be optimized stochastically.

3.1 Shrinkage on the Kernel Matrix

We consider a large-scale problem where the data size m may grow over time, while the size of the
last hidden layer (q ) of the DNN is ﬁxed. Therefore the computed kernel K can be rank de ﬁcient.
In order to ensure that the trace term in Ω(θ) is well-deﬁned, and that the log-determinant term is
bounded from below, we instead use K + δI to replace K in Ω(θ), where δ > 0 is a small shrinkage
log det(K + δI ) = log det (cid:0)Φ>Φ + δI (cid:1) + const
parameter and I is an identity matrix. Thus the log-determinant acts on a much smaller q × q matrix3
where Φ = [φ1 , . . . , φm ]> and const = (m − q) · log δ . Omitting all the irrelevant constants, we
Ω(θ) = tr (cid:2)(ΦΦ> + δI )−1Σ(cid:3) + log det(Φ>Φ + δI )
then turn the kernel regularization into
The kernel shrinkage not only remedies the ill-posedness, but also yields other conveniences in our
later development.
By noticing that Φ>Φ = Pn
3.2 Transformation of the Log-determinant Term
i=1 φiφ>
is a sum of quantities over data examples, we move it outside
i
of the log determinant for the convenience of SGD.
Theorem 3.1. Consider minθ {L(θ) = h(θ) + g(a)}, where g(·) is concave and a ≡ a(θ) is a
(cid:8)L(θ , ψ) = h(θ) + a(θ)>ψ − g• (ψ)(cid:9)
function of θ , if its local minimum w.r.t. θ exists, then the problem is equivalent to
min
θ,ψ
where g• (ψ) is the conjugate function of g(a), i.e. g• (ψ) = mina{ψ>a − g(a)}.4

(6)

(5)

Proof. For a concave function g(a), the conjugate function of its conjugate function is itself,
i.e., g(a) = minψ {a>ψ − g• (ψ)}. Since g• (ψ) is concave, a>ψ − g• (ψ) is convex w.r.t. ψ
and has the unique minimum g(a). Therefore minimizing L(θ , ψ) w.r.t. θ and ψ is equivalent to
minimizing L(θ) w.r.t. θ .

Since log-determinant is concave for q × q positive de ﬁnite matrices A, the conjugate function
of log det(A) is log det(Ψ) + q . We can use the above theorem to transform any loss function
containing log det(A) into another loss, which is an upper bound and involves A in a linear term.
" mX
#
log det (cid:0)Φ>Φ + δI (cid:1) = min
Therefore the log-determinant in Eq. (5) is turned into a variational representation
Ψ∈S+
q
i=1
where Ψ ∈ S+
q is a q × q positive deﬁnite matrix, and const = −q . As we can see, the upper bound
is a convex function of auxiliary variables Ψ and more importantly, it amounts to a sum of local
quantities caused by each of the m data examples.

i Ψφi + δ · tr(Ψ) − log det(Ψ) + const
φ>

3Hereafter in this paper, with a slight abuse of notation, we use “ const ” in equations to summarize the terms
irrelevant to the variables of interest.
4 If g(a) is convex, its conjugate function is g◦ (ψ) = maxa {ψ> a − g(a)}.

3

3.3 Transformation of the Trace Term
We assume that the kernel matrix Σ is presented in a decomposed form Σ = U U > , with U =
[u1 , . . . , um ]> , ui ∈ Rp , and p ≤ m. We have found that the trace term can be cast as a variational
problem by introducing an q × p auxiliary variable matrix η .
" mX
#
tr (cid:2)(ΦΦ> + δI )−1Σ(cid:3) = min
Proposition 3.1. The trace term in Eq. (5) is equivalent to a convex variational representation
k 1√
η∈Rq×p
δ
i=1
Proof. We ﬁrst obtain the analytical solution η∗ = 1√
(Φ>Φ + δI )−1Φ>U , where the variational
(cid:20) 1
(cid:21)
δ
representation reaches its unique minimum. Then, plugging it back into the function, we have
1
1√
U >Φη∗ + η∗>Φ>Φη∗ +
U >U − 2
U >Φ(Φ>Φ + δI )−2Φ>U
tr (cid:2)U >U − U >Φ(Φ>Φ + δI )−1Φ>U (cid:3) = tr (cid:2)(ΦΦ> + δI )−1U U > (cid:3)
δ
δ
δ
1
δ
where the last step is derived by applying the Woodbury matrix identity.

ui − η>φi k2 + δkηk2
F

tr

=

Again, we note that the upper bound is a convex function of η , and consists of a sum of local costs
over data examples.

Ω(θ) ≤

3.4 An Equivalent Learning Framework
#
"
Combining the previous results, we obtain the convex upper bound for the kernel regularization
(cid:19)
(cid:18)
mX
Eq. (5), which amounts to a sum of costs over examples under some regularization
k 1√
F + δ · tr(Ψ) − log det(Ψ)
+ δkηk2
δ
i=1
where we omit all the terms irrelevant to η , Ψ and θ . L(η , Ψ, θ) is convex w.r.t. η and Ψ, and has
i
h
a unique minimum Ω(θ), hence we can replace Ω(θ) by instead minimizing the upper bound and
formulate an equivalent learning problem
L(β , η , Ψ, θ) = L(β , θ) + γL(η , Ψ, θ)

ui − η>φi k2 + φ>
i Ψφi

L(η , Ψ, θ) =

(7)

min
β ,η ,Ψ,θ

Clearly this new optimization can be solved by SGD.

When applying the SGD method, each step based on one example needs to compute the inverse of
Ψ. This can be computationally unaffordable when the dimensionality is large (e.g. q > 1000) —
remember that the efﬁciency of SGD is dependent on the lightweight of each stochastic update. Our
next result suggests that we can dramatically reduce this complexity from O(q3 ) to O(q).
#
" mX
(cid:19)
(cid:18)
F + δ · ψ> e − qX
Proposition 3.2. Eq. (5) is equivalent to the convex variational problem
k 1√
+ δkηk2
Ω(θ) = min
δ
η ,ψ
i=1
k=1
where ψ = [ψ1 , . . . , ψq ]> , and e = [1, . . . , 1]> .
Proof. There is an ambiguity of the solutions up to rotations. Suppose {β ∗ , Φ∗ , η∗ , Ψ∗ } is an op-
timal solution set, a transformation β ∗ ← Rβ ∗ , Φ∗ ← RΦ∗ , η∗ ← Rη∗ , and Ψ∗ ← RΨ∗R>
results in the same optimality if R>R = I . Since there always exists an R to diagonalize Ψ∗ , we
can pre-restrict Ψ to be a diagonal positive de ﬁnite matrix Ψ = diag[ψ1 , . . . , ψq ], which does not
change our problem and gives rise to Eq. (8).

ui − η>φi k2 + ψ>φ2
i

log ψk

(8)

We note that the variational form is convex w.r.t. the auxiliary variables η and ψ . Therefore we can
formulate the whole learning problem as

4

(9)

k 1√
δ

L2 (η , θ) =

L3 (ψ , θ) =

log ψk

L1 (β , θ) + γ
mn

L2 (η , θ) + γ
mn

Problem 3.1.

(cid:21)
L3 (ψ , θ)

(cid:20)
1
L(β , η , ψ , θ) =
min
n
β ,η ,ψ ,θ
mX
where L1 (β , θ) is deﬁned by Eq. (1), and
ui − η>φi k2 + δkηk2
i + δ · ψ> e − qX
mX
F
i=1
ψ>φ2
i=1
k=1
To ensure the estimator of β and θ is consistent, the effect of regularization should vanish as n → ∞.
Therefore we intentionally normalize L2 (η , θ) and L3 (ψ , θ) by 1/m. The overall loss function is
averaged over the n labeled examples, consisting of three loss functions: the main classiﬁcation task
L1 (β , θ), an auxiliary least-squares regression problem L2 (η , θ), and an additional regularization
term L3 (ψ , θ), which can be interpreted as another least-squares problem. Since each of the loss
functions amounts to a summation of local costs caused by individual data examples, the whole
learning problem can be conveniently implemented by SGD, as described in Algorithm 1.
In practice, the kernel matrix Σ = U U > that represents domain knowledge can be obtained in
three different ways: (i) In the easiest case, U is directly available by computing some hand-crafted
features computed from the input data, which corresponds to a case of a linear kernel function; (ii)
U can be results of some unsupervised learning (e.g. the self-taught learning [14] based on sparse
coding), applied on a large set of unlabeled data; (iii) If a nonlinear kernel function is available, U
can be obtained by applying incomplete Cholesky decomposition on an m × m kernel matrix Σ. In
the third case, when m is so large that the matrix decomposition cannot be computed in the main
memory, we apply the Nystr ¨om method [19]: We ﬁrst randomly sample m1 examples p < m1 < m,
such that the computed kernel matrix Σ1 can be decomposed in the memory. Let V DV > be the p-
rank eigenvalue decomposition of Σ1 , then the p-rank decomposition of Σ can be approximated by
2 , where Σ:,1 is the m × m1 kernel matrix between all the m examples
Σ ≈ U U > , U = Σ:,1V D− 1
and the subset of size m1 .

Algorithm 1 Stochastic Gradient Descent
repeat
Generate a number a from uniform distribution [0, 1]
if a < n
m+n then
Randomly pick a sample i ∈ {1, · · · , n} for L1 , and update parameter by
[β , θ ] ← [β , θ ] − 
∂L1 (xi , β , θ)
∂ [β , θ ]

else
Randomly pick a sample i ∈ {1, · · · , m} for L2 , and update parameter by
[η , ψ , θ ] ← [η , ψ , θ ] − 
∂ [L2 (xi , η , θ) + L3 (xi , ψ , θ)]
m
∂ [η , ψ , θ ]

end if
until convergence

4 Visual Recognition by Deep Learning with Kernel Regularization

In the following, we apply the proposed strategy to train a class of deep models and convolutional
neural networks (CNNs, [11]) for a range of visual recognition tasks including digit recognition on
MNIST dataset, gender and ethnicity classiﬁcation on the FRGC face dataset, and object recognition
on the Caltech101 dataset. In each of these tasks, we choose a kernel function that has been reported
to have state-of-the-art or otherwise good performances in the literature. We will see whether a
kernel-regularizer can improve the recognition accuracy of the deep models, and how it is compared
with the support vector machine (SVM) using the exactly the same kernel.

5

Table 1: Percentage error rates of handwritten digit recognition on MNIST
100
600
1000
3000
60000
Training Size
1.41
3.91
6.58
8.53
22.73
SVM (RBF)
5.16
5.51
6.92
9.15
24.73
SVM (RBF, Nystr ¨om)
2.23
3.01
3.46
3.74
5.21
SVM (Graph)
2.87
4.28
5.75
6.47
7.17
SVM (Graph, Cholesky)
19.40
6.40
5.50
2.75
0.82
CNN
1.88
3.40
14.49
0.73
3.85
kCNN (RBF)
0.64
2.36
4.28
2.05
1.75
kCNN (Graph)
−
−
−
3.21
0.64
CNN (Pretrain) [15]
−
3.34
11.73
3.42
2.28
EmbedO CNN [18]
−
1.83
2.73
3.82
7.75
EmbedI 5 CNN [18]
−
7.87
3.82
2.76
2.07
EmbedA1 CNN [18]

Throughout all the experiments, “kCNN” denotes CNNs regularized by nonlinear kernels, processed
by either Cholesky or Nystr ¨om approximation, with parameters p = 600, m1 = 5000, and m the
size of each whole data set. The obtained ui are normalized to have unitary lengths. λ and δ are
ﬁxed by 1. The remaining two hyperparameters are: the learning rates  = {10−3 , 10−4 , 10−5 } and
the kernel regularization weights γ = {102 , 103 , 104 , 105 }. Their values are set once for each of the
4 recognition tasks based on a 5-fold cross validation using 500 labeled examples.

4.1 Handwritten Digit Recognition on MNIST Dataset
The data contains a training set with 60000 examples and a test set with 10000 examples. The CNN
employs 50 ﬁlters of size 7 × 7 on 34 × 34 input images, followed by down-sampling by 1/2, then
128 ﬁlters of size 5 × 5, followed by down-sampling by 1/2, and then 200 ﬁlters of size 5 × 5, giving
rise to 200 dimensional features that are fed to the output layer. Two nonlinear kernels are used: (1)
RBF kernel, and (2) Graph kernel on 10 nearest neighbor graph [6]. We perform 600-dimension
Cholesky decomposition on the whole 70000 × 70000 graph kernel because it is very sparse.
In addition to using the whole training set, we train the models on 100, 600, 1000 and 3000 random
examples from the training set and evaluate the classiﬁers on the whole test set, and repeat each
setting by 5 times independently. The results are given in Tab. 1. kCNNs effectively improve over
CNNs by leveraging the prior knowledge, and also outperform SVMs that use the same kernels. The
results are competitive with the state-of-the-art results by [15], and [18] of a different architecture.

4.2 Gender and Ethnicity Recognition on FRGC Dataset
The FRGC 2.0 dataset [13] contains 568 individuals’ 14714 face images under various lighting
conditions and backgrounds. Beside person identities, each image is annotated with gender and
ethnicity, which we put into 3 classes, “white”, “asian”, and “other”. We ﬁx
114 persons’ 3014
images (randomly chosen) as the testing set, and randomly selected 5%, 10%, 20%, 50%, and “All”
images from the rest 454 individuals’ 11700 images. For each training size, we randomize the
training data 5 times and report the average error rates.
In this experiment, CNNs operate on images represented by R/G/B planes plus horizontal and ver-
tical gradient maps of gray intensities. The 5 input planes of size 140 × 140 are processed by 16
convolution ﬁlters with size 16 × 16, followed by max pooling within each disjoint 5 × 5 neigh-
borhood. The obtained 16 feature maps of size 25 × 25 are connected to the next layer by 256
ﬁlters of size 6 × 6, with 50% random sparse connections, followed by max pooling within each
5 × 5 neighborhood. The resulting 256 × 4 × 4 features are fed to the output layer. The nonlinear
kernel used in this experiment is the RBF kernel computed directly on images, which has demon-
strated state-of-the-art accuracy for gender recognition [3]. The results shown in Tab. 2 and Tab. 3
demonstrate that kCNNs signi ﬁcantly boost the recognition accuracy of CNNs for both gender and
ethnicity recognition. The difference is prominent when small training sets are presented.

4.3 Object Recognition on Caltech101 Dataset
Caltech101 [7] contains 9144 images from 101 object categories and a background category. It is
considered one of the most diverse object databases available today, and is probably the most popular
benchmark for object recognition. We follow the common setting to train on 15 and 30 images per
class and test on the rest. Following [10], we limit the number of test images to 30 per class. The

6

Table 2: Percentage error rates of gender recognition on FRGC
5% 10% 20% 50% All
Training Size
8.6
9.1
11.3
13.4
16.7
SVM (RBF)
8.8
9.1
11.6
14.3
20.2
SVM (RBF, Nystr ¨om)
61.5
17.2
8.4
6.6
5.9
CNN
4.4
5.0
5.8
7.2
17.1
kCNN

Table 3: Percentage error rates of ethnicity recognition on FRGC
5% 10% 20% 50% All
Training Size
22.9
16.9
14.1
11.3
10.2
SVM (RBF)
11.1
11.9
15.8
20.6
24.7
SVM (RBF, Nystr ¨om)
6.3
8.2
10.0
13.9
30.0
CNN
15.6
8.7
7.3
6.2
5.8
kCNN

recognition accuracy was normalized by class sizes and evaluated over 5 random data splits. The
CNN has the same architecture as the one used in the FRGC experiment. The nonlinear kernel is the
spatial pyramid matching (SPM) kernel developed in [10].

Tab. 4 shows our results together with those reported in [12, 15] using deep hierarchical architec-
tures. The task is much more challenging than the previous three tasks for CNNs, because in each
category the data size is very small while the visual patterns are highly diverse. Thanks to the reg-
ularization by SPM kernel, kCNN dramatically improves the accuracy of CNN, and outperforms
SVM using the same kernel. This is perhaps the best performance by (trainable and hand-crafted)
deep hierarchical models on the Caltech101 dataset. Some ﬁlters trained with and without kernel
regularization are visualized in Fig. 1, which helps to understand the difference made by kCNN.

5 Related Work, Discussion, and Conclusion
Recent work on deep visual recognition models includes [17, 12, 15]. In [17] and [12] the ﬁrst layer
consisted of hard-wired Gabor ﬁlters, and then a large number of patches were sampled from the
second layer and used as the basis of the representation which was then used to train a discriminative
classiﬁer.

Deep models are powerful in representing complex functions but very difﬁcult to train. Hinton and
his coworkers proposed training deep belief networks with layer-wise unsupervised pre-training,
followed by supervised ﬁne-tuning [9]. The strategy was subsequently studied for other deep mod-
els like CNNs [15], autoassociators [4], and for document coding [16]. In recent work [18], the
authors proposed training a deep model jointly with an unsupervised embedding task, which led to
improved results as well. Though using unlabeled data too, our work differs from previous work at
the emphasis on leveraging the prior knowledge, which suggests that it can be combined with those
approaches, including neighborhood component analysis [8], to further enhance the deep learning.
This work is also related to transfer learning [2] that used auxiliary learning tasks to learn a linear
feature mapping, and more directly, our previous work [1], which created pseudo auxiliary tasks
based on hand-craft image features to train nonlinear deep networks.

One may ask, why bother training with kCNN, instead of simply combining two independently
trained CNN and SVM systems? The reason is computational speed – kCNN pays an extra cost to
exploit a kernel matrix in the training phase, but in the prediction phase the system uses CNN alone.

(b) kCNN-Caltech101
(a) CNN-Caltech101
Figure 1: First-layer ﬁlters on the B channel, learned from Caltech101 (30 examples per class)

7

Training Size
SVM (SPM) [10]
SVM (SPM, Nystr ¨om)
HMAX [12]

Table 4: Percentage accuracy on Caltech101
30
15
Training Size
64.6 CNN (Pretrain) [15]
54.0
63.1 CNN
52.1
51.0
56.0
kCNN

15
−
26.5
59.2

30
54.0
43.6
67.4

In our Caltech101 experiment, the SVM (SPM) needed several seconds to process a new image on
a PC with a 3.0 GHz processor, while kCNN can process about 40 images per second. The latest
record on Caltech101 was based on combining multiple kernels [5]. We conjecture that kCNN could
be further improved by using multiple kernels without sacriﬁcing recognition speed.

To conclude, we proposed using kernels to improve the training of deep models. The approach was
implemented by stochastic gradient descent, and demonstrated excellent performances on a range of
visual recognition tasks. Our experiments showed that prior knowledge could signiﬁcantly improve
the performance of deep models when insufﬁcient labeled data were available in hard recognition
problems. The trained model was much faster than kernel systems for making predictions.
Acknowledgment: We thank the reviewers and Douglas Gray for helpful comments.
References

[1] A. Ahmed, K. Yu, W. Xu, Y. Gong, and E. P. Xing. Training hierarchical feed-forward visual recognition
models using transfer learning from pseudo tasks. European Conference on Computer Vision, 2008.
[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-
beled data. Journal of Machine Learning Research, 2005.
[3] S. Baluja and H. Rowley. Boosting sex identiﬁcation performance.
Journal of Computer Vision, 2007.
[4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks.
Neural Information Processing Systems, 2007.
[5] A. Bosch, A. Zisserman, and X. Mun ˜oz. Image classiﬁcation using ROIs and multiple kernel learning.
2008. submitted to International Journal of Computer Vison.
[6] O. Chapelle, J. Weston, and B. Sch ¨olkopf. Cluster kernels for semi-supervised learning. Neural Informa-
tion Processing Systems, 2003.
[7] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An
incremental Bayesian approach tested on 101 object categories. CVPR Workshop, 2004.
[8] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. Neural
Information Processing Systems, 2005.
[9] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,
313(5786):504 – 507, July 2006.
[10] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing
natural scene categories. IEEE Conference on Computer Vision and Pattern Recognition, 2006.
[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324, 1998.
[12] J. Mutch and D. G. Lowe. Multiclass object recognition with sparse, localized features. IEEE Conference
on Computer Vision and Pattern Recognition, 2006.
[13] P. J. Philips, P. J. Flynn, T. Scruggs, K. W. Bower, and W. Worek. Preliminary face recognition grand
challenge results. IEEE Conference on Automatic Face and Gesture Recgonition, 2006.
[14] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: Transfer learning from
unlabeled data. International Conference on Machine Learning, 2007.
[15] M. Ranzato, F.-J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hi-
erarchies with applications to object recognition.
IEEE Conference on Computer Vision and Pattern
Recognition, 2007.
[16] M. Ranzato and M. Szummer. Semi-supervised learning of compact document representations with deep
networks. International Conferenece on Machine Learning, 2008.
[17] T. Serre, L. Wolf, and T. Poggio. Object recognition with features inspired by visual cortex.
Conference on Computer Vision and Pattern Recognition, 2005.
[18] J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding.
Conference on Machine Learning, 2008.
[19] C. Williams and M. Seeger. Using the Nystr ¨om method to speed up kernel machines. Neural Information
Processing Systems, 2001.

International

IEEE

8


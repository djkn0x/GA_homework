Extended Grassmann Kernels for
Subspace-Based Learning

Jihun Hamm
GRASP Laboratory
University of Pennsylvania
Philadelphia, PA 19104
jhham@seas.upenn.edu

Daniel D. Lee
GRASP Laboratory
University of Pennsylvania
Philadelphia, PA 19104
ddlee@seas.upenn.edu

Abstract

Subspace-based learning problems involve data whose elements are linear sub-
spaces of a vector space. To handle such data structures, Grassmann kernels have
been proposed and used previously. In this paper, we analyze the relationship be-
tween Grassmann kernels and probabilistic similarity measures. Firstly, we show
that the KL distance in the limit yields the Projection kernel on the Grassmann
manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is
suboptimal for subspace-based problems. Secondly, based on our analysis of the
KL distance, we propose extensions of the Projection kernel which can be ex-
tended to the set of afﬁne as well as scaled subspaces. We demonstrate the ad-
vantages of these extended kernels for classiﬁcation and recognition tasks with
Support Vector Machines and Kernel Discriminant Analysis using synthetic and
real image databases.

1

Introduction

In machine learning problems the data often live in a vector space, typically a Euclidean space.
However, there are many other kinds of non-Euclidean spaces suitable for data outside this conven-
tional context. In this paper we focus on the domain where each data sample is a linear subspace
of vectors, rather than a single vector, of a Euclidean space. Low-dimensional subspace structures
are commonly encountered in computer vision problems. For example, the variation of images due
to the change of pose, illumination, etc, is well-aproximated by the subspace spanned by a few
“eigenfaces”. More recent examples include the dynamical system models of video sequences from
human actions or time-varying textures, represented by the linear span of the observability matrices
[1, 14, 13].
Subspace-based learning is an approach to handle the data as a collection of subspaces instead of the
usual vectors. The appropriate data space for the subspace-based learning is the Grassmann manifold
G (m, D), which is deﬁned as the set of m-dimensional linear subspaces in RD . In particular, we
can deﬁne positive deﬁnite kernels on the Grassmann manifold, which allows us to treat the space as
if it were a Euclidean space. Previously, the Binet-Cauchy kernel [17, 15] and the Projection kernel
[16, 6] have been proposed and demonstrated the potential for subspace-based learning problems.
On the other hand, the subspace-based learning problem can be approached purely probabilistically.
Suppose the set of vectors are i.i.d samples from an arbitrary probability distribution. Then it is
possible to compare two such distributions of vectors with probabilistic similarity measures, such
as the KL distance1 , the Chernoff distance, or the Bhattacharyya/Hellinger distance, to name a few
[11, 7, 8, 18]. Furthermore, the Bhattacharyya afﬁnity is indeed a positive deﬁnite kernel function
on the space of distributions and have nice closed-form expressions for the exponential family [7].
1 by distance we mean any nonnegative measure of similarity and not necessarily a metric.

1

In this paper, we investigate the relationship between the Grassmann kernels and the probabilis-
tic distances. The link is provided by the probabilistic generalization of subspaces with a Factor
Analyzer which is a Gaussian ‘blob’ that has nonzero volume along all dimensions.
Firstly, we show that the KL distance yields the Projection kernel on the Grassmann manifold in the
limit of zero noise, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal
for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose an
extension of the Projection kernel which is originally conﬁned to the set of linear subspaces, to the
set of afﬁne as well as scaled subspaces.
We will demonstrate the extended kernels with the Support Vector Machines and the Kernel Dis-
criminant Analysis using synthetic and real image databases. The proposed kernels show the better
performance compared to the previously used kernels such as Binet-Cauchy and the Bhattacharyya
kernels.

2 Probabilistic subspace distances and kernels

2.1 KL distance in the limit

In this section we will consider the two well-known probabilistic distances, the KL distance and the
Bhattacharyya distance, and establish their relationships to the Grassmann kernels. Although these
probabilistic distances are not restricted to speciﬁc distributions, we will model the data distribution
as the Mixture of Factor Analyzers (MFA) [4]. If we have i = 1, ..., N sets in the data, then each set
is considered as i.i.d. samples from the i-th Factor Analyzer
x ∼ pi (x) = N (ui , Ci ), Ci = YiY 0
i + σ2 ID ,
(1)
where ui ∈ RD is the mean, Yi is a full-rank D × m matrix (D > m), and σ is the ambient noise
level. The FA model is a practical substitute for a Gaussian distribution in case the dimensionality D
of the images is greater than the number of samples n in a set. Otherwise it is impossible to estimate
the full covariance C nor invert it.
More importantly, we use the FA distribution to provide the link between the Grassmann manifold
and the space of probabilistic distributions. In fact a linear subspace can be considered as the ‘ﬂat-
tened’ (σ → 0) limit of a zero-mean (ui = 0), homogeneous (Y 0
i Yi = Im ) FA distribution. We will
look at the limits of the KL distance and the Bhattacharyya kernel under this condition.
Z
The (symmetrized) KL distance is deﬁned as
[p1 (x) − p2 (x)] log p1 (x)
JKL (p1 , p2 ) =
p2 (x) dx.
i be the covariance matrix, and deﬁne eYi = Yi (σ2 I + Y 0
eZ = 2−1/2 [ eY1 eY2 ]. In this case the KL distance is
i Yi )−1/2 , and
Let Ci = σ2 I + YiY 0
tr(− eY 0
2 Y2 − eY 0
2 eY2 ) + σ−2
1 eY1 − eY 0
2 eY1 − eY 0
1 eY2 )
1
(u1 − u2 )0 (cid:16)
(cid:17)
tr(Y 0
1 Y1 + Y 0
1 Y2Y 0
2 Y1Y 0
JKL (p1 , p2 ) =
2ID − eY1 eY 0
1 − eY2 eY 0
2
2
+ σ−2
(u1 − u2 ).
2
2
(cid:19)
(cid:18)
Under the subspace condition (σ → 0, ui = 0, Y 0
i Yi = Im , i = 1, ..., N ), the KL distance simpliﬁes
to
) + σ−2
1
(−2 m
tr(Y 0
1 Y2Y 0
2 Y1 )
2
σ2 + 1
σ2 + 1
1
(2m − 2tr(Y 0
1 Y2Y 0
2 Y1 ))
2σ2 (σ2 + 1)
We can ignore the multiplying factors which do not depend on Y1 or Y2 , and rewrite the distance as
JKL (p1 , p2 ) = 2m − 2tr(Y 0
1 Y2Y 0
2 Y1 ).
We immediately realize that the distance JKL (p1 , p2 ) coincides with the deﬁnition of the squared
Projection distance [2, 16, 6], with the corresponding Projection kernel
kProj (Y1 , Y2 ) = tr(Y 0
1 Y2Y 0
2 Y1 ).

JKL (p1 , p2 ) =

1
2

=

2m − 2

(2)

(3)

(4)

2

2.2 Bhattacharyya kernel in the limit

Z
Jebara and Kondor [7, 8] proposed the Probability Product kernel

kProb (p1 , p2 ) =

[p1 (x) p2 (x)]α dx (α > 0)

(5)

∝ det

Im −

which includes the Bhattacharyya kernel as a special case.
Under the subspace condition (σ → 0, ui = 0, Y 0
i Yi = Im , i = 1, ..., N ) the kernel kProb becomes
(σ2 + 1)αm det(I2m − eY 0 eY )−1/2
kProb (p1 , p2 ) = π (1−2α)D 2−αD α−D/2 σ2α(m−D)+D
(cid:18)
(cid:19)−1/2
1
1 Y2Y 0
(2σ2 + 1)2 Y 0
.
2 Y1
Suppose the two subspaces span(Y1 ) and span(Y2 ) intersect only at the origin, that is, the singular
1 Y2 are strictly less than 1. In this case kProb has a ﬁnite value as σ → 0 and the inversion
values of Y 0
(cid:18)
(cid:18) (2σ2 + 1)2
(cid:19)−1/2
(cid:19)m/2
of (7) is well-deﬁned. In contrast, the diagonal terms of kProb become
1
(1 −
kProb (Y1 , Y1 ) = det
=
(2σ2 + 1)2 )Im
(8)
,
4σ2 (σ2 + 1)
which diverges to inﬁnity as σ → 0. This implies that after normalizing the kernel by the diagonal
(cid:26) 1,
terms, the resulting kernel becomes a trivial kernel
span(Yi ) = span(Yj )
0,
otherwise
The derivations are detailed in the thesis [5]. As we claimed earlier, the Probability Product kernel,
including the Bhattacharyya kernel, loses its discriminating power as the distributions become close
to subspaces.

˜kProb (Yi , Yj ) =

, as σ → 0.

(7)

(6)

(9)

3 Extended Projection Kernel

Based on the analysis of the previous section, we will extend the Projection kernel (4) to more
general spaces than the Grassmann manifold in this section. We will examine the two directions of
extension: from linear to afﬁne, and from homogeneous to scaled subspaces.

3.1 Extension to afﬁne subspaces
An afﬁne subspace in RD is a linear subspace with an ‘offset’ . In that sense a linear subspace is
simply an afﬁne subspace with a zero offset. Analogously to the (linear) Grassmann manifold, we
can deﬁne an afﬁne Grassmann manifold as the set of all m-dim afﬁne subspaces in RD space 2 .
The afﬁne span is deﬁned from the orthonormal basis Y ∈ RD×m and an offset u ∈ RD by
aspan(Y , u) , {x | x = Y v + u, ∀v ∈ Rm}.
(10)
By deﬁnition, the representation of an afﬁne space by (Y , u) is not unique and there is an invariant
condition for the equivalent of representations:
Deﬁnition 1 (invariance to representations).
aspan(Y1 , u1 ) = aspan(Y2 , u2 ) if and only if span(Y1 ) = span(Y2 ) and Y ⊥
1 (Y ⊥
1 )0u1 = Y ⊥
2 (Y ⊥
2 )0u2 ,
where Y ⊥ is any orthonormal basis for the orthogonal complement of span(Y ).

Similarly to the deﬁnition of Grassmann kernels [6], we can now formally deﬁne the afﬁne Grass-
mann kernel as follows. Let k : (Rm×D × RD ) × (Rm×D × RD ) → R be a real valued symmetric
function k(Y1 , u1 , Y2 , u2 ) = k(Y2 , u2 , Y1 , u1 ).
2The Grassmann manifold is deﬁned as a quotient space O(D)/O(m) × O(D − m) where O is the orthog-
onal group. The afﬁne Grassmann manifold is similarly deﬁned as E(D)/E(m) × O(D − m), where E is the
Euclidean group. Fore more explanations, please refer to [5].

3

Deﬁnition 2. A real valued symmetric function k is an afﬁne Grassmann kernel if it is positive
deﬁnite and invariant to different representations:

k(Y1 , u1 , Y2 , u2 ) = k(Y3 , u3 , Y4 , u4 )
for any Y1 , Y2 , Y3 , Y4 , and u1 , u2 , u3 , u4
aspan(Y1 , u1 ) = aspan(Y3 , u3 ) and aspan(Y2 , u2 ) = aspan(Y4 , u4 ).

such that

With this deﬁnition we check if the KL distance in the limit suggests an afﬁne Grassmann kernel.
The KL distance with the homogeneity condition only Y 0
1 Y1 = Y 0
2 Y2 = Im becomes,
JKL (p1 , p2 ) → 1
1 − Y2Y 0
2 Y1 ) + (u1 − u2 )0 (2ID − Y1Y 0
2σ2 [2m − 2tr(Y 0
2 ) (u1 − u2 )] .
1 Y2Y 0
Ignoring the multiplicative factor, the ﬁrst term is the same is the original Projection kernel, which
we will denote as the ‘linear’ kernel to emphasize the underlying assumption:
1 Y2Y 0
kLin (Y1 , Y2 ) = tr(Y1Y 0
2 ),
The second term give rise to a new ‘kernel’
1 − Y2Y 0
1 (2ID − Y1Y 0
ku (Y1 , u1 , Y2 , u2 ) = u0
2 )u2 ,
(12)
which measures the similarity of the offsets u1 and u2 scaled by 2I − Y1Y 0
1 − Y2Y 0
2 . However, this
term is not invariant under the invariance condition unfortunately. We instead propose the slight
modiﬁcation:
1 )(I − Y2Y 0
1 (I − Y1Y 0
k(Y1 , u1 , Y2 , u2 ) = u0
2 )u2
The proof of the proposed form being invariant and positive deﬁnite is straightforward and is omit-
ted. Combined with the linear term kLin , this deﬁnes the new ‘afﬁne’ kernel
1 )(I − Y2Y 0
1 (I − Y1Y 0
kAff (Y1 , u1 , Y2 , u2 ) = tr(Y1Y 0
1 Y2Y 0
2 ) + u0
2 )u2 .
As we can see, the KL distance with only the homogeneity condition has two terms related to the
subspace Y and the offset u. This suggests a general construction rule for afﬁne kernels. If we have
two separate positive kernels for subspaces and for offsets, we can add or multiply them together to
construct new kernels [10].

(11)

(13)

3.2 Extension to scaled subspaces

We have assumed homogeneous subspace so far. However, if the subspaces are computed from
the PCA of real data, the eigenvalues in general will have non-homogeneous values. To incorpo-
rate these scales for afﬁne subspaces, we now allow the Y to be non-orthonormal and check if the
Let Yi be a full-rank D × m matrix, and bYi = Yi (Y 0
resultant kernel is still valid.
i Yi )−1/2 be the orthonormalization of Yi .
Ignoring the multiplicative factors, the limiting (σ → 0) ‘kernel’ from (3) becomes
tr( bY1 bY 0
1 bY2 bY 0
1 (2I − bY1 bY 0
1 − bY2 bY 0
1
1 Y2Y 0
2 + Y1Y 0
2 ) + u0
k =
2 )u2 ,
2
which is again not well-deﬁned.
1 (I − bY1 bY 0
1 )(I − bY2 bY 0
The second term is the same as (12) in the previous subsection, and can be modiﬁed in the same way
to ku = u0
2 )u2 .
The ﬁrst term is not positive deﬁnite, and there are several ways to remedy it. We propose to use the
tr(Y1 bY 0
1 bY2Y 0
2 + bY1Y 0
1 Y2 bY 0
2 ) = tr( bY 0
1 bY2Y 0
following form
1
2 Y1 ),
2
among other possibilities.
1 )(I − bY2 bY 0
1 bY2Y 0
1 (I − bY1 bY 0
kAffSc (Y1 , u1 , Y2 , u2 ) = tr(Y1 bY 0
The sum of the two modiﬁed terms, is the proposed ‘afﬁne scaled’ kernel:
2 ) + u0
2 )u2 .
This is a positive deﬁnite kernel which can be shown from the deﬁnition.

k(Y1 , Y2 ) =

(14)

4

Summary of the extended Projection kernels
bYi = Yi (Y 0
The proposed kernels are summarized below. Let Yi be a full-rank D × m matrix, and let
i Yi )−1/2 the orthonormalization of Yi as before.
1 bY2Y 0
kLinSc (Y1 , Y2 ) = tr( bY 0
1 bY2 bY 0
kLin (Y1 , Y2 ) = tr( bY 0
2 bY1 ),
1 )(I − bY2 bY 0
kAff (Y1 , Y2 ) = tr( bY 0
1 bY2 bY 0
2 bY1 ) + u0
1 (I − bY1 bY 0
2 Y1 )
kAffSc (Y1 , Y2 ) = tr( bY 0
1 bY2Y 0
1 (I − bY1 bY 0
1 )(I − bY2 bY 0
2 )u2
2 Y1 ) + u0
2 )u2
ek(Y1 , u1 , Y2 , u2 ) = k(Y1 , u1 , Y2 , u2 ) k(Y1 , u1 , Y1 , u1 )−1/2 k(Y2 , u2 , Y2 , u2 )−1/2
We also spherize the kernels
There is a caveat in implementing these kernels. Although we used the same notations Y and bY for
so that k(Y1 , u1 , Y1 , u1 ) = 1 for any Y1 and u1 .
both linear and afﬁne kernels, they are different in computation. For linear kernels the Y and bY are
computed from data assuming u = 0, whereas for afﬁne kernels the Y and bY are computed after
removing the estimated mean u from the data.

(15)

3.3 Extension to nonlinear subspaces

A systematic way of extending the Projection kernel from linear/afﬁne subspaces to nonlinear spaces
is to use an implicit map via a kernel function, where the latter kernel is to be distinguished from the
former kernels. Note that the proposed kernels (15) can be computed only from the inner products
of the column vectors of Y ’s and u’s including the orthonormalization procedure. If we replace the
inner products of those vectors y 0
i yi by a positive deﬁnite function f (yi , yj ) on Euclidean spaces,
this implicitly deﬁnes a nonlinear feature space. This ‘doubly kernel’ approach has already been
proposed for the Binet-Cauchy kernel [17, 8] and for probabilistic distances in general [18]. We
can adopt the trick for the extended Projection kernels as well to extend the kernels to operate on
‘nonlinear subspaces’3 .

4 Experiments with synthetic data

In this section we demonstrate the application of the extended Projection kernels to two-class clas-
siﬁcation problems with Support Vector Machines (SVMs).

4.1 Synthetic data

The extended kernels are deﬁned under different assumptions of data distribution. To test the kernels
we generate three types of data – ‘easy’, ‘intermediate’ and ‘difﬁcult’ – from MFA distribution,
which cover the different ranges of data distribution.
A total of N = 100 FA distributions are generated in D = 10 dimensional space. The parameters
of each FA distribution pi (x) = N (ui , Ci ) are randomly chosen such that
• ‘Easy’ data have well separarted means ui and homogeneous scales Y 0
i Yi
• ‘Intermediate’ data have partially overlapping means ui and homogeneous scales Y 0
i Yi
• ‘Difﬁcult’ data have totally overlapping means (u1 = ... = uN = 0) and randomly chosen
scales between 0 and 1.

The class label for each distribution pi is assigned as follows. We choose a pair of distribution p+
and p− which are the farthest apart from each other among all pairs of distributions. Then the labels
of the remaining distributions pi are determined from whether they are close to p+ or p− . The
distances are measured by the KL distance JKL .

3 the preimage corresponding to the linear subspaces in the RKHS via the feature map

5

4.2 Algorithms and results

We compare the performance of the Euclidean SVM with linear/ polynomial/ RBF kernels and the
performance of SVM with Grassmann kernels. To test the original SVMs, we randomly sampled
n = 50 point from each FA distribution pi (x). We evaluate the algorithm with N -fold cross valida-
tion by holding out one set and training with the other N − 1 sets. The polynomial kernel used is
k(x1 , x2 ) = (hx1 , x2 i + 1)3 .
To test the Grassmann SVM, we ﬁrst estimated the mean ui and the basis Yi from n = 50 points of
each FA distribution pi (x) used for the original SVM. The Yi , µi and σ are estimated simply from
the probabilistic PCA [12], although they can also be estimated by the Expectation Maximization
approach.
the original and the extended Pro-
1)
Six different Grassmann kernels are compared:
kBhat (p1 , p2 ) = R [p1 (x) p2 (x)]1/2 dx adapted for FA distributions. We evaluate the algo-
(Linear, Linear Scaled, Afﬁne, Afﬁne Scaled),
jection kernels
2)
the Binet-Cauchy
1 Y2Y 0
1 Y2 )2 = det Y 0
kBC (Y1 , Y2 ) = (det Y 0
kernel
kernel
the Bhattacharyya
3)
and
2 Y1 ,
rithms with leave-one-out test by holding out one subspace and training with the other N − 1
subspaces.

Table 1: Classiﬁcation rates of the Euclidean SVMs and the Grassmann SVMs. The BC and Bhat
are short for Binet-Cauchy and Bhattacharyya kernels, respectively.

Euclidean
Poly
Linear
79.85
84.63
62.40
61.76
63.74
52.00

Grassmann
Aff
Linear Lin Sc
92.70
55.30
55.10
68.10
67.50
85.20
80.30
81.00
80.10

Aff Sc
92.30
83.60
81.20

BC
54.70
60.90
68.90

Probabilistic
Bhat
46.10
59.00
77.30

Easy
Intermediate
Difﬁcult

Table 1 shows the classiﬁcation rates of the Euclidean SVMs and the Grassmann SVMs, averaged
for 10 trials. The results shows that best rates are obtained from the extended kernels, and the
Euclidean kernels lag behind for all three types of data. Interestingly the polynomial kernels often
perform worse than the linear kernels, and the RBF kernel performed even worse which we do not
report. For the ‘difﬁcult’ data where the means are zero, the linear SVMs degrade to the chance-
level (50%), which agrees with the intuitive picture that any decision hyperplane that passes the
origin will roughly halve the points from a zero-mean distribution. As expected, the linear kernel
is inappropriate for data with nonzero offsets (‘easy’ and ‘intermediate’), whereas the afﬁne kernel
performs well regardless of the offsets. However, there is no signiﬁcant difference between the
homogeneous and the scaled kernels. The Binet-Cauchy and the Bhattacharyya kernels mostly
underperformed.
We conclude that under certain conditions the extended kernels have clear advantages over the orig-
inal linear kernels and the Euclidean kernels for the subspace-based classiﬁcation problem.

5 Experiments with real-world data

In this section we demonstrate the application of the extended Projection kernels to recognition
problems with the kernel Fisher Discriminant Analysis [10].

5.1 Databases

The Yale face database and the Extended Yale face database [3] together consist of pictures of 38
subjects with 9 different poses and 45 different lighting conditions. The ETH-80 [9] is an object
database designed for object categorization test under varying poses. The database consists of pic-
tures of 8 object categories and 10 object instances for each category, recored under 41 different
poses.

6

These databases have naturally factorized structures which make them ideal to test subspace-based
learning algorithms with. In Yale Face database, a set consists of images of all illumination con-
ditions a person at a ﬁxed pose. By treating the set as a point in the Grassmann manifold, we can
perform illumination-invarint learning tasks with the data. For ETH-80 database, a set consists of
images of all possible poses of an object from a category. Also by treating such set as a point in the
Grassmann manifold, we can perform pose-invariant learning tasks with the data.
There are a total of N = 279 and 80 sets as described above respectively. The images are resized
to the dimension of D = 504 and 1024 respectively, and the maximum of m = 9 dimensional
subspaces are used to compute the kernels. The subspace parameters Yi , ui and σ are estimated
from the probabilistic PCA [12].

5.2 Algorithms and results

We perform subject recognition tests with Yale Face, and categorization tests with ETH-80 database.
Since these databases are highly multiclass (31 and 8 classes) relative to the total number of sam-
ples, we use the kernel Discriminant Analysis to reduce dimensionality and extract features, in con-
junction with a 1-NN classiﬁer. The six different Grassmann kernels are compared: the extended
Projection (Lin/LinSc/Aff/Affsc) kernels, the Binet-Cauchy kernel, and the Bhattacharyya kernel.
The baseline algorithm (Eucl) is the Linear Discriminant Analysis applied to the original images in
the data from which the subspaces are computed.
Figure 1 summarizes the average recognition/categoriazation rates from 9- and 10-fold cross vali-
dation with the Yale Face and ETH-80 databases respectively. The results shows that best rates are
achieved from the extended kernels: linear scaled kernel in Yale Face and the afﬁne kernel in ETH-
80. However the difference within the extended kernels are small. The performance of the extended
kernels remain relatively unaffected by the subspace dimensionality, which is a convenient property
in practice since we do not know the true dimensionality a priori. However the Binet-Cauchy and the
Bhattacharyya kernels do not perform as well, and degrade fast as the subspace dimension increases.
The analysis of the poor performance are given in the thesis [5].

6 Conclusion

In this paper we analyzed the relationship between probabilistic distances and the geometric Grass-
mann kernels, especially the KL distance and the Projection kernel. This analysis help us to under-
stand the limitations of the Bhattacharyya kernel for subspace-based problems, and also suggest the
extensions of the Projection kernel. With synthetic and real data we demonstrated that the extended
kernels can outperform the original Projection kernel, as well as the previously used Bhattacharyya
and the Binet-Cauchy kernels for subspace-based classiﬁcation problems. The relationship between
other probabilistic distances and the Grassmann kernels is yet to be fully explored, and we expect to
see more results from a follow-up study.

References
[1] Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and Stefano Soatto. Dynamic textures. Int. J.
Comput. Vision, 51(2):91–109, 2003.
[2] Alan Edelman, Tom ´as A. Arias, and Steven T. Smith. The geometry of algorithms with orthogonality
constraints. SIAM J. Matrix Anal. Appl., 20(2):303–353, 1999.
[3] Athinodoros S. Georghiades, Peter N. Belhumeur, and David J. Kriegman. From few to many: Illumina-
tion cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach.
Intell., 23(6):643–660, 2001.
[4] Zoubin Ghahramani and Geoffrey E. Hinton. The EM algorithm for mixtures of factor analyzers. Tech-
nical Report CRG-TR-96-1, Department of Computer Science, University of Toronto, 21 1996.
Subspace-based Learning with Grassmann Manifolds.
[5] Jihun Hamm.
Ph.D thesis
Electrical
Available
2008.
and Systems Engineering, University of Pennsylvania,
http://www.seas.upenn.edu/ jhham/Papers/thesis-jh.pdf.
[6] Jihun Hamm and Daniel Lee. Grassmann discriminant analysis: a unifying view on subspace-based
learning. In Int. Conf. Mach. Learning, 2008.

in
at

7

Figure 1: Comparison of Grassmann kernels for face recognition/ object categorization tasks with
kernel discriminant analysis. The extended Projection kernels (Lin/LinSc/Aff/ AffSc) outperform
the baseline method (Eucl) and the Binet-Cauchy (BC) and the Bhattacharyya (Bhat) kernels.

[7] Tony Jebara and Risi Imre Kondor. Bhattacharyya expected likelihood kernels. In COLT, pages 57–71,
2003.
[8] Risi Imre Kondor and Tony Jebara. A kernel between sets of vectors. In Proc. of the 20th Int. Conf. on
Mach. Learn., pages 361–368, 2003.
[9] Bastian Leibe and Bernt Schiele. Analyzing appearance and contour based methods for object catego-
rization. CVPR, 02:409, 2003.
[10] Bernhard Sch ¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regular-
ization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001.
[11] Gregory Shakhnarovich, John W. Fisher, and Trevor Darrell. Face recognition from long-term observa-
tions. In Proc. of the 7th Euro. Conf. on Computer Vision, pages 851–868, London, UK, 2002.
[12] Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal Of
The Royal Statistical Society Series B, 61(3):611–622, 1999.
[13] Pavan Turaga, Ashok Veeraraghavan, and Rama Chellappa. Statistical analysis on Stiefel and Grassmann
manifolds with applications in computer vision. In CVPR, 2008.
[14] Ashok Veeraraghavan, Amit K. Roy-Chowdhury, and Rama Chellappa. Matching shape sequences
IEEE Trans. Pattern Anal. Mach. Intell.,
in video with applications in human movement analysis.
27(12):1896–1909, 2005.
[15] S.V.N. Vishwanathan and Alexander J. Smola. Binet-Cauchy kernels. In NIPS, 2004.
[16] Liwei Wang, Xiao Wang, and Jufu Feng. Subspace distance analysis with application to adaptive bayesian
algorithm for face recognition. Pattern Recogn., 39(3):456–464, 2006.
[17] Lior Wolf and Amnon Shashua. Learning over sets using kernel principal angles. J. Mach. Learn. Res.,
4:913–931, 2003.
[18] Shaohua Kevin Zhou and Rama Chellappa. From sample similarity to ensemble similarity: Probabilis-
IEEE Trans. Pattern Anal. Mach. Intell.,
tic distance measures in Reproducing Kernel Hilbert Space.
28(6):917–929, 2006.

8

13579405060708090100Yale Facesubspace dimension (m)rate (%)  EuclLinLinScAffAffScBCBhat13579405060708090100ETH!80subspace dimension (m)rate (%)  EuclLinLinScAffAffScBCBhat
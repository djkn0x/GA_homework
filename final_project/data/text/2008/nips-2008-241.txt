Posterior Consistency of the Silverman g -prior in
Bayesian Model Choice

Zhihua Zhang
School of Computer Science & Technology
Zhejiang University, Hangzhou, China

Michael I. Jordan
Departments of EECS and Statistics
University of California, Berkeley, CA, USA

Dit-Yan Yeung
Department of Computer Science & Engineering
HKUST, Hong Kong, China

Abstract

Kernel supervised learning methods can be uniﬁed by utilizing the tools from
regularization theory. The duality between regularization and prior leads to inter-
preting regularization methods in terms of maximum a posteriori estimation and
has motivated Bayesian interpretations of kernel methods. In this paper we pursue
a Bayesian interpretation of sparsity in the kernel setting by making use of a mix-
ture of a point-mass distribution and prior that we refer to as “Silverman’s g -prior.”
We provide a theoretical analysis of the posterior consistency of a Bayesian model
choice procedure based on this prior. We also establish the asymptotic relationship
between this procedure and the Bayesian information criterion.

1 Introduction
We address a supervised learning problem over a set of training data {xi , yi }n
i=1 where xi ∈ X ⊂ Rp
is a p-dimensional input vector and yi is a univariate response. Using the theory of reproducing
kernels, we seek to ﬁnd a predictive function f (x) from the training data.
Suppose f = u + h ∈ ({1} + HK ) where HK is a reproducing kernel Hilbert space (RKHS). The
(cid:40)
(cid:41)
n(cid:88)
estimation of f (x) is then formulated as a regularization problem of the form
1
L(yi , f (xi )) + g
min
f ∈HK
2
n
i=1
where L(y , f (x)) is a loss function, (cid:107)h(cid:107)2HK
is the RKHS norm and g > 0 is the regularization
n(cid:88)
parameter. By the representer theorem [7], the solution for (1) is of the form
(cid:80)n
βj K (x, xj ),
f (x) = u +
j=1
where K (·, ·) is the kernel function. Noticing that (cid:107)h(cid:107)2HK
(cid:190)
(cid:189)
i,j=1 K (xi , xj )βiβj and substituting
=
n(cid:88)
(2) into (1), we obtain the minimization problem with respect to (w.r.t.) the βi as
1
(cid:48)Kβ
L(yi , f (xi )) + g
min
2 β
n
u,β
i=1
where K = [K (xi , xj )] is the n×n kernel matrix and β = (β1 , . . . , βn )(cid:48) is the vector of regression
coefﬁcients.

(cid:107)h(cid:107)2HK

(3)

(1)

(2)

,

,

(cid:161)
(cid:162)
(cid:48)Kβ can be captured by assign-
From the Bayesian standpoint, the role of the regularization term g
(cid:161)
(cid:162)
2 β
ing a design-dependent prior Nn (0, g−1K−1 ) to the regression vector β . The prior Nn
0, K−1
(cid:161)
(cid:162)
for β was ﬁrst proposed by [5] in his Bayesian formulation of spline smoothing. Here we refer to the
0, g−1K−1
prior β ∼ Nn
as the Silverman g -prior by analogy to the Zellner g -prior [9]. When
0, g−1K−1
K is singular, by analogy to generalized singular g-prior (gsg-prior) [8], we call Nn
a generalized Silverman g-prior.
Given the high dimensionality generally associated with RKHS methods, sparseness has emerged as
a signiﬁcant theme, particularly when computational concerns are taken into account. For example,
the number of support vectors in support vector machine (SVM) is equal to the number of nonzero
components of β . That is, if βj = 0, the j th input vector is excluded from the basis expansion in
(2); otherwise the j th input vector is a support vector. We are thus interested in a prior for β which
(cid:80)n
allows some components of β to be zero. To specify such a prior we ﬁrst introduce an indicator
vector γ = (γ1 , . . . , γn )(cid:48) such that γj = 1 if xj is a support vector and γj = 0 if it is not. Let
(cid:161)
(cid:162)
j=1 γj be the number of support vectors, let Kγ be the n×nγ submatrix of K consisting of
nγ =
those columns of K for which γj = 1, and let βγ be the corresponding subvector of β . Accordingly,
where Kγ γ is the nγ ×nγ submatrix of Kγ consisting of those rows
0, g−1K−1
we let βγ ∼ Nnγ
γ γ
of Kγ for which γj = 1.
We thus have a Bayesian model choice problem in which a family of models is indexed by an
indicator vector γ . Within the Bayesian framework we can use Bayes factors to choose among these
models [3]. In this paper we provide a frequentist theoretical analysis of this Bayesian procedure.
In particular, motivated by the work of [1] on the consistency of the Zellner g -prior, we investigate
the consistency for model choice of the Silverman g -prior for sparse kernel-based regression.

2 Main Results
Our analysis is based on the following regression model Mγ :
0, σ2 (gγ Kγ γ )−1 (cid:162)
(cid:161)
y = u1n + Kγ βγ + 
 ∼ Nn (0, σ2 In ), βγ |σ ∼ Nnγ
,
where y = (y1 , . . . , yn )(cid:48) . Here and later, 1m denotes the m×1 vector of ones and Im denotes
the m×m identity matrix. We compare each model Mγ with the null model M0 , formulating the
model choice problem via the hypotheses H0 : β = 0 and Hγ : βγ ∈ Rnγ .
Let (cid:101)Kγ = [1n , Kγ ]. The following condition is also assumed:
Throughout this paper, for any nγ , it is always assumed to take a ﬁnite value even though n → ∞.
(cid:101)K(cid:48)
(cid:101)Kγ is positive deﬁnite and
For a ﬁxed nγ < n, 1
converges to a positive deﬁnite matrix as n → ∞.
γ
n
Suppose that the sample y is generated by model Mν with parameter values u, βν and σ . We
formalize the problem of consistency for model choice as follows [1]:
p(Mν |y) = 1 and plim
p(Mγ |y) = 0 for all Mγ (cid:54)= Mν ,
plim
n→∞
n→∞
where “plim” denotes convergence in probability and the limit is taken w.r.t. the sampling distribu-
tion under the true model Mν .

(5)

(4)

(6)

2.1 A Noninformative Prior for (u, σ2 )

We ﬁrst consider the case when (u, σ2 ) is assigned the following noninformative prior:
(u, σ2 ) ∝ 1/σ2 .
(7)
Moreover, we assume 1(cid:48)
nK = 0. In this case, we have 1(cid:48)
nKγ = 0 so that the intercept u may be
regarded as a common parameter for both Mγ and M0 .
After some calculations the marginal likelihood is found to be
Γ( n−1
2 )
2 (1 − F 2
(cid:107)y − ¯y1n (cid:107)−n+1 |Qγ |− 1
p(y|Mγ ) =
γ )− n−1
√
2 ,
n−1
π
n
2

(8)

where ¯y = 1
n

(cid:80)n
γ γ K(cid:48)
−1Kγ K−1
i=1 yi , Qγ = In + gγ
γ and
y(cid:48)Kγ (gγ Kγ γ + K(cid:48)
γ Kγ )−1K(cid:48)
γ y
γ =
F 2
(cid:107)y − ¯y1n(cid:107)2
.
γ )(cid:107)y − ¯y1n(cid:107)2 be the residual sum of squares. Here,
Let RSSγ = (1 − R2
(y − ¯y1n )(cid:48)Kγ (K(cid:48)
γ (y − ¯y1n )
y(cid:48)Kγ (K(cid:48)
γ Kγ )−1K(cid:48)
γ Kγ )−1K(cid:48)
γ y
=
γ =
R2
(cid:107)y − ¯y1n(cid:107)2
(cid:107)y − ¯y1n(cid:107)2
.
and RSSγ = y(cid:48) (In − (cid:101)Hγ )y where (cid:101)Hγ = (cid:101)Kγ ( (cid:101)K(cid:48)
(cid:101)Kγ )−1 (cid:101)K(cid:48)
γ and plimgγ →0 (1 − F 2
γ )(cid:107)y − ¯y1n(cid:107)2 = RSSγ ,
γ = R2
It is easily proven that for ﬁxed n, plimgγ →0 F 2
γ . As a special case of (8), it is also
γ
immediate to obtain the marginal distribution of the null model as
Γ( n−1
2 )
(cid:107)y − ¯y1n(cid:107)−n+1 .
p(y|M0 ) =
√
n−1
π
n
2
Then the Bayes factor for Mγ versus M0 is
BFγ 0 = |Qγ |− 1
2 (1 − F 2
γ )− n−1
2 .
In the limiting case when gγ → 0 and both n and nγ are ﬁxed, BFγ 0 tends to 0. This implies that a
large spread of the prior forces the Bayes factor to favor the null model. Thus, as in the case of the
Zellner g -prior [4], Bartlett’s paradox arises for the Silverman g -prior.
The Bayes factor for Mγ versus Mκ is given by
(1 − F 2
γ )− n−1
|Qγ |− 1
BFγ 0
2
2
|Qκ |− 1
(1 − F 2
κ )− n−1
BFκ0
2
2
Based on the Bayes factor, we now explore the consistency of the Silverman g -prior. Suppose that
the sample y is generated by model Mν with parameter values u, βν and σ2 . Then the consistency
property (6) is equivalent to

(9)

BFγκ =

=

.

for all Mγ (cid:54)= Mν .
plim
BFγ ν = 0,
n→∞
Assume that under any model Mγ that does not contain Mν , i.e, Mγ (cid:43) Mν ,
(cid:101)βγ
(cid:101)β
ν (In − (cid:101)Hγ ) (cid:101)Kν
(cid:101)K(cid:48)
(cid:48)
= cγ ∈ (0, ∞),
where (cid:101)β
γ ). Note that In − (cid:101)Hγ is a symmetric idempotent matrix which projects onto
γ
lim
(10)
n→∞
n
the subspace of Rn orthogonal to the span of (cid:101)Kγ . Given that (In − (cid:101)Hγ )1n = 0 and 1(cid:48)
(cid:48)
(cid:48)
γ = (u, β
nKν = 0,
condition (10) reduces to
ν (In − Hγ )Kν βν
(cid:48)
ν K(cid:48)
= cγ ∈ (0, ∞),
β
lim
n→∞
n
where Hγ = Kγ (K(cid:48)
γ Kγ )−1K(cid:48)
γ . We now have the following theorem whose proof is given in
Sec. 3.

Theorem 1 Consider the regression model (4) with the noninformative prior for (u, σ2 ) in (7).
Assume that conditions (5) and (10) are satisﬁed and assume that gγ can be written in the form
w (cid:48)
gγ = w1 (nγ )
2 (n)
n→∞ w2 (n) = ∞ and
lim
lim
= 0
with
n→∞
w2 (n)
w2 (n)
for particular choices of functions w1 and w2 , where w2 is differentiable and w (cid:48)
2 (n) is the ﬁrst
derivative w.r.t. n. When the true model Mν is not the null model, i.e., Mν (cid:54)= M0 , the posterior
probabilities are consistent for model choice.

(11)

Theorem 1 can provide an empirical methodology for setting g . For example, it is clear that g = 1/n
where w1 (nγ ) = 1 and w2 (n) = n satisﬁes condition (11).
It is interesting to consider the (asymptotic) relationship between the Bayes factor and Bayesian
information (or Schwartz) criterion (BIC) in our setting. Given two models Mγ and Mκ , the
difference between the BICs of these two models is given by
+ nκ − nγ
RSSκ
Sγκ = n
2
RSSγ
2
We thus obtain the following asymptotic relationship (the proof is given in Sec. 3):

ln(n).

ln

Theorem 2 Under the regression model and the conditions in Theorem 1, we have
ln BFγ ν
plim
Sγ ν + nν −nγ
n→∞
ln w2 (n)
2
Furthermore, if Mν is not nested within Mγ , then plimn→∞ ln BFγν
Sγν
limits are taken w.r.t. the model Mν .

= 1.

= 1. Here the probability

2.2 A Natural Conjugate Prior for (u, σ2 )

(12)

(13)

In this section, we analyze consistency for model choice under a different prior for (u, σ2 ), namely
the standard conjugate prior:
p(u, σ2 ) = N (u|0, σ2η−1 )Ga (σ−2 |aσ /2, bσ /2)
where Ga (u|a, b) is the Gamma distribution:
p(u) = ba
Γ(a) ua−1 exp(−bu), a > 0, b > 0.
(cid:183)
(cid:184)
(cid:101)βγ ∼ Nnγ +1 (0, σ2Σ−1
We further assume that u and βγ are independent. Then
0
η
γ ) with Σγ =
.
0 gγ Kγγ
(cid:164)− aσ +n
(cid:163)
The marginal likelihood of model Mγ is thus
baσ /2
Γ( n+aσ
)
|Mγ |− 1
p(y|Mγ ) =
bσ + y(cid:48)M−1
σ
(cid:101)K(cid:48)
where Mγ = In + (cid:101)Kγ Σ−1
γ y
2
2
,
2
πn/2Γ( aσ
2 )
(cid:184) 1
(cid:183) |Mκ |
(cid:184) aσ +n
(cid:183)
γ . The Bayes factor for Mγ versus Mκ is given by
γ
bσ + y(cid:48)M−1
κ y
2
2
(cid:101)K(cid:48)
γ = In − (cid:101)Kγ Θ−1
BFγκ =
|Mγ |
.
bσ + y(cid:48)M−1
γ y
Θγ = (cid:101)K(cid:48)
(cid:101)Kγ + Σγ , we have
−nγ
|Kγ γ |−1 |Θγ | where
γ and |Mγ | = |Θγ ||Σγ |−1 = η−1 g
Because M−1
γ
γ
bσ + y(cid:48) (cid:161)
(cid:162)
(cid:184) aσ +n
(cid:183)
(cid:184) 1
(cid:183) |Kγ γ ||Θκ |
(cid:101)K(cid:48)
In− (cid:101)KκΘ−1
γ
bσ + y(cid:48) (cid:161)
(cid:162)
(cid:101)K(cid:48)
In− (cid:101)Kγ Θ−1
BFγκ = gnγ /2
y
2
2
γ
κ
κ
|Kκκ ||Θγ |
gnκ /2
y
κ
γ
γ
Theorem 3 Consider the regression model (4) with the conjugate prior for (u, σ2 ) in (12). Assume
that conditions (5) and (10) are satisﬁed and that gγ takes the form in (11) with w1 (nγ ) being a
decreasing function. When the true model Mν is not the null model, i.e., Mν (cid:54)= M0 , the posterior
probabilities are consistent for model choice.

(14)

.

Note the difference between Theorem 1 and Theorem 3: in the latter theorem w1 (nγ ) is required
to be a decreasing function of nγ . Thanks to the fact that gγ = w1 (nγ )/w2 (n), such a condition
is equivalent to assuming that gγ is a decreasing function of nγ . Again, gγ = 1/n satisﬁes these
conditions. Similarly with Theorem 2, we also have

Theorem 4 Under the regression model and the conditions in Theorem 3, we have
ln BFγ ν
plim
= 1.
Sγ ν + nν −nγ
n→∞
ln w2 (n)
2
Furthermore, if Mν is not nested within Mγ , then plimn→∞ ln BFγν
Sγν
limits are taken w.r.t. the model Mν .

= 1. Here the probability

3 Proofs

(cid:183)
(cid:184)
In order to prove these theorems, we ﬁrst give the following lemmas.
A11 A12
Lemma 1 Let A =
be symmetric and positive deﬁnite, and let B =
A21 A22
have the same size as A. Then A−1 − B is positive semideﬁnite.
(cid:184)
(cid:183)
(cid:184) (cid:183)
(cid:184) (cid:183)
Proof The proof follows readily once we express A−1 and B as
(cid:184) (cid:183)
(cid:183)
(cid:184) (cid:183)
A−1
I −A−1
0
11 A12
A−1 =
11
0 A−1
I
0
22·1
I −A−1
A−1
0
11 A12
B =
11
0
I
0
0
where A22·1 = A22 − A21A−1
11 A12 is also positive deﬁnite.

I
−A21A−1
11
0
I
−A21A−1
I
11

(cid:184)

,

(cid:183)

(cid:184)

A−1
11
0

0
0

0
I

,

The following two lemmas were presented by [1].
Lemma 2 Under the sampling model Mν : (i) if Mν is nested within or equal to a model Mγ , i.e.,
Mν (cid:106) Mγ , then
RSSγ
plim
= σ2
n→∞
n
and (ii) for any model Mγ that does not contain Mν , if (10) satisﬁes, then
RSSγ
= σ2 + cγ .
plim
(cid:180)
(cid:179)
n→∞
n
Lemma 3 Under the sampling model Mν , if Mν is nested within a model Mγ , i.e., Mν ⊂ Mγ ,
d−→ χ2
nγ −nν as n → ∞ where d−→ denotes convergence in distribution.
then n ln
RSSν
RSSγ
Lemma 4 Under the regression model (4), if limn→∞ gγ (n) = 0 and condition (5) is satisﬁed, then
(1 − F 2
γ )(cid:107)y − ¯y1n(cid:107)2 − RSSγ = 0.
plim
n→∞

Proof It is easy to compute
(1 − F 2
γ )(cid:107)y − ¯y1n(cid:107)2 − RSSγ
γ Kγ )−1 − (K(cid:48)
y(cid:48)Kγ [(K(cid:48)
γ Kγ + gγ (n)Kγ γ )−1 ]K(cid:48)
γ y
=
.
σ2
σ2
γ Kγ /n and Kγ γ are positive deﬁnite, there exists an nγ ×nγ nonsingular matrix An
Since both K(cid:48)
and an nγ ×nγ positive diagonal matrix Λnγ such that K(cid:48)
nΛnγ An and Kγγ = A(cid:48)
γ Kγ /n = A(cid:48)
nAn .
Letting z = σ−1 (nΛnγ )−1/2 (A(cid:48)
n )−1K(cid:48)
γ y, we have
z ∼ Nnγ (σ−1 (nΛnγ )1/2Anβ , Inγ )
(cid:164)−1z
(cid:163)
f (z) (cid:44) (1 − F 2
γ )(cid:107)y − ¯y1n (cid:107)2 − RSSγ
nγ(cid:88)
= z(cid:48)z − z(cid:48)nΛnγ
nΛnγ + gγ (n)Inγ
σ2
gγ (n)
nλj (n) + gγ (n) z 2
j .
j=1

and

=

= 0.

= 0 and

gγ (n)
nλj (n) + gγ (n) z 2
j

=
chi-square distribution, χ2 (1, vj ), with vj
Note
a noncentral
follows
that
z 2
j
nλj (n)(aj (n)(cid:48)β)2 /σ2 where λj (n) > 0 is the j th diagonal element of Λnγ and aj (n) is
the j th column of An . We thus have E(z 2
j ) = 1 + vj and Var(z 2
j ) = 2(1 + 2vj ). It follows from
condition (5) that
n→∞ A(cid:48)
n→∞ K(cid:48)
nΛnγ An = A(cid:48)Λγ A,
γ Kγ /n = lim
lim
(cid:180)
(cid:179)
(cid:179)
(cid:180)
where A is nonsingular and Λγ is a diagonal matrix with positive diagonal elements, and both are
independent of n. Hence,
gγ (n)
n→∞ Var
lim
n→∞ E
lim
nλj (n) + gγ (n) z 2
j
We thus have plimn→∞ f (z) = 0. The proof is completed.
κ )y ≥ y(cid:48) (In − (cid:101)Kγ Θ−1
y(cid:48) (In − (cid:101)KκΘ−1
(cid:101)K(cid:48)
(cid:101)K(cid:48)
Lemma 5 Assume that Mκ is nested within Mγ and gγ is a decreasing function of nγ . Then
Proof Since Mκ is nested within Mγ , we express (cid:101)Kγ = [ (cid:101)Kκ , K2 ] without loss of generality. We
γ )y.
(cid:184)
(cid:183)
κ
γ
(cid:35)−1
(cid:34) (cid:101)K(cid:48)
Σ11
γ Σ12
γ is of size nκ×nκ . Hence, we have
now write Σγ =
where Σ11
γ
(cid:101)K(cid:48)
(cid:101)Kκ + Σ11
γ Σ22
Σ21
γ
(cid:101)Kκ + Σ21
(cid:184)
(cid:183)
κK2 + Σ12
Θ−1
γ =
γ
κ
γ
.
K(cid:48)
γ K(cid:48)
(cid:101)Kκ+Σ11
(cid:101)Kκ+Σκ − ( (cid:101)K(cid:48)
Because 0 < gγ ≤ gκ , (cid:101)K(cid:48)
2K2 + Σ22
γ
2
(cid:101)Kκ+Σκ )−1 is positive semideﬁnite. It follows from
(cid:101)Kκ+Σ11
inite. Consequently, ( (cid:101)K(cid:48)
γ )−1 − ( (cid:101)K(cid:48)
0
0
γ ) =
0 (gκ−gγ )Kκκ
is positive semidef-
(cid:183)
(cid:184)
κ
κ
(cid:101)Kκ+Σκ )−1 0
( (cid:101)K(cid:48)
κ
κ
γ −
Lemma 1 that Θ−1
(cid:101)K(cid:48)
(cid:101)K(cid:48)
κ )y − y(cid:48) (In − (cid:101)Kγ Θ−1
y(cid:48) (In − (cid:101)KκΘ−1
is also positive semideﬁnite. We thus have
κ
0
0
(cid:35)−1
(cid:181) (cid:34) (cid:101)K(cid:48)
(cid:184) (cid:182) (cid:101)K(cid:48)
(cid:183)
(cid:101)K(cid:48)
(cid:101)Kκ+Σ11
(cid:101)Kκ+Σκ )−1 0
( (cid:101)K(cid:48)
= y(cid:48) (cid:101)Kγ
γ )y
(cid:101)Kκ+Σ21
κ
γ
κK2+Σ12
−
γ y ≥ 0.
κ
γ
γ
κ
K(cid:48)
γ K(cid:48)
0
0
2K2+Σ22
γ
2

3.1 Proof of Theorem 1

Because

ln BFγ ν =

ln

|Qγ |− 1
2 =

We now prove Theorem 1. Consider that
1
2

|Qγ | + n−1
|Qν |
2
2 |Kγ γ |1/2
nγ
gγ
|gγ Kγ γ + K(cid:48)
γ Kγ |1/2
(cid:175)(cid:175) w1 (nν )
we have
(cid:175)(cid:175) w1 (nγ )
n K(cid:48)
|Qν |
nw2 (n) Kν ν + 1
ν Kν
|Qγ | = ln w1 (nγ )nγ
+ ln
n K(cid:48)
w1 (nν )nν
(cid:175)(cid:175)
(cid:175)(cid:175) w1 (nν )
nw2 (n) Kγ γ + 1
γ Kγ
Because
(cid:175)(cid:175) = lim
(cid:175)(cid:175) w1 (nγ )
n K(cid:48)
nw2 (n) Kν ν + 1
ν Kν
n→∞ ln
n K(cid:48)
nw2 (n) Kγ γ + 1
γ Kγ

|Kγ γ |
|Kν ν | + ln

α = lim
n→∞ ln

ln

ln

(1 − F 2
ν )
(1 − F 2
γ ) .

,

(cid:175)(cid:175)
(cid:175)(cid:175) + (nν −nγ ) ln(nw2 (n)).
| 1
ν Kν |
n K(cid:48)
γ Kγ | ∈ (−∞, ∞),
| 1
n K(cid:48)

it is easily proven that

(cid:40) ∞ nγ < nν
|Qν |
−∞ nγ > nν
lim
|Qγ | =
n→∞
const nγ = nν ,
2 ln |Kγγ |
2 + 1
where const = α
|Kνν | . According to Lemma 4, we also have
(1−F 2
n−1
(1−F 2
n−1
ν )(cid:107)y− ¯y1n(cid:107)2
ν )
γ )(cid:107)y− ¯y1n(cid:107)2 = plim
(1−F 2
(1−F 2
2
γ )
2
n→∞

= plim
n→∞

plim
n→∞

1
2

ln

ln

ln

(15)

n−1
2

ln

RSSν
RSSγ

.

Now consider the following two cases:
(a) Mν is not nested within Mγ :
From Lemma 2, we obtain

.

ln

= ln

RSSν /n
RSSγ /n

σ2
σ2+cγ
(cid:105)

RSSν
= plim
ln
plim
RSSγ
n→∞
n→∞
(cid:179) σ2
(cid:180)
(cid:104)
Moreover, we have the following limit
+ nν −nγ
n−1
= −∞
lim
ln
ln(nw2 (n))
n−1
n→∞
2
σ2+cγ
(cid:179)
(cid:180)
due to limn→∞ nν −nγ
n−1 ln(nw2 (n)) = limn→∞ (nν −nγ ) w2 (n)+nw(cid:48)
2 (n)
= 0 and
nw2 (n)
< 1. This implies that limn→∞ ln BFγ ν = −∞. Thus we obtain
σ2
ln
σ2+cγ
limn→∞ BFγ ν = 0.
(b) Mν is nested within Mγ :
We always have nγ > nν . By Lemma 3, we have (n−1) ln(RSSν /RSSγ )
d−→ χ2
nγ −nν .
d−→ exp(χ2
Hence, (RSSν /RSSγ )(n−1)/2
nγ −nν /2). Combining this result with (15) leads
to a zero limit for BFγ ν .

3.2 Proof of Theorem 2

Using the same notations as those in Theorem 1, we have

=

n ln (1−F 2
γ ) + nν −nγ
n−1
ν )
ln(nw2 (n)) + 2
n Const
(1−F 2
n
+ nν −nγ
ln RSSν
ln(nw2 (n))
RSSγ
n

.

(n−1) ln (1−F 2
γ ) + (nν −nγ ) ln(nw2 (n)) + 2 × Const
ν )
(1−F 2
+ (nν −nγ ) ln(nw2 (n))
n ln RSSν
RSSγ
d−→ χ2
nγ −nν .

= 1

due to nγ > nν and n ln(RSSν /RSSγ )

Cγ ν =

ln BFγ ν
Sγ ν + nν −nγ
ln w2 (n)
2
(a) Mν is not nested within Mγ :
From Lemma 4, we obtain

In this case, we also have

ln BFγ ν
plim
Sγ ν
n→∞
(b) Mν is nested within Mγ :
We obtain

plim
n→∞

Cγ ν = plim
n→∞

plim
n→∞

Cγ ν = lim
n→∞

ln σ2
σ2+cγ
ln σ2
σ2+cγ

+ nν −nγ
n
+ nν −nγ
n

ln(nw2 (n))
ln(nw2 (n))

= 1.

= lim
n→∞

+ nν −nγ
ln σ2
ln(nw2 (n))
σ2+cγ
n
+ nν −nγ
ln σ2
ln n
σ2+cγ
n

= 1.

3.3 Proof of Theorem 3
We now sketch the proof of Theorem 3. For the case that Mν is not nested within Mγ , the proof
is similar to that of Theorem 1. When Mν is nested within Mγ , Lemma 5 shows the following
(cid:162)
y(cid:48) (cid:161)
(cid:162)
bσ + y(cid:48) (cid:161)
(cid:184)
(cid:183)
(cid:184)
(cid:183)
(cid:101)K(cid:48)
In− (cid:101)Kν Θ−1
(cid:101)K(cid:48)
In− (cid:101)Kν Θ−1
relationship
(cid:162)
bσ + y(cid:48) (cid:161)
y(cid:48) (cid:161)
(cid:162)
(cid:101)K(cid:48)
In− (cid:101)Kγ Θ−1
(cid:101)K(cid:48)
In− (cid:101)Kγ Θ−1
y
y
ν
ν
ν
ν
ln
.
y
y
y(cid:48) (cid:161)
bσ + y(cid:48) (cid:161)
(cid:162)
(cid:183)
(cid:184)
(cid:183)
γ
γ
γ
γ
In− (cid:101)Kν Θ−1
In− (cid:101)Kν Θ−1
(cid:101)K(cid:48)
(cid:162)
bσ + y(cid:48) (cid:161)
y(cid:48) (cid:161)
In− (cid:101)Kγ Θ−1
(cid:101)K(cid:48)
In− (cid:101)Kγ Θ−1
y
y(cid:48) (cid:161)
(cid:162)
(cid:183)
(cid:184)
In− (cid:101)Hν
ν
ν
ν
y
y(cid:48) (cid:161)
(cid:162)
In− (cid:101)Hγ
γ
γ
γ
y
y

(cid:162)
(cid:101)K(cid:48)
(cid:162)
(cid:101)K(cid:48)
y
ν
y
γ
∈ (0, ∞).

≤ plim
n→∞

We thus have
aσ+n
2

plim
n→∞

= plim
n→∞

≤ ln

(cid:184)

ln

aσ+n
2

aσ+n
2

ln

ln

From this result the proof follows readily.

4 Conclusions

In this paper we have presented a frequentist analysis of a Bayesian model choice procedure for
sparse regression. We have captured sparsity by a particular choice of prior distribution which we
have referred to as a “Silverman g -prior.” This prior emerges naturally from the RKHS perspective.
It is similar in spirit to the Zellner g -prior, which has been widely used for Bayesian variable selec-
tion and Bayesian model selection due to its computational tractability in the evaluation of marginal
likelihoods [6, 2]. Our analysis provides a theoretical foundation for the Silverman g -prior and
suggests that it can play a similarly wide-ranging role in the development of fully Bayesian kernel
methods.

References
[1] C. Fern ´andez, E. Ley, and M. F. J. Steel. Benchmark priors for Bayesian model averaging.
Journal of Econometrics, 100:381–427, 2001.
[2] E. I. George and R. E. McCulloch. Approaches for Bayesian variable selection. Statistica Sinica,
7:339–374, 1997.
[3] R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association,
90:773–795, 1995.
[4] F. Liang, R. Paulo, G. Molina, M. A. Clyde, and J. O. Berger. Mixtures of g-priors for Bayesian
variable selection. Journal of the American Statistical Association, 103(481):410–423, 2008.
[5] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression
curve ﬁtting (with discussion). Journal of the Royal Statistical Society, B, 47(1):1–52, 1985.
[6] M. Smith and R. Kohn. Nonparametric regression using Bayesian variable selection. Journal of
Econometrics, 75:317–344, 1996.
[7] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, 1990.
[8] M. West. Bayesian factor regression models in the “large p, small n” paradigm.
In J. M.
Bernardo, M. J. Bayarri, J. .O Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and M. West,
editors, Bayesian Statistics 7, pages 723–732. Oxford University Press, 2003.
[9] A. Zellner. On assessing prior distributions and Bayesian regression analysis with g−prior
distributions. In P. K. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques:
Essays in Honor of Bruno de Finetti, pages 233–243. North-Holland, Amsterdam, 1986.


Model Selection in Gaussian Graphical Models:
High-Dimensional Consistency of ℓ1-regularized MLE

Pradeep Ravikumar† , Garvesh Raskutti† , Martin J. Wainwright†∗ and Bin Yu†∗
Department of Statistics† , Department of EECS∗ ,
University of California, Berkeley
{pradeepr,garveshr,wainwright,binyu}@stat.berkeley.edu

Abstract

We consider the problem of estimating the graph structure associated with a Gaussian
Markov random ﬁeld (GMRF) from i.i.d. samples. We study the p erformance of study the
performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional
setting, where the number of nodes in the graph p, the number of edges in the graph s and
the maximum node degree d, are allowed to grow as a function of the number of samples
n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE
estimator to recover all the edges of the graph with high probability. Under some conditions
on the model covariance, we show that model selection can be achieved for sample sizes
n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We
illustrate our theoretical results via simulations and show good correspondences between
the theoretical predictions and behavior in simulations.

1 Introduction

The area of high-dimensional statistics deals with estimation in the “large p, small n ” setting, where
p and n correspond, respectively, to the dimensionality of the data and the sample size. Such high-
dimensional problems arise in a variety of applications, among them remote sensing, computational
biology and natural language processing, where the model dimension may be comparable or sub-
stantially larger than the sample size. It is well-known that such high-dimensional scaling can lead to
dramatic breakdowns in many classical procedures. In the absence of additional model assumptions,
it is frequently impossible to obtain consistent procedures when p ≫ n. Accordingly, an active line
of statistical research is based on imposing various restrictions on the model—-for instance, spar-
sity, manifold structure, or graphical model structure —-a
nd then studying the scaling behavior of
different estimators as a function of sample size n, ambient dimension p and additional parameters
related to these structural assumptions.

In this paper, we study the problem of estimating the graph structure of a Gauss Markov random ﬁeld
(GMRF) in the high-dimensional setting. This graphical model selection problem can be reduced to
the problem of estimating the zero-pattern of the inverse covariance or concentration matrix Θ∗ . A
line of recent work [1, 2, 3, 4] has studied estimators based on minimizing Gaussian log-likelihood
penalized by the ℓ1 norm of the entries (or the off-diagonal entries) of the concentration matrix. The
resulting optimization problem is a log-determinant program, which can be solved in polynomial
time with interior point methods [5], or by faster co-ordinate descent algorithms [3, 4]. In recent
work, Rothman et al. [1] have analyzed some aspects of high-dimensional behavior, in particular
establishing consistency in Frobenius norm under certain conditions on the model covariance and
under certain scalings of the sparsity, sample size, and ambient model dimension.

The main contribution of this paper is to provide sufﬁcient c onditions for model selection consis-
tency of ℓ1 -regularized Gaussian maximum likelihood. It is worth noting that such a consistency
result for structure learning of Gaussian graphical models cannot be derived from Frobenius norm
consistency alone. For any concentration matrix Θ, denote the set of its non-zero off-diagonal entries

1

by E (Θ) = {s 6= t | Θst 6= 0}. (As will be clariﬁed below, the notation E alludes to the fact that
this set corresponds to the edges in the graph de ﬁning the GMR F.) Under certain technical condi-
tions to be speciﬁed, we prove that the ℓ1 -regularized (on off-diagonal entries of Θ) Gaussian MLE
recovers this edge set with high probability, meaning that P[E ( bΘ) = E (Θ∗ )] → 1. In many appli-
cations of graphical models (e.g., protein networks, social network analysis), it is this edge structure
itself, as opposed to the weights Θ∗
st on the edges, that is of primary interest. Moreover, we note
that model selection consistency is useful even when one is interested in convergence in spectral or
Frobenius norm; indeed, having extracted the set E (Θ∗ ), we could then restrict to this subset, and
estimate the non-zero entries of Θ∗ at the faster rates applicable to the reduced dimension.
The remainder of this paper is organized as follows. In Section 2, we state our main result, discuss
its connections to related work, and some of its consequences. Section 3 provides an outline of the
proof. In Section 4, we provide some simulations that illustrate our results.

Notation For the convenience of the reader, we summarize here notation to be used throughout
the paper. Given a vector u ∈ Rd and parameter a ∈ [1, ∞], we use kuka to denote the usual
ℓa norm. Given a matrix U ∈ Rp×p and parameters a, b ∈ [1, ∞], we use |||U |||a,b to denote the
induced matrix-operator norm maxkyka=1 kU ykb ; see [6] for background. Three cases of particular
importance in this paper are the spectral norm |||U |||2 , corresponding to the maximal singular value
of U ; the ℓ∞/ℓ∞ -operator norm, given by
pXk=1
|||U |||∞ := max
|Ujk |,
j=1,...,p
and the ℓ1/ℓ1 -operator norm, given by |||U |||1 = |||U T |||∞ . Finally, we use kU k∞ to denote the
element-wise maximum maxi,j |Uij |; note that this is not a matrix norm, but rather a norm on the
vectorized form of the matrix. For any matrix U ∈ Rp×p , we use vec(U ) or equivalently U ∈ Rp2 to
denote its vectorized form, obtained by stacking up the rows of U . We use hhU, V ii := Pi,j Uij Vij
to denote the trace inner product on the space of symmetric matrices. Note that this inner product
induces the Frobenius norm |||U |||F := qPi,j U 2
ij . Finally, for asymptotics, we use the following
standard notation: we write f (n) = O(g (n)) if f (n) ≤ cg (n) for some constant c < ∞, and
f (n) = Ω(g (n)) if f (n) ≥ c′ g (n) for some constant c′ > 0. The notation f (n) ≍ g (n) means that
f (n) = O(g (n)) and f (n) = Ω(g (n)).
2 Background and statement of main result

(1)

In this section, we begin by setting up the problem, with some background on Gaussian MRFs and
ℓ1 -regularization. We then state our main result, and discuss some of its consequences.

f (x1 , . . . , xp ; Θ∗ ) =

2.1 Gaussian MRFs and ℓ1 penalized estimation
Consider an undirected graph G = (V , E ) with p = |V | vertices, and let X = (X1 , . . . , Xp )
denote a p-dimensional Gaussian random vector, with variate Xi identiﬁed with vertex i ∈ V . A
Gauss-Markov random ﬁeld (MRF) is described by a density of t he form
xT Θ∗x(cid:27) .
(2π det(Θ∗ ))p/2 exp (cid:26)−
1
1
2
As illustrated in Figure 1, Markov structure is re ﬂected in t he sparsity pattern of the inverse co-
variance or concentration matrix Θ∗ , a p × p symmetric matrix. In particular, by the Hammersley-
ij = 0 for all (i, j ) /∈ E . Consequently, the problem of
Clifford theorem [7], it must satisfy Θ∗
graphical model selection is equivalent to estimating the off-diagonal zero-pattern of the concentra-
tion matrix —that is, the set E (Θ∗ ) := {i, j ∈ V | i 6= j, Θ∗
ij 6= 0}.
In this paper, we study the minimizer of the ℓ1 -penalized Gaussian negative log-likelihood. Let-
ting hhA, B ii := Pi,j Aij Bij be the trace inner product on the space of symmetric matrices, this
objective function takes the form
Θ(cid:23)0 nhhΘ, bΣii − logdet(Θ) + λn kΘk1,oﬀ o = arg min
g (Θ; bΣ, λn ).
bΘ = arg min
(3)
Θ(cid:23)0
2

(2)

1

5

2

3

1

2

3

4

5

4

(a)

Zero pattern of inverse covariance

1

2

3
(b)

4

5

Figure 1. (a) Simple undirected graph. A Gauss Markov random ﬁeld has a Gaussian variable Xi
associated with each vertex i ∈ V . This graph has p = 5 vertices, maximum degree d = 3 and s = 6
edges. (b) Zero pattern of the inverse covariance Θ∗ associated with the GMRF in (a). The set E (Θ∗ )
corresponds to the off-diagonal non-zeros (white blocks); the diagonal is also non-zero (grey squares),
but these entries do not correspond to edges. The black squares correspond to non-edges, or zeros in
Θ∗ .

n Pn
Here bΣ denotes the sample covariance —that is,
bΣ := 1
ℓ=1 X (ℓ) [X (ℓ) ]T , where each X (ℓ) is
drawn in an i.i.d. manner according to the density (2). The quantity λn > 0 is a user-de ﬁned
regularization parameter. and kΘk1,oﬀ := Pi6=j |Θij | is the off-diagonal ℓ1 regularizer; note that it
does not include the diagonal. Since the negative log-determinant is a strictly convex function [5],
this problem always has a unique solution, so that there is no ambiguity in equation (3).
We let E ( bΘ) = {(i, j ) | i 6= j, bΘij 6= 0} denote the edge set associated with the estimate. Of
interest in this paper is studying the probability P[E (Θ∗) = E ( bΘ)] as a function of the graph size p
(which serves as the “model dimension ” for the Gauss-Markov model), the sample size n, and the
structural properties of bΘ. In particular, we de ﬁne both the sparsity index
:= |E (Θ∗ )| = {i, j ∈ V | i 6= j, Θ∗
(4)
s
ij 6= 0}|.
corresponding to the total number of edges, and the maximum degree or row cardinality
j=1,...,p |{i | Θ∗
d := max
ij 6= 0},
corresponding to the maximum number of non-zeros in any row of Θ∗ , or equivalently the maximum
degree in the graph G, where we include the diagonal in the degree count.

(5)

2.2 Statement of main result

Γ∗

Our assumptions involve the Hessian with respect to Θ of the objective function g de ﬁned in equa-
tion (3), evaluated at the true model Θ∗ . Using standard results on matrix derivatives [5], it can be
shown that this Hessian takes the form
Θ g (Θ)(cid:12)(cid:12)(cid:12)Θ=Θ∗
= Θ∗−1 ⊗ Θ∗−1 ,
:= ∇2
where ⊗ denotes the Kronecker matrix product. By de ﬁnition, Γ∗ is a p2 × p2 matrix indexed by
∂ 2 g
vertex pairs, so that entry Γ∗
(j,k),(ℓ,m) corresponds to the second partial derivative
, evalu-
∂Θjk ∂Θℓm
ated at Θ = Θ∗ . When X has multivariate Gaussian distribution, then Γ∗ is the Fisher information
of the model, and by standard results on cumulant functions in exponential families [8], we have the
more speciﬁc expression Γ∗
(j,k),(ℓ,m) = cov{Xj Xk , XℓXm}. For this reason, Γ∗ can be viewed as
an edge-based counterpart to the usual covariance matrix Σ∗ .
We de ﬁne the set of non-zero off-diagonal entries in the mode l concentration matrix Θ∗ :
:= {(i, j ) ∈ V × V | i 6= j, Θ∗
S (Θ∗ )
(7)
ij 6= 0},
and let S (Θ∗ ) = {S (Θ∗) ∪ {(1, 1), . . . , (p, p)} be the augmented set including the diagonal. We let
S c (Θ∗ ) denote the complement of S (Θ∗ ) in the set {1, . . . , p} × {1, . . . , p}, corresponding to all

(6)

3

ℓm = 0. When it is clear from context, we shorten our notation for these
pairs (ℓ, m) for which Θ∗
sets to S and S c , respectively. Finally, for any two subsets T and T ′ of V × V , we use Γ∗
T T ′ to
denote the |T | × |T ′ | matrix with rows and columns of Γ∗ indexed by T and T ′ respectively.
We require the following conditions on the Fisher information matrix Γ∗ :

[A1] Incoherence condition: This condition captures the intuition that variable-pairs which are
non-edges cannot exert an overtly strong effect on variable-pairs which form edges of the Gaussian
graphical model.

S cS (Γ∗
|||Γ∗
SS )−1 |||∞ ≤ (1 − α),
for some ﬁxed α > 0.
We note that similar conditions arise in the analysis of the Lasso in linear regression [9, 10, 11].

(8)

[A2] Covariance control: There exist constants KΣ∗ , KΓ∗ < ∞ such that
|||Θ∗−1 |||∞ ≤ KΣ∗ ,
and |||(Γ∗
SS )−1 |||∞ ≤ KΓ∗ .
(9)
These assumptions require that the covariance elements along any row of (Θ∗ )−1 and (Γ∗
SS )−1 have
bounded ℓ1 norms. Note that similar assumptions are are also required for consistency in Frobenius
norm [1].

Recall from equations (4) and (5) the de ﬁnitions of the spars ity index s and maximum degree d,
respectively. With this notation, we have:
Theorem 1. Consider a Gaussian distribution with concentration matrix Θ∗ that satisﬁes conditions
(A1) and (A2). Suppose the penalty is set as λn = C1q log p
n , and the minimum edge-weight Θ∗
min :=
min > C2q log p
min(i,j)∈S |Θ∗
ij | scales as Θ∗
n for some constants C1 , C2 > 0. Further, suppose the
triple (n, d, p) satisﬁes the scaling
n > L d2 log(p),
(10)
for some constant L > 0. Then the edge set E ( bΘ) speciﬁed by the estimator speciﬁes the true edge
set w.h.p. —in particular,
P[E ( bΘ) = E (Θ∗ )] ≥ 1 − exp(−c log p) → 1.
(11)
for some constant c > 0.
Remarks: Rothman et al. [1] prove that the error of the estimator in Frobenius norm obeys the
bound ||| bΘ − Θ∗ |||2
F = O {((s + p) log p)/n}, with high probability. We note that model selection
consistency does not follow from this result, since an estimate may be close in Frobenius norm while
differing substantially in terms of zero-pattern. In one sense, the model selection criterion is more
demanding, since given knowledge of the edge set E (Θ∗ ), one could restrict estimation procedures
to this subset, and so achieve faster rates. On the other hand, Theorem 1 requires incoherence
conditions [A1] on the covariance matrix, which are not required for Frobenius norm consistency [1].

2.3 Comparison to neighbor-based graphical model selection

It is interesting to compare the estimator to the Gaussian neighborhood regression method studied
by Meinshausen and B ¨uhlmann [9], in which each node is linearly regressed with an ℓ1 penalty
(Lasso) on the rest of the nodes; and the location of the non-zero regression weights is taken as the
neighborhood estimate of that node. These neighborhoods are then combined, by either an OR rule
or an AND rule, to estimate the full graph. Wainwright [12] shows that the rate n ≍ d log p is a
sharp threshold for the success/failure of neighborhood selection by Lasso. By a union bound over
the p nodes, it follows this threshold holds for the Meinshausen and B ¨uhlmann approach as well.
This is superior to the scaling in our result (10). However, the two methods rely on slightly different
underlying assumptions, and the current form of the neighborhood-based approach requires solving
a total of p Lasso programs, as opposed to a single log-determinant problem. Below we show two
cases where the Lasso irrepresentability condition holds, while the log-determinant requirement
fails. However, in general, we do not know whether the log-determinant irrepresentability strictly
dominates its analog for the Lasso.

4

2.3.1 Illustration of irrepresentability: Diamond graph

Consider the following Gaussian MRF example from [13]. Figure 2(a) shows a diamond-shaped
graph G = (V , E ), with vertex set V = {1, 2, 3, 4} and edge-set as the fully connected graph over V
with the edge (1, 4) removed. The covariance matrix Σ∗ is parameterized by the correlation param-

2

2

3

1

4

1

3
(a)

4
(b)

Figure 2: (a) Graph of the example discussed by [13]. (b) A simple 4-node star graph.
eter ρ ∈ [0, 1/√2]: the diagonal entries are set to Σ∗
ii = 1, for all i ∈ V ; the entries corresponding
ij = ρ for (i, j ) ∈ E \{(2, 3)}, Σ∗
23 = 0; and ﬁnally the entry corresponding to
to edges are set to Σ∗
14 = 2ρ2 . For this model, [13] showed that the ℓ1 -regularized MLE bΘ fails
the non-edge is set as Σ∗
to recover the graph structure for any sample size, if ρ > −1 + (3/2)1/2 ≈ 0.23. It is instructive
to compare this necessary condition to the sufﬁcient condit ion provided in our analysis, namely the
incoherence Assumption [A1] as applied to the Hessian Γ∗ . For this particular example, a little cal-
culation shows that Assumption [A1] is equivalent to the constraint 4|ρ|(|ρ| + 1) < 1, an inequality
which holds for all ρ ∈ (−0.2017, 0.2017). Note that the upper value 0.2017 is just below the nec-
essary threshold discussed by [13]. On the other hand, the irrepresentability condition for the Lasso
requires only that 2|ρ| < 1, i.e., ρ ∈ (−0.5, 0.5). Thus, in the regime |ρ| ∈ [0.2017, 0.5), the Lasso
irrepresentability condition holds while our log-determinant counterpart fails.

2.3.2 Illustration of irrepresentability: Star graphs

A second interesting example is the star-shaped graphical model, illustrated in Figure 2(b), which
consists of a single hub node connected to the rest of the spoke nodes. We consider a four node
graph, with vertex set V = {1, 2, 3, 4} and edge-set E = {(1, s) | s ∈ {2, 3, 4}}. The covariance
matrix Σ∗ is parameterized the correlation parameter ρ ∈ [−1, 1]: the diagonal entries are set to
ii = 1, for all i ∈ V ; the entries corresponding to edges are set to Σ∗
ij = ρ for (i, j ) ∈ E ; while
Σ∗
the non-edge entries are set as Σ∗
ij = ρ2 for (i, j ) /∈ E . Consequently, for this particular example,
Assumption [A1] reduces to the constraint |ρ|(|ρ|+ 2) < 1, which holds for all ρ ∈ (−0.414, 0.414).
The irrepresentability condition for the Lasso on the other hand allows the full range ρ ∈ (−1, 1).
Thus there is again a regime, |ρ| ∈ [0.414, 1), where the Lasso irrepresentability condition holds
while the log-determinant counterpart fails.

3 Proof outline

Theorem 1 follows as a corollary to Theorem 2 in Ravikumar et al [14], an extended and more
general version of this paper. There we consider the more general problem of estimation of the
covariance matrix of a random vector (that need not necessarily be Gaussian) from i.i.d. samples;
and where we relax Assumption [A2], and allow quantities KΣ∗ , KΓ∗ to grow with sample size n.
We provide here a high-level outline of the proof of Theorem 1, deferring details to the extended
version [14]. Our proofs are based on a technique that we call a primal-dual witness method, used
previously in analysis of the Lasso [12]. It involves following a speciﬁc sequence of steps to con-
struct a pair ( eΘ, eZ ) of symmetric matrices that together satisfy the optimality conditions associated
with the convex program (3) with high probability. Thus, when the constructive procedure succeeds,
eΘ is equal to the unique solution bΘ of the convex program (3), and eZ is an optimal solution to its
5

dual. In this way, the estimator bΘ inherits from eΘ various optimality properties in terms of its dis-
tance to the truth Θ∗ , and its recovery of the signed sparsity pattern. To be clear, our procedure for
constructing eΘ is not a practical algorithm for solving the log-determinant problem (3), but rather is
used as a proof technique for certifying the behavior of the ℓ1 -regularized MLE (3).
3.1 Primal-dual witness approach

(12)

At the core of the primal-dual witness method are the standard convex optimality conditions that
characterize the optimum bΘ of the convex program (3). For future reference, we note that the sub-
differential of the norm k · k1,oﬀ evaluated at some Θ consists the set of all symmetric matrices
Z ∈ Rp×p such that
Zij = 
if i = j
0
if i 6= j and Θij 6= 0
sign(Θij )

if i 6= j and Θij = 0.
∈ [−1, +1]
Lemma 1. For any λn > 0 and sample covariance bΣ with strictly positive diagonal, the ℓ1 -
regularized log-determinant problem (3) has a unique solution bΘ ≻ 0 characterized by
bΣ − bΘ−1 + λn eZ = 0,
(13)
where eZ is an element of the subdifferential ∂ k bΘk1,oﬀ .
Based on this lemma, we construct the primal-dual witness solution ( eΘ, eZ ) as follows:
(a) We determine the matrix eΘ by solving the restricted log-determinant problem
Θ≻0, ΘSc =0 (cid:8)hhΘ, bΣii − log det(Θ) + λn kΘk1,oﬀ (cid:9).
eΘ := arg min
Note that by construction, we have eΘ ≻ 0, and moreover eΘS c = 0.
(b) We choose eZS as a member of the sub-differential of the regularizer k · k1,oﬀ , evaluated at
eΘ.
(c) We set eZS c as
λn (cid:8) − bΣS c + [ eΘ−1 ]S c (cid:9),
1
eZS c =
which ensures that constructed matrices ( eΘ, eZ ) satisfy the optimality condition (13).
(d) We verify the strict dual feasibility condition
| eZij | < 1 for all (i, j ) ∈ S c .
( eΘ, eZ ) that
To clarify the nature of the construction, steps (a) through (c) sufﬁce to obtain a pair
satisfy the optimality conditions (13), but do not guarantee that eZ is an element of sub-differential
∂ k eΘk1,oﬀ . By construction, speciﬁcally step (b) of the construction ensures that the entries eZ in S
satisfy the sub-differential conditions, since eZS is a member of the sub-differential of ∂ k eΘS k1,oﬀ .
The purpose of step (d), then, is to verify that the remaining elements of eZ satisfy the necessary
conditions to belong to the sub-differential.
If the primal-dual witness construction succeeds, then it acts as a witness to the fact that the solution
eΘ to the restricted problem (14) is equivalent to the solution bΘ to the original (unrestricted) prob-
lem (3). We exploit this fact in our proof of Theorem 1: we ﬁrst
show that the primal-dual witness
technique succeeds with high-probability, from which we can conclude that the support of the opti-
mal solution bΘ is contained within the support of the true Θ∗ . The next step requires checking that
none of the entries in eΘS constructed in Equation (14) are zero. It is to verify this that we require
the lower bound assumption in Theorem 1 on the value of the minimum value Θ∗
min .
6

(14)

(15)

4 Experiments

In this section, we describe some experiments which illustrate the model selection rates in Theo-
rem 1. We solved the ℓ1 penalized log-determinant optimization problem using the “glasso ” pro-
gram [4], which builds on the block co-ordinate descent algorithm of [3]. We report experiments
for star-shaped graphs, which consist of one node connected to the rest of the nodes. These graphs
allow us to vary both d and p, since the degree of the central hub can be varied between 1 and p − 1.
Applying the algorithm to these graphs should therefore provide some insight on how the required
number of samples n is related to d and p. We tested varying graph sizes p from p = 64 upwards
to p = 375. The edge-weights were set as entries in the inverse of a covariance matrix Σ∗ with
diagonal entries set as Σ∗
ii = 1 for all i = 1, . . . , p, and Σ∗
ij = 2.5/d for all (i, j ) ∈ E , so that the
quantities (KΣ∗ , KΓ∗ , α) remain constant.
Dependence on graph size:

Star graph

 

Star graph

 

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

p=64
p=100
p=225
p=375
500

400

p=64
p=100
p=225
p=375
120
140

0
 
100

200

300
n
(a)

0
 
20

40

60

100

80
n/log p
(b)

Figure 3. Simulations for a star graph with varying number of nodes p, ﬁxed maximal degree d = 40,
and edge covariances Σ∗
ij = 1/16 for all edges. Plots of probability of correct signed edge-set recovery
versus the sample size n in panel (a), and versus the rescaled sample size n/ log p in panel (b). Each
point corresponds to the average over N = 100 trials.

Panel (a) of Figure 3 plots the probability of correct signed edge-set recovery against the sample size
n for a star-shaped graph of three different graph sizes p. For each curve, the probability of success
starts at zero (for small sample sizes n), but then transitions to one as the sample size is increased.
As would be expected, it is more difﬁcult to perform model sel ection for larger graph sizes, so
that (for instance) the curve for p = 375 is shifted to the right relative to the curve for p = 64.
Panel (b) of Figure 3 replots the same data, with the horizontal axis rescaled by (1/ log p). This
scaling was chosen because our theory predicts that the sample size should scale logarithmically
with p (see equation (10)). Consistent with this prediction, when plotted against the rescaled sample
size n/ log p, the curves in panel (b) all stack up. Consequently, the ratio (n/ log p) acts as an
effective sample size in controlling the success of model selection, consistent with the predictions
of Theorem 1.
Dependence on the maximum node degree:

Panel (a) of Figure 4 plots the probability of correct signed edge-set recovery against the sample size
n for star-shaped graphs; each curve corresponds to a different choice of maximum node degree d,
allowing us to investigate the dependence of the sample size on this parameter. So as to control these
comparisons, we ﬁxed the number of nodes to p = 200. Observe how the plots in panel (a) shift to
the right as the maximum node degree d is increased, showing that star-shaped graphs with higher
degrees are more difﬁcult. In panel (b) of Figure 4, we plot th e same data versus the rescaled sample
size n/d. Recall that if all the curves were to stack up under this rescaling, then it means the required
sample size n scales linearly with d. These plots are closer to aligning than the unrescaled plots, but
the agreement is not perfect. In particular, observe that the curve d (right-most in panel (a)) remains
a bit to the right in panel (b), which suggests that a somewhat more aggressive rescaling —perhaps
n/dγ for some γ ∈ (1, 2)—is appropriate. The sufﬁcient condition from Theorem 1, as
summarized

7

Truncated Star with Varying d

 

d=50
d=60
d=70
d=80
d=90
d=100

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

Truncated Star with Varying d

 

d=50
d=60
d=70
d=80
d=90
d=100

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

0
 
1000

1500

2000

n
(a)

2500

3000

3500

0
 
26

28

30

34

36

38

32
n/d
(b)

Figure 4. Simulations for star graphs with ﬁxed number of nodes p = 200, varying maximal (hub)
degree d, edge covariances Σ∗
ij = 2.5/d. Plots of probability of correct signed edge-set recovery
versus the sample size n in panel (a), and versus the rescaled sample size n/d in panel (b).

in equation (10), is n = Ω(d2 log p), which appears to be overly conservative based on these data.
Thus, it might be possible to tighten our theory under certain regimes.

References

[1] A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation.
Electron. J. Statist., 2:494–515, 2008.
[2] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika,
94(1):19–35, 2007.
[3] A. d’Aspr ´emont, O. Banerjee, and L. El Ghaoui. First-or der methods for sparse covariance selection.
SIAM J. Matrix Anal. Appl., 30(1):56–66, 2008.
[4] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso.
Biostat., 9(3):432–441, 2007.
[5] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, UK, 2004.
[6] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[7] S. L. Lauritzen. Graphical Models. Oxford University Press, Oxford, 1996.
[8] L.D. Brown. Fundamentals of statistical exponential families. Institute of Mathematical Statistics, Hay-
ward, CA, 1986.
[9] N. Meinshausen and P. B ¨uhlmann. High-dimensional graphs and variable selection with the Lasso. Ann.
Statist., 34(3):1436–1462, 2006.
[10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals. IEEE Trans. Info.
Theory, 51(3):1030–1051, 2006.
[11] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research,
7:2541–2567, 2006.
[12] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity using the Lasso.
Technical Report 709, UC Berkeley, May 2006. To appear in IEEE Trans. Info. Theory.
[13] N. Meinshausen. A note on the Lasso for graphical Gaussian model selection. Statistics and Probability
Letters, 78(7):880–884, 2008.
[14] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by
minimizing ℓ1 -penalized log-determinant divergence. Technical Report 767, Department of Statistics,
UC Berkeley, November 2008.

8


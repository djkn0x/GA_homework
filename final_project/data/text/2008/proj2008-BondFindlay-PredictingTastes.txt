Predicting Tastes from Friend Relationships

Chris Bond and Duncan Findlay

December 12, 2008

1

Introduction

In the last few years, online social networks have be-
come an important part of people’s lives. They have
made data describing the connections between people
more accessible than it has ever been in the past. In
light of this new technology, we can now explore the
predictive power of social data.
In this paper, we present a method to predict target
variables that describe individuals based upon their
relationships to others and knowledge of the target
variables for some of those other individuals.
In the next section, we do a survey of the previous
research on using social graphs as predictive instru-
ments. We then discuss in Section 3 how we apply
machine learning to this problem by constructing an
appropriate support vector machine. In Section 4, we
describe how we obtained test data from the Face-
book API[4], as well as the features and target vari-
ables that we used. In Section 5, we analyze how our
algorithm performs when trained with the Facebook
data using ten-fold cross validation.

2 Prior Work

We found relatively little prior research on how social
connections can be used to predict tastes. Most prior
work on social networks deals with ﬁnding cliques
and quasi-cliques within social graphs, which is an
interesting but diﬀerent problem. However, some of
these studies investigated the use of kernels for ana-
lyzing social graphs, which is relevant to our research.
In one paper [8], researchers investigated the use of
kernel-based distance for clustering a medieval peas-
ant society and found that it did provide coherent
clustering. They used the diﬀusion kernel, which is
the discrete solution of the heat equation. We will
describe the diﬀusion kernel further in Section 3.
We found one signiﬁcant study on social inﬂuence,
albeit outside the realm of internet social networks.
In this study [2], researchers examined the spread

1

of obesity through social connections. Researchers
found there is indeed a signiﬁcant correlation between
social proximity and the spread of obesity, and that
the eﬀect becomes insigniﬁcant after three-degrees
of separation. The obesity researchers used logis-
tic regression to predict the obesity of one sub ject
as a function of several variables, including the obe-
sity of another sub ject. Notably, they measured the
strength of the social inﬂuence factor by running their
regression both on the true social graph and another,
fabricated graph with the same topology and overall
incidence of obesity, but the incidences randomized.
If there were no social factor, they posited, the re-
sults of both runs should be the same. This study
also found that only same-sex friendships predicted
obesity, and that if the friendship was not mutual
then it only predicted obesity for the participant who
recognizes the friendship.
In another study [1],
researchers investigated
whether or not tagging in Flickr displayed signs of
social inﬂuence. They looked at whether or not there
is a correlation between tags from one user to the ap-
plication of tags from another user. Although they
found there is indeed such a correlation, they did
not study the predictive power of social connections.
They used the same shuﬄe-based test as the obesity
study to test how their model compared to a random
social graph. Since this paper dealt with a directed
graph, they also compared the performance of their
model against its performances on the test data with
graph edges reversed.

3 Preliminaries

The friend matrix F , given by (1) is an adjacency
matrix for an undirected graph of friendships.
(cid:26) 1
0
Since we want to make predictions solely based on

if users i and j are friends
otherwise

Fij =

(1)

understanding of the friend matrix, we cannot use
algorithms that require explicit representation of fea-
tures for each training example; we need to use an
algorithm that requires only a notion of comparison
between two examples which are, in this case, nodes
in our friendship graph. Thus we decided to use a
support vector machine (SVM) method since these
tend to perform well for high-feature models and al-
low us to use the kernel trick.
The diﬃculty lies in choosing a kernel function that
provides a notion of relevance or similarity between
users derived solely from the (complete) friend ma-
trix. We need to choose a kernel function that ap-
propriately exploits the friend relationships. Since
F is not guaranteed to be positive semi-deﬁnite, we
must ﬁnd a function of F that forms a valid kernel,
and has the properties we desire.
The ﬁrst kernel we tried was a very simple one.
As suggested in [6], we calculated the square of the
friend matrix (the “Square Transformation”). This
forces the matrix to be positive semi-deﬁnite; and the
entries happen to correspond to the number of links
between users of length 2 (i.e. in terms of “friends of
friends”).
The second kernel we tried was the diﬀusion ker-
nel, which was used with some success in [8] and de-
scribed with greater detail in [5]. The diﬀusion kernel
involves deﬁning the Laplacian of the friend graph as
shown in (2), and taking its matrix exponential to
deﬁne a kernel as described in (3). Because it can
be expensive to compute the matrix exponential of a
large matrix, we optimized this by diagonalizing the
matrix by computing the eigenvalues and eigenvec-
tors (as shown in (4)) and using this representation
to calculate the kernel for diﬀerent values of β much
(cid:26) −Fij
more quickly, as shown in (5).
P
i 6= j
if
if
i = j
k Fik
K = e−βL , β ∈ R

Lij =

(2)

(3)

L = U ΛU −1

K = U e−βΛU −1

(4)

(5)

4 Experiments

Because of the numerous privacy restrictions built
in to the Facebook API, data collection was not as

2

straightforward as one might expect. We ﬁrst col-
lected a small list of “groups” from a random set of
users whose proﬁles we could access. We requested
a list of users who were members of each of these
groups and also aﬃliated with the San Francisco net-
work. We then requested data for each of the possible
n·(n−1)
friend relationships, excluding users for whom
2
we could not determine friend relationships (due to
privacy constraints, or because they had no friends
among the users queried). Lastly we fetched the ac-
cessible proﬁle information for each user.
The Facebook API only allows you to check
whether two speciﬁc users are friends; there is no way
to request the friend list of an arbitrary user. This
means that we need to independently “discover” both
users before determining if they are friends. On the
one hand, this seems like a disadvantage because it
means we’ll have incomplete friend relationships with
which to train our model. On the other hand, it also
means our training data will be more representative
of the entire social graph, rather than being an arbi-
trary subgraph.
Using the group-based technique, we collected a
data set of 8373 users. Unfortunately, this proved to
be too much data to process in a timely manner, so we
deleted users that had less than 27 friends from our
set in an attempt to make the eigenvector calculation
more tractable. This left us will 1103 users. We will
call this data set the “trim” set.
We collected another set of data by doing a
breadth-ﬁrst search of friends starting at one of the
authors, using the web interface. We stopped after
collecting data for 1000 users. One consequence of
this technique is that any two users in this data set
are separated by a very short number of links, so it
provides an interesting contrast to the other data set.
We will refer to this data set as the “1k” set.
Figure 1 shows the number of friends per user in
each data set.
We should note that neither of these techniques
provide a particularly random sample of users be-
cause they are heavily inﬂuenced by the “seed”
groups (in the ﬁrst set) or by the starting user (in
the second set).
As target variables, we used whether users (1) are
interested in “music”, (2) are single, and (3) were
born after 1981. The distribution of positive and neg-
ative examples for each variable is shown in Tables 1
and 2.
We used 10 fold cross validation to train and test
our support vector machines for each data set, and

Figure 1: Histogram of Friends per User

y (i) = 1
y (i) = −1
Total

music
58
230
282

single > 1981
128
152
201
237
365
353

Table 1: Class distribution for 1k set

y (i) = 1
y (i) = −1
Total

music
73
250
323

single > 1981
132
143
244
174
306
387

Table 2: Class distribution for trim set

each class, with both the square kernel, and the dif-
fusion kernel for numerous values of β . We used a
simpliﬁed version of the SMO algorithm described by
Platt[7].
Lastly, as a sanity check, we performed the same
experiments using the same friend matrix, but assign-
ing the class labels to random users in the graph.

5 Results

Plots of our results are shown in Figure 2. There
is one graph for each of the three classes across both
data sets. For each set of data and each class, we used
10 fold cross validation to train and test our support
vector machine. The dashed lines on the graph show
the training error, while the solid lines show the test
error. Results with both kernels are shown on the

3

same graph. Since the square kernel is parameterless,
we represented it with a horizontal line. Since the
diﬀusion kernel requires the parameter β , we plotted
the training and test error for values of β between 0
and 2.
Lastly, with a dot-dashed line, we plotted the test
error that would be obtained using a trivial policy,
which is to predict that all users are in the more com-
mon class.
There are several notable features of these graphs.
First, our test error (for all kernels) for predicting
whether a user is interested in music or (for the “1k”
set) whether a user is single is worse than that found
by the trivial algorithm, even though test error is low
for low values of β . Furthermore, when we ran the
algorithms against the randomly shuﬄed data set, we
saw the same low training error and high test error
(higher than the trivial policy error) for all data sets.
This conﬁrms that there is minimal or no correlation
between whether a user is interested in music (or is
single) based on whether his or her friends are inter-
ested in music.
For predicting age, we found that with very low val-
ues of β the training error was extremely low, while
the test error was quite high; this suggests that the
system has high variance. As β approaches 0, the
diﬀusion kernel weights the ﬁrst-order relationships
much more than second and third level eﬀects, and
these local eﬀects make the algorithm much more
prone to overﬁtting. As β increases, we see that the
training error and test error converge somewhat. This

050100150020406080100Friends per UserFrequency1k Set01002003000510152025303540Friends per UserFrequencyTrim SetFigure 2: Plots of training and test errors

4

square kernel test errorsquare kernel training errordiffusion kernel test errordiffusion kernel training errortrivial algorithm error00.511.5200.10.20.30.40.5Interested in Music  1k Set00.511.5200.10.20.30.40.5Relationship Status00.511.5200.10.20.30.40.5betaBorn after 198100.511.5200.10.20.30.40.5Trim Set  00.511.5200.10.20.30.40.5  00.511.5200.10.20.30.40.5beta[3] Du, N., Wu, B., Pei, X., Wang, B., and
Xu, L. Community detection in large-scale social
networks. In WebKDD/SNA-KDD ’07: Proceed-
ings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network anal-
ysis (New York, NY, USA, 2007), ACM, pp. 16–
25.

[4] Facebook.
developers API
Facebook
http://developers.facebook.com, 2008.

[5] Kondor, R. I., and Lafferty, J. Diﬀusion
kernels on graphs and other discrete structures.
In In Proceedings of the ICML (2002), pp. 315–
322.

[6] Muoz, A., and n de Diego, I. M. From in-
deﬁnite to positive semi-deﬁnite matrices. Lecture
Notes in Computer Science 4109 (2006), 764–772.

[7] Platt, J. C. Sequential minimal optimization:
A fast algorithm for training support vector ma-
chines. Tech. rep., Advances in Kernel Methods -
Support Vector Learning, 1998.

[8] Villa, N., and Boulet, R. Clustering a me-
dieval social network by SOM using a kernel based
distance measure. In ESANN ’07: Proceeding of
the 15th European Symposium on Artiﬁcial Neu-
ral Networks (2007), pp. 31–36.

suggests that at higher values of β , the kernel gener-
alizes better, since it avoids depending on these local
eﬀects. The square kernel was prone to the same sort
of overﬁtting as the diﬀusion kernel for small values
of β .
Of the three classes we tried to predict, we were
best able to predict whether a user was born after
1981. This suggests that it is, of the three classes,
the one that is most likely to be in common between
friends.
The two data sets show somewhat diﬀerent rela-
tionships between test and training error.
In the
“trim” data set, training and test errors converge at
much lower values of β . This might be because the
friend matrix is much more dense (on average) than
that of the “1k” set.

6 Conclusion

Taken together these results suggest that the system
has high variance and that we might do better with
more data, allowing us to achieve better generaliza-
tion results for the diﬀusion kernel at lower values of
β . This makes sense intuitively, as human tastes and
preferences naturally have high variance and so many
complicated dynamics are involved in friend relation-
ships. The basic premise taken in this research is that
people with similar interests and characteristics are
more likely to be friends with each other. That is ob-
viously true to some extent, but there are also many
external factors that add noise to this and make it
diﬃcult to predict on such a small scale.
It would be very interesting to repeat this analysis
with much more data, but this would require more
computing power than we have at our disposal.

7 References

[1] Anagnostopoulos, A., Kumar, R., and
Inﬂuence and correlation in so-
Mahdian, M.
cial networks.
In KDD ’08: Proceeding of the
14th ACM SIGKDD international conference on
Know ledge discovery and data mining (New York,
NY, USA, 2008), ACM, pp. 7–15.

[2] Christakis, N. A., and Fowler, J. H. The
spread of obesity in a large social network over
32 years. The New England Journal of Medicine
357, 4 (2007), 370–379.

5


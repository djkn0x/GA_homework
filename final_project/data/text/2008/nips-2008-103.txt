Efﬁcient Direct Density Ratio Estimation for
Non-stationarity Adaptation and Outlier Detection

Takafumi Kanamori
Nagoya University
Nagoya, Japan
kanamori@is.nagoya-u.ac.jp

Shohei Hido
IBM Research
Kanagawa, Japan
hido@jp.ibm.com

Masashi Sugiyama
Tokyo Institute of Technology
Tokyo, Japan
sugi@cs.titech.ac.jp

Abstract

We address the problem of estimating the ratio of two probabi lity density functions
(a.k.a. the importance). The importance values can be used for various succeed-
ing tasks such as non-stationarity adaptation or outlier detection. In this paper, we
propose a new importance estimation method that has a closed-form solution; the
leave-one-out cross-validation score can also be computed analytically. Therefore,
the proposed method is computationally very efﬁcient and nu merically stable. We
also elucidate theoretical properties of the proposed method such as the conver-
gence rate and approximation error bound. Numerical experiments show that the
proposed method is comparable to the best existing method in accuracy, while it
is computationally more efﬁcient than competing approache s.

1

Introduction

In the context of importance sampling, the ratio of two probability density functions is called the
importance. The problem of estimating the importance is gathering a lot of attention these days
since the importance can be used for various succeeding tasks, e.g.,

Covariate shift adaptation: Covariate shift is a situation in supervised learning where the distri-
butions of inputs change between the training and test phases but the conditional distribution of
outputs given inputs remains unchanged [8]. Covariate shift is conceivable in many real-world
applications such as bioinformatics, brain-computer interfaces, robot control, spam ﬁltering, and
econometrics. Under covariate shift, standard learning te chniques such as maximum likelihood es-
timation or cross-validation are biased and therefore unre liable—the bias caused by covariate shift
can be compensated by weighting the training samples according to the importance [8, 5, 1, 9].

Outlier detection: The outlier detection task addressed here is to identify irregular samples in an
evaluation dataset based on a model dataset that only contains regular samples [7, 3]. The importance
values for regular samples are close to one, while those for o utliers tend to be signi ﬁcantly deviated
from one. Thus the values of the importance could be used as an index of the degree of outlyingness.

Below, we refer to the two sets of samples as the training and test sets. A naive approach to estimat-
ing the importance is to ﬁrst estimate the training and test d ensities from the sets of training and test
samples separately, and then take the ratio of the estimated densities. However, density estimation is
known to be a hard problem particularly in high-dimensional cases. In practice, such an appropriate
parametric model may not be available and therefore this naive approach is not so effective.

1

To cope with this problem, we propose a direct importance est imation method that does not involve
density estimation. The proposed method, which we call least-squares importance ﬁtting (LSIF), is
formulated as a convex quadratic program and therefore the u nique global solution can be obtained.
We give a cross-validation method for model selection and a regularization path tracking algorithm
for efﬁcient computation [4].

This regularization path tracking algorithm is turned out t o be computationally very efﬁcient since
the entire solution path can be traced without a quadratic program solver. However, it tends to share a
common weakness of path tracking algorithms, i.e., accumulation of numerical errors. To overcome
this drawback, we develop an approximation algorithm called unconstrained LSIF (uLSIF), which
allows us to obtain the closed-form solution that can be stably computed just by solving a system
of linear equations. Thus uLSIF is computationally efﬁcien t and numerically stable. Moreover,
the leave-one-out error of uLSIF can also be computed analyt ically, which further improves the
computational efﬁciency in model selection scenarios.

We experimentally show that the accuracy of uLSIF is comparable to the best existing method while
its computation is much faster than the others in covariate shift adaptation and outlier detection.

2 Direct Importance Estimation

Formulation and Notation: Let D ⊂ (Rd ) be the data domain and suppose we are given inde-
i }ntr
i=1 from a training distribution
pendent and identically distributed (i.i.d.) training samples {xtr
j }nte
with density ptr (x) and i.i.d. test samples {xte
j=1 from a test distribution with density pte (x). We
assume ptr (x) > 0 for all x ∈ D . The goal of this paper is to estimate the importance

w(x) = pte (x)
ptr (x)

j }nte
i }ntr
j=1 . Our key restriction is that we want to avoid estimating dens ities
i=1 and {xte
from {xtr
pte (x) and ptr (x) when estimating the importance w(x).

(1)

Least-squares Approach: Let us model the importance w(x) by the following linear model:
bw(x) = α⊤ϕ(x),
where ⊤ denotes the transpose, α = (α1 , . . . , αb )⊤ , is a parameter to be learned, b is the number of
parameters, ϕ(x) = (ϕ1 (x), . . . , ϕb (x))⊤ are basis functions such that ϕ(x) ≥ 0b for all x ∈ D ,
0b denotes the b-dimensional vector with all zeros, and the inequality for vectors is applied in the
ℓ=1 could be dependent on the samples i.e., kernel
element-wise manner. Note that b and {ϕℓ (x)}b
models are also allowed. We explain how the basis functions {ϕℓ (x)}b
ℓ=1 are chosen later.
We determine the parameter α so that the following squared error is minimized:
2 R (cid:16) bw(x) − pte (x)
ptr (x) (cid:17)2
2 R bw(x)2 ptr (x)dx − R bw(x)pte (x)dx + C,
ptr (x)dx = 1
J0 (α) = 1
2 R w(x)pte (x)dx is a constant and therefore can be safely ignored. Let
where C = 1
2 α⊤H α − h⊤α,
J (α) = J0 (α) − C = 1
(2)
where H = R ϕ(x)ϕ(x)⊤ ptr (x)dx, h = R ϕ(x)pte (x)dx. Using the empirical approximation
and taking into account the non-negativity of the importance function w(x), we obtain
minα∈Rb h 1
b αi
2 α⊤cH α − bh
α + λ1⊤
nte Pnte
ntr Pntr
bh = 1
where cH = 1
j ). λ1⊤
b α is a regularization term
j=1 ϕ(xte
i=1 ϕ(xtr
i )ϕ(xtr
i )⊤ ,
for avoiding overﬁtting, λ ≥ 0, and 1b is the b-dimensional vector with all ones.
The above problem is a convex quadratic program and therefor e the global optimal solution can be
obtained by a standard software. We call this method Least-Squares Importance Fitting (LSIF).

s.t. α ≥ 0b ,

(3)

⊤

2

Convergence Analysis of LSIF: Here, we theoretically analyze the convergence property of the
solution bα of the LSIF algorithm. Let α∗ be the optimal solution of the ‘ideal’ problem:
minα∈Rb h 1
b αi
2 α⊤H α − h⊤α + λ1⊤
s.t. α ≥ 0b .
Let f (n) = ω(g(n)) mean that f (n) asymptotically dominates g(n), i.e., for all C > 0, there exists
n0 such that |C g(n)| < |f (n)| for all n > n0 . Then we have the following theorem.

(4)

Theorem 1 Assume that (a) the optimal solution of the problem (4) satis ﬁes the strict comple-
mentarity condition, and (b) ntr and nte satisfy nte = ω(n2
tr ). Then we have E[J ( bα)] =
J (α∗ ) + O (cid:0)n−1
tr (cid:1), where E denotes the expectation over all possible training samples of size ntr
and all possible test samples of size nte .

Theorem 1 guarantees that LSIF converges to the ideal solution with order n−1
tr . It is possible to
explicitly obtain the coefﬁcient of the term of order n−1
tr , but we omit the detail due to lack of space.

Model Selection for LSIF: The performance of LSIF depends on the choice of the regularization
parameter λ and basis functions {ϕℓ (x)}b
ℓ=1 (which we refer to as a model). Since our objective is
to minimize the cost function J , it is natural to determine the model such that J is minimized.
Here, we employ cross-validation for estimating J ( bα), which has an accuracy guarantee for ﬁnite
i }ntr
j }nte
samples: First, the training samples {xtr
i=1 and test samples {xte
j=1 are divided into R disjoint
r=1 , respectively. Then an importance estimate bwr (x) is obtained using
r=1 and {X te
subsets {X tr
r }R
r }R
r and X te
j }j 6=r , and the cost J is approximated using the held-out samples X tr
j }j 6=r and {X te
{X tr
r
r | Pxtr∈X tr
r | Pxte∈X te
as bJ (CV)
r bwr (xtr )2 − 1
= 1
r bwr (xte ). This procedure is repeated for
r
|X te
2|X tr
r = 1, . . . , R and its average bJ (CV) is used as an estimate of J . We can show that bJ (CV) gives an
almost unbiased estimate of the true cost J , where the ‘almost’-ness comes from the fact that the
number of samples is reduced due to data splitting.

(5)

Heuristics of Basis Function Design: A good model may be chosen by cross-validation, given
that a family of promising model candidates is prepared. As model candidates, we propose using a
j }nte
Gaussian kernel model centered at the test input points {xte
j=1 , i.e.,
where Kσ (x, x′ ) = exp (cid:0)−kx − x′ k2 /(2σ2 )(cid:1) .
bw(x) = Pnte
ℓ=1 αℓKσ (x, xte
ℓ ),
j }nte
j=1 as the Gaussian centers, not the training
The reason why we chose the test input points {xte
i }ntr
i=1 , is as follows. By deﬁnition, the importance w(x) tends to take large values
input points {xtr
if the training input density ptr (x) is small and the test input density pte (x) is large; conversely,
w(x) tends to be small (i.e., close to zero) if ptr (x) is large and pte (x) is small. When a function
is approximated by a Gaussian kernel model, many kernels may be needed in the region where the
output of the target function is large; on the other hand, only a small number of kernels would be
enough in the region where the output of the target function is close to zero. Following this heuristic,
we allocate many kernels at high test input density regions, which can be achieved by setting the
j }nte
j=1 .
Gaussian centers at the test input points {xte
j }nte
i }ntr
j=1 . How-
i=1 and {xte
Alternatively, we may locate (ntr + nte ) Gaussian kernels at both {xtr
ever, in our preliminary experiments, this did not further improve the performance, but just slightly
j }nte
increased the computational cost. When nte is large, just using all the test input points {xte
j=1 as
Gaussian centers is already computationally rather demand ing. To ease this problem, we practically
j }nte
j=1 as Gaussian centers for computational efﬁciency, i.e.,
propose using a subset of {xte
bw(x) = Pb
ℓ=1 αℓKσ (x, cℓ ),
j }nte
where cℓ is a template point randomly chosen from {xte
j=1 and b (≤ nte ) is a preﬁxed number.
In the experiments shown later, we ﬁx the number of template p oints at b = min(100, nte ), and
optimize the kernel width σ and the regularization parameter λ by cross-validation with grid search.

(6)

3

Entire Regularization Path for LSIF: We can show that the LSIF solution bα is piecewise linear
with respect to the regularization parameter λ. Therefore, the regularization path (i.e., solutions for
all λ) can be computed efﬁciently based on the parametric optimization technique [4].
A basic idea of regularization path tracking is to check the v iolation of the Karush-Kuhn-
Tucker (KKT) conditions —which are necessary and sufﬁcient c
onditions for optimality of convex
programs —when the regularization parameter
λ is changed. Although the detail of the algorithm
is omitted due to lack of space, we can show that a quadratic programming solver is no longer
needed for obtaining the entire solution path of LSIF—just co mputing matrix inverses is enough.
This highly contributes to saving the computation time. However, in our preliminary experiments,
the regularization path tracking algorithm is turned out to be numerically rather unreliable since the
numerical errors tend to be accumulated when tracking the re gularization path. This seems to be a
common pitfall of solution path tracking algorithms in general.

3 Approximation Algorithm

Unconstrained Least-squares Approach: The approximation idea we introduce here is very sim-
ple: we ignore the non-negativity constraint of the parameters in the optimization problem (3). Thus
minβ∈Rb h 1
2 β⊤βi .
⊤
2 β⊤cH β − bh
β + λ
In the above, we included a quadratic regularization term λβ⊤β/2, instead of the linear one λ1⊤
b α
since the linear penalty term does not work as a regularizer w ithout the non-negativity constraint.
Eq.(7) is an unconstrained convex quadratic program, so the solution can be analytically computed.
However, since we dropped the non-negativity constraint β ≥ 0b , some of the learned parameters
could be negative. To compensate for this approximation error, we modify the solution by
eβ = (cH + λI b )−1 bh,
bβ = max(0b , eβ),
where I b is the b-dimensional identity matrix and the ‘max’ operation for vectors is applied in the
element-wise manner. This is the solution of the approximation method we propose in this section.

(7)

(8)

An advantage of the above unconstrained formulation is that the solution can be computed just by
solving a system of linear equations. Therefore, the computation is fast and stable. We call this
method unconstrained LSIF (uLSIF). Due to the ℓ2 regularizer, the solution tends to be close to
0b to some extent. Thus, the effect of ignoring the non-negativity constraint may not be so strong.
Below, we theoretically analyze the approximation error of uLSIF.

Convergence Analysis of uLSIF: Here, we theoretically analyze the convergence property of
the solution bβ of the uLSIF algorithm. Let β∗ be the optimal solution of the ‘ideal’ problem:
β∗ = max(0b , β◦ ), where β◦ = argminβ∈Rb h 1
2 β⊤βi. Then we have
2 β⊤H β − h⊤β + λ
Theorem 2 Assume that (a) β ◦
ℓ 6= 0 for ℓ = 1, . . . , b, and (b) ntr and nte satisfy nte = ω(n2
tr ).
tr (cid:1).
Then we have E[J ( bβ)] = J (β∗ ) + O (cid:0)n−1
Theorem 2 guarantees that uLSIF converges to the ideal solut ion with order n−1
tr . It is possible to
explicitly obtain the coefﬁcient of the term of order n−1
tr , but we omit the detail due to lack of space.
We can also derive upper bounds on the difference between LSIF and uLSIF and show that uLSIF
gives a good approximation to LSIF. However, we do not go into the detail due to space limitation.

Ef ﬁcient Computation of Leave-one-out Cross-validation S core: Another practically very im-
portant advantage of uLSIF is that the score of leave-one-out cross-validation (LOOCV) can also
be computed analytically—thanks to this property, the compu
tational complexity for performing
LOOCV is the same order as just computing a single solution. In the current setting, we are given
j }nte
i }ntr
j=1 , which generally have different sample size. For sim-
i=1 and {xte
two sets of samples, {xtr
i are
i and the i-th test sample xte
plicity, we assume that ntr < nte and the i-th training sample xtr
j }nte
j=ntr+1 are always used for importance estimation.
held out at the same time; the test samples {xte

4

(i)
Let bβ
i and the i-th test sample xte
λ be a parameter learned without the i-th training sample xtr
i .
ntr Pntr
(i)
(i)
i )⊤ bβ
i )⊤ bβ
1
i=1 [ 1
Then the LOOCV score is expressed as
λ ]. Our ap-
λ )2 − ϕ(xte
2 (ϕ(xtr
proach to efﬁciently computing the LOOCV score is to use the Sherman-Woodbury-Morrison for-
(i)
(i)
λ = max{0b , (ntr−1)nte
λ can be expressed as bβ
mula for computing matrix inverses — bβ
ntr (nte−1) (a +
ntr (nte−1) (atr + a⊤
a⊤ϕ(xtr
teϕ(xtr
i )·atr
i )·ate
) − (ntr−1)
)}, where, a = A−1 bh, atr = A−1ϕ(xtr
i ), ate =
ntr−ϕ(xtr
ntr−ϕ(xtr
i )⊤ate
i )⊤atr
i ), A = cH + (ntr−1)λ
A−1ϕ(xte
I b . This implies that the matrix inverse needs to be computed only
ntr
once (i.e., A−1 ) for calculating LOOCV scores. Thus LOOCV can be carried out very efﬁciently
without repeating hold-out loops.

4 Relation to Existing Methods

Kernel density estimator (KDE) is a non-parametric technique to estimate a probability density func-
tion. KDE can be used for importance estimation by ﬁrst estim ating bptr (x) and bpte (x) separately
i }ntr
j }nte
from {xtr
i=1 and {xte
j=1 and then estimating the importance by bw(x) = bpte (x)/ bptr (x). KDE
is efﬁcient in computation since no optimization is involve d, and model selection is possible by
likelihood cross validation. However, KDE may suffer from the curse of dimensionality.
The kernel mean matching (KMM) method allows us to directly obtain an estimate of the importance
values at training points without going through density estimation [5]. KMM can overcome the curse
of dimensionality by directly estimating the importance using a special property of the Gaussian
reproducing kernel Hilbert space. However, there is no obje ctive model selection method for the
regularization parameter and kernel width. As for the regul arization parameter, we may follow a
suggestion in the original paper, which is justi ﬁed by a theo retical argument to some extent [5].
As for the Gaussian width, we may adopt a popular heuristic to use the median distance between
samples, although there seems no strong justi ﬁcation for th is. The computation of KMM is rather
demanding since a quadratic programming problem has to be so lved.

Other approaches to directly estimating the importance is to directly ﬁt an importance model to the
true importance—a method based on
logistic regression (LogReg) [1], or a method based on the
kernel model (6) (which is called the Kullback-Leibler importance estimation procedure, KLIEP)
[9, 6]. Model selection of these methods is possible by cross-validation, which is a signi ﬁcant
advantage over KMM. However, LogReg and KLIEP are computationally rather expensive since
non-linear optimization problems have to be solved.

The proposed LSIF is qualitatively similar to LogReg and KLIEP, i.e., it can avoid density estima-
tion, model selection is possible, and non-linear optimization is involved. However, LSIF is advan-
tageous over LogReg and KLIEP in that it is equipped with a regularization path tracking algorithm.
Thanks to this, model selection of LSIF is computationally much more efﬁcient than LogReg and
KLIEP. However, the regularization path tracking algorithm tends to be numerically unstable.

The proposed uLSIF inherits good properties of existing methods such as no density estimation
involved and a build-in model selection method equipped. In addition to these preferable properties,
the solution of uLSIF can be computed analytically through matrix inversion and therefore uLSIF
is computationally very efﬁcient and numerically stable. F urthermore, the closed-form solution of
uLSIF allows us to compute the LOOCV score analytically without repeating hold-out loops, which
highly contributes to reducing the computation time in the model selection phase.

5 Experiments

Importance Estimation: Let ptr (x) be the d-dimensional normal distribution with mean zero and
covariance identity; let pte (x) be the d-dimensional normal distribution with mean (1, 0, . . . , 0)⊤
i )}ntr
and covariance identity. The task is to estimate the importance at training input points: {w(xtr
i=1 .
We ﬁxed the number of test input points at nte = 1000 and consider the following two settings for
the number ntr of training samples and the input dimension d: (a) ntr = 100 and d = 1, 2, . . . , 20,
(b) d = 10 and ntr = 50, 60, . . . , 150. We run the experiments 100 times for each d, each ntr , and
each method, and evaluate the quality of the importance estimates { bwi }ntr
i=1 by the normalized mean
5

10−3

10−4

10−5

10−6

10−3

10−4

10−5

10−6

)
e
l
a
c
S
 
g
o
L
 
n
i
(
 
s
l
a
i
r
T
 
0
0
1
 
r
e
v
o
 
E
S
M
N
 
e
g
a
r
e
v
A

)
e
l
a
c
S
 
g
o
L
 
n
i
(
 
s
l
a
i
r
T
 
0
0
1
 
r
e
v
o
 
E
S
M
N
 
e
g
a
r
e
v
A

KDE
KMM
LogReg
KLIEP
uLSIF

5

15
10
d (Input Dimension)

20

(a) When d is changed

KDE
KMM
LogReg
KLIEP
uLSIF

5

10
d (Input Dimension)

15

20

(a) When d is changed

]
c
e
s
[
 
e
m
i
T
 
n
o
i
t
a
t
u
p
m
o
C

0.12

0.1

0.08

0.06

0.04

0.02

0

0.15

0.1

0.05

]
c
e
s
[
 
e
m
i
T
 
n
o
i
t
a
t
u
p
m
o
C

]
c
e
s
[
 
e
m
i
T
 
n
o
i
t
a
t
u
p
m
o
C
 
l
a
t
o
T

]
c
e
s
[
 
e
m
i
T
 
n
o
i
t
a
t
u
p
m
o
C
 
l
a
t
o
T

15

10

5

0

12

10

8

6

4

2

LogReg
uLSIF

5

10
d (Input Dimension)

15

20

(a) When d is changed

50

100
ntr (Number of Training Samples)

150

(b) When ntr is changed

0
50

100
ntr (Number of Training Samples)

150

0
50

(b) When ntr is changed

100
ntr (Number of Training Samples)

150

(b) When ntr is changed

Figure 1: NMSEs averaged
over 100 trials in log scale.

Figure 2: Mean computation
time (after model selection)
over 100 trials.

Figure 3: Mean computation
time (including model selec-
tion of σ and λ over 9× 9 grid).
ntr Pntr
i ))2 , where Pntr
i ) and Pntr
squared error (NMSE): 1
i ) are
i=1 ( bw(xtr
i ) − w(xtr
i=1 bw(xtr
i=1 w(xtr
normalized to be one, respectively.
NMSEs averaged over 100 trials (a) as a function of input dimension d and (b) as a function of
the training sample size ntr are plotted in log scale in Figure 1. Error bars are omitted fo r clear
visibility—instead, the best method in terms of the mean erro
r and comparable ones based on the
t-test at the signi ﬁcance level 1% are indicated by ‘◦’; the methods with signi ﬁcant difference are
indicated by ‘×’. Figure 1(a) shows that the error of KDE sharply increases a s the input dimension
grows, while LogReg, KLIEP, and uLSIF tend to give much smaller errors than KDE. This would
be the fruit of directly estimating the importance without going through density estimation. KMM
tends to perform poorly, which is caused by an inappropriate choice of the Gaussian kernel width.
This implies that the popular heuristic of using the median distance between samples as the Gaussian
width is not always appropriate. On the other hand, model selection in LogReg, KLIEP, and uLSIF
seems to work quite well. Figure 1(b) shows that the errors of all methods tend to decrease as the
number of training samples grows. Again LogReg, KLIEP, and uLSIF tend to give much smaller
errors than KDE and KMM.

Next we investigate the computation time. Each method has a different model selection strategy,
i.e., KMM does not involve any cross-validation, KDE and KLIEP involve cross-validation over
the kernel width, and LogReg and uLSIF involve cross-validation over both the kernel width and
the regularization parameter. Thus the naive comparison of the total computation time is not so
meaningful. For this reason, we ﬁrst investigate the comput ation time of each importance estimation
method after the model parameters are ﬁxed. The average CPU c omputation time over 100 trials
are summarized in Figure 2. Figure 2(a) shows that the computation time of KDE, KLIEP, and
uLSIF is almost independent of the input dimensionality d, while that of KMM and LogReg is
rather dependent on d. Among them, the proposed uLSIF is one of the fastest methods. Figure 2(b)
shows that the computation time of LogReg, KLIEP, and uLSIF is nearly independent of the training
sample size ntr , while that of KDE and KMM sharply increase as ntr increases.
Both LogReg and uLSIF have very good accuracy and their computation time after model selection
is comparable. Finally, we compare the entire computation t ime of LogReg and uLSIF including
cross-validation, which is summarized in Figure 3. We note that the Gaussian width σ and the
regularization parameter λ are chosen over the 9 × 9 equidistant grid in this experiment for both
LogReg and uLSIF. Therefore, the comparison of the entire computation time is fair. Figures 3(a)
and 3(b) show that uLSIF is approximately 5 to 10 times faster than LogReg.

6

Overall, uLSIF is shown to be comparable to the best existing method (LogReg) in terms of the
accuracy, but is computationally more efﬁcient than LogReg .

Covariate Shift Adaptation in Regression and Classi ﬁcatio n: Next, we illustrate how the im-
portance estimation methods could be used in covariate shift adaptation [8, 5, 1, 9]. Covariate shift is
a situation in supervised learning where the input distribu tions change between the training and test
phases but the conditional distribution of outputs given inputs remains unchanged. Under covariate
shift, standard learning techniques such as maximum likelihood estimation or cross-validation are
biased; the bias caused by covariate shift can be asymptotically canceled by weighting the samples
i }ntr
according to the importance. In addition to training input samples {xtr
i=1 following a training
j }nte
j=1 following a test input density pte (x), suppose
input density ptr (x) and test input samples {xte
i }ntr
i }ntr
i=1 are given. The task is to
that training output samples {y tr
i=1 at the training input points {xtr
predict the outputs for test inputs.

We use the kernel model

bf (x; θ) = Pt
ℓ=1 θℓKh (x, mℓ )
for function learning, where Kh (x, x′ ) is the Gaussian kernel (5) and mℓ is a template point ran-
j }nte
j=1 . We set the number of kernels at t = 50. We learn the parameter θ by
domly chosen from {xte
importance weighted regularized least-squares (IWRLS):
minθ h Pntr
i ) (cid:16) bf (xtr
+ γ kθk2 i.
i (cid:17)2
i=1 bw(xtr
i ; θ) − y tr
It is known that IWRLS is consistent when the true importance w(xtr
i ) is used as weights —
unweighted RLS is not consistent due to covariate shift, given that the true learning target function
f (x) is not realizable by the model bf (x) [8].
The kernel width h and the regularization parameter γ in IWRLS (9) are chosen by importance
weighted CV (IWCV) [9]. More speci ﬁcally, we ﬁrst divide the training sam ples {z tr
| z tr
i =
i
r=1 . Then a function bfr (x) is learned using {Z tr
i )}ntr
i=1 into R disjoint subsets {Z tr
i , y tr
(xtr
r }R
j }j 6=r
by IWRLS and its mean test error for the remaining samples Z tr
r is computed:
r bw(x)loss (cid:16) bfr (x), y(cid:17) ,
r | P(x,y)∈Z tr
1
(10)
|Z tr
where loss (by , y) is (by − y)2 in regression and 1
2 (1 − sign{byy}) in classi ﬁcation. We repeat this
procedure for r = 1, . . . , R and choose the kernel width h and the regularization parameter γ so
that the average of the above mean test error over all r is minimized. We set the number of folds in
IWCV at R = 5. IWCV is shown to be an (almost) unbiased estimator of the generalization error,
while unweighted CV with misspeci ﬁed models is biased due to covariate shift.

(9)

The datasets provided by DELVE and IDA are used for performance evaluation, where training in-
put points are sampled with bias in the same way as [9]. We set the number of samples at ntr = 100
and nte = 500 for all datasets. We compare the performance of KDE, KMM, LogReg, KLIEP, and
uLSIF, as well as the uniform weight (Uniform, i.e., no adaptation is made). The experiments are
nte Pnte
j=1 loss( bf (xte
1
repeated 100 times for each dataset and evaluate the mean test error:
j ).
j ), y te
The results are summarized in Table 1, where all the error val ues are normalized by that of the uni-
form weight (no adaptation). For each dataset, the best method and comparable ones based on the
Wilcoxon signed rank test at the signi ﬁcance level 1% are described in bold face. The upper half cor-
responds to regression datasets taken from DELVE while the lower half correspond to classi ﬁcation
datasets taken from IDA.

The table shows that the generalization performance of uLSIF tends to be better than that of Uniform,
KDE, KMM, and LogReg, while it is comparable to the best existing method (KLIEP). The mean
computation time over 100 trials is described in the bottom row of the table, where the value is
normalized so that the computation time of uLSIF is one. This shows that uLSIF is computationally
more efﬁcient than KLIEP. Thus, proposed uLSIF is overall sh own to work well in covariate shift
adaptation with low computational cost.

Outlier Detection: Here, we consider an outlier detection problem of ﬁnding irr egular samples
in a dataset ( “evaluation dataset ”) based on another datase
t ( “model dataset ”) that only contains

7

Table 1: Covariate shift adaptation. Mean and standard
deviation of test error over 100 trials (smaller is better).
LogReg
KMM
KDE
Uniform
KLIEP
uLSIF
Dataset
1.31(0.39) ◦ 0.95(0.31) ◦ 1.02(0.33)
kin-8fh
1.00(0.34)
1.22(0.52)
1.55(0.39)
1.38(0.57) ◦ 0.86(0.35) ◦ 0.88(0.39)
1.84(0.58)
1.12(0.57)
1.00(0.39)
kin-8fm
1.09(0.19) ◦ 0.99(0.22) ◦ 1.02(0.18)
◦ 1.00(0.26)
kin-8nh
1.19(0.29)
1.09(0.20)
kin-8nm ◦ 1.00(0.30)
1.12(0.21) ◦ 0.97(0.25)
1.14(0.26)
1.20(0.20)
1.04(0.25)
1.02(0.41) ◦ 0.91(0.38) ◦ 0.97(0.49) ◦ 0.97(0.69) ◦ 0.96(0.61)
◦ 1.00(0.50)
abalone
1.08(0.54) ◦ 0.98(0.46) ◦ 0.94(0.44) ◦ 0.98(0.47)
◦ 1.00(0.51)
0.98(0.45)
image
0.87(0.04) ◦ 0.87(0.04)
ringnorm
1.00(0.04)
0.95(0.08)
0.99(0.06)
0.91(0.08)
1.16(0.71) ◦ 0.94(0.57) ◦ 0.91(0.61) ◦ 0.91(0.52) ◦ 0.88(0.57)
1.00(0.58)
twonorm
0.98(0.31) ◦ 0.93(0.32) ◦ 0.93(0.34) ◦ 0.92(0.32)
waveform
1.05(0.47)
1.00(0.45)
0.96(0.36)
0.95(0.35)
1.07(0.37)
1.17(0.37)
1.07(0.40)
Average
1.00(0.38)
Time
0.82
3.50
3.27
3.64
1.00

Table 2: Outlier detection. Mean AUC
values over 20 trials (larger is better).
Dataset
uLSIF KLIEP LogReg KMM OSVM LOF KDE
.915 .934
.360
.578
.447
.815
.851
banana
.488 .400
.508
.576
.627
.480
.463
b-cancer
diabetes
.558
.615
.599
.574
.563
.403 .425
.441 .378
.522
.494
.438
.485
.416
f-solar
.559 .561
.535
.529
.556
.572
.574
german
.659 .638
.681
.623
.833
.647
.659
heart
.930 .916
.540
.813
.600
.828
.812
image
.778 .845
.737
.541
.368
.748
.713
splice
.111 .256
.504
.681
.745
.720
.534
thyroid
titanic
.525
.534
.602
.502
.456
.525 .461
.889 .875
.846
.439
.161
.902
.905
t-norm
.887 .861
.861
.477
.243
.881
.890
w-form
.629 .623
.596
.608
.530
.685
.661
Average
Time
1.00
11.7
5.35
751
12.4
85.5 8.70

regular samples. Deﬁning the importance over two sets of sam ples, we can see that the importance
values for regular samples are close to one, while those for o utliers tend to be signi ﬁcantly deviated
from one. Thus the importance values could be used as an index of the degree of outlyingness in
this scenario. Since the evaluation dataset has wider support than the model dataset, we regard the
evaluation dataset as the training set (i.e., the denominator in the importance) and the model dataset
as the test set (i.e., the numerator in the importance). Then outliers tend to have smaller importance
values (i.e., close to zero).

We again test KMM, LogReg, KLIEP, and uLSIF for importance estimation; in addition, we test
native outlier detection methods such as the one-class support vector machine (OSVM) [7], the
local outlier factor (LOF) [3], and the kernel density estimator (KDE). The datasets provided by
IDA are used for performance evaluation. These datasets are binary classi ﬁcation datasets consisting
of training and test samples. We allocate all positive training samples for the “model ” set, while all
positive test samples and 1% of negative test samples are assigned in the “evaluation” se
t. Thus, we
regard the positive samples as regular and the negative samp les as irregular.

The mean AUC values over 20 trials as well as the computation time are summarized in Table 2,
showing that uLSIF works fairly well. KLIEP works slightly better than uLSIF, but uLSIF is com-
putationally much more efﬁcient. LogReg overall works rath er well, but it performs poorly for
some datasets and therefore the average AUC value is small. KMM and OSVM are not comparable
to uLSIF both in AUC and computation time. LOF and KDE work reasonably well in terms of
AUC, but their computational cost is high. Thus, proposed uLSIF is overall shown to work well and
computationally efﬁcient also in outlier detection.

6 Conclusions

We proposed a new method for importance estimation that can avoid solving a substantially more
difﬁcult task of density estimation. We are currently explo ring various possible applications of
important estimation methods beyond covariate shift adaptation and outlier detection, e.g., feature
selection, conditional distribution estimation, and independent component analysis —we believe that
importance estimation could be used as a new versatile tool in machine learning.

References
[1] S. Bickel et al. Discriminative learning for differing training and test distributions. ICML 2007.
[2] S. Bickel et al. Dirichlet-enhanced spam ﬁltering based on biased sa mples. NIPS 2006.
[3] M. M. Breunig et al. LOF: Identifying density-based local outliers. SIGMOD 2000.
[4] T. Hastie et al. The entire regularization path for the support vector m achine. JMLR 2004.
[5] J. Huang et al. Correcting sample selection bias by unlabeled data. NIPS 2006.
[6] X. Nguyen et al. Estimating divergence functions and the likelihood ratio. NIPS 2007.
[7] B. Sch ¨olkopf et al. Estimating the support of a high-dimensional distribution. Neural Computation,
13(7):1443–1471, 2001.
[8] H. Shimodaira.
Improving predictive inference under covariate s hift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.
[9] M. Sugiyama et al. Direct importance estimation with model selection. NIPS 2007.

8

—

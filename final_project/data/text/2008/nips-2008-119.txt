Designing neurophysiology experiments to optimally
constrain receptive ﬁeld models along parametric
submanifolds.

Jeremy Lewi ∗
School of Bioengineering
Georgia Institute of Technology
jeremy@lewi.us

Robert Butera
School of Electrical and Computer Engineering
Georgia Institute of Technology
rbutera@ece.gatech.edu

David M. Schneider
Departments of Neurobiology and Psychology
Columbia University
dms2159@columbia.edu

Sarah M. N. Woolley
Department of Psychology
Columbia University
sw2277@columbia.edu

Liam Paninski †
Department of Statistics and Center for Theoretical Neuroscience
Columbia University
liam@stat.columbia.edu

Abstract

Sequential optimal design methods hold great promise for improving the efﬁ-
ciency of neurophysiology experiments. However, previous methods for optimal
experimental design have incorporated only weak prior information about the un-
derlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld).
Here we describe how to use stronger prior information, in the form of paramet-
ric models of the receptive ﬁeld, in order to construct optimal stimuli and further
improve the efﬁciency of our experiments. For example, if we believe that the
receptive ﬁeld is well-approximated by a Gabor function, then our method con-
structs stimuli that optimally constrain the Gabor parameters (orientation, spatial
frequency, etc.) using as few experimental trials as possible. More generally, we
may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the
full parameter space; in this case, our method chooses stimuli in order to reduce
the uncertainty along the tangent space of this sub-manifold as rapidly as possible.
Applications to simulated and real data indicate that these methods may in many
cases improve the experimental efﬁciency.

1 Introduction

A long standing problem in neuroscience has been collecting enough data to robustly estimate the
response function of a neuron. One approach to this problem is to sequentially optimize a series
of experiments as data is collected [1, 2, 3, 4, 5, 6]. To make optimizing the design tractable, we
typically need to assume our knowledge has some nice mathematical representation. This restriction
often makes it difﬁcult to include the types of prior beliefs held by neurophysiologists; for example
that the receptive ﬁeld has some parametric form such as a Gabor function [7]. Here we consider
∗ http://www.lewilab.org
† http://www.stat.columbia.edu/∼liam/

1

Figure 1: A schematic illustrating how we use the manifold to improve stimulus design. Our
method begins with a Gaussian approximation of the posterior on the full model space after t trials,
p(~θ |~µt , C t ). The left panel shows an example of this Gaussian distribution when dim(~θ) = 2. The
next step involves constructing the tangent space approximation of the manifold M on which ~θ is be-
lieved to lie, as illustrated in the middle plot; M is indicated in blue. The MAP estimate (blue dot) is
projected onto the manifold to obtain ~µM,t (green dot). We then compute the tangent space (dashed
red line) by taking the derivative of the manifold at ~µM,t . The tangent space is the space spanned by
vectors in the direction parallel to M at ~µM,t . By deﬁnition, in the neighborhood of ~µM,t , moving
along the manifold is roughly equivalent to moving along the tangent space. Thus, the tangent space
provides a good local approximation of M. In the right panel we compute p(~θ |~µb,t , Cb,t ) by evalu-
ating p(~θ |~µt , C t ) on the tangent space. The resulting distribution concentrates its mass on models
which are probable under p(~θ |~µt , C t ) and close to the manifold.

the problem of incorporating this strong prior knowledge into an existing algorithm for optimizing
neurophysiology experiments [8].
We start by assuming that a neuron can be modeled as a generalized linear model (GLM). Our
prior knowledge deﬁnes a subset of all GLMs in which we expect to ﬁnd the best model of the
neuron. We represent this class as a sub-manifold in the parameter space of the GLM. We use the
manifold to design an experiment which will provide the largest reduction in our uncertainty about
the unknown parameters. To make the computations tractable we approximate the manifold using
the tangent space evaluated at the maximum a posteriori (MAP) estimate of the parameters projected
onto the manifold. Despite this rather crude approximation of the geometry of the manifold, our
simulations show that this method can signiﬁcantly improve the informativeness of our experiments.
Furthermore, these methods work robustly even if the best model does not happen to lie directly on
the manifold.

2 Methods

We begin by summarizing the three key elements of an existing algorithm for optimizing neuro-
physiology experiments. A more thorough discussion is available in [8]. We model the neuron’s
response function as a mapping between the neuron’s input at time t, ~st , and its response, rt . We
deﬁne the input rather generally as a vector which may consist of terms corresponding to a stimulus,
e.g. an image or a sound, or the past activity of the neuron itself, {rt−1 , rt−2 , . . .}. The response, rt ,
is typically a non-negative integer corresponding to the number of spikes observed in a small time
window. Since neural responses are typically noisy, we represent the response function as a con-
ditional distribution, p(rt |~st , ~θ). In this context, optimizing the experimental design means picking
the input for which observing the response will provide the most information about the parameters
~θ deﬁning the conditional response function.

2

θ2p(~θ|µt,Ct)θ1θ1θ2~µt~µM,tMT~µM,tMθ2p(~θ|µb,Cb)θ1,

(1)

λt = E (rt ) = f

The ﬁrst important component of this algorithm is the assumption that p(rt |~st , ~θ) can be adequately
approximated by a generalized linear model [9, 10]. The likelihood of the response depends on the
(cid:16)~θT ~st
(cid:17)
ﬁring rate, λt , which is a function of the input,
where f () is some nonlinear function which is assumed known1 . To identify the response function,
we need to estimate the coefﬁcients of the linear projection, ~θ . One important property of the GLM
is that we can easily derive sufﬁcient conditions to ensure the log-likelihood is concave [11].
The second key component of the algorithm is that we may reasonably approximate the posterior on
~θ as Gaussian. This approximation is justiﬁed by the log-concavity of the likelihood function and
asymptotic normality of the posterior distribution given sufﬁcient data [12]. As a result, we can re-
cursively compute a Gaussian approximation of the full posterior, p(~θ |r1:t , s1:t ) ≈ p(~θ |~µt , C t ) [8].
Here (~µt , C t ) denote the mean and covariance matrix of our Gaussian approximation: ~µt is set to
the MAP estimate of ~θ , and C t to the inverse Hessian of the log-posterior at ~µt .
The ﬁnal component is an efﬁcient method for picking the optimal input on the next trial, ~st+1 .
Since the purpose of an experiment is to identify the best model, we optimize the design by max-
imizing the conditional mutual information between rt+1 and ~θ given ~st+1 , I (~θ ; rt+1 |~st+1 ). The
mutual information measures how much we expect observing the response to ~st+1 will reduce our
uncertainty about ~θ . We pick the optimal input by maximizing the mutual information with respect
to ~st+1 ; as discussed in [8], this step, along with the updating of the posterior mean and covariance
(~µt , C t ), may be computed efﬁciently enough for real-time implementation in many cases.

2.1 Optimizing experiments to reduce uncertainty along parameter sub-manifolds.

For the computation of the mutual information to be tractable, the space of candidate models, Θ,
must have some convenient form so that we can derive a suitable expression for the mutual infor-
mation. Intuitively, to select the optimal design, we need to consider how much information an
experiment provides about each possible model. Evaluating the mutual information entails an in-
tegral over model space, Θ. The problem with incorporating prior knowledge is that if we restrict
the model to some complicated subset of model space we will no longer be able to efﬁciently inte-
grate over the set of candidate models. We address this problem by showing how local geometric
approximations to the parameter sub-manifold can be used to guide optimal sampling while still
maintaining a ﬂexible, tractable representation of the posterior distribution on the full model space.
In many experiments, neurophysiologists expect a-priori that the receptive ﬁeld of a neuron will have
some low-dimensional parametric structure; e.g the receptive ﬁeld might be well-approximated by
a Gabor function [13], or by a difference of Gaussians [14], or by a low rank spatiotemporal matrix
[15, 13]. We can think of this structure as deﬁning a sub-manifold, M, of the full model space, Θ,
M = {~θ : ~θ = Ψ(~φ ), ∀~φ}.

(2)

The vector, ~φ , essentially enumerates the points on the manifold and Ψ() is a function which maps
these points into Θ space. A natural example is the case where we wish to enforce the constraint
that ~θ has some parametric form, e.g. a Gabor function. The basic idea is that we want to run
experiments which can identify exactly where on the manifold the optimal model lies.
Since M can have some arbitrary nonlinear shape, computing the informativeness of a stimulus
using just the models on the manifold is not easy. Furthermore, if we completely restrict our attention
to models in M then we ignore the possibility that our prior knowledge is incorrect. Hence, we do
not force the posterior distribution of ~θ to only have support on the manifold. Rather, we maintain a
Gaussian approximation of the posterior on the full space, Θ. However, when optimizing our stimuli
we combine our posterior with our knowledge of M in order to do a better job of maximizing the
informativeness of each experiment.

1 It is worth noting that this simple GLM can be generalized in a number of directions; we may include
spike-history effects, nonlinear input terms, and so on [10].

3

Computing the mutual information I (rt+1 ; ~θ |~st+1 , s1:t , r1:t ) entails an integral over model space
weighted by the posterior probability on each model. We integrate over model space because the
informativeness of an experiment clearly depends on what we already know (i.e. the likelihood we
assign to each model given the data and our prior knowledge). Furthermore, the informativeness of
an experiment will depend on the outcome. Hence, we use what we know about the neuron to make
predictions about the experimental outcome. Unfortunately, since the manifold in general has some
arbitrary nonlinear shape we cannot easily compute integrals over the manifold. Furthermore, we
do not want to continue to restrict ourselves to models on the manifold if the data indicates our prior
knowledge is wrong.
We can solve both problems by making use of the tangent space of the manifold, as illustrated in
Figure 1 [16]. The tangent space is a linear space which provides a local approximation of the
manifold. Since the tangent space is a linear subspace of Θ, integrating over ~θ in the tangent space
is much easier than integrating over all ~θ on the manifold; in fact, the methods introduced in [8]
may be applied directly to this case. The tangent space is a local linear approximation evaluated at
a particular point, ~µM,t , on the manifold. For ~µM,t we use the projection of ~µt onto the manifold
(i.e., ~µM,t is the closest point in M to ~µt ). Depending on the manifold, computing ~µM,t can be
nontrivial; the examples considered in this paper, however, all have tractable numerical solutions to
this problem.
The challenge is representing the set of models close to ~µM,t in a way that makes integrating over the
models tractable. To ﬁnd models on the manifold close to ~µM,t we want to perturb the parameters
~φ about the values corresponding to ~µM,t . Since Ψ is in general nonlinear, there is no simple
expression for the combination of all such perturbations. However, we can easily approximate the
set of ~θ resulting from these perturbations by taking linear combinations of the partial derivatives
of Ψ with respect to ~φ . The partial derivative is the direction in Θ in which ~θ moves if we perturb
one of the manifold’s parameters. Thus, the subspace formed by linear combinations of the partial
derivatives approximates the set of models on the manifold close to ~µM,t . This subspace is the
tangent space,
(cid:18)(cid:20) ∂Ψ
(cid:21)(cid:19)
∂Ψ
T~µM,tM = {~θ : ~θ = ~µM,t + B~b, ∀~b ∈ Rdim(M) }
B = orth
(3)
. . .
,
∂φd
∂φ1
where orth is an orthonormal basis for the column space of its argument. Here TxM denotes the
tangent space at the point x. The columns of B denote the direction in which ~θ changes if we perturb
one of the manifold’s parameters. (In general, the directions corresponding to changes in different
parameters are not independent; to avoid this redundancy we compute a set of basis vectors for the
space spanned by the partial derivatives.)
We now use our Gaussian posterior on the full parameter space to compute the posterior likelihood
of the models in the tangent space. Since the tangent space is a subspace of Θ, restricting our
Gaussian approximation, p(~θ |~µt , C t ), to the tangent space means we are taking a slice through our
Gaussian approximation of the posterior. Mathematically, we are conditioning on ~θ ∈ T~µM,tM.
The result is a Gaussian distribution on the tangent space whose parameters may be obtained using
(cid:26) N (~b; ~µb,t , Cb,t )
the standard Gaussian conditioning formula:
ptan (~θ |~µb,t , Cb,t ) =
if
0
if
~µb,t = −Cb,tB T C −1
t (~µM,t − ~µt )
Cb,t = (B T C −1
t B )−1
(5)
where N denotes a normal distribution with the speciﬁed parameters. Now, rather than optimizing
the stimulus by trying to squeeze the uncertainty p(~θ |r1:t , s1:t , M) on the nonlinear manifold M
down as much as possible (a very difﬁcult task in general), we pick the stimulus which best reduces
the uncertainty ptan (~θ |~µb,t , Cb,t ) on the vector space T~µM,t . We can solve this latter problem di-
rectly using the methods presented in [8]. Finally, to handle the possibility that ~θ /∈ M, every so
often we optimize the stimulus using the full posterior p(~θ |~µt , C t ). This simple modiﬁcation en-
sures that asymptotically we do not ignore directions orthogonal to the manifold; i.e., that we do not

∃ ~b s.t ~θ = ~µM,t + B~b
~θ /∈ T~µM,t

(4)

4

Figure 2: MAP estimates of a STRF obtained using three designs: the new info. max.
tangent
space design described in the text; an i.i.d. design; and an info. max. design which did not use
the assumption that ~θ corresponds to a low rank STRF. In each case, stimuli were chosen under
the spherical power contraint, ||~st ||2 = c. The true STRF (ﬁt to real zebraﬁnch auditory responses
and then used to simulate the observed data) is shown in the last column. (For convenience we
rescaled the coefﬁcients to be between -4 and 4). We see that using the tangent space to optimize the
design leads to much faster convergence to the true parameters; in addition, either infomax design
signiﬁcantly outperforms the iid design here. In this case the true STRF did not in fact lie on the
manifold M (chosen to be the set of rank-2 matrices here); thus, these results also show that our
knowledge of M does not need to be exact in order to improve the experimental design.

get stuck obsessively sampling along the incorrect manifold. As a result, µt will always converge
asymptotically to the true parameters, even when θ 6∈ M .
To summarize, our method proceeds as follows:

0. Initial conditions: start with a log-concave (approximately Gaussian) posterior given t pre-
vious trials, summarized by the posterior mean, ~µt and covariance, C t .
1. Compute ~µM,t , the projection of ~µt on the manifold. (The procedure for computing ~µM,t
depends on the manifold.)
2. Compute the tangent space of M at ~µM,t using Eqn. 3.
3. Compute the posterior restricted to the tangent space, ptan (~θ |~µb,t , Cb,t ), using the standard
Gaussian conditioning formula (Eqn. 5).
4. Apply the methods in [8] to ﬁnd the optimal t + 1 stimulus, and observe the response rt+1 .
5. Update the posterior by recursively updating the posterior mean and covariance: ~µt →
~µt+1 and C t → C t+1 (again, as in [8]), and return to step 1.

3 Results

3.1 Low rank models

To test our methods in a realistic, high-dimensional setting, we simulated a typical auditory neu-
rophysiology [17, 15, 18] experiment. Here, the objective is to to identify the spectro-temporal
receptive ﬁeld (STRF) of the neuron. The input and receptive ﬁeld of the neuron are usually repre-
sented in the frequency domain because the cochlea is known to perform a frequency decomposition
of sound. The STRF, θ(τ , ω), is a 2-d ﬁlter which relates the ﬁring rate at time t to the amount of

5

info. max.tan. spacet=250t=500t=750t=1000qi.i.d.−20−100246info. max.fullFrequency(KHz)Time(ms)  −2024energy at frequency ω and time t − τ in the stimulus. To incorporate this spectrotemporal model in
the standard GLM setting, we simply vectorize the matrix θ(τ , ω).
Estimating the STRF can be quite difﬁcult due to its high dimensionality. Several researchers,
however, have shown that low-rank assumptions can be used to produce accurate approximations of
the receptive ﬁeld while signiﬁcantly reducing the number of unknown parameters [19, 13, 15, 20].
A low rank assumption is a more general version of the space-time separable assumption that is often
used when studying visual receptive ﬁelds [21]. Mathematically, a low-rank assumption means that
the matrix corresponding to the STRF can be written as a sum of rank one matrices,

Θ = M at ~θ = U V T

(6)

where M at indicates the matrix formed by reshaping the vector ~θ to form the STRF. U and V are
low-rank matrices with orthonormal columns. The columns of U and V are the principal components
of the column and row spaces of Θ respectively, and encode the spectral and temporal properties of
the STRF, respectively.
We simulated an auditory experiment using an STRF ﬁtted to the actual response of a neuron in the
Mesencephalicus lateralis pars dorsalis (MLd) of an adult male zebra ﬁnch [18]. To reduce the di-
mensionality we sub-sampled the STRF in the frequency domain and shortened it in the time domain
to yield a 20 × 21 STRF. We generated synthetic data by sampling a Poisson process whose instan-
taneous ﬁring rate was set to the output of a GLM with exponential nonlinearity and ~θ proportional
to the true measured zebra ﬁnch STRF.
For the manifold we used the set of ~θ corresponding to rank-2 matrices. For the STRF we used,
the rank-2 assumption turns out to be rather accurate. We also considered manifolds of rank-1 and
rank-5 matrices (data not shown), but rank-2 did slightly better. The manifold of rank r matrices
is convenient because we can easily project any ~θ onto M by reshaping ~θ as a matrix and then
computing its singular-value-decomposition (SVD). ~µM,t is the matrix formed by the ﬁrst r singular
vectors of ~µt . To compute the tangent space, Eqn. 3, we compute the derivative of ~θ with respect to
each component of the matrices U and V . Using these derivatives we can linearly approximate the
effect on Θ of perturbing the parameters of its principal components.
In Figure 3.1 we compare the effectiveness of different experimental designs by plotting the MAP
estimate ~µt on several trials. The results clearly show that using the tangent space to design the
experiments leads to much faster convergence to the true parameters. Furthermore, using the as-
sumption that the STRF is rank-2 is beneﬁcial even though the true STRF here is not in fact rank-2.

3.2 Real birdsong data

We also tested our method by using it to reshufﬂe the data collected during an actual experiment
to ﬁnd an ordering which provided a faster decrease in the error of the ﬁtted model. During the
experiments, we recorded the responses of MLd neurons when the songs of other birds and ripple
noise were presented to the bird (again, as previously described in [18]). We compared a design
log-likelihood of the spike trains, P
which randomly shufﬂed the trials to a design which used our info. max. algorithm to select the
order in which the trials are processed. We then evaluated the ﬁtted model by computing the expected
log p(rτ |~sτ , ~θ). τ denotes all the observations made
τ E~θ|~µt ,C t
when inputs in a test set are played to the bird.
To constrain the models we assume the STRF is low-rank and that its principal components are
smooth. The smoothing prior means that if we take the Fourier transform of the principal compo-
nents, the Fourier coefﬁcients of high frequencies should be zero with high probability. In other
words, each principal component (the columns of U and V ) should be a linear combination of sinu-
soidal functions with low frequencies. In this case we can write the STRF as
Θ = F νωηT T T .
(7)
Each column of F and T is a sine or cosine function representing one of the basis functions of
the principal spectral (columns of F ) or temporal (columns of T ) components of the STRF. Each
column of ν and η determines how we form one of the principal components by combining sine and
cosine functions. ω is a diagonal matrix which speciﬁes the projection of Θ onto each principal

6

Figure 3: Plots comparing the performance of an info. max. design, an info. max. design which
uses the tangent space, and a shufﬂed design. The manifold was the set of rank 2 matrices. The plot
shows the expected log-likelihood (prediction accuracy) of the spike trains in response to a birdsong
in the test set. Using a rank 2 manifold to constrain the model produces slightly better ﬁts of the
data.

component. The unknown parameters in this case are the matrices ν , η , and ω . The sinusoidal
functions corresponding to the columns of F and T should have frequencies {0, . . . , fo,f mf } and
{0, . . . , fo,tmt} respectively. fo,f and fo,t are the fundamental frequencies and are set so that 1
period corresponds to the dimensions of the STRF. mf and mt are the largest integers such that
fo,f mf and fo,tmt are less than the Nyquist frequency. Now to enforce a smoothing prior we can
simply restrict the columns of F and T to sinusoids with low frequencies. To project Θ onto the
manifold we simply need to compute ν, ω and η by evaluating the SVD of F T ΘT .
The results, Figure 3, show that both info. max. designs signiﬁcantly outperform the randomly
shufﬂed design. Furthermore, incorporating the low-rank assumption using the tangent space im-
proves the info. max. design, albeit only slightly; the estimated STRF’s are shown in Figure 4.
It is worth noting that in an actual online experiment, we would expect a larger improvement with
the info. max. design, since during the experiment we would be free to pick any input. Thus, the
different designs could choose radically different stimulus sets; in contrast, when re-analyzing the
data ofﬂine, all we can do is reshufﬂe the trials, but the stimulus sets remain the same in the info.
max. and iid settings here.

4 Conclusion

We have provided a method for incorporating detailed prior information in existing algorithms for
the information-theoretic optimal design of neurophysiology experiments. These methods use re-
alistic assumptions about the neuron’s response function and choose signiﬁcantly more informative
stimuli, leading to faster convergence to the true response function using fewer experimental trials.
We expect that the inclusion of this strong prior information will help experimentalists contend with
the high dimensionality of neural response functions.

5 Acknowledgments

We thank Vincent Vu and Bin Yu for helpful conversations. JL is supported by the Computa-
tional Science Graduate Fellowship Program administered by the DOE under contract DE-FG02-
97ER25308 and by the NSF IGERT Program in Hybrid Neural Microsystems at Georgia Tech via
grant number DGE-0333411. LP is supported by an NSF CAREER award and a Gatsby Initiative
in Brain Circuitry Pilot Grant.

7

103104−1−0.50trialEqlog p(r|st,qt)shuffled:Info. Max. full:Info. Max. Tan: rank=2Figure 4: The STRFs estimated using the bird song data. We plot ~µt for trials in the interval over
which the expected log-likelihood of the different designs differed the most in Fig. 3. The info. max.
designs converge slightly faster than the shufﬂed design. In these results, we smoothed the STRF by
only using frequencies less than or equal to 10fo,f and 2fo,t .

References
[1] P. Foldiak, Neurocomputing 38–40, 1217 (2001).
[2] R. C. deCharms, et al., Science 280, 1439 (1998).
[3] T. Gollisch, et al., Journal of Neuroscience 22, 10434 (2002).
[4] F. Edin, et al., Journal of Computational Neuroscience 17, 47 (2004).
[5] C. Machens, et al., Neuron 47, 447 (2005).
[6] K. N. O’Connor, et al., Journal of Neurophysiology 94, 4051 (2005).
[7] D. L. Ringach, J Neurophysiol 88, 455 (2002).
[8] J. Lewi, et al., Neural Computation 21 (2009).
[9] E. Simoncelli, et al., The Cognitive Neurosciences, M. Gazzaniga, ed. (MIT Press, 2004).
[10] L. Paninski, et al., Computational Neuroscience: Theoretical Insights into Brain Function
(Elsevier, 2007), chap. Statistical models for neural encoding, decoding, and optimal stimulus
design.
[11] L. Paninski, Network: Computation in Neural Systems 15, 243 (2004).
[12] L. Paninski, Neural Computation 17, 1480 (2005).
[13] A. Qiu, et al., J Neurophysiol 90, 456 (2003).
[14] C. Enroth-Cugell, et al., Journal of Physiology 187, 517 (1966).
[15] J. F. Linden, et al., Journal of Neurophysiology 90, 2660 (2003).
[16] J. M. Lee, Introduction to Smooth Manifolds (Springer, 2000).
[17] F. E. Theunissen, et al., Journal of Neuroscience 20, 2315 (2000).
[18] S. M. Woolley, et al., The Journal of Neuroscience 26, 2499 (2006).
[19] D. A. Depireux, et al., Journal of Neurophysiology 85, 1220 (2001).
[20] M. B. Ahrens, et al., Network 19, 35 (2008).
[21] G. C. DeAngelis, et al., J Neurophysiol 69, 1091 (1993).

8

shuffledTrial 1000Trial 2500Trial 5000Trial 7500Trial 10kTrial 20kTrial 50k  Info. Max. full−40−200246Info. Max. Tan rank=2Frequency (KHz)Time(ms)−202x 10−3
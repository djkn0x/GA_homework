On the Efﬁcient Minimization of Classiﬁcation
Calibrated Surrogates

Richard Nock
C EREGM IA — Univ. Antilles-Guyane
97275 Schoelcher Cedex, Martinique, France
rnock@martinique.univ-ag.fr

Frank Nielsen
L IX - Ecole Polytechnique
91128 Palaiseau Cedex, France
nielsen@lix.polytechnique.fr

Abstract

Bartlett et al (2006) recently proved that a ground condition for convex surrogates,
classiﬁcation calibration, ties up the minimization of the surrogates and classiﬁ-
cation risks, and left as an important problem the algorithmic questions about the
minimization of these surrogates. In this paper, we propose an algorithm which
provably minimizes any classiﬁcation calibrated surrogate strictly convex and dif-
ferentiable —
a set whose losses span the exponential, logistic and squared losses
—, with boosting-type guaranteed convergence rates under a weak learning as-
sumption. A particular subclass of these surrogates, that we call balanced convex
surrogates, has a key rationale that ties it to maximum likelihood estimation, zero-
sum games and the set of losses that satisfy some of the most common require-
ments for losses in supervised learning. We report experiments on more than 50
readily available domains of 11 ﬂa vors of the algorithm, that shed light on new
surrogates, and the potential of data dependent strategies to tune surrogates.

1 Introduction

A very active supervised learning trend has been ﬂourishing over the last decade: it studies functions
known as surrogates —
upperbounds of the empirical risk, generally with particular convexity prop-
erties —, whose minimization remarkably impacts on empirical / true risks minimization [3, 4, 10].
Surrogates play fundamental roles in some of the most successful supervised learning algorithms,
including AdaBoost, additive logistic regression, decision tree induction, Support Vector Machines
[13, 7, 10]. As their popularity has been rapidly spreading, authors have begun to stress the need to
set in order the huge set of surrogates, and better understand their properties. Statistical consistency
properties have been shown for a wide set containing most of the surrogates relevant to learning,
classiﬁcation calibrated surrogates (CC S) [3]; other important properties, like the algorithmic ques-
tions about minimization, have been explicitly left as important problems to settle [3].

In this paper, we address and solve this problem for all strictly convex differentiable CC S, a set
referred to as strictly convex surrogates (SC S). We propose a minimization algorithm, ULS, which
outputs linear separators, with two key properties: it provably achieves the optimum of the surrogate,
and meets Boosting-type convergence under a weak learning assumption. There is more, as we
show that SC S strictly contains another set of surrogates of important rationale, balanced convex
surrogates (BC S). This set, which contains the logistic and squared losses but not the exponential
loss, coincides with the set of losses satisfying three common requirements about losses in learning.
In fact, BC S spans a large subset of the expected losses for zero-sum games of [9], by which ULS may
also be viewed as an efﬁcient
learner for decision making (in simple environments, though).

Section 2 gives preliminary deﬁnitions;
section 3 presents surrogates losses and risks; sections 4 and
5 present ULS and its properties; section 6 discusses experiments with ULS; section 7 concludes.

2 Preliminary deﬁnitions

Unless otherwise stated, bold-faced variables like w denote vectors (components are w i , i =
1, 2, ...), calligraphic upper-cases like S denote sets, and blackboard faces like O denote subsets
of R, the set of real numbers. We let set O denote a domain (Rn , [0, 1]n , etc., where n is the
number of description variables), whose elements are observations. An example is an ordered pair
(o, c) ∈ O × {c− , c+ }, where {c− , c+ } denotes the set of classes (or labels), and c+ (resp. c− ) is
the positive class (resp. negative class). Classes are abstracted by a bijective mapping to one of two
other sets:

c ∈ {c− , c+ } (cid:11) y∗ ∈ {−1, +1} (cid:11) y ∈ {0, 1} .
(1)
The convention is c+ (cid:10) +1 (cid:10) 1 and c− (cid:10) −1 (cid:10) 0. We thus have three distinct notations for an
example: (o, c), (o, y ∗ ), (o, y), that shall be used without ambiguity. We suppose given a set of m
examples, S = {(oi , ci ), i = 1, 2, ..., m}. We wish to build a classi ﬁer H , which can either be a
function H : O → O ⊆ R (hereafter, O is assumed to be symmetric with respect to 0), or a function
H : O → [0, 1]. Following a convention of [6], we compute to which extent the outputs of H and
the labels in S disagree, ε(S , H ), by summing a loss which quanti ﬁes pointwise disagreements:
.= (cid:88)
ε(S , H )
(cid:96)(ci , H (oi )) .
i
The fundamental loss is the 0/1 loss, (cid:96)0/1 (c, H ) (to ease readability, the second argument is written
H instead of H (o)). It takes on two forms depending on im(H ):
[0,1] (y , H ) .= 1y (cid:54)=τ ◦H if im(H ) = [0, 1] .
R (y∗ , H ) .= 1y∗ (cid:54)=σ◦H if im(H ) = O , or (cid:96)0/1
(cid:96)0/1
(3)
The following notations are introduced in (3): for a clear distinction of the output of H , we put in
index to (cid:96) and ε an indication of the loss’ domain of parameters: R, meaning it is actually some
O ⊆ R, or [0, 1]. The exponent to (cid:96) gives the indication of the loss name. Finally, 1π is the indicator
variable that takes value 1 iff predicate π is true, and 0 otherwise; σ : R → {−1, +1} is +1 iff
x ≥ 0 and −1 otherwise; τ : [0, 1] → {0, 1} is 1 iff x ≥ 1/2, and 0 otherwise.
Both losses (cid:96)R and (cid:96)[0,1] are deﬁned simultaneously via popular transforms on H , such as the logit
transform logit(p) .= log(p/(1 − p)), ∀p ∈ [0, 1] [7]. We have indeed (cid:96)0/1
[0,1] (y , H ) = (cid:96)0/1
R (y∗ , logit(H ))
and (cid:96)0/1
R (y∗ , H ) = (cid:96)0/1
[0,1] (y , logit−1 (H )). We have implicitly closed the domain of the logit, adding
two symbols ±∞ to ensure that the eventual in ﬁnite values for H can be mapped back to [0, 1].
In supervised learning, the objective is to carry out the minimization of the expectation of the 0/1
loss in generalization, the so-called true risk. Very often however, this task can be relaxed to the
minimization of the empirical risk of H , which is (2) with the 0/1 loss [6]:
.= (cid:88)
ε0/1 (S , H )
i
The main classiﬁers we investigate are linear separators (LS). In this case, H (o) .= (cid:80)t αtht (o) for
features ht with im(ht ) ⊆ R and leveraging coefﬁcients αt ∈ R.
3 Losses and surrogates

(cid:96)0/1 (ci , H (oi )) .

(2)

(4)

A serious alternative to directly minimizing (4) is to rather focus on the minimization of a sur-
rogate risk [3]. This is a function ε(S , H ) as in (2) whose surrogate loss (cid:96)(c, H (o)) satisﬁes
(cid:96)0/1 (c, H (o)) ≤ (cid:96)(c, H (o)). Four are particularly important in supervised learning, deﬁned via
the following surrogate losses:
.= exp(−y∗H ) ,
(cid:96)exp
R (y∗ , H )
(5)
.= log(1 + exp(−y ∗H )) ,
(cid:96)log
R (y∗ , H )
(6)
.= (1 − y∗H )2 ,
(cid:96)sqr
R (y∗ , H )
(7)
.= max{0, 1 − y ∗H } .
(cid:96)hinge
R (y∗ , H )
(8)
(5) is the exponential loss, (6) is the logistic loss, (7) is the squared loss and (8) is hinge loss.
De ﬁnition 1 A Strictly Convex Loss (SCL) is a strictly convex function ψ : X → R+ differentiable
on int(X) with X symmetric interval with respect to zero, s. t. ∇ψ (0) < 0.

φ(x)

ˆPr[c = c+ |H ; o]
Fφ (y∗H )
aφ im(∇φ )
(cid:63)
= ∇−1
(−y∗H ) − aφ )/bφ
(H )
⊇ im(H ) = (φ
φ
−y∗H+√(1−µ)2+(y∗H )2
φµ,µ∈(0,1) (x) .= µ + (1 − µ)px(1 − x) µ
1
H
2√(1−µ)2+H 2
2 +
R
1−µ
φM (x) .= px(1 − x)
1
−y ∗H + p1 + (y∗H )2
0
H
2√1+H 2
2 +
φQ (x) .= −x log x − (1 − x) log(1 − x) 0
exp(H )
log(1 + exp(−y ∗H ))
R
1+exp(H )
φB (x) .= x(1 − x)
(1 − y ∗H )2
1
0
2 + H
[−1, 1]
2
Table 1: permissible functions, their corresponding BCLs and the matching [0, 1] predictions.

R

∇. is the gradient notation (here, the derivative). Any surrogate risk built from a SCL is called a
Strictly Convex Surrogate (SC S). From Theorem 4 in [3], it comes that SCL contains all classiﬁcation
calibrated losses (CCL) that are strictly convex and differentiable, such as (5), (6), (7).
.= supx(cid:48)∈int(X) {xx(cid:48) − ψ(x(cid:48) )}. Be-
Fix ψ ∈ SCL. The Legendre conjugate ψ (cid:63) of ψ is ψ(cid:63) (x)
cause of the strict convexity of ψ , the analytic expression of the Legendre conjugate becomes
.= x∇−1
ψ (x) − ψ(∇−1
ψ (x)). ψ(cid:63) is also strictly convex and differentiable. A function
ψ(cid:63) (x)
φ : [0, 1] → R+ is called permissible iff it is differentiable on (0, 1), strictly concave, symmet-
.= φ(1/2) − aφ > 0. Permissible
ric about x = 1/2, and with φ(0) = φ(1) = aφ ≥ 0. We let bφ
functions with aφ = 0 span a very large subset of generalized entropies [9]. Permissible func-
the following subclass of SCL, of particular interest (here, φ .= −φ).
tions are useful to deﬁne

 12

 10

 8

 6

 4

 2

 0

(f = f
B)
(f = f
M)
(f = fm = 1/3)
(f = f
Q)

-3

-2

-1

 0

 1

 2

 3

The Balanced Convex Loss

Deﬁnition 2 Let φ permissible.
(BCL) with signature φ, Fφ , is:
.= (φ

(cid:63)

Fφ (x)

(−x) − aφ )/bφ .
Balanced Convex Surrogates (BC S) are deﬁned accordingly. All
(cid:63)
(x) satisﬁes the following
BCL share a common shape. Indeed, φ
relationships:

(9)

(cid:63)

(cid:63)

(cid:63)

(10)
(11)

φ

φ

(x) = φ

(−x) + x ,
(x) = aφ .

Figure 1: Bold curves depict plots
(cid:63)
of φ
(−x) for the φ in Table 1; thin
lim
dotted half-lines are its asymptotes.
x→inf im(∇φ )
Noting that Fφ (0) = 1 and ∇Fφ (0) = −(1/bφ )∇−1
(0) < 0, it follows that BC S ⊂ SC S,
φ
where the strict inequality comes from the fact that (5) is a SCL but not a BCL.
It also follows
limx→supim(∇φ ) Fφ (x) = 0 from (11), and limx→inf im(∇φ ) Fφ (x) = −x/bφ from (10). We get that
the asymptotes of any BCL can be summarized as (cid:96)(x) .= x(σ(x) − 1)/(2bφ ). When bφ = 1, this is
the linear hinge loss [8], a generalization of (8) for which x .= y∗H − 1. Thus, while hinge loss is
not a BCL, it is the limit behavior of any BCL (see Figure 1).
Table 1 (left column) gives some examples of permissible φ. When scaled so that φ(1/2) = 1,
some confound with popular choices: φB with Gini index, φQ with the Bit-entropy, and φM with
Matsushita’s error [10, 11]. Table 1 also gives the expressions of Fφ along with the im(H ) = O ⊆ R
allowed by the BCL, for the corresponding permissible function. It is interesting to note the constraint
on im(H ) for the squared loss to be a BCL, which makes it monotonous in the interval, but implies
to rescale the outputs of classiﬁers
like linear separators to remain in [−1, 1].
4 ULS: the efﬁcient minimization of any SC S

For any strictly convex function ψ : X → R differentiable on int(X), the Bregman Loss Function
(BLF) Dψ with generator ψ is [5]:
.= ψ(x) − ψ(x(cid:48) ) − (x − x(cid:48) )∇ψ (x(cid:48) ) .
Dψ (x||x(cid:48) )
(12)
The following Lemma states some relationships that are easy to check using ψ (cid:63)(cid:63) = ψ . They are
particularly interesting when im(H ) = O ⊆ R.

Algorithm 1: Algorithm ULS(M , ψ )
Input: M ∈ Rm×T , SCL ψ with dom(ψ) = R;
Let α1 ← 0; Let w0 ← ∇−1
(0)1;
˜ψ
for j = 1, 2, ...J do
[WU] (weight update) wj ← (M αj ) (cid:5) w0 ;
Let Tj ⊆ {1, 2, ..., T }; let δj ← 0;
[LC] (leveraging coefﬁcients) ∀t ∈ Tj , pick δj,t such that: (cid:80)m
i=1 mit ((M δj ) (cid:5) wj )i = 0 ;
Let αj+1 ← αj + δj ;
Output: H (x) .= (cid:80)T
t=1 αJ +1,tht (x) ∈ LS

Lemma 1 For any SCL ψ , ψ(y ∗H ) = Dψ(cid:63) (0||∇−1
ψ(cid:63) (y∗H )) − ψ(cid:63) (0). Furthermore, for any BCL Fφ ,
(H )) = Dφ (1||∇−1
(H )) = bφFφ (y∗H ) and Dφ (y ||∇−1
Dφ (y ||∇−1
(y∗H )).
φ
φ
φ
The second equality is important because it ties real predictions (right) with [0, 1] predictions (left).
It also separates SCL and BCL, as for any ψ in SCL, it can be shown that there exists a functions ϕ
ϕ (H )) = ψ(y∗H ) iff ψ ∈ BCL. We now focus on the minimization of any SC S.
such that Dϕ (y ||∇−1
We show that there exists an algorithm, ULS, which ﬁts a linear separator H to the minimization
.= (cid:80)i ψ(y∗
of any SC S εψ
i H (oi )) for any SCL ψ with dom(ψ) = R, in order not to restrict the LS
R
built. To simplify notations, we let:

(13)

.= ψ(cid:63) (−x) .
˜ψ(x)
With this notation, the ﬁrst equality in Lemma 1 becomes:
(−y∗H )) − ˜ψ(0) .
ψ(y∗H ) = D ˜ψ (0||∇−1
(14)
˜ψ
.= dom(∇ ˜ψ ) = −im(∇ψ ), where this latter equality comes from ∇ ˜ψ (x) =
We let W
−∇ψ(cid:63) (−x) = −∇−1
ψ (−x).
It also comes im(∇ ˜ψ ) = R. Because any BLF is strictly convex
in its ﬁrst argument, we can compute its Legendre conjugate. In fact, we shall essentially need the
argument that realizes the supremum: for any x ∈ R, for any p ∈ W, we let:
.= argp(cid:48)∈W sup{xp(cid:48) − D ˜ψ (p(cid:48) ||p)} .
(15)
x (cid:5) p
We do not make reference to ˜ψ in the (cid:5) notation, as it shall be clear from context. We name x (cid:5) p
the Legendre dual of the ordered pair (x, p), closely following a notation by [6]. The Legendre dual
is unique and satisﬁes:

(16)
∇ ˜ψ (x (cid:5) p) = x + ∇ ˜ψ (p) ,
∀x, x(cid:48) ∈ R, ∀p ∈ W, x (cid:5) (x(cid:48) (cid:5) p) = (x + x(cid:48) ) (cid:5) p .
(17)
To state ULS, we follow the setting of [6] and suppose that we have T features h t (t = 1, 2, ..., T )
known in advance, the problem thus reducing to the computation of the leveraging coefﬁcients. We
deﬁne m × T matrix M with:

.= −y∗
i ht (oi ) .
mit
Given leveraging coefﬁcients vector α ∈ RT , we get:
−y∗
i H (oi ) = (M α)i .
We can specialize this setting to classical greedy induction frameworks for LS: in classical boosting,
at step j , we would ﬁt a single αt [6]; in totally corrective boosting, we would rather ﬁt {αt , 1 ≤ t ≤
j } [14]. Intermediate schemes may be used as well for Tj , provided they ensure that, at each step j of
the algorithm and for any feature ht , it may be chosen at some j (cid:48) > j . ULS is displayed in Algorithm
1. In Algorithm 1, notations are vector-based: the Legendre duals are computed component-wise;
furthermore, Tj may be chosen according to whichever scheme underlined above. The following
Theorem provides a ﬁrst general convergence property for ULS.

(18)

(19)

Theorem 1 ULS(M , ψ ) converges to a classi ﬁer H realizing the minimum of εψ
R .

Proof sketch:
In step [WU] in ULS, (17) brings wj+1 = (M αj+1 ) (cid:5) w0 = (M δj ) (cid:5) wj . After
few derivations involving the choice of δj and step [LC] in ULS, we obtain (with vector notations,
BLFs are the sum of the component-wise BLFs):
(20)
D ˜ψ (0||wj+1 ) − D ˜ψ (0||wj ) = −D ˜ψ (wj+1 ||wj )
Let A ˜ψ (wj+1 , wj ) .= −D ˜ψ (wj+1 ||wj ), which is just, from (20) and (14), the difference between
two successive SCL in Algorithm 1. Thus, A ˜ψ (wj+1 , wj ) < 0 whenever wj+1 (cid:54)= wj . Should we
be able to prove that when ULS has converged, w. ∈ KerM (cid:62) , this would make A ˜ψ (wj+1 , wj ) an
auxiliary function for ULS, which is enough to prove the convergence of ULS towards the optimum
[6]. Thus, suppose that wj+1 = wj (ULS has converged). Suppose that Tj is a singleton (e.g.
classical boosting scheme). In this case, δj = 0 and so ∀t = 1, 2, ..., T , (cid:80)m
i=1 mit (0 (cid:5) wj )i =
(cid:80)m
i=1 mitwj,i = 0, i.e. w(cid:62)
j+1M = 0(cid:62) , and wj , wj+1 ∈ KerM (cid:62) . The case of totally
j M = w(cid:62)
corrective boosting is simpler, as after the last iteration we would have wJ +1 ∈ KerM (cid:62) . Interme-
diate choices for Tj ⊂ {1, 2, ..., T } are handled in the same way.
We emphasize the fact that Theorem 1 proves the convergence towards the global optimum of εψ
R ,
regardless of ψ . The optimum is deﬁned by the LS with features in M that realizes the smallest
εψ
R . Notice that in practice, it may be a tedious task to satisfy exactly (20), in particular for totally
corrective boosting [14].

ULS has the ﬂa vor of boosting algorithms, repeatedly modifying a set of weights w over the exam-
ples. In fact, this similarity is more than syntactical, as ULS satisﬁes
two ﬁrst popular algorithmic
boosting properties, the ﬁrst of which being that step [LC] in ULS is equivalent to saying that this
LS has zero edge on wj+1 [14]. The following Lemma shows that this edge conditions is sound.
Lemma 2 Suppose that there does not exist some ht with all mit of the same sign, ∀i = 1, 2, ..., m.
Then, for any choice of Tj , step [LC] in ULS has always a ﬁnite solution.
Proof: Let:
Z .= D ˜ψ (0||(M αj+1 ) (cid:5) w0 ) .
(21)
˜ψ(−(M (δj + αj ))i ) from (14), a function convex in all leveraging
We have Z = m ˜ψ(0) + (cid:80)m
i=1
.= ∂ 2Z/(∂ δj,u δj,v ) (for the sake of simplicity,
coefﬁcients. Deﬁne |Tj | × |Tj | matrix E with euv
Tj = {1, 2, ..., |Tj |}, where |.| denotes the cardinal). We have euv = (cid:80)m
i=1 miumiv /ϕ(((M δj ) (cid:5)
wj )i ), with ϕ(x) .= d2 ˜ψ(x)/dx2 a function strictly positive in int(W) since ˜ψ is strictly convex.
.= 1/ϕ(((M δj ) (cid:5)wj )i ) > 0. It is easy to show that x(cid:62)Ex = (cid:80)m
Let qi,j
i=1 qi,j (cid:104)x, ˜mi (cid:105)2 ≥ 0, ∀x ∈
.= mit . Thus, E is positive semideﬁnite; as such, step
R|Tj | , with ˜mi ∈ R|Tj | the vector with ˜mit
[LC] in ULS, which is the same as solving ∂Z/∂ δj,u = 0, ∀u ∈ Tj (i.e. minimizing Z ) has always
a solution.
The condition for the Lemma to work is absolutely not restrictive, as if such an h t were to exist, we
would not need to run ULS: indeed, we would have either ε0/1 (S , ht ) = 0, or ε0/1 (S , −ht ) = 0. The
second property met by ULS is illustrated in the second example below.

x

p

0

We give two examples of specializations of ULS. Take for exam-
ple ψ(x) = exp(−x) (5). In this case, W = R+ , w0 = 1 and it is
not hard to see that ULS matches real AdaBoost with unnormal-
ized weights [13]. The difference is syntactical: the LS output
by ULS and real AdaBoost are the same. Now, take any BCL. In
this case, ˜ψ = φ, W = [0, 1] (scaling issues underlined for the
logit in Section 2 make it desirable to close W), and w0 = 1/21.
In all these cases, where W ⊆ R+ , wj is always a distribution
up to a normalization factor, and this would also be the case for
any strictly monotonous SC S ψ . The BCL case brings an appeal-
ing display of how the weights behave. Figure 2 displays a typ-
ical Legendre dual for a BCL. Consider example (oi , yi ), and its
Figure 2: A typical ∇φ (red:
i H (oi )) (cid:5) w0,i for
weight update, wj,i ← (M αj )i (cid:5) w0,i = (−y∗
strictly increasing, symmetric wrt
the current classiﬁer H . Fix p = w0,i and x = −y∗
i H (oi ) in Fig-
point (1/2, 0)), with Legendre dual
ure 2. We see that the new weight of the example gets larger iff
x (cid:5) p computed from x and p.
x > 0, i.e. iff the example is given the wrong class by H , which
is the second boosting property met by ULS.

1
x (cid:5) p

1/2

∇

φ

ULS turns out to meet a third boosting property, and the most important as it contributes to root the
algorithm in the seminal boosting theory of the early nineties: we have guarantees on its convergence
(WLA) [13]. To state
rate under a generalization of the well-known “W eak Learning Assumption”
the WLA, we plug the iteration in the index of the distribution normalization coefﬁcient
in (21), and
.= ||wj ||1 (||.||k is the Lk norm). The WLA is:
deﬁne Zj
(WLA)∀j, ∃γj > 0 : |(1/|Tj |) (cid:88)
t∈Tj

mitwj,i | ≥ γj .

m
(cid:88)
i=1

(1/Zj )

(22)

This is indeed a generalization of the usual WLA for boosting algorithms, that we obtain taking
|Tj | = 1, ht ∈ {−1, +1} [12]. Few algorithms are known that formally boost WLA in the sense that
requiring only WLA implies guaranteed rates for the minimization of εψ
R . We show that ULS meets
this property ∀ψ ∈ SCL. To state this, we need few more deﬁnitions. Let mt denote the tth column
.= maxt ||mt ||2 and aZ
.= minj Zj . Let aγ denote the average of γj (∀j ), and
vector of M , am
.= minx∈int(W) ϕ(x) (ϕ deﬁned in the proof of Lemma 2).
aϕ

Theorem 2 Under the WLA, ULS terminates in at most J = O(ma2
Z a2
m /(aϕa2
γ )) iterations.
Proof sketch: We use Taylor expansions with Lagrange remainder for ˜ψ , and then the mean-value
theorem, and obtain that ∀w, w + ∆ ∈ W, ∃w (cid:63) ∈ [min{w + ∆, w}, max{w + ∆, w}] such that
D ˜ψ (w + ∆||w) = ∆2ϕ(w(cid:63) )/2 ≥ (∆2 /2)aϕ ≥ 0. We use m times this inequality with w = wj,i
and ∆ = (wj+1,i − wj,i ), sum the inequalities, combine with Cauchy - Schwartz and Jensen’s
inequalities, and obtain:

(23)

−m ˜ψ(0) + D ˜ψ (0||w1 ) +

(D ˜ψ (0||wj+1 ) − D ˜ψ (0||wj )) = mψ(0) −

D ˜ψ (wj+1 ||wj ) ≥ aϕ (aZ γj /(2am ))2 .
Using (20), we obtain that D ˜ψ (0||wJ +1 ) − m ˜ψ(0) equals:
J
J
(cid:88)
(cid:88)
j=1
j=1
But, (14) together with the deﬁnition of wj in [WU] (see ULS) yields D ˜ψ (0||wJ +1,i ) = ˜ψ(0) +
i H (oi )), ∀i = 1, 2, ..., m, which ties up the SC S to (24); the guaranteed decrease in the rhs
ψ(y∗
of (24) by (23) makes that there remains to check when the rhs becomes negative to conclude that
ULS has terminated. This gives the bound of the Theorem.
The bound in Theorem 2 is mainly useful to prove that the WLA guarantees a convergence rate of
γ ) for ULS, but not the best possible as it is in some cases far from being optimal.
order O(m/a2
5 ULS, BCL, maximum likelihood and zero-sum games

D ˜ψ (wj+1 ||wj ) .(24)

BCL matches through the second equality in Lemma 1 the set of losses that satisfy the main re-
quirements about losses used in machine learning. This is a strong rationale for its use. Suppose
im(H ) ⊆ [0, 1], and consider the following requirements about some loss (cid:96) [0,1] (y , H ):
(R1) The loss is lower-bounded. ∃z ∈ R such that inf y ,H (cid:96)[0,1] (y , H ) = z .
(R2) The loss is a proper scoring rule. Consider a singleton domain O = {o}. Then, the best
(constant) prediction is arg minx∈[0,1] ε[0,1] (S , x) = p .= ˆPr[c = c+ |o] ∈ [0, 1], where p is
the relative proportion of positive examples with observation o.
(R3) The loss is symmetric in the following sense: (cid:96)[0,1] (y , H ) = (cid:96)[0,1] (1 − y , 1 − H ).
R1 is standard. For R2, we can write ε[0,1] (S , x) = p(cid:96)[0,1] (1, x) + (1 − p)(cid:96)[0,1] (0, x) = L(p, x),
which is just the expected loss of zero-sum games used in [9] (eq. (8)) with Nature states reduced
to the class labels. The fact that the minimum is achieved at x = p makes the loss a proper scoring
rule. R3 implies (cid:96)[0,1] (1, 1) = (cid:96)[0,1] (0, 0), which is virtually assumed for any domain; otherwise, it
scales to H ∈ [0, 1] a well-known symmetry in the cost matrix that holds for domains without class
dependent misclassiﬁcation costs. For these domains indeed, it is assumed (cid:96) [0,1] (1, 0) = (cid:96)[0,1] (0, 1).

Finally, we say that loss (cid:96)[0,1] is properly deﬁned iff dom((cid:96)[0,1] ) = [0, 1]2 and it is twice differentiable
on (0, 1)2 . This is only a technical convenience: even the 0/1 loss coincides on {0, 1} with properly
deﬁned losses. In addition, the differentiability condition would be satisﬁed by many popular losses.
The proof of the following Lemma involves Theorem 3 in [1] and additional facts to handle R3.
Lemma 3 Assume im(H ) ⊆ [0, 1]. Loss (cid:96)[0,1] (., .) is properly deﬁned and meets requirements R1,
R2, R3 iff (cid:96)[0,1] (y , H ) = z + Dφ (y ||H ) for some permissible φ.
Thus, φ maybe viewed as the “ signature” of the loss. The second equality in Lemma 1 makes a tight
connection between the predictions of H in [0, 1] and R. Let it be more formal: the matching [0, 1]
prediction for some H with im(H ) = O is:
.= ∇−1
ˆPrφ [c = c+ |H ; o]
φ
illustrated in Table 1, Lemma 3 and the second equality in Lemma 1 show that
With this deﬁnition,
BCL matches the set of losses of Lemma 3. This deﬁnition also brings the true nature of the mini-
mization of any BC S with real valued hypotheses like linear separators (in ULS). From Lemma 3 and
[2], there exists a bijection between BCL and a subclass of the exponential families whose members’
pdfs may be written as: Prφ [y |θ ] = exp(−Dφ (y ||∇−1
(θ)) + φ(y) − ν (y)), where θ ∈ R is the
φ
natural parameter and ν (.) is used for normalization. Plugging θ = H (o), using (25) and the second
equality in Lemma 1, we obtain that any BC S can be rewritten as εφ
R = U +(cid:80)i − log Prφ [yi |H (oi )],
where U does not play a role in its minimization. We obtain the following Lemma, in which we sup-
pose im(H ) = O.

(H (o)) ,

(25)

Lemma 4 Minimizing any BC S with classiﬁer H yields the maximum likelihood estimation, for each
observation, of the natural parameter θ = H (o) of an exponential family deﬁned by signature φ.
.=
In fact, one exponential family is concerned in ﬁne . To see this, we can factor the pdf as Pr[y |θ ]
(cid:63) the cumulant function, λ(y) the sufﬁcient
exp (θλ(y) − ψ(θ)) /z , with ψ = φ
statistic and z the
normalization function. Since y ∈ {0, 1}, we easily end up with Prφ [y |θ ] = 1/(1 + exp(−θ)), the
logistic prediction for a Bernoulli prior. To summarize, minimizing any loss that meets R1, R2 and
R3 (i.e. any BCL) amounts to the same ultimate goal; Since ULS works for any of the corresponding
surrogate risks, the crux of the choice of the BCL relies on data-dependent considerations.
Finally, we can go further in the parallel with game theory developed above for R2: using notations
in [9], the loss function of the decision maker can be written L(X, q) = Dφ (1||q(X )). R3 makes it
easy to recover losses like the log loss or the Brier score [9] respectively from φQ and φB (Table 1).
In this sense, ULS is also a sound learner for decision making in the zero-sum game of [9]. Notice
however that, to work, it requires that Nature has a restricted sample space size ({0, 1}).
6 Experiments
We have compared against each other 11 ﬂa vors of ULS, including real AdaBoost [13], on a bench-
mark of 52 domains (49 from the UCI repository). True risks are estimated via stratiﬁed 10-fold
cross validation; ULS is ran for r (ﬁx ed) features ht , each of which is a Boolean rule: If Mono-
mial then Class= ±1 else Class = ∓1, with at most l (ﬁx ed) literals, induced following the greedy
minimization of the BC S at hand. Leveraging coefﬁcients
([LC] in ULS) are approximated up to
10−10 precision. Figure 3 summarizes the results for two values of the couple (l, r). Histograms
are ordered from left to right in increasing average true risk over all domains (shown below his-
tograms). The italic numbers give, for each algorithm, the number of algorithms it beats accord-
ing to a Student paired t-test over all domains with .1 threshold probability. Out of the 10 ﬂa-
vors of ULS, the ﬁrst
four ﬂa vors pick φ in Table 1. The ﬁfth uses another permissible function:
φυ (x) .= (x(1 − x))υ , ∀υ ∈ (0, 1). The last ﬁ ve adaptively tune the BC S at hand out-of-a-bag
the BC S at each stage of the inner loop (for j ...) of ULS. Two (noted
of BC S. The ﬁrst
four ﬁt
“ F. ”) pick the BC S which minimizes the empirical risk in the bag; two others (noted “ E . ”) pick the
BC S which maximizes the current edge. There are two different bags corresponding to four permis-
sible functions each: the ﬁrst
(index “1”)
contains the φ in Table 1, the second (index “2”)
replaces
φB by φυ . We wanted to evaluate φB because it forces to renormalize the leveraging coefﬁcients
in
H each time it is selected, to ensure that the output of H lies in [−1, 1]. The last adaptive ﬂa vor,
F ∗ , “e xternalizes”
the choice of the BC S: it selects for each fold the BC S which yields the smallest
empirical risk in a bag corresponding to ﬁ ve φ: those of Table 1 plus φυ .

 25

 20

 15

 10

 5

 0

 25

 20

 15

 10

 5

 0

987654321
F∗
14.18 (10)

10

11

987654321

10

11

987654321

10

11

987654321

10

11

φM
14.70 (5)

φυ
14.71 (3)

φµ
14.83 (2)

987654321
F2
15.03 (1)

10

11

987654321

10

11

φQ
15.06 (1)

987654321
E1
15.22 (1)

10

11

987654321

10

11

φB
15.25 (1)

11

10
987654321
AdaBoost
15.35 (1)

987654321
E2
15.36 (1)

10

11

10

11

987654321
F1
17.37 (0)

987654321
F∗
12.15 (10)

10

11

987654321

10

11

φQ
12.39 (3)

987654321
10
AdaBoost
12.56 (3)

11

987654321

10

11

987654321

10

11

φM
12.59 (3)

φB
12.62 (3)

987654321
E2
12.63 (3)

10

11

987654321

10

11

987654321

10

11

φυ
12.74 (2)

φµ
12.79 (2)

10

11

987654321
F2
13.10 (2)

987654321
F1
17.57 (1)

10

11

987654321
E1
23.60 (0)

10

11

Figure 3: Summary of our results over the 52 domains for the 11 algorithms (top: l = 2, r = 10;
bottom: l = 3, r = 100). Vertical (red) bars show the average rank over all domains (see text).

Three main conclusions emerge from Figure 3. First, F ∗ appears to be superior to all other ap-
proaches, but slightly more sophisticated choices for the SC S (i.e. E . , F. ) fail at improving the
results; this is a strong advocacy for a particular treatment of this surrogate tuning problem. Second,
Matsushita’s BCL, built from φM , appears to be a serious alternative to the logistic loss. Third and
last, a remark previously made by [10] for decision trees seems to hold as well for linear separators,
as stronger concave regimes for φ in BCLs tend to improve performances at least for small r .
Conclusion
In this paper, we have shown the existence of a supervised learning algorithm which minimizes
any strictly convex, differentiable classiﬁcation calibrated surrogate [3], inducing linear separators.
Since the surrogate is now in the input of the algorithm, along with the learning sample, it opens
the interesting problem of the tuning of this surrogate to the data at hand to further reduce the true
risk. While the strategies we have experimentally tested are, with this respect, a simple primer for
eventual solutions, they probably display the potential and the non triviality of these solutions.
References
[1] A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a bregman predictor.
IEEE Trans. on Information Theory, 51:2664–2669, 2005.
[2] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. Journal of
Machine Learning Research, 6:1705–1749, 2005.
[3] P. Bartlett, M. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the Am.
Stat. Assoc., 101:138–156, 2006.
[4] P. Bartlett and M. Traskin. Adaboost is consistent. In NIPS*19, 2006.
[5] L. M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to
the solution of problems in convex programming. USSR Comp. Math. and Math. Phys., 7:200–217, 1967.
[6] M. Collins, R. Schapire, and Y. Singer. Logistic regression, adaboost and Bregman distances. In COLT’00,
pages 158–169, 2000.
[7] J. Friedman, T. Hastie, and R. Tibshirani. Additive Logistic Regression : a Statistical View of Boosting.
Ann. of Stat., 28:337–374, 2000.
[8] C. Gentile and M. Warmuth. Linear hinge loss and average margin. In NIPS*11, pages 225–231, 1998.
[9] P. Gr ¨unwald and P. Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian
decision theory. Ann. of Statistics, 32:1367–1433, 2004.
[10] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms.
Journal of Comp. Syst. Sci., 58:109–128, 1999.
[11] K. Matsushita. Decision rule, based on distance, for the classiﬁcation problem. Ann. of the Inst. for Stat.
Math., 8:67–77, 1956.
[12] R. Nock and F. Nielsen. A Real Generalization of discrete AdaBoost. Artif. Intell., 171:25 –41, 2007.
[13] R. E. Schapire and Y. Singer.
Improved boosting algorithms using conﬁdence-rated predictions.
COLT’98, pages 80–91, 1998.
[14] M. Warmuth, J. Liao, and G. R ¨atsch. Totally corrective boosting algorithms that maximize the margin. In
ICML’06, pages 1001–1008, 2006.

In


Performance analysis for L2 kernel classiﬁcation

JooSeuk Kim
Department of EECS
University of Michigan
Ann Arbor, MI, USA
stannum@umich.edu

Clayton D. Scott∗
Department of EECS
University of Michigan
Ann Arbor, MI, USA
clayscot@umich.edu

Abstract

We provide statistical performance guarantees for a recently introduced kernel
classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference
of densities. The classiﬁer is similar to a support vector machine (SVM) in that
it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike
SVMs, however, the L2 kernel classiﬁer does not involve a regularization param-
eter. We prove a distribution free concentration inequality for a cross-validation
based estimate of the ISE, and apply this result to deduce an oracle inequality and
consistency of the classiﬁer on the sense of both ISE and probability of error. Our
results also specialize to give performance guarantees for an existing method of
L2 kernel density estimation.

1 Introduction

g(x) = sign

,

αi yik(x, xi )

In the binary classiﬁcation problem we are given realizations (x1 , y1 ), . . . , (xn , yn ) of a jointly
distributed pair (X, Y ), where X ∈ Rd is a pattern and Y ∈ {−1, +1} is a class label. The goal
of classiﬁcation is to build a classiﬁer, i.e., a function taking X as input and outputting a label, such
that some measure of performance is optimized. Kernel classiﬁers [1] are an important family of
(cid:40)
(cid:41)
classiﬁers that have drawn much recent attention for their ability to represent nonlinear decision
n(cid:88)
boundaries and to scale well with increasing dimension d. A kernel classiﬁer (without offset) has
the form
i=1
where αi are parameters and k is a kernel function. For example, support vector machines (SVMs)
without offset have this form [2], as does the standard kernel density estimate (KDE) plug-in rule.
Recently Kim and Scott [3] introduced an L2 or integrated squared error (ISE) criterion to design the
coefﬁcients αi of a kernel classiﬁer with Gaussian kernel. Their L2 classiﬁer performs comparably
to existing kernel methods while possesing a number of desirable properties. Like the SVM, L2
kernel classiﬁers are the solutions of convex quadratic programs that can be solved efﬁciently using
standard decomposition algorithms.
In addition, the classiﬁers are sparse, meaning most of the
coefﬁcients αi = 0, which has advantages for representation and evaluation efﬁciency. Unlike
the SVM, however, there are no free parameters to be set by the user except the kernel bandwidth
parameter.
In this paper we develop statistical performance guarantees for the L2 kernel classiﬁer introduced
in [3]. The linchpin of our analysis is a new concentration inequality bounding the deviation of a
cross-validation based ISE estimate from the true ISE. This bound is then applied to prove an oracle
inequality and consistency in both ISE and probability of error. In addition, as a special case of
∗Both authors supported in part by NSF Grant CCF-0830490

1

our analysis, we are able to deduce performance guarantees for the method of L2 kernel density
estimation described in [4, 5].
The ISE criterion has a long history in the literature on bandwidth selection for kernel density esti-
mation [6] and more recently in parametric estimation [7]. The use of ISE for optimizing the weights
of a KDE via quadratic programming was ﬁrst described in [4] and later rediscovered in [5]. In [8],
an (cid:96)1 penalized ISE criterion was used to aggregate a ﬁnite number of pre-determined densities.
Linear and convex aggregation of densities, based on an L2 criterion, are studied in [9], where the
densities are based on a ﬁnite dictionary or an independent sample. In contrast, our proposed method
allows data-adaptive kernels, and does not require and independent (holdout) sample.
In classiﬁcation, some connections relating SVMs and ISE are made in [10], although no new algo-
rithms are proposed. Finally, the “difference of densities” perspective has been applied to classiﬁca-
tion in other settings by [11], [12], and [13]. In [11] and [13], a difference of densities are used to
ﬁnd smoothing parameters or kernel bandwidths. In [12], conditional densities are chosen among a
parameterized set of densities to maximize the average (bounded) density differences.
Section 2 reviews the L2 kernel classiﬁer, and presents a slight modiﬁcation needed for our analysis.
Our results are presented in Section 3. Conclusions are offered in the ﬁnal section, and proofs are
gathered in an appendix.

2 L2 Kernel Classiﬁcation

We review the previous work of Kim & Scott [3] and introduce an important modiﬁcation. For
convenience, we relabel Y so that it belongs to {1, −γ } and denote I+ = {i | Yi = +1} and I− =
{i | Yi = −γ }. Let f− (x) and f+ (x) denote the class-conditional densities of the pattern given the
label. From decision theory, the optimal classiﬁer has the form
g∗ (x) = sign {f+ (x) − γ f− (x)} ,
(1)
where γ incorporates prior class probabilities and class-conditional error costs (in the Bayesian
setting) or a desired tradeoff between false positives and false negatives [14]. Denote the “difference
of densities” dγ (x) := f+ (x) − γ f− (x).
(cid:88)
(cid:88)
(cid:98)f+ (x; α) =
(cid:98)f− (x; α) =
The class-conditional densities are modelled using the Gaussian kernel as
αikσ (x, Xi ) ,
αikσ (x, Xi )
i∈I+
i∈I−
 .
α |
(cid:88)
(cid:88)
with constraints α = (α1 , . . . , αn ) ∈ A where
αi = 1, αi ≥ 0 ∀i
αi =
(cid:190)
(cid:189)
i∈I−
i∈I+
(cid:161)
2πσ2 (cid:162)−d/2 exp
The Gaussian kernel is deﬁned as
− (cid:107)x − Xi (cid:107)2
kσ (x, Xi ) =
.
(cid:90) (cid:179) (cid:98)dγ (x; α) − dγ (x)
2σ2
I SE (α) = (cid:107) (cid:98)dγ (x; α) − dγ (x) (cid:107)2
The ISE associated with α is
(cid:90) (cid:98)dγ (x; α) dγ (x) dx +
(cid:90) (cid:98)d2
(cid:90)
L2 =
γ (x; α) dx − 2
=
γ (x) dx.
d2
(cid:90) (cid:98)dγ (x; α) dγ (x) dx
Since we do not know the true dγ (x), we need to estimate the second term in the above equation
H (α) (cid:44)
(cid:90)
(cid:90) (cid:98)d2
(cid:100)I SE (α) =
by Hn (α) which will be explained in detail in Section 2.1. Then, the empirical ISE is
γ (x; α) dx − 2Hn (α) +

γ (x) dx.
d2

(cid:180)2

A =

dx

(2)

(3)

2

Now, (cid:98)α is deﬁned as

and the ﬁnal classiﬁer will be

2.1 Estimation of H (α)

(cid:100)I SE (α)
(cid:98)α = arg min
(cid:40)
(cid:98)dγ (x; (cid:98)α) ≥ 0
α∈A
(cid:98)dγ (x; (cid:98)α) < 0.
+1,
−γ ,

g (x) =

(4)

Then,

H (α) =

=

=

where

(cid:90)
αiYih (Xi )

In this section, we propose a method of estimating H (α) in (2). The basic idea is to view H (α) as
an expectation and estimate it using a sample average. In [3], the resubstitution estimator for H (α)
was used. However, since this estimator is biased, we use a leave-one-out cross-validation (LOOCV)
n(cid:88)
estimator, which is unbiased and facilitates our theoretical analysis. Note that the difference of
(cid:98)dγ (x; α) = (cid:98)f+ (x) − γ (cid:98)f− (x) =
densities can be expressed as
αiYikσ (x, Xi ) .
(cid:90) (cid:98)dγ (x; α) f+ (x) dx − γ
(cid:90) (cid:98)dγ (x; α) f− (x) dx
(cid:90) (cid:98)dγ (x; α) dγ (x) dx =
i=1
(cid:90) n(cid:88)
(cid:90) n(cid:88)
αiYikσ (x, Xi ) f+ (x) dx − γ
n(cid:88)
αiYikσ (x, Xi ) f− (x) dx
i=1
i=1
(cid:90)
i=1
kσ (x, Xi ) f+ (x) dx − γ
h (Xi ) (cid:44)
kσ (x, Xi ) f− (x) dx.

(cid:88)
(cid:88)
We estimate each h (Xi ) in (5) for i = 1, . . . , n using leave-one-out cross-validation
(cid:98)hi (cid:44)
1
(cid:88)
(cid:88)
kσ (Xj , Xi ) − γ
i ∈ I+
kσ (Xj , Xi ) ,
N+ − 1
N−
j∈I−
j∈I+ ,j (cid:54)=i
1
kσ (Xj , Xi ) −
γ
(cid:80)n
kσ (Xj , Xi ) ,
N− − 1
N+
j∈I+
j∈I− ,j (cid:54)=i
where N+ = |I+ | , N− = |I− |. Then, the estimate of H (α) is Hn (α) =
i=1 αiYi
2.2 Optimization
(cid:90) (cid:195)
(cid:33)2
(cid:90) (cid:98)d2
n(cid:88)
The optimization problem (4) can be formulated as a quadratic program. The ﬁrst term in (3) is
(cid:90)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
αiYikσ (x, Xi )
γ (x; α) dx =
i=1
αiαj YiYj k√
kσ (x, Xi ) kσ (x, Xj ) dx =
2σ (Xi , Xj )
=
(cid:80)n
(cid:98)hi . Finally, since
j=1
i=1
j=1
i=1
by the convolution theorem for Gaussian kernels [15]. As we have seen in Section 2.1, the second
term Hn (α) in (3) is linear in α and can be expressed as
i=1 αi ci where ci = Yi
2σ (Xi , Xj ) − n(cid:88)
n(cid:88)
n(cid:88)
the third term does not depend on α, the optimization problem (4) becomes the following quadratic
(cid:98)α = arg min
program (QP)
1
2
α∈A
i=1
j=1
i=1
The QP (6) is similar to the dual QP of the 2-norm SVM with hinge loss [2] and can be solved by a
variant of the Sequential Minimal Optimization (SMO) algorithm [3].

αiαj YiYj k√

(cid:98)hi .

ciαi .

(6)

i ∈ I−

αiαj YiYj

(5)

dx

3

3 Statistical performance analysis

In this section, we give theoretical performance analysis on our proposed method. We assume that
{Xi }i∈I+ and {Xi }i∈I− are i.i.d samples from f+ (x) and f− (x), respectively, and treat N+ and
N− as deterministic variables n+ and n− such that n+ → ∞ and n− → ∞ as n → ∞.
Lemma 1. Conditioned on Xi , (cid:98)hi is an unbiased estimator of h (Xi ), i.e,
3.1 Concentration inequality for Hn (α)
(cid:104)(cid:98)hi | Xi
(cid:105)
= h (Xi ) .
E
(cid:190)
e−c(n+−1)2 + e−c(n−−1)2 (cid:180)
(cid:179)

(cid:189)
Furthermore, for any  > 0,
|Hn (α) − H (α)| > 
(cid:162)2d
(cid:161)√
sup
P
α∈A
/ (1 + γ )4 .
where c = 2
2πσ
Lemma 1 implies that Hn (α) → H (α) almost surely for all α ∈ A simultaneously, provided that
σ , n+ , and n− evolve as functions of n such that n+σ2d / ln n → ∞ and n−σ2d / ln n → ∞.

≤ 2n

3.2 Oracle Inequality

2

2πσ

where c =

(cid:180)
(cid:179)
Next, we establish on oracle inequality, which relates the performance of our estimator to that of the
best possible kernel classiﬁer.
(cid:161)√
(cid:162)2d
e−c(n+−1)2 + e−c(n−−1)2
Theorem 1. Let  > 0 and set δ = δ () = 2n
/ (1 + γ )4 . Then, with probability at least 1 − δ
I SE ( (cid:98)α) ≤ inf
I SE (α) + 4.
α∈A
(cid:175)(cid:175)(cid:175)I SE (α) − (cid:100)I SE (α)
(cid:175)(cid:175)(cid:175) ≤ 2,
Proof. From Lemma 1, with probability at least 1 − δ
∀α ∈ A
by using the fact I SE (α) − (cid:100)I SE (α) = 2 (Hn (α) − H (α)). Then, with probability at least 1 − δ ,
I SE ( (cid:98)α) ≤ (cid:100)I SE ( (cid:98)α) + 2 ≤ (cid:100)I SE (α) + 2 ≤ I SE (α) + 4
for all α ∈ A, we have
where the second inequality holds from the deﬁnition of (cid:98)α. This proves the theorem.
Next, we have a theorem stating that I SE ( (cid:98)α) converges to zero in probability.
ISE consistency
3.3
Theorem 2. Suppose that for f = f+ and f− , the Hessian Hf (x) exists and each entry of Hf (x)
σ → 0, n+σ2d/ ln n → ∞, and n−σ2d/ ln n → ∞, then I SE ( (cid:98)α) → 0 in probability as n → ∞
is piecewise continuous and square integrable. If σ , n+ , and n− evolve as functions of n such that
This result intuitively follows from the oracle inequality since the standard Parzen window density
estimate is consistent and uniform weights are among the simplex A. The rigorous proof is omitted
due to space limitations.

4

3.4 Bayes Error Consistency

In classiﬁcation, we are ultimately interested in minimizing the probability of error. Let us now
assume {Xi}n
i=1 is an i.i.d sample from f (x) = pf+ (x) + (1 − p)f− (x), where 0 < p < 1 is
the prior probability of the positive class. The consistency with respect to the probability of error
could be easily shown if we set γ to γ ∗ = 1−p
and apply Theorem 3 in [17]. However, since p is
p
unknown, we must estimate γ ∗ . Note that N+ and N− are binomial random variables, and we may
estimate γ ∗ as γ = N−
. The next theorem says the L2 kernel classiﬁer is consistent with respect to
N+
the probability of error.
Theorem 3. Suppose that the assumptions in Theorem 2 are satisﬁed. In addition, suppose that
f− ∈ L2 (R), i.e. (cid:107)f−(cid:107)L2 < ∞. Let γ = N−/N+ be an estimate of γ ∗ = 1−p
p . If σ evolves as
a function of n such that σ → 0 and nσ2d/ ln n → ∞ as n → ∞, then the L2 kernel classiﬁer is
(cid:110)
(cid:110) (cid:98)dγ (X; (cid:98)α)
(cid:111)
(cid:111)
consistent. In other words, given training data Dn = ((X1 , Y1 ) , . . . , (Xn , Yn )), the classiﬁcation
error
(cid:54)= Y | Dn
Ln = P
sgn
converges to the Bayes error L∗ in probability as n → ∞.

The proof is given in Appendix A.2.

3.5 Application to density estimation

with (cid:98)αi ’s optimized such that
n(cid:88)
n(cid:88)
(cid:98)α = arg min(cid:80)
1
2
αi=1
αi≥0
j=1
i=1

By setting γ = 0, our goal becomes estimating f+ and we recover the L2 kernel density estimate
of [4, 5] using leave-one-out cross-validation. Given an i.i.d sample X1 , . . . , Xn from f (x), the L2
n(cid:88)
kernel density estimate of f (x) is deﬁned as
(cid:98)f (x; (cid:98)α) =
(cid:98)αikσ (x, Xi )
i=1
 .
 1
2σ (Xi , Xj ) − n(cid:88)
αiαj k√
n − 1
i=1
Our concentration inequality, oracle inequality, and L2 consistency result immediately extend to
provide the same performance guarantees for this method. For example, we state the following
corollary.
Corollary 1. Suppose that the Hessian Hf (x) of a density function f (x) exists and each entry of
(cid:90) (cid:179) (cid:98)f (x; (cid:98)α) − f (x)
(cid:180)2
Hf (x) is piecewise continuous and square integrable. If σ → 0 and nσ2d/ ln n → ∞ as n → ∞,
then

(cid:88)
j (cid:54)=i

αi

kσ (Xi , Xj )

dx → 0

in probability.

4 Conclusion

Through the development of a novel concentration inequality, we have established statistical per-
formance guarantees on a recently introduced L2 kernel classiﬁer. We view the relatively clean
analysis of this classiﬁer as an attractive feature relative to other kernel methods. In future work, we
hope to invoke the full power of the oracle inequality to obtain adaptive rates of convergence, and
consistency for σ not necessarily tending to zero.

5

A Appendix

A.1 Proof of Lemma 1

(cid:161)√

(cid:162)d .

2πσ

(cid:190)

(7)

(cid:190)
(cid:190)

P

P

Note that for any given i, (kσ (Xj , Xi ))j (cid:54)=i are independent and bounded by M = 1/
For random vectors Z ∼ f+ (x) and W ∼ f− (x), h (Xi ) in (5) can be expressed as
h (Xi ) = E [kσ (Z, Xi ) | Xi ] − γE [kσ (W, Xi ) | Xi ] .
(cid:104)(cid:98)hi | Xi
(cid:105)
Since Xi ∼ f+ (x) for i ∈ I+ and Xi ∼ f− (x) for i ∈ I− , it can be easily shown that

= h (Xi ) .

(cid:175)(cid:175)(cid:175)(cid:175) >
γ 
1 + γ

E
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:190)
(cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) >
kσ (Xj , Xi ) − E [kσ (Z, Xi ) | Xi ]
j∈I+ ,j (cid:54)=i
kσ (Xj , Xi ) − γE [kσ (W, Xi ) | Xi ]
(cid:175)(cid:175)(cid:175)(cid:175) >
kσ (Xj , Xi ) | Xi

(cid:189)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175) > 
For i ∈ I+ ,
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
1
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) γ
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:190)
≤ P

(cid:88)
n+ − 1
1 + γ
+ P
n−
j∈I−
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:190)
By Hoeffding’s inequality [16], the ﬁrst term in (7) is
(n+ − 1) 
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:183) (cid:88)
kσ (Xj , Xi ) − (n+ − 1)E [kσ (Z, Xi ) | Xi ]
1 + γ
j∈I+ ,j (cid:54)=i
(n+ − 1) 
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:183) (cid:88)
= P
1 + γ
j∈I+ ,j (cid:54)=i
j∈I+ ,j (cid:54)=i
(n+ − 1) 
kσ (Xj , Xi ) | Xi
= P
1 + γ
j∈I+ ,j (cid:54)=i
j∈I+ ,j (cid:54)=i
≤ 2e−2(n+−1)2 /(1+γ )2M 2
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:190)
The second term in (7) is
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:183) (cid:88)
n− 
kσ (Xj , Xi ) − n−E [kσ (W, Xi ) | Xi ]
P
1 + γ
j∈I−
n− 
kσ (Xj , Xi ) | Xi
kσ (Xj , Xi ) − E
= P
1 + γ
j∈I−
j∈I−
≤ 2e−2n− 2 /(1+γ )2M 2 ≤ 2e−2(n−−1)2 /(1+γ )2M 2
.
(cid:175)(cid:175)(cid:175)(cid:175) Xi = X
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:183)
(cid:190)(cid:184)
(cid:175)(cid:175)(cid:175) ≥ 
(cid:110)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) ≥ 
(cid:111)
= E
P
≤ 2e−2(n+−1)2 /(1+γ )2M 2 + 2e−2(n−−1)2 /(1+γ )2M 2

(cid:190)

.

kσ (Xj , Xi ) − E

kσ (Xj , Xi ) − E

Therefore,

P

.

(cid:110)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) > 
(cid:111)
In a similar way, it can be shown that for i ∈ I− ,
≤ 2e−2(n+−1)2 /(1+γ )2M 2 + 2e−2(n−−1)2 /(1+γ )2M 2

P

.

6

P

(cid:41)

(cid:190)

(cid:175)(cid:175)(cid:175)(cid:175) B

≤

= n

= P

≤ P

(cid:189)
Then,

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) n(cid:88)
(cid:180)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) > 
(cid:40)
(cid:190)
(cid:179)(cid:98)hi − h (Xi )
(cid:41)
(cid:40)
|Hn (α) − H (α)| > 
(cid:175)(cid:175)(cid:175) > 
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
n(cid:88)
sup
= P
sup
αiYi
α∈A
α∈A
i=1
(cid:190)
(cid:189)
αi |Yi |
≤ P
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) > 
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) +
n(cid:88)
n(cid:88)
sup
α∈A
i=1
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189)
(cid:189)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
sup
n(cid:88)
n(cid:88)
αiγ
αi
α∈A
i∈I+
i∈I−
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189)
(cid:189)
(cid:190)
γ 

(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
sup
+ P
sup
αiγ
αi
1 + γ
1 + γ
α∈A
α∈A
i∈I−
i∈I+
(cid:190) (cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190) (cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189) (cid:91)
(cid:189) (cid:91)
(cid:190)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >


+ P
max
max
= P
i∈I+
i∈I−
1 + γ
1 + γ
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )
(cid:190)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi )


(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
(cid:88)
(cid:88)
+ P
= P
1 + γ
1 + γ
i∈I−
i∈I+
2e−2(n+−1)2 /(1+γ )4M 2 + 2e−2(n−−1)2 /(1+γ )4M 2 (cid:180)
(cid:179)


+
P
P
1 + γ
1 + γ
i∈I−
i∈I+
(cid:179)
2e−2(n+−1)2 /(1+γ )4M 2 + 2e−2(n−−1)2 /(1+γ )4M 2 (cid:180)
≤ n+
2e−2(n+−1)2 /(1+γ )4M 2 + 2e−2(n−−1)2 /(1+γ )4M 2 (cid:180)
(cid:179)
+ n−
.
A.2 Proof of Theorem 3
(cid:90) (cid:179) (cid:98)dγ (x; (cid:98)α) − dγ ∗ (x)
(cid:180)2
From Theorem 3 in [17], it sufﬁces to show that
dx → 0
(cid:107) (cid:98)dγ (x; (cid:98)α) − dγ ∗ (x) (cid:107)L2 = (cid:107) (cid:98)dγ (x; (cid:98)α) − dγ (x) + (γ − γ ∗ ) f− (x) (cid:107)L2
in probability. Since from the triangle inequality
≤ (cid:107) (cid:98)dγ (x; (cid:98)α) − dγ (x) (cid:107)L2 + (cid:107) (γ − γ ∗ ) f− (x) (cid:107)L2
(cid:112)
I SE ( (cid:98)α) + |γ − γ ∗ | · (cid:107)f− (x) (cid:107)L2 ,
we need to show that I SE ( (cid:98)α) and γ converges in probability to 0 and γ ∗ , respectively. The con-
=
In the previous analyses, we have shown the convergence of I SE ( (cid:98)α) by treating N+ , N− and γ
vergence of γ to γ ∗ can be easily shown from the strong law of large numbers.
(cid:110)
(cid:111)
(cid:170)
Dc(cid:170)
(cid:169)
(cid:169)
as deterministic variables but now we turn to the case where these variables are random. Deﬁne an
2 , N− ≥ n(1−p)
P {I SE ( (cid:98)α) > } ≤ P
I SE ( (cid:98)α) > , D
N+ ≥ np
, γ ≤ 2γ ∗
event D =
. For any  > 0,
2
(cid:175)(cid:175) n+ ≥ np
≤ 2γ ∗(cid:170)
(cid:169)
+ P
.
(cid:170)
(cid:169)
The ﬁrst term converges to 0 from the strong law of large numbers. Let deﬁne a set S =
I SE ( (cid:98)α) > , D
2 , n− ≥ n(1−p)
, n−
(n+ , n− )
. Then,
(cid:88)
(cid:175)(cid:175) N+ = n+ , N− = n−
(cid:169)
(cid:170) · P {N+ = n+ , N− = n−}
2
n+
I SE ( (cid:98)α) > , D
P
(cid:88)
(cid:175)(cid:175) N+ = n+ , N− = n−
(cid:169)
(cid:170) · P {N+ = n+ , N− = n−}
I SE ( (cid:98)α) > 
=
P
(cid:175)(cid:175) N+ = n+ , N− = n−
(cid:169)
(cid:170)
P
I SE ( (cid:98)α) > 
(n+ ,n− )∈S
max
.
(n+ ,n− )∈S
7

=

≤

P

(8)

Provided that σ → 0 and nσ2d/ ln n → ∞, any pair (n+ , n− ) ∈ S satisﬁes σ → 0, n+σ2d / ln n →
∞, and n−σ2d/ ln n → ∞ as n → ∞ and thus the term in (8) converges to 0 from Theorem 2. This
proves the theorem.

References
[1] B. Sch ¨olkopf and A. J. Smola, Learning with Kernels, MIT Press, Cambridge, MA, 2002.
[2] C. Cortes and V. Vapnik, “Support-vector networks,” Machine Learning, vol. 20, no. 3, pp. 273–297,
1995.
[3] J. Kim and C. Scott, “Kernel classiﬁcation via integrated squared error,” IEEE Workshop on Statistical
Signal Processing, August 2007.
[4] D. Kim, Least Squares Mixture Decomposition Estimation, unpublished doctoral dissertation, Dept. of
Statistics, Virginia Polytechnic Inst. and State Univ., 1995.
[5] Mark Girolami and Chao He, “Probability density estimation from optimally condensed data samples,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 10, pp. 1253–1264, OCT
2003.
[6] B.A. Turlach, “Bandwidth selection in kernel density estimation: A review,” Technical Report 9317,
C.O.R.E. and Institut de Statistique, Universit ´e Catholique de Louvain, 1993.
[7] David W.Scott, “Parametric statistical modeling by minimum integrated square error,” Technometrics 43,
pp. 274–285, 2001.
[8] A.B. Tsybakov F. Bunea and M.H. Wegkamp, “Sparse density estimation with l1 penalties,” Proceedings
of 20th Annual Conference on Learning Theory, COLT 2007, Lecture Notes in Artiﬁcial Intelligence,
v4539, pp. 530– 543, 2007.
[9] Ph. Rigollet and A.B. Tsybakov, “Linear and convex aggregation of density estimators,” https://
hal.ccsd.cnrs.fr/ccsd-00068216 , 2004.
[10] Robert Jenssen, Deniz Erdogmus, Jose C.Principe, and Torbjørn Eltoft, “Towards a uniﬁcation of infor-
mation theoretic learning and kernel method,” in Proc. IEEE Workshop on Machine Learning for Signal
Processing (MLSP2004), Sao Luis, Brazil.
[11] Peter Hall and Matthew P.Wand, “On nonparametric discrimination using density differeces,” Biometrika,
vol. 75, no. 3, pp. 541–547, Sept 1988.
[12] P. Meinicke, T. Twellmann, and H. Ritter, “Discriminative densities from maximum contrast estimation,”
in Advances in Neural Information Proceeding Systems 15, Vancouver, Canada, 2002, pp. 985–992.
[13] M. Di Marzio and C.C. Taylor, “Kernel density classiﬁcation and boosting: an l2 analysis,” Statistics and
Computing, vol. 15, pp. 113–123(11), April 2005.
[14] E. Lehmann, Testing statistical hypotheses, Wiley, New York, 1986.
[15] M.P. Wand and M.C. Jones, Kernel Smoothing, Chapman & Hall, 1995.
[16] L. Devroye and G. Lugosi, “Combinatorial methods in density estimation,” 2001.
[17] Charles T. Wolverton and Terry J. Wagner, “Asymptotically optimal discriminant fucntions for pattern
classiﬁcation,” IEEE Trans. Info. Theory, vol. 15, no. 2, pp. 258–265, Mar 1969.

8


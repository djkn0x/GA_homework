Signal-to-Noise Ratio Analysis
of Policy Gradient Algorithms

John W. Roberts and Russ Tedrake
Computer Science and
Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract

Policy gradient (PG) reinforcement learning algorithms have strong (local) con-
vergence guarantees, but their learning performance is typically limited by a large
variance in the estimate of the gradient. In this paper, we formulate the variance
reduction problem by describing a signal-to-noise ratio (SNR) for policy gradient
algorithms, and evaluate this SNR carefully for the popular Weight Perturbation
(WP) algorithm. We conﬁrm that SNR is a good predictor of long-term learn-
ing performance, and that in our episodic formulation, the cost-to-go function is
indeed the optimal baseline. We then propose two modiﬁcations to traditional
model-free policy gradient algorithms in order to optimize the SNR. First, we
examine WP using anisotropic sampling distributions, which introduces a bias
into the update but increases the SNR; this bias can be interpreted as following the
natural gradient of the cost function. Second, we show that non-Gaussian distribu-
tions can also increase the SNR, and argue that the optimal isotropic distribution is
a ‘shell’ distribution with a constant magnitude and uniform distribution in direc-
tion. We demonstrate that both modiﬁcations produce substantial improvements
in learning performance in challenging policy gradient experiments.

1

Introduction

Model-free policy gradient algorithms allow for the optimization of control policies on systems
which are impractical to model effectively, whether due to cost, complexity or uncertainty in the
very structure and dynamics of the system (Kohl & Stone, 2004; Tedrake et al., 2004). However,
these algorithms often suffer from high variance and relatively slow convergence times (Greensmith
et al., 2004). As the same systems on which one wishes to use these algorithms tend to have a
high cost of policy evaluation, much work has been done on maximizing the policy improvement
from any individual evaluation (Meuleau et al., 2000; Williams et al., 2006). Techniques such as
Natural Gradient (Amari, 1998; Peters et al., 2003a) and GPOMDP (Baxter & Bartlett, 2001) have
become popular through their ability to match the performance gains of more basic model-free
policy gradient algorithms while using fewer policy evaluations.
As practitioners of policy gradient algorithms in complicated mechanical systems, our group has a
vested interest in making practical and substantial improvements to the performance of these algo-
rithms. Variance reduction, in itself, is not a sufﬁcient metric for optimizing the performance of PG
algorithms - of greater signiﬁcance is the magnitude of the variance relative to the magnitude of the
gradient update. Here we formulate a signal-to-noise ratio (SNR) which facilitates simple and fast
evaluations of a PG algorithm’s average performance, and facilitates algorithmic performance im-
provements. Though the SNR does not capture all facets of a policy gradient algorithm’s capability
to learn, we show that achieving a high SNR will often result in a superior convergence rate with
less violent variations in the policy.

1

Through a close analysis of the SNR, and the means by which it is maximized, we ﬁnd several mod-
iﬁcations to traditional model-free policy gradient updates that improve learning performance. The
ﬁrst of these is the reshaping of distributions such that they are different on different parameters, a
modiﬁcation which introduces a bias to the update. We show that this reshaping can improve per-
formance, and that the introduced bias results in following the natural gradient of the cost function,
rather than the true point gradient. The second improvement is the use of non-Gaussian distribu-
tions for sampling, and through the SNR we ﬁnd a simple distribution which improves performance
without increasing the complexity of implementation.

2 The weight perturbation update

∆ (cid:126)w = −η

Consider minimizing a scalar function J ( (cid:126)w) with respect to the parameters (cid:126)w (note that it is pos-
sible that J ( (cid:126)w) is a long-term cost and results from running a system with the parameters (cid:126)w until
conclusion). The weight perturbation algorithm (Jabri & Flower, 1992) performs this minimization
with the update:
∆ (cid:126)w = −η (J ( (cid:126)w + (cid:126)z ) − J ( (cid:126)w)) (cid:126)z ,
(1)
where the components of the ‘perturbation’, (cid:126)z , are drawn independently from a mean-zero dis-
(cid:32)
(cid:33)
tribution, and η is a positive scalar controlling the magnitude of the update (the “learning rate”).
J ( (cid:126)w) + (cid:88)
(cid:88)
Performing a ﬁrst-order Taylor expansion of J ( (cid:126)w + (cid:126)z ) yields:
zi − J ( (cid:126)w)
i
i
In expectation, this becomes the gradient times a (diagonal) covariance matrix, and reduces to
E [∆ (cid:126)w] = −ησ2 ∂ J
∂ (cid:126)w
an unbiased estimate of the gradient, scaled by the learning rate and σ2 , the variance of the pertur-
bation. However, this unbiasedness comes with a very high variance, as the direction of an update
is uniformly distributed. It is only the fact that updates near the direction of the true gradient have a
larger magnitude than do those nearly perpendicular to the gradient that allows for the true gradient
to be achieved in expectation. Note also that all samples parallel to the gradient are equally useful,
whether they be in the same or opposite direction, as the sign does not affect the resulting update.
The WP algorithm is one of the simplest examples of a policy gradient reinforcement learning al-
gorithm, and thus is well suited for analysis. In the special case when (cid:126)z is drawn from a Gaussian
distribution, weight perturbation can be interpreted as a REINFORCE update(Williams, 1992).

(cid:126)z = −η

∂ J
∂ (cid:126)w i

zi · (cid:126)z .

∂ J
∂ (cid:126)w i

(2)

(3)

,

3 SNR for policy gradient algorithms

The SNR is the expected power of the signal (update in the direction of the true gradient) divided by
(cid:104)
(cid:105)
the expected power of the noise (update perpendicular to the true gradient). Taking care to ensure
that the magnitude of the true gradient does not effect the SNR, we have:
E (cid:2)∆ (cid:126)wT⊥∆ (cid:126)w⊥ (cid:3) ,
∆ (cid:126)wT(cid:107) ∆ (cid:126)w(cid:107)
E
∆ (cid:126)wT
 (cid:126)Jw(cid:13)(cid:13)(cid:13) (cid:126)Jw
SNR =
(cid:126)Jw(cid:13)(cid:13)(cid:13) (cid:126)Jw
(cid:13)(cid:13)(cid:13) , ∆ (cid:126)w⊥ = ∆ (cid:126)w − (cid:126)w(cid:107) ,
(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)( (cid:126)w= (cid:126)w0 )
∆ (cid:126)w(cid:107) =
for convenience.
Intuitively, this expression measures how large a proportion of the update is “useful”. If the update
is purely in the direction of the gradient the SNR would be inﬁnite, while if the update moved
perpendicular to the true gradient, it would be zero. As such, all else being equal, a higher SNR
should generally perform as well or better than a lower SNR, and result in less violent swings in cost
and policy for the same improvement in performance.

and using (cid:126)Jw ( (cid:126)w0 ) = ∂J ( (cid:126)w)
∂ (cid:126)w

(4)

(5)

2

T ·

= E

E

= E

Jwi Jwj zi zj

∆ (cid:126)wT(cid:107) ∆ (cid:126)w(cid:107)

Jwi Jwj Jwk Jwp zi zj zk zp

3.1 Weight perturbation with Gaussian distributions
Evaluating the SNR for the WP update in Equation 1 with a deterministic J ( (cid:126)w) and (cid:126)z drawn from a

 η2(cid:13)(cid:13)(cid:13) (cid:126)Jw
 (cid:126)Jw
(cid:88)
 (cid:126)Jw
(cid:88)
Gaussian distribution yields a surprisingly simple result. If one ﬁrst considers the numerator:
(cid:105)
(cid:104)
(cid:13)(cid:13)(cid:13)4
Jwk Jwp zk zp
 η2(cid:13)(cid:13)(cid:13) (cid:126)Jw
 = Q,
(cid:88)
i,j
k,p
(cid:13)(cid:13)(cid:13)2
i,j,k,p
where we have named this term Q for convenience as it occurs several times in the expansion of the
(cid:105)
(cid:104)
E (cid:2)∆ (cid:126)wT⊥∆ (cid:126)w⊥ (cid:3) = E
= E (cid:2)∆ (cid:126)wT ∆ (cid:126)w(cid:3) − 2Q + Q
SNR. We now expand the denominator as follows:
∆ (cid:126)wT ∆ (cid:126)w − 2∆ (cid:126)wT(cid:107) (∆ (cid:126)w(cid:107) + ∆ (cid:126)w⊥ ) + ∆ (cid:126)wT(cid:107) ∆ (cid:126)w(cid:107)
(7)
(cid:88)
 − Q.
Substituting Equation (1) into Equation (7) and simplifying results in:
E (cid:2)∆ (cid:126)wT⊥∆ (cid:126)w⊥ (cid:3) = η2(cid:13)(cid:13)(cid:13) (cid:126)Jw
(cid:13)(cid:13)(cid:13)2 E
Jwi Jwj zi zj z 2
k
i,j,k
We now assume that each component zi is drawn from a Gaussian distribution with variance σ2 .
 =
3σ4 (cid:88)
(cid:88)
2 (cid:88)
4 + 3σ4 (cid:88)
Taking the expected value, it may be further simpliﬁed to:
(cid:13)(cid:13)(cid:13) (cid:126)Jw
(cid:13)(cid:13)(cid:13)4
Q = η2(cid:13)(cid:13)(cid:13) (cid:126)Jw
(cid:13)(cid:13)(cid:13)4
3σ4
2 = 3σ4 ,
2Jwj
2
(9)
Jwi
Jwi
Jwi
Jwj
 − Q = σ4 (2 + N ) − 3σ4 = σ4 (N − 1), (10)
2 (cid:88)
j (cid:54)=i
i,j
i
i
2 + (cid:88)
E (cid:2)∆ (cid:126)wT⊥∆ (cid:126)w⊥ (cid:3) = η2σ4
(cid:13)(cid:13)(cid:13) (cid:126)Jw
(cid:13)(cid:13)(cid:13)2
i,j
i
where N is the number of parameters. Canceling σ results in:
3
N − 1 .
Thus, for small noises and constant σ the SNR and the parameter number have a simple inverse
relationship. This is a particularly concise model for performance scaling in PG algorithms.

Jwi

2

Jwi

(6)

(8)

SNR =

(11)

3.2 Relationship of the SNR to learning performance

To evaluate the degree to which the SNR is correlated with actual learning performance, we ran a
number of experiments on a simple quadratic bowl cost function, which may be written as:

J ( (cid:126)w) = (cid:126)wT A (cid:126)w,

(12)

where the optimal is always at the point (cid:126)0. The SNR suggests a simple inverse relationship be-
tween the number of parameters and the learning performance. To evalute this claim we performed
three tests: 1) true gradient descent on the identity cost function (A set to the identity matrix) as a
benchmark, 2) WP on the identity cost function and 3) WP on 150 randomly generated cost func-
tions (each component drawn from a Gaussian distribution), all of the form given in Equation (12),
and for values of N between 2 and 10. For each trial (cid:126)w was intially set to be (cid:126)1. As can be seen
in Figure 1a, both the SNR and the reduction in cost after running WP for 100 iterations decrease
monotonically as the number of parameters N increases. The fact that this occurs in the case of
randomly generated cost functions demonstrates that this effect is not related to the simple form of
the identity cost function, but is in fact related to the number of dimensions.

3

Figure 1: Two comparisons of SNR and learning performance: (A) Relationship as dimension N
is increased (Section 3.2). The curves are 15,000 averaged runs, each run 100 iterations. For ran-
domly generated cost functions, 150 A matrices were tested. True gradient descent was run on the
identity cost function. The SNR for each case was computed in with Equation (11). (B) Relation-
ship as Gaussian is reshaped by changing variances for case of 2D anisotropic cost function(ratio of
1 + σ2
2 = 0.1
gradients in different directions is 5) cost function (Section 4.1.1). The constraint σ2
is imposed, while σ2
1 is between 0 and .1. For each value of σ1 15,000 updates were averaged to
produce the curve plotted. The plot shows that variances which increase the SNR also improve the
performance of the update.

3.3 SNR with parameter-independent additive noise

(cid:126)z .

(13)

(cid:126)z = −η

Jwi zi − b( (cid:126)w) + v

In many real world systems, the evaluation of the cost J ( (cid:126)w) is not deterministic, a property which
can signiﬁcantly affect learning performance. In this section we investigate how additive ‘noise’ in
the function evaluation affects the analytical expression for the SNR. We demonstrate that for very
high noise WP begins to behave like a random walk, and we ﬁnd in the SNR the motivation for an
improvement in the WP algorithm that will be examined in Section 4.2.
Consider modifying the update seen in Equation (1) to allow for a parameter-independent additive
(cid:33)
(cid:32)
(cid:33)
(cid:32)(cid:88)
noise term v and a more general baseline b( (cid:126)w), and again perform the Taylor expansion. Writing
J ( (cid:126)w) + (cid:88)
the update with these terms gives:
∆ (cid:126)w = −η
Jwi zi + ξ ( (cid:126)w)
i
i
where we have combined the terms J ( (cid:126)w), b( (cid:126)w) and v into a single random variable ξ ( (cid:126)w). The new
variable ξ ( (cid:126)w) has two important properties: its mean can be controlled through the value of b( (cid:126)w),
and its distribution is independent of parameters (cid:126)w , thus ξ ( (cid:126)w) is independent of all the zi .
We now essentially repeat the calculation seen in Section 3.1, with the small modiﬁcation of includ-
ing the noise term. When we again assume independent zi , each drawn from identical Gaussian
distributions with standard deviation σ , we obtain the expression:
(J ( (cid:126)w) − b( (cid:126)w))2 + σ2
φ + 3
(N − 1)(φ + 1) , φ =
v
σ2(cid:107) (cid:126)Jw (cid:107)2
where σv is the standard deviation of the noise v and we have termed the error component φ. This
expression depends upon the fact that the noise v is mean-zero and independent of the parameters,
although as stated earlier, the assumption that v is mean-zero is not limiting. It is clear that in the
limit of small φ the expression reduces to that seen in Equation (11), while in the limit of very large
φ it becomes the expression for the SNR of a random walk (see Section 3.4). This expression makes
it clear that minimizing φ is desirable, a result that suggests two things: (1) the optimal baseline
(from the perspective of the SNR) is the value function (i.e. b∗ ( (cid:126)w) = J ( (cid:126)w)) and (2) higher values of
σ are desirable, as they reduce φ by increasing the size of its denominator. However, there is clearly
a limit on the size of σ due to higher order terms in the Taylor expansion; very large σ will result in
samples which do not represent the local gradient. Thus, in the case of noisy measurements, there
is some optimal sampling distance that is as large as possible without resulting in poor sampling of
the local gradient. This is explored in Section 4.2.1.

SNR =

(14)

4

3.4 SNR of a Random Walk

Due to the fact that the update is squared in the SNR, only its degree of parallelity to the true gradient
is relevant, not its direction. In the case of WP on a deterministic function, this is not a concern as the
update is always within 90◦ of the gradient, and thus the parallel component is always in the correct
direction. For a system with noise, however, components of the update parallel to the gradient can
in fact be in the incorrect direction, contributing to the SNR even though they do not actually result
in learning. This effect only becomes signiﬁcant when the noise is particularly large, and reaches
its extreme in the case of a true random walk (a strong bias in the “wrong” direction is in fact a
good update with an incorrect sign). If one considers moving by a vector drawn from a multivariate
Gaussian distribution without any correlation to the cost function, the SNR is particularly easy to
(cid:88)
T (cid:88)
compute, taking the form:
1
(cid:88)
(cid:88)
Jwj zj (cid:126)Jw
Jwi zi (cid:126)Jw
(cid:107) (cid:126)Jw (cid:107)4
i
j
Jwi zi (cid:126)Jw )T ((cid:126)z − 1
(cid:107) (cid:126)Jw (cid:107)2
i
i

σ2
N σ2 − 2σ2 + σ2 =

((cid:126)z − 1
(cid:107) (cid:126)Jw (cid:107)2

(15)
As was discussed in Section 3.3, this value of the SNR is the limiting case of very high measurement
noise, a situation which will in fact produce a random walk.

1
N − 1

Jwi zi (cid:126)Jw )

SNR =

=

4 Applications of SNR

4.1 Reshaping the Gaussian Distribution
Consider a generalized WP algorithm, in which we allow each component zi to be drawn inde-
pendently from separate mean-zero distributions. Returning to the derivation in Section 3.1, we no
longer assume each zi is drawn from an identical distribution, but rather associate each with its own

2 (cid:88)


σi (the vector of the σi will be referred to as (cid:126)σ ). Removing this assumption results in the SNR:
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13) (cid:126)Jw
i + (cid:88)
−1
3 (cid:88)
2σ4
Jwi
− 1
i
i,j
2σ2
2σ2
i Jwj
j
i,j
An important property of this SNR is that it depends only upon the direction of (cid:126)Jw and the rel-
ative magnitude of the σi (as opposed to parameters such as the learning rate η and the absolute
magnitudes (cid:107)(cid:126)σ(cid:107) and (cid:107) (cid:126)Jw (cid:107)).

SNR((cid:126)σ , (cid:126)Jw ) =

2σ2
i σ2
j

(16)

Jwi

Jwi

.

4.1.1 Effect of reshaping on performance

While the absolute magnitudes of the variance and true gradient do not affect the SNR given in
Equation (16), the relative magnitudes of the different σi and their relationship to the true gradient
can affect it. To study this property, we investigate a cost function with a signiﬁcant degree of
anisotropy. Using a cost function of the form given in Equation (12) and N = 2, we choose an A
matrix whose ﬁrst diagonal component is ﬁve times that of the second. We then investigate a series
1 + σ2
2 = C ). We
of possible variances σ2
1 and σ2
2 constrained such that their sum is a constant (σ2
observe the performance of the ﬁrst update (rather than the full trial) as the true gradient can vary
signiﬁcantly over the course of a trial, thereby having major effects on the SNR even as the variances
are unchanged. As is clear in Figure 1b, as the SNR is increased through the choice of variances the
performance of this update is improved. The variation of the SNR is much more signiﬁcant than the
change in performance, however this is not surprising as the SNR is inﬁnite if the update is exactly
along the correct direction, while the improvement from this update will eventually saturate.

5

4.1.2 Demonstration in simulation

The improved performance of the previous section suggests the possibility of a modiﬁcation to the
WP algorithm in which an estimate of the true gradient is used before each update to select new
variances which are more likely to learn effectively. Changing the shape of the distribution does add
a bias to the update direction, but the resulting biased update is in fact descending the natural gradient
of the cost function. To make use of this opportunity, some knowledge of the likely gradient direction
is required. This knowledge can be provided via a momentum estimate (an average of previous
updates) or through an inaccurate model that is able to capture some facets of the geometry of the
cost function. With this estimated gradient the expression given in Equation (16) can be optimized
over the σi numerically using a method such as Sequential Quadratic Programming (SQP). Care
must be taken to avoid converging to very narrow distributions (e.g. placing some small minimum
noise on all parameters regardless of the optimization), but ultimately this reshaping of the Gaussian
can provide real performance beneﬁts.

(a)

(b)

Figure 2: (a) The cart-pole system. The task is to apply a horizontal force f to the cart such that
the pole swings to the vertical position. (b) The average of 200 curves showing reduction in cost
versus trial number for both a symmetric Gaussian distribution and a distribution reshaped using the
SNR. The blue shaded region marks the area within one standard deviation for a symmetric Gaussian
distribution, the red region marks one standard deviation for the reshaped distribution and the purple
is within one standard deviation of both. The reshaping began on the eighth trial to give time for the
momentum-based gradient estimate to stabilize.

To demonstrate the improvement in convergence time this reshaping can achieve, weight perturba-
tion was used to develop a barycentric feedback policy for the cart-pole swingup task, where the
cost was deﬁned as a weighted sum of the actuation used and the squared distance from the upright
position. A gradient estimate was obtained through averaging previous updates, and SQP was used
to optimize the SNR prior to each trial. Figure 2 demonstrates the superior performance of the re-
shaped distribution over a symmetric Guassian using the same total variance (i.e. the traces of the
covariance matrices for both distributions were the same).

4.1.3 WP with Gaussian distributions follow the natural gradient
(cid:35)
(cid:34)
The natural gradient for a policy that samples with a mean-zero Gaussian of covariance Σ may be
written (see (Peters et al., 2003b)):
˜(cid:126)Jw = F −1 (cid:126)Jw , F = Eπ((cid:126)ξ ; (cid:126)w)

(17)

.

∂ log π((cid:126)ξ ; (cid:126)w)
∂wi

∂ log π((cid:126)ξ ; (cid:126)w)
∂wj

where F is the Fisher Information matrix, π is the sampling distribution, and (cid:126)ξ = (cid:126)w + (cid:126)z . Using the
Gaussian form of the sampling, F may be evaluated easily, and becomes as Σ−1 , thus:
˜(cid:126)Jw = Σ (cid:126)Jw .
(18)
This is true for all mean-zero multivariate Gaussian distributions, thus the biased update, while no
longer following the local point gradient, does follow the natural gradient. It is important to note
that the natural gradient is a function of the shape of the sampling distribution, and it is because of
this that all sampling distributions of this form can follow the natural gradient.

6

fpxlmcgθm4.2 Non-Gaussian Distributions

The analysis in Section 3.3 suggests that for a function
with noisy measurements there is an optimal sampling
distance which depends upon the local noise and gra-
dient as well as the strength of higher-order terms in
that region. For a two-dimensional cost function of
the form given in Equation (12), Figure 3 shows the
SNR’s dependence upon the radius of the shell distri-
bution (i.e. the magnitude of the sampling). For various
levels of additive mean-zero noise the SNR was com-
puted for a distribution uniform in angle and ﬁxed in its
distance from the mean (this distance is the “sampling
magnitude”). The fact that there is a unique maximum
for each case suggests the possibility of sampling only
at that maximal magnitude, rather than over all mag-
nitudes as is done with a Gaussian, and thus improv-
ing SNR and performance. While determining the ex-
act magnitude of maximum SNR may be impractical,
choosing a distribution with uniformly distributed di-
rection and a constant magnitude close to this optimal
value, performance can be improved. This idea was
tested on the benchmark proposed in (Riedmiller et al.,
2007), where comparisons showed it was able to learn
at rates similar to optimized RPROP from reasonable
initial policies, and was capable of learning from a zero
initial policy.

4.2.1 Experimental Demonstration

Figure 3: SNR vs. update magnitude for
a 2D quadratic cost function. Mean-zero
measurement noise is included with vari-
ances from 0 to .65. As the noise is in-
creased, the sampling magnitude produc-
ing the maximum SNR is larger and the
SNR achieved is lower. Note that the
highest SNR achieved is for the small-
est sampling magnitude with no noise
where it approaches the theoretical value
(for 2D) of 3. Also note that for small
sampling magnitudes and large noises the
SNR approaches the random walk value.

To provide compelling evidence of improved performance, the shell distribution was implemented
on a laboratory experimental system with actuator limitations and innate stochasticity. We have re-
cently been exploring the use of PG algorithms in an incredibly difﬁcult and exciting control domain
-ﬂuid dynamics - and as such applied the shell distribution to a ﬂuid dynamical system. Speciﬁcally,
we applied learning to a system used to sudy the dynamics of ﬂapping ﬂight via a wing submerged
in water (see Figure 4 for a description of the system (Vandenberghe et al., 2004)). The task is to
determine the vertical motion producing the highest ratio of rotational displacement to energy input.
Model-free methods are particularly exciting in this domain because direct numerical simulation
can take days(Shelley et al., 2005) - in contrast optimizationg on the experimental physical ﬂapping
wing can be done in real-time, at the cost of dealing with noise in the evaluation of the cost function;
success here would be enabling for experimental ﬂuid dynamics. We explored the idea of using a
“shell” distribution to improve the performance of our PG learning on this real-world system.

(a)

(b)

Figure 4: (a) Schematic of the ﬂapping setup. The plate rotates freely about its vertical axis, while
the vertical motion is prescribed by the learnt policy. This vertical motion is coupled with the plate’s
rotation through hydrodynamic effects. (b) 5 averaged runs on the ﬂapping plate using Gaussian or
Shell distributions for sampling. The error bars represent one standard deviation in the performance
of different runs at that trial.

7

Representing the vertical position as a function of time with a 13-point periodic cubic spline, a
5D space was searched (points 1, 7 and 13 were ﬁxed at zero, while points 2 and 8, 3 and 9 etc.
were set to equal and opposite values determined by the control parameters). Beginning with a
smoothed square wave, WP was run for 20 updates using shell distributions and Gaussians. Both
forms of distributions were run 5 times and averaged to produce the curves in Figure 4. The sampling
magnitude of the shell distribution was set to be the expected value of the length of a sample from
the Gaussian distribution, while all other parameters were set equal. With optimized sampling, we
acquired locally optimal policies in as little as 15 minutes, with repeated optimizations from very
different initial policies converging to the same waveform. The result deepened our understanding
of this ﬂuid system and suggests promising applications to other ﬂuid systems of similar complexity.
5 Conclusion
In this paper we present an expression for the SNR of PG algorithms, and looked in detail at the
common case of WP. This expression gives us a quantitative means of evaluating the expected per-
formance of a PG algorithm, although the SNR does not completely capture an algorithm’s capacity
to learn. SNR analysis revealed two distinct mechanisms for improving the WP update - perturb-
ing different parameters with different distributions, and using non-Gaussian distributions. Both of
them showed real improvement on highly nonlinear problems (the cart-pole example used a very
high-dimensional policy), without knowledge of the problem’s dynamics and structure. We believe
that SNR-optimized PG algorithms show promise for many complicated, real-world applications.
6 Acknowledgements
The authors thank Drs. Lionel Moret and Jun Zhang for valuable assistance with the heaving foil.
References
Amari, S. (1998). Natural gradient works efﬁciently in learning. Neural Computation, 10, 251–276.
Baxter, J., & Bartlett, P. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial
Intelligence Research, 15, 319–350.
Greensmith, E., Bartlett, P. L., & Baxter, J. (2004). Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5, 1471–1530.
Jabri, M., & Flower, B. (1992). Weight perturbation: An optimal architecture and learning technique
for analog VLSI feedforward and recurrent multilayer networks. IEEE Trans. Neural Netw., 3,
154–157.
Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning for fast quadrupedal locomo-
tion. Proceedings of the IEEE International Conference on Robotics and Automation (ICRA).
Meuleau, N., Peshkin, L., Kaelbling, L. P., & Kim, K.-E. (2000). Off-policy policy search. NIPS.
Peters, J., Vijayakumar, S., & Schaal, S. (2003a). Policy gradient methods for robot control (Tech-
nical Report CS-03-787). University of Southern California.
Peters, J., Vijayakumar, S., & Schaal, S. (2003b). Reinforcement learning for humanoid robotics.
Proceedings of the Third IEEE-RAS International Conference on Humanoid Robots.
Riedmiller, M., Peters, J., & Schaal, S. (2007). Evaluation of policy gradient methods and variants on
the cart-pole benchmark. Symposium on Approximate Dynamic Programming and Reinforcement
Learning (pp. 254–261).
Shelley, M., Vandenberghe, N., & Zhang, J. (2005). Heavy ﬂags undergo spontaneous oscillations
in ﬂowing water. Physical Review Letters, 94.
Tedrake, R., Zhang, T. W., & Seung, H. S. (2004). Stochastic policy gradient reinforcement learning
on a simple 3D biped. Proceedings of the IEEE International Conference on Intelligent Robots
and Systems (IROS) (pp. 2849–2854). Sendai, Japan.
Vandenberghe, N., Zhang, J., & Childress, S. (2004). Symmetry breaking leads to forward ﬂapping
ﬂight. Journal of Fluid Mechanics, 506, 147–155.
Williams, J. L., III, J. W. F., & Willsky, A. S. (2006). Importance sampling actor-critic algorithms.
Proceedings of the 2006 American Control Conference.
Williams, R. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8, 229–256.

8


Tighter Bounds for Structured Estimation

Chuong B. Do, Quoc Le
Stanford University
{chuongdo,quocle}@cs.stanford.edu

Choon Hui Teo
Australian National University and NICTA
choonhui.teo@anu.edu.au

Olivier Chapelle, Alex Smola
Yahoo! Research
chap@yahoo-inc.com,alex@smola.org

Abstract

Large-margin structured estimation methods minimize a convex upper bound of
loss functions. While they allow for efﬁcient optimization algorithms, these con-
vex formulations are not tight and sacriﬁce the ability to accurately model the true
loss. We present tighter non-convex bounds based on generalizing the notion of
a ramp loss from binary classiﬁcation to structured estimation. We show that a
small modiﬁcation of existing optimization algorithms sufﬁces to solve this mod-
iﬁed problem. On structured prediction tasks such as protein sequence alignment
and web page ranking, our algorithm leads to improved accuracy.

1

Introduction

Structured estimation [18, 20] and related techniques has proven very successful in many areas
ranging from collaborative ﬁltering to optimal path planning, sequence alignment, graph matching
and named entity tagging.
At the heart of those methods is an inverse optimization problem, namely that of ﬁnding a func-
tion f (x, y) such that the prediction y∗ which maximizes f (x, y∗ ) for a given x, minimizes some
loss ∆(y , y∗ ) on a training set. Typically x ∈ X is referred to as a pattern, whereas y ∈ Y is a
corresponding label. Y can represent a rich class of possible data structures, ranging from binary
sequences (tagging), to permutations (matching and ranking), to alignments (sequence matching),
to path plans [15]. To make such inherently discontinuous and nonconvex optimization problems
tractable, one applies a convex upper bound on the incurred loss. This has two beneﬁts: ﬁrstly, the
problem has no local minima, and secondly, the optimization problem is continuous and piecewise
differentiable, which allows for effective optimization [17, 19, 20]. This setting, however, exhibits a
signiﬁcant problem: the looseness of the convex upper bounds can sometimes lead to poor accuracy.
For binary classiﬁcation, [2] proposed to switch from the hinge loss, a convex upper bound, to
a tighter nonconvex upper bound, namely the ramp loss. Their motivation was not the accuracy
though, but the faster optimization due to the decreased number of support vectors. The resulting
optimization uses the convex-concave procedure of [22], which is well known in optimization as the
DC-programming method [9].
We extend the notion of ramp loss to structured estimation. We show that with some minor mod-
iﬁcations, the DC algorithms used in the binary case carry over to the structured setting. Unlike
the binary case, however, we observe that for structured prediction problems with noisy data, DC
programming can lead to improved accuracy in practice. This is due to increased robustness. Effec-
tively, the algorithm discards observations which it labels incorrectly if the error is too large. This
ensures that one ends up with a lower-complexity solution while ensuring that the “correctable”
errors are taken care of.

1

2 Structured Estimation
Denote by X the set of patterns and let Y be the set of labels. We will denote by X := {x1 , . . . , xm}
the observations and by Y := {y1 , . . . , ym} the corresponding set of labels. Here the pairs (xi , yi )
are assumed to be drawn from some distribution Pr on X × Y.
Let f : X × Y → R be a function deﬁned on the product space. Finally, denote by ∆ : Y × Y → R+
0
a loss function which maps pairs of labels to nonnegative numbers. This could be, for instance, the
number of bits in which y and y 0 differ, i.e. ∆(y , y 0 ) = ky − y 0 k1 or considerably more complicated
loss functions, e.g., for ranking and retrieval [21]. We want to ﬁnd f such that for
f (x, y 0 )
y∗ (x, f ) := argmax
y 0
mX
the loss ∆(y , y∗ (x, f )) is minimized: given X and Y we want to minimize the regularized risk,
1
∆(yi , y∗ (xi , f )) + λΩ[f ].
m
i=1
Here Ω[f ] is a regularizer, such as an RKHS norm Ω[f ] = kf k2
H and λ > 0 is the associated regular-
ization constant, which safeguards us against overﬁtting. Since (2) is notoriously hard to minimize
several convex upper bounds have been proposed to make ∆(yi , y∗ (xi , f )) tractable in f . The fol-
lowing lemma, which is a generalization of a result of [20] provides a strategy for convexiﬁcation:
0 → R+
Lemma 1 Denote by Γ : R+
0 a monotonically increasing nonnegative function. Then
Γ(∆(y , y 0 )) [f (x, y 0 ) − f (x, y 00 )] + ∆(y , y 0 ) ≥ ∆ (y , y∗ (x, f ))
l(x, y , y 00 , f ) := sup
y 0
for all y , y 00 ∈ Y. Moreover, l(x, y , y 00 , f ) is convex in f .

Rreg [f , X, Y ] :=

(1)

(2)

Proof Convexity follows immediately from the fact that l is the supremum over linear functions
in f . To see the inequality, plug y 0 = y∗ (x, f ) into the LHS of the inequality: by construction
f (x, y∗ (x, f )) ≥ f (x, y 00 ) for all y 00 ∈ Y.
In regular convex structured estimation, l(x, y , y , f ) is used. Methods in [18] choose the constant
function Γ(η) = 1, whereas methods in [20] choose margin rescaling by means of Γ(η) = η . This
also shows why both formulations lead to convex upper bounds of the loss. It depends very much
on the form of f and ∆ which choice of Γ is easier to handle. Note that the inequality holds for all
y 00 rather than only for the “correct” label y 00 = y . We will exploit this later.

3 A Tighter Bound
For convenience denote by β (x, y , y 0 , f ) the relative margin between y and y 0 induced by f via
β (x, y , y 0 , f ) := Γ(∆(y , y 0 ))[f (x, y 0 ) − f (x, y)].
(3)
The loss bound of Lemma 1 suffers from a signiﬁcant problem: for large values of f the loss may
grow without bound, provided that the estimate is incorrect. This is not desirable since in this
setting even a single observation may completely ruin the quality of the convex upper bound on the
misclassiﬁcation error.
Another case where the convex upper bound is not desirable is the following: imagine that there are
a lot of y which are as good as the label in the training set; this happens frequently in ranking where
there are ties between the optimal permutations. Let us denote by Yopt := {y 00 such that ∆(y , y 0 ) =
∆(y 00 , y 0 ), ∀y 0 } this set of equally good labels. Then one can replace y by any element of Yopt in
the bound of Lemma 1. Minimization over y 00 ∈ Yopt leads to a tighter non-convex upper bound:
l(x, y , y , f ) ≥ inf
β (x, y 00 , y 0 , f ) + ∆(y 00 , y 0 ) ≥ ∆ (y , y∗ (x, f )) .
sup
y 00∈Yopt
y 0

In the case of binary classiﬁcation, [2] proposed the following non-convex loss that can be minimized
using DC programming:
l(x, y , f ) := min(1, max(0, 1 − yf (x))) = max(0, 1 − yf (x)) − max(0, −yf (x)).

(4)

2

We see that (4) is the difference between a soft-margin loss and a hinge loss. That is, the difference
between a loss using a large margin related quantity and one using simply the violation of the margin.
This difference ensures that l cannot increase without bound, since in the limit the derivative of l
with respect to f vanishes. The intuition for extending this to structured losses is that the generalized
hinge loss underestimates the actual loss whereas the soft margin loss overestimates the actual loss.
Taking the difference removes linear scaling behavior while retaining the continuous properties.

Lemma 2 Denote as follows the rescaled estimate and the margin violator
β (x, y , y 0 , f ) and ¯y(x, y , f ) := argmax
β (x, y , y 0 , f ) + ∆(y , y 0 )
˜y(x, y , f ) := argmax
y 0
y 0

Moreover, denote by l(x, y , f ) the following loss function
[β (x, y , y 0 , f ) + ∆(y , y 0 )] − sup
l(x, y , f ) := sup
y 0
y 0
Then under the assumptions of Lemma 1 the following bound holds
∆(y , ¯y(x, y , f )) ≥ l(x, y , f ) ≥ ∆(y , y∗ (x, f ))

β (x, y , y 0 , f ).

(5)

(6)

(7)

This loss is a difference between two convex functions, hence it may be (approximately) minimized
by a DC programming procedure. Moreover, it is easy to see that for Γ(η) = 1 and f (x, y) =
2 yf (x) and y ∈ {±1} we recover the ramp loss of (4).
1
Proof Since ¯y(x, y , f ) maximizes the ﬁrst term in (6), replacing y 0 by ¯y(x, y , f ) in both terms yields
l(x, y , f ) ≤ β (x, y , ¯y , f ) + ∆(y , ¯y) − β (x, y , ¯y , f ) = ∆(y , ¯y).
To show the lower bound, we distinguish the following two cases:
Case 1: y∗ is a maximizer of supy 0 β (x, y , y 0 , f )
Replacing y 0 by y∗ in both terms of (6) leads to l(x, y , f ) ≥ ∆(y , y∗ ).
Case 2: y∗ is not a maximizer of supy 0 β (x, y , y 0 , f )
Let ˜y be any maximizer. Because f (x, y∗ ) ≥ f (x, ˜y), we have Γ(∆(y , ˜y)) [f (x, y∗ ) − f (x, y)] >
Γ(∆(y , ˜y)) [f (x, ˜y) − f (x, y)] > Γ(∆(y , y∗ )) [f (x, y∗ ) − f (x, y)] and thus Γ(∆(y , ˜y)) >
Γ(∆(y , y∗ )). Since Γ is non-decreasing this implies ∆(y , ˜y) > ∆(y , y∗ ). On the other hand,
plugging ˜y in (6) gives l(x, y , f ) ≥ ∆(y , ˜y). Combining both inequalities proves the claim.
Note that the main difference between the cases of constant Γ and monotonic Γ is that in the latter
case the bounds are not quite as tight as they could potentially be, since we still have some slack with
respect to ∆(y , ˜y). Monotonic Γ tend to overscale the margin such that more emphasis is placed on
avoiding large deviations from the correct estimate rather than restricting small deviations.
Note that this nonconvex upper bound is not likely to be Bayes consistent. However, it will generate
solutions which have a smaller model complexity since it is never larger than the convex upper bound
on the loss, hence the regularizer on f plays a more important role in regularized risk minimization.
As a consequence one can expect better statistical concentration properties.

4 DC Programming

We brieﬂy review the basic template of DC programming, as described in [22]. For a function
f (x) = fcave (x) + fvex (x)
which can be expressed as the sum of a convex fvex and a concave fcave function, we can ﬁnd a
convex upper bound by fcave (x0 ) + hx − x0 , f 0
cave (x0 )i + fvex (x). This follows from the ﬁrst-order
Taylor expansion of the concave part fcave at the current value of x. Subsequently, this upper bound
is minimized, a new Taylor approximation is computed, and the procedure is repeated. This will
lead to a local minimum, as shown in [22].
We now proceed to deriving an explicit instantiation for structured estimation. To keep things simple,
in particular the representation of the functional subgradients of l(x, y , f ) with respect to f , we
assume that f is drawn from a Reproducing Kernel Hilbert Space H.

3

Using the loss of Lemma 1 initialize f = argminf 0 Pm
Algorithm 1 Structured Estimation with Tighter Bounds
i=1 l(xi , yi , yi , f 0 ) + λΩ[f 0 ]
Using the tightened loss bound recompute f = argminf 0 Pm
repeat
Compute ˜yi := ˜y(xi , yi , f ) for all i.
i=1
until converged

˜l(xi , yi , ˜yi , f 0 ) + λΩ[f 0 ]

Denote by k the kernel associated with H, deﬁned on (X × Y) × (X × Y). In this case for f ∈ H
we have by the reproducing property that f (x, y) = hf , k((x, y), ·)i and the functional derivative is
given by ∂f f (x, y) = k((x, y), ·). Likewise we may perform the linearization in (6) as follows:
− sup
β (x, y , y 0 , f ) ≤ −β (x, y , ˜y , f )
y 0
In other words, we use the rescaled estimate ˜y to provide an upper bound on the concave part of
the loss function. This leads to the following instantiation of standard convex-concave procedure:
instead of the structured estimation loss it uses the loss bound ˜l(x, y , ˜y , f )
[β (x, y , y 0 , f ) + ∆(y , y 0 )] − β (x, y , ˜y , f )
˜l(x, y , ˜y , f ) := sup
y 0∈Y
In the case of Γ(η) = 1 this can be simpliﬁed signiﬁcantly: the terms in f (x, y) cancel and ˜l becomes
[f (x, y 0 ) − f (x, ˜y)] + ∆(y , y 0 ).
˜l(x, y , ˜y , f ) = sup
y 0∈Y
In other words, we replace the correct label y by the rescaled estimate ˜y . Such modiﬁcations can be
easily implemented in bundle method solvers and related algorithms which only require access to
the gradient information (and the function value). In fact, the above strategy follows directly from
Lemma 1 when replacing y 00 by the rescaled estimate ˜y .

5 Experiments

5.1 Multiclass Classiﬁcation

In this experiment, we investigate the performance of convex and ramp loss versions of the Winner-
Takes-All multiclass classiﬁcation [1] when the training data is noisy. We performed the experiments
on some UCI/Statlog datasets: DNA, LETTER, SATIMAGE, SEGMENT, SHUTTLE, and USPS,
with some ﬁxed percentages of the labels shufﬂed, respectively. Note that we reshufﬂed the labels
in a stratiﬁed fashion. That is, we chose a ﬁxed fraction from each class and we permuted the label
assignment subsequently.
Table 1 shows the results (average accuracy ± standard deviation) on several datasets with different
percentages of labels shufﬂed. We used nested 10-fold crossvalidation to adjust the regularization
constant and to compute the accuracy. A linear kernel was used. It can be seen that ramp loss
outperforms the convex upper bound when the datasets are noisy. For clean data the convex upper
bound is slightly superior, albeit not in a statistically signiﬁcant fashion. This supports our conjecture
that, compared to the convex upper bound, the ramp loss is more robust on noisy datasets.

5.2 Ranking with Normalized Discounted Cumulative Gains

Recently, [12] proposed a method for learning to rank for web search. They compared several meth-
ods showing that optimizing the Normalized Discounted Cumulative Gains (NDCG) score using
a form of structured estimation yields best performance. The algorithm used a linear assignment
problem to deal with ranking.
In this experiment, we perform ranking experiments with the OHSUMED dataset which is publicly
available [13]. The dataset is already preprocessed and split into 5 folds. We ﬁrst carried out the
structured output training algorithm which optimizes the convex upper bound of NDCG as described
in [21]. Unfortunately, the returned solution was f = 0. The convex upper bounds led to the

4

Dataset
DNA

LETTER

SATIMAGE

SEGMENT

SHUTTLE

USPS

Methods
convex
ramp loss
convex
ramp loss
convex
ramp loss
convex
ramp loss
convex
ramp loss
convex
ramp loss

0%
95.2 ± 1.1
95.1 ± 0.8
76.8 ± 0.9
78.6 ± 0.8
85.1 ± 0.9
85.4 ± 1.2
95.4 ± 0.9
95.2 ± 1.0
97.4 ± 0.2
97.1 ± 0.2
95.1 ± 0.7
95.1 ± 0.9

10%
88.9 ± 1.5
89.1 ± 1.3
64.6 ± 0.7
70.8 ± 0.8
77.0 ± 1.6
78.1 ± 1.6
84.8 ± 2.3
85.9 ± 2.1
89.5 ± 0.2
90.6 ± 0.8
85.3 ± 1.3
86.1 ± 1.6

20%
83.1 ± 2.4
83.5 ± 2.2
50.1 ± 1.4
63.0 ± 1.5
66.4 ± 1.3
70.7 ± 1.0
73.8 ± 2.1
77.5 ± 2.0
83.8 ± 0.2
88.1 ± 0.3
76.5 ± 1.4
77.6 ± 1.1

Table 1:
Average accu-
racy for multiclass classiﬁ-
cation using the convex up-
per bound and the ramp
loss. The third through ﬁfth
columns represent results for
datasets with none, 10%, and
20% of the labels randomly
shufﬂed, respectively.

Figure 1: NDCG comparison against
ranking SVM and RankBoost. We
report the NDCG computed at vari-
ous truncation levels. Our non-convex
upper bound consistently outperforms
other
rankers.
In the context of
web page ranking an improvement of
0.01 − 0.02 in the NDCG score is con-
sidered substantial.

undesirable situation where no nonzero solution would yield any improvement, since the linear
function class was too simple.
This problem is related to the fact that there are a lot of rankings which are equally good because of
the ties in the editorial judgments (see beginning of section 3). As a result, there is no w that learns
the data well, and for each w the associated maxy 0 f (x, y 0 ) − f (x, y) + ∆(y , y 0 ) causes either the
ﬁrst part or the second part of the loss to be big such that the total value of the loss function always
exceeds max ∆(y , y 0 ).
When using the non-convex formulation the problem can be resolved because we do not entirely rely
on the y given in the training set, but instead ﬁnd the y that minimizes the loss. We compared the
results of our method and two standard methods for ranking: ranking SVM [10, 8] and RankBoost
[6] (the baselines for OHSUMED are shown in [13]) and used NDCG as the performance criterion.
We report the aggregate performance in Figure 1.
As can be seen from the ﬁgure, the results from the new formulation are better than standard methods
for ranking. It is worth emphasizing that the most important contribution is not only that the new
formulation can give comparable results to the state-of-the-art algorithms for ranking but also that it
provides useful solutions when the convex structured estimation setting provides only useless results
(obviously f = 0 is highly undesirable).

5.3 Structured classiﬁcation

We also assessed the performance of the algorithm on two different structured classiﬁcation tasks for
computational biology, namely protein sequence alignment and RNA secondary structure prediction.

Protein sequence alignment
is the problem of comparing the amino acid sequences correspond-
ing to two different proteins in order to identify regions of the sequences which have common ances-
try or biological function. In the pairwise sequence alignment task, the elements of the input space
X consist of pairs of amino acid sequences, represented as strings of approximately 100-1000 char-

5

123456789100.430.440.450.460.470.480.490.50.510.520.53truncation levelNDCG@k  svmrankrankboostndcg optimizationMethod
CRF
convex
ramp loss

0-10% 11-20% 21-30% 31-40% Overall
(1785)
(239)
(429)
(793)
(324)
0.430
0.877
0.634
0.316
0.111
0.472
0.891
0.699
0.369
0.116
0.138
0.387
0.708
0.905
0.488

Table 2: Protein pairwise sequence alignment results, stratiﬁed by reference alignment percentage
identity. The second through ﬁfth columns refer to the four non-overlapping reference alignment
percentage identity ranges described in the text, and the sixth column corresponds to overall results,
pooled across all four subsets. Each non-bolded value represents the average test set recall for
a particular algorithm on alignment from the corresponding subset. The numbers in parentheses
indicate the total number of sequences in each subset.
101-200
51-100
1-50
(118)
(489)
(478)
0.467 / 0.523
0.586 / 0.727
0.546 / 0.862
0.571 / 0.501
0.664 / 0.629
0.690 / 0.755
0.725 / 0.708
0.705 / 0.602
0.612 / 0.489

Overall
(1359)
0.505 / 0.614
0.608 / 0.565
0.646 / 0.542

Method
CRF
convex
ramp loss

201+
(274)
0.414 / 0.472
0.542 / 0.484
0.569 / 0.461

Table 3: RNA secondary structure prediction results. The second through ﬁfth columns represent
subsets of the data stratiﬁed by sequence length. The last column presents overall results, pooled
across all four subsets. Each pair of non-bolded numbers indicates the sensitivity / selectivity for
structures in the two-fold cross-validation. The numbers in parentheses indicate the total number of
sequences in each subset.

acters in length. The output space Y contains candidate alignments which identify the corresponding
positions in the two sequences which are hypothesized to be evolutionarily related.
We developed a structured prediction model for pairwise protein sequence alignment, using the
types of features described in [3, 11] For the loss function, we used ∆(y , y 0 ) = 1 − recall (where
recall is the proportion of aligned amino acid matches in the true alignment y that appear in the
predicted alignment y 0 . For each inner optimization step, we used a fast-converging subgradient-
based optimization algorithm with an adaptive Polyak-like step size [23].
We performed two-fold cross-validation over a collection of 1785 pairs of structurally aligned pro-
tein domains [14]. All hyperparameters were selected via holdout cross validation on the training
set, and we pooled the results from the two folds. For evaluation, we used recall, as described previ-
ously, and compared the performance of our algorithm to a standard conditional random ﬁeld (CRF)
model and max-margin model using the same features. The percentage identity of a reference align-
ment is deﬁned as the proportion of aligned residue pairs corresponding to identical amino acids.
We partitioned the alignments in the testing collection into four subsets based on percent identity
(0-10%, 11-20%, 21-30%, and 31+%), showed the recall of the algorithm for each subset in addition
to overall recall (see Table 2).
Here, it is clear that our method obtains better accuracy than both the CRF and max-margin models.1
We note that the accuracy differences are most pronounced at the low percentage identity ranges,
the ‘twilight zone’ regime where better alignment accuracy has far reaching consequences in many
other computational biology applications [16].

RNA secondary structure prediction Ribonucleic acid (RNA) refers to a class of long linear
polymers composed of four different types of nucleotides (A, C, G, U). Nucleotides within a single
RNA molecule base-pair with each other, giving rise to a pattern of base-pairing known as the
RNA’s secondary structure. In the RNA secondary structure prediction problem, we are given an
RNA sequence (a string of approximately 20-500 characters) and are asked to predict the secondary
structure that the RNA molecule will form in vivo. Conceptually, an RNA secondary structure can
be thought of as a set of unordered pairs of nucleotide indices, where each pair designates two

1We note that the results here are based on using the Viterbi algorithm for parsing, which differs from the
inference method used in [3]. In practice this is preferable to posterior decoding as it is signiﬁcantly faster
which is crucial applications to large amounts of data.

6

(c)
(b)
(a)
Figure 2: Tightness of the nonconvex bound. Figures (a) and (b) show the value of the nonconvex
loss, the convex loss and the actual loss as a function of the number of iterations when minimizing the
nonconvex upper bound. At each relinearization, which occurs every 1000 iterations, the nonconvex
upper bound decreases. Note that the convex upper bound increases in the process as convex and
nonconvex bound diverge further from each other. We chose λ = 2−6 in Figure (a) and λ = 27 for
Figure (b). Figure (c) shows the tightness of the ﬁnal nonconvex bound at the end of optimization
for different values of the regularization parameter λ.

nucleotides in the RNA molecule which base-pair with each other. Following convention, we take
the structured output space Y to be the set of all possible pseudoknot-free structures.
We used a max-margin model for secondary structure prediction. The features of the model were
chosen to match the energetic terms in standard thermodynamic models for RNA folding [4]. As
our loss function, we used ∆(y , y 0 ) = 1 − recall (where recall is the proportion of base-pairs in the
reference structure y that are recovered in the predicted structure y 0 ). We again used the subgradient
algorithm for optimization.
To test the algorithm, we performed two-fold cross-validation over a large collection of 1359 RNA
sequences with known secondary structures from the RFAM database (release 8.1) [7]. We evaluated
the methods using two standard metrics for RNA secondary structure prediction accuracy known as
sensitivity and selectivity (which are the equivalent of recall and precision, respectively, for this
domain). For reporting, we binned the sequences in the test collection by length into four ranges (1-
50, 51-100, 101-200, 201+ nucleotides), and evaluated the sensitivity and selectivity of the algorithm
for each subset in addition to overall accuracy (see Table 3).
Again, our algorithm consistently outperforms an equivalently parameterized CRF and max-margin
model in terms of sensitivity.2 The selectivity of the predictions from our algorithm is often worse
than that of the other two models. This is likely because we opted for a loss function that penalizes
for “false negative” base-pairings but not “false-positives” since our main interest is in identifying
correct base-pairings (a harder task than predicting only a small number of high-conﬁdence base-
pairings). An alternative loss function that chooses a different balance between penalizing false
positives and false negatives would achieve a different trade-off of sensitivity and selectivity.
Tightness of the bound: We generated plots of the convex, nonconvex, and actual losses (which
correspond to l(x, y , y , f ), l(x, y , f ), and ∆(y , y∗ (x, f )), respectively, from Lemma 2) over the
course of optimization for our RNA folding task (see Figure 2). From Figures 2a and 2b, we see
that the nonconvex loss provides a much tighter upper bound on the actual loss function. Figure 2c
shows that the tightness of the bound decreases for increasing regularization parameters λ.
In summary, our bound leads to improvements whenever there is a large number of instances (x, y)
which cannot be classiﬁed perfectly. This is not surprising as for “clean” datasets even the convex
upper bound vanishes when no margin errors are encountered. Hence noticeable improvements can
be gained mainly in the structured output setting rather than in binary classiﬁcation.

2Note that the results here are based on using the CYK algorithm for parsing, which differs from the infer-
ence method used in [4].

7

6 Summary and Discussion

We proposed a simple modiﬁcation of the convex upper bound of the loss in structured estimation
which can be used to obtain tighter bounds on sophisticated loss functions. The advantage of our
approach is that it requires next to no modiﬁcation of existing optimization algorithms but rather
repeated invocation of a structured estimation solver such as SVMStruct, BMRM, or Pegasos.
In several applications our approach outperforms the convex upper bounds. This can be seen both
for multiclass classiﬁcation, for ranking where we encountered underﬁtting and undesirable trivial
solutions for the convex upper bound, and in the context of sequence alignment where in particular
for the hard-to-align observations signiﬁcant gains can be found.
From this experimental study, it seems that the tighter non-convex upper bound is useful in two
scenarios: when the labels are noisy and when for each example there is a large set of labels which
are (almost) as good as the label in the training set. Future work includes studying other types
of structured estimation problems such as the ones encountered in NLP to check if our new upper
bound can also be useful for these problems.

References
[1] K. Crammer, and Y. Singer. On the Learnability and Design of Output Codes for Multiclass Problems. In
COLT 2000, pages 35–46. Morgan Kaufmann, 2000.
[2] R. Collobert, F.H. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability. In ICML 2006, pages
201–208. ACM, 2006.
[3] C. B. Do, S. S. Gross, and S. Batzoglou. CONTRAlign: discriminative training for protein sequence
alignment. In RECOMB, pages 160–174, 2006.
[4] C. B. Do, D. A. Woods, and S. Batzoglou. CONTRAfold: RNA secondary structure prediction without
physics-based models. Bioinformatics, 22(14):e90–e98, 2006.
[5] S. R. Eddy. Non-coding RNA genes and the modern RNA world. Nature Reviews Genetics, 2(12):919–
929, 2001.
[6] Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining prefer-
ences. In ICML 1998, pages 170–178., 1998.
[7] S. Grifﬁths-Jones, S. Moxon, M. Marshall, A. Khanna, S. R. Eddy, and A. Bateman. Rfam: annotating
non-coding RNAs in complete genomes. Nucl. Acids Res., 33:D121–D124, 2005.
[8] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In
Advances in Large Margin Classiﬁers, pages 115–132, 2000. MIT Press.
[9] T. Hoang. DC optimization: Theory, methods, and applications. In R. Horst and P. Pardalos, editors,
Handbook of Global Optimization, Kluwer.
[10] T. Joachims. Optimizing search engines using clickthrough data. In KDD. ACM, 2002.
[11] T. Joachims, T. Galor, and R. Elber. Learning to align sequences: A maximum-margin approach. In New
Algorithms for Macromolecular Simulation, LNCS 49, 57–68. Springer, 2005.
[12] Q. Le and A.J. Smola. Direct optimization of ranking measures. NICTA-TR, 2007.
[13] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank
for information retrieval. In LR4IR, 2007.
[14] J. Pei and N. V. Grishin. MUMMALS: multiple sequence alignment improved by using hidden Markov
models with local structural information. Nucl. Acids Res., 34(16):4364–4374, 2006.
[15] N. Ratliff, J. Bagnell, and M. Zinkevich.
(online) subgradient methods for structured prediction.
AISTATS, 2007.
[16] B. Rost. Twilight zone of protein sequence alignments. Protein Eng., 12(2):85–94, 1999.
[17] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In
Proc. Intl. Conf. Machine Learning, 2007.
[18] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS 16, pages 25–32, 2004.
MIT Press.
[19] C.H. Teo, Q. Le, A.J. Smola, and S.V.N. Vishwanathan. A scalable modular convex solver for regularized
risk minimization. In KDD. ACM, 2007.
[20] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and
interdependent output variables. J. Mach. Learn. Res., 6:1453–1484, 2005.
[21] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. Coﬁ rank - maximum margin matrix factorization for
collaborative ranking. In NIPS 20. MIT Press, 2008.
[22] A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15:915–936, 2003.
[23] A. Nedic and D. P. Bertsekas. Incremental subgradient methods for nondifferentiable optimization. Siam
J. on Optimization, 12:109–138, 2001.

In

8


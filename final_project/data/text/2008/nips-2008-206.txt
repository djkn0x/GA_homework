Correlated Bigram LSA for Unsupervised Language
Model Adaptation

Yik-Cheung Tam∗
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
yct@cs.cmu.edu

Tanja Schultz
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
tanja@cs.cmu.edu

Abstract

We present a correlated bigram LSA approach for unsupervised LM adaptation for
automatic speech recognition. The model is trained using efﬁcient variational EM
and smoothed using the proposed fractional Kneser-Ney smoothing which handles
fractional counts. We address the scalability issue to large training corpora via
bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram
and bigram LSA are integrated into the background N-gram LM via marginal
adaptation and linear interpolation respectively. Experimental results on the Man-
darin RT04 test set show that applying unigram and bigram LSA together yields
6%–8% relative perplexity reduction and 2.5% relative char acter error rate reduc-
tion which is statistically signi ﬁcant compared to applyin g only unigram LSA.
On the large-scale evaluation on Arabic, 3% relative word error rate reduction is
achieved which is also statistically signi ﬁcant.

1

Introduction

Language model (LM) adaptation is crucial to automatic speech recognition (ASR) as it enables
higher-level contextual information to be effectively incorporated into a background LM improving
recognition performance. Exploiting topical context for LM adaptation has shown to be effective
for ASR using latent semantic analysis (LSA) such as LSA using singular value decomposition [1],
Latent Dirichlet Allocation (LDA) [2, 3, 4] and HMM-LDA [5, 6]. One issue in LSA is the bag-
of-word assumption which ignores word ordering. For document classi ﬁcation, word ordering may
not be important. But in the LM perspective, word ordering is crucial since a trigram LM normally
performs signi ﬁcantly better than a unigram LM for word pred iction. In this paper, we investigate
whether relaxing the bag-of-word assumption in LSA helps improving the ASR performance via
LM adaptation.

We employ bigram LSA [7] which is a natural extension of LDA to relax the bag-of-word assump-
tion by connecting the adjacent words in a document together to form a Markov chain. There are
two main challenges in bigram LSA which are not addressed properly in [7] especially for large-
scale application. Firstly, the model can be very sparse since it covers topical bigrams in O(V 2 · K )
where V and K denote the vocabulary size and the number of topics. Therefore, model smoothing
becomes critical. Secondly, model initialization is important for EM training, especially for bigram
LSA due to the model sparsity. To tackle the ﬁrst challenge, w e represent bigram LSA as a set
of K topic-dependent backoff LM. We propose fractional Kneser-Ney smoothing 1 which supports
∗This work is partly supported by the Defense Advanced Research Projects Agency (DARPA) under Con-
tract No. HR0011-06-2-0001. Any opinions, ﬁndings and conclusion s or recommendations expressed in this
material are those of the authors and do not necessarily reﬂect the view s of DARPA.
1This method was brieﬂy mentioned in [8] without detail. To the best of our kn owledge, our formulation in
this paper is considered new to the research community.

Prior distribution over topic mixture weights

θ

Latent topics

z1

z2

Observed words

<s>

w1

w2

zN

wN

Figure 1: Graphical representation of bigram LSA. Adjacent words in a document are linked to-
gether to form a Markov chain from left to right.

fractional counts to smooth each backoff LM. We show that our formulation recovers the original
Kneser-Ney smoothing [9] which supports only integral counts. To address the second challenge,
we propose a bootstrapping approach for bigram LSA training using a well-trained unigram LSA as
an initial model.

During unsupervised LM adaptation, word hypotheses from the ﬁrst-pass decoding are used to es-
timate the topic mixture weight of each test audio to adapt both unigram and bigram LSA. The
adapted unigram and bigram LSA are combined with the background LM in two stages. Firstly,
marginal adaptation [10] is applied to integrate unigram LSA into the background LM. Then the in-
termediately adapted LM from the ﬁrst stage is combined with bigram LSA via linear interpolation
with the interpolation weights estimated by minimizing the word perplexity on the word hypotheses.
The ﬁnal adapted LM is employed for re-decoding.

Related work includes topic mixtures [11] which perform document clustering and train a trigram
LM for each document cluster as an initial model. Sentence-level topic mixtures are modeled so that
the topic label is ﬁxed within a sentence. Topical N-gram mod el [12] focuses on phrase discovery
and information retrieval. We do not apply this model because the phrase-based LM seems not
outperform the word-based LM.

The paper is organized as follows: In Section 2, we describe the bigram LSA training and the
fractional Kneser-Ney smoothing algorithm. In Section 3, we present the LM adaptation approach
based on marginal adaptation and linear interpolation. In Section 4, we report LM adaptation results
on Mandarin and Arabic ASR, followed by conclusions and future work in Section 5.

2 Correlated bigram LSA

Latent semantic analysis such as LDA makes a bag-of-word assumption that each word in a docu-
ment is generated irrespective of its position in a document. To relax this assumption, bigram LSA
has been proposed [7] to modify the graphical structure of LDA by connecting adjacent words in a
document together to form a Markov chain. Figure 1 shows the graphical representation of bigram
LSA where the top node represents the prior distribution over the topic mixture weights and the
middle layer represents the latent topic label associated to each observed word at the bottom layer.
The document generation procedure of bigram LSA is similar to LDA except that the previous word
is taken into consideration for generating the current word:

1. Sample θ from a prior distribution p(θ)
2. For each word wi at the i-th position of a document:
(a) Sample topic label: zi ∼ Multinomial(θ)
(b) Sample wi given the previous word wi−1 and the topic label zi : wi ∼ p(·|wi−1 , zi )

Our incremental contributions for bigram LSA are three-folded: Firstly, we present a technique
for topic correlation modeling using Dirichlet-Tree prior in Section 2.1. Secondly, we propose
efﬁcient algorithm for bigram LSA training via variational Bayes approach and model bootstrapping
which are scalable to large settings in Section 2.2. Thirdly, we formulate the fractional Kneser-Ney
smoothing to generalize the original Kneser-Ney smoothing which supports only integral counts in
Section 2.3.

j=1
Dir(.)

j=2
Dir(.)

j=3
Dir(.)

j=J
Dir(.)

propagate

j=1
Dir(.)

0.1+0.2

0.3+0.4

j=2
Dir(.)

j=3
Dir(.)

Latent topics

topic 1

topic 2 topic 3

topic 4

topic K−1

topic K

q(z=k)

0.1

0.2

0.3

0.4

Figure 2: Left: Dirichlet-Tree prior of depth two. Right: Variational E-step as bottom-up propaga-
tion and summation of fractional topic counts.

2.1 Topic correlation

Modeling topic correlations is motivated by an observation that documents such as newspaper arti-
cles are usually organized into main-topic and sub-topic hierarchy for document browsing. From this
perspective, a Dirichlet prior is not appropriate since it assumes topic independence. A Dirichlet-
Tree prior [13, 14] is employed to capture topic correlations. Figure 2 (Left) illustrates a depth-two
Dirichlet-Tree. A depth-one Dirichlet-tree is equivalent to a Dirichlet prior in LDA. The sampling
procedure for the topic mixture weight θ ∼ p(θ) can be described as follows:

1. Sample a vector of branch probabilities bj ∼ Dirichlet(·; {αj c }) for each node j = 1...J
where {αj c } denotes the parameter of the Dirichlet distribution at node j , i.e. the pseudo-
counts of the outgoing branch c at node j .
δjc (k)
where δj c (k) is an indicator function
2. Compute the topic mixture weight as θk = Qj c b
j c
which sets to unity when the c-th branch of the j -th node leads to the leaf node of topic k
and zero otherwise. The k-th topic weight θk is computed as the product of sampled branch
probabilities from the root node to the leaf node corresponding to topic k .

The structure and the number of outgoing branches of each Dirichlet node can be arbitrary. In this
paper, we employ a balanced binary Dirichlet-tree.

2.2 Model training

(1)

p(wN
1 , zN
1 , θ) = p(θ) ·

Gibbs sampling was employed for bigram LSA training [7]. Despite the simplicity, it can be slow
and inefﬁcient since it usually requires many sampling iter ations for convergence. We present a
1 , the latent
variational Bayes approach for model training. The joint likelihood of a document wN
1 and θ using the bigram LSA can be written as follows:
topic sequence zN
N
Yi=1
p(zi |θ) · p(wi |wi−1 , zi )
1 , θ ; Γ) = q(θ) · QN
By introducing a factorizable variational posterior distribution q(zN
i=1 q(zi )
over the latent variables and applying the Jensen’s inequality, the lower bound of the marginalized
document likelihood can be derived as follows:
1 , zN
p(wN
1 , θ ; Λ)
1 ; Λ, Γ) = log Zθ Xz1 ...zN
log p(wN
q(zN
1 , θ ; Γ)
1 , zN
p(wN
1 , θ ; Λ)
≥ Zθ Xz1 ...zN
q(zN
1 , θ ; Γ) · log
q(zN
1 , θ ; Γ)
N
N
p(θ)
Xi=1
Xi=1
= Eq [log
q(θ)
= Q(wN
(5)
1 ; Λ, Γ)
where the expectation is taken using the variational posterior q(zN
1 , θ). For the E-step, we compute
the partial derivative of the auxiliary function Q(·) with respect to q(zi ) and the parameter γj c in the
Dirichlet-Tree posterior q(θ). Setting the derivatives to zero yields:

(By Jensen’s Inequality) (3)

p(zi |θ)
q(zi )

q(zN
1 , θ ; Γ) ·

(2)

] +

Eq [log

] +

Eq [log p(wi |wi−1 , zi )]

(4)

E-Steps:

(6)

γj c = αj c +

q(zi = k) ∝ p(wi |wi−1 , k) · eEq [log θk ;{γjc }] for k = 1..K
K
N
N
Xk=1
Xi=1
Xi=1
(7)
q(zi = k) · δj c (k)
Eq [δj c (zi )] = αj c +
δj c (k) ÃΨ(γj c ) − Ψ(Xc
γj c )! (8)
where Eq [log θk ] = Xj c
δj c (k) · Eq [log bj c ] = Xj c
where Eqn 7 is motivated from the conjugate property that the Dirichlet-Tree posterior given the
topic sequence zN
1 has the same form as the Dirichlet-Tree prior:
j c 
1 ; {αj c }) ∝ 
N
δjc (zi )
 · Yj c
Yi=1 Yj c
1 |bJ
1 ) ∝ p(zN
1 |zN
p(bJ
1 ) · p(bJ

J
i=1 δjc (zi ))−1
(αjc+P N
γ 0
jc−1
= Yj c
Yj=1
= Yj c
Dirichlet(bj ; {γ 0
j c })
b
j c
j c
Figure 2 (Right) illustrates that Eqn 7 can be implemented as propagation of fractional topic counts
in a bottom-up fashion with each branch as an accumulator for γj c . Eqn 6 and Eqn 7 are applied
iteratively until convergence is reached. For the M-step, we compute the partial derivative of the aux-
iliary function Q(·) over all training documents d with respect to topic bigram probability p(v |u, k)
and set it to zero:
M-Step (unsmoothed):

αjc−1
j c

(10)

=

b

b

(9)

b

(11)

Nd
Xi=1
p(v |u, k) ∝ Xd
q(zi = k |d) · δ(wi−1 , u)δ(wi , v)
Pd Cd (u, v |k)
C (u, v |k)
=
PV
Pd PV
v 0=1 Cd (u, v 0 |k)
v 0=1 C (u, v 0 |k)
where Nd denote the number of words in document d and δ(wi , v) is a 0-1 Kronecker Delta function
to test if the i-th word in document d is vocabulary v . Cd (u, v |k) denotes the fractional counts of a
bigram (u, v) belonging to topic k in document d. Intuitively, Eqn 12 simply computes the relative
frequency of the bigram (u, v). However, this solution is not practical since bigram LSA assigns
zero probability to unseen bigrams. Therefore, bigram LSA should be smoothed properly. One
simple approach is to use Laplace-smoothing by adding a small count δ to all bigrams. However,
this approach can lead to worse performance since it will bias the bigram probability towards a
uniform distribution when the vocabulary size V gets large. Our approach is to represent p(v |u, k)
as a standard backoff LM smoothed by fractional Kneser-Ney smoothing as described in Section 2.3.

(12)

=

Model initialization is crucial for variational EM training. We employ a bootstrapping approach
using a well-trained unigram LSA as an initial model for bigram LSA so that p(wi |wi−1 , k) is
approximated by p(wi |k) in Eqn 6. It saves computation and avoids keeping the full initial bigram
LSA in memory during the EM training. To make the training procedure more practical, we apply
bigram pruning during statistics accumulation in the M-step when the bigram count in a document
is less than 0.1. This heuristic is reasonable since only a small number of topics are “active” to
a bigram. With the sparsity, there is no need to store K copies of accumulators for each bigram
and thus reducing the memory requirement signi ﬁcantly. The pruned bigram counts are re-assigned
to the most likely topic of the current document so that the counts are conserved. For practical
implementation, accumulators are saved into the disk in batches for count merging. In the ﬁnal step,
each topic-dependent LM is smoothed individually using the merged count ﬁle.

2.3 Fractional Kneser-Ney smoothing

Standard backoff N-gram LM is widely used in the ASR community. The state-of-the-art smoothing
for the backoff LM is based on Kneser-Ney smoothing [9]. The belief of its success is due to the
preservation of marginal distributions. However, the original formulation only works for integral

(13)

+ λ(u)

(14)

(15)

pKN (v |u) =

(17)

(18)

+ λ(u) · pKN (v)

C (u, v) − D
C (u)

counts which is not suitable for bigram LSA using fractional counts. Therefore, we propose the
fractional Kneser-Ney smoothing as a generalization of the original formulation. The interpolated
form using absolute discounting can be expressed as follows:
max{C (u, v) − D, 0}
C (u)
where D is a discounting factor. In the original formulation, D lies between 0 and 1. But in our
formulation, D can be any positive number. Intuitively, D controls the degree of smoothing. If D is
set to zero, the model is unsmoothed; If D is too big, bigrams with counts smaller than D are pruned
from the LM. λ(u) ensures the bigram probability sums to unity. After summing over all possible v
on both sides of Eqn 13 and re-arranging terms, λ(u) becomes:
max{C (u, v) − D, 0}
1 = Xv
C (u)
max{C (u, v) − D, 0}
=⇒ λ(u) = 1 − Xv
= 1 − Xv :C (u,v)>D
C (u)
C (u) − Pv :C (u,v)>D C (u, v) + D Pv :C (u,v)>D 1
=
C (u)
= Pv :C (u,v)≤D C (u, v) + D Pv :C (u,v)>D 1
C (u)
C≤D (u, ·) + D · N>D (u, ·)
=
C (u)
where C≤D (u, ·) denotes the sum of bigram counts following u and smaller than D . N>D (u, ·)
denotes the number of word types following u with the bigram counts bigger than D .
In Kneser-Ney smoothing, the lower-order distribution pKN (v) is treated as unknown parameters
which can be estimated using the preservation of marginal distributions:
ˆp(v) = Xu
where ˆp(v) is the marginal distribution estimated from the background training data so that ˆp(v) =
C (v)
P v0 C (v 0 ) . Therefore, we substitute Eqn 13 into Eqn 19:
C (v) = Xu µ max{C (u, v) − D, 0}
+ λ(u) · pKN (v)¶ · C (u)
C (u)
= ÃXu
max{C (u, v) − D, 0}! + pKN (v) · Xu
C (u) · λ(u)
C (v) − Pu max{C (u, v) − D, 0}
=⇒ pKN (v) =
Pu C (u) · λ(u)
C (v) − C>D (·, v) + D · N>D (·, v)
Pu C (u) · λ(u)
C≤D (·, v) + D · N>D (·, v)
Pu C≤D (u, ·) + D · N>D (u, ·)
C≤D (·, v) + D · N>D (·, v)
Pv C≤D (·, v) + D · N>D (·, v)
Eqn 25 generalizes Kneser-Ney smoothing to integral and fractional counts. For the original formu-
lation, C≤D (u, ·) equals to zero since each observed bigram count must be at least one by deﬁnition
with D less than one. As a result, the D term cancels out yielding the original formulation which
counts the number of words preceding v and thus recovering the original formulation. Intuitively,
the numerator in Eqn 25 measures the total discounts of observed bigrams ending at v . In other
words, fractional Kneser-Ney smoothing estimates the lower-order probability distribution using the
relative frequency over discounts instead of word counts. With this approach, each topic-dependent
LM in bigram LSA can be smoothed using our formulation.

pKN (v |u) · ˆp(u)

(using Eqn 18)

=

=

=

(22)

(23)

(24)

(25)

(16)

(19)

(20)

(21)

3 Unsupervised LM adaptation

Unsupervised LM adaptation is performed by ﬁrst inferring t he topic distribution of each test audio
using the word hypotheses from the ﬁrst-pass decoding via va riational inference in Eqn 6–7. Relative
frequency over the branch posterior counts γj c is applied on each Dirichlet node j . The MAP topic
mixture weight ˆθ and the adapted unigram and bigram LSA are computed as follows:
Pc0 γj c0 ¶δjc (k)
ˆθk ∝ Yj c µ γj c
K
K
p(v |k) · ˆθk and pa (v |u) =
Xk=1
Xk=1
pa (v) =
The unigram LSA marginals are integrated into the background N-gram LM pbg (v |h) via marginal
adaptation [10] as follows:

p(v |u, k) · ˆθk

for k = 1...K

(26)

(27)

(28)

· pbg (v |h)

pbg (v) ¶β
a (v |h) ∝ µ pa (v)
p(1)
Marginal adaptation has a close connection to maximum entropy modeling since the marginal con-
straints can be encoded as unigram features. Intuitively, bigram LSA would be integrated in the same
fashion by introducing bigram marginal constraints. However, we found that integrating bigram
features via marginal adaptation did not offer further improvement compared to only integrating un-
igram features. Since marginal adaptation integrates a unigram feature as a likelihood ratio between
the adapted marginal pa (v) and the background marginal pbg (v) in Eqn 28, perhaps the unigram and
bigram likelihood ratios are very similar and thus the latter does not give extra information. Another
explanation is that marginal adaptation corresponds to only one iteration of generalized iterative
scaling (GIS). Due to the large number of bigram features in terms of millions, one GIS iteration
may not be sufﬁcient for convergence. On the other hand, simp le linear LM interpolation is found
to be effective in our experiment. The ﬁnal LM adaptation for mula is provided using results from
Eqn 27 and Eqn 28 as a two-stage process:

p(2)
a (v |h) = λ · p(1)
a (v |h) + (1 − λ) · pa (v |u)

(29)

where λ is tuned to optimize perplexity on word hypotheses from the ﬁ rst-pass decoding on a per-
audio basis.

4 Experimental setup

Our LM adaptation approach was evaluated using the RT04 Mandarin Broadcast News evaluation
system. The system employed context-dependent Initial-Final acoustic models trained using 100-
hour broadcast news audio from the Mandarin HUB4 1997 training set and a subset of TDT4. 42-
dimension features were extracted after linear discriminant analysis projected from a window of
MFCC and energy features. The system employed a two-pass decoding strategy using speaker-
independent and speaker-adaptive acoustic models. For the second-pass decoding, we applied stan-
dard acoustic model adaptation such as vocal tract length normalization and maximum likelihood
linear regression on the feature and model spaces. The training corpora include Xinhua News 2002
(January–September) containing 13M words and 64k document s. A background 4-gram LM was
trained using modi ﬁed Kneser-Ney smoothing using the SRILM toolkit [15]. The same training
corpora were used for unigram and bigram LSA training with 200 topics. The vocabulary size is
108k words. Discounting factor D for fractional Kneser-Ney smoothing was set to 0.4.
First-pass decoding was ﬁrst performed to obtain an automat
ic transcript for each audio show. Then
unsupervised LM adaptation was applied using the automatic transcript to obtain an adapted LM
for second-pass decoding using the approach described in Section 3. Word perplexity and character
error rates (CER) were measured on the Mandarin RT04 test set. Matched pairs sentence-segment
word error test was performed for signi ﬁcance test using the NIST scoring tool.

Table 1: Correlated bigram topics extracted from bigram LSA.

Topic index

Top bigrams sorted by p(u, v |k)

“topic-61”

“topic-62”

“topic-63”

“topic-64”

“topic-65”

ሇ+ࣣᅴ(’s student), ሇ+಑ᔯ(’s education), ಑ᔯ+ሇ(education ’s)
ࣣච+ሇ(school ’s), ळ৯+ᄪ(youth class), ᐆᤌ+಑ᔯ(quality of education)
Оୣ+߯Ԧ(expert cultivation), ࠵ࣣ+චᮿ(university chancellor)
቉+ٍ(famous), ୛+ᴱච(high-school), ሇ+ࣣᅴ(’s student)
ڔ+የѕұᰧ(and social security), ሇ+ीχ(’s employment),
࠼χ+Оٽ(unemployed ofﬁcer), ीχ+७Ѭ(employment position)
ሇ+኎ፀ(’s research), ρए+ࣣᓥ(expert people), ᎋ+Ეߨ(etc area)
ᅴႪ+୼ഴ(biological technology), ኎ፀ+ୄ൤(research result)
ОᏚ+߰ݿᐵ(Human DNA sequence), ሇ+߰ݿ(’s DNA)
ᅴႪ+୼ഴ(biological technology), ᕃᔿ+৭ᐷᕇ(embryo stem cell)

Table 2: Character Error Rates (Word perplexity) on the RT04 test set. Bigram LSA was applied in
addition to unigram LSA.

LM (13M)

CCTV

NTDTV

RFA

OVERALL

background LM
+unigram LSA
+bigram LSA (Kneser-Ney, 30 topics)
+bigram LSA (Witten-Bell)
+bigram LSA (Kneser-Ney)

15.3% (748)
14.4 (629)
14.5 (604)
14.1 (594)
14.0 (587)

21.8 (1718)
21.5 (1547)
20.7 (1502)
20.9 (1452)
20.8 (1448)

39.5 (3655)
38.9 (3015)
39.0 (2736)
38.3 (2628)
38.2 (2586)

24.9
24.3
24.1
23.8
23.7

4.1 LM adaptation results

Table 1 shows the correlated bigram topics sorted by the joint bigram probability p(v |u, k) · p(u|k).
Most of the top bigrams appear either as phrases or words attached with a stopword such as ሇ(’s in
English). Table 2 shows the LM adaptation results in CER and perplexity. Applying both unigram
and bigram LSA yields consistent improvement over unigram LSA in the range of 6.4%–8.5%
relative reduction in perplexity and 2.5% relative reduction in the overall CER. The CER reduction is
statistically signi ﬁcant at 0.1% signi ﬁcance level. We com pared our proposed fractional Kneser-Ney
smoothing with Witten-Bell smoothing which also supports fractional counts. The results showed
that Kneser-Ney smoothing performs slightly better than Witten-Bell smoothing.
Increasing the
number of topics in bigram LSA helps despite model sparsity. We applied extra EM iterations on
top of the bootstrapped bigram LSA but no further performance improvement was observed.

4.2 Large-scale evaluation

We evaluated our approach using the CMU-InterACT vowelized Arabic transcription system dis-
criminatively trained on 1500-hour transcribed audio using MMIE for the GALE Phase-3 evaluation.
A large background 4-gram LM was trained using 962M-word text corpora with 737k vocabulary.
Unigram and bigram LSA were trained on the same corpora and were applied to lattice rescoring on
Dev07 and unseen Dev08 test sets with 2.6-hour and 3-hour audio shows containing broadcast news
(BN) and broadcast conversation (BC) genre. Table 3 shows that bigram LSA rescoring reduces the
overall word error rate by more than 3.0% relative compared to the unadapted baseline on both sets
which are statistically signi ﬁcant at 0.1% signi ﬁcance lev
el. However, degradation is observed using
trigram LSA compared to bigram LSA which may be due to data sparseness.

Table 3: Lattice rescoring results in word error rate on Dev07 (unseen Dev08) using the CMU-
InterACT Arabic transcription system for the GALE Phase-3 evaluation.
GALE LM (962M)
BN
BC OVERALL
14.3 (16.4)
11.6% 19.4
background LM
14.2 (16.3)
19.2
11.5
+unigram LSA
11.0
13.9 (15.9)
19.0
+bigram LSA (Witten-Bell)
13.8 (15.9)
11.0
18.9
+bigram LSA (Kneser-Ney)
18.8
+trigram LSA (Kneser-Ney)
11.3
14.0 (-)

5 Conclusion

We present a correlated bigram LSA approach for unsupervised LM adaptation for ASR. Our con-
tributions include efﬁcient variational EM for model train ing and fractional Kneser-Ney approach
for LM smoothing with fractional counts. Bigram LSA yields additional improvement in both per-
plexity and recognition performance in addition to unigram LSA. Increasing the number of topics
for bigram LSA helps despite the model sparsity. Bootstrapping bigram LSA from unigram LSA
saves computation and memory requirement during EM training. Our approach is scalable to large
training corpora and works well on different languages. The improvement from bigram LSA is
statistically signi ﬁcant compared to the unadapted baseli ne. Future work includes applying the pro-
posed approach for statistical machine translation.

Acknowledgement

We would like to thank Mark Fuhs for help parallelizing the bigram LSA training via condor.

References

in

in

[1] J. R. Bellegarda, “Large Vocabulary Speech Recognition with Multispan Statistical Language
Models,”
IEEE Transactions on Speech and Audio Processing, vol. 8, no. 1, pp. 76–84, Jan
2000.
[2] D. Blei, A. Ng, and M. Jordan, “Latent Dirichlet Allocati on,” in Journal of Machine Learning
Research, 2003, pp. 1107–1135.
[3] Y. C. Tam and T. Schultz, “Language model adaptation usin g variational Bayes inference,” in
Proceedings of Interspeech, 2005.
[4] D. Mrva and P. C. Woodland, “Unsupervised language model adaptation for mandarin broad-
cast conversation transcription,” in Proceedings of Interspeech, 2006.
[5] T. Grifﬁths, M. Steyvers, D. Blei, and J. Tenenbaum,
“Int
egrating topics and syntax,”
Advances in Neural Information Processing Systems, 2004.
[6] B. J. Hsu and J. Glass,
“Style and topic language model ada ptation using HMM-LDA,”
Proceedings of Empirical Methods on Natural Language Processing (EMNLP), 2006.
[7] Hanna M. Wallach,
“Topic Modeling: Beyond Bag-of-Words, ” in International Conference
on Machine Learning, 2006.
[8] P. Xu, A. Emami, and F. Jelinek, “Training connectionist models for the structured language
model,”
in Proceedings of Empirical Methods on Natural Language Processing (EMNLP),
2003.
[9] R. Kneser and H. Ney, “Improved backing-off for M-gram la nguage modeling,” in Proceedings
of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
1995, vol. 1, pp. 181–184.
[10] R. Kneser, J. Peters, and D. Klakow, “Language model ada ptation using dynamic marginals,”
in Proceedings of European Conference on Speech Communication and Technology (EU-
ROSPEECH), 1997, pp. 1971–1974.
[11] R. Iyer and M. Ostendorf, “Modeling long distance depen dence in language: Topic mixtures
versus dynamic cache models,”
IEEE Transactions on Speech and Audio Processing, vol. 7,
no. 1, pp. 30–39, Jan 1999.
[12] X. Wang, A. McCallum, and X. Wei, “Topical N-grams: Phra se and topic discovery, with an
application to information retrieval,” in IEEE International Conference on Data Mining, 2007.
[13] T. Minka, “The dirichlet-tree distribution,” 1999.
[14] Y. C. Tam and T. Schultz, “Correlated latent semantic mo del for unsupervised language model
adaptation,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), 2007.
[15] A. Stolcke,
“SRILM - an extensible language modeling to olkit,” in Proceedings of Interna-
tional Conference on Spoken Language Processing (ICSLP), 2002.

